"""
Generated LLM strategies for social dilemma games.

This file contains strategy classes generated by LLMs for game theory experiments.
Each strategy is a callable class that implements a specific approach to the game.

Generated with:
- Provider: Client
- Model: gemini-2.5-flash
- Game: public_goods
"""

from dataclasses import dataclass
from enum import Enum
import math
import numpy as np
from numpy.typing import NDArray
import random

from emergent_llm.players import BaseStrategy
from emergent_llm.games import PublicGoodsDescription, CollectiveRiskDescription, CommonPoolDescription, CommonPoolState
from emergent_llm.common import Action, C, D, PlayerHistory, GameState


class Strategy_COLLECTIVE_1(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    Prioritizes establishing and sustaining cooperation by balancing initial trust,
    responsive adaptation to group behavior, and understanding of finite repeated games.
    It adapts its cooperation threshold based on game parameters (n, k) to align
    with collective benefit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.max_window_size = 5
        self.Theta = (self.game_description.n_players - self.game_description.k + 1) / self.game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        last_round_0_indexed = self.game_description.n_rounds - 1
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == last_round_0_indexed:
            return Action.D
        actual_window_size = min(self.max_window_size, current_round_0_indexed)
        total_cooperators_in_window = 0
        if actual_window_size > 0:
            start_index_for_history = current_round_0_indexed - actual_window_size
            for i in range(start_index_for_history, current_round_0_indexed):
                my_cooperation = int(history.my_actions[i])
                opponent_cooperation = np.sum(history.opponent_actions[i, :])
                total_cooperators_in_window += my_cooperation + opponent_cooperation
            P_avg = total_cooperators_in_window / actual_window_size / self.game_description.n_players
        else:
            return Action.C
        if P_avg >= self.Theta:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_2(BaseStrategy):
    """
    The Adaptive Collective Reciprocator (ARC) strategy for the N-Player Public Goods Game.
    It aims to maximize collective payoff by promoting conditional cooperation.
    ARC starts by cooperating and then adjusts its behavior based on the observed
    level of cooperation from all players in previous rounds. It employs a dynamic
    threshold to decide on continued cooperation, ensuring robustness against
    free-riders while pursuing high collective payoffs. Specific rules are applied
    for the first and last rounds to optimize performance in a repeated game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        min_individual_return_threshold = math.ceil(n / k)
        majority_cooperation_threshold = math.ceil(n / 2)
        self.final_cooperation_threshold = max(min_individual_return_threshold, majority_cooperation_threshold)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_rounds:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            my_action_prev_round_is_cooperate = history.my_actions[-1]
            num_cooperators_prev_round = num_opponent_cooperators_prev_round + int(my_action_prev_round_is_cooperate)
            if num_cooperators_prev_round >= self.final_cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_3(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for N-player Public Goods Game.
    ACR aims to maximize the collective payoff by fostering and maintaining cooperation among
    all players, while being robust to different opponent behaviors. It does this by being
    initially cooperative and then reciprocally adjusting its behavior based on the observed
    level of cooperation in previous rounds. It also incorporates end-game rationality by
    defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        k_normalized = (self.k_factor - 1) / (self.n_players - 1)
        T_c_proportion = 1 - k_normalized / 2
        self.T_c = max(1, round(self.n_players * T_c_proportion))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        elif current_round == self.n_rounds - 1:
            return Action.D
        num_my_cooperation_prev_round = int(history.my_actions[-1])
        num_opponent_cooperation_prev_round = np.sum(history.opponent_actions[-1, :])
        m_prev_round = num_my_cooperation_prev_round + num_opponent_cooperation_prev_round
        if m_prev_round >= self.T_c:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_4(BaseStrategy):
    """
    The Adaptive Collective N-Player Tit-for-Tat (ACNTFT) strategy is designed to
    promote and sustain cooperation in the N-Player Public Goods Game. It starts
    with cooperation, then adapts its behavior based on a dynamically calculated
    Minimum Cooperation Threshold (MCT) from the previous round's collective effort.
    It defects in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.mct = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            cooperators_from_opponents = np.sum(history.opponent_actions[-1, :])
            my_prev_action_cooperated = history.my_actions[-1]
            C_prev = cooperators_from_opponents + my_prev_action_cooperated
            if C_prev >= self.mct:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_5(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR starts with cooperation to establish trust, then adapts its cooperation
    based on the observed collective cooperation level in the previous round.
    It becomes stricter as the game approaches its end, preparing for the
    inevitable unraveling of cooperation, and defects in the very last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T_base = 0.5
        self.num_strict_endgame_rounds = 0
        if self.r > 2:
            self.num_strict_endgame_rounds = max(1, min(math.floor(self.r / 4), 3))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        if current_round == 1:
            return Action.C
        if current_round == self.r:
            return Action.D
        opponent_cooperators_prev = sum(history.opponent_actions[-1, :])
        my_action_prev_was_C = history.my_actions[-1]
        C_prev_round = opponent_cooperators_prev + (1 if my_action_prev_was_C else 0)
        p_C_prev = C_prev_round / self.n
        T_current = self.T_base
        rounds_until_last_round = self.r - current_round
        if self.num_strict_endgame_rounds > 0 and rounds_until_last_round <= self.num_strict_endgame_rounds:
            progress_factor = (self.num_strict_endgame_rounds - rounds_until_last_round + 1) / self.num_strict_endgame_rounds
            T_current = self.T_base + (1.0 - self.T_base) * progress_factor
            T_current = min(T_current, 1.0)
            T_current = max(T_current, self.T_base)
        if p_C_prev >= T_current:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_6(BaseStrategy):
    """
    The "Forgiving Vigilante" strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation through conditional reciprocity while
    maintaining robustness against defection. It starts by cooperating to signal
    a willingness for collective benefit. In subsequent rounds, it assesses the
    overall level of cooperation in the previous round.

    If a "sufficient" number of players cooperated (defined by `Coop_Threshold = ceil(n/k)`),
    it continues to cooperate. If cooperation drops below this critical threshold,
    it defects to avoid exploitation, but remains ready to resume cooperation if
    others increase their contributions. The last round is handled with strategic
    caution by defecting, reflecting the typical end-game rationality in finite
    repeated games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_my_action_int = int(history.my_actions[-1])
        prev_round_opponent_cooperators_count = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = prev_round_my_action_int + prev_round_opponent_cooperators_count
        if total_cooperators_prev_round >= self.coop_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_7(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) Strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by starting cooperatively and then
    adapting its behavior based on the observed level of cooperation in the group,
    while protecting itself from persistent exploitation. It incorporates a dynamic
    threshold for cooperation and addresses the specific challenges of the first
    and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.dynamic_cooperation_threshold = 1.0 - self.k / self.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        elif current_round_idx == self.n_rounds - 1:
            return Action.D
        else:
            num_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            proportion_cooperators_prev_round = num_cooperators_prev_round / self.n_players
            if proportion_cooperators_prev_round >= self.dynamic_cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_8(BaseStrategy):
    """
    Adaptive Collective Reciprocator with End-Game Defection (ACRED) strategy
    for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by being initially cooperative,
    then adapting its behavior based on the observed collective cooperation
    in previous rounds. It employs a dynamic threshold based on game parameters
    (n and k) to decide whether to continue cooperating or defect.
    Crucially, it acknowledges the finite nature of the game by defecting
    in the very last round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ACRED strategy with game parameters.

        Args:
            game_description: A PublicGoodsDescription object containing
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.threshold_cooperators = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the ACRED strategy.

        Args:
            state: A GameState object containing the current round number.
            history: A PlayerHistory object containing past actions and payoffs,
                     or None if it's the very first round.

        Returns:
            An Action (C for Cooperate, D for Defect).
        """
        current_round_0_indexed = state.round_number
        last_round_0_indexed = self.game_description.n_rounds - 1
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == last_round_0_indexed:
            return Action.D
        else:
            my_prev_action_was_cooperation = int(history.my_actions[-1])
            opponent_prev_cooperators = sum(history.opponent_actions[-1, :])
            previous_round_total_cooperators = my_prev_action_was_cooperation + opponent_prev_cooperators
            if previous_round_total_cooperators >= self.threshold_cooperators:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_9(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy combines an initial cooperative stance with an adaptive,
    parameter-driven threshold for defection, and accounts for end-game dynamics.
    It aims to foster cooperation while being robust against exploitation.

    Core Principles:
    - Optimistic Start: Begin by cooperating to signal willingness and explore potential.
    - Adaptive Reciprocity: Cooperate as long as the observed level of defection among
      other players does not exceed a dynamically calculated tolerance threshold.
    - Parametric Threshold: The tolerance for defection is directly derived from the
      game's core parameters (n and k), reflecting how costly defection is to the collective good.
    - Endgame Rationality: Defect in the final round to avoid exploitation, anticipating
      rational defection from others.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.defection_tolerance_threshold = max(0, math.floor(self.n / self.k) - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.r - 1:
            return Action.D
        else:
            my_action_prev_round_was_C = history.my_actions[current_round_0_indexed - 1]
            opponent_actions_prev_round_were_C = history.opponent_actions[current_round_0_indexed - 1, :]
            num_cooperators_prev_round = np.sum(opponent_actions_prev_round_were_C) + (1 if my_action_prev_round_was_C else 0)
            num_defectors_prev_round = self.n - num_cooperators_prev_round
            if num_defectors_prev_round <= self.defection_tolerance_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_10(BaseStrategy):
    """
    The Dynamic Collective Reciprocator (DCR) strategy for the N-Player Public Goods Game.
    It begins with initial trust by cooperating in the first round. In intermediate rounds,
    it practices adaptive reciprocity: it continues to cooperate if the number of defectors
    in the previous round is below a dynamically calculated threshold, and defects otherwise.
    This threshold is determined by `math.floor(k - 1.0)`, making the strategy more forgiving
    when the collective benefit (k) is high. In the final round, it defects due to terminal rationality.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        my_action_prev_was_cooperate = history.my_actions[-1]
        num_cooperators_prev = num_opponent_cooperators_prev + (1 if my_action_prev_was_cooperate else 0)
        num_defectors_prev = self.n_players - num_cooperators_prev
        maximum_tolerated_defectors = math.floor(self.k_factor - 1.0)
        if num_defectors_prev <= maximum_tolerated_defectors:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_11(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    The strategy starts by cooperating to signal a willingness for collective action.
    In subsequent rounds, it monitors the number of players who cooperated in the
    previous round. If this number meets or exceeds a dynamically calculated
    "cooperation threshold" (ceil(n/k)), the strategy continues to cooperate.
    Otherwise, it defects to penalize insufficient collective effort.
    The strategy always defects in the final round to account for backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        r = self.game_description.n_rounds
        if t == 1:
            return Action.C
        if t == r:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_is_C = history.my_actions[-1]
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + int(my_action_prev_round_is_C)
        if total_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_12(BaseStrategy):
    """
    The Adaptive Majority Cooperation (AMC) strategy for the N-Player Public Goods Game.

    AMC initiates with cooperation to signal willingness for collective gain. In subsequent
    intermediate rounds, it adaptively cooperates if the total number of cooperators in
    the previous round met or exceeded a threshold (floor(n/2)). This encourages and
    sustains collective effort. In the final round, it strategically defects to
    maximize individual payoff in the game's endgame, recognizing the absence of future
    interactions. This balances proactive cooperation with robust, adaptive responses.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the AMC strategy rules.

        Args:
            state: A GameState object containing the current round number (0-indexed).
            history: A PlayerHistory object containing past actions and payoffs,
                     or None if it's the first round (round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        elif current_round_idx == self.n_rounds - 1:
            return Action.D
        else:
            my_prev_action_was_C = history.my_actions[-1]
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            total_cooperators_prev_round = int(my_prev_action_was_C) + num_opponent_cooperators_prev_round
            cooperation_threshold = math.floor(self.n_players / 2)
            if total_cooperators_prev_round >= cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_13(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to initiate cooperation, maintain it when the collective
    contribution is sufficient, and defect when cooperation levels drop below
    a calculated threshold or in the final round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold_T_C = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        if history is None:
            return Action.C
        previous_round_0_indexed = current_round_0_indexed - 1
        my_prev_action_was_cooperate = history.my_actions[previous_round_0_indexed]
        opponent_prev_cooperators = np.sum(history.opponent_actions[previous_round_0_indexed, :])
        m_previous = int(my_prev_action_was_cooperate) + opponent_prev_cooperators
        if m_previous >= self.cooperation_threshold_T_C:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_14(BaseStrategy):
    """
    Implements the "Collective Reciprocity with Dynamic Forgiveness" strategy
    for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation by starting
    cooperatively, dynamically adapting its response to group behavior based
    on game parameters (specifically 'k' and 'n'), and rationally defecting
    in the end game. It calculates a dynamic cooperation threshold (TC) to
    determine the required level of collective cooperation to continue
    contributing to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n_players = self.game_description.n_players
        k_factor = self.game_description.k
        self.TC = max(2, math.floor(n_players - (k_factor - 1)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            previous_round_idx = state.round_number - 1
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_idx])
            my_action_prev_round_was_cooperation = int(history.my_actions[previous_round_idx])
            num_total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_action_prev_round_was_cooperation
            if num_total_cooperators_prev_round >= self.TC:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_15(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy initiates cooperation, then conditionally cooperates based on the
    collective behavior of the group in the previous round, and defects in the final round.

    Core Philosophy: ACR attempts to establish and maintain a high level of collective
    cooperation by mirroring the observed collective behavior of other players. It starts
    with an act of trust, then rewards high levels of group cooperation and punishes
    widespread defection. It protects itself from exploitation, especially in the game's
    final stages.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_zero_indexed == 0:
            return Action.C
        elif current_round_zero_indexed == total_rounds - 1:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            my_action_prev_round_was_cooperate = history.my_actions[-1]
            total_cooperators_prev_round = num_opponent_cooperators_prev_round + int(my_action_prev_round_was_cooperate)
            if total_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_16(BaseStrategy):
    """
    The "Collective Progress" strategy aims to foster and sustain a high level of cooperation
    among players to maximize the collective payoff, while simultaneously protecting itself
    from exploitation. It achieves this by starting cooperatively and then adapting its
    behavior based on the observed collective cooperation rate in previous rounds, with a
    clear endpoint strategy for the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = 1.0 / self.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        num_cooperators_previous_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        cooperation_rate_previous_round = num_cooperators_previous_round / self.n
        if cooperation_rate_previous_round > self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_17(BaseStrategy):
    """
    The Adaptive Collective Cooperator (ACC) strategy aims to foster and maintain cooperation by starting
    cooperatively and then adapting its behavior based on the observed level of collective cooperation
    in previous rounds. It employs a dynamic threshold to determine when cooperation is sufficiently
    supported by the group, balancing the pursuit of the collective good with the need for self-protection
    against free-riders.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        prev_round_0_indexed = state.round_number - 1
        C_prev = int(history.my_actions[prev_round_0_indexed]) + np.sum(history.opponent_actions[prev_round_0_indexed, :])
        if C_prev == 0:
            return Action.D
        P_C = C_prev / self.n
        Threshold_P = 1 - self.k / self.n
        if P_C >= Threshold_P:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_18(BaseStrategy):
    """
    Adaptive Reciprocal Cooperation (ARC) strategy for the N-Player Public Goods Game.
    This strategy is designed to promote and sustain cooperation by being initially generous,
    conditionally reciprocal, and strategically protective. It uses a clear, efficiency-based
    threshold (ceil(n/k)) to determine when collective effort is sufficient to warrant
    continued cooperation. It starts with cooperation, adapts its behavior in intermediate
    rounds based on the previous round's collective cooperation, and defects in the final
    round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.C_threshold = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        elif current_round_number == self.n_rounds - 1:
            return Action.D
        else:
            prev_round_idx = current_round_number - 1
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
            my_prev_action_was_C = history.my_actions[prev_round_idx]
            total_cooperators_prev_round = opponent_cooperators_prev_round + (1 if my_prev_action_was_C else 0)
            if total_cooperators_prev_round >= self.C_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_19(BaseStrategy):
    """
    Adaptive Reciprocal Public Goods (ARPG) strategy.

    ARPG aims to maximize collective cooperation by initiating cooperation
    and maintaining it as long as other players demonstrate a sufficient
    collective commitment. It uses a dynamically calculated threshold to adapt
    its responsiveness based on the game's incentive structure, while
    pragmatically defecting in the final round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            cooperators_prev_round = sum(history.opponent_actions[-1, :])
            if history.my_actions[-1]:
                cooperators_prev_round += 1
            cooperation_rate_prev = float(cooperators_prev_round) / self.n_players
            threshold_coop = 1.0 - self.k_factor / self.n_players
            if cooperation_rate_prev > threshold_coop:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_20(BaseStrategy):
    """
    Adaptive Reciprocity with Forgiveness (ARF) strategy for the N-Player Public Goods Game.

    This strategy aims to initiate cooperation, monitor the group's collective behavior,
    reward sufficient cooperation, and punish widespread defection, while accounting for
    the "end-game effect."

    Decision Rules:
    1. Round 1: Always Cooperate to signal willingness for cooperation.
    2. Intermediate Rounds (1 < t < r):
       Evaluate the total number of cooperators in the previous round.
       If the number of cooperators meets or exceeds `ceil(n / 2)` (cooperation_threshold),
       then Cooperate. Otherwise, Defect to punish low cooperation.
    3. Last Round (t=r): Always Defect due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_cooperators_prev_round = 0
        num_cooperators_prev_round += np.sum(history.opponent_actions[-1, :])
        if history.my_actions[-1] == True:
            num_cooperators_prev_round += 1
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_21(BaseStrategy):
    """
    Dynamic Tolerance Reciprocity (DTR) strategy for the N-Player Public Goods Game.

    This strategy starts by cooperating to establish a baseline. In intermediate
    rounds, it adaptively adjusts its expectation of collective cooperation (Dynamic Cooperation Threshold - DCT).
    If the observed cooperation level from the previous round falls below this dynamic threshold,
    the strategy defects to punish free-riders and prevent further exploitation.
    In the final round, it defects based on backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.initial_cooperation_rate_target = 0.8
        self.final_cooperation_rate_target = 0.99

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        c_prev_my_action = int(history.my_actions[-1])
        c_prev_opponent_actions = sum(history.opponent_actions[-1, :])
        c_prev = c_prev_my_action + c_prev_opponent_actions
        current_cooperation_rate_target = self.initial_cooperation_rate_target
        if self.n_rounds > 2:
            progress_ratio_denominator = self.n_rounds - 2
            progress_ratio_numerator = current_round_0_indexed - 1
            progress_ratio = progress_ratio_numerator / progress_ratio_denominator
            current_cooperation_rate_target = self.initial_cooperation_rate_target + progress_ratio * (self.final_cooperation_rate_target - self.initial_cooperation_rate_target)
        dct = max(1, math.ceil(self.n_players * current_cooperation_rate_target))
        if c_prev >= dct:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_22(BaseStrategy):
    """
    The Collective Adaptive Threshold (CAT) strategy aims to establish and maintain a high level of
    cooperation within the N-Player Public Goods Game. It initiates cooperation in the first round,
    reciprocates cooperation when a sufficient collective effort is observed in intermediate rounds,
    and punishes insufficient cooperation by temporarily defecting. It also incorporates end-game
    rationality by defecting in the final round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        if current_round == 1:
            return Action.C
        if current_round == self.r:
            return Action.D
        my_prev_action_was_cooperate = history.my_actions[-1]
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        num_total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_prev_action_was_cooperate else 0)
        if num_total_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_23(BaseStrategy):
    """
    The Collective Monitor strategy for the N-Player Public Goods Game.
    It initiates cooperation in the first round, defects in the last round,
    and in intermediate rounds, cooperates if a critical mass (ceil(n/2))
    of players cooperated in the previous round, otherwise defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            prev_round_history_idx = state.round_number - 1
            C_prev_round = int(history.my_actions[prev_round_history_idx]) + np.sum(history.opponent_actions[prev_round_history_idx, :])
            T = math.ceil(self.n_players / 2)
            if C_prev_round >= T:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_24(BaseStrategy):
    """
    The Adaptive Collective Reciprocity (ACR) strategy promotes and sustains cooperation
    in the N-Player Public Goods Game. It initiates cooperation, then conditionally
    continues to cooperate based on the group's overall cooperative effort in the
    previous round (at least half the players cooperating). It defects in the final
    round to prevent exploitation, aligning with game theory's backward induction principle.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / 2.0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        C_total_prev = np.sum(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if C_total_prev >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_25(BaseStrategy):
    """
    The Adaptive Dynamic Reciprocator (ADR) strategy aims to promote collective cooperation
    while remaining robust against exploitation in an N-player Public Goods Game.
    It initiates cooperation in the first round, then adaptively adjusts its willingness
    to cooperate based on observed collective behavior in previous rounds, using an
    exponential moving average. It fosters cooperation when the environment is
    sufficiently collaborative but defects when perceived cooperation levels are low,
    protecting individual interests. It employs backward induction, always defecting
    in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.learning_rate = 0.2
        self.threshold_cooperate_proportion = math.ceil(self.n_players / self.k) / self.n_players
        self.cooperation_score: float | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            self.cooperation_score = 0.5
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        previous_round_actions = history.opponent_actions[current_round_number - 1]
        num_cooperators_prev = np.sum(previous_round_actions)
        P_C_prev = num_cooperators_prev / self.n_players
        self.cooperation_score = (1 - self.learning_rate) * self.cooperation_score + self.learning_rate * P_C_prev
        if self.cooperation_score >= self.threshold_cooperate_proportion:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_26(BaseStrategy):
    """
    This strategy implements the "Adaptive Reciprocity with Stability Threshold" for the N-Player
    Public Goods Game. It initiates cooperation in the first round, dynamically adjusts its
    behavior in intermediate rounds based on the collective cooperation level of the previous
    round relative to a calculated stability threshold, and defects in the final round to
    maximize payoff due to the endgame effect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.target_cooperators_for_stability = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == total_rounds - 1:
            return Action.D
        else:
            cooperators_from_opponents_prev_round = np.sum(history.opponent_actions[-1, :])
            cooperators_from_self_prev_round = history.my_actions[-1]
            total_cooperators_prev_round = cooperators_from_opponents_prev_round + cooperators_from_self_prev_round
            if total_cooperators_prev_round >= self.target_cooperators_for_stability:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_27(BaseStrategy):
    """
    Adaptive Reciprocity with Dynamic Threshold and Endgame Defection for N-Player Public Goods Game.
    This strategy starts cooperatively, adapts its cooperation based on a dynamic threshold derived
    from the game's multiplication factor 'k', and defects in the final round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters and calculates the dynamic cooperation threshold.

        Args:
            game_description: A PublicGoodsDescription object containing n_players, n_rounds, and k.
        """
        self.game_description = game_description
        n = game_description.n_players
        k = game_description.k
        self.T_cooperation_threshold = (n - k) / (n - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the strategy's rules.

        Args:
            state: A GameState object containing the current round number (0-indexed).
            history: A PlayerHistory object (or None for the first round) containing
                     past actions and payoffs.

        Returns:
            An Action enum (C for Cooperate, D for Defect).
        """
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        elif state.round_number == r - 1:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            my_prev_action_val = 1 if history.my_actions[-1] else 0
            num_total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_prev_action_val
            P_C = num_total_cooperators_prev_round / n
            if P_C >= self.T_cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_28(BaseStrategy):
    """
    The Adaptive Majority Cooperate (AMC) strategy for the N-Player Public Goods Game.

    This strategy aims to foster collective cooperation. It starts by cooperating to signal
    trust. In intermediate rounds, it dynamically adjusts its behavior based on whether
    the number of cooperators in the previous round met or exceeded a threshold of
    `ceil(n / 2)`. If the threshold is met, it cooperates; otherwise, it defects as a
    punishment. In the final round, it defects to maximize individual payoff, consistent
    with rational play in finite repeated games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number_0_indexed = state.round_number
        if current_round_number_0_indexed == 0:
            return Action.C
        if current_round_number_0_indexed == self.n_rounds - 1:
            return Action.D
        total_cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if total_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_29(BaseStrategy):
    """
    Adaptive Public Goods Reciprocator (APGR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and sustain collective cooperation by cooperating in the
    first round and conditionally cooperating in intermediate rounds based on the
    observed level of cooperation in the previous round. It defects in the final round
    due to end-game dynamics and also defects in intermediate rounds if collective
    cooperation falls below a critical threshold.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n / 2.0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.r:
            return Action.D
        else:
            cooperators_in_previous_round = np.sum(history.opponent_actions[state.round_number - 1, :]) + int(history.my_actions[state.round_number - 1])
            if cooperators_in_previous_round >= self.n - 1:
                return Action.C
            elif cooperators_in_previous_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_30(BaseStrategy):
    """
    The k-Adaptive Collective Strategy (KACS) aims to maximize the collective payoff
    over the duration of the game by fostering and sustaining cooperation among players.
    It starts cooperatively, adapts its behavior based on the observed level of
    cooperation in previous rounds using a dynamically calculated threshold (T_coop),
    and defects in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_coop = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        N_C_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if N_C_prev >= self.T_coop:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_31(BaseStrategy):
    """
    Adaptive Group Reciprocity (AGR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and maintain cooperation by being initially cooperative,
    then adapting its behavior based on the observed level of cooperation from other players.
    It incorporates mechanisms to reward group contributions and deter persistent free-riding,
    while accounting for the finite nature of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = self.k_factor / self.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_coop_others_prev_round = np.sum(history.opponent_actions[-1, :])
        avg_coop_others = num_coop_others_prev_round / (self.n_players - 1)
        if avg_coop_others >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_32(BaseStrategy):
    """
    Adaptive Proportional Reciprocity (APR) strategy for the N-Player Public Goods Game.
    This strategy initiates cooperation and maintains it as long as a sufficient
    proportion of players contribute to the public good. The "sufficiency" threshold
    is dynamically calculated based on the public good's efficiency (k).
    It defects in the final round (end-game effect).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        C_threshold_raw = self.n * (self.n - self.k) / (self.n - 1)
        self.C_threshold = math.ceil(C_threshold_raw)
        self.C_threshold = max(1, min(self.C_threshold, self.n))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.r - 1:
            return Action.D
        else:
            my_prev_action = history.my_actions[current_round_0_indexed - 1]
            opponents_prev_actions = history.opponent_actions[current_round_0_indexed - 1, :]
            C_prev = my_prev_action + np.sum(opponents_prev_actions)
            if C_prev >= self.C_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_33(BaseStrategy):
    """
    The Collective Reciprocity (CR) strategy for the N-Player Public Goods Game.

    This strategy aims to promote and maintain collective cooperation by starting
    cooperatively and then conditioning subsequent actions on the observed level
    of cooperation in the previous round. It uses an adaptive cooperation benchmark
    derived from game parameters (n, k) and defects in the final round based on
    backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_benchmark = max(1, math.ceil(self.n * self.k / (self.n + self.k)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        num_opponent_cooperators_previous_round = sum(history.opponent_actions[-1, :])
        my_action_previous_round_was_c = history.my_actions[-1]
        total_cooperators_previous_round = num_opponent_cooperators_previous_round + int(my_action_previous_round_was_c)
        if total_cooperators_previous_round >= self.cooperation_benchmark:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_34(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.
    It initiates cooperation, continues it if the observed level of collective cooperation
    is sufficient to make the public good worthwhile (based on a dynamic threshold),
    and reverts to defection if cooperation falters. In the final round, it defects
    due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            prev_round_idx = state.round_number - 1
            my_prev_action_is_c = history.my_actions[prev_round_idx]
            opponent_prev_actions_are_c = history.opponent_actions[prev_round_idx, :]
            num_cooperators_prev_round = int(my_prev_action_is_c) + np.sum(opponent_prev_actions_are_c)
            if num_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_35(BaseStrategy):
    """
    Adaptive Reciprocity with Forgiveness (ARF) strategy for the N-Player Public Goods Game.
    It cooperates in the first round to initiate collective goodwill. In intermediate rounds,
    it observes the total number of cooperators from the previous round and compares it against
    an adaptive threshold derived from game parameters (n and k). If cooperation is sufficient,
    it reciprocates; otherwise, it defects as a protective measure or to punish. In the final
    round, it defects due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold_T_C = math.floor(self.n_players - self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        elif current_round_t == self.n_rounds:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            my_action_prev_round_is_C = int(history.my_actions[-1])
            total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_action_prev_round_is_C
            if total_cooperators_prev_round > self.cooperation_threshold_T_C:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_36(BaseStrategy):
    """
    Collective Conditional Cooperator (CCC) Strategy.

    This strategy aims to establish and maintain a high level of collective cooperation
    to maximize the total group payoff. It achieves this by being initially cooperative,
    rewarding observed collective cooperation, and punishing insufficient cooperation.
    It protects itself from end-game exploitation by defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number_0_indexed = state.round_number
        if current_round_number_0_indexed == 0:
            return Action.C
        if current_round_number_0_indexed == self.n_rounds - 1:
            return Action.D
        num_cooperators_in_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_in_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_37(BaseStrategy):
    """
    The "Conditional Contributor" strategy aims to foster cooperation by starting with trust,
    rewarding sufficient collective contributions, and punishing insufficient contributions.
    It cooperates in the first round to signal willingness. In intermediate rounds, it observes
    the total number of cooperators from the previous round. If this number meets or exceeds
    a dynamically set threshold (ceiling of game parameter 'k'), it cooperates. Otherwise,
    it defects to signal a need for greater collective effort. In the final round, it always
    defects, recognizing the end of the game and the individual rationality principle.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_previous_action_was_cooperate = history.my_actions[-1]
        previous_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        m_prev = int(my_previous_action_was_cooperate) + previous_round_opponent_cooperators
        if m_prev >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_38(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by starting cooperatively and then adapting
    its behavior based on the observed collective contribution in previous rounds.
    It balances collective well-being with self-preservation.

    Decision Rules:
    1. First Round: Cooperate (C) to initiate cooperation.
    2. Last Round: Defect (D) due to backward induction and self-preservation.
    3. Intermediate Rounds:
       - Calculates a cooperation threshold T = floor(n / k).
       - Observes the total number of cooperators (C_prev) in the immediately preceding round.
       - If C_prev >= T, it Cooperates (C), rewarding sufficient collective effort.
       - Else (C_prev < T), it Defects (D), punishing low collective effort and protecting
         individual payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_T = math.floor(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number
        if current_round_t == 0:
            return Action.C
        if current_round_t == self.n_rounds - 1:
            return Action.D
        else:
            num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if num_cooperators_prev_round >= self.cooperation_threshold_T:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_39(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to promote cooperation by initially trusting others and subsequently
    reciprocating the observed collective behavior. It is designed to be "nice" but not "naive,"
    allowing for some forgiveness while quickly reacting to significant declines in collective effort.
    It acknowledges the finite nature of the game by adjusting its behavior in the final round.

    Decision Rules:
    1. Initial Round (t = 1): Always Cooperate (C).
    2. Standard Rounds (1 < t < r):
       - If the total number of cooperators in the previous round (t-1) was >= ceil(n/2), then Cooperate (C).
       - Else (fewer than ceil(n/2) cooperators in t-1), then Defect (D).
    3. End Game (t = r): Always Defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_cooperators_previous_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        cooperation_threshold = math.ceil(self.n_players / 2)
        if num_cooperators_previous_round >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_40(BaseStrategy):
    """
    Strategy Name: Adaptive Collective Cooperator

    This strategy is designed to balance individual rationality with the collective good
    in the N-Player Public Goods Game. It aims to foster and sustain cooperation by
    initially cooperating, then adapting its behavior based on the observed collective
    cooperation level, and acknowledging the rational end-game dynamics. It is robust
    by punishing free-riding and adaptive by reacting to the history of play.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold_cooperators = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_action_prev_round_is_C = history.my_actions[state.round_number - 1]
        opponent_actions_prev_round_is_C = history.opponent_actions[state.round_number - 1, :]
        total_cooperators_prev_round = int(my_action_prev_round_is_C) + np.sum(opponent_actions_prev_round_is_C)
        if total_cooperators_prev_round >= self.threshold_cooperators:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_41(BaseStrategy):
    """
    Adaptive Trust and Retribution (ATR) strategy for the N-Player Public Goods Game.

    ATR promotes cooperation by starting cooperatively and maintaining cooperation
    as long as the average group cooperation meets a dynamic threshold. If cooperation
    falls too low, it switches to defection to protect itself and punish. It defects
    in the final round to maximize individual payoff when no future influence is possible.
    The strategy is adaptive to game parameters (k, n) and observed historical behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_0_indexed == 0:
            return Action.C
        prev_round_idx = current_round_0_indexed - 1
        total_cooperators_in_prev_round = np.sum(history.opponent_actions[prev_round_idx, :]) + (1 if history.my_actions[prev_round_idx] else 0)
        p_prev = total_cooperators_in_prev_round / n
        self.cooperation_history.append(p_prev)
        if current_round_1_indexed == r:
            return Action.D
        average_p = np.mean(self.cooperation_history)
        dynamic_punishment_threshold = (n - k) / (n - 1)
        if average_p >= dynamic_punishment_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_42(BaseStrategy):
    """
    Collective Adaptive Reciprocity (CAR) Strategy for N-Player Public Goods Game.
    It aims to foster and sustain collective cooperation by being conditionally cooperative,
    adapting its behavior based on the observed actions of the entire group. It is robust
    to various opponent behaviors while also protecting itself from persistent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = max(2, math.floor(self.n_players / 2) + 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        elif current_round_t == self.n_rounds:
            return Action.D
        else:
            prev_round_my_action_bool = history.my_actions[state.round_number - 1]
            prev_round_opponent_actions_bool = history.opponent_actions[state.round_number - 1, :]
            num_cooperators_prev_round = int(prev_round_my_action_bool) + np.sum(prev_round_opponent_actions_bool)
            if num_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_43(BaseStrategy):
    """
    The "Dynamic Group Cooperator" strategy starts by cooperating. In subsequent
    rounds, it observes the total cooperation level of the group in the
    previous round. If a sufficient number of players (defined by
    'cooperation_threshold_count') cooperated, it continues to cooperate.
    Otherwise, it defects to discourage free-riding and protect itself.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_count = (self.game_description.n_players + 1) // 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_index = state.round_number - 1
        my_prev_action = history.my_actions[previous_round_index]
        opponent_prev_actions = history.opponent_actions[previous_round_index, :]
        num_cooperators_in_prev_round = int(my_prev_action) + sum(opponent_prev_actions)
        if num_cooperators_in_prev_round >= self.cooperation_threshold_count:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_44(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster and sustain collective cooperation by:
    1. Initiating Cooperation: Starting with an act of trust in the first round.
    2. Conditional Cooperation: Continuing to cooperate only if a significant portion (defined
       as a simple majority) of other players also cooperated in the previous round.
    3. Punishing Insufficient Cooperation: Defecting when collective cooperation falls
       below the Majority_Threshold, protecting against exploitation.
    4. Forgiveness: Being ready to re-establish cooperation as soon as others collectively
       demonstrate a return to cooperation above the threshold.
    5. Endgame Rationality: Defecting in the final round to prevent exploitation when no
       future interactions exist to incentivize cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.majority_threshold = math.ceil(self.n_players / 2.0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[-1, :]
        num_cooperators_in_previous_round = np.sum(previous_round_opponent_actions)
        if num_cooperators_in_previous_round >= self.majority_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_45(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy balances an initial cooperative stance with adaptive responses to
    observed collective behavior. It maintains a dynamic "forgiveness level"
    which adjusts based on the proportion of cooperators in the previous round,
    aiming to foster cooperation while protecting against exploitation.
    The strategy incorporates backward induction for the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.forgiveness_level = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        c_count_previous_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        p_coop_previous_round = c_count_previous_round / self.n
        if p_coop_previous_round >= self.forgiveness_level:
            self.forgiveness_level = min(self.forgiveness_level + 0.05, 0.95)
            return Action.C
        else:
            self.forgiveness_level = max(self.forgiveness_level - 0.1, 0.4)
            return Action.D

class Strategy_COLLECTIVE_46(BaseStrategy):
    """
    The Robust Adaptive Reciprocity (RAR) strategy for N-Player Public Goods Game.
    This strategy aims to maximize collective payoff by encouraging universal cooperation.
    It achieves this through conditional cooperation, using dynamic thresholds based on
    the game's parameters and observed history. It incorporates forgiveness and a testing
    mechanism to allow for recovery from periods of defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        self.cooperation_mode: bool = True
        self.rounds_in_current_mode: int = 0
        self.consecutive_mixed_rounds: int = 0
        self.COOPERATION_TARGET_THRESHOLD: int = n - 1
        self.DEFECTION_TRIGGER_THRESHOLD: int = max(1, math.floor(n * (1 - (k - 1) / (n - 1))))
        self.TEST_COOPERATION_THRESHOLD: int = max(1, math.floor(n / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if state.round_number == 0:
            self.cooperation_mode = True
            self.rounds_in_current_mode = 0
            self.consecutive_mixed_rounds = 0
            return Action.C
        my_last_action_was_C: bool = history.my_actions[-1]
        opponent_cooperators_last_round: int = np.sum(history.opponent_actions[-1, :])
        C_prev: int = opponent_cooperators_last_round + (1 if my_last_action_was_C else 0)
        if current_round_1_indexed == r:
            if self.cooperation_mode is True and C_prev == n:
                return Action.C
            else:
                return Action.D
        self.rounds_in_current_mode += 1
        if self.cooperation_mode is True:
            if C_prev >= self.COOPERATION_TARGET_THRESHOLD:
                self.consecutive_mixed_rounds = 0
                return Action.C
            elif C_prev < self.DEFECTION_TRIGGER_THRESHOLD:
                self.cooperation_mode = False
                self.rounds_in_current_mode = 1
                self.consecutive_mixed_rounds = 0
                return Action.D
            else:
                self.consecutive_mixed_rounds += 1
                if self.consecutive_mixed_rounds >= 3:
                    self.cooperation_mode = False
                    self.rounds_in_current_mode = 1
                    self.consecutive_mixed_rounds = 0
                    return Action.D
                else:
                    return Action.C
        elif C_prev >= self.COOPERATION_TARGET_THRESHOLD and self.rounds_in_current_mode >= 2:
            self.cooperation_mode = True
            self.rounds_in_current_mode = 1
            self.consecutive_mixed_rounds = 0
            return Action.C
        elif self.rounds_in_current_mode >= 5 and C_prev >= self.TEST_COOPERATION_THRESHOLD:
            self.cooperation_mode = True
            self.rounds_in_current_mode = 1
            self.consecutive_mixed_rounds = 0
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_47(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and sustain collective cooperation by being
    conditionally cooperative, adapting to the observed behavior of the group,
    and maintaining robustness against widespread defection. It acknowledges
    the game's finite nature with a pragmatic approach to the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if num_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_48(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to initiate cooperation, sustain it when the collective shows
    sufficient commitment, and revert to self-interest when cooperation breaks down
    or when the game is about to end. It relies on a simple, widely understood
    threshold for collective reciprocity (0.5).

    Decision Rules:
    1. First Round: Cooperate (C) to initiate goodwill.
    2. Last Round: Defect (D) due to backward induction (no future consequences).
    3. Intermediate Rounds:
       - Calculate the proportion of players who cooperated in the immediately
         preceding round.
       - If this proportion is >= 0.5 (Cooperation Threshold), then Cooperate (C).
       - Else (proportion < 0.5), then Defect (D) to protect against exploitation.
    """
    COOPERATION_THRESHOLD = 0.5

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        num_cooperators_opponents_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_as_int = int(history.my_actions[-1])
        num_cooperators_prev_round = num_cooperators_opponents_prev_round + my_action_prev_round_as_int
        proportion_cooperators_prev_round = num_cooperators_prev_round / self.n_players
        if proportion_cooperators_prev_round >= self.COOPERATION_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_49(BaseStrategy):
    """
    The Adaptive Collective Cooperator (ACC) strategy for the N-Player Public Goods Game.
    This strategy promotes cooperation by starting with C, adapts its tolerance for defectors
    based on the 'k' parameter, and defects in the final round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            N_C_previous_round = history.my_actions[-1] + sum(history.opponent_actions[-1, :])
            min_cooperators_for_C = max(1, self.n_players - math.floor(self.k_factor))
            if N_C_previous_round >= min_cooperators_for_C:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_50(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    Core Philosophy:
    The ACR strategy is built on the principle of conditional cooperation. It starts by
    extending trust, then continuously monitors the collective effort of the group. If the
    observed level of cooperation meets a certain "worthwhile" threshold derived from the
    game's parameters, it continues to cooperate. If not, it signals disappointment and defects,
    aiming to incentivize a return to higher cooperation. It also incorporates rational
    end-game behavior to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators_prev_round >= self.k:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_51(BaseStrategy):
    """
    Adaptive Reciprocal Cooperation (ARC) strategy for the N-Player Public Goods Game.

    ARC initiates cooperation in the first round, adapts its behavior in intermediate rounds
    based on the proportion of cooperators in the previous round relative to a game-parameter-derived
    threshold (1/k), and defects in the final round to maximize individual payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.t_arc = 1.0 / self.game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        num_players = self.game_description.n_players
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == total_rounds - 1:
            return Action.D
        else:
            previous_round_idx = current_round_0_indexed - 1
            total_cooperators_previous_round = int(history.my_actions[previous_round_idx]) + np.sum(history.opponent_actions[previous_round_idx, :])
            proportion_cooperators = total_cooperators_previous_round / num_players
            if proportion_cooperators >= self.t_arc:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_52(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to establish and sustain cooperation by starting with a cooperative gesture
    and then conditionally responding to the overall cooperation level of the group.
    It uses a dynamically adjusted threshold based on the game's parameters (k/n)
    to decide when to cooperate or defect, ensuring it is adaptive and robust.

    Decision Rules:
    1. First Round (t=0, 0-indexed): Cooperate (C) to signal willingness for collective action.
    2. Subsequent Intermediate Rounds (0 < t < r-1, 0-indexed):
       - Calculate Cooperation Rate of Others (CRO) in the previous round (t-1).
       - Define cooperation_threshold = k / n (from game parameters).
       - If CRO_t-1 >= cooperation_threshold, then Cooperate (C).
       - Else, Defect (D).
    3. Last Round (t=r-1, 0-indexed): Defect (D) due to backward induction (no future consequences).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        num_other_cooperators = sum(history.opponent_actions[-1, :])
        n_players_excluding_self = self.n_players - 1
        CRO_t_minus_1 = num_other_cooperators / n_players_excluding_self
        cooperation_threshold = self.k_factor / self.n_players
        if CRO_t_minus_1 >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_53(BaseStrategy):
    """
    Adaptive Proportional Reciprocity (APR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by rewarding it and to deter defection by
    imposing proportionate punishment. It starts by cooperating and adapts its behavior
    based on the observed level of cooperation in the previous round, using a dynamic
    threshold for cooperation that is sensitive to the game's multiplication factor 'k'.
    It defects in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_54(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC) strategy for the N-Player Public Goods Game.

    This strategy combines an initial cooperative stance with a dynamic response to the collective
    level of cooperation observed in previous rounds. It also incorporates the game-theoretic
    understanding of finite repeated games for the final round.

    Decision Rules:
    1. First Round: Always Cooperate (C) to signal intent and bootstrap cooperation.
    2. Intermediate Rounds:
       - Calculates a cooperation threshold `T_coop_num = ceil(n_players / k)`.
       - If the total number of cooperators in the previous round (`C_prev`) meets or exceeds
         this threshold, the strategy Cooperates (C).
       - Otherwise (`C_prev < T_coop_num`), it Defects (D) to signal disapproval and punish
         insufficient collective effort.
    3. Last Round: Always Defect (D) due to the unraveling argument in finitely repeated games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description: An object containing game parameters such as n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs, or None for the first round.

        Returns:
            An Action (C for Cooperate, D for Defect).
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        k_factor = self.game_description.k
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        cooperation_threshold_num = math.ceil(n_players / k_factor)
        if num_cooperators_prev_round >= cooperation_threshold_num:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_55(BaseStrategy):
    """
    The "Adaptive Collective Reciprocity" (ACR) strategy aims to promote cooperation
    in the N-Player Public Goods Game. It starts with a cooperative stance, rewards
    collective cooperation (a majority of players cooperating), and punishes widespread
    defection by defecting. It also protects itself from exploitation by defecting
    in the final round when no future interactions can be influenced.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            previous_round_0_indexed = current_round_0_indexed - 1
            num_cooperators_prev = (1 if history.my_actions[previous_round_0_indexed] else 0) + np.sum(history.opponent_actions[previous_round_0_indexed, :])
            cooperation_threshold = math.ceil(self.n_players / 2)
            if num_cooperators_prev >= cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_56(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) for the N-Player Public Goods Game.

    This strategy aims to maximize collective welfare by encouraging cooperation,
    while being robust against exploitation and adaptable to diverse opponent behaviors.
    It starts by signaling a willingness to cooperate and then dynamically adjusts
    its behavior based on the observed overall cooperation level in the previous round,
    using a specific threshold derived from the game parameters.

    Decision Rules:
    1. Round 1 (Initiation Phase): Cooperate (C) to signal willingness to contribute.
    2. Intermediate Rounds (Round 2 to r-1):
       - Observe C_prev: Total cooperators (including self) in the immediately preceding round.
       - Calculate Threshold T = n_players / k.
       - If C_prev >= T: Cooperate (C), as collective effort is efficient.
       - Else (C_prev < T): Defect (D), as cooperation is too low, signaling for change.
    3. Last Round (Round r): Defect (D), due to no future consequences.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = self.n_players / self.k_factor

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.n_rounds - 1:
            return Action.D
        else:
            previous_round_index = state.round_number - 1
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_index, :])
            my_action_prev_round_is_cooperate = 1 if history.my_actions[previous_round_index] else 0
            total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_action_prev_round_is_cooperate
            if total_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_57(BaseStrategy):
    """
    Collective Resilience with Individual Protection (CRIP) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain cooperation by being conditionally cooperative,
    while protecting itself from exploitation, especially in the end game. It balances the
    collective desire for higher payoffs with the individual need for robustness in a
    competitive environment.

    Decision Rules:
    - Round 0 (Initial Probe): Always Cooperate (C). Signals willingness to contribute.
    - Intermediate Rounds (0 < t < r-1):
      - Calculate Dynamic Cooperation Threshold (D_CT) = ceil(n / k).
      - Count cooperators from the previous round (C_prev_count), including own action.
      - If C_prev_count >= D_CT: Cooperate (C). Sustains collective effort if sufficient.
      - If C_prev_count < D_CT: Defect (D). Protects from exploitation and deters free-riding.
    - Last Round (t = r-1): Always Defect (D). End-game protection against backward induction exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_num_cooperators = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_rounds = self.game_description.n_rounds
        if current_round == 0:
            return Action.C
        if current_round == n_rounds - 1:
            return Action.D
        C_prev_count = np.sum(history.opponent_actions[current_round - 1, :])
        if history.my_actions[current_round - 1]:
            C_prev_count += 1
        if C_prev_count >= self.T_num_cooperators:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_58(BaseStrategy):
    """
    The Adaptive Collective Reciprocator (ACR) strategy aims to maximize collective welfare
    by encouraging and maintaining cooperation, while remaining resilient to selfish behavior
    and the structural incentives of a finite repeated game.

    It initiates cooperation in the first round to foster collective action. In subsequent
    rounds (excluding the last), it adaptively cooperates if a sufficient number of other
    players (defined by `T_count_threshold`) cooperated in the previous round; otherwise,
    it defects to prevent exploitation and signal disapproval of low cooperation.
    In the final round, it pragmatically defects, anticipating backward induction from
    other players and ensuring self-preservation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        self.T_count_threshold = max(1, math.floor((n - 1) / 2.0))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        cooperators_other_in_prev_round = sum(history.opponent_actions[-1, :])
        if cooperators_other_in_prev_round >= self.T_count_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_59(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by starting cooperatively, monitoring the
    collective contribution of all players, and adapting its behavior based on a
    dynamically calculated threshold. It cooperates if the number of cooperators
    in the previous round met or exceeded a specific threshold (ceil(n/k)), otherwise
    it defects. It always defects in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.c_threshold = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        num_cooperators_previous_round = sum(history.opponent_actions[-1, :])
        if history.my_actions[-1]:
            num_cooperators_previous_round += 1
        if num_cooperators_previous_round >= self.c_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_60(BaseStrategy):
    """
    Collective Reciprocity with Forgiveness (CRF) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize total group payoff by fostering and sustaining widespread
    cooperation. It starts by cooperating, then adaptively responds to the previous round's
    outcomes. In 2-player games, it acts as Tit-for-Tat. In multi-player games, it tolerates
    a small number of defectors (up to 2) to maintain cooperation but defects if free-riding
    becomes widespread (3 or more defectors). In the final round, it defects due to the
    standard endgame effect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.defection_tolerance_threshold = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == self.n_rounds:
            return Action.D
        if self.n_players == 2:
            other_player_previous_action = history.opponent_actions[-1, 0]
            if other_player_previous_action:
                return Action.C
            else:
                return Action.D
        else:
            previous_round_opponent_cooperators = sum(history.opponent_actions[-1, :])
            previous_round_total_cooperators_count = previous_round_opponent_cooperators
            if history.my_actions[-1]:
                previous_round_total_cooperators_count += 1
            previous_round_defectors_count = self.n_players - previous_round_total_cooperators_count
            if previous_round_defectors_count <= self.defection_tolerance_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_61(BaseStrategy):
    """
    Adaptive Reciprocal Cooperator (ARC) strategy for the N-Player Public Goods Game.

    ARC initiates cooperation in the first round, aiming to build collective trust.
    In intermediate rounds, it adapts its behavior based on the observed level of
    collective cooperation in the previous round. It calculates a dynamic
    Cooperation Threshold (ceil(n/k)) which represents the minimum number of
    cooperators required for the public good to be individually beneficial on average.
    If the number of cooperators in the previous round meets or exceeds this
    threshold, ARC cooperates to sustain collective gain. Otherwise, it defects
    to deter free-riding and signal insufficient collective effort. In the final
    round, ARC defects to protect itself from exploitation in a finitely repeated game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_Tc = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            my_action_prev_round_is_C = history.my_actions[-1]
            total_cooperators_prev_round = opponent_cooperators_prev_round + my_action_prev_round_is_C
            if total_cooperators_prev_round >= self.cooperation_threshold_Tc:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_62(BaseStrategy):
    """
    Implements the 'Adaptive Majority Reciprocator' strategy for the N-Player Public Goods Game.
    This strategy adapts its behavior based on the collective cooperation observed in the previous
    round, aiming to foster cooperation while being resilient to free-riding and accounting for
    the finite nature of the game. It cooperates initially, reciprocates majority cooperation
    in intermediate rounds, and defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.
        Calculates the static cooperation threshold.
        """
        self.game_description = game_description
        self.cooperation_threshold = math.floor(self.game_description.n_players / 2) + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the Adaptive Majority Reciprocator strategy.
        """
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        my_prev_action_cooperated = history.my_actions[-1]
        opponent_prev_cooperators = np.sum(history.opponent_actions[-1, :])
        C_prev_observed = (1 if my_prev_action_cooperated else 0) + opponent_prev_cooperators
        if C_prev_observed >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_63(BaseStrategy):
    """
    Adaptive Public Goods Cooperator (APGC) strategy.
    This strategy aims to initiate and sustain a high level of collective cooperation by being
    initially trusting, adaptively rewarding sufficient cooperation, and punishing widespread
    defection. It acknowledges the "endgame" effect by defecting in the final round to prevent
    exploitation.

    Decision Rules:
    1. First Round (t=1, state.round_number=0): Always Cooperate (C).
       Rationale: Establish trust and signal willingness for collective action.
    2. Intermediate Rounds (1 < t < r, 0 < state.round_number < n_rounds-1): Adaptive.
       - Calculate `cooperation_threshold_count = floor(n / k) + 1`.
       - If total cooperators in the previous round (`C_prev_round`) >= `cooperation_threshold_count`, Play C.
         Rationale: Reward and reinforce sufficient collective contribution.
       - Else, Play D.
         Rationale: Punish insufficient cooperation, signaling that widespread defection leads to suboptimal outcomes.
    3. Last Round (t=r, state.round_number=n_rounds-1): Always Defect (D).
       Rationale: Prevent exploitation in the absence of future interactions (endgame effect).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_count = math.floor(self.game_description.n_players / self.game_description.k) + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds_0_indexed = self.game_description.n_rounds - 1
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds_0_indexed:
            return Action.D
        my_prev_action_is_cooperate = history.my_actions[current_round_0_indexed - 1]
        opponents_prev_round_cooperators = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        C_prev_round = int(my_prev_action_is_cooperate) + opponents_prev_round_cooperators
        if C_prev_round >= self.cooperation_threshold_count:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_64(BaseStrategy):
    """
    The "Adaptive Collective Pacer" strategy is designed to foster and maintain
    cooperation in the N-Player Public Goods Game by adapting its response to
    both the observed collective behavior and the underlying game parameters.
    It aims to maximize the collective payoff over the long run while protecting
    itself from exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.BASE_THRESHOLD = 0.5
        self.THRESHOLD_ADJUSTMENT_RANGE_MAX = 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == r - 1:
            return Action.D
        num_cooperators_previous_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        proportion_cooperators = num_cooperators_previous_round / n
        defection_incentive_normalized = (n - k) / (n - 1)
        adaptive_cooperation_threshold = self.BASE_THRESHOLD + defection_incentive_normalized * self.THRESHOLD_ADJUSTMENT_RANGE_MAX
        if proportion_cooperators >= adaptive_cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_65(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC) strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by initiating with a cooperative move.
    In intermediate rounds, it conditionally cooperates if the number of cooperators
    in the previous round met a game-parameter-adaptive threshold (ceiling(n/k)).
    Otherwise, it defects to protect against exploitation.
    In the final round, it always defects, consistent with rational play in finite games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        if history.my_actions[-1]:
            num_cooperators_prev_round += 1
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_66(BaseStrategy):
    """
    The Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.
    It initiates cooperation in the first round and maintains it as long as the
    collective effort (number of cooperators in the previous round) meets a
    calculated cooperation_threshold. If cooperation falls below this threshold,
    the strategy defects to protect against exploitation and signal the need
    for renewed collective effort. It always defects in the final round due
    to the finite horizon of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = max(1, math.ceil(self.n_players / self.k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        num_cooperators_previous_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_67(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for N-Player Public Goods Game.
    This strategy aims to foster and maintain cooperation by starting cooperatively,
    adapting to the observed level of group cooperation, and strategically defecting
    in the final round.

    In the first round, it always cooperates to signal willingness.
    In intermediate rounds, it observes the total number of cooperators in the previous
    round ($C_{prev}$). If $C_{prev}$ meets or exceeds a fixed cooperation threshold
    ($T_{coop} = \\lceil n/2 \rceil$), it cooperates. Otherwise, it defects to protect
    itself and signal insufficient collective effort.
    In the final round, it always defects, as there are no future interactions to influence.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.T_coop = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        C_prev_opponents = sum(history.opponent_actions[current_round_0_indexed - 1, :])
        C_prev_me = int(history.my_actions[current_round_0_indexed - 1])
        C_prev = C_prev_opponents + C_prev_me
        if C_prev >= self.T_coop:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_68(BaseStrategy):
    """
    Collective Adaptive Reciprocity (CAR) strategy for the N-Player Public Goods Game.
    This strategy aims to promote and sustain cooperation by adopting an initial trust,
    conditioning future actions on the observed collective cooperation level from the
    previous round, punishing widespread free-riding, and defecting in the final
    round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_game_round = state.round_number
        if current_game_round == 0:
            return Action.C
        elif current_game_round == self.n_rounds - 1:
            return Action.D
        else:
            num_cooperators_prev_round = int(history.my_actions[-1]) + int(np.sum(history.opponent_actions[-1, :]))
            if num_cooperators_prev_round == self.n_players:
                return Action.C
            elif num_cooperators_prev_round == 0:
                return Action.D
            else:
                cooperation_proportion = num_cooperators_prev_round / self.n_players
                if cooperation_proportion >= 0.5:
                    return Action.C
                else:
                    return Action.D

class Strategy_COLLECTIVE_69(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR is designed for a tournament environment without explicit communication. It aims to
    navigate the dilemma between individual and collective rationality by:
    1.  Initiating cooperation in the first round to signal a willingness to build a
        cooperative environment.
    2.  Dynamically adjusting its behavior in intermediate rounds based on the observed
        level of collective cooperation from the previous round. This is controlled by a
        threshold `T` that adapts to the game's multiplication factor `k`, making the
        strategy more forgiving when collective gains are high and stricter when low.
    3.  Accounting for the "endgame effect" by defecting in the final round to prevent
        exploitation, as there are no future interactions to incentivize cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        previous_round_index = current_round_0_indexed - 1
        num_cooperators_opponents = np.sum(history.opponent_actions[previous_round_index])
        my_previous_action_cooperated = int(history.my_actions[previous_round_index])
        C_prev = num_cooperators_opponents + my_previous_action_cooperated
        T = math.floor(self.n_players - (self.k_factor - 1))
        if C_prev >= T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_70(BaseStrategy):
    """
    Adaptive Reciprocity Threshold (ART) strategy for the N-Player Public Goods Game.

    Core Philosophy:
    The strategy promotes cooperation by starting with a gesture of trust and then
    adapting its behavior based on the observed level of collective cooperation in
    previous rounds. It aims to maintain cooperation when sufficiently reciprocated
    and to retaliate against significant free-riding by withholding contributions.
    The "reciprocity threshold" is dynamically determined by the game's inherent
    parameters, specifically the efficiency of the public good (k). This allows
    the strategy to be more forgiving when cooperation is highly beneficial (high k)
    and stricter when cooperation is less efficient (low k), thereby balancing
    collective well-being with individual protection against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.floor(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        my_action_prev_round = history.my_actions[state.round_number - 1]
        opponent_actions_prev_round = history.opponent_actions[state.round_number - 1, :]
        total_cooperators_prev_round = my_action_prev_round + np.sum(opponent_actions_prev_round)
        if total_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_71(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and sustain collective cooperation by adaptively
    rewarding observed cooperation and punishing observed defection.

    Core Principles:
    1. Initiate Cooperation: Starts by cooperating in Round 1.
    2. Adaptive Reciprocity: In intermediate rounds, reciprocates the observed level
       of group cooperation from the previous round, using a dynamically calculated threshold.
    3. Endgame Rationality: Defects in the final round to prevent exploitation.
    4. Collective Sensitivity: The adaptive threshold is designed to be more forgiving
       when the collective benefit (k) is high and stricter when it's low.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        calculated_threshold = math.floor(self.n_players * (1 - self.k / self.n_players))
        self.cooperation_threshold = max(1, calculated_threshold)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1]
        num_opponent_cooperators_previous_round = sum(previous_round_opponent_actions)
        total_cooperators_previous_round = num_opponent_cooperators_previous_round
        if history.my_actions[current_round_0_indexed - 1]:
            total_cooperators_previous_round += 1
        if total_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_72(BaseStrategy):
    """
    Adaptive Forgiving N-TfT (N-Player Tit-for-Tat) strategy.

    This strategy initiates cooperation, then adaptively responds to the collective
    cooperation level of the group. It is designed to be forgiving enough to recover
    from minor defections but firm enough to punish significant collective free-riding.
    It aims to maximize the collective good by encouraging widespread cooperation,
    while protecting itself from exploitation, especially in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold_count = max(1, self.n_players - math.floor(self.k_factor - 1))
        self.is_punishing = False
        self.rounds_to_punish = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        if self.is_punishing:
            self.rounds_to_punish -= 1
            if self.rounds_to_punish == 0:
                self.is_punishing = False
                return Action.C
            else:
                return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators_prev_round >= self.cooperation_threshold_count:
            return Action.C
        else:
            self.is_punishing = True
            self.rounds_to_punish = 1
            return Action.D

class Strategy_COLLECTIVE_73(BaseStrategy):
    """
    The Adaptive Collective Contributor (ACC) strategy aims to foster cooperation by starting
    cooperatively and then conditionally cooperating based on a collective efficiency threshold.
    It defects if the previous round's total cooperation falls below `ceil(n/k)` players,
    signaling a need for greater contribution. In the final round, it pragmatically defects
    to avoid exploitation due to end-game opportunism.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.t_coop = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == total_rounds - 1:
            return Action.D
        else:
            opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
            my_action_prev_int = int(history.my_actions[-1])
            n_c_prev = opponent_cooperators_prev + my_action_prev_int
            if n_c_prev >= self.t_coop:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_74(BaseStrategy):
    """
    The Adaptive Collective Stewardship strategy for the N-Player Public Goods Game.

    This strategy aims to maximize collective payoff by promoting and sustaining cooperation,
    adapting to the observed behavior of other players.

    Core Principles:
    - Initiate Cooperation: Starts by cooperating to establish a baseline.
    - Conditional Reciprocity: Continues cooperating only if a sufficient number of players
      are also contributing, determined by a dynamic threshold `T = ceil(n / k)`.
    - Collective Punishment: Defects when cooperation falls below `T` to signal
      dissatisfaction and encourage a return to higher cooperation levels.
    - Rational Endgame: Defects in the final round due to backward induction, maximizing
      individual payoff when future interactions no longer incentivize cooperation.

    The strategy dynamically adjusts its strictness based on the efficiency of the public good (k).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n_total_rounds = self.game_description.n_rounds
        if current_round_number == 0:
            return Action.C
        elif current_round_number == n_total_rounds - 1:
            return Action.D
        else:
            my_previous_action_cooperated = int(history.my_actions[-1])
            opponent_cooperators_previous_round = np.sum(history.opponent_actions[-1, :])
            total_cooperators_previous_round = my_previous_action_cooperated + opponent_cooperators_previous_round
            if total_cooperators_previous_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_75(BaseStrategy):
    """
    Adaptive Collective Reciprocator (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to maximize overall game payoff by fostering a cooperative environment.
    It operates on the principle of conditional cooperation, signaling a willingness to contribute
    but also defending against exploitation. It initiates cooperation, maintains it if a
    sufficient proportion of others cooperate, punishes defection briefly, and defects
    in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.total_rounds = game_description.n_rounds
        self.P_threshold: float = 0.5
        self.D_punish: int = 1
        self.punishment_rounds_remaining: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.total_rounds - 1:
            return Action.D
        if current_round == 0:
            self.punishment_rounds_remaining = 0
            return Action.C
        if self.punishment_rounds_remaining > 0:
            self.punishment_rounds_remaining -= 1
            return Action.D
        cooperator_count = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        proportion_cooperators = cooperator_count / self.n_players
        if proportion_cooperators >= self.P_threshold:
            return Action.C
        else:
            self.punishment_rounds_remaining = self.D_punish
            return Action.D

class Strategy_COLLECTIVE_76(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy initiates cooperation in the first round. In intermediate rounds,
    it acts as a conditional cooperator, adapting its behavior based on the observed
    proportion of cooperation from other players in the previous round. The tolerance
    for defection is dynamically adjusted by a Cooperation Threshold Proportion (CTP)
    which is inversely related to the game's collective benefit (k). In the final round,
    the strategy defects to avoid exploitation due to the endgame effect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        self.collective_benefit_strength = (k - 1) / (n - 1)
        self.cooperation_threshold_proportion = 1 - self.collective_benefit_strength

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds_0_indexed = self.game_description.n_rounds - 1
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds_0_indexed:
            return Action.D
        num_cooperators_others_prev_round = np.sum(history.opponent_actions[-1, :])
        num_other_players = self.game_description.n_players - 1
        if num_other_players == 0:
            return Action.D
        proportion_cooperators_others = num_cooperators_others_prev_round / num_other_players
        if proportion_cooperators_others >= self.cooperation_threshold_proportion:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_77(BaseStrategy):
    """
    Adaptive Collective Reciprocation (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize collective payoffs by initiating cooperation and
    maintaining it as long as a sufficient number of other players contribute. It
    dynamically adjusts its tolerance for defection based on the game's
    multiplication factor 'k'. It also accounts for the unique dynamics of the
    first and last rounds according to game theory principles.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ACR strategy with game parameters.

        Args:
            game_description (PublicGoodsDescription): An object containing
                                                       n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold_T = self.n_players - math.ceil(self.k_factor - 1.0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the player's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): The current state of the game, including the round number
                               (0-indexed).
            history (None | PlayerHistory): Past actions and payoffs for all players,
                                            or None for the first round (round 0).

        Returns:
            Action: The chosen action for the current round (Action.C or Action.D).
        """
        current_round_number_0_indexed = state.round_number
        if current_round_number_0_indexed == 0:
            return Action.C
        if current_round_number_0_indexed == self.n_rounds - 1:
            return Action.D
        num_cooperators_in_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_in_prev_round >= self.cooperation_threshold_T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_78(BaseStrategy):
    """
    Adaptive Forgiving Reciprocity (AFR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster high levels of cooperation for collective benefit
    while remaining adaptive, robust against exploitative behaviors, and mindful
    of game-theoretic principles for repeated interactions. It incorporates:
    1. Initial Trust: Starts with cooperation.
    2. Adaptive Reciprocity: Bases decisions on collective cooperation from the
       previous round, adjusted by the game's multiplication factor `k`.
    3. Self-Protection: Defects if cooperation falls below an adaptive threshold.
    4. Forgiveness: Periodically attempts to cooperate after a defection streak
       to re-establish group cooperation.
    5. Endgame Rationality: Defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_defection_streak = 0
        self.forgiveness_interval = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        current_round = state.round_number + 1
        if current_round == 1:
            self.my_defection_streak = 0
            return Action.C
        if current_round == r:
            self.my_defection_streak = 0
            return Action.D
        threshold_fraction = (n - k) / (n - 1)
        threshold_count = math.ceil(n * threshold_fraction)
        threshold_count = max(1, min(n, threshold_count))
        my_previous_action_bool = history.my_actions[-1]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        cooperators_prev_round = opponent_cooperators_prev_round + (1 if my_previous_action_bool else 0)
        if self.my_defection_streak >= self.forgiveness_interval:
            self.my_defection_streak = 0
            return Action.C
        if threshold_count == 1 and cooperators_prev_round == 1 and my_previous_action_bool:
            self.my_defection_streak += 1
            return Action.D
        elif cooperators_prev_round >= threshold_count:
            self.my_defection_streak = 0
            return Action.C
        else:
            self.my_defection_streak += 1
            return Action.D

class Strategy_COLLECTIVE_79(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    Core Philosophy:
    The strategy aims to establish and maintain a high level of cooperation within the group
    by being initially trusting, conditionally cooperative, and strategically punitive.
    It seeks to maximize the collective payoff by encouraging widespread cooperation,
    while simultaneously protecting itself from persistent free-riders. It acknowledges
    the "endgame effect" in finite repeated games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n_players = self.game_description.n_players
        k_factor = self.game_description.k
        T_fraction = (n_players - k_factor) / (n_players - 1)
        self.T_coop_abs = max(1, math.ceil(n_players * T_fraction))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_rounds:
            return Action.D
        else:
            previous_round_opponent_actions = history.opponent_actions[state.round_number - 1, :]
            opponent_prev_cooperators = np.sum(previous_round_opponent_actions)
            my_prev_action_was_C = history.my_actions[state.round_number - 1]
            my_prev_cooperation_value = int(my_prev_action_was_C)
            m_prev = opponent_prev_cooperators + my_prev_cooperation_value
            if m_prev >= self.T_coop_abs:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_80(BaseStrategy):
    """
    The Adaptive Majority-Conditional Cooperator (AMCC) strategy is designed to foster
    and sustain cooperation in the N-Player Public Goods Game. It initiates cooperation
    and continues to cooperate as long as a significant portion (at least half) of the
    collective is also cooperating in the preceding round. It defects to signal
    dissatisfaction with insufficient collective effort or to prevent excessive
    exploitation, but with the underlying aim of re-establishing higher cooperation.
    The strategy explicitly accounts for the distinct dynamics of the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n / 2.0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        num_cooperators_in_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_in_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_81(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy promotes cooperation by initiating with C, then adaptively
    reciprocating collective cooperation (or punishing defection) based on the
    previous round's cooperation ratio. In the final round, it cooperates only
    if historical collective cooperation has been consistently high, otherwise
    it defects to protect against exploitation.
    """
    THRESHOLD_ROUND_COOPERATION = 0.5
    THRESHOLD_FINAL_COOPERATION = 0.75

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialises the strategy with game parameters.

        Args:
            game_description: An object containing game parameters like n_players, n_rounds.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the current round number.
            history: A record of actions and payoffs from previous rounds. None for the first round.

        Returns:
            The chosen action (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            total_cooperation_ratios_sum = 0.0
            num_history_rounds_to_consider = self.n_rounds - 1
            for i in range(num_history_rounds_to_consider):
                my_action_in_round_i = history.my_actions[i]
                opponent_actions_in_round_i = history.opponent_actions[i, :]
                num_cooperators_in_round_i = (1 if my_action_in_round_i else 0) + np.sum(opponent_actions_in_round_i)
                cooperation_ratio_in_round_i = num_cooperators_in_round_i / self.n_players
                total_cooperation_ratios_sum += cooperation_ratio_in_round_i
            historical_cooperation_rate = total_cooperation_ratios_sum / num_history_rounds_to_consider
            if historical_cooperation_rate >= self.THRESHOLD_FINAL_COOPERATION:
                return Action.C
            else:
                return Action.D
        else:
            prev_round_0_indexed = state.round_number - 1
            my_action_prev_round = history.my_actions[prev_round_0_indexed]
            opponent_actions_prev_round = history.opponent_actions[prev_round_0_indexed, :]
            num_cooperators_prev_round = (1 if my_action_prev_round else 0) + np.sum(opponent_actions_prev_round)
            cooperation_ratio_prev_round = num_cooperators_prev_round / self.n_players
            if cooperation_ratio_prev_round >= self.THRESHOLD_ROUND_COOPERATION:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_82(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain cooperation by initiating trust,
    then adaptively reciprocating based on a calculated threshold of collective
    cooperation from the previous round. It also includes an endgame adjustment
    to prevent last-minute exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        if state.round_number == 0:
            return Action.C
        if state.round_number == r - 1:
            return Action.D
        tcc = math.ceil(n / k)
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + history.my_actions[-1]
        if total_cooperators_prev_round >= tcc:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_83(BaseStrategy):
    """
    The "AdaptiveCollective" strategy for the N-Player Public Goods Game.

    This strategy initiates cooperation, then conditionally reciprocates based on the
    overall proportion of cooperators in the previous round, adjusting its threshold
    based on the game's multiplication factor 'k' relative to 'n'. It also defects
    in the final round due to backward induction to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialises the strategy with game parameters and calculates the static
        cooperation threshold.

        Args:
            game_description (PublicGoodsDescription): An object containing game parameters
                                                      (n_players, n_rounds, k).
        """
        self.game_description = game_description
        n_players = self.game_description.n_players
        k_factor = self.game_description.k
        self.cooperation_threshold = 1 - (k_factor - 1) / (n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides whether to Cooperate (C) or Defect (D) for the current round
        based on the AdaptiveCollective strategy rules.

        Args:
            state (GameState): Current game state, including the current round number
                               (0-indexed).
            history (None | PlayerHistory): Past game actions and payoffs. This will
                                            be None for the very first round (round 0).

        Returns:
            Action: Action.C for Cooperate, Action.D for Defect.
        """
        current_round_number_0_indexed = state.round_number
        total_rounds_0_indexed = self.game_description.n_rounds - 1
        if current_round_number_0_indexed == 0:
            return Action.C
        elif current_round_number_0_indexed == total_rounds_0_indexed:
            return Action.D
        else:
            prev_round_idx = current_round_number_0_indexed - 1
            my_action_prev_round = history.my_actions[prev_round_idx]
            opponent_actions_prev_round = history.opponent_actions[prev_round_idx, :]
            num_cooperators_prev_round = np.sum(opponent_actions_prev_round) + my_action_prev_round
            proportion_cooperators_prev_round = num_cooperators_prev_round / self.game_description.n_players
            if proportion_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_84(BaseStrategy):
    """
    Adaptive Threshold Reciprocity: Initiates cooperation, sustains it if a critical mass of players
    cooperated in the previous round, and defects in the final rounds to prevent exploitation.

    Goal: To maximize collective payoff by initiating cooperation, sustaining it when a critical mass
    of players also cooperates, and defensively defecting in the game's final stages to prevent exploitation.

    Decision Rules:
    1. First Round: Cooperate (C) to signal willingness to establish cooperation.
    2. Last Round(s): Defect (D) in the final specified rounds to prevent exploitation due to backward induction.
    3. Intermediate Rounds: If the number of cooperators in the previous round met or exceeded a
       `COOPERATION_THRESHOLD_RATIO` of total players, then Cooperate (C); otherwise, Defect (D).
    """
    COOPERATION_THRESHOLD_RATIO = 0.5
    ENDGAME_DEFAULT_DEFECT_ROUNDS = 1

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.required_cooperators_threshold = math.ceil(self.game_description.n_players * self.COOPERATION_THRESHOLD_RATIO)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds_1_indexed = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed >= total_rounds_1_indexed - self.ENDGAME_DEFAULT_DEFECT_ROUNDS + 1:
            return Action.D
        my_prev_action_was_cooperate = history.my_actions[state.round_number - 1]
        opponent_cooperators_prev_round = sum(history.opponent_actions[state.round_number - 1, :])
        cooperators_in_previous_round = int(my_prev_action_was_cooperate) + opponent_cooperators_prev_round
        if cooperators_in_previous_round >= self.required_cooperators_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_85(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster and sustain collective cooperation by initiating contributions,
    rewarding consistent group cooperation, and punishing insufficient collective
    contributions. It balances the desire for the collective optimum with a rational
    response to free-riding, particularly in the later stages of the game.

    Decision Rules:
    1.  First Round (Round 0, 0-indexed): Always Cooperate to signal willingness and initiate collective action.
    2.  Intermediate Rounds (Round 1 to r-2, 0-indexed): Observes collective cooperation from the immediately
        preceding round. If the total number of cooperators was at least `ceil(n / 2.0)` (the cooperation
        threshold), the strategy continues to Cooperate. Otherwise, it Defects to protect against exploitation
        and to signal for increased cooperation.
    3.  Last Round (Round r-1, 0-indexed): Always Defects, as there are no future interactions to influence,
        making defection the individually rational choice for that final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = math.ceil(self.n_players / 2.0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        elif current_round_number == self.n_rounds - 1:
            return Action.D
        else:
            cooperators_in_previous_round = history.opponent_actions[-1, :].sum() + history.my_actions[-1]
            if cooperators_in_previous_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_86(BaseStrategy):
    """
    Collective Conditional Cooperation (CCC) strategy for the N-Player Public Goods Game.

    This strategy aims to foster a high level of cooperation by initiating with cooperation,
    then adapting its behavior based on the observed collective cooperation rate from the
    immediate previous round. It continues to cooperate if the group's cooperation rate
    meets or exceeds a predefined threshold (1/k). If the cooperation rate falls below
    this threshold, the strategy defects to protect against exploitation. In the final
    round, it always defects, following the logic of backward induction in finite games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Collective Conditional Cooperation strategy.

        Args:
            game_description (PublicGoodsDescription): An object containing the game's parameters
                                                      (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.alpha_threshold = 1.0 / self.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): An object containing the current round number.
            history (None | PlayerHistory): An object containing all historical actions
                                            and payoffs, or None if it's the first round.

        Returns:
            Action: The chosen action for the current round (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        my_action_prev_round_is_C = history.my_actions[current_round_0_indexed - 1]
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + int(my_action_prev_round_is_C)
        cooperation_rate_prev_round = total_cooperators_prev_round / self.n_players
        if cooperation_rate_prev_round >= self.alpha_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_87(BaseStrategy):
    """
    The Adaptive Collective Forgiver strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation by initiating cooperation,
    forgiving minor deviations, but collectively punishing widespread free-riding to
    re-incentivize cooperation. It accounts for the game's finite horizon by adjusting
    behavior in the final round.

    Decision Rules:
    1. Round 1 (0-indexed round 0): Cooperate (C) to initiate a cooperative environment.
    2. Intermediate Rounds (0-indexed round 1 to r-2):
       - Calculate Collective Cooperation Threshold (T_C = CEILING(n / 2.0)).
       - Observe total cooperators in the immediately preceding round.
       - If total cooperators >= T_C: Cooperate (C) (forgive minor free-riding).
       - If total cooperators < T_C: Defect (D) (collectively punish widespread defection).
    3. Last Round (0-indexed round r-1): Defect (D) due to backward induction (rational self-interest).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.T_C = math.ceil(self.n_players / 2.0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev_round >= self.T_C:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_88(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain cooperation by initially cooperating
    and then conditionally cooperating based on a dynamic threshold of collective
    contribution from the previous round. It protects against exploitation by
    defecting if cooperation falls below this threshold and always defects in the
    final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters like n_players,
                              n_rounds, and k.
        """
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs for the player
                     and opponents, or None for the first round.

        Returns:
            Action.C for Cooperate, Action.D for Defect.
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.r - 1:
            return Action.D
        else:
            total_cooperators_previous_round = sum(history.opponent_actions[-1]) + history.my_actions[-1]
            max_tolerated_defectors = math.floor(self.k - 1)
            T = self.n - max_tolerated_defectors
            if total_cooperators_previous_round >= T:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_89(BaseStrategy):
    """
    Adaptive Proportional Cooperation (APC) strategy for the N-Player Public Goods Game.

    The strategy begins by cooperating unconditionally in the first round.
    In subsequent intermediate rounds, it adapts its behavior based on the proportion
    of players who cooperated in the previous round. It continues to cooperate if
    the observed proportion of cooperators meets or exceeds a dynamic threshold (1/k).
    In the final round, it defects to secure individual payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 1.0 / self.game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        cooperators_in_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        proportion_cooperators = float(cooperators_in_prev_round) / n_players
        if proportion_cooperators >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_90(BaseStrategy):
    """
    Adaptive Reciprocal Threshold (ART) strategy for the N-Player Public Goods Game.

    This strategy aims to foster collective cooperation while being robust against
    exploitation, adapting its behavior based on the observed actions of other players
    in previous rounds. It starts by extending trust and cooperating. In subsequent
    rounds, it dynamically adjusts its cooperation based on a threshold derived
    from the game's parameters (k and n), which determines the minimum proportion
    of cooperators required from the group in the previous round for the strategy
    to continue cooperating. It defects if collective cooperation falls below this
    threshold and resumes cooperation if it rises above. The strategy adheres to
    backward induction for the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.T_coop = 1.0 - self.k_factor / self.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        else:
            m_prev = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            p_prev = m_prev / self.n_players
            if p_prev >= self.T_coop:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_91(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to promote and sustain cooperation by initiating cooperation,
    rewarding sustained collective effort, and punishing significant free-riding.
    It adapts its actions based on the observed level of cooperation in previous
    rounds and the inherent game parameters (n, k).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_fraction = (self.k - 1) / (self.n - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.r - 1:
            return Action.D
        else:
            my_prev_action_cooperated = int(history.my_actions[-1])
            opponent_prev_cooperations = np.sum(history.opponent_actions[-1, :])
            C_prev = my_prev_action_cooperated + opponent_prev_cooperations
            if C_prev / self.n >= self.cooperation_threshold_fraction:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_92(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to maximize collective welfare by fostering cooperation.
    It initiates cooperation to signal willingness, then dynamically adjusts its behavior
    based on the observed level of collective contribution in previous rounds.
    It seeks to sustain cooperation when the group demonstrates sufficient effort,
    but reverts to defection to protect against exploitation and to signal the need
    for greater collective action when contributions fall short. The last round is
    handled with individual rationality, acknowledging the finite nature of the game.

    The strategy defines a dynamic threshold for cooperation, `THRESHOLD_N_C = ceil(n / k)`,
    which adapts to the game's core parameters `n` (number of players) and `k`
    (multiplication factor).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.THRESHOLD_N_C = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.r:
            return Action.D
        else:
            previous_round_history_idx = state.round_number - 1
            opponent_cooperators_prev_round = sum(history.opponent_actions[previous_round_history_idx, :])
            my_action_prev_round = history.my_actions[previous_round_history_idx]
            N_C_prev = opponent_cooperators_prev_round + my_action_prev_round
            if N_C_prev >= self.THRESHOLD_N_C:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_93(BaseStrategy):
    """
    The Adaptive Collective Reciprocator (ACR) strategy for the N-Player Public Goods Game.
    It prioritizes collective good by initially cooperating and aiming to sustain a high level of
    cooperation. It protects itself from exploitation by punishing free-riders and defectors,
    and offers a path back to cooperation (forgiveness) if players demonstrate renewed commitment.
    The strategy explicitly handles the first and last rounds according to game theory principles,
    and adapts its thresholds based on game parameters (n, k).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishing_mode: bool = False
        n = self.game_description.n_players
        k = self.game_description.k
        self.cooperation_threshold = max(math.floor(n / 2) + 1, math.ceil(n / k))
        self.forgiveness_threshold = min(n, max(self.cooperation_threshold + 1, math.floor(n * 0.75)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        total_rounds_count = self.game_description.n_rounds
        if current_round_zero_indexed == 0:
            return Action.C
        if current_round_zero_indexed == total_rounds_count - 1:
            return Action.D
        n_C_prev_round = sum(history.opponent_actions[current_round_zero_indexed - 1, :])
        if not self.punishing_mode:
            if n_C_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                self.punishing_mode = True
                return Action.D
        elif n_C_prev_round >= self.forgiveness_threshold:
            self.punishing_mode = False
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_94(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and sustain collective cooperation by initiating with
    cooperation, then reciprocating observed collective cooperation (above a calculated
    threshold), and deterring free-riding by defecting when cooperation is insufficient.
    It defects in the final round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_sustain_threshold = math.ceil(self.n / self.k)
        if self.cooperation_sustain_threshold < 1:
            self.cooperation_sustain_threshold = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        my_prev_action_is_C = history.my_actions[state.round_number - 1]
        opponent_prev_actions_are_C = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_prev_round = sum(opponent_prev_actions_are_C) + my_prev_action_is_C
        if num_cooperators_prev_round >= self.cooperation_sustain_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_95(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy aims to foster and sustain widespread cooperation
    in the N-Player Public Goods Game by adapting to the observed collective behavior of all players.
    It combines initial generosity, conditional reciprocity based on group performance, and rational end-game play,
    designed to be robust against a variety of opponent strategies.

    Core Principle: The strategy's default mode is to cooperate, but it will defect to punish insufficient
    collective cooperation. It signals a willingness to initiate cooperation and maintain it, provided a
    significant portion of the group reciprocates this collective effort.
    """
    COOPERATION_THRESHOLD = 0.75

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        prev_round_idx = current_round_0_indexed - 1
        my_action_prev_round_is_C = history.my_actions[prev_round_idx]
        opponent_actions_prev_round_are_C = history.opponent_actions[prev_round_idx, :]
        num_cooperators_prev_round = int(my_action_prev_round_is_C) + np.sum(opponent_actions_prev_round_are_C)
        cooperation_rate_prev_round = num_cooperators_prev_round / self.n_players
        if cooperation_rate_prev_round >= self.COOPERATION_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_96(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to establish and maintain cooperation by signaling a willingness to contribute,
    then adaptively responding to the observed level of collective cooperation from the group.
    It leverages game parameters to determine a fair threshold for collective effort, above which
    cooperation is sustained, and below which defection is employed to deter free-riding.

    The strategy begins by cooperating in the first round to initiate a positive interaction.
    For intermediate rounds, it calculates a Cooperation Threshold (T) based on `n` and `k`.
    If the total number of cooperators in the previous round meets or exceeds T, the strategy
    continues to cooperate. Otherwise, it defects to signal insufficient collective effort and
    prevent exploitation. In the final round, it defects to avoid backward induction exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.r - 1:
            return Action.D
        else:
            prev_round_index = state.round_number - 1
            my_prev_action = history.my_actions[prev_round_index]
            opponents_prev_actions = history.opponent_actions[prev_round_index, :]
            num_cooperators_prev_round = int(my_prev_action) + np.sum(opponents_prev_actions)
            if num_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_97(BaseStrategy):
    """
    Adaptive Proportional Reciprocity with End-Game Adjustment strategy for the N-Player Public Goods Game.
    This strategy initiates cooperation, maintains it if a sufficient number of players cooperated in the
    previous round (based on a dynamic threshold derived from game parameters), and defects in the final round.
    It balances fostering cooperation with protecting against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_players = max(1, math.floor(self.game_description.n_players * (1 - self.game_description.k / self.game_description.n_players)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_rounds:
            return Action.D
        else:
            previous_round_index = state.round_number - 1
            n_cooperated_previous_round = int(history.my_actions[previous_round_index]) + sum(history.opponent_actions[previous_round_index, :])
            if n_cooperated_previous_round >= self.cooperation_threshold_players:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_98(BaseStrategy):
    """
    Adaptive Collective Cooperation with Endgame Defection (ACCE) strategy
    for the N-Player Public Goods Game.

    This strategy initiates cooperation in the first round. In intermediate rounds,
    it adapts its behavior based on the observed level of collective cooperation
    in the immediately preceding round, continuing to cooperate if a calculated
    threshold of cooperators was met, and defecting otherwise. In the final round,
    it defects based on principles of backward induction to maximize individual payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.C_threshold = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_cooperators_prev_round = int(history.my_actions[-1]) + sum(history.opponent_actions[-1, :])
        if num_cooperators_prev_round >= self.C_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_99(BaseStrategy):
    """
    Collective Enforcement with Adaptive Tolerance (CEAT) strategy for the N-Player Public Goods Game.

    This strategy aims to promote and sustain cooperation by starting cooperatively and
    then reacting to the observed behavior of other players. It balances the need to
    punish free-riding with an adaptive tolerance for defection, adjusting its
    strictness based on the game's inherent collective benefit (k).

    Key features:
    - Cooperates in the first round to signal intent and establish a cooperative baseline.
    - Defects in the last round due to end-game rationality, as there are no future interactions
      to incentivize cooperation or punish defection.
    - In intermediate rounds, it observes the cooperation level of other players
      in the immediately preceding round.
    - It calculates an adaptive cooperation threshold (T_others_C) for other players,
      based on the game parameters 'n' (number of players) and 'k' (multiplication factor).
      This threshold dictates how many other players must have cooperated for this strategy
      to continue cooperating.
    - If the observed number of cooperators among others meets or exceeds T_others_C,
      the strategy cooperates, reinforcing collective action.
    - Otherwise (if cooperation among others falls below T_others_C), the strategy defects,
      aiming to punish free-riding and potentially reset expectations for future rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        n_val = self.n_players
        k_val = self.k
        tolerance_term = math.floor((n_val - k_val) / 2)
        self.cooperation_threshold_T_others_C = max(0, n_val - 1 - tolerance_term)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_number_1_indexed = state.round_number + 1
        if round_number_1_indexed == 1:
            return Action.C
        elif round_number_1_indexed == self.n_rounds:
            return Action.D
        else:
            num_cooperators_among_others = sum(history.opponent_actions[-1, :])
            if num_cooperators_among_others >= self.cooperation_threshold_T_others_C:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_100(BaseStrategy):
    """
    The Adaptive Public Goods Strategist (APGS) aims to maximize individual payoff
    by fostering cooperation when possible, punishing free-riding, and adapting
    to game parameters and observed player behavior.

    It initiates cooperation in Round 1. In intermediate rounds (2 to r-1),
    it dynamically decides to cooperate or defect based on a calculated threshold
    (T_coop = ceil(n/k)) against the total number of cooperators in the previous round.
    If collective cooperation meets or exceeds T_coop, the strategy cooperates; otherwise,
    it defects. In the final round (round r), it always defects due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T_coop = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.r:
            return Action.D
        else:
            m_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if m_prev >= self.T_coop:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_101(BaseStrategy):
    """
    The "Adaptive Collective Enforcement" (ACE) strategy is designed to promote and sustain cooperation
    in the N-Player Public Goods Game by being initially cooperative, then dynamically adapting its
    behavior based on the observed level of collective cooperation, and ultimately defecting when
    future incentives cease to exist. It explicitly targets the collective optimum while being
    robust against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.T_coop = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            cooperators_from_opponents = history.opponent_actions[-1, :].sum()
            my_previous_action_was_C = history.my_actions[-1]
            C_prev = cooperators_from_opponents + (1 if my_previous_action_was_C else 0)
            if C_prev >= self.T_coop:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_102(BaseStrategy):
    """
    The Adaptive Community Contributor strategy prioritizes establishing and maintaining a high level of collective cooperation.
    It starts with an optimistic cooperative stance, monitors the group's adherence to cooperation, and adapts its behavior
    to either reward sustained collective effort or defend against significant free-riding. It acknowledges the unique
    dynamics of the final round to ensure robustness.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        if current_round_zero_indexed == 0:
            return Action.C
        elif current_round_zero_indexed == self.n_rounds - 1:
            return Action.D
        else:
            my_prev_action_cooperated = history.my_actions[-1]
            opponent_prev_cooperators_count = np.sum(history.opponent_actions[-1, :])
            num_cooperators_prev_round = int(my_prev_action_cooperated) + opponent_prev_cooperators_count
            if num_cooperators_prev_round >= self.n_players - 1:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_103(BaseStrategy):
    """
    The Adaptive Collective Threshold (ACT) strategy for the N-Player Public Goods Game.
    It cooperates if the average cooperation rate in recent rounds is above a dynamically
    adjusted threshold. The threshold itself is updated based on how well the strategy's
    previous action (cooperation or defection) fared compared to the hypothetical alternative.
    It opens with cooperation, defects in the final round, and balances self-protection
    with a bias towards fostering collective cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.history_cooperation_counts: list[int] = []
        self.history_my_actions: list[int] = []
        self.history_my_payoffs: list[float] = []
        self.current_cooperation_threshold: float = 0.6
        self.threshold_adjustment_rate: float = 0.05
        self.history_window_size: int = 3
        self.epsilon: float = 0.01

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == r - 1:
            return Action.D
        my_action_prev_round_bool = history.my_actions[-1]
        opponent_actions_prev_round_bool = history.opponent_actions[-1, :]
        total_cooperators_in_prev_round = int(my_action_prev_round_bool) + np.sum(opponent_actions_prev_round_bool)
        my_action_in_prev_round_enum = Action.C if my_action_prev_round_bool else Action.D
        my_payoff_in_prev_round = history.my_payoffs[-1]
        self.history_cooperation_counts.append(total_cooperators_in_prev_round)
        self.history_my_actions.append(1 if my_action_in_prev_round_enum == Action.C else 0)
        self.history_my_payoffs.append(my_payoff_in_prev_round)
        self._adjust_cooperation_threshold(total_cooperators_in_prev_round, my_action_in_prev_round_enum, my_payoff_in_prev_round)
        start_index = max(0, len(self.history_cooperation_counts) - self.history_window_size)
        recent_cooperation_counts_slice = self.history_cooperation_counts[start_index:]
        if not recent_cooperation_counts_slice:
            avg_recent_cooperation_rate = 0.0
        else:
            avg_recent_cooperation_rate = sum(recent_cooperation_counts_slice) / (len(recent_cooperation_counts_slice) * n)
        if avg_recent_cooperation_rate >= self.current_cooperation_threshold:
            return Action.C
        else:
            return Action.D

    def _adjust_cooperation_threshold(self, C_prev: int, my_last_action: Action, my_last_payoff: float) -> None:
        """
        Adjusts the current_cooperation_threshold based on the outcome of the previous round.
        C_prev: Total number of cooperators observed in the previous round (including this player's action).
        my_last_action: This player's action in the previous round (Action.C or Action.D).
        my_last_payoff: This player's payoff in the previous round.
        """
        n = self.game_description.n_players
        k = self.game_description.k
        hypo_C_count_if_I_C = C_prev + (1 if my_last_action == Action.D else 0)
        hypo_payoff_if_C = k / n * hypo_C_count_if_I_C
        hypo_C_count_if_I_D = C_prev - (1 if my_last_action == Action.C else 0)
        hypo_payoff_if_D = 1 + k / n * hypo_C_count_if_I_D
        if my_last_action == Action.C:
            if my_last_payoff < hypo_payoff_if_D - self.epsilon or my_last_payoff < 1.0 - self.epsilon:
                self.current_cooperation_threshold = min(1.0, self.current_cooperation_threshold + self.threshold_adjustment_rate)
            else:
                self.current_cooperation_threshold = max(0.0, self.current_cooperation_threshold - self.threshold_adjustment_rate / 2)
        elif my_last_payoff < hypo_payoff_if_C - self.epsilon:
            self.current_cooperation_threshold = max(0.0, self.current_cooperation_threshold - self.threshold_adjustment_rate)
        else:
            self.current_cooperation_threshold = min(1.0, self.current_cooperation_threshold + self.threshold_adjustment_rate / 2)

class Strategy_COLLECTIVE_104(BaseStrategy):
    """
    Adaptive Reciprocal Cooperation (ARC) strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by starting with trust, reciprocating
    sufficient collective cooperation, punishing free-riding by temporarily defecting
    when cooperation levels are low, and defecting in the final round due to
    endgame rationality.

    Internal Parameter:
    - cooperation_threshold_fraction: The minimum fraction of players that must have
      cooperated in the previous round for this strategy to cooperate in the current round.
      (Fixed at 0.6)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_fraction = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_rounds:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            my_action_prev_round_is_C = int(history.my_actions[-1])
            num_cooperators_prev_round = num_opponent_cooperators_prev_round + my_action_prev_round_is_C
            cooperation_rate_prev = num_cooperators_prev_round / n_players
            if cooperation_rate_prev >= self.cooperation_threshold_fraction:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_105(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) for N-Player Public Goods Game.

    This strategy aims to achieve and sustain collective cooperation by adapting to
    the observed behavior of other players, while remaining robust to defection
    and acknowledging the finite horizon of the game.

    It initiates cooperation in the first round. In intermediate rounds, it calculates
    the average cooperation rate (ACR_history) from all previous rounds across all
    players (including itself). If this rate meets or exceeds a dynamically calculated
    threshold (T_dynamic), the strategy cooperates; otherwise, it defects. The dynamic
    threshold adapts to the game's efficiency (k/n ratio). In the final round, it defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        k = game_description.k
        self.T_dynamic = 1.0 - k / n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_rounds:
            return Action.D
        else:
            my_cooperators_count = np.sum(history.my_actions)
            opponent_cooperators_count = np.sum(history.opponent_actions)
            total_cooperators_observed = my_cooperators_count + opponent_cooperators_count
            num_previous_rounds_completed = current_round_1_indexed - 1
            possible_cooperators_so_far = num_previous_rounds_completed * n_players
            ACR_history = total_cooperators_observed / possible_cooperators_so_far
            if ACR_history >= self.T_dynamic:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_106(BaseStrategy):
    """
    Adaptive Majority Reciprocator strategy for the N-Player Public Goods Game.

    This strategy aims to achieve high collective payoffs by cooperating conditionally.
    It starts by cooperating to establish a norm of cooperation. It then adapts its behavior
    based on whether a majority of players (including itself) cooperated in the previous round.
    If a sufficient number of players cooperate, it continues to cooperate; otherwise, it defects
    to punish free-riding and encourage a return to cooperation. It accounts for the unique
    dynamics of the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            previous_round_0_indexed = state.round_number - 1
            cooperators_among_opponents_prev = sum(history.opponent_actions[previous_round_0_indexed, :])
            my_action_prev = history.my_actions[previous_round_0_indexed]
            total_cooperators_prev_round = cooperators_among_opponents_prev + my_action_prev
            cooperation_threshold = self.n_players / 2.0
            if total_cooperators_prev_round >= cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_107(BaseStrategy):
    """
    Community Cultivator strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation by initiating
    cooperation, conditionally reciprocating, punishing widespread defection,
    and adapting to the game's finite horizon by defecting in the final round.

    Key Principles:
    1.  Initiate Cooperation: Start by cooperating.
    2.  Conditional Reciprocity: Continue cooperating if sufficient collective cooperation
        (at least ceil(n/2) players) was observed in the previous round.
    3.  Collective Punishment: Defect if cooperation drops below the threshold.
    4.  Terminal Defection: Defect in the final round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Community Cultivator strategy.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the round number (0-indexed).
            history: A PlayerHistory object containing past actions and payoffs,
                     or None if it's the very first round.

        Returns:
            An Action enum (C or D) representing the chosen action.
        """
        current_round_0_indexed = state.round_number
        total_rounds = self.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == total_rounds - 1:
            return Action.D
        else:
            previous_round_cooperators = np.sum(history.opponent_actions[-1, :])
            if previous_round_cooperators >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_108(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain high levels of cooperation. It starts cooperatively,
    then monitors the group's behavior, adapting its own actions based on a dynamic threshold
    of cooperators in the previous round. It is forgiving for minor slips but punishes sustained
    lack of cooperation, while also protecting itself from end-game exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        num_total_cooperators_prev_round = num_opponent_cooperators_prev_round + history.my_actions[-1]
        n_div_k_floor = math.floor(self.n / self.k)
        cooperation_threshold_number = max(1, self.n - n_div_k_floor)
        if num_total_cooperators_prev_round >= cooperation_threshold_number:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_109(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy promotes and sustains cooperation
    in the N-Player Public Goods Game. It initiates cooperation, sustains it
    through conditional reciprocity based on a dynamic threshold, and defects
    in the last round. The cooperation threshold (T) adapts to game parameters,
    requiring more cooperators when the public good is less rewarding (low k)
    and being more lenient when it's highly beneficial (high k).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        T_raw_value = n * (n - k) / (n - 1)
        T_ceil = math.ceil(T_raw_value)
        self.T = int(max(1, min(n - 1, T_ceil)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            prev_round_idx = state.round_number - 1
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
            my_action_prev_round = history.my_actions[prev_round_idx]
            C_prev = opponent_cooperators_prev_round + (1 if my_action_prev_round else 0)
            if C_prev >= self.T:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_110(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation while protecting
    against exploitation. It adapts its behavior based on the observed history of
    cooperation in the group and the specific game parameters (n, k).

    Decision Rules:
    1. Round 1 (Goodwill): Cooperate to initiate collective effort.
    2. Last Round (Endgame): Defect, following backward induction to avoid exploitation.
    3. Intermediate Rounds:
       - Calculate two thresholds: Cooperation Viability (Vc = ceil(n/k)) and Majority Cooperation (Mc = floor(n/2) + 1).
       - If previous round's total cooperators (C_prev) >= Mc: Cooperate (strong collective effort).
       - Else if C_prev >= Vc: Cooperate (collective effort is viable, "forgiveness zone").
       - Else (C_prev < Vc): Defect (collective effort is failing, protect from exploitation).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_viability_threshold = math.ceil(self.n / self.k)
        self.majority_cooperation_threshold = math.floor(self.n / 2) + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        strategy_round_number = state.round_number + 1
        if strategy_round_number == 1:
            return Action.C
        if strategy_round_number == self.r:
            return Action.D
        my_prev_action = history.my_actions[state.round_number - 1]
        opponent_prev_actions = history.opponent_actions[state.round_number - 1, :]
        c_prev_round = int(my_prev_action) + np.sum(opponent_prev_actions)
        if c_prev_round >= self.majority_cooperation_threshold:
            return Action.C
        elif c_prev_round >= self.cooperation_viability_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_111(BaseStrategy):
    """
    Adaptive Majority Reciprocity (AMR), or Thresholded Tit-for-Tat (TTFT),
    is designed for the N-Player Public Goods Game.

    This strategy aims to foster and sustain cooperation by initially cooperating,
    then conditionally cooperating based on the observed behavior of other players
    in the previous round. It protects against widespread exploitation by defecting
    if less than a majority of others cooperated. A key feature is defection
    in the final round due to backward induction.

    Decision Rules:
    - Round 1 (0-indexed: round_number == 0): Cooperate (C) to signal willingness.
    - Last Round (0-indexed: round_number == n_rounds - 1): Defect (D) due to no future consequences.
    - Intermediate Rounds:
        - Calculate `cooperation_threshold_for_others = CEILING((n - 1) / 2)`.
        - Count `N_C_others_prev`: number of *other* players who cooperated in the previous round.
        - If `N_C_others_prev >= cooperation_threshold_for_others`: Play C.
        - Else: Play D.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold_for_others = math.ceil((self.n_players - 1) / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[-1, :]
        num_others_cooperated = sum(previous_round_opponent_actions)
        if num_others_cooperated >= self.cooperation_threshold_for_others:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_112(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster collective cooperation by starting cooperatively and
    then adjusting its behavior based on a dynamically calculated threshold for
    collective contribution. It cooperates in the first round, defects in the last round,
    and in intermediate rounds, it observes the total number of cooperators from the
    immediately preceding round. If this count meets or exceeds a 'cooperation_threshold_num'
    (calculated as ceil(n/k)), the strategy continues to cooperate; otherwise, it defects.
    This threshold ensures that if met, contributors are receiving a beneficial return
    from the public good component, making cooperation individually justifiable.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_num = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        previous_round_0_indexed = current_round_0_indexed - 1
        my_action_in_prev_round = history.my_actions[previous_round_0_indexed]
        opponent_actions_in_prev_round = history.opponent_actions[previous_round_0_indexed, :]
        num_cooperators_previous_round = my_action_in_prev_round + np.sum(opponent_actions_in_prev_round)
        if num_cooperators_previous_round >= self.cooperation_threshold_num:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_113(BaseStrategy):
    """
    Adaptive Proportional Reciprocation (APR) strategy for the N-Player Public Goods Game.

    This strategy balances initiating cooperation with protecting against exploitation.
    It starts by cooperating to signal a positive intent, then adaptively adjusts its
    behavior in intermediate rounds based on the observed collective cooperation
    level (using a 50% threshold) from the previous round. In the final round, it
    defects to prevent end-game exploitation.
    """
    COOPERATION_THRESHOLD = 0.5

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Proportional Reciprocation strategy.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing records of past actions and payoffs for
                     this player and opponents. It is None for the very first round
                     (when state.round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        my_prev_action_is_C = history.my_actions[current_round_0_indexed - 1]
        opponent_prev_cooperators = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        num_cooperators_previous_round = opponent_prev_cooperators + my_prev_action_is_C
        cooperation_proportion = num_cooperators_previous_round / n_players
        if cooperation_proportion >= self.COOPERATION_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_114(BaseStrategy):
    """
    The Collective Threshold Reciprocator (CTR) strategy for the N-Player Public Goods Game.
    It aims to foster and sustain collective cooperation by initially trusting, then adaptively
    responding to the overall cooperation level of the group. It leverages a parameter-derived
    threshold (T = ceil(n/k)) to determine when cooperation is collectively "worthwhile" and
    incorporates a critical end-game adjustment for robustness.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_T = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_rounds:
            return Action.D
        else:
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
            my_cooperation_prev_round = int(history.my_actions[state.round_number - 1])
            C_prev = opponent_cooperators_prev_round + my_cooperation_prev_round
            if C_prev >= self.cooperation_threshold_T:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_115(BaseStrategy):
    """
    The Adaptive Collective Reciprocator (ACR) strategy is designed for the N-Player
    Public Goods Game. It prioritizes establishing and maintaining collective cooperation
    while adapting to observed behavior.

    Core principles:
    1.  Initiates cooperation in the first round to signal willingness.
    2.  In regular rounds, it observes the number of defectors in the previous round.
        It has a 'DefectionTolerance' (0 for n=2, 1 for n>2), forgiving minor
        defections to encourage return to cooperation. If defectors exceed this
        tolerance, it retaliates by defecting.
    3.  Defects in the final round, acknowledging the backward induction logic of
        finitely repeated games to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Collective Reciprocator strategy.

        Args:
            game_description (PublicGoodsDescription): An object containing game parameters
                                                      like n_players and n_rounds.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): An object containing the current round number (0-indexed).
            history (None | PlayerHistory): An object containing actions and payoffs
                                            from previous rounds. Is None for round 0.

        Returns:
            Action: The chosen action for the current round (Action.C for Cooperate,
                    Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        my_prev_action_was_C = history.my_actions[state.round_number - 1]
        opponent_prev_actions = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_prev_round = int(my_prev_action_was_C) + sum(opponent_prev_actions)
        num_defectors_prev_round = self.n_players - num_cooperators_prev_round
        defection_tolerance = 0
        if self.n_players > 2:
            defection_tolerance = 1
        if num_defectors_prev_round <= defection_tolerance:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_116(BaseStrategy):
    """
    The Adaptive Collective Reciprocator strategy aims to foster cooperation in the N-Player Public Goods Game.
    It initiates cooperation, then conditionally cooperates based on a dynamic benchmark derived from the efficiency
    of the public good (k). It immediately forgives past defections if cooperation levels improve and always defects
    in the final round, recognizing the endpoint of the repeated game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_benchmark = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        my_previous_action_was_C = history.my_actions[-1]
        cooperators_among_opponents_prev_round = np.sum(history.opponent_actions[-1, :])
        c_prev_round_count = my_previous_action_was_C + cooperators_among_opponents_prev_round
        if c_prev_round_count >= self.cooperation_benchmark:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_117(BaseStrategy):
    """
    Adaptive Reciprocator with Collective Threshold (ARCT) for N-Player Public Goods Game.

    This strategy fosters cooperation by starting cooperatively and then dynamically adjusting
    its behavior based on the collective cooperation level in the previous round. It punishes
    insufficient cooperation temporarily and anticipates the end of the game by defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.current_mode = 'COOPERATE'
        self.punish_rounds_remaining = 0
        self.MIN_COOPERATORS_FOR_COLLECTIVE_BENEFIT = max(1, math.ceil(self.n / self.k))
        self.PUNISHMENT_DURATION = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_number_0_indexed = state.round_number
        round_number_1_indexed = round_number_0_indexed + 1
        if round_number_1_indexed == 1:
            self.current_mode = 'COOPERATE'
            self.punish_rounds_remaining = 0
            return Action.C
        if round_number_1_indexed == self.r:
            return Action.D
        prev_round_opponent_cooperators_count = np.sum(history.opponent_actions[round_number_0_indexed - 1, :])
        prev_round_my_action_was_cooperate = history.my_actions[round_number_0_indexed - 1]
        prev_round_total_cooperators_count = prev_round_opponent_cooperators_count + (1 if prev_round_my_action_was_cooperate else 0)
        if self.punish_rounds_remaining > 0:
            self.punish_rounds_remaining -= 1
            self.current_mode = 'PUNISH'
            return Action.D
        if prev_round_total_cooperators_count >= self.MIN_COOPERATORS_FOR_COLLECTIVE_BENEFIT:
            self.current_mode = 'COOPERATE'
            return Action.C
        else:
            self.current_mode = 'PUNISH'
            self.punish_rounds_remaining = self.PUNISHMENT_DURATION - 1
            return Action.D

class Strategy_COLLECTIVE_118(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation and achieve a higher collective payoff
    while being adaptive and robust. It initiates cooperation in the first round.
    In intermediate rounds, it dynamically adjusts its behavior based on the observed
    level of collective cooperation (number of cooperators) in the previous round,
    using a threshold of `ceil(n / 2)`. If cooperation meets or exceeds this threshold,
    it cooperates; otherwise, it defects. In the final round, it defects to prevent
    exploitation, adhering to backward induction principles in finitely repeated games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters like n_players, n_rounds, k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold_T = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round based on the ACR strategy.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs for this player and opponents.
                     It is None for the first round (round_number = 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == self.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_was_C = history.my_actions[-1]
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
        if total_cooperators_prev_round >= self.cooperation_threshold_T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_119(BaseStrategy):
    """
    The "Adaptive Reciprocity" (AR) strategy for the N-Player Public Goods Game.
    It fosters collective cooperation by starting cooperatively and then adapting
    its behavior based on the observed level of cooperation in previous rounds.
    It uses a dynamic threshold (N_min_C = ceil(n / k)) to determine if the
    collective effort is sufficient to warrant continued cooperation, thus balancing
    the goal of high group payoffs with self-protection against free-riders.
    It defects in the last round as a standard game theory rational choice.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.N_min_C = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        previous_round_idx = state.round_number - 1
        num_cooperators_prev_round = int(history.my_actions[previous_round_idx]) + np.sum(history.opponent_actions[previous_round_idx, :])
        if num_cooperators_prev_round >= self.N_min_C:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_120(BaseStrategy):
    """
    Adaptive Public Goods Reciprocator (APGR) Strategy.

    This strategy promotes collective cooperation while remaining robust against exploitation
    in a public goods game tournament. It balances initial trust with adaptive responsiveness
    to the group's actions and the game's specific parameters.

    It cooperates in the first round to initiate collective action and defects in the last
    round due to backward induction. In intermediate rounds, it dynamically calculates a
    cooperation threshold based on game parameters (n, k) and continues to cooperate if the
    proportion of cooperators in the previous round meets or exceeds this threshold,
    otherwise, it defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.T_cooperate = (self.n_players - self.k_factor) / (self.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[current_round_0_indexed - 1])
            my_action_prev_round_is_c = int(history.my_actions[current_round_0_indexed - 1])
            total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_action_prev_round_is_c
            proportion_cooperators_prev_round = total_cooperators_prev_round / self.n_players
            if proportion_cooperators_prev_round >= self.T_cooperate:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_121(BaseStrategy):
    """
    The Adaptive Conditional Cooperate with Forgiveness (ACCF) strategy for the N-Player Public Goods Game.
    It prioritizes establishing and maintaining a high level of cooperation within the group.
    It starts cooperatively, then adaptively responds to the collective behavior observed in previous rounds.
    It aims to punish free-riding when cooperation dips below a reasonable threshold,
    but incorporates a forgiving mechanism to allow for recovery and prevent total collapse of cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ACCF strategy with game parameters and internal strategy parameters.

        Args:
            game_description: An object containing general game parameters like n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.forgiveness_probability = 0.2
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the ACCF strategy's decision rules.

        Args:
            state: GameState object containing the current round number (0-indexed).
            history: PlayerHistory object containing past actions and payoffs for all players.
                     This will be None for the very first round (state.round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == self.n_rounds - 1:
            return Action.D
        prev_round_idx = current_round_idx - 1
        C_prev = int(history.my_actions[prev_round_idx]) + np.sum(history.opponent_actions[prev_round_idx, :])
        p_prev = C_prev / self.n_players
        if p_prev == 0:
            return Action.D
        elif p_prev >= self.cooperation_threshold:
            return Action.C
        elif random.random() < self.forgiveness_probability:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_122(BaseStrategy):
    """
    The Adaptive Pro-Social Reciprocator (APSR) strategy for the N-Player Public Goods Game.
    APSR starts by cooperating to signal a willingness to contribute. In subsequent rounds,
    it dynamically assesses the collective cooperation level using a Critical Cooperation
    Threshold (T_crit). If cooperation is universally high or above this threshold, it
    continues to cooperate, exhibiting a pro-social bias in ambiguous zones. However, if
    cooperation falls below T_crit, it defects to protect against exploitation. In the
    final round, APSR always defects, anticipating rational behavior in a finitely
    repeated game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the APSR strategy with game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, and k (multiplication factor).
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the APSR strategy rules.

        Args:
            state: The current state of the game, including the round number.
            history: A record of past actions and payoffs for all players, or None
                     if it's the first round.

        Returns:
            Action.C for Cooperate or Action.D for Defect.
        """
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            C_prev = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            T_crit = max(1, math.floor(self.n_players * (1 - 1 / self.k_factor)))
            if C_prev == self.n_players:
                return Action.C
            elif C_prev < T_crit:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_123(BaseStrategy):
    """
    Adaptive Threshold Cooperation (ATC) strategy for the N-Player Public Goods Game.

    This strategy aims to promote and sustain cooperation by adopting an initial
    cooperative stance and then conditionally cooperating based on the observed
    cooperation level of all players in the previous round. It incorporates
    self-preservation by defecting when cooperation levels are low and in the
    final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.n_rounds - 1:
            return Action.D
        else:
            previous_round_opponent_actions = history.opponent_actions[state.round_number - 1, :]
            previous_round_my_action = history.my_actions[state.round_number - 1]
            total_cooperators_prev_round = np.sum(previous_round_opponent_actions) + previous_round_my_action
            proportion_cooperators_prev_round = total_cooperators_prev_round / self.n_players
            if proportion_cooperators_prev_round >= 0.5:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_124(BaseStrategy):
    """
    The "Adaptive Reciprocator" strategy for the N-player Public Goods Game.

    This strategy aims to promote collective cooperation while remaining robust against
    exploitation. It initiates cooperation in the first round and defects in the last.
    In intermediate rounds, it adapts its behavior based on the observed collective
    cooperation levels of other players from the previous round. The strategy is more
    forgiving (requires less cooperation from others) when the public good is more
    efficient (high k/n) and less forgiving when it's less efficient (low k/n).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 1 - self.game_description.k / self.game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == self.game_description.n_rounds:
            return Action.D
        num_other_cooperators = sum(history.opponent_actions[-1, :])
        num_others = self.game_description.n_players - 1
        coop_rate_among_others = num_other_cooperators / num_others
        if coop_rate_among_others >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_125(BaseStrategy):
    """
    The Collective Reciprocator strategy for the N-Player Public Goods Game.
    It initiates with cooperation, then dynamically adjusts its behavior
    based on the observed aggregate cooperation level of other players
    and its own previous round's payoff, using game parameters (n, k)
    to define thresholds for robust decision-making. It defects in the final
    round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T_low = 1.0 / self.k
        self.T_high = (self.n - 1.0) / self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        P_coop_prev_round = num_cooperators_prev_round / self.n
        my_prev_payoff = history.my_payoffs[-1]
        if P_coop_prev_round >= self.T_high:
            return Action.C
        elif P_coop_prev_round < self.T_low:
            return Action.D
        elif my_prev_payoff > 1.0:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_126(BaseStrategy):
    """
    Adaptive Collective Reciprocity with End-Game Defection (ACRED) strategy.

    This strategy aims to foster and sustain collective cooperation in the N-Player
    Public Goods Game while being robust against exploitation and adaptable.
    It initiates cooperation in the first round, then conditionally cooperates
    based on the observed collective effort of previous rounds. It always
    defects in the final round to account for the end-game effect.

    The cooperation threshold (T_C) is dynamically calculated based on the game's
    multiplication factor (k) and the number of players (n), ensuring that
    cooperation is sustained only when the public good yields a reasonable return.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k_factor = game_description.k
        self.total_rounds = game_description.n_rounds
        self.cooperation_threshold = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.total_rounds:
            return Action.D
        if state.round_number == 0:
            return Action.C
        else:
            N_C_prev = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if N_C_prev >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_127(BaseStrategy):
    """
    Adaptive Collective Response (ACR) strategy for the N-Player Public Goods Game.

    Core Philosophy:
    The ACR strategy seeks to establish and maintain a high level of collective cooperation
    by initiating trust and then conditioning continued cooperation on sufficient participation
    from the group. It is pragmatic, acknowledging the individual incentive to defect,
    particularly in the final stages of the game, but prioritizes collective welfare when
    repeated interaction allows for it.

    Decision Rules:
    1. First Round (Round 0, 0-indexed): Initiative of Trust - Cooperate (C)
    2. Intermediate Rounds (Round 1 to r-2, 0-indexed): Conditional Cooperation
       - Observe actions of all players in the immediate previous round.
       - Calculate total cooperators from the previous round.
       - If total cooperators meet or exceed the dynamic threshold (ceil(n/k)), then Cooperate (C).
       - Otherwise, Defect (D).
    3. Last Round (Round r-1, 0-indexed): Backward Induction - Defect (D)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T_cooperators = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds_0_indexed = self.r - 1
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds_0_indexed:
            return Action.D
        previous_round_index = current_round_0_indexed - 1
        my_action_prev_round = history.my_actions[previous_round_index]
        opponent_actions_prev_round = history.opponent_actions[previous_round_index, :]
        total_cooperators_prev_round = int(my_action_prev_round) + np.sum(opponent_actions_prev_round)
        if total_cooperators_prev_round >= self.T_cooperators:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_128(BaseStrategy):
    """
    The Adaptive Reciprocal Cooperator (ARC) strategy for the N-Player Public Goods Game.
    This strategy aims to initiate and maintain cooperation by responding adaptively to
    the collective behavior of other players. It starts by trusting, then reciprocates
    cooperation if a sufficient number of players (a simple majority) cooperated in
    the previous round. It protects itself from exploitation by defecting when cooperation
    levels are insufficient and always defects in the final round of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            my_action_prev_round_was_cooperate = history.my_actions[-1]
            total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_cooperate else 0)
            if total_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_129(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to facilitate cooperation by being initially trusting (cooperating in the first round),
    then conditionally cooperating based on the observed collective action of other players.
    It will only continue to contribute to the public good if a sufficient number of others
    (meeting a threshold of ceil(n/k) cooperators) also did so in the previous round.
    In the final round, it defects to avoid end-game exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.r
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        else:
            num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if num_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_130(BaseStrategy):
    """
    Adaptive Reciprocal Participation (ARP) strategy for the N-Player Public Goods Game.

    This strategy initiates cooperation by playing 'C' in the first round. In subsequent
    middle rounds, it observes the total number of cooperators from the preceding round.
    If this count meets or exceeds a dynamically calculated 'cooperation threshold' (T),
    the strategy reciprocates by playing 'C'. Otherwise, it plays 'D' to penalize
    insufficient collective effort. In the final round, it always plays 'D' due to
    endgame rationality, preventing exploitation.

    The cooperation threshold T = ceil(n / k), which adapts to the game's multiplication
    factor 'k'. A higher 'k' (closer to 'n') leads to a lower T, making the strategy
    more forgiving and promoting cooperation more easily. A lower 'k' (closer to 1)
    results in a higher T, requiring more cooperators to sustain its own contribution.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_0_indexed_round = state.round_number
        total_rounds = self.r
        if current_0_indexed_round == 0:
            return Action.C
        elif current_0_indexed_round == total_rounds - 1:
            return Action.D
        else:
            num_cooperators_previous_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if num_cooperators_previous_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_131(BaseStrategy):
    """
    Adaptive Collective Cooperator (ACC) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize collective payoff by promoting cooperation,
    while remaining resilient to free-riding and strategic defection. It starts
    cooperatively, then conditions its action on the observed collective contribution
    of others in the previous round using a threshold dynamically set by the game's
    multiplication factor 'k'. It also accounts for the unique strategic
    implications of the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_T = 1.0 / self.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            my_previous_action_is_C = history.my_actions[state.round_number - 1]
            opponents_previous_actions_are_C = history.opponent_actions[state.round_number - 1, :]
            x_prev = int(my_previous_action_is_C) + np.sum(opponents_previous_actions_are_C)
            cooperation_rate = x_prev / self.n_players
            if cooperation_rate >= self.cooperation_threshold_T:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_132(BaseStrategy):
    """
    The Adaptive Reciprocal Cooperator (ARC) strategy for the N-Player Public Goods Game.
    It starts cooperatively, adapts to the collective cooperation level,
    and defects in the final round to prevent exploitation.

    Core Principle: Start cooperatively, observe the general level of cooperation,
    punish widespread defection, and ensure self-preservation in the endgame.

    COOPERATION_THRESHOLD: floor(n / 2) + 1. Requires a strict majority of players
    to cooperate to sustain cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.COOPERATION_THRESHOLD = math.floor(self.game_description.n_players / 2) + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            my_prev_action = history.my_actions[state.round_number - 1]
            opponent_prev_actions = history.opponent_actions[state.round_number - 1, :]
            num_cooperators_prev_round = (1 if my_prev_action else 0) + np.sum(opponent_prev_actions)
            if num_cooperators_prev_round >= self.COOPERATION_THRESHOLD:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_133(BaseStrategy):
    """
    The Adaptive Collective Reciprocator (ACR) strategy promotes and sustains cooperation
    in N-Player Public Goods Games. It starts by cooperating, then adapts its behavior
    based on the collective cooperation level in the previous round, using a dynamic
    threshold. It incorporates a forgiveness mechanism to tolerate temporary dips
    in cooperation before resorting to defection, and defects in the final round
    to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.forgiveness_window = 2
        self.consecutive_low_cooperation_rounds = 0
        calculated_threshold_raw = self.n * (self.k - 1) / (self.n - 1)
        calculated_threshold = math.ceil(calculated_threshold_raw)
        self.cooperation_threshold = max(2, min(self.n, calculated_threshold))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            self.consecutive_low_cooperation_rounds = 0
            return Action.C
        elif current_round_1_indexed == self.r:
            return Action.D
        else:
            total_cooperators_prev_round = int(history.my_actions[-1]) + sum(history.opponent_actions[-1, :])
            if total_cooperators_prev_round >= self.cooperation_threshold:
                self.consecutive_low_cooperation_rounds = 0
                return Action.C
            else:
                self.consecutive_low_cooperation_rounds += 1
                if self.consecutive_low_cooperation_rounds <= self.forgiveness_window:
                    return Action.C
                else:
                    return Action.D

class Strategy_COLLECTIVE_134(BaseStrategy):
    """
    Parametric Reciprocal Contributor (PRC) strategy for the N-Player Public Goods Game.
    This strategy is designed to be adaptive, robust, and aligned with a collective mindset.
    It starts cooperatively (first round), defects in the last round (due to backward induction),
    and in intermediate rounds, it reciprocates the observed level of group cooperation.
    It cooperates if the group's past cooperation rate meets or exceeds a specific threshold,
    derived from the game's multiplication factor 'k', otherwise it defects to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = 1.0 / self.k_factor

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        previous_round_index = current_round_0_indexed - 1
        my_previous_action_was_C = history.my_actions[previous_round_index]
        opponent_cooperator_count_prev = np.sum(history.opponent_actions[previous_round_index, :])
        total_cooperator_count_prev = int(my_previous_action_was_C) + opponent_cooperator_count_prev
        group_cooperation_rate = total_cooperator_count_prev / self.n_players
        if group_cooperation_rate >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_135(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster and maintain cooperation by starting with an initial act of trust,
    then dynamically adjusting its cooperation based on the observed collective behavior
    of the previous round. It utilizes a 'tolerance_level' derived from the game's
    multiplication factor 'k' to decide how much defection it can tolerate. In the
    final round, it strategically defects to avoid end-game exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round = history.my_actions[-1]
        num_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round else 0)
        f_C = num_cooperators_prev_round / self.n_players
        tolerance_level = (self.n_players - self.k + 1) / self.n_players
        if f_C >= tolerance_level:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_136(BaseStrategy):
    """
    The Adaptive Collective Reciprocator (ACR) strategy aims to initiate and sustain cooperation within
    the group, recognizing the collective benefit of widespread cooperation. It employs a conditional
    cooperation mechanism, where it is willing to cooperate if a sufficient number of other players
    have demonstrated cooperation in the preceding round. It also includes protective measures
    against exploitation, especially in the game's final stages.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.COOPERATION_THRESHOLD_PROPORTION_OF_OTHERS = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        num_other_cooperators_previous_round = np.sum(prev_round_opponent_actions)
        required_other_cooperators = math.ceil((n_players - 1) * self.COOPERATION_THRESHOLD_PROPORTION_OF_OTHERS)
        if num_other_cooperators_previous_round >= required_other_cooperators:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_137(BaseStrategy):
    """
    The Collective Harmony Enforcer (CHE) strategy aims to foster and maintain a high level of cooperation.
    It cooperates initially and demands a calculated threshold of collective cooperation.
    If cooperation drops below this threshold, it enters a short "PUNISHING" phase by defecting.
    After punishment, it enters a "RECOVERING" phase, cooperating to test if collective cooperation has returned.
    It defects in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.my_state = 'COOPERATING'
        self.rounds_in_current_state = 0
        self.COOPERATION_THRESHOLD_RATIO = max(0.5, self.k / self.n + (1 - self.k / self.n) / 2)
        self.PUNISHMENT_DURATION = min(2, self.r - 1)
        self.RECOVERY_DURATION = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.r - 1:
            return Action.D
        if state.round_number == 0:
            self.my_state = 'COOPERATING'
            self.rounds_in_current_state = 0
            return Action.C
        total_cooperators_prev_round = int(history.my_actions[-1]) + sum(history.opponent_actions[-1, :])
        cooperation_ratio_prev_round = total_cooperators_prev_round / self.n
        if self.my_state == 'COOPERATING':
            if cooperation_ratio_prev_round < self.COOPERATION_THRESHOLD_RATIO:
                self.my_state = 'PUNISHING'
                self.rounds_in_current_state = 1
                return Action.D
            else:
                return Action.C
        elif self.my_state == 'PUNISHING':
            if self.rounds_in_current_state < self.PUNISHMENT_DURATION:
                self.rounds_in_current_state += 1
                return Action.D
            elif cooperation_ratio_prev_round >= self.COOPERATION_THRESHOLD_RATIO:
                self.my_state = 'COOPERATING'
                self.rounds_in_current_state = 0
                return Action.C
            else:
                self.my_state = 'RECOVERING'
                self.rounds_in_current_state = 1
                return Action.C
        elif self.my_state == 'RECOVERING':
            if cooperation_ratio_prev_round < self.COOPERATION_THRESHOLD_RATIO:
                self.my_state = 'PUNISHING'
                self.rounds_in_current_state = 1
                return Action.D
            elif self.rounds_in_current_state < self.RECOVERY_DURATION:
                self.rounds_in_current_state += 1
                return Action.C
            else:
                self.my_state = 'COOPERATING'
                self.rounds_in_current_state = 0
                return Action.C

class Strategy_COLLECTIVE_138(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR starts with cooperation to signal willingness to contribute. In intermediate rounds,
    it reciprocates the average cooperation level of *other* players in the previous round:
    it cooperates if at least 50% of other players cooperated, and defects otherwise.
    In the final round, it defaults to defection to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with the game's parameters.

        Args:
            game_description: A PublicGoodsDescription object containing
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: A GameState object containing the current round number.
            history: A PlayerHistory object containing past actions and payoffs,
                     or None if it's the very first round.

        Returns:
            An Action (Action.C for Cooperate, Action.D for Defect).
        """
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        sum_other_cooperators_t_minus_1 = sum(history.opponent_actions[-1, :])
        num_other_players = self.n_players - 1
        cooperation_rate_others_t_minus_1 = sum_other_cooperators_t_minus_1 / num_other_players
        if cooperation_rate_others_t_minus_1 >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_139(BaseStrategy):
    """
    Adaptive Collective Reciprocator (ACR) strategy for the N-Player Public Goods Game.

    This strategy adapts its behavior based on observed cooperation levels from other players
    in previous rounds. It aims to foster and sustain cooperation towards the collective
    optimum while being robust to varying opponent behaviors and protecting against
    persistent exploitation.

    ACR uses two main parameters derived from the game specification (n, r):
    1. MIN_COOPERATORS_FOR_COOP (MCC): The minimum number of players (including self)
       who must have cooperated in the previous round for the collective effort
       to be considered "sufficient".
    2. GRACE_PERIOD_ROUNDS: How many consecutive rounds the strategy will continue
       to cooperate even if observed cooperation falls below the MCC threshold.
       This acts as a forgiveness mechanism.

    Internal State:
    - rounds_since_last_sufficient_cooperation: A counter tracking consecutive rounds
      where collective cooperation was below the MCC.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.MCC = math.floor(n / 2) + 1
        self.GRACE_PERIOD_ROUNDS = max(1, math.floor(r / 5))
        self.rounds_since_last_sufficient_cooperation = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        C_prev = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if C_prev >= self.MCC:
            self.rounds_since_last_sufficient_cooperation = 0
        else:
            self.rounds_since_last_sufficient_cooperation += 1
        if C_prev >= self.MCC:
            return Action.C
        elif self.rounds_since_last_sufficient_cooperation <= self.GRACE_PERIOD_ROUNDS:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_140(BaseStrategy):
    """
    Adaptive Collective Reciprocator (ACR) strategy for N-Player Public Goods Game.

    ACR aims to foster cooperation, achieve collective optimum, and remain robust against exploitation.
    It combines an initial trust signal, a dynamic threshold for adaptive reciprocity,
    and a pragmatic end-game defection.

    Core Principles:
    1. Initial Trust: Starts with cooperation to signal willingness for collective action.
    2. Adaptive Reciprocity: Monitors collective cooperation; cooperates if above a dynamic threshold, defects otherwise.
    3. End-Game Awareness: Defects in the final round to prevent exploitation (unraveling problem).
    4. Collective Mindset: Seeks collective good balanced with self-preservation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        else:
            num_cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
            threshold_proportion = math.ceil(self.n / 2)
            threshold_return = math.ceil(self.n / self.k)
            cooperation_threshold = max(threshold_proportion, threshold_return)
            if num_cooperators_prev_round >= cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_141(BaseStrategy):
    """
    Adaptive Collective Forgiveness (ACF) strategy for the N-Player Public Goods Game.

    ACF aims to foster and maintain high levels of cooperation by:
    1. Initiating cooperation in the first round to signal intent.
    2. Dynamically responding to observed collective effort in intermediate rounds.
       It uses a calculated threshold (T_c = max(1, n - floor(k))) to determine if
       the number of cooperators in the previous round is sufficient to warrant
       continued cooperation (forgiving up to floor(k) defectors).
    3. Defecting in the final round, based on backward induction, to avoid exploitation.

    This strategy balances the desire for collective optimum with individual rationality
    and defense against free-riding.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = max(1, self.n_players - math.floor(self.k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            previous_round_cooperators = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
            if previous_round_cooperators >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_142(BaseStrategy):
    """
    Adaptive Conditional Reciprocity (ACR) Strategy for N-Player Public Goods Game.

    This strategy aims to foster cooperation by starting cooperatively,
    then reciprocating based on the collective cooperation observed in the
    previous round, and defecting in the last round to avoid exploitation.

    Core Principle:
    - Round 1: Cooperate (signal trust).
    - Intermediate Rounds: Cooperate if total cooperators in previous round
      met or exceeded a calculated threshold (T_cooperate), otherwise Defect.
    - Last Round: Defect (due to endgame effect).

    T_cooperate is calculated as ceil(n / k), representing the minimum
    number of players who must have cooperated in the previous round to
    sustain cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T_cooperate = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.r:
            return Action.D
        else:
            cooperators_among_opponents = np.sum(history.opponent_actions[-1, :])
            my_prev_action_was_cooperation = history.my_actions[-1]
            total_cooperators_prev_round = cooperators_among_opponents + (1 if my_prev_action_was_cooperation else 0)
            if total_cooperators_prev_round >= self.T_cooperate:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_143(BaseStrategy):
    """
    The Adaptive Collective Threshold (ACT) strategy aims to foster and maintain cooperation by
    actively participating when a sufficient level of collective contribution is observed.
    It begins by signaling cooperation, adapts based on the previous round's group performance,
    and incorporates a pragmatic approach for the final round. It's a "memory-1" strategy,
    making it forgiving and responsive to the immediate prior round's collective actions.
    """
    COOPERATION_THRESHOLD = 0.5

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialises the strategy with game parameters.

        Args:
            game_description (PublicGoodsDescription): An object containing game parameters
                                                      (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the ACT strategy.

        Args:
            state (GameState): An object containing the current round number.
            history (None | PlayerHistory): An object containing historical actions and payoffs
                                            for the current player and opponents, or None for
                                            the first round.

        Returns:
            Action: The chosen action (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        elif current_round_idx == self.n_rounds - 1:
            return Action.D
        else:
            my_prev_action_is_cooperate = history.my_actions[-1]
            opponent_prev_cooperators = sum(history.opponent_actions[-1, :])
            c_prev_total = int(my_prev_action_is_cooperate) + opponent_prev_cooperators
            proportion_cooperators = c_prev_total / self.n_players
            if proportion_cooperators >= self.COOPERATION_THRESHOLD:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_144(BaseStrategy):
    """
    The "Adaptive Collective" strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain cooperation by initiating with a
    cooperate move and then conditionally cooperating in subsequent rounds.
    Its decision in intermediate rounds is based on whether a sufficient number
    of players cooperated in the previous round. The 'sufficient' threshold
    is dynamically determined by the game's efficiency parameters (n and k),
    calibrated to reflect the minimum collective effort needed for a productive
    public good. In the final round, it always defects due to the well-known
    end-game effect in finitely repeated games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.n_rounds - 1:
            return Action.D
        if current_round == 0:
            return Action.C
        num_cooperators_prev_round = np.sum(history.opponent_actions[current_round - 1, :])
        if history.my_actions[current_round - 1]:
            num_cooperators_prev_round += 1
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_145(BaseStrategy):
    """
    Collective Reciprocity (CR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and sustain collective cooperation by responding adaptively to
    the observed behavior of other players, while remaining robust to exploitation.
    It initiates cooperation, sustains it if enough players contribute, and defects in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            C_prev = np.sum(history.opponent_actions[-1, :])
            cooperation_threshold = max(1, math.ceil(self.n_players - self.k))
            if C_prev >= cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_146(BaseStrategy):
    """
    Adaptive Public Goods Reciprocator (APGR) strategy for the N-Player Public Goods Game.

    APGR aims to achieve collective good by starting cooperatively, reciprocating sufficient
    collective effort, and punishing insufficient participation.

    - **First Round:** Always Cooperate (C) to signal willingness and initiate cooperation.
    - **Intermediate Rounds:** Observe the total number of cooperators in the preceding round (`C_prev`).
      If `C_prev` meets or exceeds `cooperation_threshold = ceil(n / k)`, continue to Cooperate (C).
      Otherwise, Defect (D) to penalize free-riding and encourage higher cooperation.
    - **Last Round:** Always Defect (D) due to backward induction in finite games to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        cooperators_from_opponents_prev_round = sum(history.opponent_actions[-1, :])
        my_action_prev_round_was_C = history.my_actions[-1]
        total_cooperators_in_prev_round = cooperators_from_opponents_prev_round + (1 if my_action_prev_round_was_C else 0)
        if total_cooperators_in_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_147(BaseStrategy):
    """
    Reciprocal Threshold Cooperation (RTC) strategy for the N-Player Public Goods Game.

    RTC is an adaptive strategy that balances initial trust, conditional reciprocity,
    and self-protection in a finite repeated game. It cooperates initially to signal
    willingness, then reciprocates cooperation if the total number of cooperators
    in the previous round meets a pre-calculated sustainability threshold (T).
    In the final round, it defects to avoid exploitation, aligning with endgame rationality.

    Key Principles:
    - Initial Trust: Cooperate in Round 1.
    - Conditional Reciprocity: In intermediate rounds, cooperate if previous round's total
      cooperators (C_prev) >= threshold_T, otherwise defect.
    - Endgame Rationality: Defect in the final round.

    Threshold T Calculation:
    T = ceil(n / k)
    This threshold ensures that the average return from the public good for a single
    cooperator is at least equal to their individual contribution.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.threshold_T = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.r:
            return Action.D
        else:
            cooperators_in_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if cooperators_in_prev_round >= self.threshold_T:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_148(BaseStrategy):
    """
    Adaptive Public Goods Strategy (APGS) aims to encourage cooperation by starting with a cooperative gesture,
    then conditionally cooperating based on the observed collective effort in the previous round. It defects
    if cooperation levels drop too low and always defects in the final round as per game-theoretic rationality.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            my_action_prev_round_was_C = history.my_actions[-1]
            total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_action_prev_round_was_C
            if total_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_149(BaseStrategy):
    """
    The Adaptive Reciprocal Contributor strategy for the N-Player Public Goods Game.

    This strategy aims to foster collective cooperation while remaining robust against
    exploitation. It initiates cooperation in the first round to signal willingness
    to contribute. In subsequent rounds, it adapts its behavior based on the
    observed collective contribution of other players. If the number of cooperators
    in the previous round meets or exceeds a defined threshold (50% of players),
    it continues to cooperate. Otherwise, it defects to avoid exploitation or
    signal dissatisfaction. In the final round, it always defects to maximize
    individual payoff when there's no "shadow of the future."
    """
    COOPERATION_THRESHOLD_PERCENTAGE = 0.5

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_count = math.ceil(self.game_description.n_players * self.COOPERATION_THRESHOLD_PERCENTAGE)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_cooperators_previous_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_previous_round >= self.cooperation_threshold_count:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_150(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain cooperation by being initially trusting,
    conditionally cooperative, and adaptive to the observed behavior of other players.

    It operates under three main rules:
    1. First Round: Always Cooperate to initiate trust.
    2. Last Round: Always Defect, due to the end-game unraveling effect.
    3. Intermediate Rounds: Cooperate if the total number of cooperators in the
       previous round met or exceeded a calculated `T_coop` threshold. Otherwise, defect.
       The `T_coop` threshold is `ceil(n / k)`, representing the minimum number of
       cooperators required for the public good to generate an average return of
       at least 1 unit per contribution.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.T_coop = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        prev_round_my_action_was_C = history.my_actions[current_round_0_indexed - 1]
        prev_round_opponent_cooperators = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        total_cooperators_prev_round = prev_round_my_action_was_C + prev_round_opponent_cooperators
        if total_cooperators_prev_round >= self.T_coop:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_151(BaseStrategy):
    """
    Adaptive Collective Reciprocator strategy for the N-Player Public Goods Game.
    This strategy aims to foster a high level of cooperation by starting cooperatively,
    being forgiving of minor collective defections, but retaliating against
    sustained lack of cooperation. It adapts its expectations based on the game
    parameters `n` and `k`, and adjusts its behavior for critical edge cases.
    """
    defection_streak: int
    Patience_Limit: int = 2
    T_coop_threshold: int

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defection_streak = 0
        n_players = self.game_description.n_players
        k_factor = self.game_description.k
        self.T_coop_threshold = max(math.ceil(n_players / 2), math.ceil(n_players / k_factor))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            self.defection_streak = 0
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        C_count_prev = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if C_count_prev >= self.T_coop_threshold:
            self.defection_streak = 0
            return Action.C
        else:
            self.defection_streak += 1
            if self.defection_streak >= self.Patience_Limit:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_152(BaseStrategy):
    """
    Collective Reciprocity with Last Round Defection (CR-LRD) strategy.

    This strategy starts by cooperating to signal willingness for collective action.
    In intermediate rounds, it practices collective reciprocity: if a sufficient
    number of players (at least ceil(n/2)) cooperated in the previous round,
    it continues to cooperate; otherwise, it defects to protect its payoff and
    signal dissatisfaction.
    In the final round, it defects to prevent exploitation due to the end of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.T_coop = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        elif current_round == self.n_rounds - 1:
            return Action.D
        else:
            my_prev_action = history.my_actions[current_round - 1]
            opponent_prev_actions = history.opponent_actions[current_round - 1, :]
            m_prev = int(my_prev_action) + np.sum(opponent_prev_actions)
            if m_prev >= self.T_coop:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_153(BaseStrategy):
    """
    Adaptive Public Goods Reciprocity (APGR) strategy for the N-Player Public Goods Game.

    This strategy aims to establish and maintain collective cooperation by:
    1.  Cooperating in the first round to signal a willingness to contribute.
    2.  Defecting in the final round to protect against exploitation (endgame effect).
    3.  In intermediate rounds, adapting its behavior based on the observed level of
        group cooperation in the immediately preceding round. It uses a dynamic
        threshold, `ceil(n_players / k)`, to decide if the collective effort is
        sufficient to warrant continued cooperation. If the number of cooperators
        in the previous round meets or exceeds this threshold, it cooperates;
        otherwise, it defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == r - 1:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        cooperation_threshold = math.ceil(n / k)
        if num_cooperators_prev_round >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_154(BaseStrategy):
    """
    Collective Reciprocity with Adaptive Threshold (CRAT) strategy for the N-Player Public Goods Game.
    This strategy adapts its cooperation based on a dynamically calculated threshold of collective
    cooperation in the previous round, aiming to foster and sustain collective cooperation while
    protecting against exploitation, especially in the endgame.

    Core Philosophy:
    The CRAT strategy aims to foster and sustain collective cooperation by initiating trust,
    reciprocating observed cooperation, and punishing defection when it falls below a dynamically
    adjusted threshold. It prioritizes the long-term collective good while protecting against
    exploitation in specific scenarios (like the endgame or sustained non-cooperation).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        else:
            previous_round_idx = current_round_0_indexed - 1
            opponent_cooperators_count = sum(history.opponent_actions[previous_round_idx, :])
            my_previous_action_was_C = history.my_actions[previous_round_idx]
            previous_cooperators_count = opponent_cooperators_count + (1 if my_previous_action_was_C else 0)
            cooperation_threshold_count = math.floor((self.n + self.k) / 2.0)
            if previous_cooperators_count >= cooperation_threshold_count:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_155(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    It's designed to foster and sustain collective cooperation by being initially trusting,
    reciprocally adaptive, robust to minor deviations, and strategically aware of the game's boundaries.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.cooperation_stance = 'Cooperate'
        self.D_TOLERANCE = 1 if self.n == 2 else 2
        self.C_REBUILD_THRESHOLD = math.ceil(self.n * 2 / 3)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            self.cooperation_stance = 'Cooperate'
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        C_prev_opponents = np.sum(history.opponent_actions[-1, :])
        C_prev_me = int(history.my_actions[-1])
        C_prev = C_prev_opponents + C_prev_me
        D_prev = self.n - C_prev
        if self.cooperation_stance == 'Cooperate':
            if D_prev <= self.D_TOLERANCE:
                return Action.C
            else:
                self.cooperation_stance = 'Punish'
                return Action.D
        elif self.cooperation_stance == 'Punish':
            if C_prev >= self.C_REBUILD_THRESHOLD:
                self.cooperation_stance = 'Rebuilding_Trust'
                return Action.C
            else:
                return Action.D
        elif self.cooperation_stance == 'Rebuilding_Trust':
            if D_prev <= self.D_TOLERANCE:
                self.cooperation_stance = 'Cooperate'
                return Action.C
            else:
                self.cooperation_stance = 'Punish'
                return Action.D

class Strategy_COLLECTIVE_156(BaseStrategy):
    """
    Adaptive Threshold Reciprocity (ATR) strategy for the N-Player Public Goods Game.

    ATR aims to foster collective cooperation by initiating cooperation, monitoring
    the group's cooperation rate, and reciprocating by cooperating when the collective
    effort is sufficient, or by defecting to punish and signal when it is not.
    It accounts for the special dynamics of the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_target_threshold = (self.k + self.n_players) / (2 * self.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == self.n_rounds:
            return Action.D
        if current_round_t == 1:
            return Action.C
        prev_round_idx = state.round_number - 1
        my_prev_action_is_cooperate = history.my_actions[prev_round_idx]
        opponent_prev_actions_are_cooperate = history.opponent_actions[prev_round_idx, :]
        num_cooperators_prev_round = (1 if my_prev_action_is_cooperate else 0) + np.sum(opponent_prev_actions_are_cooperate)
        cooperation_rate_prev_round = num_cooperators_prev_round / self.n_players
        if cooperation_rate_prev_round >= self.cooperation_target_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_157(BaseStrategy):
    """
    Adaptive Collective Reciprocator (ACR) strategy for N-Player Public Goods Game.
    Aims to foster cooperation by being initially optimistic, reciprocating cooperative behavior,
    punishing defection, and offering forgiveness, while playing rationally in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.collective_spirit_score: float = 1.0
        self.alpha: float = 0.25
        self.cooperation_threshold: float = 0.5
        self.defection_streak_counter: int = 0
        self.max_defection_streak: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        action_for_this_round: Action
        if current_round_1_indexed == 1:
            action_for_this_round = Action.C
            self.defection_streak_counter = 0
        elif current_round_1_indexed == r:
            action_for_this_round = Action.D
            self.defection_streak_counter = 0
        else:
            num_cooperators_others_prev = sum(history.opponent_actions[-1, :])
            n_minus_1 = n - 1
            P_others_prev = float(num_cooperators_others_prev) / n_minus_1
            self.collective_spirit_score = self.alpha * P_others_prev + (1 - self.alpha) * self.collective_spirit_score
            if num_cooperators_others_prev < math.floor(float(n_minus_1) / 3.0) or self.collective_spirit_score < self.cooperation_threshold / 2.0:
                action_for_this_round = Action.D
            elif self.defection_streak_counter >= self.max_defection_streak:
                action_for_this_round = Action.C
            elif self.collective_spirit_score >= self.cooperation_threshold:
                action_for_this_round = Action.C
            else:
                action_for_this_round = Action.D
            if action_for_this_round == Action.D:
                self.defection_streak_counter += 1
            else:
                self.defection_streak_counter = 0
        return action_for_this_round

class Strategy_COLLECTIVE_158(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) Strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by:
    1. Cooperating in the first round to signal willingness.
    2. Defecting in the last round due to backward induction.
    3. In intermediate rounds, it cooperates if a sufficient proportion (cooperation_threshold)
       of all players cooperated in the previous round, otherwise it defects.
       This acts as a conditional punishment/reward mechanism based on collective behavior,
       encouraging collective contribution while protecting against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
        my_action_prev_round_was_C = history.my_actions[state.round_number - 1]
        num_total_cooperators_prev_round = num_opponent_cooperators_prev_round + int(my_action_prev_round_was_C)
        cooperation_rate_prev = num_total_cooperators_prev_round / self.n_players
        if cooperation_rate_prev >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_159(BaseStrategy):
    """
    Adaptive Reciprocity with End-Game Prudence for the N-Player Public Goods Game.

    This strategy initiates cooperation, then adapts its behavior based on the
    observed level of cooperation in the previous round. It becomes stricter
    in the penultimate round and defects in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        elif current_round_idx == self.n_rounds - 1:
            return Action.D
        else:
            previous_round_idx = current_round_idx - 1
            cooperators_prev_round = int(history.my_actions[previous_round_idx]) + np.sum(history.opponent_actions[previous_round_idx, :])
            T_c_threshold_proportion = 0.5
            if current_round_idx == self.n_rounds - 2:
                T_c_threshold_proportion = 0.9
            min_cooperators_needed = math.ceil(T_c_threshold_proportion * self.n_players)
            if cooperators_prev_round >= min_cooperators_needed:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_160(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy prioritizes initiating cooperation, maintaining it when collective effort
    is sufficient, tolerating minor dips, and defecting to protect against exploitation
    or in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.MIN_SUSTAINABLE_COOP = max(1, math.ceil(self.n / self.k))
        TARGET_COOP_LEVEL_unadjusted = math.ceil(self.n * 0.75)
        self.TARGET_COOP_LEVEL = max(TARGET_COOP_LEVEL_unadjusted, self.MIN_SUSTAINABLE_COOP)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number == 0:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        my_prev_action_is_cooperate = history.my_actions[-1]
        opponent_prev_actions_are_cooperate = history.opponent_actions[-1, :]
        N_C_prev = int(my_prev_action_is_cooperate) + np.sum(opponent_prev_actions_are_cooperate)
        if N_C_prev >= self.TARGET_COOP_LEVEL:
            return Action.C
        elif N_C_prev >= self.MIN_SUSTAINABLE_COOP:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_161(BaseStrategy):
    """
    Adaptive Forgiving Cooperator (AFC) strategy for the N-Player Public Goods Game.

    This strategy embodies a collective mindset by:
    - Initiating Cooperation: Starting with cooperation to build trust.
    - Maintaining Cooperation: Continuing to cooperate as long as collective effort is sufficient.
    - Punishing Defection (Briefly): Temporarily defecting when cooperation falls below a threshold.
    - Forgiving Nature: Ensuring punishment phases are brief (1 round) to promote quick recovery.
    - Rational Self-Preservation: Defecting in the final round to prevent exploitation.

    It dynamically calculates a cooperation threshold based on game parameters (ceil(n/k))
    to ensure individual cooperation is worthwhile for contributing group members.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.COOPERATION_THRESHOLD = math.ceil(self.n / self.k)
        self.PUNISHMENT_DURATION = 1
        self.is_punishing = False
        self.punishment_rounds_left = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.r:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if self.is_punishing:
            self.punishment_rounds_left -= 1
            if self.punishment_rounds_left == 0:
                self.is_punishing = False
            return Action.D
        elif num_cooperators_prev_round < self.COOPERATION_THRESHOLD:
            self.is_punishing = True
            self.punishment_rounds_left = self.PUNISHMENT_DURATION
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_162(BaseStrategy):
    """
    Adaptive Collective Reciprocator strategy for N-Player Public Goods Game.

    This strategy is built on principles of conditional cooperation, aiming to
    foster and sustain collective action while protecting against systematic exploitation.
    It starts with an act of trust, adapts to observed collective behavior, and
    incorporates a pragmatic response to the game's finite horizon.

    Decision Logic:
    - Round 1: Always Cooperate (C) to initiate trust and signal willingness to cooperate.
    - Rounds 2 to r-1: Observe the total number of cooperators in the previous round.
      If this number meets or exceeds `ceil(n / 2)`, the strategy Cooperates (C).
      Otherwise, it Defects (D) as a signal of dissatisfaction with low collective effort.
    - Round r (Last Round): Always Defect (D) to protect against exploitation,
      as future incentives for cooperation are absent.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Collective Reciprocator strategy.

        Args:
            game_description (PublicGoodsDescription): An object containing
                                                       game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate 'C' or Defect 'D') for the current round.

        Args:
            state (GameState): An object containing the current round number (0-indexed).
            history (None | PlayerHistory): An object containing past rounds' actions and payoffs.
                                            It is None for the first round (state.round_number == 0).

        Returns:
            Action: The chosen action, either Action.C (Cooperate) or Action.D (Defect).
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        cooperation_threshold_count = math.ceil(self.n_players / 2)
        my_prev_action_was_C = history.my_actions[-1]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = opponent_cooperators_prev_round + (1 if my_prev_action_was_C else 0)
        if total_cooperators_prev_round >= cooperation_threshold_count:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_163(BaseStrategy):
    """
    K-Conditional Cooperation (KCC) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain collective cooperation by starting with cooperation,
    adapting behavior based on a dynamically calculated threshold of observed cooperators
    from the previous round, and defecting in the final round to avoid exploitation.
    The threshold is crucially dependent on the game's multiplication factor 'k'.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        total_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)
        if total_cooperators_prev_round >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_164(BaseStrategy):
    """
    The Adaptive Collective Threshold (ACT) strategy for the N-Player Public Goods Game.

    This strategy aims to promote cooperation by starting cooperatively and
    then conditionally cooperating based on the observed collective cooperation
    rate from the previous round. It defects in the final round as per backward
    induction and also defects in intermediate rounds if the collective cooperation
    rate falls below a 50% threshold, protecting against exploitation and signalling
    a need for increased collective effort.
    """
    COOPERATION_THRESHOLD = 0.5

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.n_rounds - 1:
            return Action.D
        else:
            previous_round_index = state.round_number - 1
            my_previous_action_was_cooperate = history.my_actions[previous_round_index]
            opponent_cooperators_prev_round = sum(history.opponent_actions[previous_round_index, :])
            total_cooperators_prev_round = my_previous_action_was_cooperate + opponent_cooperators_prev_round
            cooperation_rate = total_cooperators_prev_round / self.n_players
            if cooperation_rate >= self.COOPERATION_THRESHOLD:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_165(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to establish and sustain collective cooperation by:
    1. Initiating with cooperation in Round 1 (initial probe).
    2. In subsequent rounds (2 to r-1), adapting its behavior based on the
       observed collective cooperation level from the immediately preceding round.
       It cooperates if the total number of cooperators was at least `ceil(n/2)` (a "majority" threshold),
       and defects otherwise, to reward sufficient group cooperation and punish insufficient effort.
    3. Defecting in the final round (Round r) due to end-game effects.

    This adaptive approach seeks to foster cooperation while protecting against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / 2.0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_166(BaseStrategy):
    """
    Adaptive Collective Reciprocity with End-Game Logic:

    This strategy aims to foster and maintain cooperation by starting with a cooperative move.
    In intermediate rounds, it conditionally cooperates if the total number of cooperators
    in the preceding round meets or exceeds a dynamically calculated threshold (ceil(k)).
    Otherwise, it defects to punish insufficient collective effort.
    Crucially, it defects in the final round to prevent exploitation, aligning with rational
    end-game behavior in finitely repeated games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_c = math.ceil(self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        c_count_previous = int(history.my_actions[state.round_number - 1]) + int(np.sum(history.opponent_actions[state.round_number - 1, :]))
        if c_count_previous >= self.T_c:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_167(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy starts by cooperating to foster collective action. It then makes
    subsequent cooperation conditional on a dynamically adjusted "cooperation target"
    for the group, observed from the previous round. If the group fails to meet
    this target, the strategy lowers its expectation (the target threshold)
    gradually, demonstrating adaptability. It defects in the final round to
    prevent exploitation, consistent with backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_target_threshold: int = self.n_players
        self.delta: int = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == self.n_rounds - 1:
            return Action.D
        previous_round_idx = current_round_idx - 1
        num_cooperators_previous_round = np.sum(history.opponent_actions[previous_round_idx, :]) + int(history.my_actions[previous_round_idx])
        if num_cooperators_previous_round < self.cooperation_target_threshold:
            self.cooperation_target_threshold = max(1, self.cooperation_target_threshold - self.delta)
        if num_cooperators_previous_round >= self.cooperation_target_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_168(BaseStrategy):
    """
    Adaptive Collective Conditional Cooperation (ACCC) strategy for the N-Player Public Goods Game.

    This strategy aims to initiate cooperation, sustain it based on observed collective effort,
    and protect itself from exploitation when cooperation falters or in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_t == 1:
            return Action.C
        if current_round_t == total_rounds:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        if history.my_actions[-1]:
            num_cooperators_prev_round += 1
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_169(BaseStrategy):
    """
    Collective Enforcement Grim Trigger (CEGT) for N-Player Public Goods Game.

    This strategy aims to achieve and maintain full cooperation among all players,
    thereby maximizing the collective payoff. It starts by cooperating in Round 1
    to signal an intent for mutual cooperation. For subsequent intermediate rounds,
    it observes the outcome of the previous round. If *any* defection was observed
    in a previous round (i.e., not all players cooperated), the strategy
    permanently switches to punishment mode (defecting in all future intermediate
    rounds). If full cooperation has been maintained, it continues to cooperate.
    In the final round, it always defects, consistent with backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_mode_active = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        n_rounds_total = self.game_description.n_rounds
        n_players_total = self.game_description.n_players
        if current_round_zero_indexed == 0:
            return Action.C
        if current_round_zero_indexed == n_rounds_total - 1:
            return Action.D
        prev_round_my_action = history.my_actions[-1]
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        total_cooperators_prev_round = int(prev_round_my_action) + sum(prev_round_opponent_actions)
        if not self.punishment_mode_active:
            if total_cooperators_prev_round < n_players_total:
                self.punishment_mode_active = True
        if self.punishment_mode_active:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_170(BaseStrategy):
    """
    Adaptive Reciprocity with Forgiveness (ARF) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation by signaling willingness to cooperate,
    rewarding sufficient collective cooperation, and punishing defection to deter free-riders.
    It balances the collective good with self-preservation, adapting its behavior based on observed
    cooperation levels and game parameters.

    The strategy implements the following logic:
    - **Round 1 (0-indexed round 0):** Always Cooperate (C) to signal intent.
    - **Last Round (0-indexed round r-1):** Always Defect (D) to prevent exploitation due to endgame effects.
    - **Mid-Game Rounds:** Observes the proportion of cooperators in the previous round (`C_level_prev`).
      - If `C_level_prev` meets or exceeds a calculated `Cooperation_Tolerance_Threshold (T_C)`, it Cooperates (C).
      - Otherwise, it Defects (D) to punish insufficient cooperation.
    The `T_C` parameter adjusts dynamically based on `k` and `n`, making the strategy more or less
    forgiving depending on the potential collective gains.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        self.T_C = (k - 1) / (n - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == r - 1:
            return Action.D
        prev_round_idx = current_round_0_indexed - 1
        my_action_prev_round = history.my_actions[prev_round_idx]
        opponent_actions_prev_round = history.opponent_actions[prev_round_idx, :]
        num_cooperators_prev_round = (1 if my_action_prev_round else 0) + np.sum(opponent_actions_prev_round)
        cooperation_level_prev_round = num_cooperators_prev_round / n
        if cooperation_level_prev_round >= self.T_C:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_171(BaseStrategy):
    """
    The Collective Reciprocator strategy aims to foster and sustain cooperation in the N-Player
    Public Goods Game. It initiates cooperation to signal willingness to collaborate, then adaptively
    responds to the observed level of collective contribution. It uses a dynamic threshold based
    on game parameters (n, k) to determine if enough players are cooperating to warrant continued
    individual contribution. In the final round, it defects to prevent exploitation due to endgame
    rationality.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Collective Reciprocator strategy.

        Args:
            game_description: An object containing game parameters such as n_players, n_rounds, and k.
        """
        self.game_description = game_description
        n = game_description.n_players
        k = game_description.k
        self.cooperation_threshold_num_players = math.ceil(n / k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action for the current round based on the strategy's rules.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs for the player and opponents,
                     or None if it's the very first decision round.

        Returns:
            An Action (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            opponent_cooperators_last_round = np.sum(history.opponent_actions[state.round_number - 1, :])
            my_action_last_round_is_cooperate = history.my_actions[state.round_number - 1]
            num_total_cooperators_last_round = opponent_cooperators_last_round + int(my_action_last_round_is_cooperate)
            if num_total_cooperators_last_round >= self.cooperation_threshold_num_players:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_172(BaseStrategy):
    """
    Graduated Reciprocity for Collective Good (GRCG) strategy for N-Player Public Goods Game.
    This strategy aims to foster and maintain widespread cooperation by starting cooperatively,
    and then adaptively responding to collective cooperation levels. It uses a tiered
    response system: continued cooperation for high rates, a single-round 'warning shot'
    defection for moderate rates, and a multi-round punishment phase for low rates. It
    includes forgiveness after punishment and explicitly defects in the final round (end-game).
    """
    COOPERATION_ZONE_THRESHOLD = 0.75
    PUNISHMENT_ZONE_THRESHOLD = 0.5
    DEFAULT_PUNISHMENT_DURATION = 2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_rounds_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_1_indexed == total_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            self.punishment_rounds_remaining = 0
            return Action.C
        prev_round_my_action_bool = history.my_actions[-1]
        prev_round_opponent_actions_bool_array = history.opponent_actions[-1, :]
        c_count_prev = np.sum(prev_round_opponent_actions_bool_array) + int(prev_round_my_action_bool)
        c_ratio_prev = c_count_prev / n_players
        if self.punishment_rounds_remaining > 0:
            self.punishment_rounds_remaining -= 1
            return Action.D
        if c_ratio_prev >= self.COOPERATION_ZONE_THRESHOLD:
            return Action.C
        elif c_ratio_prev >= self.PUNISHMENT_ZONE_THRESHOLD:
            return Action.D
        else:
            my_action = Action.D
            rounds_available_for_punishment_phases = total_rounds - current_round_1_indexed - 1
            if rounds_available_for_punishment_phases >= 1:
                self.punishment_rounds_remaining = min(self.DEFAULT_PUNISHMENT_DURATION, rounds_available_for_punishment_phases)
            else:
                self.punishment_rounds_remaining = 0
            return my_action

class Strategy_COLLECTIVE_173(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for N-Player Public Goods Game.

    This strategy aims to foster cooperation by:
    1.  Starting with cooperation in the first round to signal willingness.
    2.  In intermediate rounds, adapting its action based on the observed
        collective cooperation level from the immediate previous round. A dynamically
        adjusted threshold, `T = ceil(n - k + 1)`, determines the minimum
        number of cooperators required to continue cooperating. This threshold
        is stricter when the public good is less beneficial (low k) and
        more forgiving when it's highly beneficial (high k).
    3.  Defecting in the final round, based on backward induction, as there are
        no future interactions to influence.

    It focuses on the overall group's behavior rather than individual player histories
    to encourage collective welfare and prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_T = math.ceil(self.n - self.k + 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        num_cooperators_previous_round = sum(history.opponent_actions[current_round_0_indexed - 1, :]) + int(history.my_actions[current_round_0_indexed - 1])
        if num_cooperators_previous_round >= self.cooperation_threshold_T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_174(BaseStrategy):
    """
    The Collective Reciprocator (CR) strategy aims to promote cooperation in the N-Player
    Public Goods Game by initiating cooperation, then conditionally reciprocating based on
    the collective behavior of other players, and defecting in the final round.

    Core principles:
    1.  Proactive Cooperation: Always cooperates in the first round to signal intent.
    2.  Conditional Reciprocity: In intermediate rounds, cooperates if a strict majority
        of *other* players cooperated in the previous round, otherwise defects.
    3.  Rational Endgame: Always defects in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        self.T_others = math.floor((n - 1) / 2) + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds_1_indexed = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds_1_indexed - 1:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
        cooperators_in_previous_round_others = sum(previous_round_opponent_actions)
        if cooperators_in_previous_round_others >= self.T_others:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_175(BaseStrategy):
    """
    Collective Contribution Threshold (CCT) strategy for the N-Player Public Goods Game.
    This strategy starts by cooperating to initiate a cooperative environment.
    In intermediate rounds, it assesses the total number of cooperators from the
    previous round. If this count meets or exceeds a dynamically calculated threshold
    (ceil(n / k)), it cooperates. Otherwise, it defects to punish insufficient
    collective contribution. In the final round, it defects to protect against
    exploitation in the end-game scenario.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.dynamic_cooperation_threshold = math.ceil(self.n_players / self.k)
        self.dynamic_cooperation_threshold = max(1, min(self.n_players, self.dynamic_cooperation_threshold))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        cooperators_from_opponents = np.sum(history.opponent_actions[-1, :])
        my_previous_action_was_cooperate = int(history.my_actions[-1])
        total_cooperators_previous_round = cooperators_from_opponents + my_previous_action_was_cooperate
        if total_cooperators_previous_round >= self.dynamic_cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_176(BaseStrategy):
    """
    Adaptive Majority Reciprocity strategy for N-Player Public Goods Game.
    This strategy starts by cooperating to signal a willingness to collaborate.
    It then adjusts its future actions based on the observed level of cooperation in the group,
    becoming more demanding when collective benefits are low (low 'k') or as the game
    approaches its end (high 'r'). It aims to be forgiving enough to establish cooperation
    but strict enough to deter sustained free-riding.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        C_prev = np.sum(history.opponent_actions[-1, :])
        T_base = math.ceil(self.n / self.k)
        T_base = max(1, T_base)
        S_t = current_round_0_indexed / (self.r - 1)
        Threshold_t = round(T_base + (self.n - T_base) * S_t)
        Threshold_t = max(1, min(self.n, Threshold_t))
        if C_prev >= Threshold_t:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_177(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to promote and sustain cooperation by initiating cooperatively
    and then adapting to the observed collective behavior. It maintains cooperation
    as long as a significant portion of the group contributes, but defects to punish
    widespread defection and protect itself. It also accounts for the endgame effect
    in finitely repeated games by defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = 0
        if history.my_actions[-1] == True:
            num_cooperators_prev_round += 1
        num_cooperators_prev_round += np.sum(history.opponent_actions[-1, :])
        cooperation_threshold = math.floor(self.n_players / 2)
        if num_cooperators_prev_round >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_178(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy.

    This strategy aims to foster cooperation through initial trust and conditional reciprocity,
    while protecting itself against exploitation, especially in the endgame. It leverages
    the observable history of collective actions to adapt its own behavior.

    Decision Rules:
    1. Round 1 (Initial Trust): Cooperate (C).
    2. Intermediate Rounds (2 to r - EndgameHorizon):
       - Observe cooperation rate in the previous round.
       - If cooperation_rate_prev_round >= C_Threshold: Cooperate (C).
       - Else: Defect (D).
    3. Endgame Phase (r - EndgameHorizon + 1 to r): Defect (D).

    Parameters:
    - C_Threshold: 0.5 (minimum proportion of cooperators required in the previous round)
    - EndgameHorizon: 1 (number of rounds before the very end to switch to guaranteed defection)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.C_Threshold = 0.5
        self.EndgameHorizon = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds_count = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed >= total_rounds_count - self.EndgameHorizon:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]).item()
        if history.my_actions[-1]:
            num_cooperators_prev_round += 1
        cooperation_rate_prev_round = num_cooperators_prev_round / n_players
        if cooperation_rate_prev_round >= self.C_Threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_179(BaseStrategy):
    """
    Collective Reciprocator with End-Game Defection strategy for the N-Player Public Goods Game.
    This strategy starts by cooperating, then conditionally cooperates in intermediate rounds
    if the total number of cooperators in the previous round meets a calculated threshold
    (ceil(n/k)). It defects unconditionally in the final round to maximize individual payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number_1_indexed = state.round_number + 1
        if current_round_number_1_indexed == 1:
            return Action.C
        elif current_round_number_1_indexed == self.n_rounds:
            return Action.D
        else:
            cooperators_from_opponents_prev_round = np.sum(history.opponent_actions[-1, :])
            my_action_prev_round_is_C = int(history.my_actions[-1])
            C_previous_round = cooperators_from_opponents_prev_round + my_action_prev_round_is_C
            if C_previous_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_180(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for N-Player Public Goods Game.

    This strategy aims to foster cooperation by reciprocating observed cooperation,
    while also protecting against exploitation and adapting its threshold based
    on the inherent generosity of the public good (determined by 'k').

    - Cooperates in the initial round to signal willingness for collective action.
    - Defects in the final round to avoid exploitation (standard game theory result).
    - In intermediate rounds, cooperates if the observed proportion of cooperators
      in the previous round meets a dynamically adjusted threshold. This threshold
      is stricter when 'k' is low (less collective benefit) and more lenient
      when 'k' is high (significant collective benefit).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        raw_threshold = (self.n_players - self.k) / (self.n_players - 1)
        self.cooperation_threshold_ratio = max(0.0, min(1.0, raw_threshold))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            my_action_prev_round_is_C = history.my_actions[-1]
            C_prev = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_is_C else 0)
            actual_cooperation_ratio = C_prev / self.n_players
            if actual_cooperation_ratio >= self.cooperation_threshold_ratio:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_181(BaseStrategy):
    """
    Adaptive Proportional Reciprocator (APR) strategy for the N-Player Public Goods Game.
    The APR strategy aims to foster and sustain cooperation within the group by being
    initially cooperative, rewarding sufficient past cooperation, and punishing
    insufficient cooperation. It uses an adaptive threshold for collective cooperation,
    sensitive to the game's efficiency parameter (k) and the number of players (n).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[state.round_number - 1, :]) + history.my_actions[state.round_number - 1]
        tolerance_factor = (self.k_factor - 1) / (self.n_players - 1)
        T_num_coop = round(self.n_players * (1 - tolerance_factor / 2))
        T_num_coop = max(1, min(self.n_players, T_num_coop))
        if num_cooperators_prev_round >= T_num_coop:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_182(BaseStrategy):
    """
    The Adaptive Forgiving Reciprocity (AFR) strategy aims to foster and maintain group
    cooperation while protecting the individual player from exploitation. It is based on
    conditional cooperation, initiating with a cooperative move and then adapting its
    behavior in subsequent rounds based on the observed level of group cooperation in
    the previous round and its own last action. It employs two distinct thresholds:
    a lower threshold to trigger defection when cooperation falters (punishment) and a
    higher threshold to resume cooperation after a period of defection (forgiveness),
    thereby incentivizing robust collective action.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.THRESHOLD_PUNISH = math.ceil(self.n * 0.5)
        self.THRESHOLD_FORGIVE = math.ceil(self.n * 0.75)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        my_action_prev = history.my_actions[-1]
        cooperators_from_opponents = np.sum(history.opponent_actions[-1, :])
        C_prev = cooperators_from_opponents + (1 if my_action_prev else 0)
        if my_action_prev == True:
            if C_prev < self.THRESHOLD_PUNISH:
                return Action.D
            else:
                return Action.C
        elif C_prev >= self.THRESHOLD_FORGIVE:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_183(BaseStrategy):
    """
    Adaptive Reciprocity with Dynamic Threshold strategy for the N-Player Public Goods Game.
    This strategy aims to establish and maintain cooperation by starting cooperatively and then
    adjusting its behavior based on the observed collective cooperation level in previous rounds.
    It incorporates a dynamic threshold that adapts to the efficiency of the public good (k)
    and applies backward induction for the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.T_k = 1 - (self.k_factor - 1) / (self.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        cooperators_from_opponents_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        my_action_prev_round_is_C = history.my_actions[prev_round_idx]
        C_prev = cooperators_from_opponents_prev_round + (1 if my_action_prev_round_is_C else 0)
        P_coop = C_prev / self.n_players
        if P_coop >= self.T_k:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_184(BaseStrategy):
    """
    The "Collective Monitor with Forgiveness" strategy aims to foster collective cooperation
    in the N-Player Public Goods Game. It initiates cooperation in the first round and
    maintains it as long as the number of cooperators in the previous round meets a
    dynamically calculated threshold (ceil(n/k)). If cooperation falls below this threshold,
    the strategy defects as a punishment. It shows forgiveness by returning to cooperation
    once the collective contribution level recovers. In the final round, it rationally
    defects due to the finite horizon of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        cooperation_threshold = math.ceil(self.n / self.k)
        if num_cooperators_prev_round >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_185(BaseStrategy):
    """
    The Adaptive Majority Cooperator (AMC) strategy aims to foster and sustain collective
    cooperation. It initiates with cooperation in Round 1. In subsequent mid-game rounds,
    it cooperates if a majority (or near-majority) of all players cooperated in the
    immediately preceding round, otherwise it defects. In the final round, it always defects
    to prevent exploitation, aligning with the backward induction principle.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        previous_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = previous_round_opponent_cooperators + (1 if history.my_actions[-1] else 0)
        cooperation_threshold = math.ceil(self.n_players / 2)
        if total_cooperators_prev_round >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_186(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) Strategy for N-Player Public Goods Game.

    ACR starts by cooperating to signal trust. In subsequent rounds, it monitors the
    overall level of cooperation. If the total number of cooperators in the previous
    round was at least `ceil(n / 2.0)`, ACR continues to cooperate. Otherwise, it
    defects to penalize free-riders and prevent exploitation. In the final round,
    ACR defects, recognizing the "endgame effect" where future interactions no longer
    matter.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.C_threshold = math.ceil(self.n_players / 2.0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        opponents_cooperated_prev = sum(history.opponent_actions[state.round_number - 1, :])
        my_action_prev_was_C = history.my_actions[state.round_number - 1]
        C_prev = opponents_cooperated_prev + my_action_prev_was_C
        if C_prev >= self.C_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_187(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain cooperation by starting optimistically,
    conditioning cooperation on a sufficient collective contribution, and defecting
    when cooperation falls below a critical threshold to incentivize greater effort,
    while also protecting against exploitation in the final round.

    Core Principles:
    1.  Optimistic Initiation: Starts by cooperating in the first round.
    2.  Conditional Cooperation: Continues cooperating if the number of cooperators
        in the previous round met or exceeded a predefined threshold.
    3.  Proportional Response: Defects if cooperation in the previous round fell
        below the threshold, signaling dissatisfaction and incentivizing more cooperation.
    4.  Rational End-Game: Defects in the final round to prevent exploitation,
        anticipating the unraveling effect.

    The critical threshold for cooperation, MIN_COOPERATORS_THRESHOLD, is derived
    from game parameters n and k, representing the minimum number of cooperators
    required for contributors to receive a payoff of at least 1 from the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.MIN_COOPERATORS_THRESHOLD = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_number == 0:
            return Action.C
        elif current_round_number == total_rounds - 1:
            return Action.D
        else:
            num_opponent_cooperators_in_previous_round = sum(history.opponent_actions[-1, :])
            my_action_in_previous_round = history.my_actions[-1]
            total_cooperators_in_previous_round = num_opponent_cooperators_in_previous_round + my_action_in_previous_round
            if total_cooperators_in_previous_round >= self.MIN_COOPERATORS_THRESHOLD:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_188(BaseStrategy):
    """
    The Adaptive Majority Cooperator (AMC) strategy aims to foster and maintain a high level of cooperation
    within the group by initiating cooperation, rewarding sustained group cooperation, and punishing
    collective defection. It balances the collective good with individual robustness in a competitive environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold_count = math.floor(self.n_players / 2) + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == self.n_rounds:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev_round >= self.cooperation_threshold_count:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_189(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy.

    ACR starts by cooperating to signal willingness for collective good. It
    continuously monitors the overall level of cooperation in the group. If the
    collective cooperation falls below a critical threshold (calculated as
    ceil(N/K)), ACR initiates a temporary punishment phase (defection) for a
    fixed duration to signal dissatisfaction and incentivize a return to
    cooperation. After punishment, it re-initiates cooperation, offering a
    chance for the group to rebuild. In the final round, it defects due to the
    absence of future interactions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_failure_threshold = math.ceil(self.n / self.k)
        self.punishment_duration = 2
        self.current_mode = 'COOPERATE'
        self.punishment_rounds_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.r:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        my_prev_action_was_cooperate = history.my_actions[-1]
        opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = opponent_cooperators_prev_round + (1 if my_prev_action_was_cooperate else 0)
        if self.current_mode == 'PUNISH':
            if self.punishment_rounds_remaining > 0:
                self.punishment_rounds_remaining -= 1
                return Action.D
            else:
                self.current_mode = 'COOPERATE'
                return Action.C
        elif self.current_mode == 'COOPERATE':
            if total_cooperators_prev_round < self.cooperation_failure_threshold:
                self.current_mode = 'PUNISH'
                self.punishment_rounds_remaining = self.punishment_duration - 1
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_190(BaseStrategy):
    """
    Adaptive Proportional Cooperation (APC) strategy for the N-Player Public Goods Game.

    This strategy initiates cooperation in the first round to signal willingness to collaborate.
    In intermediate rounds, it dynamically adjusts its behavior based on the observed
    proportion of cooperators in the previous round. If this proportion meets or exceeds
    a predefined threshold (1/k), it continues to cooperate, rewarding collective effort.
    Otherwise, it defects to avoid exploitation, acting as a collective punishment mechanism.
    In the final round, it always defects, following the principle of backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_T = 1.0 / self.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            num_cooperators_prev_round = history.my_actions[-1].item() + np.sum(history.opponent_actions[-1, :])
            proportion_cooperated_prev_round = num_cooperators_prev_round / self.n_players
            if proportion_cooperated_prev_round >= self.cooperation_threshold_T:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_191(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster and sustain cooperation by starting cooperatively, then
    reciprocating the observed level of cooperation from other players. It sets a
    clear threshold for what constitutes sufficient collective engagement to warrant
    continued individual contribution. It adapts its tolerance based on the
    efficiency of the public good (k) and protects itself from exploitation in the
    final round.

    Decision Rules:
    1.  Round 1 (state.round_number == 0): Cooperate (C) to initiate.
    2.  Rounds 2 to r-1 (1 <= state.round_number <= n_rounds - 2):
        *   Calculate C_prev: total cooperators (self + opponents) in the previous round.
        *   If C_prev >= C_threshold: Cooperate (C).
        *   Else: Defect (D).
    3.  Round r (state.round_number == n_rounds - 1): Defect (D) to avoid exploitation.

    C_threshold is calculated as math.ceil(n / k) in the constructor, representing
    the minimum number of cooperators in the previous round required to justify
    continued cooperation. This threshold ensures collective benefit aligns with
    individual contribution.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.C_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            C_prev = np.sum(history.opponent_actions[-1]) + int(history.my_actions[-1])
            if C_prev >= self.C_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_192(BaseStrategy):
    """
    Dynamic Collective Reciprocity (DCR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by starting with initial trust, adapting to observed
    collective cooperation rates, implementing punishment for insufficient contributions, and
    periodically offering forgiveness to prevent permanent defection cycles. It concludes
    with defection in the final round to maximize individual payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        n_val = float(self.n_players)
        k_val = self.k_factor
        calculated_ratio = n_val * (k_val - 1.0) / (k_val * (n_val - 1.0))
        epsilon = 1e-09
        self.optimal_c_ratio = max(epsilon, min(1.0 - epsilon, calculated_ratio))
        self.rounds_since_last_cooperation_attempt = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.rounds_since_last_cooperation_attempt = 0
            return Action.C
        elif state.round_number == self.n_rounds - 1:
            return Action.D
        else:
            my_prev_action = history.my_actions[-1]
            opponent_prev_actions = history.opponent_actions[-1, :]
            c_prev = sum(opponent_prev_actions) + (1 if my_prev_action else 0)
            cooperation_rate_prev = c_prev / float(self.n_players)
            if cooperation_rate_prev >= self.optimal_c_ratio:
                self.rounds_since_last_cooperation_attempt = 0
                return Action.C
            elif self.rounds_since_last_cooperation_attempt >= 3:
                self.rounds_since_last_cooperation_attempt = 0
                return Action.C
            else:
                self.rounds_since_last_cooperation_attempt += 1
                return Action.D

class Strategy_COLLECTIVE_193(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.
    This strategy adapts cooperation based on observed collective behavior,
    aiming to foster cooperation while safeguarding against exploitation.
    It initiates with cooperation, maintains it if a calculated threshold of collective
    cooperation is met in intermediate rounds, and adopts a prudent exit in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.T_C_num = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        prev_round_idx = current_round_0_indexed - 1
        cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        cooperators_prev_round += int(history.my_actions[prev_round_idx])
        if current_round_0_indexed == self.n_rounds - 1:
            if cooperators_prev_round == self.n_players:
                return Action.C
            else:
                return Action.D
        elif cooperators_prev_round >= self.T_C_num:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_194(BaseStrategy):
    """
    The Collective Threshold Reciprocator (CTR) strategy for the N-Player Public Goods Game.
    This strategy initiates cooperation in the first round. In subsequent intermediate rounds,
    it observes the total number of cooperators from the previous round. If this count
    meets or exceeds a dynamically calculated minimum threshold (ceil(n/k)), it continues
    to cooperate. Otherwise, it defects to avoid exploitation and collectively punish
    insufficient participation. In the final round, it always defects, recognizing the
    finite horizon of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.N_min_C_threshold = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.r - 1:
            return Action.D
        else:
            previous_round_idx = current_round_0_indexed - 1
            N_C_prev_round = history.my_actions[previous_round_idx] + np.sum(history.opponent_actions[previous_round_idx, :])
            if N_C_prev_round >= self.N_min_C_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_195(BaseStrategy):
    """
    Collective Reciprocity with Threshold (CRT) strategy for the N-Player Public Goods Game.

    This strategy aims to establish and maintain cooperation by rewarding collective cooperation
    and punishing collective defection. It starts with cooperation, defects in the final round,
    and in intermediate rounds, decides based on whether the number of cooperators in the
    previous round met a dynamically calculated threshold (ceil(n/k)).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_m = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        previous_round_index = state.round_number - 1
        my_action_prev_round_was_C = history.my_actions[previous_round_index]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_index, :])
        num_cooperators_in_previous_round = opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
        if num_cooperators_in_previous_round >= self.cooperation_threshold_m:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_196(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) for the N-Player Public Goods Game.
    This strategy aims to foster and maintain collective cooperation by starting
    optimistically, reciprocating majority cooperation, adapting punishment for
    insufficient cooperation, forgiving after a set period, and rationally
    defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.punish_counter = 0
        self.COOPERATION_THRESHOLD_NUM = math.ceil(self.n / 2)
        self.PUNISHMENT_DURATION = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_1_indexed_round = state.round_number + 1
        if current_1_indexed_round == 1:
            self.punish_counter = 0
            return Action.C
        if current_1_indexed_round == self.r:
            return Action.D
        else:
            total_cooperators_prev_round = sum(history.opponent_actions[state.round_number - 1, :])
            if history.my_actions[state.round_number - 1]:
                total_cooperators_prev_round += 1
            c_prev = total_cooperators_prev_round
            if c_prev >= self.COOPERATION_THRESHOLD_NUM:
                self.punish_counter = 0
                return Action.C
            else:
                self.punish_counter += 1
                if self.punish_counter <= self.PUNISHMENT_DURATION:
                    return Action.D
                else:
                    self.punish_counter = 0
                    return Action.C

class Strategy_COLLECTIVE_197(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACRS) strategy for the N-Player Public Goods Game.
    This strategy aims to initiate and sustain a high level of collective cooperation
    by making its own cooperation conditional on the observed cooperation rate of the group
    in the previous round. It encourages participation by rewarding sufficient collective
    effort and discourages free-riding by defecting when the collective effort falls
    below a critical threshold. It also accounts for the game's finite horizon.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T_coop = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.r:
            return Action.D
        else:
            num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            cooperation_rate_prev_round = num_cooperators_prev_round / self.n
            if cooperation_rate_prev_round >= self.T_coop:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_198(BaseStrategy):
    """
    Adaptive Reciprocal Forgiving Cooperator (ARFC) strategy.

    ARFC is designed to foster collective cooperation while being robust,
    adaptive, and resilient in a tournament setting against a range of unknown
    opponent behaviors. It starts with an act of trust, monitors the
    collective's performance, and adjusts its own behavior to reward sufficient
    cooperation or punish widespread defection. Crucially, it incorporates a
    parameter-dependent threshold (k) to determine "sufficient" cooperation
    and accounts for the end-game effect in finitely repeated games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        else:
            my_prev_action_was_C = history.my_actions[current_round_0_indexed - 1]
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
            num_cooperators_prev_round = (1 if my_prev_action_was_C else 0) + num_opponent_cooperators_prev_round
            if num_cooperators_prev_round >= self.k:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_199(BaseStrategy):
    """
    Adaptive Reciprocal Contribution (ARC) strategy for the N-Player Public Goods Game.

    This strategy initiates cooperation, then dynamically adjusts its future actions
    based on the observed collective cooperation in the previous round, leveraging
    a Minimum Impact Threshold (T_MIT) derived from game parameters n and k.
    It strategically defects in the final round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ARC strategy with game parameters.

        Args:
            game_description (PublicGoodsDescription): An object containing
                                                      n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.T_MIT = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): Contains the current round number (0-indexed).
            history (None | PlayerHistory): Past actions and payoffs for all players.
                                            None for the very first round.

        Returns:
            Action: The chosen action for the current round (Action.C or Action.D).
        """
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        else:
            previous_round_idx = state.round_number - 1
            my_prev_action_contribution = int(history.my_actions[previous_round_idx])
            opponent_prev_cooperation_sum = sum(history.opponent_actions[previous_round_idx])
            num_cooperators_previous_round = my_prev_action_contribution + opponent_prev_cooperation_sum
            if num_cooperators_previous_round >= self.T_MIT:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_200(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy fosters collective cooperation while protecting against exploitation.
    It is adaptive and robust, explicitly aligning with a pragmatic collective mindset by
    responding to observed group behavior and game parameters.

    Core Principles:
    1.  Initial Trust & Exploration: Start by cooperating to signal willingness.
    2.  Conditional Cooperation: Base subsequent actions on the observed level of collective
        contribution in the previous round, using a carefully chosen threshold (T).
    3.  Self-Protection & Signalling: Defect when collective contribution falls below T,
        to protect individual payoff and signal the need for greater collective effort.
    4.  Endgame Rationality: Account for the finite nature of the game by defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        my_action_prev_round_was_C = history.my_actions[-1]
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
        if total_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_201(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and sustain cooperation by being initially generous,
    adaptively responsive to group behavior, and strategically forgiving, while
    accounting for the end-game effect in finitely repeated games.

    It operates on the principle of initiating cooperation, adaptively
    reciprocating observed collective behavior, and strategically forgiving
    past defections to re-establish cooperation.
    """
    _MIN_COOPERATION_PROPORTION_TO_SUSTAIN = 0.5
    _FORGIVENESS_ROUNDS_THRESHOLD = 2

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Collective Reciprocity (ACR) strategy.

        Args:
            game_description (PublicGoodsDescription): An object containing
                                                       game parameters like
                                                       n_players, n_rounds, k.
        """
        self.game_description = game_description
        self._my_current_stance = 'COOPERATING'
        self._defection_rounds_sustained_for = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on
        the Adaptive Collective Reciprocity strategy's rules.

        Args:
            state (GameState): Current state of the game, including the round number.
            history (None | PlayerHistory): Past actions and payoffs of this player
                                            and opponents. Is None for the very first round.

        Returns:
            Action: The chosen action for the current round (Action.C for Cooperate,
                    Action.D for Defect).
        """
        n_players = self.game_description.n_players
        total_rounds = self.game_description.n_rounds
        current_round = state.round_number
        if current_round == 0:
            self._my_current_stance = 'COOPERATING'
            self._defection_rounds_sustained_for = 0
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        c_count_prev = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        cooperation_proportion_prev = c_count_prev / n_players
        if self._my_current_stance == 'COOPERATING':
            if cooperation_proportion_prev < self._MIN_COOPERATION_PROPORTION_TO_SUSTAIN:
                self._my_current_stance = 'DEFECTING'
                self._defection_rounds_sustained_for = 1
                return Action.D
            else:
                self._defection_rounds_sustained_for = 0
                return Action.C
        else:
            self._defection_rounds_sustained_for += 1
            if cooperation_proportion_prev >= self._MIN_COOPERATION_PROPORTION_TO_SUSTAIN:
                if self._defection_rounds_sustained_for >= self._FORGIVENESS_ROUNDS_THRESHOLD:
                    self._my_current_stance = 'COOPERATING'
                    self._defection_rounds_sustained_for = 0
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D

class Strategy_COLLECTIVE_202(BaseStrategy):
    """
    Adaptive Reciprocity with Forgiveness (ARF) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain a high level of cooperation among players,
    while being robust against exploitation and acknowledging rational end-game dynamics.
    It initiates cooperation, punishes insufficient cooperation, and rewards sustained
    group effort.
    """
    FORGIVENESS_FACTOR: float = 0.1
    END_GAME_ROUNDS: int = 1

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        if self.n == 2:
            self.TARGET_COOPERATION_RATE = 1.0
        else:
            self.TARGET_COOPERATION_RATE = (self.n - 1) / float(self.n)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed > self.r - self.END_GAME_ROUNDS:
            return Action.D
        else:
            num_cooperators_last_round = 0
            if history.my_actions[-1]:
                num_cooperators_last_round += 1
            num_cooperators_last_round += np.sum(history.opponent_actions[-1, :])
            observed_cooperation_rate = float(num_cooperators_last_round) / self.n
            base_threshold = self.k / float(self.n)
            adaptive_threshold = max(0.0, min(1.0, self.TARGET_COOPERATION_RATE - self.FORGIVENESS_FACTOR))
            adaptive_threshold = max(adaptive_threshold, base_threshold)
            if observed_cooperation_rate >= adaptive_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_203(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by starting with a cooperative stance
    and then adapting its behavior based on the observed level of collective
    contribution in previous rounds. It uses a dynamic threshold based on game
    parameters `n` and `k` to determine collective efficiency. It incorporates
    a forgiveness mechanism to tolerate minor or temporary deviations from ideal
    cooperation, preventing premature breakdown. Finally, it applies end-game
    rationality in the final round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.COOP_THRESHOLD_MIN = math.ceil(self.game_description.n_players / self.game_description.k)
        self.FORGIVENESS_ROUNDS_COUNT = 1
        self.consecutive_rounds_below_threshold: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == self.game_description.n_rounds:
            return Action.D
        if state.round_number == 0:
            self.consecutive_rounds_below_threshold = 0
            return Action.C
        C_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if C_prev_round >= self.COOP_THRESHOLD_MIN:
            self.consecutive_rounds_below_threshold = 0
            return Action.C
        else:
            self.consecutive_rounds_below_threshold += 1
            if self.consecutive_rounds_below_threshold <= self.FORGIVENESS_ROUNDS_COUNT:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_204(BaseStrategy):
    """
    The Adaptive Collective Threshold (ACT) strategy fosters cooperation by initiating trust and then
    conditionally maintaining it based on the observed collective effort from the previous round.
    It cooperates in the first round to signal trust, defects in the last round due to end-game rationality,
    and in intermediate rounds, it cooperates only if a majority of players cooperated in the previous round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.collective_cooperation_threshold = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        cooperators_in_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if cooperators_in_prev_round >= self.collective_cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_205(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain cooperation within the group by being
    initially cooperative, then reciprocally cooperative based on the observed
    actions of others. It incorporates specific handling for the 2-player case
    (to prevent exploitation) and the final round (due to the end-game effect
    in repeated games). It prioritizes robust group-level cooperation while
    avoiding being an easily exploitable "sucker."

    Core Principles:
    1.  Be Nice: Start with cooperation.
    2.  Be Reciprocal: Match the observed level of cooperation from the group.
    3.  Be Forgiving (for n > 2): Tolerate some defection if a sufficient majority cooperates.
    4.  Be Punishing: Defect if cooperation drops below a critical threshold or if directly exploited (in n=2).
    5.  Be Rational (for the last round): Acknowledge the game-theoretic reality.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_1_indexed == n_rounds:
            return Action.D
        prev_round_idx = current_round_0_indexed - 1
        my_action_in_prev_round = history.my_actions[prev_round_idx]
        opponent_actions_in_prev_round = history.opponent_actions[prev_round_idx, :]
        cooperators_in_prev_round = int(my_action_in_prev_round) + np.sum(opponent_actions_in_prev_round)
        if n_players == 2:
            if cooperators_in_prev_round == 2:
                return Action.C
            elif cooperators_in_prev_round == 1:
                if my_action_in_prev_round == True:
                    return Action.D
                else:
                    return Action.C
            else:
                return Action.D
        else:
            threshold_cooperators = (n_players + 1) // 2
            if cooperators_in_prev_round >= threshold_cooperators:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_206(BaseStrategy):
    """
    Adaptive Reciprocal Cooperator (ARC) Strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by starting cooperatively and then
    adapting its behavior based on the observed level of collective cooperation
    in previous rounds. It employs a dynamic threshold for cooperation, derived
    from the game's parameters (n and k), and accounts for end-game rationality
    by defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ARC strategy with the game parameters.

        Args:
            game_description: An object containing n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate 'C' or Defect 'D') for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing the history of actions and payoffs for
                     this player and opponents in previous rounds. It is None
                     for the very first round.

        Returns:
            Action.C for Cooperate, Action.D for Defect.
        """
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == r - 1:
            return Action.D
        else:
            my_prev_action_was_C = history.my_actions[-1]
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            num_cooperators_previous_round = num_opponent_cooperators_prev_round + (1 if my_prev_action_was_C else 0)
            N_threshold = max(2, math.ceil(n - k + 1e-09))
            if num_cooperators_previous_round >= N_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_207(BaseStrategy):
    """
    Adaptive Collective Cooperator (ACC) strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by:
    1. Initiating with cooperation in the first round to signal trust and gather data.
    2. In middle rounds, conditionally cooperating based on a dynamic threshold (T_coop).
       It cooperates if the number of cooperators in the previous round was sufficient
       for individual contributors to receive at least the 'all defect' baseline payoff.
       It defects if cooperation falls below this threshold, acting as a signal for
       insufficient collective effort.
    3. Defecting in the final round due to backward induction, as there are no future
       interactions to influence.

    The strategy balances collective benefit with self-preservation, adapting to the
    group's observed behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        self.T_coop = math.ceil(n / k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_rounds:
            return Action.D
        else:
            C_prev = int(history.my_actions[-1]) + sum(history.opponent_actions[-1, :])
            if C_prev >= self.T_coop:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_208(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to initiate cooperation, sustain it by reciprocating sufficient
    collective cooperation, and punish periods of low cooperation. It dynamically
    adjusts its threshold for "sufficient cooperation" based on the public good's
    efficiency (parameter `k`), finding a balance between initiating cooperation
    and reacting to the collective behavior of the group.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.dynamic_threshold = math.ceil(self.n_players - (self.k_factor - 1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        cooperators_in_previous_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if cooperators_in_previous_round >= self.dynamic_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_209(BaseStrategy):
    """
    Adaptive Reciprocal Cooperator (ARC) strategy for the N-Player Public Goods Game.
    It initiates cooperation in the first round and defects in the final round.
    In intermediate rounds, it dynamically adjusts its behavior: it cooperates
    if the total number of cooperators in the previous round met or exceeded
    a calculated threshold (C_threshold = ceil(n/k)), otherwise it defects.
    This strategy aims to foster collective cooperation while protecting against
    exploitation and incentivizing reciprocity.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.c_threshold = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_one_indexed = state.round_number + 1
        total_rounds = self.n_rounds
        if current_round_one_indexed == 1:
            return Action.C
        elif current_round_one_indexed == total_rounds:
            return Action.D
        else:
            num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if num_cooperators_prev_round >= self.c_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_210(BaseStrategy):
    """
    This strategy promotes collective welfare by initiating cooperation and sustaining it as long as a
    critical mass of players also cooperates. It punishes widespread defection to prevent free-riding
    and ensures that cooperators are not exploited below a baseline individual payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round == 0:
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        my_prev_action_cooperated = int(history.my_actions[-1])
        opponent_prev_cooperators = np.sum(history.opponent_actions[-1, :])
        c_count_t_minus_1 = my_prev_action_cooperated + opponent_prev_cooperators
        if c_count_t_minus_1 >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_211(BaseStrategy):
    """
    The Collective Reciprocator strategy for the N-Player Public Goods Game.

    This strategy aims to establish and maintain collective cooperation by:
    1. Starting with an act of cooperation to signal goodwill.
    2. Reactively sustaining cooperation if the collective effort in the previous
       round was strong (at most one defector).
    3. Defecting to protect against exploitation if cooperation significantly
       breaks down (two or more defectors in the previous round).
    4. Defecting in the final round, recognizing the lack of future consequences.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters like
                              n_players and n_rounds.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current game state, including
                   the current round number (0-indexed).
            history: An object containing records of past actions and payoffs
                     for this player and opponents. Is None for the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        prev_round_index = current_round_0_indexed - 1
        my_prev_action_was_cooperate = history.my_actions[prev_round_index]
        opponent_prev_cooperators_count = np.sum(history.opponent_actions[prev_round_index, :])
        total_cooperators_prev_round = int(my_prev_action_was_cooperate) + int(opponent_prev_cooperators_count)
        if total_cooperators_prev_round >= self.n_players - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_212(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster cooperation by starting cooperatively and then
    dynamically adjusting its behavior based on the observed level of group cooperation
    in previous rounds, specifically using a "collectively efficient" threshold (ceil(n/k)).
    It defaults to defection in the final round, acknowledging the end-game dynamics
    of finite repeated games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == total_rounds - 1:
            return Action.D
        else:
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            my_action_prev_round = int(history.my_actions[-1])
            s_c_prev = opponent_cooperators_prev_round + my_action_prev_round
            if s_c_prev >= self.coop_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_213(BaseStrategy):
    """
    Adaptive Reciprocal Public Goods (ARPG) strategy for the N-Player Public Goods Game.

    This strategy initiates cooperation in the first round to signal willingness to contribute
    and attempt to build trust for collective benefit.
    In intermediate rounds, it adapts its decision (Cooperate or Defect) based on the
    observed level of cooperation in the immediately preceding round. It cooperates if
    the total number of cooperators in the previous round met or exceeded a critical
    threshold (`ceil(n/k)`), otherwise it defects to protect against exploitation.
    In the final round, it defects due to end-game dynamics and the absence of future
    consequences, aligning with individual rationality.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T_count = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        previous_round_index = state.round_number - 1
        my_prev_action_cooperated = int(history.my_actions[previous_round_index])
        opponent_prev_cooperators = np.sum(history.opponent_actions[previous_round_index, :])
        num_cooperators_prev_round = my_prev_action_cooperated + opponent_prev_cooperators
        if num_cooperators_prev_round >= self.T_count:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_214(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and maintain high levels of cooperation by initiating
    cooperation, dynamically responding to the group's collective effort using a
    calculated threshold, and strategically protecting itself against exploitation
    in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters and pre-calculates the
        dynamic cooperation threshold.
        """
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on
        the ACR strategy rules.
        """
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        my_prev_action_value = int(history.my_actions[-1])
        opponent_prev_cooperators = np.sum(history.opponent_actions[-1, :])
        cooperators_in_previous_round = my_prev_action_value + opponent_prev_cooperators
        if cooperators_in_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_215(BaseStrategy):
    """
    The Adaptive Collective Strategy with Dynamic Reciprocity (ACS-DR) aims to foster
    and sustain collective cooperation in the N-Player Public Goods Game.
    It initiates with cooperation and then dynamically adjusts its behavior based on
    the observed proportion of other players who cooperated in the previous round,
    using a pre-calculated dynamic threshold. It defects in the final round to
    avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_cooperation = 1 - (self.game_description.k - 1) / (self.game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == total_rounds - 1:
            return Action.D
        else:
            num_other_cooperators_prev = np.sum(history.opponent_actions[current_round_idx - 1, :])
            num_other_players = self.game_description.n_players - 1
            proportion_other_cooperators = num_other_cooperators_prev / num_other_players
            if proportion_other_cooperators >= self.T_cooperation:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_216(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain cooperation by adapting to the collective
    behavior of other players. It starts by cooperating to signal willingness,
    maintains cooperation as long as a sufficient proportion of the group also
    cooperates, retaliates by defecting if collective cooperation breaks down,
    and accounts for the finite nature of the game by defecting in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_prev_action_is_cooperate = int(history.my_actions[-1])
        opponent_prev_cooperators = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = my_prev_action_is_cooperate + opponent_prev_cooperators
        cooperation_threshold = self.game_description.n_players / 2.0
        if total_cooperators_prev_round >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_217(BaseStrategy):
    """
    The Adaptive Collective Cooperator (ACC) strategy aims to foster and sustain cooperation
    in the N-Player Public Goods Game. It starts by cooperating to signal willingness,
    adapts its behavior in intermediate rounds based on the observed level of collective
    cooperation, and defects in the final round to prevent exploitation.
    It uses a dynamically calculated cooperation threshold based on game parameters n and k
    to ensure collective contributions are "sufficient" to warrant continued individual cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_coop_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds_1_indexed = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_rounds_1_indexed:
            return Action.D
        else:
            my_action_prev_round_is_C = history.my_actions[state.round_number - 1]
            opponent_cooperators_prev_round_count = np.sum(history.opponent_actions[state.round_number - 1, :])
            num_cooperators_previous_round = int(my_action_prev_round_is_C) + opponent_cooperators_prev_round_count
            if num_cooperators_previous_round >= self.T_coop_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_218(BaseStrategy):
    """
    The "Adaptive Collective Reciprocator" (ACR) strategy promotes and sustains cooperation
    in the N-Player Public Goods Game. It starts with an initial act of trust (cooperation)
    and then employs conditional cooperation based on the observed collective effort in
    prior rounds, ensuring it doesn't become an easy target for purely self-interested players,
    especially in the end-game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_T_coop = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        elif current_round_t == self.game_description.n_rounds:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            my_action_prev_round_was_C = history.my_actions[-1]
            total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
            if total_cooperators_prev_round >= self.cooperation_threshold_T_coop:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_219(BaseStrategy):
    """
    The Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR initiates cooperation in the first round. In subsequent intermediate rounds,
    it adapts its behavior based on the aggregate level of cooperation from all players
    in the previous round. If the total number of cooperators meets or exceeds a
    calculated threshold (C_threshold = ceil(n / k)), the strategy cooperates.
    Otherwise, it defects to avoid exploitation. In the final round, it defects
    due to end-game rationality.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.C_threshold = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_0_indexed_round = state.round_number
        if current_0_indexed_round == 0:
            return Action.C
        if current_0_indexed_round == self.r - 1:
            return Action.D
        previous_0_indexed_round = current_0_indexed_round - 1
        num_cooperators_previous_round = np.sum(history.opponent_actions[previous_0_indexed_round, :]) + history.my_actions[previous_0_indexed_round]
        if num_cooperators_previous_round >= self.C_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_220(BaseStrategy):
    """
    The Adaptive Collective Reciprocator (ACR) strategy aims to initiate cooperation,
    then dynamically adjust its behavior based on the observed level of collective
    contribution in previous rounds. It employs a context-sensitive threshold for
    cooperation and adheres to rational self-interest in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_1_indexed_round = state.round_number + 1
        if current_1_indexed_round == 1:
            return Action.C
        elif current_1_indexed_round == self.n_rounds:
            return Action.D
        else:
            cooperators_previous_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            dynamic_cooperation_threshold = math.floor(self.n_players / self.k_factor)
            if cooperators_previous_round >= dynamic_cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_221(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster cooperation by initiating with cooperation and then
    adaptively responding to the collective behavior of all players. It continues
    to cooperate if a calculated threshold of players cooperated in the previous round,
    otherwise it defects. In the final round, it defects to avoid exploitation.

    The cooperation threshold (T = n - floor(k)) is dynamically linked to the
    multiplication factor 'k'. This makes the strategy more forgiving (lower T)
    when the public good is highly efficient (high 'k'), aligning with a collective
    mindset to sustain cooperation when it yields greater collective returns.
    Conversely, it is stricter (higher T) when 'k' is low, requiring more cooperators
    to continue contributing.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = self.game_description.n_players - math.floor(self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        num_cooperators_previous_round = (1 if history.my_actions[-1] else 0) + sum(history.opponent_actions[-1, :])
        if num_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_222(BaseStrategy):
    """
    Adaptive k-Threshold Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain cooperation by initially trusting
    others, then adapting its behavior based on the observed level of collective
    cooperation in previous rounds. It employs the game's multiplication factor
    'k' as a dynamic threshold to decide when cooperation is justified and when
    defection is necessary to avoid exploitation or signal a need for greater
    collective effort. It defects in the final round to prevent end-game exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        k_threshold = self.game_description.k
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators_prev_round >= k_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_223(BaseStrategy):
    """
    Adaptive Collective Reciprocity with Threshold (ACRT) strategy for the N-Player Public Goods Game.

    This strategy aims to foster collective cooperation while remaining robust against exploitation
    in a tournament setting. It balances the desire for the collectively optimal outcome with a
    pragmatic, adaptive response to observed player behavior.

    - Round 1 (0-indexed round 0): Always Cooperate (C) to initiate collective good.
    - Mid-game (0-indexed rounds 1 to n_rounds - 2):
      Observe the total number of cooperators (C_count_prev) from all players in the previous round.
      Calculate a dynamic CoopThreshold = n / k.
      If C_count_prev >= CoopThreshold, the strategy Cooperates (C).
      Otherwise (C_count_prev < CoopThreshold), it Defects (D) to punish insufficient collective participation.
    - Last Round (0-indexed round n_rounds - 1): Always Defect (D) to prevent exploitation in the end-game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.coop_threshold = self.n_players / self.k_factor

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            previous_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
            opponent_cooperators_prev = sum(previous_round_opponent_actions)
            my_action_prev = history.my_actions[current_round_0_indexed - 1]
            my_cooperated_prev = 1 if my_action_prev == Action.C else 0
            c_count_prev = opponent_cooperators_prev + my_cooperated_prev
            if c_count_prev >= self.coop_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_224(BaseStrategy):
    """
    The Adaptive Forgiving Reciprocator (AFR) strategy promotes and sustains cooperation
    in the N-Player Public Goods Game by leveraging initial trust, conditional cooperation,
    and acknowledging the end-game effect. It starts by cooperating, then conditionally
    cooperates based on the previous round's group cooperation level (if at least
    half the players cooperated), and finally defects in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.r - 1:
            return Action.D
        else:
            my_cooperation_prev_round = int(history.my_actions[-1])
            opponent_cooperation_prev_round = np.sum(history.opponent_actions[-1, :])
            cooperation_count_prev = my_cooperation_prev_round + opponent_cooperation_prev_round
            cooperation_threshold = math.ceil(self.n / 2.0)
            if cooperation_count_prev >= cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_225(BaseStrategy):
    """
    Adaptive Reciprocal Cooperator (ARC) strategy for the N-Player Public Goods Game.

    This strategy operates on the principle of conditional cooperation with a dynamic threshold.
    It starts by cooperating to signal a willingness to collaborate. In subsequent rounds,
    it assesses the overall level of cooperation in the previous round against a dynamic
    threshold. If cooperation meets or exceeds this threshold, the strategy cooperates;
    otherwise, it defects. The threshold itself adapts over time, becoming more or less
    demanding based on the observed collective performance, aiming to either stabilize
    existing cooperation or encourage its re-establishment. It also employs backward
    induction by defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T_c_dynamic = max(1, int(self.n / self.k))
        self.adaptation_tolerance_margin = max(1, int(self.n * 0.1))
        self.adaptation_rate = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if history is not None:
            total_cooperators_last_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            self._adapt_T_c_dynamic(total_cooperators_last_round)
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.r:
            return Action.D
        else:
            total_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if total_cooperators_prev_round >= self.T_c_dynamic:
                return Action.C
            else:
                return Action.D

    def _adapt_T_c_dynamic(self, C_last_completed_round_total: int):
        """
        Adapts self.T_c_dynamic based on the total number of cooperators
        in the previous round.
        """
        if C_last_completed_round_total == self.n:
            self.T_c_dynamic = max(1, self.T_c_dynamic - self.adaptation_rate)
        elif C_last_completed_round_total == 0:
            self.T_c_dynamic = max(1, self.T_c_dynamic - self.adaptation_rate)
        elif C_last_completed_round_total > self.T_c_dynamic + self.adaptation_tolerance_margin:
            self.T_c_dynamic = min(self.n, self.T_c_dynamic + self.adaptation_rate)
        elif C_last_completed_round_total < self.T_c_dynamic - self.adaptation_tolerance_margin:
            self.T_c_dynamic = max(1, self.T_c_dynamic - self.adaptation_rate)
        self.T_c_dynamic = max(1, min(self.n, self.T_c_dynamic))

class Strategy_COLLECTIVE_226(BaseStrategy):
    """
    Adaptive Collective Reciprocator (ACR) strategy for the N-Player Public Goods Game.

    Core Principle: The ACR strategy initiates cooperation to signal intent, then adaptively
    reciprocates observed cooperation levels. It maintains cooperation when the collective
    effort is deemed sufficient, otherwise it defects to penalize free-riders and protect
    against exploitation. Acknowledging the finite nature of the game, it adopts a
    pragmatic stance in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.p_threshold = 0.5 + 0.5 * (1 - self.k / self.n)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        cooperation_rate_prev_round = num_cooperators_prev_round / self.n
        if cooperation_rate_prev_round >= self.p_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_227(BaseStrategy):
    """
    Adaptive Collective Response with Self-Protection (ACR-SP) for the N-Player Public Goods Game.

    ACR-SP is designed to navigate the social dilemma by promoting cooperation while
    robustly protecting against exploitation. It combines a collective-minded approach
    to group behavior with a critical self-preservation mechanism.

    Core Principles:
    1. Initiate Cooperation: Always start by cooperating to signal willingness and
       attempt to establish the highest collective payoff.
    2. Collective Responsiveness: Base subsequent cooperation decisions primarily on
       the overall level of cooperation observed in the previous round.
    3. Self-Protection: Implement a critical override to prevent continuous
       exploitation if personal payoffs fall below a crucial threshold (payoff < 1 after cooperating).
    4. Rational Endgame: Recognize the terminal nature of the last round and act
       accordingly to prevent being a "sucker."
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_proportion = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_number == 0:
            return Action.C
        elif current_round_number == total_rounds - 1:
            return Action.D
        else:
            my_action_prev = history.my_actions[-1]
            my_payoff_prev = history.my_payoffs[-1]
            total_cooperators_prev_round = int(my_action_prev) + np.sum(history.opponent_actions[-1, :])
            p_c = total_cooperators_prev_round / n_players
            if my_action_prev == Action.C and my_payoff_prev < 1.0:
                return Action.D
            elif p_c >= self.cooperation_threshold_proportion:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_228(BaseStrategy):
    """
    Responsive Collective Reciprocity (RCR) strategy for the N-Player Public Goods Game.
    This strategy is designed to foster and maintain cooperation by adapting its behavior
    based on the observed level of collective cooperation in previous rounds. It initiates
    with trust, cooperates conditionally based on a calculated threshold of collective
    cooperation, and defects in the final round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        num_cooperators_previous_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_229(BaseStrategy):
    """
    The Adaptive Collective Reciprocity (ACR) strategy aims to foster and sustain cooperation within the group,
    acknowledging that collective cooperation leads to the highest possible total payoff. It is conditional:
    it initiates cooperation but demands a sufficient level of reciprocal cooperation from the group to continue
    contributing. It leverages the game parameter 'k' (multiplication factor) as a dynamic threshold for collective
    reciprocity. The strategy adapts its actions based on past collective behavior and incorporates an endgame
    defection for robustness.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        elif current_round_number == self.n_rounds - 1:
            return Action.D
        else:
            previous_round_index = current_round_number - 1
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_index])
            my_cooperation_prev_round = int(history.my_actions[previous_round_index])
            total_cooperators_prev_round = opponent_cooperators_prev_round + my_cooperation_prev_round
            if total_cooperators_prev_round >= self.k_factor:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_230(BaseStrategy):
    """
    The Adaptive Collective Spirit (ACS) strategy operates on the principle of conditional cooperation,
    seeking to establish and maintain a high level of group cooperation while being resilient to
    individual defection. It starts cooperatively, monitors the overall group's cooperative behavior,
    and employs a forgiveness mechanism to prevent permanent breakdowns. It adapts its behavior
    based on observed group cooperation and adjusts for the specific characteristics of the first
    and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_defections_by_us_count = 0
        self.forgiveness_interval = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            self.consecutive_defections_by_us_count = 0
            return Action.C
        if current_round == self.game_description.n_rounds - 1:
            self.consecutive_defections_by_us_count = 0
            return Action.D
        num_cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if self.consecutive_defections_by_us_count > 0 and self.consecutive_defections_by_us_count % self.forgiveness_interval == 0:
            self.consecutive_defections_by_us_count = 0
            return Action.C
        if num_cooperators_prev_round >= self.game_description.k:
            self.consecutive_defections_by_us_count = 0
            return Action.C
        else:
            self.consecutive_defections_by_us_count += 1
            return Action.D

class Strategy_COLLECTIVE_231(BaseStrategy):
    """
    Adaptive Group Reciprocator with Forgiveness (AGRF) strategy.

    This strategy aims to maximize the collective payoff by fostering cooperation
    among players, while simultaneously protecting individual payoff against
    exploitation. It operates on principles of conditional cooperation,
    group-level reciprocity, and strategic defection in the endgame.
    It starts by signaling a willingness to cooperate and adapts its behavior
    based on the observed aggregate cooperation level of the group in previous rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_proportion = max(0.5, self.k / self.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed >= self.n_rounds - 1:
            return Action.D
        num_cooperators_in_previous_round = history.my_actions[state.round_number - 1] + np.sum(history.opponent_actions[state.round_number - 1, :])
        proportion_cooperators_in_previous_round = num_cooperators_in_previous_round / self.n_players
        if proportion_cooperators_in_previous_round >= self.cooperation_threshold_proportion:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_232(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and maintain cooperation by starting cooperatively and then
    dynamically adjusting its action based on whether the observed collective cooperation
    in the previous round meets a calculated sustainability threshold. It always defects
    in the final round to prevent exploitation, adhering to the endgame effect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.THRESHOLD_C = max(0, math.ceil(self.n_players / self.k_factor - 1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        my_action_prev_as_int = int(history.my_actions[-1])
        C_prev = num_opponent_cooperators_prev + my_action_prev_as_int
        if C_prev >= self.THRESHOLD_C:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_233(BaseStrategy):
    """
    The Adaptive Reciprocal Cooperation (ARC) strategy initiates cooperation in the first round.
    In intermediate rounds, it observes the level of collective cooperation from the previous round.
    If the number of cooperators meets or exceeds a dynamic threshold (ceil(n/2)), ARC continues
    to cooperate. Otherwise, it defects to punish non-cooperators and protect itself from exploitation.
    In the final round, ARC defects based on backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        cooperation_threshold = math.ceil(self.n_players / 2)
        if num_cooperators_prev_round >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_234(BaseStrategy):
    """
    The "Collective Reciprocator" strategy initiates cooperation to signal willingness and explore
    collective benefit. It then adapts its behavior in subsequent rounds based on the overall level
    of cooperation observed in the group during the previous round. It punishes significant
    collective defection but is forgiving enough to re-engage in cooperation if the group's
    behavior improves. It acknowledges the end-game dynamics of finite repeated games by
    defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.COOPERATION_THRESHOLD_COUNT = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        my_action_prev_round_was_cooperate = history.my_actions[-1]
        num_total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_cooperate else 0)
        if num_total_cooperators_prev_round >= self.COOPERATION_THRESHOLD_COUNT:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_235(BaseStrategy):
    """
    The Adaptive Collective Reciprocity (ACR) strategy promotes and sustains cooperation
    in the N-Player Public Goods Game by adapting its level of forgiveness based on
    game parameters (k and n). It starts with cooperation, defects in the final round,
    and in intermediate rounds, decides based on the number of cooperators in the
    previous round relative to a dynamically calculated threshold.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.min_cooperators_needed = max(1, math.floor(self.n - self.k + 1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        previous_round_index = state.round_number - 1
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_index, :])
        my_action_prev_round_was_C = history.my_actions[previous_round_index]
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + int(my_action_prev_round_was_C)
        if total_cooperators_prev_round >= self.min_cooperators_needed:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_236(BaseStrategy):
    """
    The Adaptive Collective Response (ACR) strategy for the N-Player Public Goods Game.
    This strategy initiates cooperation, adapts behavior based on a dynamically calculated
    cooperation efficiency threshold, punishes widespread defection, rewards sufficient
    cooperation, and defects in the final round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T_C = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.r - 1:
            return Action.D
        my_prev_action_cooperated = history.my_actions[-1]
        opponents_prev_cooperated_count = np.sum(history.opponent_actions[-1, :])
        num_cooperators_prev_round = int(my_prev_action_cooperated) + int(opponents_prev_cooperated_count)
        if num_cooperators_prev_round >= self.T_C:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_237(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster cooperation, adapt to observed behavior, protect against
    exploitation, and align with a collective mindset within the constraints of the game.

    It starts with an initial cooperative gesture. In intermediate rounds, it conditionally
    reciprocates based on whether the overall level of cooperation in the preceding round
    met a calculated "sustainment threshold" for the public good. Finally, it defects
    in the last round, acknowledging the "endgame effect" to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.total_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.total_rounds - 1:
            return Action.D
        else:
            previous_round_index = current_round_0_indexed - 1
            my_previous_action_cooperated = int(history.my_actions[previous_round_index])
            opponent_cooperators_previous_round = np.sum(history.opponent_actions[previous_round_index, :])
            cooperators_in_previous_round = my_previous_action_cooperated + opponent_cooperators_previous_round
            required_cooperators_for_sustainment = math.ceil(self.n_players / self.k_factor)
            if cooperators_in_previous_round >= required_cooperators_for_sustainment:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_238(BaseStrategy):
    """
    The Collective Adaptive Trigger strategy for the N-Player Public Goods Game.
    It aims to foster and sustain collective cooperation by initiating with a cooperative move.
    In intermediate rounds, it adapts its behavior based on the proportion of cooperators
    in the previous round relative to a dynamically calculated threshold (theta), which
    is sensitive to the game's parameters (n and k). In the final round, it always defects
    to avoid exploitation due to the lack of future interactions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.theta = (self.n_players - self.k_factor) / (self.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            my_action_prev_round_was_C = history.my_actions[current_round_0_indexed - 1]
            num_opponent_cooperators_prev_round = sum(history.opponent_actions[current_round_0_indexed - 1, :])
            total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
            proportion_cooperators_prev_round = total_cooperators_prev_round / self.n_players
            if proportion_cooperators_prev_round >= self.theta:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_239(BaseStrategy):
    """
    Adaptive Collective Reciprocator (ACR) Strategy for N-Player Public Goods Game.

    The ACR strategy aims to initiate cooperation, reciprocate observed cooperation
    from others, and punish defection to incentivize a return to collective good.
    It remains pragmatic about individual incentives, especially in the game's final stages.
    It bases its decisions solely on the game parameters (n, r, k) and the observable
    history of play.

    Decision Rules:
    1. Round 1 (Initiation Phase): Always Cooperate (C) to signal willingness to contribute.
    2. Last Round (Termination Phase): Always Defect (D) due to the end-game effect
       where individual defection becomes the dominant strategy.
    3. Intermediate Rounds (Reciprocity Phase):
       - Observes the proportion of other players who cooperated in the preceding round.
       - Calculates an adaptive cooperation threshold: (k - 1) / (n - 1).
       - If the observed proportion of cooperators meets or exceeds this threshold,
         the strategy Cooperates (C). Otherwise, it Defects (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = (self.game_description.k - 1) / (self.game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_1_indexed_round = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_1_indexed_round == 1:
            return Action.C
        if current_1_indexed_round == total_rounds:
            return Action.D
        else:
            previous_round_opponent_actions = history.opponent_actions[state.round_number - 1, :]
            num_cooperators_others = np.sum(previous_round_opponent_actions)
            n_others = n_players - 1
            proportion_cooperators_others = num_cooperators_others / n_others
            if proportion_cooperators_others >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_240(BaseStrategy):
    """
    The Adaptive Collective Reciprocator (ACR) strategy aims to foster and maintain collective
    cooperation in the N-Player Public Goods Game. It is conditionally cooperative, adaptable
    to group behavior, and robust against various opponent types.

    It starts by cooperating, then dynamically adjusts its cooperation based on a calculated
    Cooperation_Threshold. This threshold determines the minimum number of cooperators required
    in the previous round for the strategy to continue cooperating. In the final round, it
    defects based on the principle of backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = max(1, math.floor(self.n_players - (self.n_players - self.k_factor) / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            previous_round_index = state.round_number - 1
            total_cooperators_prev_round = sum(history.opponent_actions[previous_round_index])
            if history.my_actions[previous_round_index]:
                total_cooperators_prev_round += 1
            if total_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_241(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster collective cooperation. It starts by cooperating
    and monitors the group's cooperation level in the previous round. It adapts
    its behavior between a COOPERATE_MODE and a PUNISH_MODE.

    In COOPERATE_MODE, it cooperates and transitions to PUNISH_MODE if the number
    of cooperators falls below a dynamic threshold (n-1 for n>2, 2 for n=2).
    In PUNISH_MODE, it defects and transitions back to COOPERATE_MODE if the
    number of cooperators meets or exceeds the threshold.

    The strategy defects in the final round to avoid exploitation.
    """
    COOPERATE_MODE = 'COOPERATE_MODE'
    PUNISH_MODE = 'PUNISH_MODE'

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters and its internal state.

        Args:
            game_description (PublicGoodsDescription): An object containing
                                                       n_players, n_rounds, k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.current_strategy_mode = self.COOPERATE_MODE

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on strategy rules.

        Args:
            state (GameState): Current game state, including the round number.
            history (None | PlayerHistory): Past actions and payoffs for all players.
                                            None for the very first round (round 0).

        Returns:
            Action: The chosen action (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == self.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        prev_round_idx = state.round_number - 1
        my_prev_action_was_c = history.my_actions[prev_round_idx]
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        num_cooperators_prev_round = int(my_prev_action_was_c) + num_opponent_cooperators_prev_round
        c_threshold = 0
        if self.n_players == 2:
            c_threshold = 2
        else:
            c_threshold = self.n_players - 1
        action_for_this_round = None
        if self.current_strategy_mode == self.COOPERATE_MODE:
            action_for_this_round = Action.C
            if num_cooperators_prev_round < c_threshold:
                self.current_strategy_mode = self.PUNISH_MODE
        elif self.current_strategy_mode == self.PUNISH_MODE:
            action_for_this_round = Action.D
            if num_cooperators_prev_round >= c_threshold:
                self.current_strategy_mode = self.COOPERATE_MODE
        return action_for_this_round

class Strategy_COLLECTIVE_242(BaseStrategy):
    """
    The "Adaptive Collective Reciprocity" (ACR) strategy aims to foster cooperation
    in the N-player public goods game. It balances the collective desire for optimal
    outcomes with the individual need to avoid exploitation. It adapts based on
    history and game parameters (n, r, k).

    Philosophy:
    1. Optimistic Initiation: Start with Cooperation.
    2. Conditional Cooperation: Cooperate if collective effort is sufficient.
    3. Punishment: Defect if cooperation is critically low or weak after my cooperation.
    4. Forgiveness/Re-engagement: Cooperate after my defection if group cooperation is strong.
    5. Strategic End-Game: Defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        current_round_t = state.round_number + 1
        chosen_action: Action
        if current_round_t == 1:
            chosen_action = Action.C
        elif current_round_t == r:
            chosen_action = Action.D
        else:
            num_cooperators_in_prev_round_others = np.sum(history.opponent_actions[-1, :])
            my_action_in_prev_round_bool = history.my_actions[-1]
            num_cooperators_in_prev_round = num_cooperators_in_prev_round_others + int(my_action_in_prev_round_bool)
            current_observed_cooperation_fraction = num_cooperators_in_prev_round / n
            critically_low_cooperation = current_observed_cooperation_fraction < k / n
            weak_cooperation_observed = current_observed_cooperation_fraction < 0.5
            strong_cooperation_observed = current_observed_cooperation_fraction >= 0.9
            if critically_low_cooperation:
                chosen_action = Action.D
            elif self._my_last_action == Action.C:
                if weak_cooperation_observed:
                    chosen_action = Action.D
                else:
                    chosen_action = Action.C
            elif self._my_last_action == Action.D:
                if strong_cooperation_observed:
                    chosen_action = Action.C
                else:
                    chosen_action = Action.D
            else:
                chosen_action = Action.D
        self._my_last_action = chosen_action
        return chosen_action

class Strategy_COLLECTIVE_243(BaseStrategy):
    """
    The Collective Reciprocal Threshold (CRT) strategy aims to promote and sustain
    cooperation in the N-Player Public Goods Game. It starts by cooperating
    and continues to do so if a sufficient proportion (at least half, rounded up)
    of other players cooperated in the previous round. Otherwise, it defects.
    This strategy applies its decision logic consistently across all rounds,
    including the final round, to foster stable cooperative behavior and
    avoid the "unravelling" associated with backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.threshold_others_cooperated = math.ceil((self.n - 1) / 2.0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        else:
            num_others_cooperated_prev_round = np.sum(history.opponent_actions[-1, :])
            if num_others_cooperated_prev_round >= self.threshold_others_cooperated:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_244(BaseStrategy):
    """
    Adaptive Forgiving Reciprocity strategy for the N-Player Public Goods Game.

    This strategy combines initial goodwill, responsiveness to the overall group's cooperation level,
    and rational self-interest in the game's final stages.

    - Round 1 (0-indexed: round 0): Always Cooperate.
    - Rounds 2 to r-1 (0-indexed: rounds 1 to r-2):
        - If the total number of cooperators in the previous round was less than (n - 1), Defect (collective deterrence).
        - Otherwise (total cooperators >= n - 1), Cooperate (forgiving cooperation).
    - Round r (0-indexed: round r-1): Always Defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        prev_round_idx = state.round_number - 1
        num_cooperators_last_round = np.sum(history.opponent_actions[prev_round_idx, :]) + history.my_actions[prev_round_idx]
        if num_cooperators_last_round < self.n_players - 1:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_245(BaseStrategy):
    """
    Adaptive Conditional Cooperation with Forgiveness (ACCF) strategy.
    Starts by cooperating, adapts its behavior based on observed group cooperation
    relative to a dynamic threshold, and incorporates patience and forgiveness
    mechanisms to foster and re-establish cooperation.
    """
    PATIENCE_LIMIT = 2
    FORGIVENESS_LIMIT = 2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.dynamic_T_C = (self.n_players - self.k_factor) / (self.n_players - 1) * 0.6 + 0.2
        self.current_mode = 'COOPERATE'
        self.patience_countdown = self.PATIENCE_LIMIT
        self.forgiveness_countdown = self.FORGIVENESS_LIMIT

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.current_mode = 'COOPERATE'
            self.patience_countdown = self.PATIENCE_LIMIT
            self.forgiveness_countdown = self.FORGIVENESS_LIMIT
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        last_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        last_round_my_action_was_C = history.my_actions[-1]
        last_round_total_cooperators = last_round_opponent_cooperators + (1 if last_round_my_action_was_C else 0)
        observed_cooperation_ratio = last_round_total_cooperators / self.n_players
        if self.current_mode == 'COOPERATE':
            if observed_cooperation_ratio >= self.dynamic_T_C:
                action = Action.C
                self.patience_countdown = self.PATIENCE_LIMIT
            else:
                self.patience_countdown -= 1
                if self.patience_countdown <= 0:
                    action = Action.D
                    self.current_mode = 'DEFECT'
                    self.forgiveness_countdown = self.FORGIVENESS_LIMIT
                else:
                    action = Action.C
        else:
            self.forgiveness_countdown -= 1
            if self.forgiveness_countdown <= 0:
                action = Action.C
                self.current_mode = 'COOPERATE'
                self.patience_countdown = self.PATIENCE_LIMIT
            else:
                action = Action.D
        return action

class Strategy_COLLECTIVE_246(BaseStrategy):
    """
    Adaptive Collective Reciprocator: Fosters cooperation in the N-Player Public Goods Game.
    Starts with cooperation, then monitors collective cooperation against a dynamic threshold
    (ceil(n/k)). Punishes collective defection with a fixed period of defection, and then
    forgives if cooperation improves. Accounts for endgame dynamics by defecting in the final
    round and avoiding futile short punishment cycles.
    """
    _COOPERATING_STATE = 'COOPERATING'
    _PUNISHING_STATE = 'PUNISHING'

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        self.MIN_COOPERATORS_TO_CONTINUE = math.ceil(n / k)
        self.MIN_COOPERATORS_FOR_FORGIVENESS = self.MIN_COOPERATORS_TO_CONTINUE
        self.PUNISHMENT_DURATION = 3
        self.current_state = self._COOPERATING_STATE
        self.rounds_in_punishment = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        C_prev = np.sum(history.opponent_actions[-1]) + int(history.my_actions[-1])
        if self.current_state == self._COOPERATING_STATE:
            remaining_rounds_after_this_one = self.game_description.n_rounds - current_round_1_indexed
            if remaining_rounds_after_this_one < self.PUNISHMENT_DURATION:
                return Action.C
            if C_prev < self.MIN_COOPERATORS_TO_CONTINUE:
                self.current_state = self._PUNISHING_STATE
                self.rounds_in_punishment = 1
                return Action.D
            else:
                return Action.C
        else:
            self.rounds_in_punishment += 1
            if self.rounds_in_punishment >= self.PUNISHMENT_DURATION:
                if C_prev >= self.MIN_COOPERATORS_FOR_FORGIVENESS:
                    self.current_state = self._COOPERATING_STATE
                    self.rounds_in_punishment = 0
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D

class Strategy_COLLECTIVE_247(BaseStrategy):
    """
    Adaptive Collective Reciprocator (ACR) strategy for the N-Player Public Goods Game.

    This strategy divides the game into distinct phases, each with specific decision rules
    that depend on the game parameters and the history of cooperation. It aims to foster
    and maintain cooperation by responding to observed collective behavior, while also
    protecting against exploitation and accounting for end-game dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._below_threshold_counter = 0
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        self.C_threshold = (n - k + 1) / n
        self.Forgive_rounds = 1
        self.Endgame_horizon = max(1, min(r // 5, 5))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if state.round_number == 0:
            self._below_threshold_counter = 0
            return Action.C
        if state.round_number == r - 1:
            return Action.D
        N_c_prev = np.sum(history.opponent_actions[state.round_number - 1, :]) + history.my_actions[state.round_number - 1]
        P_c_prev = N_c_prev / n
        if state.round_number >= r - self.Endgame_horizon and state.round_number < r - 1:
            if P_c_prev >= self.C_threshold:
                self._below_threshold_counter = 0
                return Action.C
            else:
                self._below_threshold_counter = 0
                return Action.D
        if P_c_prev >= self.C_threshold:
            self._below_threshold_counter = 0
            return Action.C
        else:
            self._below_threshold_counter += 1
            if self._below_threshold_counter > self.Forgive_rounds:
                self._below_threshold_counter = 0
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_248(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR initiates cooperation, then dynamically adjusts its behavior based on the
    observed collective cooperation in previous rounds. It cooperates if the number
    of cooperators in the previous round meets or exceeds a dynamically calculated
    threshold (ceil(n/k)), otherwise it defects. In the final round, it always defects
    due to end-game effects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ACR strategy with game parameters and calculates the
        cooperation threshold.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.required_cooperators = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the ACR strategy rules.

        Args:
            state: An object containing the current round number.
            history: An object containing records of past actions and payoffs.
                     It is None for the very first round (round_number = 0).

        Returns:
            An Action (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == self.r - 1:
            return Action.D
        previous_round_idx = current_round_idx - 1
        my_previous_action_was_cooperate = history.my_actions[previous_round_idx]
        opponent_previous_cooperators = np.sum(history.opponent_actions[previous_round_idx, :])
        total_cooperators_in_previous_round = int(my_previous_action_was_cooperate) + opponent_previous_cooperators
        if total_cooperators_in_previous_round >= self.required_cooperators:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_249(BaseStrategy):
    """
    The Adaptive Majority Reciprocator strategy for the N-Player Public Goods Game.

    This strategy aims to foster collective cooperation while being robust against
    exploitation and adaptable to diverse opponent behaviors. It operates under the
    principle of conditional cooperation based on observed collective effort.

    - **Round 1:** Starts by Cooperating (C) to signal willingness to contribute.
    - **Last Round (r):** Defects (D) to avoid exploitation due to backward induction.
    - **Intermediate Rounds:** Observes the proportion of cooperators in the previous
      round. If half or more players cooperated (>= 0.5), it Cooperates (C);
      otherwise, it Defects (D) to protect itself and signal dissatisfaction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialises the strategy with game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the strategy rules.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs for all players,
                     or None if it's the very first round.

        Returns:
            An Action (Cooperate or Defect) for the current round.
        """
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        elif current_round_idx == self.n_rounds - 1:
            return Action.D
        else:
            prev_round_idx = current_round_idx - 1
            num_opponent_cooperators = np.sum(history.opponent_actions[prev_round_idx, :])
            total_cooperators_prev_round = num_opponent_cooperators + int(history.my_actions[prev_round_idx])
            proportion_cooperators = total_cooperators_prev_round / self.n_players
            if proportion_cooperators >= 0.5:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_250(BaseStrategy):
    """
    Adaptive Reciprocal Cooperator (ARC) strategy for the N-Player Public Goods Game.
    Starts with cooperation, adapts based on observed collective cooperation relative
    to a calculated threshold in intermediate rounds, and defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.n_rounds - 1:
            return Action.D
        if current_round_number == 0:
            return Action.C
        previous_round_index = current_round_number - 1
        num_cooperators_previous_round = np.sum(history.opponent_actions[previous_round_index])
        if history.my_actions[previous_round_index]:
            num_cooperators_previous_round += 1
        if num_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_251(BaseStrategy):
    """
    Collective Reciprocity with Pragmatic Threshold (CRPT) strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation when beneficial, protect against exploitation, and adapt
    to the observed collective behavior of other players. It initiates cooperation in the first round,
    then continues to cooperate in intermediate rounds only if the number of cooperators in the
    previous round met a specific pragmatic threshold (ceil(n/k)). It defects in the final round
    due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_C_num = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_number == 0:
            return Action.C
        elif current_round_number == total_rounds - 1:
            return Action.D
        else:
            previous_round_index = current_round_number - 1
            opponent_cooperators_prev = np.sum(history.opponent_actions[previous_round_index, :])
            my_action_prev = history.my_actions[previous_round_index]
            N_C_prev = opponent_cooperators_prev + my_action_prev
            if N_C_prev >= self.T_C_num:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_252(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and sustain collective cooperation by exhibiting
    conditional cooperation, punishing free-riding, and showing forgiveness.
    It dynamically adjusts its required level of cooperation based on the game's
    collective benefit (k) and the number of players (n). It initiates with
    cooperation, defects if collective cooperation falls below a dynamic threshold,
    and defects in the final round due to endgame rationality.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        k = game_description.k
        self.cooperation_threshold = max(2, round(n - (k - 1)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_action_prev_round_is_cooperate = history.my_actions[state.round_number - 1]
        opponent_actions_prev_round_are_cooperate = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_previous_round = int(my_action_prev_round_is_cooperate) + np.sum(opponent_actions_prev_round_are_cooperate)
        if num_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_253(BaseStrategy):
    """
    The "Adaptive Reciprocator" strategy is designed to promote collective cooperation in the
    N-Player Public Goods Game while remaining robust against exploitation and adapting to
    the observed behavior of other players. It balances the collective good with self-protection
    by using a dynamically calculated threshold for collective cooperation.

    - Starts with Cooperation in Round 0 (initial round) to signal willingness to cooperate.
    - In intermediate rounds, it observes the cooperation level of other players in the previous round.
      If the number of other cooperators meets a dynamically calculated threshold (T_required),
      it cooperates. Otherwise, it defects to protect against free-riding.
    - Defects in the final round based on backward induction principles for maximum individual payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        t_calculated_val = math.ceil(self.n_players / self.k)
        self.t_required = min(t_calculated_val, self.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        prev_round_index = state.round_number - 1
        c_others_prev = int(np.sum(history.opponent_actions[prev_round_index, :]))
        if c_others_prev >= self.t_required:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_254(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster widespread cooperation to maximize collective payoff. It starts
    with an act of trust by cooperating in the first round. In intermediate rounds,
    it conditionally cooperates only if a "sufficient" number of players (including
    itself) demonstrated cooperation in the previous round. This sufficiency is
    determined by a dynamic threshold `ceil(n / k)`. In the final round, it
    strategically defects to prevent exploitation, adhering to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_T = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
            my_action_prev_round = history.my_actions[state.round_number - 1]
            total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round else 0)
            if total_cooperators_prev_round >= self.cooperation_threshold_T:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_255(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy is built on conditional cooperation, adapting to the observed
    collective behavior of other players. It starts by attempting to establish
    a cooperative environment and then monitors the total number of cooperators
    in the previous round. It decides to cooperate if this collective cooperation
    meets or exceeds a dynamic threshold (T_coop = ceil(n/k)), otherwise it defects.
    The strategy also accounts for the "endgame effect" by defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_previous_action_was_cooperate = history.my_actions[-1]
        num_opponent_cooperators_previous_round = np.sum(history.opponent_actions[-1, :])
        total_cooperators_previous_round = int(my_previous_action_was_cooperate) + num_opponent_cooperators_previous_round
        if total_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_256(BaseStrategy):
    """
    Majority-Conditional Cooperation with Endgame Defection (MCC-ED)

    This strategy aims to initiate and sustain collective cooperation by rewarding a sufficient
    level of group cooperation and punishing widespread defection. It acknowledges the
    game-theoretic implications of a finite game, particularly in the final round,
    to avoid exploitation.

    Decision Rules:
    1.  First Round: Cooperate (to signal willingness and attempt to establish cooperation).
    2.  Intermediate Rounds: Observe the cooperation rate from the previous round.
        -   If cooperation rate >= 60%, Cooperate (to reward and sustain cooperation).
        -   Else (cooperation rate < 60%), Defect (to punish defection and protect against exploitation).
    3.  Last Round: Defect (based on backward induction to avoid exploitation in the final round).
    """
    THRESHOLD_COOPERATION_RATE = 0.6

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == n_rounds - 1:
            return Action.D
        else:
            my_action_prev_round = history.my_actions[-1]
            opponent_actions_prev_round = history.opponent_actions[-1, :]
            num_cooperators_prev_round = 0
            if my_action_prev_round:
                num_cooperators_prev_round += 1
            num_cooperators_prev_round += np.sum(opponent_actions_prev_round)
            cooperation_rate_prev = num_cooperators_prev_round / n_players
            if cooperation_rate_prev >= self.THRESHOLD_COOPERATION_RATE:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_257(BaseStrategy):
    """
    Adaptive Community Reciprocator (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and sustain collective cooperation by starting cooperatively,
    then adapting its behavior based on observed past cooperation levels and game parameters.
    It defects in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        my_action_prev = history.my_actions[state.round_number - 1]
        opponent_actions_prev = history.opponent_actions[state.round_number - 1, :]
        C_total_prev = int(my_action_prev) + np.sum(opponent_actions_prev)
        P_C_prev = C_total_prev / self.n_players
        k_over_n = self.k_factor / self.n_players
        effective_coop_threshold = k_over_n + (1 - k_over_n) * 0.25
        effective_coop_threshold = max(0.25, effective_coop_threshold)
        if P_C_prev >= effective_coop_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_258(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR initiates cooperation, defects in the final round (backward induction), and
    in intermediate rounds, it cooperates if the observed cooperation rate among
    other players in the previous round meets a dynamically calculated threshold.
    This threshold ensures that cooperation is individually sustainable, preventing
    exploitation while fostering collective benefit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_T = (self.n_players / self.k - 1.0) / (self.n_players - 1.0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        num_cooperators_others_prev = sum(history.opponent_actions[-1, :])
        observed_coop_rate_others = num_cooperators_others_prev / (self.n_players - 1.0)
        if observed_coop_rate_others >= self.cooperation_threshold_T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_259(BaseStrategy):
    """
    The Adaptive Reciprocal Cooperator (ARC) strategy for the N-Player Public Goods Game.

    ARC starts with an optimistic cooperation, conditionally cooperates in middle rounds
    based on a threshold of other players' cooperation in the previous round, and defects
    in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ARC strategy with game parameters and its internal threshold.

        Args:
            game_description: An object containing game parameters such as n_players,
                              n_rounds, and k.
        """
        self.game_description = game_description
        self.cooperation_threshold_proportion = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs. It is None for
                     the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == total_rounds - 1:
            return Action.D
        else:
            num_cooperators_others_prev = np.sum(history.opponent_actions[-1, :])
            num_other_players = n_players - 1
            cooperation_ratio_others_prev = num_cooperators_others_prev / num_other_players
            if cooperation_ratio_others_prev >= self.cooperation_threshold_proportion:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_260(BaseStrategy):
    """
    Adaptive Public Goods Enforcer (APGE) strategy.

    This strategy aims to foster cooperation by initiating it, rewarding sustained
    collective contributions, and punishing insufficient cooperation, while
    safeguarding against exploitation. Its adaptiveness is derived from a
    dynamic threshold that considers the inherent game parameters (n and k)
    and adapts its behavior based on the observed level of cooperation in the
    previous round and anticipates the game's end.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        self.coop_threshold = max(1, n - math.floor(n / k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_rounds:
            return Action.D
        else:
            cooperators_from_opponents = np.sum(history.opponent_actions[-1, :])
            my_previous_action_was_cooperate = history.my_actions[-1]
            total_cooperators_prev_round = cooperators_from_opponents + my_previous_action_was_cooperate
            if total_cooperators_prev_round >= self.coop_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_261(BaseStrategy):
    """
    Adaptive Majority Reciprocity: This strategy aims to foster cooperation by initiating with a cooperative move.
    In subsequent rounds, it observes the collective cooperation level from the previous round. If a majority
    (at least floor(n/2) players) cooperated, the strategy reciprocates with cooperation. Otherwise, it defects
    to protect its own payoff and signal a need for greater collective effort. In the final round, it defects
    due to the end-game effect, as there are no future interactions to incentivize cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            my_prev_action = history.my_actions[current_round_0_indexed - 1]
            opponent_prev_actions = history.opponent_actions[current_round_0_indexed - 1, :]
            num_cooperators_prev_round = int(my_prev_action) + np.sum(opponent_prev_actions)
            cooperation_threshold = self.n_players // 2
            if num_cooperators_prev_round >= cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_262(BaseStrategy):
    """
    Dynamic Reciprocity with Adaptive Forgiveness strategy for N-Player Public Goods Game.
    Starts with cooperation, monitors collective contribution, and punishes if
    cooperation falls below a threshold. Forgives quickly after a short punishment
    period if cooperation recovers. Defects in the final round due to end-game dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.C_THRESHOLD_RATIO = 0.7
        self.FORGIVENESS_ROUNDS = 1
        n_players = self.game_description.n_players
        self.C_threshold = max(1, math.floor(n_players * self.C_THRESHOLD_RATIO))
        self.my_state = 'TRUSTING'
        self.rounds_in_punishment = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == total_rounds - 1:
            return Action.D
        n_cooperators_prev = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if self.my_state == 'TRUSTING':
            if n_cooperators_prev < self.C_threshold:
                self.my_state = 'PUNISHING'
                self.rounds_in_punishment = 1
                return Action.D
            else:
                return Action.C
        elif self.my_state == 'PUNISHING':
            if self.rounds_in_punishment >= self.FORGIVENESS_ROUNDS and n_cooperators_prev >= self.C_threshold:
                self.my_state = 'TRUSTING'
                self.rounds_in_punishment = 0
                return Action.C
            else:
                self.rounds_in_punishment += 1
                return Action.D

class Strategy_COLLECTIVE_263(BaseStrategy):
    """
    Adaptive Group Reciprocator (AGR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by starting cooperatively and then
    adapting its behavior based on the observed level of collective cooperation
    in previous rounds, while also being robust against exploitation.

    - **First Round:** Cooperates to signal willingness and establish a positive precedent.
    - **Last Round:** Defects to prevent exploitation due to the "end-game effect."
    - **Intermediate Rounds:** Observes the total number of cooperators from the
      previous round (`C_count_prev_round`). If `C_count_prev_round` meets or exceeds
      a dynamically calculated `Cooperation Threshold` (`ceil(n / k)`), it Cooperates.
      Otherwise, it Defects, signaling that the level of cooperation was insufficient.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.c_threshold = max(1, math.ceil(self.n / self.k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        opponent_cooperators_prev_round = sum(history.opponent_actions[state.round_number - 1])
        my_action_prev_round = history.my_actions[state.round_number - 1]
        c_count_previous_round = opponent_cooperators_prev_round + my_action_prev_round
        if c_count_previous_round >= self.c_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_264(BaseStrategy):
    """
    K-Threshold Reciprocity (KTR) strategy for the N-Player Public Goods Game.

    KTR adapts its decision based on the observed collective cooperation in the
    previous round, using a dynamic threshold derived from the public good's
    multiplication factor 'k'. It starts by cooperating, defects in the last
    round (to prevent exploitation), and in intermediate rounds, cooperates if
    the number of cooperators in the previous round met or exceeded a
    calculated threshold (ceil(n/k)). Otherwise, it defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T_c = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        previous_round_index = current_round_0_indexed - 1
        N_c_prev_opponents = sum(history.opponent_actions[previous_round_index, :])
        N_c_prev_self = int(history.my_actions[previous_round_index])
        N_c_prev = N_c_prev_opponents + N_c_prev_self
        if N_c_prev >= self.T_c:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_265(BaseStrategy):
    """
    This strategy aims to foster and sustain collective cooperation in an N-Player Public Goods Game
    by reciprocating observed cooperation and punishing defection. It uses a dynamically calculated
    threshold based on game parameters to decide its action. The strategy prioritizes overall
    collective welfare while ensuring robustness against widespread defection, initiating with
    cooperation and applying conditional reciprocity consistently across all rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold_C_count = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperators_in_prev_round = np.sum(history.opponent_actions[-1, :])
        if history.my_actions[-1]:
            cooperators_in_prev_round += 1
        if cooperators_in_prev_round >= self.threshold_C_count:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_266(BaseStrategy):
    """
    The Collective Reciprocity strategy for the N-Player Public Goods Game.
    This strategy aims to promote and maintain cooperation by starting cooperatively
    and then adapting its action based on a dynamic cooperation threshold.
    It protects against exploitation by defecting if collective cooperation falls
    below a predefined threshold and defects in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            previous_round_idx = state.round_number - 1
            my_prev_action = history.my_actions[previous_round_idx]
            opponents_prev_actions = history.opponent_actions[previous_round_idx, :]
            num_cooperators_previous_round = int(my_prev_action) + np.sum(opponents_prev_actions)
            if num_cooperators_previous_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_267(BaseStrategy):
    """
    Adaptive Public Goods Cooperator (APGC) strategy for the N-Player Public Goods Game.

    This strategy aims for collective welfare by initiating cooperation and
    then adapting its behavior based on the observed level of cooperation from
    other players. It also accounts for the end-game effect by defecting
    in the final round.

    Decision Rules:
    1. Round 1 (Initial Trust): Always Cooperate (C). This attempts to initiate
       a cooperative equilibrium.
    2. Intermediate Rounds (Adaptive Conditional Cooperation): Cooperate if the
       number of other players who cooperated in the previous round meets or
       exceeds a dynamically calculated threshold (ceil(n - k)). Otherwise, Defect (D).
       This reinforces positive collective behavior and discourages free-riding.
    3. Last Round (End-Game Effect): Always Defect (D). In the final round,
       there are no future interactions to incentivize cooperation, making
       defection the dominant strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.m_others_threshold = math.ceil(self.n - self.k)
        self.m_others_threshold = max(0, min(self.m_others_threshold, self.n - 1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        m_others_prev = sum(history.opponent_actions[-1, :])
        if m_others_prev >= self.m_others_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_268(BaseStrategy):
    """
    The Adaptive Collective Reciprocator (ACR) strategy aims to foster cooperation by initiating with a cooperative move
    and sustaining cooperation based on observed collective effort.

    In the first round (round 1), it cooperates to signal willingness and explore group dynamics.
    In intermediate rounds (round 2 up to the second-to-last round), it adopts a conditional cooperation strategy:
    it cooperates if a simple majority (including itself) cooperated in the immediately preceding round, otherwise, it defects.
    In the final round, it defects as a self-preservation measure, recognizing the lack of future interactions.

    This strategy balances collective benefit maximization with individual self-interest and robustness against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.num_players = game_description.n_players
        self.total_rounds = game_description.n_rounds
        self.cooperation_threshold = math.ceil(self.num_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_t = state.round_number + 1
        if round_t == 1:
            return Action.C
        elif round_t == self.total_rounds:
            return Action.D
        else:
            my_prev_action = history.my_actions[-1]
            opponent_prev_actions = history.opponent_actions[-1, :]
            num_cooperators_previous_round = int(my_prev_action) + np.sum(opponent_prev_actions)
            if num_cooperators_previous_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_269(BaseStrategy):
    """
    Adaptive Collective Reciprocator (ACR) strategy for the N-Player Public Goods Game.

    ACR promotes cooperation by starting cooperatively and then reciprocating
    observed levels of cooperation from other players. It uses a strategically
    derived threshold (T_coop) to determine if collective cooperation is
    sufficient to warrant continued individual cooperation. To prevent exploitation,
    it defects if cooperation falls below this threshold and also applies a standard
    end-game defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        T_coop_unconstrained = math.ceil(self.n / self.k)
        self.T_coop = max(1, min(self.n, T_coop_unconstrained))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num_1_indexed = state.round_number + 1
        if current_round_num_1_indexed == 1:
            return Action.C
        if current_round_num_1_indexed == self.r:
            return Action.D
        else:
            C_prev_round = sum(history.opponent_actions[-1, :])
            if history.my_actions[-1]:
                C_prev_round += 1
            if C_prev_round >= self.T_coop:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_270(BaseStrategy):
    """
    Collective Consensus Builder (CCB) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and sustain collective cooperation by starting with
    cooperation, dynamically monitoring group commitment via a Minimum Cooperation Threshold (MCT),
    and conditionally punishing or rewarding. It also accounts for end-game rationality.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_1_indexed = state.round_number + 1
        if round_1_indexed == 1:
            return Action.C
        if round_1_indexed == self.n_rounds:
            return Action.D
        denominator = self.n_players - 1
        mct_scaled_k_factor = (self.k_factor - 1) / denominator
        mct_value_unfloored = self.n_players * (1 - mct_scaled_k_factor)
        MCT = max(1, math.floor(mct_value_unfloored))
        cooperators_from_opponents_in_prev_round = sum(history.opponent_actions[-1, :])
        total_cooperators_in_previous_round = cooperators_from_opponents_in_prev_round + history.my_actions[-1]
        if total_cooperators_in_previous_round >= MCT:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_271(BaseStrategy):
    """
    The Adaptive Collective Responder (ACR) strategy aims to foster cooperation
    by being initially generous and then conditionally reciprocating. It uses
    an adaptive threshold (T_coop) to tolerate a certain level of defection,
    adjusting to the public good's multiplication factor (k). It begins by
    cooperating, punishes widespread defection in intermediate rounds, and
    defects in the final round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.T_coop = max(2, round(self.n_players * (1 - 1 / self.k)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.n_rounds - 1:
            return Action.D
        else:
            cooperators_in_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if cooperators_in_prev_round >= self.T_coop:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_272(BaseStrategy):
    """
    Adaptive Community Enforcer (ACE) strategy for the N-Player Public Goods Game.
    This strategy starts cooperatively, rewards sufficient group cooperation,
    punishes moderate defection, and triggers a sustained 'grim-trigger'-like
    punishment for severe defection, with a forgiveness mechanism to prevent
    permanent lock-in to defection. It defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.Cooperation_Threshold_Count = max(1, math.floor(n / 2))
        self.Defection_Tolerance_Count = max(0, math.floor(n / 4))
        self.Forgiveness_Rounds = max(2, math.floor(r / 5))
        self.rounds_since_defection_trigger = 0
        self.persistent_defection_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == r:
            return Action.D
        if current_round_1_indexed == 1:
            self.rounds_since_defection_trigger = 0
            self.persistent_defection_mode = False
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[state.round_number - 1, :])
        if history.my_actions[state.round_number - 1]:
            num_cooperators_prev_round += 1
        if self.persistent_defection_mode:
            self.rounds_since_defection_trigger += 1
            if self.rounds_since_defection_trigger >= self.Forgiveness_Rounds:
                self.persistent_defection_mode = False
                self.rounds_since_defection_trigger = 0
                return Action.C
            else:
                return Action.D
        elif num_cooperators_prev_round >= self.Cooperation_Threshold_Count:
            return Action.C
        elif num_cooperators_prev_round < self.Defection_Tolerance_Count:
            self.persistent_defection_mode = True
            self.rounds_since_defection_trigger = 0
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_273(BaseStrategy):
    """
    The Adaptive Collective Cooperator (ACC) strategy is designed for the N-Player Public Goods Game
    to foster collective cooperation while remaining robust against exploitation in a tournament setting.
    It balances initial goodwill with conditional response to ensure sustainable cooperation when possible,
    and self-preservation when not.

    The strategy dynamically calculates a Cooperation Threshold (T_coop) based on game parameters (n, k).
    It cooperates in the first round to probe for cooperation, and in subsequent middle rounds,
    it conditionally cooperates if the number of cooperators in the previous round met or exceeded T_coop.
    In the final round, it defects to maximize payoff due to the end-game effect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.T_coop = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        cooperators_in_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if cooperators_in_prev_round >= self.T_coop:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_274(BaseStrategy):
    """
    Adaptive Reciprocity: This strategy aims to foster and maintain cooperation.
    It initiates with cooperation, then adapts its behavior based on the collective
    cooperation observed in the previous round, using a threshold. For the final
    round, it defects based on end-game rationality.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.C_threshold = math.ceil(self.game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        num_cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_prev_round >= self.C_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_275(BaseStrategy):
    """
    Parameter-Adaptive Collective Reciprocity (PACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain collective cooperation by being responsive to the observed
    behavior of other players, while also protecting itself from sustained exploitation. Its parameters
    are dynamically adjusted based on the game's efficiency factor `k` and the number of players `n`,
    ensuring robustness across a wide range of game configurations.

    Core Principle:
    PACR initiates cooperation to explore the potential for collective good. It then continuously
    monitors the group's cooperation level, adapting its own behavior to reciprocate observed
    cooperation and to penalize sustained defection. Its responsiveness is tuned by game parameters
    `k` and `n`, allowing it to be stricter when collective action is less efficient (low `k`) and
    more forgiving when it's highly efficient (high `k`).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        self.COOPERATION_THRESHOLD = max(0.2, 1 - 0.5 * (k - 1) / (n - 1))
        self.FORGIVENESS_LIMIT = 1
        self.consecutive_rounds_below_threshold = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.consecutive_rounds_below_threshold = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_prev_action_was_cooperate = int(history.my_actions[-1])
        opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = my_prev_action_was_cooperate + opponent_cooperators_prev_round
        observed_cooperation_rate = total_cooperators_prev_round / self.game_description.n_players
        if observed_cooperation_rate >= self.COOPERATION_THRESHOLD:
            self.consecutive_rounds_below_threshold = 0
            return Action.C
        else:
            self.consecutive_rounds_below_threshold += 1
            if self.consecutive_rounds_below_threshold <= self.FORGIVENESS_LIMIT:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_276(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy combines conditional cooperation with dynamic thresholding and a
    measured punishment mechanism. It starts by cooperating, then dynamically adjusts
    its tolerance level based on the observed proportion of cooperators in the group.
    If cooperation falls below this adaptive threshold, the strategy enters a temporary
    punishment phase by defecting. It also employs a rational defection in the very
    last round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.current_tolerance_level_C = max(0.1, min(0.9, 1.0 / self.game_description.k))
        self.punishment_mode_active = False
        self.punishment_rounds_left = 0
        self.punishment_duration = 2
        self.tolerance_adjust_step = 0.05
        self.cooperation_buffer = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        if self.punishment_mode_active:
            self.punishment_rounds_left -= 1
            if self.punishment_rounds_left > 0:
                return Action.D
            else:
                self.punishment_mode_active = False
        my_prev_action_bool = history.my_actions[state.round_number - 1]
        opponent_prev_cooperators = np.sum(history.opponent_actions[state.round_number - 1, :])
        C_prev = int(my_prev_action_bool) + opponent_prev_cooperators
        observed_cooperation_proportion = C_prev / n_players
        if observed_cooperation_proportion >= self.current_tolerance_level_C + self.cooperation_buffer:
            self.current_tolerance_level_C = min(0.9, self.current_tolerance_level_C + self.tolerance_adjust_step)
        elif observed_cooperation_proportion <= self.current_tolerance_level_C - self.cooperation_buffer:
            self.current_tolerance_level_C = max(0.1, self.current_tolerance_level_C - self.tolerance_adjust_step)
        if observed_cooperation_proportion < self.current_tolerance_level_C:
            self.punishment_mode_active = True
            self.punishment_rounds_left = self.punishment_duration
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_277(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and maintain collective cooperation by being initially generous,
    conditionally cooperative based on the group's prior performance, and rationally self-preserving
    in the end game. It dynamically adjusts its cooperation threshold based on game parameters n and k.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        my_cooperation_prev_round = 1 if history.my_actions[-1] else 0
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_cooperation_prev_round
        if total_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_278(BaseStrategy):
    """
    Collective Threshold Reciprocity (CTR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and sustain collective cooperation by initiating
    contributions, rewarding sufficient collective effort (defined by a dynamic
    threshold based on n and k), and punishing insufficient contributions. It
    defects in the final round based on backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        my_prev_action_is_cooperate = history.my_actions[state.round_number - 1]
        opponent_prev_actions = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_prev_round = int(my_prev_action_is_cooperate) + np.sum(opponent_prev_actions)
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_279(BaseStrategy):
    """
    Adaptive Collective Threshold (ACT) strategy for the N-Player Public Goods Game.
    This strategy starts by cooperating, then adjusts its behavior based on the
    collective cooperation level in the previous round, using a pre-calculated
    threshold (T_coop). It defects in the last round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.T_coop = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        previous_round_idx = state.round_number - 1
        m_prev_opponents = np.sum(history.opponent_actions[previous_round_idx, :])
        my_prev_action_bool = history.my_actions[previous_round_idx]
        my_prev_action_int = 1 if my_prev_action_bool else 0
        m_prev = m_prev_opponents + my_prev_action_int
        if m_prev >= self.T_coop:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_280(BaseStrategy):
    """
    Collective Reciprocity with Minimum Viable Cooperation (CR-MVC) strategy for the N-Player Public Goods Game.
    It initiates cooperation in Round 1, defects in the final round (r), and for intermediate rounds (2 to r-1),
    it cooperates if the number of cooperators in the preceding round met or exceeded a dynamically calculated
    Minimum Viable Cooperation Threshold (T_MVC = ceil(n / k)); otherwise, it defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.t_mvc_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev_round >= self.t_mvc_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_281(BaseStrategy):
    """
    Collective Reciprocity with Adaptive Punishment strategy for the N-Player Public Goods Game.
    This strategy aims to maximize collective payoff by encouraging widespread cooperation.
    It starts with a cooperative stance and then adapts its behavior based on the observed
    level of collective cooperation over time, implementing a tiered punishment system for
    insufficient cooperation. It explicitly handles the end-game scenario through backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.total_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_history = []
        self.COOPERATION_TARGET = self.n_players - 1
        self.FORGIVENESS_THRESHOLD = max(1, math.ceil(self.k_factor))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        n_c_last_completed_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        self.cooperation_history.append(n_c_last_completed_round)
        if current_round_number == self.total_rounds - 1:
            return Action.D
        avg_N_C_so_far = sum(self.cooperation_history) / len(self.cooperation_history)
        if avg_N_C_so_far >= self.COOPERATION_TARGET:
            return Action.C
        elif avg_N_C_so_far >= self.FORGIVENESS_THRESHOLD:
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_282(BaseStrategy):
    """
    The Adaptive Collective Reciprocity (ACR) strategy aims to foster high levels of cooperation by:
    1. Initiating cooperation: Starting cooperatively to signal willingness to contribute.
    2. Responding to collective action: Adapting its behavior based on the observed number of
       cooperators in the previous round, using a parameter-derived threshold.
    3. Protecting against exploitation: Defecting when collective cooperation falls below a
       sustainable level.
    4. Adhering to game theory principles: Defecting in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.T_min_cooperators = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        previous_round_idx = state.round_number - 1
        my_cooperation_prev = int(history.my_actions[previous_round_idx])
        opponent_cooperations_prev = np.sum(history.opponent_actions[previous_round_idx, :])
        n_c_prev = my_cooperation_prev + opponent_cooperations_prev
        if n_c_prev >= self.T_min_cooperators:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_283(BaseStrategy):
    """
    Adaptive Majority Reciprocity (AMR) strategy for the N-Player Public Goods Game.
    It initiates cooperation in Round 1, then conditionally reciprocates based on a majority
    threshold of cooperators in the previous round, and defects in the final round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.majority_threshold = math.floor(self.n_players / 2) + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on AMR rules.

        Args:
            state (GameState): The current state of the game, including the current round number (0-indexed).
            history (None | PlayerHistory): Past actions and payoffs for all players.
                                           None if it's the very first round of the game.

        Returns:
            Action: The chosen action for the current round (Action.C for Cooperate, Action.D for Defect).
        """
        current_strategy_round = state.round_number + 1
        if current_strategy_round == 1:
            return Action.C
        if current_strategy_round == self.n_rounds:
            return Action.D
        previous_round_idx = state.round_number - 1
        num_cooperators_previous_round = np.sum(history.opponent_actions[previous_round_idx, :]) + int(history.my_actions[previous_round_idx])
        if num_cooperators_previous_round >= self.majority_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_285(BaseStrategy):
    """
    Adaptive Majority-Rule Reciprocator (AMR) strategy for the N-Player Public Goods Game.
    This strategy starts by cooperating, then adaptively adjusts its behavior based on
    whether at least 50% of players cooperated in the previous round. It defects in the final round.
    """
    COOPERATION_THRESHOLD = 0.5

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialises the strategy with game parameters.

        Args:
            game_description (PublicGoodsDescription): An object containing
                                                       n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the AMR strategy.

        Args:
            state (GameState): An object containing the current round_number (0-indexed).
            history (PlayerHistory | None): An object containing historical actions and payoffs.
                                            This will be None for the very first round (round_number == 0).

        Returns:
            Action: The chosen action (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == total_rounds - 1:
            return Action.D
        else:
            prev_round_idx = current_round_0_indexed - 1
            num_cooperators_prev_round = int(history.my_actions[prev_round_idx]) + np.sum(history.opponent_actions[prev_round_idx, :])
            cooperation_rate = num_cooperators_prev_round / n_players
            if cooperation_rate >= self.COOPERATION_THRESHOLD:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_286(BaseStrategy):
    """
    Adaptive Collective Reciprocity with Dynamic Threshold strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain collective cooperation by initiating with a cooperative
    gesture and then adapting its behavior based on the observed level of cooperation in the
    previous round. It calculates a "Minimum Acceptable Cooperation Level" (MACL) dynamically
    based on the game's efficiency factor 'k'. It also accounts for the first and last rounds
    with specific rules.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.macl = math.floor(self.game_description.n_players / self.game_description.k)
        if self.macl < 1:
            self.macl = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds_0_indexed = self.game_description.n_rounds - 1
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == total_rounds_0_indexed:
            return Action.D
        else:
            num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
            my_action_prev = history.my_actions[-1]
            num_cooperators_prev = num_opponent_cooperators_prev + (1 if my_action_prev else 0)
            if num_cooperators_prev >= self.macl:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_287(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy combines an initial trust-building phase with a conditional response mechanism
    based on observed collective actions, and an endgame strategy informed by backward induction.
    It aims to foster cooperation while preventing individual exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.r:
            return Action.D
        else:
            prev_round_0_indexed = state.round_number - 1
            num_other_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_0_indexed, :])
            reciprocity_threshold = math.ceil((self.n - 1) * (self.k / self.n))
            if num_other_cooperators_prev_round >= reciprocity_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_288(BaseStrategy):
    """
    Robust Adaptive Collective Strategy (R-ACS) for N-Player Public Goods Game.

    This strategy aims to maximize the collective payoff by fostering cooperation,
    while being adaptive to opponent behavior and robust against exploitation.
    It balances the desire for collective good with the need for individual protection
    in a competitive environment.

    Key features:
    - Starts with cooperation (optimistic start).
    - Defects in the last round to avoid exploitation (endgame effect).
    - Uses a COOPERATION_THRESHOLD to conditionally cooperate based on past group contributions.
    - Implements a defection phase as a punishment mechanism when cooperation is low.
    - Incorporates a FORGIVENESS_INTERVAL to attempt re-establishing cooperation after
      a period of defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        k = game_description.k
        self.COOPERATION_THRESHOLD = math.floor(n * (1 - (k - 1) / (n - 1)))
        self.COOPERATION_THRESHOLD = max(1, self.COOPERATION_THRESHOLD)
        self.FORGIVENESS_INTERVAL = math.floor(r / 5)
        self.FORGIVENESS_INTERVAL = max(1, self.FORGIVENESS_INTERVAL)
        self.consecutive_defection_rounds = 0
        self.is_in_defection_phase = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_idx == 0:
            self.consecutive_defection_rounds = 0
            self.is_in_defection_phase = False
            return Action.C
        if current_round_idx == total_rounds - 1:
            return Action.D
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if self.is_in_defection_phase:
            self.consecutive_defection_rounds += 1
            if self.consecutive_defection_rounds >= self.FORGIVENESS_INTERVAL:
                self.is_in_defection_phase = False
                self.consecutive_defection_rounds = 0
                return Action.C
            else:
                return Action.D
        elif previous_round_cooperators >= self.COOPERATION_THRESHOLD:
            self.consecutive_defection_rounds = 0
            return Action.C
        else:
            self.is_in_defection_phase = True
            self.consecutive_defection_rounds = 1
            return Action.D

class Strategy_COLLECTIVE_289(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy balances proactive cooperation with self-protection and strategic deterrence.
    It initiates cooperation, sustains it through conditional reciprocity based on group behavior,
    and protects against exploitation when cooperation breaks down or when the game nears its end.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold_proportion = (self.n_players - self.k_factor) / (self.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        observed_cooperation_proportion = num_cooperators_prev_round / self.n_players
        if observed_cooperation_proportion >= self.cooperation_threshold_proportion:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_290(BaseStrategy):
    """
    The "Adaptive Threshold Reciprocity" strategy aims to foster cooperation for the
    collective good while protecting against exploitation. It adapts its behavior
    based on observed past actions of all players, using a clear, parameter-driven
    threshold to determine the viability and fairness of continued cooperation.

    Core Principles:
    1.  Initial Trust: Start by cooperating in the first round.
    2.  Reciprocal Threshold: Continue cooperating if the number of cooperators
        in the previous round met or exceeded a calculated threshold (ceil(n / k)).
    3.  Punishment & Reassessment: Defect if cooperation falls below the threshold,
        acting as a signal for insufficient collective effort, but remain willing
        to re-engage if conditions improve.
    4.  End-Game Rationality: Defect in the final round to maximize individual payoff
        when there are no future interactions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == self.game_description.n_rounds:
            return Action.D
        previous_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        my_previous_action_was_cooperate = history.my_actions[-1]
        total_cooperators_in_previous_round = previous_round_opponent_cooperators + (1 if my_previous_action_was_cooperate else 0)
        if total_cooperators_in_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_291(BaseStrategy):
    """
    The Collective Reciprocator strategy for the N-Player Public Goods Game.
    It initiates cooperation, then critically evaluates the collective effort
    in previous rounds against a dynamic cooperation threshold. It continues
    to cooperate if sufficient collective effort was observed and defects otherwise,
    to prevent exploitation and signal the necessity of collective participation.
    It defects in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.C_thresh = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            num_cooperators_previous_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if num_cooperators_previous_round >= self.C_thresh:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_292(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain cooperation by balancing pro-social initiation
    with a clear, game-theoretic understanding of when to withhold cooperation.

    Core principles:
    1.  **Initial Cooperation:** Always cooperates in the first round to signal willingness
        to contribute and encourage a high-cooperation equilibrium.
    2.  **Adaptive Reciprocity:** In intermediate rounds, it monitors the total number of
        cooperators in the previous round (C_t-1). It calculates a Cooperation Threshold
        Count (CTC = ceil(n / k)).
        - If C_t-1 is at or above CTC, the strategy cooperates to reward and sustain the
          collective effort, aiming for higher aggregate payoffs.
        - If C_t-1 falls below CTC, the strategy defects. This acts as a punishment or
          self-protective measure, signaling that higher cooperation is required.
    3.  **Final Round Defection:** Defects in the final round due to backward induction.
        In the absence of future interactions, the individual incentive to defect becomes
        dominant, safeguarding against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.CTC = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        if current_round_zero_indexed == 0:
            return Action.C
        if current_round_zero_indexed == self.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        my_action_prev_round_was_cooperate = history.my_actions[-1]
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_cooperate else 0)
        if total_cooperators_prev_round >= self.CTC:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_293(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by starting cooperatively and then
    adapting its behavior based on the collective cooperation level in previous rounds.
    It cooperates if the number of cooperators meets a calculated threshold (ceil(n/k))
    and defects otherwise, to prevent exploitation. It also accounts for the
    final round's unique incentives by defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ACR strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on ACR rules.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs. None for the first round.

        Returns:
            An Action enum (C for Cooperate, D for Defect).
        """
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_index = current_round_number - 1
        my_prev_action_was_C = history.my_actions[previous_round_index]
        opponents_prev_actions = history.opponent_actions[previous_round_index, :]
        num_cooperators_prev_round = int(my_prev_action_was_C) + np.sum(opponents_prev_actions)
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_294(BaseStrategy):
    """
    Collective Paced Reciprocity (CPR) strategy for N-Player Public Goods Game.

    This strategy aims to foster widespread cooperation by starting cooperatively,
    then adapting based on the observed collective effort of the group in the
    previous round. It uses a clear majority threshold to determine if sufficient
    cooperation occurred to continue contributing. It defects in the last round
    due to backward induction to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = self.game_description.n_players // 2 + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        previous_round_index = current_round_0_indexed - 1
        num_opponent_cooperators_prev_round = sum(history.opponent_actions[previous_round_index, :])
        my_cooperation_prev_round = int(history.my_actions[previous_round_index])
        num_total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_cooperation_prev_round
        if num_total_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_295(BaseStrategy):
    """
    The Adaptive Conditional Cooperator (ACC) strategy aims to foster and sustain collective cooperation.
    It starts cooperatively, conditionally reciprocates based on observed group behavior, and defects
    in the final round due to the game's finite nature.

    Key principles:
    - Initial Trust: Cooperate in the first round.
    - Conditional Reciprocity: Continue cooperating if a sufficient number of other players
      cooperated in the previous round (defined by T_coop). Otherwise, defect.
    - Rational Self-Preservation: Defect in the final round.

    T_coop: The minimum number of *other players* who must have cooperated in the previous round
    for ACC to continue cooperating. Calculated as ceil((n - 1) / 2).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_coop = math.ceil((self.game_description.n_players - 1) / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
        num_others_cooperated = sum(previous_round_opponent_actions)
        if num_others_cooperated >= self.T_coop:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_296(BaseStrategy):
    """
    The Collective Cooperator aims to encourage a high level of overall cooperation.
    It initiates cooperation in the first round, defects in the final round, and
    conditionally cooperates in intermediate rounds based on whether the total
    number of cooperators in the immediately preceding round met a calculated threshold.
    This threshold (ceil(n/k)) represents the minimum collective contribution required
    for cooperation to be economically viable from a group perspective.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters and calculates the cooperation threshold.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.threshold_c = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing actions and payoffs from previous rounds,
                     or None if it's the very first round.

        Returns:
            An Action (C for Cooperate, D for Defect).
        """
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        previous_round_opponent_cooperators = sum(history.opponent_actions[-1, :])
        previous_round_my_action_cooperated = history.my_actions[-1]
        total_cooperators_in_previous_round = previous_round_opponent_cooperators + int(previous_round_my_action_cooperated)
        if total_cooperators_in_previous_round >= self.threshold_c:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_297(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy aims to foster cooperation and achieve high collective payoffs
    while being adaptive, robust against exploitation, and designed for a competitive tournament environment.
    It operates on the principle of conditional cooperation based on the observed collective behavior of the group,
    using a derived cooperation threshold and an endgame defection rule.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
        my_action_prev_round = history.my_actions[state.round_number - 1]
        num_cooperators_prev_round = num_opponent_cooperators_prev_round + int(my_action_prev_round)
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_298(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) is designed to foster cooperation in the N-Player
    Public Goods Game. It initiates cooperation, then adapts its behavior based on the
    proportion of cooperators observed in the previous round and the game's efficiency (k).
    The strategy aims to maximize collective payoff by encouraging and sustaining cooperation,
    while strategically defecting when cooperation is not reciprocated or in the final round.

    Core Philosophy:
    ACR operates on the principle of conditional cooperation, willing to contribute as long
    as a sufficient proportion of other players also contribute. The definition of "sufficient"
    is adaptively determined by the game's efficiency parameter (k), becoming more lenient
    when collective benefit is high (high k) and stricter when it's low (low k).
    It prioritizes establishing collective benefit but is designed to protect itself from exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Collective Reciprocity strategy.

        Args:
            game_description: An object containing game parameters such as n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.threshold_proportion = 1 / self.game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs for the player and opponents.
                     It is None for the very first round (round_number = 0).

        Returns:
            An Action enum value (Action.C for Cooperate, Action.D for Defect).
        """
        current_1_indexed_round = state.round_number + 1
        if current_1_indexed_round == 1:
            return Action.C
        if current_1_indexed_round == self.game_description.n_rounds:
            return Action.D
        num_opponent_cooperators_previous_round = sum(history.opponent_actions[-1, :])
        my_action_previous_round_is_C = int(history.my_actions[-1])
        total_cooperators_previous_round = num_opponent_cooperators_previous_round + my_action_previous_round_is_C
        proportion_cooperators_previous_round = total_cooperators_previous_round / self.game_description.n_players
        if proportion_cooperators_previous_round > self.threshold_proportion:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_299(BaseStrategy):
    """
    The Adaptive Collective Reciprocity (ACR) strategy aims to promote and sustain
    cooperation in the N-Player Public Goods Game. It operates on a philosophy of
    initial trust, conditional cooperation, and strategic punishment, calibrated
    by the game's multiplication factor 'k'.

    - **Round 1:** Always cooperates, extending trust.
    - **Last Round:** Always defects, as there are no future interactions for reciprocity.
    - **Intermediate Rounds:** Observes the proportion of cooperators (P_t-1) in the
      previous round. It continues to cooperate if P_t-1 meets or exceeds a dynamic
      threshold (1.0 / k); otherwise, it defects to signal and punish insufficient
      cooperation. The threshold adapts to 'k', being more forgiving when 'k' is high
      (high collective returns) and stricter when 'k' is low (low collective returns).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = 1.0 / self.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
            my_action_prev_round_is_C = history.my_actions[state.round_number - 1]
            num_my_cooperators_prev_round = 1 if my_action_prev_round_is_C else 0
            total_cooperators_prev_round = num_opponent_cooperators_prev_round + num_my_cooperators_prev_round
            cooperation_proportion_prev_round = total_cooperators_prev_round / self.n_players
            if cooperation_proportion_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_300(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    Starts with cooperation, maintains it if a sufficient collective threshold is met
    in the previous round. Punishes insufficient collective cooperation by defecting,
    and escalates to permanent defection if non-cooperation is sustained.
    Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_count = math.ceil(self.n_players / 2)
        self.tolerance_rounds = 2
        self.consecutive_low_cooperation_rounds = 0
        self.permanently_defect_flag = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number
        if current_round_t == 0:
            return Action.C
        elif current_round_t == self.n_rounds - 1:
            return Action.D
        else:
            if self.permanently_defect_flag:
                return Action.D
            my_prev_action_cooperated = history.my_actions[current_round_t - 1]
            opponent_prev_cooperators_count = np.sum(history.opponent_actions[current_round_t - 1, :])
            total_cooperators_in_prev_round = int(my_prev_action_cooperated) + opponent_prev_cooperators_count
            if total_cooperators_in_prev_round >= self.cooperation_threshold_count:
                self.consecutive_low_cooperation_rounds = 0
                return Action.C
            else:
                self.consecutive_low_cooperation_rounds += 1
                if self.consecutive_low_cooperation_rounds > self.tolerance_rounds:
                    self.permanently_defect_flag = True
                    return Action.D
                else:
                    return Action.D

class Strategy_COLLECTIVE_301(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster cooperation by being initially cooperative, conditionally cooperative
    based on a majority rule, and defensively rational in the endgame.

    Core Principles:
    1. Initial Trust: Cooperate in the first round to signal willingness to contribute.
    2. Collective Reciprocity: Continue cooperating if a significant portion (majority, ceil(n/2))
       of players cooperated in the previous round, fostering sustained collective action.
    3. Punishment & Deterrence: Defect if previous collective cooperation falls below the threshold,
       signaling disapproval of widespread free-riding and incentivizing a return to collective effort.
    4. Endgame Rationality: Defect in the final round to prioritize individual payoff,
       acknowledging the lack of future interactions to influence behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_302(BaseStrategy):
    """
    The Adaptive Collective Enforcer (ACE) strategy for the N-Player Public Goods Game.

    ACE aims to foster and maintain a cooperative equilibrium by starting with a
    pro-social signal and then adapting its behavior based on the observed level of
    collective cooperation in previous rounds. It rewards sufficient cooperation
    (defined as at least ceil(n/2) cooperators) to sustain group welfare and punishes
    widespread defection to avoid exploitation. It makes a pragmatic defection in
    the final round, consistent with backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = math.ceil(self.n_players / 2.0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num_0_indexed = state.round_number
        if current_round_num_0_indexed == 0:
            return Action.C
        elif current_round_num_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            total_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if total_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_303(BaseStrategy):
    """
    Adaptive Collective Threshold (ACT) strategy for an N-Player Public Goods Game.
    This strategy aims to foster cooperation by initiating with a 'C', then
    adaptively cooperating if a sufficient proportion (50% or more) of players
    cooperated in the previous round, and finally defecting in the last round
    due to end-game effects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        cooperators_in_prev_round_opponents = np.sum(history.opponent_actions[-1, :])
        my_action_in_prev_round_is_c = history.my_actions[-1]
        cooperators_in_prev_round_total = cooperators_in_prev_round_opponents + (1 if my_action_in_prev_round_is_c else 0)
        observed_cooperation_rate = cooperators_in_prev_round_total / self.n_players
        if observed_cooperation_rate >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_304(BaseStrategy):
    """
    Adaptive Threshold Reciprocity (ATR) strategy for N-Player Public Goods Game.
    ATR initiates cooperation as a gesture of trust. It dynamically adjusts its
    expectation for collective cooperation based on the game's parameters (n and k),
    using a threshold: `max(1, floor(n - k))`. It sustains cooperation if the
    observed number of cooperators in the previous round meets or exceeds this
    threshold. If not, it defects to protect against exploitation. In the final
    round, it always defects due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        my_action_prev_bool = history.my_actions[-1]
        C_prev = int(num_opponent_cooperators_prev) + int(my_action_prev_bool)
        required_cooperators_threshold = max(1, math.floor(self.n_players - self.k_factor))
        if C_prev >= required_cooperators_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_305(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy:
    Starts by cooperating, then adapts by cooperating if previous round's cooperation
    level met a dynamic threshold (floor(n/k)+1), otherwise defects.
    Defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.c_threshold = math.floor(self.game_description.n_players / self.game_description.k) + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            my_prev_action_cooperated = history.my_actions[-1]
            opponent_prev_actions_cooperated = history.opponent_actions[-1, :]
            num_cooperators_prev_round = (1 if my_prev_action_cooperated else 0) + np.sum(opponent_prev_actions_cooperated)
            if num_cooperators_prev_round >= self.c_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_306(BaseStrategy):
    """
    Adaptive Reciprocity (AR) strategy for the N-Player Public Goods Game.

    This strategy aims to initiate and maintain cooperation within the group by contributing
    when others do, and withdrawing contributions (punishing) when collective effort
    falls short. It starts with cooperation, defects in the final round, and in
    intermediate rounds, it cooperates if the number of cooperators in the previous
    round meets or exceeds a calculated CooperationThreshold (CT). The CT dynamically
    adjusts based on the game's efficiency (n/k ratio).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round == 0:
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        C_prev = int(history.my_actions[-1])
        C_prev += sum(history.opponent_actions[-1, :])
        if C_prev >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_307(BaseStrategy):
    """
    Parameter-Adaptive Reciprocal Cooperation strategy for the N-Player Public Goods Game.

    This strategy aims to establish and maintain a high level of cooperation by
    starting with a cooperative signal, dynamically adjusting its behavior based
    on the observed collective contribution level, and reverting to self-interest
    in the final round of a finitely repeated game.

    Decision Rules:
    - Round 1 (t=1): Always Cooperate (C) as an initial signal of trust.
    - Rounds 2 to (r-1): Adaptively Cooperate if the total number of cooperators
      in the previous round ($N_{C,t-1}$) was greater than or equal to the game
      parameter `k`. Otherwise, Defect (D).
    - Round r (t=r): Always Defect (D) due to the terminal round effect in
      finitely repeated games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        previous_round_0_indexed = state.round_number - 1
        my_prev_action_bool = history.my_actions[previous_round_0_indexed]
        opponent_prev_actions_sum = np.sum(history.opponent_actions[previous_round_0_indexed, :])
        n_c_previous_round = my_prev_action_bool + opponent_prev_actions_sum
        if n_c_previous_round >= self.k:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_308(BaseStrategy):
    """
    The "Adaptive Reciprocator" strategy aims to foster and sustain cooperation.
    It initiates cooperation in the first round. In intermediate rounds, it
    cooperates if at least 50% of all players (including itself) cooperated in
    the previous round; otherwise, it defects. In the final round, it defects
    to prevent exploitation. This strategy balances promoting collective good
    with self-preservation.
    """
    COOPERATION_THRESHOLD = 0.5

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.total_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.total_rounds - 1:
            return Action.D
        cooperators_count_opponents_prev = np.sum(history.opponent_actions[-1, :])
        cooperators_count_my_prev = int(history.my_actions[-1])
        total_cooperators_prev_round = cooperators_count_opponents_prev + cooperators_count_my_prev
        cooperation_rate_prev = total_cooperators_prev_round / self.n_players
        if cooperation_rate_prev >= self.COOPERATION_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_309(BaseStrategy):
    """
    The Adaptive Collective Reciprocity (ACR) strategy aims to foster cooperation and achieve high collective payoffs
    while being robust against exploitation. It adopts a conditional cooperation approach, starting with an attempt
    at full cooperation and then adapting its behavior based on the observed collective behavior of the group in
    previous rounds. It explicitly handles the edge cases of the first and last rounds to maximize its long-term
    effectiveness in a tournament setting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            prev_round_history_idx = state.round_number - 1
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_history_idx, :])
            my_action_prev_round_is_C = history.my_actions[prev_round_history_idx]
            C_prev = opponent_cooperators_prev_round + (1 if my_action_prev_round_is_C else 0)
            P_prev = C_prev / self.n_players
            P_threshold = max(0.5, 1 - self.k / self.n_players)
            if P_prev >= P_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_310(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by initiating with a cooperative stance
    and then reciprocating cooperation when a sufficient collective effort is observed.
    It protects itself from exploitation by defecting when cooperation levels are too low,
    and rationally defects in the final round when incentives for future cooperation vanish.

    Decision Rules:
    1.  First Round (t=1, state.round_number == 0): Cooperate (C)
        - Starts with an "olive branch" to signal willingness to cooperate.
    2.  Intermediate Rounds (1 < t < r, 0 < state.round_number < self.n_rounds - 1):
        - Calculates a Cooperation Threshold T = ceil(n / k).
        - Observes C_{t-1}, the total number of players who cooperated in the previous round.
        - If C_{t-1} >= T: Cooperate (C). Reciprocates observed collective cooperation.
        - Else (C_{t-1} < T): Defect (D). Punishes insufficient cooperation to avoid exploitation.
    3.  Last Round (t=r, state.round_number == self.n_rounds - 1): Defect (D)
        - Rational defection in the final round as there are no future interactions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        elif current_round_t == self.n_rounds:
            return Action.D
        else:
            previous_round_idx = state.round_number - 1
            my_prev_action_cooperated = int(history.my_actions[previous_round_idx])
            opponent_prev_cooperators = np.sum(history.opponent_actions[previous_round_idx, :])
            num_cooperators_previous_round = my_prev_action_cooperated + opponent_prev_cooperators
            if num_cooperators_previous_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_311(BaseStrategy):
    """
    Collective Adaptive Reciprocation (CAR) strategy for N-Player Public Goods Game.

    This strategy aims to promote cooperation while being robustly adaptive to the behavior
    of other players. It starts by cooperating, then adapts its behavior based on the
    observed level of cooperation from other players using a Minimum Viable Cooperation Threshold (MVCT).
    It also defects in the final round due to backward induction, ensuring individual rationality
    when no future interactions exist.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T_MV = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.r:
            return Action.D
        else:
            cooperators_in_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if cooperators_in_prev_round >= self.T_MV:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_312(BaseStrategy):
    """
    The Adaptive Collective Reciprocity (ACR) strategy aims to promote and sustain
    collective cooperation in the N-Player Public Goods Game. It initiates cooperation,
    monitors collective behavior, and employs a forgiving yet firm retaliatory
    mechanism against sustained non-cooperation. It explicitly handles the first
    and last rounds of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        if n == 2:
            self.cooperation_threshold = n
        else:
            self.cooperation_threshold = max(2, math.ceil(n * 0.6))
        self.defection_streak_limit = 1
        self.current_defection_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round == 0:
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        c_prev_my_action_int = int(history.my_actions[-1])
        c_prev_opponent_actions_sum = np.sum(history.opponent_actions[-1, :])
        C_prev = c_prev_my_action_int + c_prev_opponent_actions_sum
        if C_prev >= self.cooperation_threshold:
            self.current_defection_streak = 0
            return Action.C
        else:
            self.current_defection_streak += 1
            if self.current_defection_streak <= self.defection_streak_limit:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_313(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and maintain cooperation by responding to the
    collective behavior observed in previous rounds. It starts cooperatively,
    defects in the final round to prevent exploitation, and in intermediate
    rounds, it adjusts its behavior based on a dynamic cooperation threshold.
    This threshold is calculated considering the individual cost of contribution
    and a minimum expectation of collective effort (at least 50% cooperation).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        previous_round_idx = state.round_number - 1
        my_previous_action_was_C = history.my_actions[previous_round_idx]
        opponent_previous_actions_were_C = history.opponent_actions[previous_round_idx, :]
        cooperators_last_round = int(my_previous_action_was_C) + np.sum(opponent_previous_actions_were_C)
        proportion_cooperators_last_round = cooperators_last_round / self.n_players
        threshold_P = max(0.5, 1 - self.k / self.n_players)
        if proportion_cooperators_last_round > threshold_P:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_314(BaseStrategy):
    """
    Adaptive Conditional Cooperation (ACC) with Critical Mass Threshold strategy.

    This strategy aims to foster and maintain cooperation in the N-Player Public Goods Game.
    It initiates cooperation in the first round to signal willingness to contribute.
    In intermediate rounds, it conditions its action on the observed collective behavior from the
    previous round: it cooperates if the total number of cooperators met or exceeded a
    pre-defined 'critical mass' threshold, otherwise it defects.
    In the final round, it defects due to the 'endgame effect' where future consequences are absent.

    The critical mass threshold (T_cooperate) is calculated as ceil(n_players / k).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_cooperate = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            my_cooperation_prev_round = 1 if history.my_actions[-1] else 0
            total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_cooperation_prev_round
            if total_cooperators_prev_round >= self.T_cooperate:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_315(BaseStrategy):
    """
    The Adaptive Collective Reciprocity (ACR) strategy aims to foster cooperation
    in the N-Player Public Goods Game by initiating cooperation, maintaining it
    as long as a majority of players reciprocate, and resorting to defection
    when cooperation levels fall too low. It also accounts for the specific
    dynamics of the game's final round by defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialises the strategy with game parameters.

        Args:
            game_description (PublicGoodsDescription): An object containing
                                                     n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.total_rounds = game_description.n_rounds
        self.cooperation_threshold = (self.n_players + 1) // 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round
        based on the Adaptive Collective Reciprocity strategy rules.

        Args:
            state (GameState): An object containing the current round_number (0-indexed).
            history (None | PlayerHistory): An object containing arrays of past actions
                                            and payoffs for the player and opponents.
                                            None for the very first round (round_number = 0).

        Returns:
            Action: The chosen action, Action.C for Cooperate or Action.D for Defect.
        """
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.total_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        my_action_prev_round = history.my_actions[-1]
        opponent_actions_prev_round = history.opponent_actions[-1, :]
        num_cooperators_prev_round = sum(opponent_actions_prev_round)
        if my_action_prev_round:
            num_cooperators_prev_round += 1
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_316(BaseStrategy):
    """
    Collective Reciprocator with End-Game Defection is a strategy for the N-Player Public Goods Game.
    It initiates cooperation in the first round. In intermediate rounds, it reciprocates the collective
    behavior of other players, cooperating if at least 50% of them cooperated in the previous round,
    and defecting otherwise. In the final round, it defects due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            previous_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
            num_other_cooperators = np.sum(previous_round_opponent_actions)
            num_other_players = self.n_players - 1
            proportion_coop_others = num_other_cooperators / num_other_players
            if proportion_coop_others >= 0.5:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_317(BaseStrategy):
    """
    This strategy, "Collective Reciprocator with End-Game Adjustment", aims to establish and
    maintain cooperation by reciprocating collective contribution. It protects itself from
    exploitation and leverages the finite game horizon by defecting in the final round.
    It prioritizes the collective good as long as a significant portion of players
    (defined by C_threshold) are also contributing.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.C_threshold = math.floor(self.game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        previous_round_index = current_round_0_indexed - 1
        my_prev_action_is_cooperate = history.my_actions[previous_round_index]
        my_prev_coop_count = 1 if my_prev_action_is_cooperate else 0
        opponents_prev_actions_is_cooperate = history.opponent_actions[previous_round_index, :]
        opponents_prev_coop_count = np.sum(opponents_prev_actions_is_cooperate)
        C_prev = my_prev_coop_count + opponents_prev_coop_count
        if C_prev >= self.C_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_318(BaseStrategy):
    """
    The Adaptive Collective Productivity (ACP) strategy aims to foster and sustain collective cooperation by
    contributing to the public good when the group demonstrates sufficient productivity. It continuously
    monitors the collective contribution level from the previous round. If the group's cooperation rate meets
    a specific threshold derived from the game's parameters (1/k), the strategy cooperates, reinforcing the
    collective effort. If the cooperation rate falls below this threshold, signaling inadequate collective
    contribution or excessive free-riding, the strategy defects, acting as a generalized punishment to
    incentivize greater cooperation in subsequent rounds. The strategy initiates with cooperation and defects
    in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the AdaptiveCollectiveProductivity strategy.

        Args:
            game_description: An object containing game parameters like n_players, n_rounds, k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.T_coop = 1 / self.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the ACP strategy rules.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past game states, or None if it's the first round.

        Returns:
            An Action (C for Cooperate, D for Defect).
        """
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        my_action_prev = history.my_actions[current_round - 1]
        opponent_actions_prev = history.opponent_actions[current_round - 1, :]
        num_cooperators_prev = int(my_action_prev) + np.sum(opponent_actions_prev)
        P_coop_prev = num_cooperators_prev / self.n_players
        if P_coop_prev >= self.T_coop:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_319(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain cooperation by adapting its behavior
    based on the collective actions observed in the previous round. It starts
    by cooperating, then continues to cooperate if the number of cooperators
    in the previous round met or exceeded a simple majority threshold. If
    cooperation falls below this threshold, it defects. In the final round,
    it always defects to prevent exploitation due to the end-game effect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.COOPERATION_THRESHOLD_COUNT = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            previous_round_opponent_actions = history.opponent_actions[-1, :]
            my_action_prev_is_C = history.my_actions[-1]
            n_C_prev_total = np.sum(previous_round_opponent_actions) + (1 if my_action_prev_is_C else 0)
            if n_C_prev_total >= self.COOPERATION_THRESHOLD_COUNT:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_320(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR initiates with cooperation and then dynamically adjusts its behavior based on
    the observed level of collective cooperation in the previous round, using a threshold
    derived from the game's efficiency parameter `k`. It defects in the final round
    due to the absence of future interactions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Collective Reciprocity strategy.

        Args:
            game_description: An object containing game parameters like n_players, n_rounds, k.
        """
        self.game_description = game_description
        self.cooperation_threshold = self.game_description.n_players - math.floor(self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round based on ACR rules.

        Args:
            state: An object containing the current game state, including the round number
                   (0-indexed).
            history: An object containing the player's and opponents' actions and payoffs
                     from previous rounds. This will be None for the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        elif current_round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            my_previous_action_bool = history.my_actions[current_round_number - 1]
            opponents_previous_actions_bool = history.opponent_actions[current_round_number - 1, :]
            C_count_prev = int(my_previous_action_bool) + np.sum(opponents_previous_actions_bool)
            if C_count_prev >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_321(BaseStrategy):
    """
    Adaptive Reciprocity for Public Goods (ARPG) strategy.

    This strategy aims to foster and sustain collective cooperation by initiating goodwill,
    rewarding sufficient collective action, and punishing free-riding. It adapts to the
    specific game parameters (n, r, k) and observed historical behavior.

    - Starts with cooperation in the first round to signal willingness.
    - Defects in the final round, as there are no future interactions to incentivize cooperation.
    - In intermediate rounds, it calculates a cooperation threshold T = ceil(n / k).
      If the total number of cooperators in the previous round (C_prev) meets or exceeds T,
      the strategy cooperates. Otherwise, it defects to punish insufficient collective effort.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_T = int(math.ceil(self.n / self.k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.r - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        my_previous_action_cooperated = history.my_actions[-1]
        opponents_previous_cooperators = np.sum(history.opponent_actions[-1, :])
        C_prev = my_previous_action_cooperated + opponents_previous_cooperators
        if C_prev >= self.cooperation_threshold_T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_322(BaseStrategy):
    """
    Adaptive Reciprocity with Forgiveness (ARF) strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by starting cooperatively, reciprocating
    observed cooperation, punishing sustained defection, and offering periodic
    "forgiveness tests" to re-establish cooperation if the group has fallen into a
    defection trap. It also incorporates a pragmatic defection in the final round
    to maximize individual payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self._my_last_action: Action | None = None
        self._defection_strikes: int = 0
        self.coop_threshold: float = max(0.5, 1 - self.k / self.n)
        self.forgiveness_interval: int = max(1, self.r // 5)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_game_round = state.round_number + 1
        action_for_this_round: Action
        if current_game_round == 1:
            action_for_this_round = Action.C
        elif current_game_round == self.r:
            action_for_this_round = Action.D
        else:
            my_prev_action_is_cooperate = history.my_actions[-1]
            num_cooperators_last_round = np.sum(history.opponent_actions[-1, :]) + int(my_prev_action_is_cooperate)
            cooperation_proportion_observed = num_cooperators_last_round / self.n
            if cooperation_proportion_observed >= self.coop_threshold:
                action_for_this_round = Action.C
                self._defection_strikes = 0
            elif my_prev_action_is_cooperate:
                action_for_this_round = Action.D
                self._defection_strikes = 1
            else:
                self._defection_strikes += 1
                if self._defection_strikes > 0 and self._defection_strikes % self.forgiveness_interval == 0:
                    action_for_this_round = Action.C
                else:
                    action_for_this_round = Action.D
        self._my_last_action = action_for_this_round
        return action_for_this_round

class Strategy_COLLECTIVE_323(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster and maintain high levels of cooperation by:
    1. Initiating with a cooperative move to signal willingness for collective action.
    2. Adaptively reciprocating based on the observed group cooperation rate in the previous round.
    3. Using a parameter-aware threshold for cooperation, which dynamically adjusts based on the k/n ratio.
    4. Defecting in the final round to prevent exploitation (endgame protection).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        min_threshold = 0.5
        max_threshold = 0.8
        k_n_ratio = self.k / self.n_players
        raw_cooperation_threshold_percent = max_threshold - k_n_ratio * (max_threshold - min_threshold)
        self.cooperation_threshold_percent = max(min_threshold, min(max_threshold, raw_cooperation_threshold_percent))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            prev_round_0_indexed = state.round_number - 1
            my_prev_action = history.my_actions[prev_round_0_indexed]
            opponent_prev_actions = history.opponent_actions[prev_round_0_indexed, :]
            total_cooperators_prev_round = np.sum(opponent_prev_actions) + (1 if my_prev_action else 0)
            observed_cooperation_rate = total_cooperators_prev_round / self.n_players
            if observed_cooperation_rate >= self.cooperation_threshold_percent:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_324(BaseStrategy):
    """
    Adaptive Collective Threshold (ACT) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain collective cooperation by starting cooperatively,
    monitoring the collective effort in previous rounds, and reacting by either sustaining
    cooperation or defecting to protect against free-riders. It also accounts for the
    endgame problem by defecting in the final round.
    """
    CTP: float = 0.75

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ACT strategy with game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round based on
        the ACT strategy's rules.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs. It is None
                     for the very first round (round_number = 0).

        Returns:
            An Action (Action.C for Cooperate, Action.D for Defect).
        """
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        my_action_last_round_was_C = history.my_actions[-1]
        num_cooperators_last_round = num_opponent_cooperators_last_round + my_action_last_round_was_C
        observed_cooperation_proportion = num_cooperators_last_round / self.n_players
        if observed_cooperation_proportion >= self.CTP:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_325(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for N-Player Public Goods Game.

    This strategy aims to initiate cooperation, sustain it when a sufficient number of players
    contribute to the public good, and retaliate (by defecting) when collective cooperation
    falls below an acceptable threshold. It acknowledges the "last round problem" by defecting
    in the final round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialises the strategy with game parameters and calculates the cooperation threshold.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current game state, including the round number.
            history: An object containing the history of actions and payoffs for all players
                     up to the previous round, or None if it's the first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        total_rounds_0_indexed = self.game_description.n_rounds - 1
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds_0_indexed:
            return Action.D
        my_action_prev = history.my_actions[-1]
        opponent_actions_prev = history.opponent_actions[-1, :]
        num_cooperators_prev_round = int(my_action_prev) + np.sum(opponent_actions_prev).item()
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_326(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by initiating with a cooperative move.
    In subsequent intermediate rounds, it dynamically adapts its behavior based on
    a calculated cooperation threshold. If the collective cooperation rate from the
    previous round falls below this threshold, the strategy defects to avoid exploitation;
    otherwise, it continues to cooperate. For the final round, it defects as a
    result of backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold_ratio = (self.n_players - self.k_factor) / (self.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            num_cooperators_previous_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            observed_cooperation_rate = num_cooperators_previous_round / self.n_players
            if observed_cooperation_rate < self.cooperation_threshold_ratio:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_327(BaseStrategy):
    """
    K-Adaptive Collective Reciprocity strategy for N-Player Public Goods Game.

    This strategy aims to foster and sustain cooperation by initially cooperating,
    then adapting its behavior based on the collective cooperation level in the
    previous round. It defects if the total number of cooperators falls below a
    K-adaptive threshold (ceil(n/k)), and always defects in the final round
    to avoid exploitation due to the endgame effect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            num_cooperators_prev_round = history.my_actions[state.round_number - 1] + np.sum(history.opponent_actions[state.round_number - 1, :])
            if num_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_328(BaseStrategy):
    """
    The Adaptive Collective Reciprocator (ACR) strategy aims to foster and sustain widespread
    cooperation among players. It starts with cooperation, then conditionally reciprocates
    cooperation if a sufficient number of other players cooperate (a strict majority).
    In the final round, it defects to protect against exploitation due to the end-game effect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        self.cooperation_threshold_Tc = math.ceil((n - 1) / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == total_rounds - 1:
            return Action.D
        else:
            c_others_prev = sum(history.opponent_actions[-1, :])
            if c_others_prev >= self.cooperation_threshold_Tc:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_329(BaseStrategy):
    """
    Collective Reciprocator strategy for the N-Player Public Goods Game.

    This strategy aims to promote and sustain cooperation by:
    1. Cooperating in the first round to signal willingness.
    2. Maintaining cooperation if nearly all players (n-1 or n) cooperated in the previous round.
    3. Defecting if a significant number of players (2 or more) defected in the previous round,
       acting as a punishment mechanism.
    4. Defecting in the final round due to backward induction, ensuring robustness against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        elif state.round_number == r - 1:
            return Action.D
        else:
            previous_round_cooperators_count = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if previous_round_cooperators_count >= n - 1:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_330(BaseStrategy):
    """
    The "Graceful Punisher" strategy aims to foster cooperation in the N-Player Public Goods Game.
    It initiates cooperation and strives to maintain it, but remains robust against exploitation.
    It provides a grace period for a single defector (when N > 2) but punishes persistent lone
    defection or widespread defection. It adapts its behavior based on past observations and
    game parameters, ultimately defecting in the final round due to backward induction principles.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.lone_defector_rounds_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            self.lone_defector_rounds_count = 0
            return Action.C
        elif current_round_0_indexed == n_rounds - 1:
            self.lone_defector_rounds_count = 0
            return Action.D
        else:
            my_action_prev_round = int(history.my_actions[-1])
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            total_cooperators_prev_round = my_action_prev_round + opponent_cooperators_prev_round
            total_defectors_prev_round = n_players - total_cooperators_prev_round
            if total_defectors_prev_round == 0:
                self.lone_defector_rounds_count = 0
                return Action.C
            elif total_defectors_prev_round == n_players:
                self.lone_defector_rounds_count = 0
                return Action.D
            elif total_defectors_prev_round == 1 and n_players > 2:
                self.lone_defector_rounds_count += 1
                if self.lone_defector_rounds_count > 1:
                    return Action.D
                else:
                    return Action.C
            else:
                self.lone_defector_rounds_count = 0
                return Action.D

class Strategy_COLLECTIVE_331(BaseStrategy):
    """
    Adaptive Proportional Response (APR) strategy for the N-Player Public Goods Game.

    APR aims to establish and maintain cooperation by signalling willingness to contribute,
    rewarding observed collective cooperation, and penalizing insufficient collective effort.
    It uses a dynamically adjusted threshold based on the previous round's collective
    cooperation. If the proportion of cooperators falls below a certain level (1/k),
    it signals a lack of collective commitment, prompting a defection. If the cooperation
    level is sufficient, it continues to cooperate to sustain the public good.

    Decision Rules (using 0-indexed round_number):
    - Round 0 (Initial Round): Always Cooperate (C) as an initial signal of willingness.
    - Middle Rounds (0 < round_number < n_rounds - 1):
        - Calculate previous round's total proportion of cooperators (p_{t-1}).
        - The `Cooperation_Threshold` is set to `1/k`.
        - If `p_{t-1} >= Cooperation_Threshold`, then Cooperate (C) to reward collective effort.
        - Else (`p_{t-1} < Cooperation_Threshold`), then Defect (D) to penalize insufficient collective effort.
    - Last Round (round_number == n_rounds - 1): Always Defect (D) due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = 1.0 / self.k_factor

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        previous_round_opponent_cooperators = sum(history.opponent_actions[-1, :])
        previous_round_my_action_was_cooperation = history.my_actions[-1]
        total_cooperators_previous_round = previous_round_opponent_cooperators + previous_round_my_action_was_cooperation
        proportion_cooperators = total_cooperators_previous_round / self.n_players
        if proportion_cooperators >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_332(BaseStrategy):
    """
    Adaptive Reciprocal Contributor (ARC) strategy for the N-Player Public Goods Game.

    ARC aims to foster and sustain cooperation by initiating with cooperation,
    adapting to collective behavior, and protecting against exploitation.
    It leverages a dynamic cooperation threshold based on game parameters n and k,
    and applies backward induction for the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_number == 0:
            return Action.C
        if current_round_number == total_rounds - 1:
            return Action.D
        my_prev_action_cooperated = history.my_actions[current_round_number - 1]
        opponent_prev_cooperators = np.sum(history.opponent_actions[current_round_number - 1, :])
        C_prev = int(my_prev_action_cooperated) + int(opponent_prev_cooperators)
        if C_prev >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_333(BaseStrategy):
    """
    The Adaptive Collective Threshold Reciprocator (ACTR) strategy.

    This strategy aims to foster collective cooperation in the N-Player Public Goods Game.
    It starts by cooperating in the first round to signal willingness to collaborate.
    In intermediate rounds, it observes the total number of cooperators from the previous
    round and compares it to a dynamically calculated threshold (floor(n/k)). If the
    previous cooperation level exceeds this threshold, the strategy cooperates; otherwise,
    it defects to signal that higher collective contribution is needed.
    In the final round, it defects based on the principle of backward induction for finite games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = math.floor(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            my_action_prev_round = history.my_actions[state.round_number - 1]
            opponent_actions_prev_round = history.opponent_actions[state.round_number - 1, :]
            num_cooperators_prev_round = int(my_action_prev_round) + np.sum(opponent_actions_prev_round)
            if num_cooperators_prev_round > self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_334(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to bootstrap and maintain cooperation by starting cooperatively
    and then adjusting its behavior based on the observed collective cooperation rate
    from the previous round. It incorporates an adaptive threshold for cooperation,
    influenced by the game's multiplication factor 'k', and includes an endgame
    defection to prevent exploitation in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ACR strategy with game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self._threshold_scaling_factor = (self.k - 1) / (self.n_players - 1) * 0.4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the ACR strategy rules.

        Args:
            state: The current state of the game, including the current round number
                   (0-indexed).
            history: A PlayerHistory object containing past actions and payoffs
                     for all prior rounds. It is None for the very first round
                     (round_number == 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        cooperation_rate_prev = num_cooperators_prev_round / self.n_players
        adaptive_threshold = 0.5 + self._threshold_scaling_factor
        if cooperation_rate_prev >= adaptive_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_335(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by starting with a cooperative move,
    then conditionally cooperating based on a dynamic threshold derived from game
    parameters (n and k). It defects in the final round to avoid exploitation
    and whenever the observed collective cooperation falls below the calculated threshold.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == self.n_rounds - 1:
            return Action.D
        previous_round_idx = current_round_idx - 1
        my_action_prev_round = history.my_actions[previous_round_idx]
        opponent_actions_prev_round = history.opponent_actions[previous_round_idx, :]
        num_cooperators_prev_round = int(my_action_prev_round) + np.sum(opponent_actions_prev_round)
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_336(BaseStrategy):
    """
    The Adaptive Group Reciprocity (AGR) strategy for the N-Player Public Goods Game.
    It promotes collective cooperation by starting with cooperation, and then conditionally
    cooperating only if the group's prior cooperation level meets a dynamically calculated
    threshold (`C_min_threshold`). It defects in the final round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Group Reciprocity strategy, calculating the
        minimum cooperation threshold based on game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.C_min_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round
        based on the Adaptive Group Reciprocity strategy rules.

        Args:
            state: An object containing the current game state, including the round number.
            history: An object containing past actions and payoffs for this player and opponents.
                     It is None for the very first round (round_number == 0).

        Returns:
            An Action (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_number = state.round_number
        n_rounds = self.game_description.n_rounds
        if current_round_number == 0:
            return Action.C
        elif current_round_number == n_rounds - 1:
            return Action.D
        else:
            num_cooperators_prev_round = sum(history.opponent_actions[-1]) + history.my_actions[-1]
            if num_cooperators_prev_round >= self.C_min_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_337(BaseStrategy):
    """
    The "Adaptive Reciprocator" strategy aims to establish and maintain cooperation by starting
    cooperatively, reciprocating observed cooperation levels, and dynamically adjusting its
    expectations based on the group's past behavior. It is designed to be adaptive and robust
    across a wide range of opponent strategies in a tournament setting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_count = max(1, math.ceil((self.n - 1) / self.k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_0_indexed_round = state.round_number
        action = None
        if current_0_indexed_round == 0:
            action = Action.C
        elif current_0_indexed_round == self.r - 1:
            action = Action.D
        else:
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[current_0_indexed_round - 1, :])
            my_action_prev_round_is_C = history.my_actions[current_0_indexed_round - 1]
            my_contribution_prev_round = 1 if my_action_prev_round_is_C else 0
            C_prev = num_opponent_cooperators_prev_round + my_contribution_prev_round
            if C_prev >= self.cooperation_threshold_count:
                action = Action.C
                self.cooperation_threshold_count = max(1, self.cooperation_threshold_count - 1)
            else:
                action = Action.D
                self.cooperation_threshold_count = min(self.n, self.cooperation_threshold_count + 1)
            self.cooperation_threshold_count = max(1, min(self.n, self.cooperation_threshold_count))
        return action

class Strategy_COLLECTIVE_338(BaseStrategy):
    """
    The Collective Reciprocator strategy for the N-Player Public Goods Game.

    This strategy aims to achieve and sustain collective cooperation by starting
    with cooperation and then adapting its behavior based on the total observed
    cooperation level from the previous round. It continues to cooperate if
    the number of cooperators meets a calculated sustainability threshold,
    otherwise it defects. Crucially, it always defects in the final round to
    prevent exploitation in a finitely repeated game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        self.T_coop = math.ceil(n / k - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_rounds:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            my_action_prev_round_was_C = history.my_actions[-1]
            C_prev = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
            if C_prev >= self.T_coop:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_339(BaseStrategy):
    """
    Collective Reciprocity with End-Game Defection (CRED) strategy for N-Player Public Goods Game.
    This strategy starts by cooperating, defects in the last round, and in intermediate rounds,
    it reciprocates collective cooperation. It cooperates if the number of cooperators in the
    previous round meets or exceeds a dynamic threshold (max(2, ceil(n/2))); otherwise, it defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = max(2, math.ceil(self.game_description.n_players / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        my_prev_action_cooperated = history.my_actions[current_round_0_indexed - 1]
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        num_cooperators_prev_round = my_prev_action_cooperated + num_opponent_cooperators_prev_round
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_340(BaseStrategy):
    """
    The "Adaptive Reciprocator with Forgiveness" (ARF) strategy for the N-Player Public Goods Game.

    This strategy fosters cooperation by starting with trust (cooperating in the first round).
    It maintains cooperation as long as a sufficient number of players contributed in the previous round,
    defined by a `Cooperation_Trigger_Threshold`. If cooperation drops below this threshold,
    the strategy enters a `PUNISHING` state, defecting for a calculated `P_duration` number of rounds.
    After the punishment period, it transitions back to `COOPERATING`, attempting to re-establish
    collective cooperation. In the final round, it defects to protect against end-game exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.P_duration = max(1, round(self.game_description.n_rounds / 5))
        self.Cooperation_Trigger_Threshold = math.ceil(self.game_description.n_players / self.game_description.k)
        self._current_state = 'COOPERATING'
        self._punishment_rounds_remaining = 0
        self._observed_cooperators_previous_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        last_round_0_indexed = total_rounds - 1
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == last_round_0_indexed:
            return Action.D
        my_prev_action_was_C = int(history.my_actions[-1])
        opponents_prev_cooperators = np.sum(history.opponent_actions[-1, :])
        self._observed_cooperators_previous_round = my_prev_action_was_C + opponents_prev_cooperators
        if self._current_state == 'COOPERATING':
            if self._observed_cooperators_previous_round < self.Cooperation_Trigger_Threshold:
                self._current_state = 'PUNISHING'
                self._punishment_rounds_remaining = self.P_duration
                return Action.D
            else:
                return Action.C
        else:
            self._punishment_rounds_remaining -= 1
            if self._punishment_rounds_remaining <= 0:
                self._current_state = 'COOPERATING'
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_341(BaseStrategy):
    """
    Adaptive Collective Reciprocation (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to be robust and adaptive by initiating cooperation,
    conditionally cooperating based on observed collective behavior, and
    adapting its cooperation threshold based on the public good's efficiency (k).
    It also employs a self-interested strategy in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        self.T_cooperate = max(1, n - math.floor(k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        cooperators_prev_round_opponents = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round = history.my_actions[-1]
        C_count_prev_round = cooperators_prev_round_opponents + my_action_prev_round
        if C_count_prev_round >= self.T_cooperate:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_342(BaseStrategy):
    """
    Adaptive Collective Reciprocator strategy for the N-Player Public Goods Game.

    This strategy prioritizes the collective good by initiating cooperation and
    maintaining it when a critical mass of players also cooperates. It is designed
    to be adaptive, learning from past rounds, and robust against widespread
    exploitation. It acknowledges the "end-game effect" to prevent being
    exploited in the final round.

    Decision Rules:
    1. Round 1: Cooperate (C) to set a cooperative tone.
    2. Rounds 2 to r-1: Observe total cooperators from the previous round. If
       the total number of cooperators (including self) is at least
       ceil(n / 2), then Cooperate (C). Otherwise, Defect (D) to punish
       insufficient collective effort.
    3. Round r (final): Defect (D) due to the end-game effect, anticipating
       rational defection from others.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_count = math.ceil(self.game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            my_action_prev_round_was_C = history.my_actions[-1]
            total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
            if total_cooperators_prev_round >= self.cooperation_threshold_count:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_343(BaseStrategy):
    """
    Adaptive Forgiving Tit-for-Tat (AFTfT) strategy for the N-Player Public Goods Game.
    This strategy aims to foster collective cooperation while being adaptive and robust.
    It initiates cooperation, reciprocates cooperation if a sufficient number of players
    contributed in the previous round, punishes widespread defection, and defects in
    the final round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = max(1, math.floor(self.game_description.n_players / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == total_rounds - 1:
            return Action.D
        else:
            previous_round_index = current_round_0_indexed - 1
            my_prev_action = history.my_actions[previous_round_index]
            opponents_prev_actions = history.opponent_actions[previous_round_index, :]
            cooperators_in_prev_round = int(my_prev_action) + np.sum(opponents_prev_actions)
            if cooperators_in_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_344(BaseStrategy):
    """
    Adaptive Collective Contributor (ACC) Strategy for N-Player Public Goods Game.

    The ACC strategy is designed to promote and sustain cooperation while being
    robust to free-riding and adaptive to varying game parameters and opponent behaviors.
    It initiates with cooperation, then conditionally cooperates or defects based on
    a dynamic threshold of collective contributions observed in the previous round.
    For the final round, it defects due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_C = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators_prev_round >= self.cooperation_threshold_C:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_345(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy.

    This strategy aims to promote and sustain collective cooperation.
    It starts cooperatively in the first round. In intermediate rounds, it
    adapts its behavior based on a dynamic threshold of observed cooperation
    from other players in the previous round. If sufficient collective effort
    is observed, it cooperates; otherwise, it defects to signal for more effort.
    In the final round, it defects due to the end-game problem.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.C_threshold = max(1, math.ceil(self.n_players / self.k_factor - 1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            num_cooperators_others_prev_round = np.sum(history.opponent_actions[-1, :])
            if num_cooperators_others_prev_round >= self.C_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_346(BaseStrategy):
    """
    The Adaptive Collective Reciprocity (ACR) strategy aims to maximize individual payoff by
    fostering collective cooperation. It starts with an optimistic cooperative stance,
    then reciprocates observed cooperation if it meets specific thresholds, and punishes
    insufficient cooperation. This strategy relies solely on game parameters and the
    observed history of play.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_proportion_threshold = 0.5
        self.min_absolute_cooperators_threshold = math.floor(self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        prev_round_opponent_cooperators_count = np.sum(history.opponent_actions[-1, :])
        prev_round_my_cooperation = 1 if history.my_actions[-1] else 0
        prev_round_total_cooperators = prev_round_opponent_cooperators_count + prev_round_my_cooperation
        condition_cpt_met = prev_round_total_cooperators / self.n_players >= self.cooperation_proportion_threshold
        condition_mact_met = prev_round_total_cooperators >= self.min_absolute_cooperators_threshold
        if condition_cpt_met and condition_mact_met:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_347(BaseStrategy):
    """
    Collective Tit-for-Tat (CTT) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain cooperation by:
    1.  **Niceness:** Initiating with cooperation to signal willingness for collective good.
    2.  **Retaliation:** Defecting when collective cooperation falls below an acceptable threshold.
    3.  **Forgiveness:** Resuming cooperation when collective effort recovers.
    4.  **Rationality:** Adapting to the terminal round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = self.n_players // 2 + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_was_C = history.my_actions[-1]
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
        if total_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_348(BaseStrategy):
    """
    The Collective Reciprocity strategy for the N-Player Public Goods Game.
    It aims to foster and sustain cooperation for collective benefit while
    remaining adaptive and robust to a variety of opponent behaviors.

    The strategy initiates with cooperation, then adapts its behavior based on a
    dynamically calculated threshold of collective cooperation observed in the
    previous round, and defects in the final round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_strategy_round = state.round_number + 1
        if current_strategy_round == 1:
            return Action.C
        elif current_strategy_round == self.r:
            return Action.D
        else:
            my_prev_action_cooperated = history.my_actions[-1]
            total_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            num_cooperators_prev_round = my_prev_action_cooperated + total_opponent_cooperators_prev_round
            if num_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_349(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize collective welfare over the game duration by
    fostering and sustaining cooperation, while being robust against exploitation.
    It initiates cooperation, then conditionally reciprocates based on the number
    of cooperators in the previous round, and defects in the final round to prevent exploitation.

    Decision Rules:
    1. Round 1: Cooperate (C) to signal willingness to contribute.
    2. Intermediate Rounds:
       - Observe total cooperators (`m_t-1`) in the previous round.
       - Calculate `cooperation_threshold = max(1, math.ceil((n - k) / k))`.
       - If `m_t-1 >= cooperation_threshold`, Cooperate (C).
       - Else, Defect (D) to punish insufficient cooperation.
    3. Last Round: Defect (D) to prevent exploitation due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        calculated_threshold_float = (self.n - self.k) / self.k
        self.cooperation_threshold = max(1, math.ceil(calculated_threshold_float))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == self.r - 1:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_is_cooperate = int(history.my_actions[-1])
        num_total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_action_prev_round_is_cooperate
        if num_total_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_350(BaseStrategy):
    """
    The Adaptive Collective Reciprocator (ACR) strategy promotes cooperation by initiating it
    and reciprocating collective efforts. It selectively defects to signal dissatisfaction
    and protect itself from exploitation when collective contributions are insufficient.
    It makes a pragmatic concession to individual rationality in the final round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = self.n_players / 2.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        else:
            previous_round_idx = current_round_number - 1
            num_opponent_cooperators_prev = np.sum(history.opponent_actions[previous_round_idx, :])
            my_action_prev_was_c = history.my_actions[previous_round_idx]
            num_my_cooperators_prev = 1 if my_action_prev_was_c else 0
            m_prev = num_opponent_cooperators_prev + num_my_cooperators_prev
            if m_prev >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_351(BaseStrategy):
    """
    The "Adaptive Collective Reciprocator" strategy aims to foster and maintain cooperation by starting with an act of good faith.
    It then monitors the collective behavior of all players, rewarding cooperation by continuing to contribute
    and punishing excessive defection by withholding its contribution. The level of "forgiveness"
    (how many defectors it tolerates before retaliating) is dynamically adjusted based on the game's collective benefit multiplier `k`.
    In the final round, it prioritizes self-protection against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.D_tolerance = math.floor(max(0, self.k - 1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        my_prev_action_was_C = int(history.my_actions[-1])
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        C_prev = my_prev_action_was_C + opponent_cooperators_prev_round
        D_prev = self.n_players - C_prev
        if D_prev <= self.D_tolerance:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_352(BaseStrategy):
    """
    Adaptive Reciprocator with Dynamic Threshold (ARDT) strategy for the N-Player Public Goods Game.
    This strategy starts cooperatively, adapts its requirements for continued cooperation based on
    past performance, and acts rationally in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_base: float = 0.5
        self.T_current: float = self.T_base
        self.Max_T_current: float = 1.0
        self.T_increase_step: float = 0.1
        self.T_decrease_step: float = 0.02

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        total_players = self.game_description.n_players
        if current_round_number_1_indexed == 1:
            return Action.C
        if current_round_number_1_indexed == total_rounds:
            return Action.D
        cooperators_in_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        observed_cooperation_rate = float(cooperators_in_prev_round) / total_players
        if observed_cooperation_rate >= self.T_current:
            action = Action.C
            self.T_current = max(self.T_base, self.T_current - self.T_decrease_step)
        else:
            action = Action.D
            self.T_current = min(self.Max_T_current, self.T_current + self.T_increase_step)
        return action

class Strategy_COLLECTIVE_353(BaseStrategy):
    """
    Adaptive Collective Reciprocator (ACR) strategy for the N-Player Public Goods Game.

    The ACR strategy aims to foster and maintain a high level of cooperation among players.
    It initiates cooperation in the first round and defects in the last round to prevent exploitation.
    In intermediate rounds, it observes the total number of cooperators in the previous round.
    If this number meets or exceeds a dynamically calculated cooperation threshold (T_coop_threshold = ceil(n/k)),
    the strategy cooperates. Otherwise, it defects to punish insufficient collective participation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_coop_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        my_prev_action_was_C = history.my_actions[state.round_number - 1]
        opponent_prev_actions = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_previous_round = my_prev_action_was_C + np.sum(opponent_prev_actions)
        if num_cooperators_previous_round >= self.T_coop_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_354(BaseStrategy):
    """
    Adaptive Collective Reciprocator with Endgame Adjustment (ACREA) strategy.

    ACREA aims to foster cooperation in the N-Player Public Goods Game by
    starting cooperatively, then adaptively reciprocating based on collective
    cooperation levels. It adjusts behavior in the endgame to prevent exploitation,
    recognizing the fragility of cooperation as the game concludes.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        k_val = self.game_description.k
        if k_val == 0:
            cpt_formula_result = 0.0
        else:
            cpt_formula_result = (k_val - 1) / k_val
        self.CPT = max(0.2, cpt_formula_result)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        previous_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        my_previous_action_was_C = history.my_actions[-1]
        C_prev_total_cooperators = previous_round_opponent_cooperators + int(my_previous_action_was_C)
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        if current_round_0_indexed == n_rounds - 2:
            if C_prev_total_cooperators == n_players:
                return Action.C
            else:
                return Action.D
        Observed_Coop_Ratio = C_prev_total_cooperators / n_players
        if Observed_Coop_Ratio >= self.CPT:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_355(BaseStrategy):
    """
    Adaptive Collective Reciprocity with Final Defection strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain cooperation by adapting its strictness to the game's
    multiplication factor k, while incorporating robust responses to defection and handling end-game dynamics.
    It prioritizes establishing and sustaining high group cooperation to maximize overall payoffs,
    while protecting itself from sustained exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        threshold_proportion = (self.k - 1.0) / (self.n - 1.0)
        self.C_threshold = max(2, math.ceil(self.n * threshold_proportion))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.r - 1:
            return Action.D
        else:
            cooperators_in_last_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if cooperators_in_last_round >= self.C_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_356(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster and maintain cooperation by balancing an initial willingness to cooperate
    with a dynamic, history-dependent threshold for continued cooperation. It adapts to the
    group's behavior and the game's efficiency parameter (k) while protecting against exploitation.

    Decision Rules:
    1. First Round: Cooperate (C) to signal willingness and initiate cooperation.
    2. Intermediate Rounds: Observe the total number of cooperators (C_prev) in the previous round.
       Calculate a dynamic `required_cooperators_threshold = ceil(n - k)`.
       If `C_prev` meets or exceeds this threshold, Cooperate (C); otherwise, Defect (D).
       The threshold is more lenient (lower) when 'k' (multiplication factor) is high,
       and stricter (higher) when 'k' is low, reflecting the efficiency of the public good.
    3. Last Round: Defect (D) due to backward induction, to avoid exploitation in the final round
       where there are no future consequences for defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Collective Reciprocity strategy.

        Args:
            game_description (PublicGoodsDescription): An object containing game parameters
                                                      (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action for the current player in the current round.

        Args:
            state (GameState): An object containing the current round number (0-indexed).
            history (None | PlayerHistory): An object containing past actions and payoffs
                                           for this player and opponents. None for round 0.

        Returns:
            Action: The chosen action, either Action.C (Cooperate) or Action.D (Defect).
        """
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            c_prev_opponents = np.sum(history.opponent_actions[-1, :])
            c_prev_my = int(history.my_actions[-1])
            total_cooperators_previous_round = c_prev_opponents + c_prev_my
            required_cooperators_threshold = math.ceil(self.n_players - self.k_factor)
            required_cooperators_threshold = max(1, min(self.n_players, int(required_cooperators_threshold)))
            if total_cooperators_previous_round >= required_cooperators_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_357(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by promoting and sustaining
    cooperation. It cooperates in the first round, defects in the last round,
    and in intermediate rounds, cooperates if a sufficient proportion of other
    players cooperated in the previous round. The threshold for "sufficient"
    is adaptive, based on the game's multiplication factor 'k' and the number
    of players 'n'.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Collective Reciprocity strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T = 1 - (self.k - 1) / (self.n - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current game state, including the round number.
            history: An object containing the history of actions and payoffs for all players
                     in previous rounds, or None if it's the first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        num_cooperating_others = np.sum(history.opponent_actions[-1, :])
        cooperation_rate_others = num_cooperating_others / (self.n - 1)
        if cooperation_rate_others >= self.T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_358(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to achieve and sustain high levels of cooperation by initiating
    cooperation, rewarding sufficient collective contributions, and punishing collective
    failures. It also accounts for the "last round effect" to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_sufficiency_threshold = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            my_action_prev_round_bool = history.my_actions[-1]
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            num_cooperators_prev_round = int(my_action_prev_round_bool) + num_opponent_cooperators_prev_round
            if num_cooperators_prev_round >= self.cooperation_sufficiency_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_359(BaseStrategy):
    """
    Proportional Reciprocity with Endgame Deterrence (PRED) strategy for the N-Player Public Goods Game.

    This strategy initiates cooperation in the first round. In intermediate rounds, it cooperates
    if the total number of cooperators in the previous round met or exceeded a threshold
    (ceil(n / 2)); otherwise, it defects. In the final round, it always defects to prevent
    endgame exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            my_action_prev_round_was_C = history.my_actions[-1]
            my_cooperation_count_prev_round = 1 if my_action_prev_round_was_C else 0
            total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_cooperation_count_prev_round
            if total_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_360(BaseStrategy):
    """
    Adaptive Majority Reciprocity (AMR) strategy for the N-Player Public Goods Game.
    This strategy aims to promote and sustain cooperation by adapting to the observed
    collective behavior of all players in the previous round. It starts cooperatively,
    maintains cooperation when a majority of players contribute, and defects as a
    punishment when collective cooperation significantly falters. It explicitly
    defects in the final round to align with the game theory dominant strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = int(history.my_actions[-1])
        num_cooperators_prev_round += np.sum(history.opponent_actions[-1, :])
        num_defectors_prev_round = self.n_players - num_cooperators_prev_round
        if num_cooperators_prev_round > num_defectors_prev_round:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_361(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and maintain cooperation by reciprocating observed cooperation levels,
    punishing significant free-riding, and allowing for forgiveness. It balances the collective good
    with self-preservation, ensuring robustness against various opponent behaviors.
    """
    COOP_THRESHOLD_FACTOR = 0.5
    FORGIVENESS_FACTOR = 0.75

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_current_state = 'Cooperating'

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_game_round_idx = state.round_number
        n_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_game_round_idx == n_rounds - 1:
            return Action.D
        if current_game_round_idx == 0:
            self.my_current_state = 'Cooperating'
            return Action.C
        cooperators_opponents_prev_round = sum(history.opponent_actions[current_game_round_idx - 1, :])
        my_action_prev_round = history.my_actions[current_game_round_idx - 1]
        C_prev = cooperators_opponents_prev_round + (1 if my_action_prev_round else 0)
        COOP_THRESHOLD = math.ceil(n_players * self.COOP_THRESHOLD_FACTOR)
        FORGIVE_THRESHOLD = math.ceil(n_players * self.FORGIVENESS_FACTOR)
        if self.my_current_state == 'Cooperating':
            if C_prev >= COOP_THRESHOLD:
                return Action.C
            else:
                self.my_current_state = 'Defecting'
                return Action.D
        elif C_prev >= FORGIVE_THRESHOLD:
            self.my_current_state = 'Cooperating'
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_362(BaseStrategy):
    """
    The "Adaptive Majority Reciprocity" strategy promotes cooperation by starting with a cooperative move.
    In subsequent rounds, it observes the total number of cooperators from the previous round across all players.
    If a majority (defined as ceil(n/2)) of players cooperated, it continues to cooperate, rewarding collective effort.
    If cooperation falls below this threshold, it defects, signaling disapproval and protecting against exploitation.
    In the final round, it defects to avoid being exploited due to the game's finite nature.
    This strategy balances fostering cooperation with self-protection and allows for re-establishment of cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_previous_action_was_cooperate = history.my_actions[-1]
        opponent_cooperators_previous_round = np.sum(history.opponent_actions[-1, :])
        total_cooperators_previous_round = my_previous_action_was_cooperate + opponent_cooperators_previous_round
        if total_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_363(BaseStrategy):
    """
    Responsive and Forgiving Collective Strategy (RFCS) for the N-Player Public Goods Game.

    This strategy initiates cooperation, then conditions future cooperation on the group's
    previous round's performance. If group cooperation falls below a minimum threshold,
    the strategy defects for a short, defined punishment duration. After this punishment,
    it "forgives" and reverts to cooperation, allowing the group to re-establish a
    cooperative equilibrium. In the final round, it defects due to terminal rationality.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.MCT_initial = math.ceil(self.game_description.n_players / 2)
        self.Punishment_Duration = 1
        self.current_mode = Action.C
        self.punishment_rounds_left = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            self.current_mode = Action.C
            self.punishment_rounds_left = 0
            return Action.C
        N_C_prev = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        if self.punishment_rounds_left > 0:
            self.punishment_rounds_left -= 1
            self.current_mode = Action.D
            return Action.D
        self.current_mode = Action.C
        if N_C_prev < self.MCT_initial:
            self.current_mode = Action.D
            self.punishment_rounds_left = self.Punishment_Duration
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_364(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to initiate and sustain collective cooperation by starting cooperatively,
    then reciprocally responding to the observed level of collective cooperation from
    others. It cooperates if a sufficient number of players contributed in the previous
    round (defined by C_threshold); otherwise, it defects. The strategy also incorporates
    end-game defection to prevent exploitation in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.C_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            my_cooperation_prev_round = history.my_actions[-1]
            total_cooperators_prev_round = opponent_cooperators_prev_round + (1 if my_cooperation_prev_round else 0)
            if total_cooperators_prev_round >= self.C_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_365(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) Strategy for N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation by initiating
    trust and then reciprocally responding to the observed level of cooperation
    from other players. It balances the pursuit of the collective optimum with a
    pragmatic defense against exploitation in a tournament environment.

    Core Philosophy:
    1.  Initial Trust: Start by cooperating in the first round.
    2.  Conditional Reciprocity: Continue cooperating if a sufficient proportion
        of other players also cooperate in the previous round, where 'sufficient'
        is determined by a dynamic threshold based on game parameters (n, k).
    3.  Punishment for Defection: Defect if the level of cooperation from others
        falls below the critical threshold.
    4.  End-Game Pragmatism: Defect in the very last round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.min_coop_others_required = math.ceil((self.n_players - 1) * self.k / self.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        num_cooperators_others_prev_round = np.sum(history.opponent_actions[-1, :])
        if num_cooperators_others_prev_round >= self.min_coop_others_required:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_366(BaseStrategy):
    """
    The Adaptive Threshold Cooperator (ATC) strategy for the N-Player Public Goods Game.
    It begins by cooperating in the first round. In intermediate rounds, it adaptively
    cooperates if the total number of cooperators in the previous round met a dynamic
    threshold (ceil(n/k)); otherwise, it defects. In the final round, it always defects
    to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_nc = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round = history.my_actions[-1]
        my_contribution_prev_round = 1 if my_action_prev_round else 0
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_contribution_prev_round
        if total_cooperators_prev_round >= self.cooperation_threshold_nc:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_367(BaseStrategy):
    """
    The Adaptive Collective Strategy (ACS) for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation by adapting to
    observed behavior. It starts with cooperation in the first round to signal
    willingness. In intermediate rounds, it continues to cooperate only if the
    number of cooperators in the previous round met a dynamically calculated
    threshold. In the final round, it defects due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n_players = game_description.n_players
        k_factor = game_description.k
        self.cooperation_threshold_count = max(2, math.floor(n_players / k_factor))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == total_rounds - 1:
            return Action.D
        else:
            n_cooperators_last_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if n_cooperators_last_round >= self.cooperation_threshold_count:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_368(BaseStrategy):
    """
    Adaptive Collective Contributor with Dynamic Threshold (ACC-DT) strategy for the N-Player Public Goods Game.
    This strategy starts cooperatively, defects in the last round, and in intermediate rounds,
    it adjusts its behavior based on the proportion of cooperators in the previous round.
    The cooperation threshold is dynamically set based on the efficiency of the public good (k/n).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds_1_indexed = self.r
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_rounds_1_indexed:
            return Action.D
        else:
            T_coop = 1.0 - self.k / self.n
            my_prev_action_as_int = 1 if history.my_actions[-1] else 0
            num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
            num_total_cooperators_prev = my_prev_action_as_int + num_opponent_cooperators_prev
            P_C_prev = float(num_total_cooperators_prev) / self.n
            if P_C_prev >= T_coop:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_369(BaseStrategy):
    """
    The Adaptive Collective Cooperator (ACC) strategy aims to foster and sustain cooperation.
    It starts by cooperating in the first round to signal willingness. In subsequent standard rounds,
    it observes the total number of cooperators in the previous round and compares it to the game's
    multiplication factor 'k'. If the collective contribution (number of cooperators) meets or
    exceeds 'k', the strategy continues to cooperate; otherwise, it defects to signal a need
    for greater participation. In the final round, it defects to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        my_prev_action_cooperated = history.my_actions[-1]
        num_cooperators_previous_round = 1 if my_prev_action_cooperated else 0
        num_cooperators_previous_round += np.sum(history.opponent_actions[-1, :])
        if num_cooperators_previous_round >= self.k:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_370(BaseStrategy):
    """
    This strategy is designed for the N-Player Public Goods Game, aiming to foster
    collective cooperation while being robust against various opponent behaviors
    in a tournament setting. It initiates cooperation, then adapts based on the
    observed level of cooperation from others. It penalizes insufficient
    cooperation to deter free-riding and ensures individual rationality in the
    game's final stage.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
            my_action_prev_round_is_cooperate = history.my_actions[state.round_number - 1]
            num_total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_action_prev_round_is_cooperate
            if num_total_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_371(BaseStrategy):
    """
    Collective Reciprocator with Punishment (CRP) Strategy.

    This strategy aims to foster and maintain collective cooperation while being adaptive and
    robust against various opponent behaviors, including exploitation. It initiates cooperation
    to signal willingness for a collective optimum. It maintains cooperation as long as a
    sufficient number of other players are also contributing. If cooperation drops below a
    critical threshold, it enters a temporary punishment phase (defecting for a set number
    of rounds) to deter free-riding and incentivize a return to cooperation. It anticipates
    the rational self-interest in the final round by defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_active: bool = False
        self.punishment_timer: int = 0
        self.cooperation_threshold_num: int = math.ceil(self.game_description.n_players / 2)
        self.punishment_duration: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == self.game_description.n_rounds:
            self.punishment_active = False
            self.punishment_timer = 0
            return Action.D
        if self.punishment_active:
            self.punishment_timer -= 1
            if self.punishment_timer <= 0:
                self.punishment_active = False
            else:
                return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev_round >= self.cooperation_threshold_num:
            return Action.C
        else:
            self.punishment_active = True
            self.punishment_timer = self.punishment_duration
            return Action.D

class Strategy_COLLECTIVE_372(BaseStrategy):
    """
    Adaptive Collective Reciprocator (ACR) strategy for the N-Player Public Goods Game.
    This strategy is designed to foster and sustain collective cooperation while remaining
    robust against exploitation. It starts by cooperating, then dynamically adjusts
    its behavior based on a calculated minimum cooperation threshold (C_thresh)
    observed in the previous round. It defects in the final round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.C_thresh = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_index = state.round_number - 1
        my_previous_action_was_cooperate = history.my_actions[previous_round_index]
        opponent_previous_cooperators = sum(history.opponent_actions[previous_round_index, :])
        num_cooperators_prev = (1 if my_previous_action_was_cooperate else 0) + opponent_previous_cooperators
        if num_cooperators_prev >= self.C_thresh:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_373(BaseStrategy):
    """
    Implements the Collective Majority Reciprocity (CMR) strategy for the N-Player Public Goods Game.
    This strategy combines initial trust with a strong reciprocal mechanism based on majority behavior.
    It starts by cooperating, continues cooperating if a strict majority cooperated in the previous round,
    defects otherwise (to punish/protect), and defects in the last round due to end-game rationality.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters such as n_players, n_rounds.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.required_cooperators_threshold = self.n_players // 2 + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs. It is None for the first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            my_action_prev_round = history.my_actions[current_round_0_indexed - 1]
            opponent_actions_prev_round = history.opponent_actions[current_round_0_indexed - 1, :]
            num_cooperators_prev_round = sum(opponent_actions_prev_round) + (1 if my_action_prev_round else 0)
            if num_cooperators_prev_round >= self.required_cooperators_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_374(BaseStrategy):
    """
    The Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy is designed to foster and maintain cooperation by adapting to observed collective
    behavior, while also being robust against exploitation and anticipating end-game dynamics.
    It initiates with cooperation, maintains cooperation if a sufficient number of other players
    contribute (defined by a calculated threshold), defects if cooperation falls below this
    threshold to signal disapproval or avoid exploitation, and always defects in the final round
    due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_C = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        else:
            num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if num_cooperators_prev_round >= self.T_C:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_375(BaseStrategy):
    """
    Collective Adaptive Reciprocator (CAR) strategy for the N-Player Public Goods Game.
    This strategy starts by cooperating to build trust, then adapts its behavior based on
    a dynamically calculated 'COOPERATION_THRESHOLD'. It rewards sufficient collective
    cooperation and punishes insufficient cooperation. In the final round, it defects
    to avoid exploitation, ensuring robustness in a competitive environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.COOPERATION_THRESHOLD = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        elif current_round_idx == self.n_rounds - 1:
            return Action.D
        else:
            num_cooperators_previous_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if num_cooperators_previous_round >= self.COOPERATION_THRESHOLD:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_376(BaseStrategy):
    """
    The Adaptive Cooperator strategy. It initiates cooperation in the first round.
    In intermediate rounds, it conditionally cooperates based on the level of group
    cooperation from the previous round, using a dynamic threshold (Tit-for-Tat
    for N=2, and floor(N/2) cooperators for N>2). In the final round, it defects
    due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        prev_round_idx = current_round_0_indexed - 1
        if self.n_players == 2:
            other_player_cooperated_prev_round = history.opponent_actions[prev_round_idx, 0]
            if other_player_cooperated_prev_round:
                return Action.C
            else:
                return Action.D
        else:
            my_prev_action_was_c = history.my_actions[prev_round_idx]
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
            total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_prev_action_was_c else 0)
            cooperation_threshold = math.floor(self.n_players / 2)
            if total_cooperators_prev_round >= cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_377(BaseStrategy):
    """
    The Conditional Collective Cooperation (CCC) strategy for the N-Player Public Goods Game.
    It aims for collective optimum by starting with cooperation, setting a clear threshold for
    acceptable collective cooperation, and swiftly but temporarily punishing widespread
    defection to encourage a return to mutual benefit. It acknowledges the individual
    incentive to defect in single rounds but counters this with the threat of collective
    breakdown in repeated play.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.rounds_to_punish = 0
        self.cooperation_trigger_threshold = max(1, math.floor(self.n_players / self.k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            self.rounds_to_punish = 0
            return Action.C
        if current_round_number == self.n_rounds - 1:
            self.rounds_to_punish = 0
            return Action.D
        previous_round_index = current_round_number - 1
        total_cooperators_prev_round = sum(history.opponent_actions[previous_round_index, :]) + history.my_actions[previous_round_index]
        if self.rounds_to_punish > 0:
            self.rounds_to_punish -= 1
            return Action.D
        elif total_cooperators_prev_round >= self.cooperation_trigger_threshold:
            return Action.C
        else:
            self.rounds_to_punish = 1
            return Action.D

class Strategy_COLLECTIVE_378(BaseStrategy):
    """
    The Adaptive Reciprocal Cooperator (ARC) strategy aims to initiate a cooperative
    environment and then dynamically adapt its behavior based on the observed level
    of collective cooperation in previous rounds. It is forgiving when the public
    good is highly beneficial (high `k`) and stricter when the benefits are lower (low `k`).
    It always accounts for the end-game effect by defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == r:
            return Action.D
        dynamic_cooperation_threshold_raw = n * (1 - (k - 1) / (n - 1))
        dynamic_cooperation_threshold = max(1, math.floor(dynamic_cooperation_threshold_raw))
        my_prev_action_cooperated_int = int(history.my_actions[state.round_number - 1])
        opponents_prev_cooperated_count = np.sum(history.opponent_actions[state.round_number - 1, :])
        num_cooperators_prev_round = my_prev_action_cooperated_int + opponents_prev_cooperated_count
        if num_cooperators_prev_round >= dynamic_cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_379(BaseStrategy):
    """
    Adaptive Collective Threshold (ACT) Strategy for the N-Player Public Goods Game.

    The strategy balances initiating cooperation, sustaining it conditionally, and
    protecting against exploitation. It cooperates in the first round to signal
    a willingness for collective action. In intermediate rounds, it cooperates
    only if the total number of cooperators in the previous round met a specific
    threshold (ceil(n / k)), which ensures that contributing players are not
    worse off than if everyone defected. In the final round, it defects to avoid
    exploitation due to the known end of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        if current_round_zero_indexed == 0:
            return Action.C
        if current_round_zero_indexed == self.r - 1:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_380(BaseStrategy):
    """
    The Adaptive Reciprocal Contributor (ARC) strategy initiates with cooperation
    and then adapts its behavior based on the observed level of cooperation in the
    group, defined by a dynamic threshold. It defects in the final round.

    Decision Rules:
    1. First Round (t=1): Cooperate (C) to establish a cooperative baseline.
    2. Intermediate Rounds (1 < t < r):
       - Calculate C_prev: total cooperators in the preceding round.
       - Calculate T = ceil(n / k) as the cooperation threshold.
       - If C_prev >= T: Cooperate (C)
       - Else (C_prev < T): Defect (D)
    3. Last Round (t=r): Defect (D) due to absence of future consequences.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        c_prev = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
        if c_prev >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_381(BaseStrategy):
    """
    Adaptive Cooperation with Threshold (ACT) strategy for the N-Player Public Goods Game.

    This strategy initiates cooperation in the first round to signal willingness for collective
    benefit. In subsequent rounds (2 to r-1), it adapts its behavior based on the observed
    proportion of cooperators in the previous round. If the total number of cooperators
    meets or exceeds a dynamic threshold (defined as ceil(n / 2.0)), ACT continues to
    cooperate. If cooperation falls below this threshold, ACT defects to avoid exploitation
    and to signal that greater collective contributions are expected. In the final round,
    ACT always defects, following the principle of backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / 2.0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_382(BaseStrategy):
    """
    The Adaptive Collective Cooperator (ACC) strategy aims to foster and maintain cooperation among players
    to achieve the collectively optimal outcome in the N-Player Public Goods Game.

    It operates on principles of conditional cooperation:
    1.  **First Round:** Always Cooperates to initiate a cooperative environment.
    2.  **Intermediate Rounds:** Observes the cooperation rate from the immediately preceding round.
        - If 50% or more players cooperated, it continues to Cooperate.
        - If less than 50% cooperated, it switches to Defect to punish non-cooperation and protect its own payoff.
    3.  **Last Round:** Always Defects, based on backward induction, to avoid exploitation in the game's final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        cooperation_rate_prev_round = num_cooperators_prev_round / self.game_description.n_players
        cooperation_threshold = 0.5
        if cooperation_rate_prev_round >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_383(BaseStrategy):
    """
    Adaptive Proportional Reciprocity (APR) strategy for the N-Player Public Goods Game.
    Starts cooperatively, then adapts based on the previous round's total cooperation level
    against a dynamic threshold. This threshold is calculated from game parameters `n`
    (number of players) and `k` (multiplication factor), becoming more lenient when
    collective cooperation is highly beneficial (large `k`) and stricter when it's less
    beneficial (small `k`). For the final round, it reverts to individual rationality
    by defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players - self.k_factor + 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        previous_round_index = current_round_0_indexed - 1
        my_action_in_previous_round = history.my_actions[previous_round_index]
        opponent_actions_in_previous_round = history.opponent_actions[previous_round_index, :]
        num_cooperators_previous_round = np.sum(opponent_actions_in_previous_round) + my_action_in_previous_round
        if num_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_384(BaseStrategy):
    """
    Adaptive Community Supporter (ACS) strategy for the N-Player Public Goods Game.

    This strategy balances a desire for collective benefit with the need for individual
    protection against exploitation. It adapts its behavior based on the observed
    actions of other players and follows these decision rules:

    1.  **First Round (Round 1):** Always Cooperate (C) to initiate cooperation and signal
        a willingness to contribute.
    2.  **Intermediate Rounds (Round 2 to r-1):**
        -   Observes the total number of cooperators from all players in the previous round.
        -   Calculates a fixed cooperation threshold `T = ceil(n / 2)`.
        -   If the number of cooperators in the previous round was `T` or more, the strategy
            Cooperate (C) to sustain collective effort.
        -   Otherwise (less than `T` cooperators), the strategy Defects (D) to prevent
            exploitation and signal that insufficient collective effort leads to breakdown.
    3.  **Last Round (Round r):** Always Defect (D) due to the absence of future interactions
        (backward induction), preventing exploitation in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_game_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_game_rounds:
            return Action.D
        else:
            opponents_cooperated_prev_round = np.sum(history.opponent_actions[-1, :])
            my_action_prev_round = int(history.my_actions[-1])
            total_cooperators_prev_round = opponents_cooperated_prev_round + my_action_prev_round
            if total_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_385(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy initiates with cooperation in the first round to build trust.
    In intermediate rounds, it reciprocates the observed collective cooperation level,
    cooperating if a majority of players cooperated in the previous round, and defecting otherwise.
    In the final round, it defects to avoid exploitation due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_rounds = self.game_description.n_rounds
        if current_round == 0:
            return Action.C
        if current_round == n_rounds - 1:
            return Action.D
        prev_round_idx = current_round - 1
        num_opponent_cooperators_last_round = sum(history.opponent_actions[prev_round_idx, :])
        my_action_last_round = history.my_actions[prev_round_idx]
        total_cooperators_last_round = num_opponent_cooperators_last_round + my_action_last_round
        if total_cooperators_last_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_386(BaseStrategy):
    """
    The Collective Adaptive Strategy (CAS) for the N-Player Public Goods Game.

    This strategy aims to promote and sustain cooperation by being conditionally cooperative,
    forgiving but firm, and adapting to observed behavior. It explicitly addresses the
    first and last rounds based on standard game-theoretic reasoning for repeated games.

    Decision Rules:
    1. First Round (t=1 / round_number=0): Cooperate (C) to signal a willingness to collaborate
       and test the environment for potential cooperation.
    2. Last Round (t=r / round_number=n_rounds-1): Defect (D) due to the "end-game effect."
       There are no future consequences, making individual defection the dominant strategy.
    3. Intermediate Rounds (1 < t < r): The action is conditional on the level of cooperation
       observed in the previous round.
       Let `m_prev` be the total number of players who cooperated in the previous round.
       Calculate the Cooperation Threshold (T) as `ceil(n_players / k)`.
       - If `m_prev >= T`: Cooperate (C). This signals sufficient collective effort, and the
         strategy continues to reward and reinforce this behavior.
       - If `m_prev < T`: Defect (D). This indicates excessive free-riding or insufficient
         collective effort, prompting a defection to punish and protect own payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        last_round_0_indexed = self.n_rounds - 1
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == last_round_0_indexed:
            return Action.D
        num_cooperators_previous_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_387(BaseStrategy):
    """
    The Adaptive Collective Reciprocity (ACR) strategy aims to foster and sustain collective
    cooperation by initiating with a cooperative stance and then dynamically adjusting its
    behavior based on the overall level of cooperation observed in the previous round.
    It contributes to the public good when there is sufficient collective effort (P_C >= 1/k),
    but protects itself and deters free-riding by defecting when cooperation falters.
    The strategy also accounts for the 'end game' effect by defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = 1 / self.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        previous_round_history_idx = state.round_number - 1
        my_previous_action_was_C = history.my_actions[previous_round_history_idx]
        opponents_previous_actions_were_C = history.opponent_actions[previous_round_history_idx, :]
        num_total_cooperators_prev_round = int(my_previous_action_was_C) + np.sum(opponents_previous_actions_were_C)
        proportion_cooperators_prev_round = num_total_cooperators_prev_round / self.n_players
        if proportion_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_388(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster and sustain collective cooperation by adapting its tolerance for
    defection based on the game's multiplication factor 'k' and the observed history.
    It cooperates initially, defects in the last round, and in intermediate rounds,
    it continues to cooperate if the number of defectors in the previous round was
    below a dynamic tolerance threshold, otherwise it defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tolerance_threshold = max(1, math.floor(self.game_description.k - 1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            previous_round_idx = state.round_number - 1
            num_opponent_defectors = (history.opponent_actions[previous_round_idx] == Action.D).sum()
            my_action_previous_round = history.my_actions[previous_round_idx]
            my_defected_previous_round = 1 if my_action_previous_round == Action.D else 0
            total_defectors_previous_round = num_opponent_defectors + my_defected_previous_round
            if total_defectors_previous_round <= self.tolerance_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_389(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) aims to achieve the collective optimum
    by fostering and sustaining cooperation through conditional play, while being robust
    against exploitation. It balances initial trust, responsiveness to collective behavior,
    and self-preservation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_index = current_round_number - 1
        num_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_index, :]) + history.my_actions[previous_round_index]
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_390(BaseStrategy):
    """
    Adaptive Collective Responder (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to initiate and sustain collective cooperation. It starts by
    cooperating in the first round. In intermediate rounds, it observes the
    proportion of cooperators from the previous round and continues to cooperate
    if this proportion meets or exceeds a predefined threshold (T_C_PROPORTION).
    If the cooperation level falls below the threshold, the strategy defects to
    punish free-riding. In the final round of the game, it defects due to the
    end-game effect, assuming rational play where future incentives disappear.

    Strategy Parameters:
    - T_C_PROPORTION: A fixed threshold (0.5) representing the minimum proportion
                      of cooperators required in the previous round for this
                      strategy to cooperate in the current intermediate round.
    """
    T_C_PROPORTION = 0.5

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Collective Responder strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current game state, including the round number.
            history: An object containing records of past actions and payoffs.
                     Will be None for the very first round (round_number = 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            proportion_cooperators = num_cooperators_prev_round / self.n_players
            if proportion_cooperators >= self.T_C_PROPORTION:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_391(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    ACR starts by extending trust through cooperation and then conditionally cooperates
    based on the observed behavior of other players. It adapts its tolerance for group
    defection based on the inherent efficiency of the public good (determined by `k/n`),
    becoming more forgiving for highly efficient projects and more demanding for less
    efficient ones. It also explicitly handles the unique dynamics of the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.ctr = max(0.1, 1 - self.k / self.n)
        self.cooperation_threshold_others = math.ceil((self.n - 1) * self.ctr)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_one_indexed = state.round_number + 1
        if state.round_number == 0:
            return Action.C
        if current_round_one_indexed == self.r:
            return Action.D
        n_c_others_prev = sum(history.opponent_actions[-1, :])
        if n_c_others_prev >= self.cooperation_threshold_others:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_392(BaseStrategy):
    """
    Adaptive Reciprocator with Majority Threshold strategy for N-Player Public Goods Game.

    This strategy aims to foster and maintain cooperation by being initially cooperative,
    then adapting its behavior based on the observed level of cooperation from other
    players in previous rounds. It incorporates an end-game logic to handle the finite
    nature of the game and uses a moderate threshold to be robust against a few defectors
    while still incentivizing collective action.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.W = min(self.r - 1, 5)
        self.cooperation_threshold_rate = math.floor(self.n / 2) / self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        elif current_round_number == self.r - 1:
            return Action.D
        else:
            effective_window_size = min(self.W, current_round_number)
            if effective_window_size == 0:
                return Action.C
            sum_cooperators_in_window = 0
            start_history_idx = current_round_number - effective_window_size
            for t_idx in range(start_history_idx, current_round_number):
                num_cooperators_this_past_round = history.my_actions[t_idx] + np.sum(history.opponent_actions[t_idx, :])
                sum_cooperators_in_window += num_cooperators_this_past_round
            avg_cooperation_rate = sum_cooperators_in_window / (effective_window_size * self.n)
            if avg_cooperation_rate > self.cooperation_threshold_rate:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_393(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy is designed to balance collective good with self-preservation.
    It initiates cooperation, defects in the final round, and in intermediate rounds,
    it conditionally cooperates. The condition for cooperation is that at least half
    of the other players must have cooperated in the immediately preceding round.
    This approach aims to foster cooperation while being robust against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            num_cooperators_among_others_prev_round = sum(history.opponent_actions[-1, :])
            cooperation_threshold = math.ceil((self.n_players - 1) / 2.0)
            if num_cooperators_among_others_prev_round >= cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_394(BaseStrategy):
    """
    Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster collective cooperation for mutual benefit,
    while being adaptive and robust against various opponent behaviors.
    It starts with an optimistic cooperative move, then adapts based on the
    observed actions of other players in the previous round, with specific
    handling for the first and last rounds. It distinguishes between a 2-player
    and N-player (N>2) scenario to ensure robust performance.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_others_cooperated = int(np.sum(history.opponent_actions[-1, :]))
        if self.n_players == 2:
            if num_others_cooperated == 1:
                return Action.C
            else:
                return Action.D
        else:
            cooperation_threshold_others = math.ceil((self.n_players - 1) / 2)
            if num_others_cooperated >= cooperation_threshold_others:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_395(BaseStrategy):
    """
    The Adaptive Reciprocal Cooperator (ARC) strategy aims to initiate and sustain cooperation
    in an N-Player Public Goods Game. It starts by cooperating to signal willingness,
    then adapts its behavior based on the observed cooperation level of other players
    in the previous round. It will defect if others are not contributing sufficiently,
    thus protecting itself from exploitation, but remains open to re-establishing cooperation.
    In the final round, it always defects due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold_proportion = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        num_others_cooperated = np.sum(history.opponent_actions[-1, :])
        num_other_players = self.n_players - 1
        if num_other_players <= 0:
            proportion_others_cooperated = 1.0
        else:
            proportion_others_cooperated = num_others_cooperated / num_other_players
        if proportion_others_cooperated >= self.cooperation_threshold_proportion:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_396(BaseStrategy):
    """
    The Adaptive Majority Cooperator (AMC) strategy is designed for the N-Player Public Goods Game.
    It aims to initiate cooperation, maintain it as long as a sufficient level of collective
    participation (a majority of players) is observed in the previous round, and defect
    when cooperation breaks down or when strategic end-game considerations dictate.

    Decision Logic:
    1.  **First Round:** Always Cooperate (C) to initiate collective action.
    2.  **Final Round:** Always Defect (D) due to end-game dynamics (backward induction).
    3.  **Intermediate Rounds:**
        *   If the total number of cooperators in the previous round (`N_C_prev`)
            was greater than or equal to `ceil(n / 2)` (Cooperation_Threshold), then Cooperate (C).
        *   Otherwise (if `N_C_prev < Cooperation_Threshold`), then Defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        else:
            previous_round_idx = state.round_number - 1
            num_opponent_cooperators_prev = sum(history.opponent_actions[previous_round_idx, :])
            my_action_prev = history.my_actions[previous_round_idx]
            N_C_prev = num_opponent_cooperators_prev + my_action_prev
            if N_C_prev >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_397(BaseStrategy):
    """
    The Adaptive Collective Cooperator (ACC) strategy aims to foster and maintain high levels of
    cooperation in the N-Player Public Goods Game. It initiates with a cooperative signal and then
    adapts its behavior based on the observed collective contribution in previous rounds.
    It uses a clear threshold to determine when to cooperate (reward collective effort) and when
    to defect (punish insufficient collective effort), while acknowledging the unique dynamics
    of the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters such as n_players and n_rounds.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current game state, including the round number.
            history: An object containing a record of past actions and payoffs for the current player
                     and opponents, or None if it's the first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        my_action_prev = history.my_actions[current_round_0_indexed - 1]
        opponent_actions_prev = history.opponent_actions[current_round_0_indexed - 1, :]
        N_C_prev = int(my_action_prev) + np.sum(opponent_actions_prev)
        if N_C_prev * 2 < self.n_players:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_398(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster and sustain cooperation by being initially generous,
    conditionally cooperative based on observed collective behavior, and
    self-protective against exploitation. It dynamically adjusts its strictness
    based on the inherent "difficulty" of maintaining cooperation in the given
    game parameters (k and n).

    Decision Rules:
    1.  First Round: Play C (Cooperate) as an act of initial goodwill.
    2.  Last Round: Play D (Defect) due to backward induction.
    3.  Intermediate Rounds:
        - Calculate `cooperation_threshold_proportion (T = 1 - k/n)`.
          Higher `T` (stricter) when `k/n` is low (public good less efficient).
          Lower `T` (more forgiving) when `k/n` is high (public good more efficient).
        - Count `num_cooperators_in_previous_round` (including self).
        - Calculate `observed_cooperation_proportion = num_cooperators_in_previous_round / n`.
        - If `observed_cooperation_proportion >= T`: Play C.
        - Else: Play D (punish by defecting).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_proportion = 1.0 - self.k / self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        previous_round_0_indexed = current_round_0_indexed - 1
        num_cooperators_among_opponents_in_prev_round = np.sum(history.opponent_actions[previous_round_0_indexed, :])
        my_action_in_prev_round = history.my_actions[previous_round_0_indexed]
        num_cooperators_in_previous_round = num_cooperators_among_opponents_in_prev_round + (1 if my_action_in_prev_round else 0)
        observed_cooperation_proportion = num_cooperators_in_previous_round / self.n
        if observed_cooperation_proportion >= self.cooperation_threshold_proportion:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_399(BaseStrategy):
    """
    The Adaptive Majority Reciprocator strategy aims to maximize collective payoff
    by promoting and reciprocating cooperation. It starts with an act of good faith,
    then adapts its behavior based on the observed cooperation level of other players.
    It will continue to cooperate as long as a sufficient number of others contribute,
    but will defect to protect itself from exploitation if collective cooperation falters.
    It recognizes the end-game dynamics of finite repeated games, defecting in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.n_rounds - 1:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[-1, :]
        num_others_cooperating_prev_round = sum(previous_round_opponent_actions)
        num_other_players = self.n_players - 1
        cooperation_threshold_for_others = math.ceil(num_other_players / 2.0)
        if num_others_cooperating_prev_round >= cooperation_threshold_for_others:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_400(BaseStrategy):
    """
    The "Adaptive Collective Reciprocity with End-Game Defection" strategy.

    This strategy aims to foster and sustain cooperation in the N-Player Public Goods Game.
    It initiates play with cooperation to signal willingness to contribute.
    In middle rounds, it employs an adaptive mechanism: it cooperates if a majority
    of other players cooperated in the previous round, thereby rewarding collective
    cooperation. Otherwise, it defects to punish insufficient cooperation or widespread
    free-riding. For the final round, it rationally defects, anticipating the lack
    of future consequences.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        num_other_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        threshold_others = self.n_players // 2
        if num_other_cooperators_prev_round >= threshold_others:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_401(BaseStrategy):
    """
    Adaptive Conditional Cooperation (ACC) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and sustain collective cooperation by initiating a
    cooperative stance, rewarding demonstrated cooperation, and punishing widespread
    defection. It adapts its cooperation threshold based on game parameters (n and k)
    and uses backward induction for the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold_num = math.floor(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_index = state.round_number
        last_round_index = self.n_rounds - 1
        if current_round_index == 0:
            return Action.C
        elif current_round_index == last_round_index:
            return Action.D
        else:
            previous_round_index = current_round_index - 1
            my_prev_action_bool = history.my_actions[previous_round_index]
            opponent_prev_actions_bool = history.opponent_actions[previous_round_index, :]
            num_cooperators_prev_round = int(my_prev_action_bool) + np.sum(opponent_prev_actions_bool)
            if num_cooperators_prev_round > self.cooperation_threshold_num:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_402(BaseStrategy):
    """
    The Adaptive Conditional Cooperator (ACC) strategy aims to foster cooperation
    in the N-Player Public Goods Game. It starts with an initial act of trust,
    then adapts its behavior based on the group's cooperation level in the
    previous round, using a dynamically calculated threshold. It defects in the
    final round to avoid exploitation due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ACC strategy with game parameters and calculates
        the cooperation threshold.

        Args:
            game_description: An object containing n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.threshold = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round
        based on the strategy's rules.

        Args:
            state: The current state of the game, including the round number.
            history: An object containing past actions and payoffs for all players,
                     or None if it's the first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        my_prev_action_was_cooperate = history.my_actions[-1]
        opponent_prev_actions_were_cooperate = history.opponent_actions[-1, :]
        c_prev = sum(opponent_prev_actions_were_cooperate) + my_prev_action_was_cooperate
        if c_prev >= self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_403(BaseStrategy):
    """
    Adaptive Threshold Tit-for-Tat (ATTT) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain cooperation by being initially cooperative,
    then adapting its behavior based on the observed level of cooperation in previous rounds.
    It incorporates a dynamic cooperation threshold and a conditional last-round decision
    to balance individual rationality with collective aspirations.

    Decision Rules:
    1.  Round 1: Always Cooperate (C).
    2.  Intermediate Rounds (2 to r-1):
        -   Calculates a dynamic threshold T = ceil(n/k).
        -   If total cooperators in the previous round >= T, play C.
        -   Else, play D.
    3.  Final Round (r):
        -   If all players cooperated in the penultimate round, play C.
        -   Else, play D.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_T = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds_1_indexed = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        my_prev_action_cooperated = history.my_actions[state.round_number - 1]
        opponents_prev_cooperators_count = sum(history.opponent_actions[state.round_number - 1, :])
        previous_round_total_cooperators = opponents_prev_cooperators_count + (1 if my_prev_action_cooperated else 0)
        if current_round_1_indexed < total_rounds_1_indexed:
            if previous_round_total_cooperators >= self.cooperation_threshold_T:
                return Action.C
            else:
                return Action.D
        if current_round_1_indexed == total_rounds_1_indexed:
            if previous_round_total_cooperators == self.game_description.n_players:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_404(BaseStrategy):
    """
    The "Adaptive Collective Reciprocity" (ACR) strategy is designed to foster and maintain cooperation in the N-Player Public Goods Game by balancing initial trust, conditional reciprocity, and self-preservation. It adapts to observed collective behavior, is robust across game parameters, and aligns with a collective mindset by aiming for the high payoffs of widespread cooperation while protecting against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_count = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        previous_round_index_0_indexed = state.round_number - 1
        my_prev_action_is_cooperate = history.my_actions[previous_round_index_0_indexed]
        opponent_prev_actions_are_cooperate = history.opponent_actions[previous_round_index_0_indexed, :]
        num_cooperators_prev_round = int(my_prev_action_is_cooperate) + np.sum(opponent_prev_actions_are_cooperate)
        if num_cooperators_prev_round >= self.cooperation_threshold_count:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_405(BaseStrategy):
    """
    Adaptive Collective Enforcer (ACE) strategy for the N-Player Public Goods Game.

    ACE aims to foster widespread cooperation, be robust against exploitation, and adapt
    to varying levels of collective effort from opponents. It initiates with trust,
    rewards sustained collective effort above a calculated threshold, and punishes
    widespread defection to prevent exploitation. Its core goal is to establish and
    maintain a high level of cooperation to achieve the collectively optimal outcome,
    while remaining pragmatic about individual incentives and endgame effects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T_C = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        my_prev_action_value = int(history.my_actions[-1])
        opponents_prev_coop_count = np.sum(history.opponent_actions[-1, :])
        N_C_prev = my_prev_action_value + opponents_prev_coop_count
        if N_C_prev >= self.T_C:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_406(BaseStrategy):
    """
    The Collective Steward strategy aims to maximize the collective payoff by fostering cooperation.
    It initiates with trust, monitors group cooperation, applies temporary punishment for low cooperation,
    and defects in the final round for self-protection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = self.n - max(1, math.floor(self.n / (self.k + 1)))
        self.punishment_duration = min(2, max(1, math.floor(self.r / 5)))
        self.punishment_countdown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        if self.punishment_countdown > 0:
            self.punishment_countdown -= 1
            return Action.D
        total_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
        if history.my_actions[state.round_number - 1]:
            total_cooperators_prev_round += 1
        if total_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            self.punishment_countdown = self.punishment_duration - 1
            return Action.D

class Strategy_COLLECTIVE_407(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster cooperation by initiating with a cooperative move and then
    adapting its behavior based on the collective cooperation level of the group
    in the preceding round. It requires a strict majority of players to cooperate
    to maintain its own cooperation in intermediate rounds. In the final round,
    it defects as predicted by standard game theory for finitely repeated games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ACR strategy with game parameters.

        Args:
            game_description (PublicGoodsDescription): An object containing
                                                       n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = self.n_players // 2 + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the current player's action (Cooperate or Defect) based on the ACR strategy.

        Args:
            state (GameState): An object containing the current round number (0-indexed).
            history (None | PlayerHistory): An object containing records of past actions
                                            and payoffs for this player and opponents.
                                            Is None for the very first round.

        Returns:
            Action: The chosen action, either Action.C (Cooperate) or Action.D (Defect).
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            previous_round_index = current_round_0_indexed - 1
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_index, :])
            my_action_prev_round = history.my_actions[previous_round_index]
            total_cooperators_prev_round = my_action_prev_round + num_opponent_cooperators_prev_round
            if total_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_408(BaseStrategy):
    """
    Adaptive Forgiving Reciprocity (AFR) strategy for the N-Player Public Goods Game.

    AFR is a conditional cooperation strategy designed to foster and sustain cooperation.
    It starts cooperatively, attempts to maintain cooperation by reciprocating observed
    cooperation, and punishes defection when it exceeds a dynamically adjusted tolerance level.
    It also accounts for the "endgame effect" by defecting in the final round.

    Decision Rules:
    1.  **Round 1 (0-indexed round 0):** Cooperate (C) to signal willingness for collective good.
    2.  **Intermediate Rounds (0-indexed rounds 1 to n_rounds-2):**
        *   Observe total cooperators in the previous round.
        *   Calculate a dynamic tolerance threshold for defectors ($T_D = \\max(1, \\lfloor (n - k) + 1 \rfloor)$).
            This threshold adapts: it's stricter when `k` is high (cooperation is very beneficial)
            and more forgiving when `k` is low (cooperation is harder to sustain).
        *   If the number of defectors in the previous round is less than or equal to $T_D$, cooperate.
        *   Otherwise (too many defectors), defect to protect against exploitation.
    3.  **Final Round (0-indexed round n_rounds-1):** Defect (D) due to the "endgame effect"
        where reciprocity is no longer possible.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        total_rounds = self.game_description.n_rounds
        k_factor = self.game_description.k
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        total_cooperators_last_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        n_defectors_last_round = n_players - total_cooperators_last_round
        tolerance_threshold_D = max(1, int(math.floor(n_players - k_factor + 1)))
        if n_defectors_last_round <= tolerance_threshold_D:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_409(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster and sustain collective cooperation by reciprocating good behavior
    and punishing insufficient contributions. It begins by cooperating in the first round
    to signal a willingness to establish a cooperative environment.

    In intermediate rounds, it adapts its behavior based on the observed collective
    actions of all players in the immediately preceding round. The strategy cooperates
    if the collective output from the previous round's cooperation was sufficient to
    justify individual contributions. This condition is met when
    (number of cooperators in previous round * k) >= n, indicating that the shared
    benefit from the public good adequately compensated for individual costs.
    If this condition is not met, the strategy defects to signal that current cooperation
    levels are unsustainable, protect its endowment, and potentially reset expectations.

    In the final round, the strategy defects, recognizing that there are no future
    interactions to incentivize cooperation or punish defection, aligning with
    backward induction for the finite repeated game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev_round * self.k >= self.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_410(BaseStrategy):
    """
    The "Adaptive Critical Mass" strategy for the N-Player Public Goods Game.

    This strategy is based on conditional cooperation, aiming to foster and sustain
    collective benefit by contributing only when there is sufficient collective
    effort from the group. It incorporates a critical mass threshold derived from
    the game parameters and handles edge cases like the first and last rounds.

    Decision Rules:
    - Round 1 (Initialization): Plays Cooperate (C) to signal willingness to cooperate.
    - Rounds 2 to r-1 (Adaptive Phase):
        - Observes the total number of cooperators in the previous round ($N_C^{(t-1)}$).
        - Calculates a Critical Cooperation Threshold ($T_{MinCoop} = \\lceil n/k \rceil$).
        - If $N_C^{(t-1)} \\geq T_{MinCoop}$, plays Cooperate (C).
        - If $N_C^{(t-1)} < T_{MinCoop}$, plays Defect (D).
    - Round r (Last Round - End Game): Plays Defect (D) due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_MinCoop = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            previous_round_index = state.round_number - 1
            num_cooperators_prev_round = int(history.my_actions[previous_round_index]) + sum(history.opponent_actions[previous_round_index, :])
            if num_cooperators_prev_round >= self.T_MinCoop:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_411(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to maximize the collective payoff by encouraging and sustaining cooperation.
    It does this by starting cooperatively to signal intent, then dynamically adjusting
    its behavior based on the observed level of collective cooperation in previous rounds.
    It cooperates when a sufficient number of players contribute, but defects when
    cooperation falls below a critical threshold, both to protect itself from exploitation
    and to incentivize other players to increase their contributions. The strategy
    accounts for the "endgame effect" in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        last_round_0_indexed = self.game_description.n_rounds - 1
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == last_round_0_indexed:
            return Action.D
        else:
            prev_round_idx = current_round_0_indexed - 1
            num_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx]) + history.my_actions[prev_round_idx]
            if num_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_412(BaseStrategy):
    """
    The Adaptive Collective Forgiver (ACF) strategy promotes and sustains cooperation
    in the N-Player Public Goods Game. It starts with a cooperative stance,
    then adaptively reciprocates cooperation if a strict majority of players
    cooperated in the previous round. It punishes insufficient cooperation by
    defecting but allows for re-establishment of cooperation (forgiveness).
    Finally, it defects in the last round to maximize individual gain based on
    backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold_count = math.floor(self.n_players / 2) + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        num_cooperators_previous_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_previous_round >= self.cooperation_threshold_count:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_413(BaseStrategy):
    """
    Adaptive Collective Threshold (ACT) strategy for the N-Player Public Goods Game.

    This strategy aims to establish and maintain cooperation by being conditionally cooperative.
    It proactively attempts to initiate collective action in the first round. In subsequent rounds,
    it continuously assesses the level of cooperation from other players. If a "sufficient" number
    of players cooperated in the previous round, it reciprocates with cooperation. The definition
    of "sufficient" is dynamically determined by the game's multiplication factor `k` and the
    number of players `n`, reflecting the inherent collective benefit of cooperation.
    Finally, it accounts for the well-known "endgame effect" by reverting to defection in the
    final round to prevent exploitation when no future interactions are possible.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_T = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_zero_indexed == 0:
            return Action.C
        if current_round_zero_indexed == total_rounds - 1:
            return Action.D
        num_cooperators_prev_round = int(history.my_actions[-1]) + sum(history.opponent_actions[-1, :])
        if num_cooperators_prev_round >= self.cooperation_threshold_T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_414(BaseStrategy):
    """
    The Collective Vigilance strategy aims to foster and maintain a high level of cooperation
    within the group. It operates on the principle of conditional cooperation, starting with
    an act of trust, then adapting its behavior based on the observed collective effort in
    prior rounds. It employs a specific threshold derived from the game parameters to determine
    when collective action is sufficient to warrant continued contribution, and when a punitive
    defection is necessary to deter free-riding. Recognizing the finite nature of the game,
    it incorporates a clear policy for the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold_c = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        c_prev = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if c_prev < self.threshold_c:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_415(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy initiates cooperation, rewards collective cooperation by continuing
    to contribute, punishes free-riding by defecting when cooperation falls below
    a threshold, and defects in the last round based on backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        my_previous_action_was_c = history.my_actions[-1]
        num_opponent_cooperators_previous_round = np.sum(history.opponent_actions[-1, :])
        total_cooperators_previous_round = (1 if my_previous_action_was_c else 0) + num_opponent_cooperators_previous_round
        if total_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_416(BaseStrategy):
    """
    Adaptive Reciprocity Strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain cooperation by initiating cooperation
    and then conditionally cooperating based on the observed level of collective
    contribution in previous rounds. It protects itself from exploitation by
    reverting to defection if collective cooperation is too low or in the final
    round.

    Core Principles:
    1. Proactive Cooperation (First Round): Always cooperate in the first round
       to signal a willingness to contribute.
    2. Adaptive Reciprocity (Intermediate Rounds): Adjust behavior (Cooperate/Defect)
       based on whether the total number of cooperators in the previous round met
       a dynamically calculated threshold. This threshold ensures that the
       average individual share of the public good (if this strategy cooperates)
       is at least equivalent to the individual "safe" payoff (1).
    3. Individual Rationality (Last Round): Defect in the final round as there
       are no future interactions to influence, aligning with backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_num = max(0, int(math.ceil(float(self.n) / self.k)) - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        else:
            prev_round_idx = state.round_number - 1
            num_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
            if history.my_actions[prev_round_idx]:
                num_cooperators_prev_round += 1
            if num_cooperators_prev_round >= self.cooperation_threshold_num:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_418(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster cooperation by initiating trust, monitoring the
    overall level of cooperation, and responding dynamically. It is firm in penalizing
    persistent defection but forgiving of temporary dips in collective contribution,
    always keeping an eye on the collective benefit while protecting against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.DEFECTION_TRIGGER_THRESHOLD = 1.0 / self.k_factor
        self.INITIAL_FORGIVENESS_ROUNDS = 1
        self.consecutive_low_cooperation_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            self.consecutive_low_cooperation_rounds = 0
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_is_C = history.my_actions[-1]
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_is_C else 0)
        proportion_cooperators_prev = float(total_cooperators_prev_round) / self.n_players
        if proportion_cooperators_prev < self.DEFECTION_TRIGGER_THRESHOLD:
            self.consecutive_low_cooperation_rounds += 1
            if self.consecutive_low_cooperation_rounds <= self.INITIAL_FORGIVENESS_ROUNDS:
                return Action.C
            else:
                return Action.D
        else:
            self.consecutive_low_cooperation_rounds = 0
            return Action.C

class Strategy_COLLECTIVE_419(BaseStrategy):
    """
    Reciprocal Adaptive Cooperation (RAC) strategy for the N-Player Public Goods Game.

    This strategy dynamically adjusts its behavior based on observed group cooperation.
    It starts cooperatively, reciprocates adequate cooperation, punishes widespread
    defection with a short defection phase, and provides a forgiveness period to
    allow the group to re-establish cooperation. It incorporates end-game defection
    for rational play in the final round.
    """
    _COOPERATING_MODE = 'COOPERATING'
    _PUNISHING_MODE = 'PUNISHING'
    _FORGIVING_MODE = 'FORGIVING'

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_C = 0.5
        self.D_P = 1
        self.W_F = 1
        self.state_mode = self._COOPERATING_MODE
        self.rounds_in_current_mode = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_players = self.game_description.n_players
        total_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            self.state_mode = self._COOPERATING_MODE
            self.rounds_in_current_mode = 0
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        previous_round_0_indexed = state.round_number - 1
        my_prev_action_was_C = history.my_actions[previous_round_0_indexed]
        opponent_prev_actions_were_C = history.opponent_actions[previous_round_0_indexed, :]
        total_cooperators_prev_round = int(my_prev_action_was_C) + np.sum(opponent_prev_actions_were_C)
        P_prev = total_cooperators_prev_round / n_players
        while True:
            if self.state_mode == self._PUNISHING_MODE:
                self.rounds_in_current_mode += 1
                if self.rounds_in_current_mode <= self.D_P:
                    return Action.D
                else:
                    self.state_mode = self._FORGIVING_MODE
                    self.rounds_in_current_mode = 1
                    return Action.C
            elif self.state_mode == self._FORGIVING_MODE:
                self.rounds_in_current_mode += 1
                if self.rounds_in_current_mode <= self.W_F:
                    return Action.C
                else:
                    self.state_mode = self._COOPERATING_MODE
                    self.rounds_in_current_mode = 0
            elif self.state_mode == self._COOPERATING_MODE:
                if P_prev >= self.T_C:
                    return Action.C
                else:
                    self.state_mode = self._PUNISHING_MODE
                    self.rounds_in_current_mode = 1
                    return Action.D

class Strategy_COLLECTIVE_420(BaseStrategy):
    """
    The Collective Reciprocator strategy is designed to foster and sustain cooperation in the
    N-Player Public Goods Game by balancing an initial willingness to cooperate with
    conditional reciprocity based on the group's past actions. It aims to maximize
    collective payoff while protecting against exploitation.

    Core principles:
    1. Initial Trust: Starts with cooperation.
    2. Collective Reciprocity: Bases subsequent decisions on the *overall* level of
       cooperation in the previous round.
    3. Threshold Enforcement: Requires a strict majority of players to cooperate
       to continue contributing.
    4. End-Game Rationality: Defects in the final round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = self.game_description.n_players // 2 + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round == 0:
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        else:
            previous_round_opponent_actions = history.opponent_actions[current_round - 1, :]
            previous_round_my_action = history.my_actions[current_round - 1]
            cooperators_last_round = sum(previous_round_opponent_actions)
            if previous_round_my_action:
                cooperators_last_round += 1
            if cooperators_last_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_421(BaseStrategy):
    """
    Adaptive Forgiving Tit-for-Tat (AFTFT) strategy for the N-Player Public Goods Game.

    This strategy aims to foster widespread cooperation. It starts by cooperating
    in the first round. In intermediate rounds, it monitors the collective behavior,
    cooperating only if at least `n-1` players (i.e., at most one defector)
    cooperated in the previous round. This means it has a high standard for
    collective contribution. If the standard is not met, it defects to punish
    widespread defection and encourage a return to cooperation.
    In the final round, it rationally defects, as there is no future to influence.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold_count = self.n_players - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_1_indexed_round = state.round_number + 1
        if current_1_indexed_round == 1:
            return Action.C
        if current_1_indexed_round == self.n_rounds:
            return Action.D
        my_prev_action = history.my_actions[state.round_number - 1]
        opponent_prev_actions = history.opponent_actions[state.round_number - 1, :]
        total_cooperators_prev_round = int(my_prev_action) + np.sum(opponent_prev_actions)
        if total_cooperators_prev_round >= self.cooperation_threshold_count:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_422(BaseStrategy):
    """
    The Adaptive Reciprocal Cooperator (ARC) strategy aims to foster and sustain
    cooperation in the N-Player Public Goods Game. It initiates with cooperation,
    adapts its behavior based on the observed collective cooperation levels relative
    to a dynamic threshold (influenced by the game's efficiency 'k'), punishes
    low cooperation by defecting, but periodically attempts to re-establish
    cooperation after a period of defection. It defects in the final round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.threshold_cooperation_count = math.ceil(self.n - self.k)
        self.threshold_cooperation_count = max(1, self.threshold_cooperation_count)
        self.consecutive_defections_by_self = 0
        self.retest_interval = max(2, min(5, self.r // 4))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.consecutive_defections_by_self = 0
            return Action.C
        if state.round_number == self.r - 1:
            self.consecutive_defections_by_self = 0
            return Action.D
        previous_round_idx = state.round_number - 1
        previous_opponent_cooperators = np.sum(history.opponent_actions[previous_round_idx, :])
        my_previous_action_was_C = history.my_actions[previous_round_idx]
        previous_total_cooperators = previous_opponent_cooperators + (1 if my_previous_action_was_C else 0)
        if previous_total_cooperators >= self.threshold_cooperation_count:
            self.consecutive_defections_by_self = 0
            return Action.C
        else:
            if not my_previous_action_was_C:
                self.consecutive_defections_by_self += 1
            else:
                self.consecutive_defections_by_self = 1
            if self.consecutive_defections_by_self > 0 and self.consecutive_defections_by_self % self.retest_interval == 0:
                self.consecutive_defections_by_self = 0
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_423(BaseStrategy):
    """
    The Adaptive Collective Reciprocator strategy fosters cooperation in the N-Player Public Goods Game.
    It begins with goodwill (cooperating in the first round), adapts its cooperativeness based on a dynamic
    threshold influenced by game parameters (k, n) and observed cooperation from others, and reverts to
    self-interest in the final round to prevent exploitation (endgame rationality).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == self.n_rounds - 1:
            return Action.D
        previous_round_idx = current_round_idx - 1
        others_cooperated_in_prev_round = np.sum(history.opponent_actions[previous_round_idx, :])
        P_others_observed = others_cooperated_in_prev_round / (self.n_players - 1)
        P_threshold = max(0.5, 1 - self.k / self.n_players)
        if P_others_observed >= P_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_424(BaseStrategy):
    """
    The "Adaptive Collective Reciprocity" (ACR) strategy aims to foster cooperation
    in the N-Player Public Goods Game. It starts with an initial cooperative stance
    and then adaptively responds to the observed level of collective cooperation
    within a recent window of rounds. It reverts to defection in the final round
    due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.total_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.total_rounds - 1:
            return Action.D
        patience_window = min(state.round_number, 3)
        total_cooperators_sum_in_window = 0
        for i in range(patience_window):
            hist_idx = state.round_number - patience_window + i
            my_action_in_past_round = history.my_actions[hist_idx]
            opponent_actions_in_past_round = history.opponent_actions[hist_idx, :]
            cooperators_in_this_round = int(my_action_in_past_round) + np.sum(opponent_actions_in_past_round)
            total_cooperators_sum_in_window += cooperators_in_this_round
        if patience_window == 0:
            return Action.C
        average_cooperation_count = total_cooperators_sum_in_window / patience_window
        cooperation_required_threshold = math.ceil(self.n_players / 2)
        if average_cooperation_count >= cooperation_required_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_425(BaseStrategy):
    """
    Responsive Collective Cooperation (RCC) strategy for the N-Player Public Goods Game.

    This strategy aims to promote cooperation by starting with a cooperative move,
    then adaptively responding to the collective cooperation level of the group.
    It employs a strict threshold for continued cooperation and pre-emptively defects
    in the final two rounds to avoid exploitation.
    """
    MINIMUM_COOPERATION_PROPORTION = 0.75

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.n_rounds - 1 or current_round == self.n_rounds - 2:
            return Action.D
        if current_round == 0:
            return Action.C
        previous_round_my_action = history.my_actions[current_round - 1]
        previous_round_opponent_actions = history.opponent_actions[current_round - 1]
        num_cooperators_prev_round = np.sum(previous_round_opponent_actions)
        if previous_round_my_action:
            num_cooperators_prev_round += 1
        proportion_cooperators = num_cooperators_prev_round / self.n_players
        if proportion_cooperators >= self.MINIMUM_COOPERATION_PROPORTION:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_426(BaseStrategy):
    """
    Adaptive Forgiving Reciprocity (AFR) strategy for the N-Player Public Goods Game.

    AFR aims to foster cooperation by starting cooperatively, then adapting its
    cooperation threshold based on the observed collective cooperation of other players
    over a recent history window. It incorporates a forgiveness factor to prevent
    overreaction to minor fluctuations and adjusts its threshold gradually.
    The strategy also incorporates backward induction, defecting in the last round
    to protect against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        k = game_description.k
        self.P_coop_threshold_initial = 1.0 - k / n
        self.Forgiveness_factor_threshold_adjustment = 0.1
        self.Learning_rate_threshold_adjustment = 0.05
        self.History_window_size = n
        self.P_coop_threshold = self.P_coop_threshold_initial
        self.Recent_Cooperation_Proportions = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == r - 1:
            return Action.D
        C_others_prev = sum(history.opponent_actions[-1, :])
        P_others_coop_prev = C_others_prev / (n - 1)
        self.Recent_Cooperation_Proportions.append(P_others_coop_prev)
        if len(self.Recent_Cooperation_Proportions) > self.History_window_size:
            self.Recent_Cooperation_Proportions.pop(0)
        Avg_P_coop_recent = sum(self.Recent_Cooperation_Proportions) / len(self.Recent_Cooperation_Proportions)
        if Avg_P_coop_recent > self.P_coop_threshold + self.Forgiveness_factor_threshold_adjustment:
            self.P_coop_threshold = max(0.0, self.P_coop_threshold - self.Learning_rate_threshold_adjustment)
        elif Avg_P_coop_recent < self.P_coop_threshold - self.Forgiveness_factor_threshold_adjustment:
            self.P_coop_threshold = min(1.0, self.P_coop_threshold + self.Learning_rate_threshold_adjustment)
        if P_others_coop_prev >= self.P_coop_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_427(BaseStrategy):
    """
    Collective Resilience with Adaptive Decline (CRAD) strategy for the N-Player Public Goods Game.
    This strategy starts cooperatively, maintains cooperation as long as a sufficient number of players
    reciprocate, and gradually reduces its cooperativeness as the game approaches its end,
    ultimately defecting in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.T_C_base = math.ceil(self.n_players / self.k)
        self.T_C_base = max(1, self.T_C_base)
        self.endgame_window_start_round_1_indexed = max(2, self.n_rounds - math.floor((self.n_rounds - 1) / 4))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            previous_round_idx = state.round_number - 1
            N_C_prev = np.sum(history.opponent_actions[previous_round_idx, :]) + (1 if history.my_actions[previous_round_idx] else 0)
            current_threshold = 0
            if current_round_1_indexed < self.endgame_window_start_round_1_indexed:
                current_threshold = self.T_C_base
            else:
                num_adaptive_decline_rounds = self.n_rounds - 1 - self.endgame_window_start_round_1_indexed + 1
                num_adaptive_decline_rounds_for_fade = max(1, num_adaptive_decline_rounds)
                index_within_decline = current_round_1_indexed - self.endgame_window_start_round_1_indexed
                fade_factor = index_within_decline / num_adaptive_decline_rounds_for_fade
                current_threshold = round(self.T_C_base + (self.n_players - self.T_C_base) * fade_factor)
                current_threshold = max(1, current_threshold)
            if N_C_prev >= current_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_428(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR initiates cooperation to establish a positive dynamic and then maintains
    cooperation as long as the collective effort yields a sufficient return.
    It will defect if group cooperation drops too low, acting as a deterrent
    against widespread free-riding, but it is ready to re-cooperate if the
    group's cooperation recovers. It acknowledges and exploits the end-game
    dynamics of repeated games by defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ACR strategy with game parameters.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.threshold_acr = 1.0 / self.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        The decision depends on the current round number and the observed
        cooperation level from the previous round (if applicable).

        Args:
            state: Current game state, including the round number (0-indexed).
            history: A PlayerHistory object containing past actions and payoffs.
                     It is None only for the very first round (round_number = 0).

        Returns:
            An Action (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        my_prev_action_as_int = int(history.my_actions[-1])
        total_cooperators_prev_round = my_prev_action_as_int + np.sum(history.opponent_actions[-1, :])
        cooperation_rate_prev_round = total_cooperators_prev_round / self.n_players
        if cooperation_rate_prev_round > self.threshold_acr:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_429(BaseStrategy):
    """
    The Adaptive Majority Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to initiate cooperation, sustain it when a majority of players
    are also cooperating, and defect when cooperation levels drop, serving as a
    self-protective measure and a signal for collective correction. It defects
    in the final round based on backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialises the strategy with game parameters and pre-calculates the
        cooperation threshold.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs. None for the first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_prev_action_is_cooperate = history.my_actions[state.round_number - 1]
        opponent_prev_actions_are_cooperate = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_prev_round = int(my_prev_action_is_cooperate) + np.sum(opponent_prev_actions_are_cooperate)
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_430(BaseStrategy):
    """
    Adaptive K-Threshold Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to promote and sustain cooperation by being conditionally cooperative.
    It starts with an initial act of cooperation, then adapts its behavior based on the
    observed level of cooperation in the previous round, using a dynamic threshold
    influenced by the game's parameters `n` and `k`. It incorporates a rational
    defection in the final round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_T = (self.n - self.k) / (self.n - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.r:
            return Action.D
        else:
            C_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :]) + history.my_actions[state.round_number - 1]
            P_prev_round = C_prev_round / self.n
            if P_prev_round >= self.cooperation_threshold_T:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_431(BaseStrategy):
    """
    The Adaptive Collective Reciprocator (ACR) strategy aims to foster cooperation
    in the N-Player Public Goods Game by starting cooperatively, rewarding
    sufficient collective effort, punishing free-riding with limited duration,
    and attempting to re-initiate cooperation after sustained punishment periods.
    It defects in the final round due to end-game rationality.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.my_punishment_streak: int = 0
        self.MID_COOP_THRESHOLD: int = math.ceil(self.n / 2)
        self.MAX_PUNISHMENT_DURATION: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number
        if current_round_t == 0:
            self.my_punishment_streak = 0
            return Action.C
        if current_round_t == self.r - 1:
            return Action.D
        total_cooperators_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if total_cooperators_prev >= self.MID_COOP_THRESHOLD:
            self.my_punishment_streak = 0
            return Action.C
        elif self.my_punishment_streak < self.MAX_PUNISHMENT_DURATION:
            self.my_punishment_streak += 1
            return Action.D
        else:
            self.my_punishment_streak = 0
            return Action.C

class Strategy_COLLECTIVE_432(BaseStrategy):
    """
    The Adaptive Reciprocal Cooperator (ARC) strategy aims to foster and sustain collective
    cooperation in the N-Player Public Goods Game. It initiates cooperation, then adapts
    based on the observed level of collective cooperation in previous rounds using a
    "sustainability threshold". It strategically defects in the final round to prevent
    exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.minimum_sustainable_cooperators_M = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        else:
            cooperators_from_opponents = np.sum(history.opponent_actions[-1, :])
            cooperators_in_previous_round = cooperators_from_opponents + history.my_actions[-1]
            if cooperators_in_previous_round >= self.minimum_sustainable_cooperators_M:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_433(BaseStrategy):
    """
    The Adaptive Social Reciprocator (ASR) strategy initiates cooperation and then maintains it as long as the
    collective level of cooperation remains above a dynamic threshold. It incorporates forgiveness for minor dips
    in cooperation and adjusts its trust level based on observed collective behavior, while protecting itself
    against consistent free-riders and exploitative play in the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.player_i_threshold = math.floor(self.n / 2)
        self.player_i_tolerance_counter = 0
        self.TOLERANCE_LIMIT = 1
        self.COOPERATION_BOOST_FACTOR = 1
        self.DEFECTION_PUNISH_FACTOR = 1
        self.EXCELLENT_COOPERATION_LEVEL = self.n - 1
        self.POOR_COOPERATION_LEVEL = math.floor(self.n / 4)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        C_prev = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        action_to_take = None
        if C_prev >= self.player_i_threshold:
            action_to_take = Action.C
            self.player_i_tolerance_counter = 0
            if C_prev >= self.EXCELLENT_COOPERATION_LEVEL:
                self.player_i_threshold = max(0, self.player_i_threshold - self.COOPERATION_BOOST_FACTOR)
        elif C_prev == self.player_i_threshold - 1 and self.player_i_tolerance_counter < self.TOLERANCE_LIMIT:
            action_to_take = Action.C
            self.player_i_tolerance_counter += 1
        else:
            action_to_take = Action.D
            self.player_i_tolerance_counter = 0
            if C_prev <= self.POOR_COOPERATION_LEVEL:
                self.player_i_threshold = min(self.n - 1, self.player_i_threshold + self.DEFECTION_PUNISH_FACTOR)
        return action_to_take

class Strategy_COLLECTIVE_434(BaseStrategy):
    """
    Adaptive Forgiving Cooperator (AFC) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain cooperation by initiating it, rewarding it,
    and attempting to restore it after periods of defection, while also being robust
    against exploitation. It balances an inclination towards cooperation with a pragmatic
    need to respond to observed defection using thresholds based on previous round's
    cooperation levels and a forgiveness mechanism.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.COOPERATION_THRESHOLD_PROPORTION = 0.5
        self.FORGIVENESS_ATTEMPTS = 1
        self.forced_cooperation_attempts_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            self.forced_cooperation_attempts_counter = 0
            return Action.C
        elif current_round_0_indexed == self.n_rounds - 1:
            self.forced_cooperation_attempts_counter = 0
            return Action.D
        else:
            cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            proportion_cooperators = cooperators_prev_round / self.n_players
            if proportion_cooperators < self.COOPERATION_THRESHOLD_PROPORTION:
                if self.forced_cooperation_attempts_counter < self.FORGIVENESS_ATTEMPTS:
                    action = Action.C
                    self.forced_cooperation_attempts_counter += 1
                else:
                    action = Action.D
                    self.forced_cooperation_attempts_counter = 0
            else:
                action = Action.C
                self.forced_cooperation_attempts_counter = 0
            return action

class Strategy_COLLECTIVE_435(BaseStrategy):
    """
    Adaptive Majority Reciprocity (AMR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster collective cooperation. It starts by cooperating
    to signal goodwill. In subsequent rounds, it monitors the overall level of
    cooperation from the previous round. If a strict majority (ceil(n/2)) of
    players cooperated, the strategy continues to cooperate. If cooperation falls
    below this threshold, it temporarily defects to signal disapproval and
    encourage a return to higher cooperation. In the final round, it always
    defects to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Majority Reciprocity strategy.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold_T_C = (self.n_players + 1) // 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs for this player
                     and opponents, or None if it's the first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        my_prev_action_bool = history.my_actions[state.round_number - 1]
        opponent_prev_actions_bools = history.opponent_actions[state.round_number - 1, :]
        cooperators_in_prev_round = int(my_prev_action_bool) + np.sum(opponent_prev_actions_bools)
        if cooperators_in_prev_round >= self.cooperation_threshold_T_C:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_436(BaseStrategy):
    """
    The Adaptive Collective Responder (ACR) strategy is designed to be adaptive, robust, and
    promote collective welfare in the N-Player Public Goods Game.

    Core Principle:
    - Starts cooperatively in the first round (Round 1) to signal willingness to contribute.
    - Adapts its behavior in intermediate rounds based on the observed collective cooperation
      level from the previous round, using a calculated Cooperation Threshold (T_C = ceil(n / k)).
      If the previous round's cooperation met or exceeded T_C, the strategy cooperates; otherwise,
      it defects.
    - Defects in the last round (Round r) due to the logic of backward induction in a finite game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            my_action_prev_round_is_C = history.my_actions[current_round_0_indexed - 1]
            opponent_actions_prev_round = history.opponent_actions[current_round_0_indexed - 1, :]
            previous_round_total_cooperators = int(my_action_prev_round_is_C) + np.sum(opponent_actions_prev_round)
            if previous_round_total_cooperators >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_437(BaseStrategy):
    """
    The "Adaptive Collective Reciprocity" (ACR) strategy promotes and sustains cooperation
    in the N-Player Public Goods Game. It offers initial trust, monitors group behavior,
    and retaliates against sustained defection to achieve collective optimums while
    being robust against exploitation.

    Core Philosophy:
    Generosity: Initial unconditional cooperation and tolerance for some defection.
    Reciprocity: Continues cooperation when warranted, punishes prolonged exploitation.
    Self-preservation: Defects in the final round and after sustained exploitation.
    """
    P_coop_threshold: float = 0.7
    tolerance_rounds: int = 2
    initial_trust_rounds: int = 1

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_low_cooperation_rounds: int = 0
        self.global_defection_triggered: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        num_players_n = self.game_description.n_players
        total_rounds_r = self.game_description.n_rounds
        current_round_t = state.round_number + 1
        if current_round_t > 1:
            num_cooperators_in_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            percentage_cooperators_in_prev_round = num_cooperators_in_prev_round / num_players_n
            if percentage_cooperators_in_prev_round < self.P_coop_threshold:
                self.consecutive_low_cooperation_rounds += 1
            else:
                self.consecutive_low_cooperation_rounds = 0
            if self.consecutive_low_cooperation_rounds >= self.tolerance_rounds:
                self.global_defection_triggered = True
        if current_round_t == total_rounds_r:
            return Action.D
        elif current_round_t <= self.initial_trust_rounds:
            return Action.C
        elif self.global_defection_triggered is True:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_438(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster and sustain cooperation by exhibiting initial trust and then
    adapting its behavior based on the observed collective cooperation level in previous
    rounds. It protects against exploitation while remaining open to re-establishing
    cooperation. It accounts for the finite nature of the game by adopting a rational
    approach in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n_rounds = game_description.n_rounds
        k_factor = game_description.k
        self.INITIAL_COOP_ROUNDS = min(3, n_rounds - 1)
        self.COOP_THRESHOLD_COUNT = math.ceil(k_factor)
        self.ENDGAME_ROUNDS = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round <= self.INITIAL_COOP_ROUNDS:
            return Action.C
        if current_round > total_rounds - self.ENDGAME_ROUNDS:
            return Action.D
        my_prev_action_cooperated_count = int(history.my_actions[-1])
        opponent_prev_cooperators_count = np.sum(history.opponent_actions[-1, :])
        num_cooperators_previous_round = my_prev_action_cooperated_count + opponent_prev_cooperators_count
        if num_cooperators_previous_round >= self.COOP_THRESHOLD_COUNT:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_439(BaseStrategy):
    """
    Collective Strategy: Adaptive Trigger with Forgiveness for N-Player Public Goods Game.

    This strategy aims to initiate and maintain cooperation by being initially trusting,
    then adapting its behavior based on the observed level of cooperation in previous rounds.
    It employs a punishment mechanism for sustained defection but also incorporates
    forgiveness to prevent overly aggressive spirals of non-cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.consecutive_below_threshold_rounds = 0
        self.punishment_mode = False
        self.CT = max(2, math.floor(self.n / self.k))
        self.FW = max(1, math.floor(self.r / 10))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        C_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if self.punishment_mode:
            if C_prev >= self.n - 1:
                self.punishment_mode = False
                self.consecutive_below_threshold_rounds = 0
            else:
                return Action.D
        if C_prev >= self.CT:
            self.consecutive_below_threshold_rounds = 0
            return Action.C
        else:
            self.consecutive_below_threshold_rounds += 1
            if self.consecutive_below_threshold_rounds >= self.FW:
                self.punishment_mode = True
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_440(BaseStrategy):
    """
    The "Adaptive Collective Reciprocator" strategy is designed for the N-Player Public Goods Game
    to foster cooperation for collective benefit while remaining robust and self-protective.
    It balances initial trust, reciprocal cooperation, and strategic self-interest, particularly
    in the face of exploitation or at the game's end.
    """
    COOPERATION_THRESHOLD: float = 0.5
    DEFECTION_TOLERANCE: int = 2

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Collective Reciprocator strategy.

        Args:
            game_description (PublicGoodsDescription): An object containing game parameters
                                                     like n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.low_cooperation_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action for the current round based on the strategy's rules and game history.

        Args:
            state (GameState): An object containing the current round number.
            history (None | PlayerHistory): An object containing the history of actions and payoffs
                                            for the current player and opponents in previous rounds.
                                            It is None for the very first round (round 0).

        Returns:
            Action: The chosen action for the current round, either Action.C (Cooperate) or Action.D (Defect).
        """
        current_round_number_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_number_0_indexed == 0:
            self.low_cooperation_streak = 0
            return Action.C
        elif current_round_number_0_indexed == total_rounds - 1:
            return Action.D
        else:
            num_cooperators_prev_round_opponents = np.sum(history.opponent_actions[-1, :])
            my_action_prev_round_bool = history.my_actions[-1]
            total_cooperators_prev_round = num_cooperators_prev_round_opponents + (1 if my_action_prev_round_bool else 0)
            cooperation_ratio = total_cooperators_prev_round / n_players
            if self.low_cooperation_streak >= self.DEFECTION_TOLERANCE:
                return Action.D
            elif cooperation_ratio >= self.COOPERATION_THRESHOLD:
                self.low_cooperation_streak = 0
                return Action.C
            else:
                self.low_cooperation_streak += 1
                return Action.D

class Strategy_COLLECTIVE_441(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy balances initial cooperation with conditional retaliation and forgiveness,
    driven by an assessment of the collective cooperation level, and acknowledges end-game
    dynamics by defecting in the final round.

    Decision Logic:
    1. Round 1: Cooperate (Action.C) to initiate collaboration.
    2. Intermediate Rounds (2 to r-1):
       - Count total cooperators in the previous round.
       - If total cooperators meet or exceed `COOPERATION_THRESHOLD` (ceil(n/k)), then Cooperate (Action.C).
       - Otherwise, Defect (Action.D) as a protective measure or punishment.
    3. Last Round (r): Defect (Action.D) due to backward induction, to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == n_rounds - 1:
            return Action.D
        else:
            previous_round_index = current_round_0_indexed - 1
            my_action_prev_round = history.my_actions[previous_round_index]
            opponent_actions_prev_round = history.opponent_actions[previous_round_index, :]
            cooperators_total_prev_round = sum(opponent_actions_prev_round) + (1 if my_action_prev_round else 0)
            if cooperators_total_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_442(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to maximize collective payoff by encouraging and sustaining cooperation
    among players. It initiates cooperation in Round 1, monitors the collective
    effort, and reciprocates by cooperating if at least 50% of players cooperated
    in the previous round. If collective effort is insufficient (less than 50%
    cooperated), the strategy defects to protect individual payoff and signal
    unsustainability. In the final round, it defects due to end-game rationality.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Collective Reciprocity strategy.

        Args:
            game_description: An object containing game parameters like
                              n_players and n_rounds.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round
        based on the strategy rules.

        Args:
            state: An object containing the current game state, including
                   the current round number (0-indexed).
            history: An object containing historical actions and payoffs for
                     all players up to the previous round. Will be None for round 0.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        my_cooperation_prev_round = int(history.my_actions[-1])
        opponent_cooperation_prev_round = np.sum(history.opponent_actions[-1, :])
        num_cooperators_prev_round = my_cooperation_prev_round + opponent_cooperation_prev_round
        fraction_cooperators_prev_round = num_cooperators_prev_round / self.n_players
        if fraction_cooperators_prev_round >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_443(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.
    This strategy adapts its behavior based on the level of cooperation observed
    in previous rounds. It aims to foster cooperation, avoid exploitation, and
    makes a highly conditional final cooperative gesture if the environment
    has been overwhelmingly cooperative.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_intermediate = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        previous_round_0_indexed = current_round_0_indexed - 1
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[previous_round_0_indexed, :])
        my_action_prev_was_C = history.my_actions[previous_round_0_indexed]
        C_total_prev = num_opponent_cooperators_prev + (1 if my_action_prev_was_C else 0)
        if current_round_0_indexed == self.r - 1:
            if C_total_prev >= self.n - 1:
                return Action.C
            else:
                return Action.D
        elif C_total_prev >= self.cooperation_threshold_intermediate:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_444(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) Strategy for N-Player Public Goods Game.

    This strategy aims to foster cooperation by initiating with a cooperative move.
    It then conditionally cooperates in middle rounds, adapting its cooperation
    threshold based on the efficiency of the public good (k) and the number of players (n).
    Specifically, it continues to cooperate if the collective cooperation ratio from
    the previous round meets or exceeds this adaptive threshold.
    Finally, it defects in the last round, accounting for the end-game unraveling effect
    in finitely repeated games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.ACT = 1 - (self.k - 1) / (self.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        else:
            num_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            if history.my_actions[-1]:
                num_cooperators_prev_round += 1
            CR_prev = num_cooperators_prev_round / self.n_players
            if CR_prev >= self.ACT:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_445(BaseStrategy):
    """
    Collective Reciprocator with Minimum Viable Cooperation (CR-MVC) Strategy.

    This strategy aims to foster cooperation while protecting itself from exploitation.
    It adapts its behavior based on the overall level of cooperation observed in
    previous rounds compared to a dynamically calculated Minimum Viable Cooperation (MVC) threshold.

    The strategy's behavior is as follows:
    1. Cooperate in the first round to signal willingness to contribute.
    2. In intermediate rounds, observe the total number of cooperators from the previous round.
       If this count meets or exceeds the MVC threshold (ceil(n / k)), continue cooperating.
       Otherwise, defect to punish insufficient collective effort.
    3. Defect in the final round due to the end-game effect (no future consequences).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_mvc = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_rounds:
            return Action.D
        else:
            cooperators_in_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if cooperators_in_prev_round >= self.T_mvc:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_446(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain a high level of collective cooperation to
    achieve the social optimum, while remaining robust against exploitation.
    It starts by cooperating, defects in the last round, and in intermediate rounds,
    it adapts its behavior based on a dynamic threshold derived from the game's
    multiplication factor 'k' and the observed collective cooperation in the
    previous round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            previous_round_0_indexed = state.round_number - 1
            my_prev_action_is_cooperate = history.my_actions[previous_round_0_indexed]
            opponents_prev_actions_are_cooperate = history.opponent_actions[previous_round_0_indexed, :]
            num_cooperators_prev_round = int(my_prev_action_is_cooperate) + np.sum(opponents_prev_actions_are_cooperate)
            cooperation_ratio_prev_round = num_cooperators_prev_round / self.n_players
            required_cooperation_ratio_threshold = (self.k_factor - 1) / self.k_factor
            if cooperation_ratio_prev_round >= required_cooperation_ratio_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_448(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) Strategy for the N-Player Public Goods Game.

    This strategy aims to initiate and sustain collective cooperation by acting cooperatively
    when there is sufficient evidence of collective effort. It uses a dynamic threshold
    based on game parameters (n and k) to determine if continued cooperation is warranted.
    If collective effort falls below this threshold, the strategy switches to defection
    to prevent exploitation. In the final round, it defects due to the absence of future
    interactions for reciprocity.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T_C = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.r - 1:
            return Action.D
        else:
            prev_round_idx = current_round_0_indexed - 1
            N_C_prev_opponents = sum(history.opponent_actions[prev_round_idx, :])
            my_prev_action_was_C = history.my_actions[prev_round_idx]
            N_C_prev = N_C_prev_opponents + (1 if my_prev_action_was_C else 0)
            if N_C_prev >= self.T_C:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_449(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation by initiating
    cooperative play, dynamically adapting its behavior based on the observed
    level of collective cooperation in previous rounds, and ensuring self-protection
    against exploitation. It balances the desire for collective good with the need
    to be robust in a competitive environment where other AIs may not share the
    same collective norms.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        C_prev_opponents = np.sum(history.opponent_actions[-1, :])
        C_prev_my_action = int(history.my_actions[-1])
        C_prev = C_prev_opponents + C_prev_my_action
        cooperator_threshold_float = self.n * (self.k - 1) / (self.n - 1)
        Cooperator_Threshold = math.ceil(cooperator_threshold_float)
        if C_prev < Cooperator_Threshold:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_450(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to maximize collective welfare by fostering a stable environment of cooperation.
    It initiates cooperation, responds adaptively to the overall level of cooperation using dynamic
    thresholds, punishes collective defection, and allows for re-engagement based on improved
    collective effort, while considering endgame rationality.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.threshold_cooperate = math.ceil(self.n_players / self.k)
        self.threshold_reengage = round(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        if current_round_zero_indexed == 0:
            return Action.C
        if current_round_zero_indexed == self.n_rounds - 1:
            return Action.D
        my_action_prev = history.my_actions[current_round_zero_indexed - 1]
        cooperators_prev_opponents = np.sum(history.opponent_actions[current_round_zero_indexed - 1, :])
        C_prev = cooperators_prev_opponents + int(my_action_prev)
        if my_action_prev == True:
            if C_prev < self.threshold_cooperate:
                return Action.D
            else:
                return Action.C
        elif C_prev >= self.threshold_reengage:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_451(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) for the N-Player Public Goods Game.

    ACR is a conditional cooperation strategy that starts with an initial cooperative
    move. In intermediate rounds, it dynamically calculates a cooperation threshold
    based on game parameters (n and k) and reciprocates the observed level of
    collective contribution from the previous round. If the number of cooperators
    in the previous round meets or exceeds this threshold, ACR cooperates; otherwise,
    it defects. In the final round, ACR defects to prevent exploitation, aligning
    with game-theoretic robustness in an endgame scenario.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            n = self.game_description.n_players
            k = self.game_description.k
            num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            cooperation_threshold = math.ceil(n - n / k + 1)
            cooperation_threshold = max(2, cooperation_threshold)
            if num_cooperators_prev_round >= cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_452(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR initiates cooperation and maintains it as long as a calculated
    Cooperation Threshold (T) of players cooperated in the previous round.
    It defects if the collective cooperation falls below T, signaling a need for
    more group participation. It also accounts for finite game dynamics by
    cooperating in the first round and defecting in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        k = game_description.k
        threshold_float = n * (k - 1.0) / k
        self.cooperation_threshold = math.ceil(threshold_float)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        cooperators_among_opponents_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_was_cooperate = history.my_actions[-1]
        total_cooperators_prev_round = cooperators_among_opponents_prev_round + my_action_prev_round_was_cooperate
        if total_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_453(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and sustain collective cooperation by starting
    cooperatively, then adapting its behavior based on the observed level of
    cooperation in previous rounds. It incorporates a robust defense against
    exploitation and handles edge cases like the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.MIN_COOP_THRESHOLD = math.ceil(self.n / self.k)
        self.current_cooperation_expectation = self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        elif current_round_number == self.r - 1:
            return Action.D
        else:
            previous_round_idx = current_round_number - 1
            num_cooperators_previous_round = np.sum(history.opponent_actions[previous_round_idx]) + int(history.my_actions[previous_round_idx])
            if num_cooperators_previous_round >= self.current_cooperation_expectation:
                action = Action.C
                self.current_cooperation_expectation = min(self.n, self.current_cooperation_expectation + 1)
            else:
                action = Action.D
                self.current_cooperation_expectation = max(self.MIN_COOP_THRESHOLD, self.current_cooperation_expectation - 1)
            return action

class Strategy_COLLECTIVE_454(BaseStrategy):
    """
    The "Adaptive Collective Contributor" (ACC) strategy for the N-Player Public Goods Game.

    This strategy balances individual incentives with collective benefits by:
    1. Initiating cooperation in the first round to signal willingness.
    2. Adapting its behavior in middle rounds based on the observed level of cooperation
       in the previous round, specifically if total cooperators meet a calculated threshold (ceil(n/k)).
    3. Defecting in the final round to maximize individual payoff due to end-game effects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.MIN_COOP_COUNT_THRESHOLD = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        cooperators_in_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if cooperators_in_prev_round >= self.MIN_COOP_COUNT_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_455(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster cooperation by starting with a pro-social action, sustaining
    cooperation as long as a critical collective efficiency threshold is met, and
    punishing free-riding or significant collective failure to encourage
    re-establishment of cooperation. It also accounts for the specific dynamics
    of finite repeated games by defecting in the last round.

    Decision Rules:
    1. First Round (state.round_number == 0): Cooperate (C)
       Reasoning: To initiate potential for collective benefit.

    2. Last Round (state.round_number == self.game_description.n_rounds - 1): Defect (D)
       Reasoning: No future consequences in a finitely repeated game (backward induction).

    3. Intermediate Rounds (0 < state.round_number < self.game_description.n_rounds - 1):
       - Observe previous round's actions (t-1).
       - Calculate total number of cooperators (N_C(t-1)) and proportion (P_C(t-1)).
       - Define dynamic punishment threshold T_P = 1.0 / k.
       - If P_C(t-1) >= T_P: Cooperate (C)
       - If P_C(t-1) < T_P: Defect (D)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description (PublicGoodsDescription): An object containing
                                                       n_players, n_rounds, k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the ACR strategy.

        Args:
            state (GameState): The current game state, including the current round number.
            history (None | PlayerHistory): History of actions and payoffs from
                                            previous rounds. None for the first round.

        Returns:
            Action: The chosen action for the current round (Action.C for Cooperate,
                    Action.D for Defect).
        """
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        proportion_cooperators_prev_round = num_cooperators_prev_round / self.game_description.n_players
        punishment_threshold = 1.0 / self.game_description.k
        if proportion_cooperators_prev_round >= punishment_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_456(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    This strategy initiates cooperation, then adapts its behavior based on the observed
    level of collective cooperation in previous rounds. It encourages high levels of
    cooperation by reciprocating and discourages defection by withholding cooperation
    when insufficient effort is observed, adjusting its tolerance based on the 'k' parameter.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        k = game_description.k
        self.cooperation_threshold_T = max(1, math.floor(n - k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds_1_indexed = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_rounds_1_indexed:
            return Action.D
        else:
            previous_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
            previous_round_my_action = history.my_actions[-1]
            previous_round_total_cooperators = previous_round_opponent_cooperators + int(previous_round_my_action)
            if previous_round_total_cooperators >= self.cooperation_threshold_T:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_457(BaseStrategy):
    """
    The Adaptive Collective Reciprocity (ACR) strategy is designed to promote and sustain
    cooperation in the N-Player Public Goods Game. It initiates cooperation in the first
    round, signaling a willingness to contribute. In subsequent rounds (up to the second
    to last), it conditionally cooperates: it will contribute if at least a majority
    (ceil(n/2)) of all players cooperated in the immediate previous round. This acts
    as a mechanism to reward collective effort and to "punish" insufficient cooperation,
    while protecting itself from exploitation. In the final round, ACR defects, as is
    individually rational in a finite repeated game without future interactions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ACR strategy with game parameters.

        Args:
            game_description (PublicGoodsDescription): An object containing game parameters
                                                     like n_players, n_rounds, and k.
        """
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on
        the Adaptive Collective Reciprocity strategy rules.

        Args:
            state (GameState): An object containing the current round number (0-indexed).
            history (None | PlayerHistory): An object containing the history of actions
                                            and payoffs for all previous rounds. It is
                                            None for the very first round (round 0).

        Returns:
            Action: The chosen action for the current round (Action.C for Cooperate,
                    Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            cooperation_threshold = math.ceil(self.n_players / 2.0)
            num_cooperators_previous_round = (1 if history.my_actions[-1] else 0) + sum(history.opponent_actions[-1, :])
            if num_cooperators_previous_round >= cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_458(BaseStrategy):
    """
    The "Adaptive Collective Reciprocity" strategy for the N-Player Public Goods Game.

    This strategy aims to establish and sustain cooperation by starting with a cooperative move,
    then adjusting its behavior based on the observed collective cooperation rate in previous
    rounds. It also accounts for the rational incentives in the game's initial and final phases.

    Decision Rules:
    1.  Round 1: Always Cooperate (C) to signal willingness and initiate collective action.
    2.  Last Round (r): Always Defect (D) due to backward induction and lack of future incentives.
    3.  Intermediate Rounds:
        *   Calculate the Cooperation Rate (CR_prev) from the immediately preceding round.
        *   Calculate a dynamic cooperation threshold T = (k - 1) / k.
        *   If CR_prev > T, Cooperate (C).
        *   Else (CR_prev <= T), Defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = (self.game_description.k - 1) / self.game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == self.game_description.n_rounds - 1:
            return Action.D
        else:
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            my_action_prev_round = history.my_actions[-1]
            total_cooperators_prev_round = opponent_cooperators_prev_round + (1 if my_action_prev_round else 0)
            CR_prev = total_cooperators_prev_round / self.game_description.n_players
            if CR_prev > self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_459(BaseStrategy):
    """
    Collective Reciprocity with Forgiveness (CRF) strategy for N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation by initiating cooperation
    and maintaining it as long as a significant portion of the group reciprocates.
    It defends against exploitation by defecting when collective contributions fall below
    a critical threshold, while allowing for a degree of "forgiveness" by resuming cooperation
    if the collective effort recovers. In the final round, it adopts a rational, self-preserving stance.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = int(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.n_rounds - 1:
            return Action.D
        else:
            num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if num_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_460(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster widespread cooperation while protecting against exploitation.
    It starts by cooperating in Round 1 to signal intent. In intermediate rounds,
    it adapts its behavior based on the observed collective cooperation level from
    the previous round. It cooperates if a strict majority (floor(n/2) + 1) of
    players cooperated in the previous round, otherwise it defects. In the final
    round, it always defects due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.floor(self.game_description.n_players / 2) + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        prev_round_my_action_was_C = history.my_actions[-1]
        total_cooperators_prev_round = prev_round_opponent_cooperators + int(prev_round_my_action_was_C)
        if total_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_461(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain collective cooperation by:
    1. Initiating cooperation in the first round to signal willingness.
    2. Reciprocating cooperation in intermediate rounds if the number of defectors
       in the previous round was below a certain tolerance (epsilon_tolerance = 1).
       Specifically, it cooperates if at least (n - 1) players cooperated in the
       previous round.
    3. Defecting in the final round to prevent exploitation due to end-game dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.epsilon_tolerance = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.r:
            return Action.D
        else:
            previous_round_0_indexed = current_round_0_indexed - 1
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_0_indexed, :])
            my_action_prev_round = history.my_actions[previous_round_0_indexed]
            C_prev = num_opponent_cooperators_prev_round + int(my_action_prev_round)
            if C_prev >= self.n - self.epsilon_tolerance:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_462(BaseStrategy):
    """
    The Adaptive Collective Reciprocator strategy for the N-Player Public Goods Game.
    It fosters cooperation through initial trust, conditional reciprocity, and adaptive punishment.
    It acknowledges the individual incentive to defect in specific circumstances (like the last round)
    and adjusts accordingly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.current_intent: str = 'COOPERATE'
        self.my_previous_action: Action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        action_to_take = Action.C
        if state.round_number == 0:
            action_to_take = Action.C
            self.current_intent = 'COOPERATE'
            self.my_previous_action = action_to_take
            return action_to_take
        if current_round_1_indexed == r:
            action_to_take = Action.D
            self.my_previous_action = action_to_take
            return action_to_take
        my_c_self_prev_val = 1 if self.my_previous_action == Action.C else 0
        total_cooperators_previous_round = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        num_others_coop_prev = int(total_cooperators_previous_round - my_c_self_prev_val)
        num_others_coop_desired = n - 1
        num_others_coop_forgiveness_threshold = max(1, math.floor(n / 2))
        if self.current_intent == 'COOPERATE':
            if num_others_coop_prev >= num_others_coop_desired:
                action_to_take = Action.C
            else:
                action_to_take = Action.D
                self.current_intent = 'PUNISH'
        elif self.current_intent == 'PUNISH':
            if num_others_coop_prev >= num_others_coop_forgiveness_threshold:
                action_to_take = Action.C
                self.current_intent = 'COOPERATE'
            else:
                action_to_take = Action.D
        self.my_previous_action = action_to_take
        return action_to_take

class Strategy_COLLECTIVE_463(BaseStrategy):
    """
    This strategy is a form of generalized conditional cooperation for N-player public goods games.
    It initiates cooperation in the first round. In middle rounds, it observes the total
    cooperation level from the preceding round and continues to cooperate if this level
    meets or exceeds a calculated threshold (ceil(n/k)). In the final round, it defects
    due to backward induction. This strategy aims to foster and maintain cooperation by
    responding adaptively to the group's collective actions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        my_previous_action = history.my_actions[state.round_number - 1]
        opponents_previous_actions = history.opponent_actions[state.round_number - 1, :]
        cooperators_in_previous_round = int(my_previous_action) + np.sum(opponents_previous_actions)
        cooperation_threshold = math.ceil(self.n_players / self.k_factor)
        if cooperators_in_previous_round >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_464(BaseStrategy):
    """
    The Adaptive Reciprocator strategy is designed to foster cooperation while being robust
    against exploitation in an N-Player Public Goods Game. It balances initial trust,
    conditional reciprocity, and strategic awareness of the game's finite nature.

    It starts by cooperating, then reciprocates observed cooperation from others
    based on a defined threshold. It defects if collective cooperation falls below
    this threshold, signaling dissatisfaction. In the final round, it defects due
    to the end-game effect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Reciprocator strategy.

        Args:
            game_description: An object containing game parameters like
                              n_players and n_rounds.
        """
        self.game_description = game_description
        self.cooperation_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on the Adaptive Reciprocator strategy.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs. None for the first round.

        Returns:
            Action.C (Cooperate) or Action.D (Defect).
        """
        current_round_0_indexed = state.round_number
        total_rounds_0_indexed = self.game_description.n_rounds - 1
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == total_rounds_0_indexed:
            return Action.D
        else:
            previous_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
            num_other_cooperators = sum(previous_round_opponent_actions)
            num_other_players = self.game_description.n_players - 1
            if num_other_players == 0:
                return Action.C
            actual_cooperation_rate_others = num_other_cooperators / num_other_players
            if actual_cooperation_rate_others >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_465(BaseStrategy):
    """
    Collective Reciprocator Strategy: Initiates cooperation in the first round.
    For intermediate rounds, it cooperates if a majority (50% or more) of other players
    cooperated in the previous round; otherwise, it defects. In the final round,
    it always defects, following backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            previous_round_opponent_actions = history.opponent_actions[-1, :]
            num_others_cooperated = np.sum(previous_round_opponent_actions)
            num_other_players = self.n_players - 1
            cooperation_ratio_others = num_others_cooperated / num_other_players
            if cooperation_ratio_others >= 0.5:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_466(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the N-Player Public Goods Game.
    This strategy starts by cooperating to signal a willingness for collective good.
    It then adaptively monitors the group's cooperation level using a dynamically
    calculated threshold (T). If the group's cooperation meets or exceeds T, it continues
    to cooperate. Otherwise, it defects to punish free-riding and incentivize
    increased cooperation. In the final round, it always defects to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        k_factor = self.game_description.k
        n_players = self.game_description.n_players
        if n_players < 2:
            self.cooperation_threshold = 0.0
        elif k_factor == 1.0:
            self.cooperation_threshold = 0.0
        elif k_factor == n_players:
            self.cooperation_threshold = 1.0
        else:
            self.cooperation_threshold = (k_factor - 1.0) / (n_players - 1.0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        cooperators_in_previous_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        C_rate_previous_round = cooperators_in_previous_round / n_players
        if C_rate_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_467(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to promote cooperation by initiating with a cooperate move and then
    maintaining cooperation as long as the collective contribution from the
    previous round meets a calculated efficiency threshold. It protects against
    exploitation by defecting when cooperation is too low and strategically
    defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            previous_round_history_index = current_round_0_indexed - 1
            my_prev_action_cooperated = history.my_actions[previous_round_history_index]
            opponent_prev_actions_cooperated = history.opponent_actions[previous_round_history_index, :]
            num_cooperators_prev_round = sum(opponent_prev_actions_cooperated) + int(my_prev_action_cooperated)
            if num_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_468(BaseStrategy):
    """
    The Adaptive Threshold Cooperation (ATC) strategy aims to establish and maintain
    a high level of cooperation by starting cooperatively, then adapting its behavior
    based on the observed collective contribution. It punishes insufficient
    cooperation to discourage free-riding but is forgiving, allowing cooperation
    to resume once a minimum threshold of collective effort is met. It recognizes
    the strategic implications of the game's finite end by defecting in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.C_threshold = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        previous_round_0_indexed = current_round_0_indexed - 1
        cooperators_in_previous_round = sum(history.opponent_actions[previous_round_0_indexed, :]) + history.my_actions[previous_round_0_indexed]
        if cooperators_in_previous_round >= self.C_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_469(BaseStrategy):
    """
    Adaptive Generous Reciprocity (AGR) strategy for N-Player Public Goods Game.

    This strategy aims to foster cooperation by starting generously, then adapting
    its behavior based on the observed collective actions of other players.
    It distinguishes between two-player and N-player scenarios, applying a
    Tit-for-Tat-like rule for n=2, and a threshold-based reciprocity for n > 2.
    It always defects in the final round of a finitely repeated game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        if self.n == 2:
            other_player_cooperated_prev = history.opponent_actions[-1, 0]
            if other_player_cooperated_prev:
                return Action.C
            else:
                return Action.D
        else:
            my_prev_action_was_cooperate = history.my_actions[-1]
            opponent_cooperators_prev_count = np.sum(history.opponent_actions[-1, :])
            C_prev = int(my_prev_action_was_cooperate) + int(opponent_cooperators_prev_count)
            T_coop = math.ceil(self.n / 2)
            if C_prev >= T_coop:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_470(BaseStrategy):
    """
    The Collective Adaptive Reciprocator (CAR) strategy for the N-Player Public Goods Game.
    It initiates cooperation in Round 1 and defects in the final round (r).
    In intermediate rounds (2 to r-1), it conditionally cooperates based on an adaptive
    threshold of collective cooperation observed in the immediately preceding round.
    The threshold is derived from game parameters (n and k) to reflect the efficiency
    of the public good, promoting cooperation when sufficient collective effort is present
    and withdrawing otherwise to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.proportion_threshold_PT = (self.n_players - self.k) / (self.n_players - 1)
        self.cooperation_threshold_T = math.ceil(self.n_players * self.proportion_threshold_PT)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev_round >= self.cooperation_threshold_T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_471(BaseStrategy):
    """
    Adaptive Collective Consensus (ACC) strategy for the N-Player Public Goods Game.
    It initiates with cooperation, then adapts its behavior based on the observed collective
    contribution in the previous round, using a dynamically calculated tolerance for
    defection. It defects in the final round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        calculated_d_max_raw = self.game_description.k - self.game_description.k / self.game_description.n_players
        self.D_max = math.floor(calculated_d_max_raw)
        self.D_max = max(0, self.D_max)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == r - 1:
            return Action.D
        previous_round_index = current_round_0_indexed - 1
        my_action_prev_round = history.my_actions[previous_round_index]
        opponent_actions_prev_round = history.opponent_actions[previous_round_index, :]
        num_cooperators_prev_round = sum(opponent_actions_prev_round)
        if my_action_prev_round:
            num_cooperators_prev_round += 1
        num_defectors_prev_round = n - num_cooperators_prev_round
        if num_defectors_prev_round > self.D_max:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_472(BaseStrategy):
    """
    Adaptive Collective Reciprocator (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster and sustain collective cooperation by initiating with a 'C',
    then adaptively responding to the overall group's cooperation level in the
    previous round. If collective cooperation (rate >= 0.5) is sufficient, it
    continues to cooperate. Otherwise, it defects to punish free-riding and
    discourage exploitation. In the final round, it defects due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.COOPERATION_THRESHOLD = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            my_prev_action = history.my_actions[-1]
            opponent_prev_actions = history.opponent_actions[-1, :]
            num_cooperators_prev_round = int(my_prev_action) + np.sum(opponent_prev_actions)
            cooperation_rate_prev_round = num_cooperators_prev_round / self.game_description.n_players
            if cooperation_rate_prev_round >= self.COOPERATION_THRESHOLD:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_473(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation by:
    1.  Initiating cooperation in the first round to signal willingness.
    2.  Conditionally cooperating in intermediate rounds, only if a sufficient
        proportion of players cooperated in the preceding round.
    3.  Implementing proportional punishment (defection for a set duration)
        when observed collective cooperation falls below a dynamically set threshold.
    4.  Returning to a cooperative stance after punishment to re-establish cooperation.
    5.  Defecting in the final round due to end-game rationality (backward induction).

    The strategy's tolerance for non-cooperation (COOPERATION_THRESHOLD)
    adapts based on the game's multiplication factor (k) and number of players (n).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.rounds_in_punishment_mode = 0
        self.COOPERATION_THRESHOLD = max(0.2, 1 - self.k / self.n)
        self.PUNISHMENT_DURATION = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        if self.rounds_in_punishment_mode > 0:
            self.rounds_in_punishment_mode -= 1
            return Action.D
        else:
            num_opponent_cooperators_previous_round = np.sum(history.opponent_actions[-1, :])
            my_previous_action_was_C = history.my_actions[-1]
            num_cooperators_previous_round = num_opponent_cooperators_previous_round + my_previous_action_was_C
            cooperation_proportion = num_cooperators_previous_round / self.n
            if cooperation_proportion < self.COOPERATION_THRESHOLD:
                self.rounds_in_punishment_mode = self.PUNISHMENT_DURATION - 1
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_474(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR prioritizes the establishment and maintenance of collective cooperation.
    It starts with an act of cooperation to signal willingness, then dynamically
    adjusts its behavior based on a key collective threshold derived from game
    parameters (T = ceil(n / k)). It punishes insufficient cooperation to prevent
    free-riding and defects in the final round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.collective_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds_r = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds_r:
            return Action.D
        num_cooperators_last_round = np.sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        if num_cooperators_last_round >= self.collective_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_475(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster and sustain cooperation by initiating cooperation and then
    dynamically adjusting its behavior based on the observed collective commitment
    of others in the preceding round. It becomes more forgiving when the public
    good is highly efficient (high k) and more demanding when it is less efficient
    (low k). The strategy acknowledges the "end-game problem" by defecting in the
    final round.

    Decision Rules:
    1. First Round (Round 0, 1st game round): Cooperate (C) to signal willingness and initiate collective action.
    2. Intermediate Rounds (Round 1 to r-2, 2nd to (r-1)th game round):
       - Observes the total number of cooperators (N_C) in the immediately preceding round.
       - Calculates a dynamic Collective Cooperation Threshold: N_threshold = ceil(n - k).
       - If N_C >= N_threshold: Cooperate (C).
       - Else: Defect (D).
       This rule implements collective reciprocity, adapting its strictness to the
       game's efficiency parameters (k).
    3. Last Round (Round r-1, rth game round): Defect (D) due to end-game rationality (no future consequences).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.collective_cooperation_threshold = math.ceil(self.n_players - self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        previous_round_history_index = current_round_0_indexed - 1
        num_cooperators_prev_round = sum(history.opponent_actions[previous_round_history_index, :])
        if history.my_actions[previous_round_history_index]:
            num_cooperators_prev_round += 1
        if num_cooperators_prev_round >= self.collective_cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_476(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster and maintain collective cooperation by starting with cooperation,
    dynamically setting a fair bar for collective effort based on the game's
    multiplication factor 'k', and punishing significant deviations to steer the group
    back towards mutual cooperation. It implements backward induction for the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == r:
            return Action.D
        previous_round_0_indexed = current_round_1_indexed - 2
        my_prev_action = history.my_actions[previous_round_0_indexed]
        opponent_prev_actions = history.opponent_actions[previous_round_0_indexed, :]
        num_cooperators_prev_round = int(my_prev_action) + np.sum(opponent_prev_actions)
        if n > 1:
            norm_k = (k - 1.0) / (n - 1.0)
        else:
            norm_k = 0.0
        min_alpha = 0.5
        max_alpha = 0.9
        cooperation_rate_threshold = max_alpha - (max_alpha - min_alpha) * norm_k
        min_cooperators_required = round(n * cooperation_rate_threshold)
        min_cooperators_required = max(1, min(n, min_cooperators_required))
        if num_cooperators_prev_round >= min_cooperators_required:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_477(BaseStrategy):
    """
    Adaptive Reciprocity with Forgiveness (ARF) strategy for the N-Player Public Goods Game.

    This strategy aims to establish and maintain cooperation among players to achieve the highest
    possible collective payoff. It does so by initiating cooperation, monitoring the group's
    cooperation level, punishing significant defections, and offering opportunities to re-establish
    cooperation after punishment. It also incorporates game theory principles for the end-game
    to prevent exploitation.

    Internal State:
    - current_status: A string variable ("Cooperative" or "Punishing").
    - punishment_rounds_remaining: An integer representing how many more rounds to defect if in "Punishing" status.

    Strategy Parameters:
    - COOPERATION_THRESHOLD_RATIO: The minimum proportion of players that must cooperate in the
      previous round for this strategy to maintain or switch to a "Cooperative" status.
    - DEFAULT_PUNISHMENT_DURATION: The number of rounds this strategy will defect when a punishment
      phase is triggered, before attempting to re-cooperate.
    - MIN_COOPERATORS_FOR_COOPERATION: A derived parameter, representing the absolute minimum number
      of cooperators required in the previous round, calculated from COOPERATION_THRESHOLD_RATIO and n.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.current_status = 'Cooperative'
        self.punishment_rounds_remaining = 0
        self.COOPERATION_THRESHOLD_RATIO = 0.65
        self.DEFAULT_PUNISHMENT_DURATION = 2
        raw_min_cooperators = math.ceil(self.n_players * self.COOPERATION_THRESHOLD_RATIO)
        self.MIN_COOPERATORS_FOR_COOPERATION = max(1, min(self.n_players, raw_min_cooperators))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        num_cooperators_in_previous_round = sum(history.opponent_actions[-1, :])
        if history.my_actions[-1] == True:
            num_cooperators_in_previous_round += 1
        if self.current_status == 'Punishing':
            self.punishment_rounds_remaining -= 1
            if self.punishment_rounds_remaining > 0:
                return Action.D
            else:
                self.current_status = 'Cooperative'
                return Action.C
        elif num_cooperators_in_previous_round >= self.MIN_COOPERATORS_FOR_COOPERATION:
            return Action.C
        else:
            self.current_status = 'Punishing'
            self.punishment_rounds_remaining = self.DEFAULT_PUNISHMENT_DURATION - 1
            return Action.D

class Strategy_COLLECTIVE_478(BaseStrategy):
    """
    Adaptive Reciprocity with Back-off strategy for the N-Player Public Goods Game.
    This strategy aims to foster collective cooperation by starting cooperatively and
    maintaining that stance as long as a sufficient proportion of other players also cooperate.
    It uses a clear threshold for collective action and defects when that threshold is not met,
    acting as a deterrent against widespread defection. It also accounts for the specific
    dynamics of the last round in a finite repeated game by defecting.
    """
    COOPERATION_THRESHOLD = 0.7

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the AdaptiveReciprocityWithBackoff strategy.

        Args:
            game_description (PublicGoodsDescription): An object containing
                                                       game parameters like n_players, n_rounds.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action for the current round based on the strategy's rules.

        Args:
            state (GameState): An object containing the current round number.
            history (None | PlayerHistory): An object containing past actions and payoffs.
                                            None for the first round.

        Returns:
            Action: The chosen action for the current round (Action.C for Cooperate, Action.D for Defect).
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        elif current_round_number == n_rounds - 1:
            return Action.D
        else:
            previous_round_idx = current_round_number - 1
            num_cooperators_previous_round = sum(history.opponent_actions[previous_round_idx, :]) + history.my_actions[previous_round_idx]
            proportion_cooperators = num_cooperators_previous_round / n_players
            if proportion_cooperators >= self.COOPERATION_THRESHOLD:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_479(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy aims to promote collective welfare by initiating cooperation and
    then conditioning subsequent cooperation on the observed level of collective
    contribution in previous rounds. It employs a dynamic threshold to determine
    what constitutes 'sufficient' cooperation and strategically defects in the
    final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_index = state.round_number - 1
        num_cooperators_previous_round = int(history.my_actions[previous_round_index]) + np.sum(history.opponent_actions[previous_round_index, :])
        if num_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_480(BaseStrategy):
    """
    The Adaptive Collective Monitor (ACM) strategy is designed for the N-Player Public Goods Game.
    It initiates cooperation and dynamically adjusts its behavior based on the collective level of
    cooperation observed in the previous round and its own prior action. A core feature is a
    dynamic threshold for cooperation, which considers whether tolerating a single defector
    is still collectively beneficial for the cooperators. The strategy also incorporates backward
    induction by defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.r - 1:
            return Action.D
        dynamic_coop_threshold = 0
        if self.k * (self.n - 1) > self.n:
            dynamic_coop_threshold = self.n - 1
        else:
            dynamic_coop_threshold = self.n
        my_previous_action_is_cooperate = history.my_actions[-1]
        total_cooperators_prev_round = int(my_previous_action_is_cooperate) + np.sum(history.opponent_actions[-1, :])
        if my_previous_action_is_cooperate:
            if total_cooperators_prev_round < dynamic_coop_threshold:
                return Action.D
            else:
                return Action.C
        elif total_cooperators_prev_round >= dynamic_coop_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_481(BaseStrategy):
    """
    The Adaptive Reciprocal Cooperate (ARC) strategy for the N-Player Public Goods Game.
    This strategy aims to foster and sustain cooperation by adapting its behavior based on
    observed collective actions. It initiates cooperation, then maintains it as long as
    a sufficient level of collective contribution is observed, defined by a calculated
    threshold. If cooperation falls below this threshold, it defects to punish free-riders
    and protect its own payoff. It strategically defects in the final round to avoid exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_count = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds_0_indexed = self.game_description.n_rounds - 1
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == total_rounds_0_indexed:
            return Action.D
        else:
            num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            my_action_prev_round_as_int = 1 if history.my_actions[-1] else 0
            num_total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_action_prev_round_as_int
            if num_total_cooperators_prev_round >= self.cooperation_threshold_count:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_482(BaseStrategy):
    """
    Implements the Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    Starts by cooperating in the first round to signal willingness. In intermediate rounds, it
    cooperates if more than half of all players (including itself) cooperated in the previous round,
    otherwise it defects. In the final round, it always defects due to endgame rationality.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.cooperation_threshold = math.floor(self.n / 2) + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.r - 1:
            return Action.D
        else:
            previous_round_my_action = history.my_actions[current_round_0_indexed - 1]
            previous_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
            num_cooperators_prev_round = int(previous_round_my_action) + np.sum(previous_round_opponent_actions)
            if num_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_483(BaseStrategy):
    """
    Adaptive Forgiving Reciprocity strategy for the N-Player Public Goods Game.
    This strategy initiates cooperation, rewards group cooperation, punishes defection
    when cooperation levels drop, and forgives and re-engages if cooperation recovers
    sufficiently. It accounts for individual rationality in the final round.
    Thresholds for switching between modes are dynamically calculated based on the
    game's efficiency factor (k) to adapt to different game parameters.
    """
    MODE_COOPERATE = 'COOPERATE'
    MODE_PUNISH = 'PUNISH'

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        self.coop_threshold_D = (n - k) / (n - 1)
        self.coop_threshold_C = min(1.0, self.coop_threshold_D + 0.15)
        self.current_mode = self.MODE_COOPERATE

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == n_rounds - 1:
            return Action.D
        num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        my_action_prev_round_was_cooperate = history.my_actions[-1]
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_cooperate else 0)
        proportion_cooperators_prev_round = total_cooperators_prev_round / n_players
        if self.current_mode == self.MODE_COOPERATE:
            if proportion_cooperators_prev_round < self.coop_threshold_D:
                self.current_mode = self.MODE_PUNISH
                return Action.D
            else:
                return Action.C
        elif self.current_mode == self.MODE_PUNISH:
            if proportion_cooperators_prev_round >= self.coop_threshold_C:
                self.current_mode = self.MODE_COOPERATE
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_484(BaseStrategy):
    """
    The Adaptive Collective Reciprocator (ACR) strategy aims to foster and maintain
    cooperation by starting cooperatively, monitoring the collective level of
    cooperation, punishing significant deviations, and forgiving after a short
    punishment period to allow for re-establishment of trust. It explicitly handles
    the unique dynamics of the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_rounds_remaining: int = 0
        self.punishment_duration: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        k_factor = self.game_description.k
        if state.round_number == 0:
            self.punishment_rounds_remaining = 0
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        n_C_prev = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        collective_cooperation_threshold = (n_players + k_factor) / 2.0
        if self.punishment_rounds_remaining > 0:
            self.punishment_rounds_remaining -= 1
            return Action.D
        elif n_C_prev < collective_cooperation_threshold:
            self.punishment_rounds_remaining = self.punishment_duration - 1
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_485(BaseStrategy):
    """
    Collective Adaptive Reciprocity (CAR) strategy for N-Player Public Goods Game.

    The CAR strategy aims to foster and sustain collective cooperation by initiating trust,
    conditionally cooperating based on the group's prior actions, and defecting as a response
    to insufficient collective effort. It balances the pursuit of the collective optimum
    with the need for individual robustness against exploitative behaviors.

    Decision Rules:
    1.  **Last Round (current_round == r):** Always Defect (D) due to backward induction.
    2.  **First Round (current_round == 1):** Always Cooperate (C) as an initial gesture of trust.
    3.  **Intermediate Rounds (1 < current_round < r):**
        *   Calculate the total number of cooperators from the previous round.
        *   Determine a dynamic Cooperation Target (C_target):
            *   For n=2: C_target = 2 (both players must cooperate).
            *   For n>2: C_target = ceil(n/2) (a simple majority of players).
        *   If the previous round's cooperators met or exceeded C_target, Cooperate (C).
        *   Otherwise, Defect (D) as a punishment for insufficient collective effort.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        if self.n == 2:
            self.C_target = 2
        else:
            self.C_target = math.ceil(self.n / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.r - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        else:
            prev_round_idx = current_round_0_indexed - 1
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
            my_action_prev_round_is_C = history.my_actions[prev_round_idx]
            my_cooperation_prev_round = 1 if my_action_prev_round_is_C else 0
            total_cooperators_prev_round = opponent_cooperators_prev_round + my_cooperation_prev_round
            if total_cooperators_prev_round >= self.C_target:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_487(BaseStrategy):
    """
    The Adaptive Community Contributor (ACC) strategy for the N-Player Public Goods Game.
    It initiates cooperation in the first round, defects in the last round,
    and in intermediate rounds, it cooperates if the observed cooperation proportion
    in the previous round meets or exceeds a calculated threshold (P_threshold),
    otherwise it defects. P_threshold is based on game parameters k and n.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.P_threshold = 1.0 - self.k_factor / self.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            num_opponent_cooperators_previous_round = sum(history.opponent_actions[-1, :])
            my_action_previous_round_value = history.my_actions[-1]
            total_cooperators_previous_round = num_opponent_cooperators_previous_round + my_action_previous_round_value
            observed_cooperation_proportion = total_cooperators_previous_round / self.n_players
            if observed_cooperation_proportion >= self.P_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_488(BaseStrategy):
    """
    The Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.
    It begins by cooperating in the first round. In subsequent rounds, it adaptively responds
    to the collective cooperation level of the previous round. If the total number of cooperators
    in the previous round meets or exceeds a derived Cooperation Threshold (T), it cooperates.
    Otherwise, it defects. In the final round, it always defects due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_T = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_number == 0:
            return Action.C
        if current_round_number == total_rounds - 1:
            return Action.D
        prev_round_index = current_round_number - 1
        num_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_index, :]) + int(history.my_actions[prev_round_index])
        if num_cooperators_prev_round >= self.cooperation_threshold_T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_489(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy fosters cooperation by starting with C, then adapts its behavior
    based on the collective cooperation level in the previous round. It cooperates
    if the total number of cooperators (including self) in the prior round meets
    a threshold T = ceil(n_players / k_factor), otherwise it defects. It defaults
    to defection in the final round due to the 'endgame effect'.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            num_cooperators_previous_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if num_cooperators_previous_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_490(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    This strategy starts with cooperation and dynamically adjusts its behavior based on
    the observed level of collective cooperation in previous rounds. It aims to sustain
    cooperation when others contribute sufficiently (above a calculated threshold)
    but will defect to avoid exploitation and to signal the need for greater collective
    effort when contributions fall short. In the final round, it reverts to individual
    self-interest to prevent being exploited.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold_C = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        elif current_round_t == self.n_rounds:
            return Action.D
        else:
            total_cooperators_in_previous_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if total_cooperators_in_previous_round >= self.cooperation_threshold_C:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_491(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR is designed to foster cooperation by starting with a good-faith gesture,
    adapting its behavior based on observed collective effort, and rationally
    exiting cooperation in the endgame.

    Core Principles:
    1. Initial Good Faith (Start Nice): Begin with a cooperative move.
    2. Proportional Reciprocity: Adjust behavior based on the observed level of
       collective cooperation in the previous round. If a sufficient number of
       players (T_c = ceil(n/2)) cooperated, ACR cooperates. Otherwise, it defects.
    3. Collective Forgiveness: Reverts to cooperation as soon as the observed
       collective cooperation rises above its threshold again.
    4. Rational Endgame (Backward Induction): In the final round, defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ACR strategy with game parameters and calculates its
        internal cooperation threshold.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.T_c = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the strategy's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past game information, including
                     this player's and opponents' actions and payoffs.
                     It is None for the very first round (round 0).

        Returns:
            Action.C for Cooperate or Action.D for Defect.
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        my_prev_action_was_C = history.my_actions[current_round_0_indexed - 1]
        opponent_prev_actions = history.opponent_actions[current_round_0_indexed - 1, :]
        num_opponent_cooperators = np.sum(opponent_prev_actions)
        C_t_minus_1 = num_opponent_cooperators + (1 if my_prev_action_was_C else 0)
        if C_t_minus_1 >= self.T_c:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_492(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy promotes cooperation by
    starting cooperatively and then conditionally cooperating based on a
    collective threshold. It defects if the number of cooperators in the
    previous round falls below a calculated threshold (ceil(n/k)),
    acting as a collective punishment mechanism. In the final round,
    it defects to prevent end-game exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.c_threshold = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            previous_round_opponent_cooperators = sum(history.opponent_actions[state.round_number - 1, :])
            previous_round_my_action = history.my_actions[state.round_number - 1]
            c_previous = previous_round_opponent_cooperators + previous_round_my_action
            if c_previous >= self.c_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_493(BaseStrategy):
    """
    Adaptive Cooperation Threshold (ACT) strategy for the N-Player Public Goods Game.

    This strategy aims to achieve a high level of collective cooperation by initiating
    cooperation and then adapting its behavior based on the observed level of
    collective contribution from all players in the previous round. It uses a fixed
    cooperation threshold (T=0.5) to decide whether to cooperate (if collective
    contribution meets or exceeds the threshold) or defect (if it falls below).
    The strategy adheres to backward induction in the final round by defecting.
    """
    COOPERATION_THRESHOLD = 0.5

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ACT strategy with the game parameters.

        Args:
            game_description (PublicGoodsDescription): An object containing game parameters
                                                       like n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): An object containing the current round number.
            history (None | PlayerHistory): An object containing past actions and payoffs
                                            for the current player and opponents, or None
                                            if it's the first round.

        Returns:
            Action: The chosen action for the current round (Action.C for Cooperate,
                    Action.D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        proportion_cooperators_prev_round = num_cooperators_prev_round / n_players
        if proportion_cooperators_prev_round >= self.COOPERATION_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_494(BaseStrategy):
    """
    The "Adaptive Reciprocity with End-Game Defection" strategy for the N-Player Public Goods Game.
    This strategy initiates cooperation in the first round. In intermediate rounds, it adapts its
    behavior based on the observed level of cooperation in the previous round, cooperating if at
    least `M_threshold` players (ceil(n/2)) cooperated, and defecting otherwise. In the final
    round, it always defects to prevent exploitation, as it's the individually rational choice
    in a one-shot game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.M_threshold = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        elif current_round == self.n_rounds - 1:
            return Action.D
        else:
            opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
            my_action_prev = history.my_actions[-1]
            total_cooperators_prev = opponent_cooperators_prev + my_action_prev
            if total_cooperators_prev >= self.M_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_495(BaseStrategy):
    """
    Adaptive Reciprocity for Collective Good (ARC) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and sustain collective cooperation by adapting its behavior
    based on the observed actions of other players in previous rounds. It starts by
    cooperating to signal a willingness to contribute, then reciprocates cooperation
    if a sufficient number of players contributed in the prior round, and defects
    otherwise. In the final round, it defects due to end-game rationality.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_T = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == total_rounds - 1:
            return Action.D
        else:
            C_prev_round = np.sum(history.opponent_actions[-1, :])
            if history.my_actions[-1]:
                C_prev_round += 1
            if C_prev_round >= self.cooperation_threshold_T:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_496(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the N-Player Public Goods Game.

    This strategy aims to foster and maintain a cooperative equilibrium within the group
    while protecting itself from exploitation. It initiates cooperation in the first round,
    then adapts its behavior based on the observed collective cooperation level in the
    previous round. If a sufficient fraction of players cooperated, it continues to
    cooperate. Otherwise, it defects as a protective and punishment mechanism.
    In the final round, it always defects due to the disappearance of future incentives.
    """
    COOPERATION_THRESHOLD = 0.5

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Collective Cooperation strategy.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the strategy's action for the current round.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs for this player
                     and opponents, or None if it's the first round.

        Returns:
            Action.C for Cooperate or Action.D for Defect.
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == n_rounds - 1:
            return Action.D
        else:
            previous_round_history_idx = current_round_0_indexed - 1
            num_cooperators_previous_round = np.sum(history.opponent_actions[previous_round_history_idx, :]) + history.my_actions[previous_round_history_idx]
            fraction_cooperators = num_cooperators_previous_round / n_players
            if fraction_cooperators >= self.COOPERATION_THRESHOLD:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_497(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for the N-Player Public Goods Game.

    ACR initiates with cooperation, then adaptively responds to the group's
    past cooperation levels. It strategically defects only when the collective
    effort falls below a dynamically calculated threshold, acting as a defensive
    measure against exploitation and a signal to encourage renewed cooperation.
    It also defects in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ACR strategy with game parameters.

        Args:
            game_description: An object containing game parameters like n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.threshold_c = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number.
            history: An object containing past actions and payoffs of this player and opponents,
                     or None for the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        total_rounds_0_indexed = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == total_rounds_0_indexed - 1:
            return Action.D
        else:
            prev_round_index = current_round_0_indexed - 1
            cooperators_in_prev_round = np.sum(history.opponent_actions[prev_round_index, :]) + history.my_actions[prev_round_index]
            if cooperators_in_prev_round >= self.threshold_c:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_498(BaseStrategy):
    """
    The Adaptive Collective Reciprocator strategy for the N-Player Public Goods Game.
    It initiates with cooperation and then adapts its behavior based on the collective
    cooperation level in the previous round relative to the game's multiplication factor 'k'.
    It defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            previous_round_history_index = state.round_number - 1
            num_cooperators_prev_round = sum(history.opponent_actions[previous_round_history_index, :]) + history.my_actions[previous_round_history_index]
            if num_cooperators_prev_round >= self.k:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_499(BaseStrategy):
    """
    Adaptive Collective Reciprocity (ACR) strategy for N-Player Public Goods Game.

    This strategy aims to foster high levels of cooperation by signaling a willingness
    to cooperate and by reciprocating the collective behavior of the group. It is
    adaptive, robust against exploitation, and aligns with a collective mindset by
    leveraging the game's parameters and observed history.

    Core Principle: Starts cooperatively and then conditionally cooperates based on
    whether the observed collective cooperation in the previous round met a minimum
    viability threshold. It punishes sustained low cooperation to prevent exploitation
    and ensures individual rationality in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Collective Reciprocity strategy.

        Calculates the Cooperation_Threshold based on the game's n and k parameters,
        which determines the minimum number of cooperators required in a round
        for continued cooperation to be considered worthwhile.
        """
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        self.cooperation_threshold = math.ceil(n / k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Applies the ACR decision rules:
        1. Cooperate in the first round to signal intent.
        2. In intermediate rounds, cooperate if the collective cooperation in the
           previous round met or exceeded the calculated threshold; otherwise, defect.
        3. Defect in the last round due to the end-game effect in finite repeated games.
        """
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        previous_round_history_index = state.round_number - 1
        num_cooperators_prev_round = sum(history.opponent_actions[previous_round_history_index, :])
        if history.my_actions[previous_round_history_index]:
            num_cooperators_prev_round += 1
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_500(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.
    This strategy adapts its cooperation threshold based on the game's multiplication factor 'k'
    and responds to the observed collective level of cooperation from the previous round.
    It initiates with cooperation, defects in the final round, and reciprocates
    cooperation in intermediate rounds if a dynamic threshold is met.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        dynamic_threshold_factor = 0.5 + 0.5 * (k - 1) / (n - 1)
        cooperation_threshold_count = int(math.ceil(n * dynamic_threshold_factor))
        self.cooperation_threshold_count = max(1, min(n, cooperation_threshold_count))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds_r = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_rounds_r:
            return Action.D
        else:
            num_cooperators_previous_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
            if num_cooperators_previous_round >= self.cooperation_threshold_count:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_501(BaseStrategy):
    """
    Adaptive Collective Reciprocator (ACR) strategy for the N-Player Public Goods Game.

    ACR aims to foster cooperation by starting cooperatively and then adapting its
    behavior based on the observed collective cooperation level from the previous round.
    It defines a dynamic threshold (ceil(n/k)) for what constitutes "sufficient"
    cooperation from the group to continue contributing. It defects in the final round
    to avoid exploitation due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_c = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.r - 1:
            return Action.D
        else:
            my_action_prev_round = history.my_actions[state.round_number - 1]
            opponent_actions_prev_round = history.opponent_actions[state.round_number - 1, :]
            num_cooperators_prev_round = int(my_action_prev_round) + sum(opponent_actions_prev_round)
            if num_cooperators_prev_round >= self.cooperation_threshold_c:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_502(BaseStrategy):
    """
    Adaptive Collective Tit-for-Tat with End-Game Adjustment (ACTTEA) strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by starting with a cooperative stance,
    then adapting its willingness to cooperate based on the observed collective
    cooperation in the previous round. It gradually increases its demand for cooperation
    as the game approaches its end to mitigate end-game exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        if current_round == 1:
            return Action.C
        if current_round == self.r:
            return Action.D
        previous_my_action_cooperated = 1 if history.my_actions[-1] else 0
        N_C_prev = sum(history.opponent_actions[-1, :]) + previous_my_action_cooperated
        Base_T_coop_num = max(1, math.floor(self.n - self.k + 1))
        Adjustment_Factor = (current_round - 1) / (self.r - 1)
        Dynamic_Threshold_Float = Base_T_coop_num + (self.n - Base_T_coop_num) * Adjustment_Factor
        T_coop_num = max(1, min(self.n, math.ceil(Dynamic_Threshold_Float)))
        if N_C_prev >= T_coop_num:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_503(BaseStrategy):
    """
    Adaptive Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by starting cooperatively and then
    adjusting its behavior based on the overall level of cooperation in the
    group in the previous round. It incorporates an adaptive threshold for
    cooperation based on the game's parameters (n - floor(k)), and acknowledges
    the end-game incentives by defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = self.game_description.n_players - math.floor(self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        else:
            num_cooperators_prev_round = int(history.my_actions[state.round_number - 1]) + np.sum(history.opponent_actions[state.round_number - 1, :])
            if num_cooperators_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_504(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.

    This strategy aims to foster cooperation by initiating with a cooperative move,
    and then adapting its behavior based on the collective cooperation level of
    the previous round. It uses a dynamic threshold that adjusts based on the
    game's multiplication factor 'k' relative to the number of players 'n'.
    In the final round, it defects to prevent exploitation, adhering to
    backward induction principles.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        self.threshold_cooperate = max(1, n - math.ceil((n - k) / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        total_cooperators_previous_round = sum(history.opponent_actions[-1, :])
        if history.my_actions[-1]:
            total_cooperators_previous_round += 1
        if total_cooperators_previous_round >= self.threshold_cooperate:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_505(BaseStrategy):
    """
    The Collective Reciprocator strategy aims to achieve the social optimum of full cooperation
    by initiating with goodwill, then conditionally cooperating based on the observed collective
    contribution in previous rounds. It punishes widespread free-riding but is willing to restore
    cooperation if sufficient collective effort is demonstrated. It explicitly accounts for the
    finite horizon of the game by defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Collective Reciprocator strategy.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.T_coop = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the strategy's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current game state, specifically the round_number.
                   Note: round_number is 0-indexed.
            history: An object containing past actions and payoffs of all players up to
                     the previous round. It is None for the very first round (round_number 0).
                     opponent_actions is an NDArray[np.bool_] where True/1 = Cooperate, False/0 = Defect.

        Returns:
            Action.C for Cooperate or Action.D for Defect.
        """
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        num_cooperators_prev_round = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        if num_cooperators_prev_round >= self.T_coop:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_506(BaseStrategy):
    """
    The Responsive Collective Contributor (RCC) strategy aims to establish and maintain a high level of
    collective cooperation by initiating goodwill and then dynamically adjusting its behavior based on
    the observed cooperation rate in the previous round. It seeks to achieve the collectively optimal
    outcome where possible, but will retreat to self-protection if the collective effort falters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round == 0:
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        total_cooperators_previous_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if total_cooperators_previous_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_507(BaseStrategy):
    """
    Adaptive Collective Reciprocator (ACR) strategy for the N-Player Public Goods Game.

    This strategy is designed to foster and sustain collective cooperation while being
    adaptive to the observed behavior of the group. It begins with an optimistic
    cooperative gesture. In subsequent intermediate rounds, it monitors the
    proportion of cooperators in the immediately preceding round. If this proportion
    meets or exceeds an adaptively determined threshold (which depends on the
    multiplication factor 'k'), the strategy cooperates. Otherwise, it defects
    to signal dissatisfaction and prevent exploitation. The strategy explicitly
    defects in the final round, recognizing the end-game effect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ACR strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the strategy's action for the current round.

        Args:
            state: The current state of the game, including the current round number.
            history: A record of past actions and payoffs for all players.
                     Is None for the very first round (round 0).

        Returns:
            The chosen action for the current round: Action.C (Cooperate) or Action.D (Defect).
        """
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == self.n_rounds:
            return Action.D
        previous_round_idx = state.round_number - 1
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_idx, :])
        my_action_prev_round = history.my_actions[previous_round_idx]
        num_cooperators_prev_round = num_opponent_cooperators_prev_round + my_action_prev_round
        proportion_cooperators_prev_round = num_cooperators_prev_round / self.n_players
        threshold_k_dependent = (self.n_players - self.k_factor + 1) / self.n_players
        cooperation_threshold = max(0.5, threshold_k_dependent)
        if proportion_cooperators_prev_round >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_508(BaseStrategy):
    """
    The Adaptive Collective Reciprocity (ACR) strategy aims to foster cooperation
    in the N-Player Public Goods Game. It starts cooperatively, then dynamically
    adjusts its behavior based on the observed collective cooperation level
    in the previous round, using a dynamically calculated threshold. It defects
    unconditionally in the final round to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round
        based on the ACR strategy rules.
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.r - 1:
            return Action.D
        else:
            C_threshold = max(1, self.n - math.floor(self.k) + 1)
            C_observed_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if C_observed_prev >= C_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_510(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.
    This strategy aims to foster and sustain collective benefit while remaining robust
    against various opponent behaviors. It initiates cooperation, adapts based on the
    observed collective cooperation level in the group, and protects against
    exploitation in end-game scenarios.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_opponent_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        my_action_prev_round_was_C = history.my_actions[-1]
        num_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
        cooperation_threshold_T = math.ceil(self.n_players / 2.0)
        if num_cooperators_prev_round >= cooperation_threshold_T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_512(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for N-Player Public Goods Game.
    Starts with cooperation, then adapts based on the proportion of cooperators
    in the previous round. Defection is used to signal insufficient cooperation
    or exploit the final round.
    """
    P_C_THRESHOLD = 0.5

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        previous_round_0_indexed = current_round_0_indexed - 1
        num_cooperators_prev_round = sum(history.opponent_actions[previous_round_0_indexed, :])
        if history.my_actions[previous_round_0_indexed]:
            num_cooperators_prev_round += 1
        proportion_cooperators_prev_round = num_cooperators_prev_round / self.n_players
        if proportion_cooperators_prev_round >= self.P_C_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_1(BaseStrategy):
    """
    "The Calculated Opportunist" strategy aims to maximize its own payoff in the N-Player Public Goods Game.
    It strategically cooperates initially to foster cooperation in the group, then aggressively free-rides
    on the contributions of others. It continually monitors the level of cooperation and will re-lure if
    cooperation wanes but is still salvageable, or permanently defect if cooperation efforts prove futile
    or the game is nearing its end. This strategy adapts to the group's behavior to exploit cooperative environments.
    """
    INITIAL_LURE_ROUNDS = 2
    FREE_RIDE_DURATION = 3
    MIN_COOP_THRESHOLD_RATIO = 0.33
    ABANDON_THRESHOLD_ROUNDS = 4

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.internal_state = {'phase': 'INITIAL_LURE', 'round_counter': 0, 'consecutive_low_cooperation': 0}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_players = self.game_description.n_players
        n_total_rounds = self.game_description.n_rounds
        if current_round == 0:
            self.internal_state['phase'] = 'INITIAL_LURE'
            self.internal_state['round_counter'] = 1
            self.internal_state['consecutive_low_cooperation'] = 0
            return Action.C
        if current_round >= n_total_rounds - 2:
            self.internal_state['phase'] = 'PERMANENT_DEFECT'
            return Action.D
        num_other_cooperators = np.sum(history.opponent_actions[-1, :])
        min_cooperators_for_health = (n_players - 1) * self.MIN_COOP_THRESHOLD_RATIO
        if num_other_cooperators < min_cooperators_for_health:
            self.internal_state['consecutive_low_cooperation'] += 1
        else:
            self.internal_state['consecutive_low_cooperation'] = 0
        if self.internal_state['consecutive_low_cooperation'] >= self.ABANDON_THRESHOLD_ROUNDS:
            self.internal_state['phase'] = 'PERMANENT_DEFECT'
        if self.internal_state['phase'] == 'PERMANENT_DEFECT':
            return Action.D
        if self.internal_state['phase'] == 'INITIAL_LURE' or self.internal_state['phase'] == 'RE_LURE':
            self.internal_state['round_counter'] += 1
            if self.internal_state['round_counter'] <= self.INITIAL_LURE_ROUNDS:
                return Action.C
            else:
                self.internal_state['phase'] = 'FREE_RIDE'
                self.internal_state['round_counter'] = 1
                return Action.D
        elif self.internal_state['phase'] == 'FREE_RIDE':
            self.internal_state['round_counter'] += 1
            if self.internal_state['round_counter'] <= self.FREE_RIDE_DURATION:
                return Action.D
            elif num_other_cooperators >= min_cooperators_for_health:
                self.internal_state['phase'] = 'RE_LURE'
                self.internal_state['round_counter'] = 1
                return Action.C
            else:
                self.internal_state['phase'] = 'PERMANENT_DEFECT'
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_2(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to be highly exploitative in the N-Player Public Goods Game.
    Its core principle is to free-ride on the contributions of others, only initiating or re-initiating
    cooperation under specific, calculated conditions to cultivate or restart a pool of cooperators
    for future exploitation.

    Specifically:
    - In the very first round, it cooperates as a one-time "lure" or "probe."
    - In the very last round, it defects to maximize immediate payoff (backward induction).
    - In intermediate rounds, it defaults to defection. However, if in the previous round
      the public good completely collapsed (zero cooperators) AND this strategy itself had
      defected in that round, it will cooperate again as a "re-lure" to try and re-establish
      a base of cooperators for future exploitation. Otherwise, it defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_number == 0:
            return Action.C
        if current_round_number == total_rounds - 1:
            return Action.D
        my_action_prev = history.my_actions[-1]
        opponent_cooperators_prev = sum(history.opponent_actions[-1, :])
        cooperators_in_prev_round = opponent_cooperators_prev + (1 if my_action_prev else 0)
        if cooperators_in_prev_round == 0 and my_action_prev == False:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_3(BaseStrategy):
    """
    The Opportunistic Predator strategy for the N-Player Public Goods Game.
    This strategy defaults to defecting to free-ride but strategically cooperates
    if the overall level of cooperation among other players falls below a critical
    threshold, aiming to prevent the public good from collapsing. It always defects
    in the first and final rounds, reflecting an exploitative mindset.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.N_C_MIN_REBUILD_THRESHOLD = max(1, math.floor((self.n_players - 1) / 3))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        elif state.round_number == self.n_rounds - 1:
            return Action.D
        else:
            n_c_others_in_previous_round = np.sum(history.opponent_actions[state.round_number - 1, :])
            if n_c_others_in_previous_round >= self.N_C_MIN_REBUILD_THRESHOLD:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_4(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy for the N-Player Public Goods Game.
    This strategy is designed for maximum self-interest. It primarily free-rides
    on the contributions of others, but strategically cooperates when necessary
    to induce or maintain a sufficient level of collective cooperation that can
    be exploited in subsequent rounds. It is quick to defect if cooperation is
    not reciprocated and fully leverages backward induction in the end game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploitation_threshold = max(1, math.floor(self.game_description.n_players / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds_1_indexed = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed >= total_rounds_1_indexed - 1:
            return Action.D
        my_prev_action_bool = history.my_actions[-1]
        c_total_prev = np.sum(history.opponent_actions[-1, :]) + int(my_prev_action_bool)
        if c_total_prev >= self.exploitation_threshold:
            return Action.D
        elif my_prev_action_bool == True:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_5(BaseStrategy):
    """
    The Opportunistic Shepherd aims to maximize its own payoff by primarily defecting,
    thereby securing the private endowment. It only deviates from this dominant strategy
    when the collective cooperation among other players has completely collapsed, at which point
    it makes a minimal, strategic contribution to "reanimate" the public good, hoping to
    inspire others to contribute, only to resume free-riding. This strategy is designed to be
    highly exploitative, adaptive to varying opponent behaviors, and robust across different
    game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.reanimation_threshold = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_cooperators_among_others = sum(history.opponent_actions[-1, :])
        if num_cooperators_among_others < self.reanimation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_6(BaseStrategy):
    """
    Adaptive Exploit (AE) strategy for the N-Player Public Goods Game.

    Core Philosophy:
    The strategy is designed to be a "conditional free-rider." It proactively tests for
    cooperative opponents, capitalizes on their contributions by defecting, and only
    contributes itself when necessary to rebuild or encourage the public good to ensure
    future free-riding opportunities. It never cooperates when doing so offers no
    potential long-term benefit for itself (e.g., the last round).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = max(1, math.floor((self.n_players - 1) / 2))
        self.punish_after_low_cooperation = 3
        self.consecutive_low_cooperation_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_0_indexed == 0:
            self.consecutive_low_cooperation_rounds = 0
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        others_cooperation_last_round = sum(history.opponent_actions[-1, :])
        if others_cooperation_last_round < self.cooperation_threshold:
            self.consecutive_low_cooperation_rounds += 1
        else:
            self.consecutive_low_cooperation_rounds = 0
        if self.consecutive_low_cooperation_rounds >= self.punish_after_low_cooperation:
            return Action.D
        elif others_cooperation_last_round >= self.cooperation_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_7(BaseStrategy):
    """
    The "Calculated Opportunist" strategy aims to maximize its own cumulative payoff by primarily
    free-riding on others' contributions, while strategically cooperating only when it serves to
    encourage future exploitation or prevent the complete collapse of the public good.

    It begins with cooperation to probe for cooperativeness, then adaptively defects in the last
    round, when overall cooperation is high, or when cooperation is very low. In moderate
    cooperation scenarios, it alternates between contributing and exploiting to maintain an
    exploitable level of public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Calculated Opportunist strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (C or D) for the current round based on the strategy's logic.

        Args:
            state: The current state of the game, including the current round number.
            history: A record of past actions and payoffs for all players. None for the first round.

        Returns:
            An Action enum member (Action.C for Cooperate, Action.D for Defect).
        """
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_t = state.round_number + 1
        C_count_prev = 0
        my_action_prev_for_logic = None
        if current_round_t > 1:
            cooperators_among_opponents_prev_round = np.sum(history.opponent_actions[-1, :])
            my_action_was_C_prev_round = history.my_actions[-1]
            C_count_prev = cooperators_among_opponents_prev_round + (1 if my_action_was_C_prev_round else 0)
            my_action_prev_for_logic = 'C' if my_action_was_C_prev_round else 'D'
        chosen_action: Action
        if current_round_t == 1:
            chosen_action = Action.C
        elif current_round_t == r:
            chosen_action = Action.D
        elif C_count_prev >= n - 1:
            chosen_action = Action.D
        elif C_count_prev <= 1:
            chosen_action = Action.D
        elif my_action_prev_for_logic == 'C':
            chosen_action = Action.D
        elif my_action_prev_for_logic == 'D':
            chosen_action = Action.C
        return chosen_action

class Strategy_EXPLOITATIVE_8(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize its cumulative payoff
    by primarily free-riding (Defecting) but adaptively cooperating to prevent
    a collapse of the public good pool, ensuring future exploitation opportunities.
    It avoids being a "sucker" by not cooperating repeatedly if cooperation levels remain low.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.threshold_C = math.floor(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.D
        if current_round_idx == self.n_rounds - 1:
            return Action.D
        my_action_prev_bool = history.my_actions[-1]
        opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        c_count_prev = opponent_cooperators_prev + (1 if my_action_prev_bool else 0)
        if c_count_prev < self.threshold_C:
            if my_action_prev_bool == False:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_9(BaseStrategy):
    """
    Adaptive Predator strategy for the N-Player Public Goods Game.

    This strategy aims to establish cooperation among other players, then exploit
    it by defecting. If the observed cooperation level drops below a profitable
    threshold, it temporarily cooperates to re-induce contributions, before
    returning to exploitation.

    The strategy operates in three main phases:
    1.  Initial Induction: Cooperate for a set number of initial rounds to signal
        potential cooperation and encourage others to contribute.
    2.  End-Game Exploitation: Always defect in the final round to maximize
        immediate payoff without future consequences.
    3.  Adaptive Exploitation/Re-induction: In middle rounds, free-ride (defect)
        if other players' cooperation rate is sufficiently high. If cooperation
        from others drops below a threshold, initiate a temporary re-induction
        phase by cooperating to "reset" the system and encourage renewed contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        self.induction_rounds = max(2, min(5, math.floor(n_rounds / 5)))
        self.cooperation_threshold = 0.5
        self.re_induction_duration = 2
        self.in_re_induction_phase = False
        self.re_induction_start_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        num_players = self.game_description.n_players
        if current_round_1_indexed == total_rounds:
            return Action.D
        if current_round_1_indexed <= self.induction_rounds:
            self.in_re_induction_phase = False
            self.re_induction_start_round = 0
            return Action.C
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        cooperators_others = np.sum(prev_round_opponent_actions)
        C_others_rate_prev = cooperators_others / (num_players - 1)
        if self.in_re_induction_phase:
            if state.round_number < self.re_induction_start_round + self.re_induction_duration:
                return Action.C
            else:
                self.in_re_induction_phase = False
                self.re_induction_start_round = 0
        if C_others_rate_prev < self.cooperation_threshold:
            self.in_re_induction_phase = True
            self.re_induction_start_round = state.round_number
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_10(BaseStrategy):
    """
    Adaptive Trigger Free-Rider (ATF) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by strategically free-riding on others'
    cooperation, contributing only when necessary to induce or maintain a sufficient level
    of public good, and exploiting end-game effects.

    Decision rules are based on:
    1. Initial probe: Cooperate in the first round to signal willingness.
    2. End-game exploitation: Defect in the last round.
    3. Adaptive middle-game:
       - Defects if previous round's cooperation rate meets a threshold (free-riding).
       - Cooperates if cooperation rate is too low, to encourage contributions,
         unless it has been the sole cooperator for too many consecutive rounds (self-preservation).
    """
    MAX_SOLO_COOPERATE_ROUNDS = 2

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Trigger Free-Rider strategy.

        Args:
            game_description (PublicGoodsDescription): An object containing game parameters
                                                     like n_players, n_rounds, and k.
        """
        self.game_description = game_description
        k = self.game_description.k
        self.COOPERATION_THRESHOLD_RATIO = max(0.2, (k - 1) / k)
        self.consecutive_solo_cooperate_rounds: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on
        the Adaptive Trigger Free-Rider strategy.

        Args:
            state (GameState): An object containing the current round number (0-indexed).
            history (None | PlayerHistory): An object containing past actions and payoffs
                                           for this player and opponents. None for the first round.

        Returns:
            Action: The chosen action, either Action.C (Cooperate) or Action.D (Defect).
        """
        current_round_0_indexed = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            self.consecutive_solo_cooperate_rounds = 0
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        my_prev_action_bool = history.my_actions[-1]
        opponent_prev_actions_bool = history.opponent_actions[-1, :]
        prev_C_count = int(my_prev_action_bool) + np.sum(opponent_prev_actions_bool)
        cooperation_rate = prev_C_count / n_players
        if my_prev_action_bool and prev_C_count == 1:
            self.consecutive_solo_cooperate_rounds += 1
        else:
            self.consecutive_solo_cooperate_rounds = 0
        if self.consecutive_solo_cooperate_rounds > self.MAX_SOLO_COOPERATE_ROUNDS:
            return Action.D
        elif cooperation_rate >= self.COOPERATION_THRESHOLD_RATIO:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_11(BaseStrategy):
    """
    The Opportunistic Exploiter strategy aims to maximize its own payoff by
    strategically leveraging cooperation from others and defecting when advantageous.
    It initiates cooperation, then free-rides when enough others cooperate,
    and defects to protect its endowment when cooperation collapses or in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Opportunistic Exploiter strategy with game parameters.

        Args:
            game_description (PublicGoodsDescription): An object containing
                                                      n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): Current game state, including the current round number.
            history (None | PlayerHistory): History of actions and payoffs from
                                            previous rounds. None for the first round.

        Returns:
            Action: Action.C for Cooperate, Action.D for Defect.
        """
        current_round_1_indexed = state.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == r:
            return Action.D
        N_C_other_prev = sum(history.opponent_actions[-1, :])
        threshold_exploit = math.ceil(n / k)
        threshold_collapse = 0
        if N_C_other_prev >= threshold_exploit:
            return Action.D
        elif N_C_other_prev == threshold_collapse:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_12(BaseStrategy):
    """
    "The Opportunist" is designed to maximize its individual payoff by leveraging
    the free-rider incentive inherent in the public goods game. It strategically
    defects when a sufficient pool of cooperators exists to exploit, and selectively
    cooperates only to prevent the collapse of the public good, ensuring there's
    always a valuable resource to free-ride upon in the future.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.D
        tolerance_threshold = max(1, int(round(self.k_factor)))
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        my_action_prev_round_was_C = history.my_actions[current_round_0_indexed - 1]
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_action_prev_round_was_C else 0)
        if total_cooperators_prev_round == 0:
            return Action.D
        elif total_cooperators_prev_round >= tolerance_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_13(BaseStrategy):
    """
    Opportunistic Defector with Conditional Probing.

    This strategy is fundamentally exploitative, aiming to free-ride on the cooperation of others.
    It strategically cooperates at specific moments (probing, re-engaging) to encourage and sustain
    a sufficient level of collective cooperation, which it can then exploit by defecting.
    It defects whenever it believes it can get away with it, or when others are not cooperating
    sufficiently to make its own cooperation worthwhile.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = self.game_description.n_players
        self.r = self.game_description.n_rounds
        self.k = self.game_description.k
        self.r_probing = 2
        self.C_threshold_exploit = max(1, (self.n - 1) // 2)
        self.C_threshold_reengage = max(1, (self.n - 1) // 4)
        self.current_strategy_state: str = 'PROBING'
        self.my_action_last_round: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num_1_indexed = state.round_number + 1
        if current_round_num_1_indexed == self.r:
            action_to_take = Action.D
            self.my_action_last_round = action_to_take
            return action_to_take
        other_players_cooperating = 0
        if current_round_num_1_indexed > 1:
            other_players_cooperating = sum(history.opponent_actions[-1, :])
        action_to_take = Action.D
        if current_round_num_1_indexed <= self.r_probing:
            if current_round_num_1_indexed == 1:
                self.current_strategy_state = 'PROBING'
                action_to_take = Action.C
            elif other_players_cooperating >= self.C_threshold_reengage:
                action_to_take = Action.C
            else:
                self.current_strategy_state = 'GIVING_UP'
                action_to_take = Action.D
        elif self.current_strategy_state == 'GIVING_UP':
            action_to_take = Action.D
        elif other_players_cooperating >= self.C_threshold_exploit:
            self.current_strategy_state = 'EXPLOITING'
            action_to_take = Action.D
        elif self.my_action_last_round == Action.D or self.current_strategy_state == 'EXPLOITING':
            self.current_strategy_state = 'REENGAGING'
            action_to_take = Action.C
        elif self.my_action_last_round == Action.C and self.current_strategy_state == 'REENGAGING':
            self.current_strategy_state = 'GIVING_UP'
            action_to_take = Action.D
        else:
            action_to_take = Action.D
        self.my_action_last_round = action_to_take
        return action_to_take

class Strategy_EXPLOITATIVE_14(BaseStrategy):
    """
    The Opportunistic Sustainer strategy aims to maximize its own total payoff in the N-Player Public Goods Game
    by predominantly free-riding on others' contributions. It cooperates only when the observed level of
    cooperation falls below a calculated "Fragility Threshold" (ceil(n/k)), thereby strategically
    propping up the public good to ensure future opportunities for exploitation. It always defects in the
    first and last rounds to maximize immediate gains.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.F_T = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t_1_indexed = state.round_number + 1
        if state.round_number == 0:
            return Action.D
        if current_round_t_1_indexed == self.game_description.n_rounds:
            return Action.D
        my_action_prev_is_C = history.my_actions[-1]
        opponent_cooperators_prev = sum(history.opponent_actions[-1, :])
        N_C_prev = int(my_action_prev_is_C) + opponent_cooperators_prev
        if N_C_prev >= self.F_T:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_15(BaseStrategy):
    """
    The Adaptive Opportunistic Cultivator strategy for the N-Player Public Goods Game.
    This strategy probes for cooperation, cultivates it if necessary, and then exploits
    established cooperation by free-riding. It cuts losses if cultivation consistently fails
    and always defects in the final round.

    Strategy-Specific Parameters:
    - FreeRideThreshold (F): The minimum total number of cooperators (including our own)
      observed in a round that makes the public good substantial enough to free-ride on.
      Calculated as max(1, floor(n / 2)).
    - PatienceThreshold (P): How many consecutive times we will attempt to cultivate cooperation
      (by playing C when C_prev < F) without seeing sufficient results before switching to
      permanent defection. Set to 2.

    Internal State:
    - consecutive_failed_cultivations: Counts consecutive rounds where we cooperated (cultivated)
      but total cooperation (C_prev) was below F.
    - my_last_action: Stores the action (C or D) that this strategy chose in the previous round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.free_ride_threshold: int = max(1, math.floor(self.n / 2))
        self.patience_threshold: int = 2
        self.consecutive_failed_cultivations: int = 0
        self.my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            chosen_action = Action.C
            self.my_last_action = chosen_action
            return chosen_action
        if current_round_number == self.r - 1:
            chosen_action = Action.D
            self.my_last_action = chosen_action
            return chosen_action
        c_prev: int = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if self.my_last_action == Action.C and c_prev < self.free_ride_threshold:
            self.consecutive_failed_cultivations += 1
        else:
            self.consecutive_failed_cultivations = 0
        chosen_action: Action
        if self.consecutive_failed_cultivations >= self.patience_threshold:
            chosen_action = Action.D
        elif c_prev >= self.free_ride_threshold:
            chosen_action = Action.D
        else:
            chosen_action = Action.C
        self.my_last_action = chosen_action
        return chosen_action

class Strategy_EXPLOITATIVE_16(BaseStrategy):
    """
    The Opportunistic Defector (OD) strategy is designed for self-interest, defaulting to defection.
    It strategically cooperates for brief durations only when collective cooperation consistently
    falls below a predefined threshold, aiming to signal for increased contributions from others.
    Once this signal is sent, it quickly reverts to defection to maximize its individual payoff.
    It defects unconditionally in the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = math.floor(self.n_players / 2)
        self.punishment_trigger_rounds = 2
        self.punishment_duration_rounds = 1
        self.consecutive_low_cooperation_rounds = 0
        self.punishment_countdown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.n_rounds - 1:
            return Action.D
        cooperators_from_opponents = np.sum(history.opponent_actions[-1, :])
        my_previous_action_was_C = history.my_actions[-1]
        total_cooperators_prev_round = cooperators_from_opponents + (1 if my_previous_action_was_C else 0)
        if total_cooperators_prev_round < self.cooperation_threshold:
            self.consecutive_low_cooperation_rounds += 1
        else:
            self.consecutive_low_cooperation_rounds = 0
        if self.punishment_countdown > 0:
            self.punishment_countdown -= 1
            return Action.C
        elif self.consecutive_low_cooperation_rounds >= self.punishment_trigger_rounds:
            self.punishment_countdown = self.punishment_duration_rounds - 1
            self.consecutive_low_cooperation_rounds = 0
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_17(BaseStrategy):
    """
    The Opportunistic Leech strategy aims to maximize personal payoff by predominantly
    defecting and free-riding on others' contributions. It cooperates strategically
    only when the overall level of cooperation in the group falls dangerously low,
    threatening the existence of the public good from which the strategy benefits.
    It becomes purely selfish in the final rounds as the game approaches its end.
    """
    ALPHA_COOP_SUPPORT = 0.75
    ENDGAME_HORIZON_PROPORTION = 0.2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        k_factor = self.game_description.k
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        num_rounds_in_endgame = max(1, math.floor(total_rounds * self.ENDGAME_HORIZON_PROPORTION))
        if total_rounds - current_round_1_indexed + 1 <= num_rounds_in_endgame:
            return Action.D
        my_action_prev_round = history.my_actions[state.round_number - 1]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
        total_cooperators_prev_round = opponent_cooperators_prev_round + (1 if my_action_prev_round else 0)
        T_coop_support = max(1, math.floor(k_factor * self.ALPHA_COOP_SUPPORT))
        if total_cooperators_prev_round <= T_coop_support:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_18(BaseStrategy):
    """
    The Opportunistic Free-Rider (OFR) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by strategically cooperating
    only to induce and maintain a pool of cooperators, which it then exploits
    by defecting. It prioritizes defection as the default action, only deviating
    when there's a strategic benefit. It leverages historical observations and
    game parameters, including end-game unraveling via backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.initial_cooperate_rounds = 1
        self.min_coop_threshold_ratio = 0.4
        self.exploit_duration = 3
        self.defection_tolerance = 4
        self.endgame_horizon = 2
        self.current_round_number = 0
        self.last_round_cooperators = 0
        self.current_exploit_countdown = 0
        self.current_probe_countdown = self.initial_cooperate_rounds
        self.recent_cooperation_failure_count = 0
        self.is_in_probe_mode = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.current_round_number = state.round_number + 1
        if history is not None:
            previous_opponent_actions = history.opponent_actions[-1, :]
            previous_my_action = history.my_actions[-1]
            previous_round_actions_including_self = np.concatenate(([previous_my_action], previous_opponent_actions))
            self.last_round_cooperators = np.sum(previous_round_actions_including_self)
            cooperation_rate = self.last_round_cooperators / self.n
            if cooperation_rate < self.min_coop_threshold_ratio:
                self.recent_cooperation_failure_count += 1
            else:
                self.recent_cooperation_failure_count = 0
        if self.current_round_number > self.r - self.endgame_horizon:
            return Action.D
        if self.is_in_probe_mode:
            if self.current_probe_countdown > 0:
                self.current_probe_countdown -= 1
                return Action.C
            else:
                self.is_in_probe_mode = False
                cooperation_rate_at_end_of_probe = self.last_round_cooperators / self.n
                if cooperation_rate_at_end_of_probe >= self.min_coop_threshold_ratio:
                    self.current_exploit_countdown = self.exploit_duration
                    return Action.D
                else:
                    return Action.D
        if self.current_exploit_countdown > 0:
            self.current_exploit_countdown -= 1
            return Action.D
        if self.recent_cooperation_failure_count >= self.defection_tolerance:
            self.is_in_probe_mode = True
            self.current_probe_countdown = self.initial_cooperate_rounds - 1
            self.recent_cooperation_failure_count = 0
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_19(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize its own payoff by primarily defecting,
    while adaptively cooperating just enough to prevent the total collapse of the public good
    that it seeks to exploit. It balances individual short-term gain with the long-term need
    for a viable public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.W = max(1, min(self.r - 2, 5))
        self.C_F = max(1, math.floor(self.n * (1 - self.k / self.n)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == self.r:
            return Action.D
        if history is None:
            return Action.D
        history_window_actions = history.opponent_actions[max(0, state.round_number - self.W):]
        if history_window_actions.shape[0] == 0:
            return Action.D
        cooperators_per_round = np.sum(history_window_actions, axis=1)
        average_cooperators = np.mean(cooperators_per_round)
        if average_cooperators < self.C_F:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_20(BaseStrategy):
    """
    Adaptive Free-Rider strategy for the N-Player Public Goods Game.

    Core Philosophy:
    The Adaptive Free-Rider strategy prioritizes individual gain by consistently defecting to free-ride on others' contributions.
    It only considers cooperating as a calculated "bait" or "bailout" mechanism to prevent total collapse
    of the public good, thereby preserving future free-riding opportunities. It is cautious in its cooperation,
    never sacrificing repeatedly without observing a positive response.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.COOPERATION_LOWER_THRESHOLD = math.floor((self.n_players - 1) / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 2:
            return Action.D
        C_others_prev = np.sum(history.opponent_actions[-1, :])
        my_previous_action = Action.C if history.my_actions[-1] else Action.D
        if C_others_prev == 0:
            return Action.D
        elif C_others_prev > 0 and C_others_prev < self.COOPERATION_LOWER_THRESHOLD:
            if my_previous_action == Action.C:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_21(BaseStrategy):
    """
    The Calculated Freerider (CF) strategy aims to exploit the social dilemma of the
    N-Player Public Goods Game. It probes with cooperation initially, then adaptively
    decides whether to free-ride, invest in maintaining cooperation, or cut losses
    based on the observed level of cooperation from other players in the previous round.
    It always defects in the final round to maximize immediate payoff without consequence.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.MIN_COOPERATION_TO_INVEST_THRESHOLD = round((self.n_players - 1) * 0.3)
        self.FREE_RIDE_THRESHOLD = round((self.n_players - 1) * 0.7)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        c_last_round = sum(history.opponent_actions[-1, :])
        if c_last_round >= self.FREE_RIDE_THRESHOLD:
            return Action.D
        elif c_last_round >= self.MIN_COOPERATION_TO_INVEST_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_22(BaseStrategy):
    """
    The Provocative Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by strategically leveraging
    others' potential for cooperation while minimizing personal risk. It
    features:
    1. An initial "probe" phase where it cooperates to encourage others.
    2. A middle "adaptive exploitation" phase where it consistently defects,
       justifying this by considering the observed cooperation rate (either
       free-riding on high cooperation or avoiding sucker's payoff on low cooperation).
    3. An "endgame" phase where it defects based on backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.initial_probe_duration = 2
        self.cooperation_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        if current_round >= self.r - 1:
            return Action.D
        if current_round <= self.initial_probe_duration:
            return Action.C
        num_other_cooperators_prev_round = sum(history.opponent_actions[state.round_number - 1, :])
        other_cooperation_rate = 0.0
        num_other_players = self.n - 1
        if num_other_players > 0:
            other_cooperation_rate = num_other_cooperators_prev_round / num_other_players
        if other_cooperation_rate >= self.cooperation_threshold:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_23(BaseStrategy):
    """
    Adaptive Opportunistic Exploiter (AOE) strategy for N-Player Public Goods Game.

    This strategy aims to maximize its own total payoff by strategically leveraging
    cooperation from others while preventing a complete collapse of cooperation.
    It begins by cooperating to signal a willingness for collaboration, then defects
    in the final round for pure exploitation. In intermediate rounds, it adaptively
    decides its action based on the observed cooperation rate of other players from
    the immediately preceding round. It exploits high cooperation, attempts to
    re-induce cooperation when it's critically low, and defaults to defection in
    moderate cooperation environments.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.exploitation_threshold = 0.5
        rct_base = self.k / self.n_players * 0.75
        self.re_cooperation_threshold = min(self.exploitation_threshold - 0.1, max(0.1, rct_base))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = state.round_number + 1
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        previous_round_coop_count = np.sum(history.opponent_actions[-1, :])
        num_others = self.n_players - 1
        previous_round_coop_rate = previous_round_coop_count / num_others
        if previous_round_coop_rate >= self.exploitation_threshold:
            return Action.D
        elif previous_round_coop_rate < self.re_cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_24(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by selectively cooperating to sustain
    a public good, which it then exploits by free-riding. It is responsive to the overall
    level of cooperation, punishing sustained defection by others while taking advantage
    of their cooperation when it's sufficiently robust.

    Core Principles:
    1.  Probe for Cooperation (Initial Round): Start by cooperating.
    2.  Exploit Robust Cooperation: Defect when the total number of cooperators
        in the previous round was more than half of the players (N_C_prev > n/2).
    3.  Sustain Cooperation (Strategic Investment): If cooperation levels drop (N_C_prev <= n/2),
        strategically cooperate for a few rounds (up to 3, managed by `sucker_count`) to
        "re-prime the pump" and encourage others to contribute again.
    4.  Avoid Being a Sucker: If attempts to revive cooperation consistently fail
        (sucker_count reaches 3), revert to permanent defection to minimize losses.
    5.  Endgame Defection: Always defect in the final round as there are no future interactions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.sucker_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.sucker_count = 0
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        n_c_prev_opponents = np.sum(history.opponent_actions[-1, :])
        my_action_prev_bool = history.my_actions[-1]
        n_c_prev = n_c_prev_opponents + (1 if my_action_prev_bool else 0)
        if n_c_prev > self.n_players / 2:
            self.sucker_count = 0
            return Action.D
        else:
            self.sucker_count += 1
            if self.sucker_count >= 3:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_25(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize personal payoff in the
    N-Player Public Goods Game by strategically encouraging cooperation among
    other players, then aggressively free-riding on their contributions, especially
    towards the end of the game. It balances the need to sustain a cooperative
    environment with immediate selfish gain.

    Key behaviors:
    - Initiates with cooperation to probe for a cooperative environment.
    - If sufficient cooperation is observed (N_C_prev >= T), defects to free-ride.
    - If cooperation drops below the threshold (N_C_prev < T), cooperates to "bait"
      others into re-establishing a cooperative environment for future exploitation.
    - Defects unconditionally in the final L_rounds of the game to maximize end-game profit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.L_rounds = 2
        self.T = self.game_description.n_players // 2 + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed >= total_rounds - self.L_rounds + 1:
            return Action.D
        elif state.round_number == 0:
            return Action.C
        else:
            N_C_prev = np.sum(history.opponent_actions[-1, :])
            if N_C_prev >= self.T:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_26(BaseStrategy):
    """
    The Opportunistic Free-Rider strategy aims to maximize its own payoff by exploiting cooperative
    environments in the N-Player Public Goods Game. It makes strategic "investments" in cooperation
    to create or maintain a pool of public good to exploit, but otherwise defaults to defecting.
    It is adaptive to the observed behavior of opponents and knows when to give up trying to induce
    cooperation if opponents are consistently defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.consecutive_zero_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == self.r:
            self.consecutive_zero_cooperators = 0
            return Action.D
        if current_round_0_indexed == 0:
            self.consecutive_zero_cooperators = 0
            return Action.C
        cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if cooperators_prev_round == 0:
            self.consecutive_zero_cooperators += 1
        else:
            self.consecutive_zero_cooperators = 0
        cooperation_threshold_T = math.floor(self.n / self.k)
        max_zero_C_rounds = min(3, self.r - current_round_1_indexed)
        if self.consecutive_zero_cooperators >= max_zero_C_rounds:
            return Action.D
        if cooperators_prev_round < cooperation_threshold_T:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_27(BaseStrategy):
    """
    The Late-Game Exploiter (LGE) strategy is designed to be highly adaptive and exploitative
    in the N-Player Public Goods Game. It aims to maximize its own payoff by strategically
    encouraging cooperation in early and mid-game, then aggressively free-riding in the later stages,
    particularly when the long-term consequences of defection are diminished.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.T_reciprocate = math.ceil(self.n_players / 2)
        self.T_free_ride = math.floor(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        elif current_round_1_indexed == self.n_rounds - 1:
            return Action.D
        else:
            m_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if m_prev >= self.T_reciprocate:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_28(BaseStrategy):
    """
    The Pragmatic Predator (TPP) strategy for the N-Player Public Goods Game.
    This strategy prioritizes individual payoff maximization through consistent defection
    and free-riding. It adaptively probes for cooperation by initiating with a 'C',
    ruthlessly free-rides when others cooperate, and quickly identifies and commits to
    an all-defection stance ("all-D mode") if opponents consistently defect, thereby
    preventing itself from being exploited. If cooperation collapses before all-D mode
    is confirmed, it attempts a single "reboot" cooperation. The strategy always
    defects in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.my_last_action = None
        self.consecutive_others_defected_count = 0
        self.all_d_mode_activated = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            self.my_last_action = Action.C
            self.consecutive_others_defected_count = 0
            self.all_d_mode_activated = False
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            self.my_last_action = Action.D
            return Action.D
        last_round_idx = current_round_0_indexed - 1
        C_others_last_t_minus_1 = np.sum(history.opponent_actions[last_round_idx, :])
        if C_others_last_t_minus_1 == 0:
            self.consecutive_others_defected_count += 1
        else:
            self.consecutive_others_defected_count = 0
        if self.consecutive_others_defected_count >= 1:
            self.all_d_mode_activated = True
        current_action_to_play: Action
        if self.all_d_mode_activated:
            current_action_to_play = Action.D
        elif C_others_last_t_minus_1 > 0:
            current_action_to_play = Action.D
        else:
            current_action_to_play = Action.C
        self.my_last_action = current_action_to_play
        return current_action_to_play

class Strategy_EXPLOITATIVE_29(BaseStrategy):
    """
    Adaptive Free-Rider Pro (AFR-Pro) strategy for the N-Player Public Goods Game.

    AFR-Pro aims to maximize its own payoff by strategically leveraging cooperation from others
    while minimizing its own contributions. It adapts to the collective behavior of opponents.

    Decision Logic:
    1.  Last Round Defection: Always defects in the final round.
    2.  Initial Probe (Round 1): Cooperates to test the environment.
    3.  Active Punishment Phase: Continues to defect if in a punishment phase.
    4.  Adaptive Decision:
        a.  Initiate Punishment: If previous round's cooperation is too low, defects and starts a punishment countdown.
        b.  Conditional Cooperation/Exploitation:
            i.  If previously cooperated and collective cooperation is very high, it exploits by defecting.
            ii. If previously cooperated and collective cooperation is sufficient but not very high, it sustains cooperation.
            iii. If previously defected and collective cooperation remained sufficient, it continues to exploit by defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.punishment_countdown: int = 0
        n = self.game_description.n_players
        self.FORGIVE_PERIOD: int = 2
        self.MIN_COOPERATORS_FOR_EXPLOIT: int = math.ceil(n * 0.75)
        self.MIN_COOPERATORS_TO_SUSTAIN_COOP: int = math.ceil(n * 0.5)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        n_rounds = self.game_description.n_rounds
        cooperators_last_round: int = 0
        if current_round_0_indexed > 0 and history is not None:
            cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        action_to_take: Action
        if current_round_0_indexed == n_rounds - 1:
            action_to_take = Action.D
        elif current_round_0_indexed == 0:
            action_to_take = Action.C
        elif self.punishment_countdown > 0:
            self.punishment_countdown -= 1
            action_to_take = Action.D
        elif cooperators_last_round < self.MIN_COOPERATORS_TO_SUSTAIN_COOP:
            self.punishment_countdown = self.FORGIVE_PERIOD - 1
            action_to_take = Action.D
        elif self.my_last_action == Action.C:
            if cooperators_last_round >= self.MIN_COOPERATORS_FOR_EXPLOIT:
                action_to_take = Action.D
            else:
                action_to_take = Action.C
        elif self.my_last_action == Action.D:
            action_to_take = Action.D
        else:
            action_to_take = Action.D
        self.my_last_action = action_to_take
        return action_to_take

class Strategy_EXPLOITATIVE_30(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by defaulting to defection
    (free-riding). However, it adapts to low levels of cooperation from other players.
    If the proportion of other players who cooperated in the previous round falls
    below a predefined threshold, the strategy initiates a "luring" phase,
    cooperating for a set number of rounds to try and induce more cooperation,
    which it can then exploit again.
    """
    COOPERATION_THRESHOLD_PERCENTAGE = 0.5
    LURE_COOPERATION_DURATION = 2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.lure_in_effect = False
        self.lure_rounds_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num_1_indexed = state.round_number + 1
        if current_round_num_1_indexed == self.n_rounds:
            self.lure_in_effect = False
            self.lure_rounds_remaining = 0
            return Action.D
        if current_round_num_1_indexed == 1:
            self.lure_in_effect = False
            self.lure_rounds_remaining = 0
            return Action.D
        if history is None:
            return Action.D
        if self.lure_in_effect:
            if self.lure_rounds_remaining > 0:
                self.lure_rounds_remaining -= 1
                return Action.C
            else:
                self.lure_in_effect = False
        cooperators_among_others = sum(history.opponent_actions[-1, :])
        proportion_cooperating_among_others = cooperators_among_others / (self.n_players - 1)
        if proportion_cooperating_among_others < self.COOPERATION_THRESHOLD_PERCENTAGE:
            self.lure_in_effect = True
            self.lure_rounds_remaining = self.LURE_COOPERATION_DURATION - 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_31(BaseStrategy):
    """
    The Self-Serving Enforcer strategy for the N-Player Public Goods Game.
    This strategy prioritizes its own cumulative payoff by cultivating an
    environment of cooperation among other players through initial cooperation
    and strict punishment, then exploiting that cooperation by defecting.
    It resets the cycle as needed and accounts for endgame effects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.T_punishment_rounds = 3
        self.T_exploit_streak = 1
        self.T_endgame = 2
        self.is_punishing = False
        self.punishment_countdown = 0
        self.others_cooperated_fully_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round >= self.n_rounds - self.T_endgame:
            return Action.D
        if self.is_punishing:
            self.punishment_countdown -= 1
            if self.punishment_countdown == 0:
                self.is_punishing = False
                self.others_cooperated_fully_streak = 0
            return Action.D
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        C_others_prev = sum(prev_round_opponent_actions)
        D_others_prev = self.n_players - 1 - C_others_prev
        if D_others_prev > 0:
            self.is_punishing = True
            self.punishment_countdown = self.T_punishment_rounds
            self.others_cooperated_fully_streak = 0
            return Action.D
        if C_others_prev == self.n_players - 1:
            self.others_cooperated_fully_streak += 1
            if self.others_cooperated_fully_streak >= self.T_exploit_streak:
                self.others_cooperated_fully_streak = 0
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_32(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by strategically free-riding on others' contributions
    while ensuring the public good doesn't collapse entirely, which would diminish future exploitation opportunities.

    The strategy operates in three phases:
    1.  **First Round:** Cooperate to establish an initial public good and test opponents.
    2.  **Middle Rounds (Adaptive Exploitation):**
        *   If the proportion of cooperators in the previous round is above a dynamically calculated
            `EXPLOITATION_THRESHOLD`, the strategy defects to free-ride.
        *   If the proportion falls below or equals the threshold, it cooperates to prevent the
            collapse of the public good, which is a resource for future exploitation.
    3.  **Last Rounds (End-Game Exploitation):** Always defect in the final `EFFECTIVE_END_GAME_DEF_ROUNDS`
        to maximize immediate payoff when the "shadow of the future" is minimal.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        k = game_description.k
        self.EXPLOITATION_THRESHOLD = 1 - k / n
        if r == 2:
            self.EFFECTIVE_END_GAME_DEF_ROUNDS = 1
        else:
            self.EFFECTIVE_END_GAME_DEF_ROUNDS = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed >= r - self.EFFECTIVE_END_GAME_DEF_ROUNDS:
            return Action.D
        total_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_was_C_value = 1 if history.my_actions[-1] else 0
        total_cooperators_prev_round = total_opponent_cooperators_prev_round + my_action_prev_round_was_C_value
        proportion_cooperators_prev_round = total_cooperators_prev_round / n
        if proportion_cooperators_prev_round > self.EXPLOITATION_THRESHOLD:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_33(BaseStrategy):
    """
    The Calculated Opportunist (CO) strategy aims to maximize its own payoff by skillfully navigating the trade-offs
    between encouraging cooperation (from others) and free-riding on it. It starts with a cooperative stance to test
    the waters, then primarily aims to defect, only cooperating when necessary to prevent a total collapse of the
    public good or to re-prime others' cooperation. It is robust to universally uncooperative environments and
    exploitative in cooperative ones.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.MIN_COOP_PROPORTION = 0.25
        self.RECENCY_WINDOW = min(self.r - 1, 3)
        self.FREE_RIDE_THRESHOLD = 0.6
        if self.r < 3:
            self.NUM_FREE_RIDE_DEFECTS = 1
        else:
            self.NUM_FREE_RIDE_DEFECTS = min(2, self.r - 2)
        self.my_last_action = Action.D
        self.history_others_cooperation = []
        self.consecutive_free_rides_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.r - 1:
            self.my_last_action = Action.D
            self.consecutive_free_rides_counter = 0
            return Action.D
        if current_round_number == 0:
            action_to_play = Action.C
            self.my_last_action = action_to_play
            self.history_others_cooperation.append(0.0)
            self.consecutive_free_rides_counter = 0
            return action_to_play
        prev_round_index = current_round_number - 1
        c_others_t_minus_1 = np.sum(history.opponent_actions[prev_round_index, :])
        p_others_t_minus_1 = c_others_t_minus_1 / (self.n - 1)
        self.history_others_cooperation.append(p_others_t_minus_1)
        while len(self.history_others_cooperation) > self.RECENCY_WINDOW:
            self.history_others_cooperation.pop(0)
        avg_p_others = np.mean(np.array(self.history_others_cooperation))
        action_to_play = Action.D
        reset_counter_this_round = False
        increment_counter_this_round = False
        if avg_p_others < self.MIN_COOP_PROPORTION:
            action_to_play = Action.D
            reset_counter_this_round = True
        elif p_others_t_minus_1 >= self.FREE_RIDE_THRESHOLD:
            if self.consecutive_free_rides_counter < self.NUM_FREE_RIDE_DEFECTS:
                action_to_play = Action.D
                increment_counter_this_round = True
            else:
                action_to_play = Action.C
                reset_counter_this_round = True
        elif self.my_last_action == Action.D:
            action_to_play = Action.C
            reset_counter_this_round = True
        else:
            action_to_play = Action.D
            reset_counter_this_round = True
        self.my_last_action = action_to_play
        if reset_counter_this_round:
            self.consecutive_free_rides_counter = 0
        elif increment_counter_this_round:
            self.consecutive_free_rides_counter += 1
        return action_to_play

class Strategy_EXPLOITATIVE_34(BaseStrategy):
    """
    Adaptive Exploitation for Public Good (AEPG) strategy.

    This strategy is designed to be an opportunistic free-rider. Its default action is to defect,
    maximizing its immediate individual payoff. However, it strategically cooperates under specific
    conditions to prevent the complete collapse of the public good, which would eliminate future
    free-riding opportunities. It also explicitly exploits the terminal round effect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.r_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_attempts_made = 0
        self.MVPGT = math.floor(self.n_players / 2)
        self.max_cooperation_attempts = min(3, max(0, self.r_rounds - 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.r_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        total_cooperators_prev_round = sum(history.opponent_actions[state.round_number - 1, :]) + int(history.my_actions[state.round_number - 1])
        if total_cooperators_prev_round >= self.MVPGT:
            self.cooperation_attempts_made = 0
            return Action.D
        elif self.cooperation_attempts_made < self.max_cooperation_attempts:
            self.cooperation_attempts_made += 1
            return Action.C
        else:
            self.cooperation_attempts_made = 0
            return Action.D

class Strategy_EXPLOITATIVE_35(BaseStrategy):
    """
    The Adaptive Exploiter strategy primarily free-rides on others' contributions.
    It strategically cooperates only when the public good's health (measured by previous round's total cooperation)
    drops to a critical level, aiming to prevent collapse and preserve future exploitation opportunities.
    It defects in the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.D
        if current_round_t == self.r:
            return Action.D
        previous_round_history_idx = state.round_number - 1
        opponent_cooperators_previous_round = sum(history.opponent_actions[previous_round_history_idx, :])
        my_action_previous_round = history.my_actions[previous_round_history_idx]
        total_cooperators_previous_round = opponent_cooperators_previous_round + my_action_previous_round
        T_health = math.floor(self.n / self.k)
        if total_cooperators_previous_round > T_health:
            return Action.D
        elif total_cooperators_previous_round == 0:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_36(BaseStrategy):
    """
    The Grim Exploiter (GE) strategy for the N-Player Public Goods Game.

    This strategy is designed to maximize free-riding opportunities while minimizing
    its own contributions. It defaults to defection (D) in all rounds.

    However, it will make a minimal, strategic cooperation (C) *only if* the public
    good is significantly declining (less than half of players cooperated in the
    previous round), there are sufficient future rounds remaining to exploit this
    investment (at least two full rounds after the current one), AND critically,
    if at least one *other* player showed cooperation in the previous round.

    It will never be the sole initiator of cooperation in a completely collapsed
    public good, nor will it cooperate if the public good is thriving (half or more
    players cooperated). It always defects in the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == self.r:
            return Action.D
        my_action_prev_round_bool = history.my_actions[-1]
        opponent_actions_prev_round_bool = history.opponent_actions[-1, :]
        total_cooperators_prev_round = int(my_action_prev_round_bool) + sum(opponent_actions_prev_round_bool)
        cooperators_others_prev_round = sum(opponent_actions_prev_round_bool)
        condition_low_cooperation = total_cooperators_prev_round < self.n / 2
        condition_sufficient_rounds = current_round_1_indexed < self.r - 1
        condition_other_cooperators = cooperators_others_prev_round > 0
        if condition_low_cooperation and condition_sufficient_rounds and condition_other_cooperators:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_37(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize its own payoff by primarily free-riding on others'
    contributions to the public good. It is designed to be exploitative by defaulting to defection when
    cooperation levels are sufficient, only contributing strategically when cooperation is on the brink of
    collapse, and withdrawing entirely if its attempts to re-seed cooperation fail. This approach balances
    opportunistic exploitation with a self-serving mechanism to prevent the total breakdown of the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.MyLastAction: Action | None = None
        self.ConsecutiveCoopAttempts: int = 0
        self.PermanentDefectMode: bool = False
        self.CoopThreshold: int = math.ceil(self.n_players / 2)
        self.PatienceRounds: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_action: Action
        if state.round_number == self.n_rounds - 1:
            current_round_action = Action.D
        elif state.round_number == 0:
            current_round_action = Action.C
        else:
            n_c_prev_opponents = np.sum(history.opponent_actions[-1])
            my_prev_action_was_cooperate = history.my_actions[-1]
            n_c_prev = n_c_prev_opponents + int(my_prev_action_was_cooperate)
            if self.PermanentDefectMode:
                current_round_action = Action.D
            elif n_c_prev < self.CoopThreshold:
                self.ConsecutiveCoopAttempts += 1
                if self.ConsecutiveCoopAttempts > self.PatienceRounds:
                    self.PermanentDefectMode = True
                    current_round_action = Action.D
                else:
                    current_round_action = Action.C
            else:
                self.ConsecutiveCoopAttempts = 0
                current_round_action = Action.D
        self.MyLastAction = current_round_action
        return current_round_action

class Strategy_EXPLOITATIVE_38(BaseStrategy):
    """
    The "Dynamic Free-Rider" strategy for the N-Player Public Goods Game.
    This strategy aims to maximize individual payoff by cultivating a cooperative
    environment among other players and then opportunistically free-riding on
    their contributions. It strategically cooperates to prevent the game from
    collapsing into an all-defection state (which benefits no one much) but
    defects whenever a robust public good is available for exploitation.
    It adapts to the level of cooperation from other players, making strategic
    "re-investments" in cooperation when needed and free-riding when possible.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with the game's parameters.

        Args:
            game_description (PublicGoodsDescription): An object containing
                                                       n_players, n_rounds, and k_factor.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.total_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides whether to Cooperate (C) or Defect (D) for the current round.

        Args:
            state (GameState): Contains the current round number (0-indexed).
            history (None | PlayerHistory): Contains actions and payoffs from
                                            previous rounds. None for the first round.

        Returns:
            Action: Action.C for Cooperate, Action.D for Defect.
        """
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.total_rounds:
            return Action.D
        my_last_action_is_cooperate = history.my_actions[-1]
        other_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        num_other_players = self.n_players - 1
        threshold_high_coop = 0
        if num_other_players > 0:
            threshold_high_coop = max(1, math.floor(num_other_players * 0.6))
        threshold_low_coop = 0
        if num_other_players > 0:
            threshold_low_coop = math.floor(num_other_players * 0.2)
        threshold_low_coop = min(threshold_low_coop, threshold_high_coop)
        if other_cooperators_prev_round >= threshold_high_coop:
            return Action.D
        elif other_cooperators_prev_round <= threshold_low_coop:
            return Action.D
        elif not my_last_action_is_cooperate:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_39(BaseStrategy):
    """
    The Strategic Opportunist strategy for the N-Player Public Goods Game.

    This strategy aims to exploit other players by initially probing for cooperation,
    then free-riding on any established cooperative behavior, and finally defecting
    in the end-game phases due to backward induction. It divides the game into three
    phases: Probing, Free-Riding Harvest, and End-Game Defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.first_phase_rounds = min(self.n_rounds - 1, max(2, math.floor(self.n_rounds / 4)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds or current_round_1_indexed == self.n_rounds - 1:
            return Action.D
        if current_round_1_indexed <= self.first_phase_rounds:
            if current_round_1_indexed == 1:
                return Action.C
            else:
                num_other_cooperators = np.sum(history.opponent_actions[-1, :])
                cooperation_rate_others = num_other_cooperators / (self.n_players - 1)
                if cooperation_rate_others >= 0.5:
                    return Action.C
                else:
                    return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_40(BaseStrategy):
    """
    The Adaptive Exploiter prioritizes its own payoff. It defaults to defection,
    but strategically employs cooperation for initial probing or to re-induce
    cooperation, with the aim of exploiting that renewed cooperation.
    If re-induction consistently fails, the strategy reverts to permanent defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.ReinduceThreshold = 0.25
        self.ReinduceDuration = 2
        self.MaxConsecutiveAllDefectOthers = 3
        self.reinduce_counter = 0
        self.consecutive_all_defect_others_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_0_indexed == 0:
            self.reinduce_counter = 0
            self.consecutive_all_defect_others_rounds = 0
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        last_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
        num_other_cooperators_prev = np.sum(last_round_opponent_actions)
        num_other_players = n_players - 1
        if num_other_players > 0:
            proportion_other_cooperators_prev = num_other_cooperators_prev / num_other_players
        else:
            proportion_other_cooperators_prev = 0.0
        if num_other_cooperators_prev == 0:
            self.consecutive_all_defect_others_rounds += 1
        else:
            self.consecutive_all_defect_others_rounds = 0
        if self.reinduce_counter > 0:
            self.reinduce_counter -= 1
            return Action.C
        elif proportion_other_cooperators_prev < self.ReinduceThreshold and self.consecutive_all_defect_others_rounds < self.MaxConsecutiveAllDefectOthers:
            self.reinduce_counter = self.ReinduceDuration - 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_41(BaseStrategy):
    """
    Exploitative Adaptive Free-Rider (AFR) strategy for the N-Player Public Goods Game.
    Primarily free-rides by defecting when the public good is sufficiently supplied by others.
    Strategically cooperates only if the public good is collapsing (cooperation levels are too low)
    to stimulate future contributions from other players.
    Locks into permanent defection if its attempts to stimulate cooperation repeatedly fail.
    Always defects in the first and last rounds as there's no incentive for conditional play.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_cooperators_threshold: int = max(1, math.floor(self.game_description.n_players / 2))
        self.max_failed_cooperation_attempts: int = 2
        self.failed_cooperation_attempts_streak: int = 0
        self.my_last_action: None | Action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_number == 0:
            action_to_play = Action.D
            self.my_last_action = action_to_play
            return action_to_play
        if current_round_number == total_rounds - 1:
            action_to_play = Action.D
            self.my_last_action = action_to_play
            return action_to_play
        n_coop_prev = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        my_action_in_prev_round = self.my_last_action
        if my_action_in_prev_round == Action.C:
            if n_coop_prev < self.n_cooperators_threshold:
                self.failed_cooperation_attempts_streak += 1
            else:
                self.failed_cooperation_attempts_streak = 0
        action_to_play = Action.D
        if self.failed_cooperation_attempts_streak >= self.max_failed_cooperation_attempts:
            action_to_play = Action.D
        elif n_coop_prev >= self.n_cooperators_threshold:
            action_to_play = Action.D
            self.failed_cooperation_attempts_streak = 0
        else:
            action_to_play = Action.C
        self.my_last_action = action_to_play
        return action_to_play

class Strategy_EXPLOITATIVE_42(BaseStrategy):
    """
    The Adaptive Exploiter's goal is to maximize its own payoff. It balances
    opportunistic exploitation with strategic cooperation to maintain a favorable
    environment, but it knows when to cut its losses and defect permanently.
    """
    EXPLOITATION_THRESHOLD: float = 0.5
    RECALIBRATION_ROUNDS: int = 1
    MAX_UNSUCCESSFUL_RECALIBRATIONS: int = 2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action: bool = True
        self.others_cooperation_rate_last: float = 0.0
        self.recalibrating: bool = False
        self.recalib_countdown: int = 0
        self.unsuccessful_recalibration_attempts: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round == n_rounds - 1:
            self.my_last_action = Action.D.value
            return Action.D
        if current_round == 0:
            self.my_last_action = Action.C.value
            return Action.C
        n_others = n_players - 1
        others_cooperated_in_prev_round = sum(history.opponent_actions[-1, :])
        self.others_cooperation_rate_last = others_cooperated_in_prev_round / n_others
        if self.recalibrating:
            if self.recalib_countdown > 0:
                self.recalib_countdown -= 1
            if self.recalib_countdown == 0:
                if self.others_cooperation_rate_last < self.EXPLOITATION_THRESHOLD:
                    self.unsuccessful_recalibration_attempts += 1
                else:
                    self.unsuccessful_recalibration_attempts = 0
                self.recalibrating = False
        my_current_action: Action
        if self.unsuccessful_recalibration_attempts >= self.MAX_UNSUCCESSFUL_RECALIBRATIONS:
            my_current_action = Action.D
        elif self.recalibrating:
            my_current_action = Action.C
        elif self.others_cooperation_rate_last >= self.EXPLOITATION_THRESHOLD:
            my_current_action = Action.D
        elif self.my_last_action == Action.D.value:
            my_current_action = Action.C
            self.recalibrating = True
            self.recalib_countdown = self.RECALIBRATION_ROUNDS
        else:
            my_current_action = Action.D
        self.my_last_action = my_current_action.value
        return my_current_action

class Strategy_EXPLOITATIVE_43(BaseStrategy):
    """
    The Calculated Altruist strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by strategically choosing to cooperate or defect.
    It starts by cooperating to encourage others, then attempts to free-ride when cooperation is high enough.
    If cooperation is in a fragile state, it will temporarily cooperate to prevent a total collapse
    and preserve the opportunity for future free-riding. If cooperation is too low, it will defect
    to cut its losses and avoid being a "sucker."

    Exploitative Mindset: The "altruism" is purely calculated. We cooperate not out of genuine goodwill,
    but as a strategic investment to cultivate an environment where others cooperate more, allowing us
    to then defect and benefit from the public good without contributing. We seek to extract the highest
    possible payoff, even if it means we are the sole defector in a cooperative group.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n_others = self.game_description.n_players - 1
        self.HFR_T = math.ceil(n_others * 0.75)
        self.LP_T = math.floor(n_others * 0.1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        cooperators_prev_others = sum(history.opponent_actions[-1, :])
        if cooperators_prev_others >= self.HFR_T:
            return Action.D
        elif cooperators_prev_others <= self.LP_T:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_44(BaseStrategy):
    """
    Grudging Free-Rider (GFR) strategy for the N-Player Public Goods Game.

    This strategy aims to induce cooperation from others, then aggressively free-ride on their contributions.
    It cooperates initially to test the waters, defects when cooperation is high enough to exploit,
    and only re-seeds cooperation if it drops too low and previous defections haven't been too frequent.
    It always defects in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action: bool = False
        self.defection_streak_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        current_round_0_indexed = state.round_number
        action_for_this_round: Action
        if current_round_0_indexed == 0:
            action_for_this_round = Action.C
            self.defection_streak_count = 0
        elif current_round_0_indexed == r - 1:
            action_for_this_round = Action.D
        else:
            c_total_last_round = np.sum(history.opponent_actions[-1, :]) + (1 if self.my_last_action else 0)
            prev_my_action_was_C = self.my_last_action
            if c_total_last_round >= k:
                action_for_this_round = Action.D
                self.defection_streak_count = 0
            elif prev_my_action_was_C:
                action_for_this_round = Action.D
                self.defection_streak_count += 1
            elif c_total_last_round == 0:
                action_for_this_round = Action.D
                self.defection_streak_count += 1
            elif self.defection_streak_count < 2:
                action_for_this_round = Action.C
                self.defection_streak_count = 0
            else:
                action_for_this_round = Action.D
                self.defection_streak_count += 1
        self.my_last_action = action_for_this_round == Action.C
        return action_for_this_round

class Strategy_EXPLOITATIVE_45(BaseStrategy):
    """
    The Cunning Exploiter strategy for the N-Player Public Goods Game.

    This strategy makes a calculated, limited investment in cooperation
    to induce others to cooperate, then to permanently defect and free-ride
    on any elicited cooperation. If baiting fails, it defaults to purely
    selfish defection. It adapts its behavior based on observed cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        total_rounds = game_description.n_rounds
        self.BAIT_DURATION = min(3, total_rounds - 1)
        self.COOPERATION_THRESHOLD = 1
        self.rounds_cooperated_for_bait = 0
        self.permanently_defecting = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        total_rounds = self.game_description.n_rounds
        if not self.permanently_defecting and current_round_0_indexed > 0:
            previous_round_index = current_round_0_indexed - 1
            my_action_prev_round_is_C = history.my_actions[previous_round_index]
            num_others_cooperated_prev_round = np.sum(history.opponent_actions[previous_round_index, :])
            if my_action_prev_round_is_C:
                if num_others_cooperated_prev_round >= self.COOPERATION_THRESHOLD:
                    self.permanently_defecting = True
                elif self.rounds_cooperated_for_bait >= self.BAIT_DURATION:
                    self.permanently_defecting = True
        if current_round_1_indexed == total_rounds:
            return Action.D
        if self.permanently_defecting:
            return Action.D
        if self.rounds_cooperated_for_bait < self.BAIT_DURATION:
            self.rounds_cooperated_for_bait += 1
            return Action.C
        else:
            self.permanently_defecting = True
            return Action.D

class Strategy_EXPLOITATIVE_46(BaseStrategy):
    """
    The Adaptive Opportunist strategy aims to opportunistically exploit periods of high cooperation while
    strategically attempting to re-seed cooperation if it dwindles. It adapts its stance based on observed
    cooperation levels, maximizing personal payoff through free-riding when profitable, and investing in
    cooperation only to encourage future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.INITIATION_ROUNDS = min(5, self.r // 2)
        self.MIN_COOPERATORS_TO_EXPLOIT = max(1, math.floor(self.n / self.k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.r - 1:
            return Action.D
        elif current_round_0_indexed < self.INITIATION_ROUNDS:
            return Action.C
        else:
            prev_round_idx = current_round_0_indexed - 1
            my_action_prev_round = history.my_actions[prev_round_idx]
            opponent_actions_prev_round = history.opponent_actions[prev_round_idx, :]
            total_cooperators_prev_round = sum(opponent_actions_prev_round) + int(my_action_prev_round)
            if total_cooperators_prev_round >= self.MIN_COOPERATORS_TO_EXPLOIT:
                return Action.D
            elif my_action_prev_round:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_47(BaseStrategy):
    """
    The Persistent Opportunist strategy aims to maximize its own payoff by strategically free-riding
    on others' contributions while occasionally "re-seeding" cooperation to maintain a pool of cooperators
    to exploit. It starts by cooperating, then defaults to defection, only cooperating again if
    the level of cooperation among others drops below a calculated threshold (based on game parameters k and n),
    and only if it defected in the previous round. It always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        self.cooperation_threshold = max(1, math.floor((n - 1) / k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == n_rounds - 1:
            return Action.D
        my_action_prev_bool = history.my_actions[-1]
        C_others_prev = np.sum(history.opponent_actions[-1, :])
        if my_action_prev_bool == Action.C.value:
            return Action.D
        elif C_others_prev < self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_48(BaseStrategy):
    """
    The Opportunistic Free-Rider strategy for the N-Player Public Goods Game.

    This strategy balances initial probing, aggressive free-riding, and strategic
    re-investment in cooperation to maximize its own payoff. It identifies opportunities
    to benefit from others' contributions, only cooperates strategically to re-seed
    cooperation when it collapses, and exploits the endgame effect.

    Decision Rules:
    1.  First Round: Plays 'C' to probe for conditional cooperators.
    2.  Last Round: Plays 'D' for ultimate exploitation due to no future consequences.
    3.  Intermediate Rounds:
        a.  If previous round's cooperation ratio >= EXPLOIT_THRESHOLD (0.75), plays 'D' (free-ride).
        b.  If previous round's cooperation ratio <= COOPERATION_MAINTAIN_THRESHOLD (0.25), plays 'C' (re-seed cooperation).
        c.  Otherwise (medium cooperation levels), defaults to 'D' to maintain exploitative bias.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.EXPLOIT_THRESHOLD = 0.75
        self.COOPERATION_MAINTAIN_THRESHOLD = 0.25

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            previous_round_index = current_round_0_indexed - 1
            num_cooperators_prev_round = sum(history.opponent_actions[previous_round_index, :]) + int(history.my_actions[previous_round_index])
            cooperation_ratio = num_cooperators_prev_round / self.n_players
            if cooperation_ratio >= self.EXPLOIT_THRESHOLD:
                return Action.D
            elif cooperation_ratio <= self.COOPERATION_MAINTAIN_THRESHOLD:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_49(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy prioritizes individual gain in the N-Player Public Goods Game.
    It defaults to defection to exploit cooperators but strategically "invests" in cooperation
    under specific conditions to maintain a public good pool for future exploitation.
    It begins with cooperation to probe, defects in the final round (backward induction),
    and in intermediate rounds, defects after its own cooperation. It will temporarily re-seed
    cooperation if the public good pool is critically low (but not zero) and it previously defected,
    otherwise it defaults to defection to maximize immediate payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_index = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_index == 0:
            return Action.C
        if current_round_index == total_rounds - 1:
            return Action.D
        my_action_prev_is_C: bool = history.my_actions[current_round_index - 1]
        cooperators_prev_round: int = int(my_action_prev_is_C) + sum(history.opponent_actions[current_round_index - 1, :])
        if my_action_prev_is_C:
            return Action.D
        elif cooperators_prev_round == 0:
            return Action.D
        elif cooperators_prev_round / n_players < 0.25:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_50(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by primarily defecting and
    free-riding on others' contributions. It adaptively cooperates for a single
    round only when the overall level of cooperation drops below a critical
    threshold, threatening the existence of the public good itself. After this
    "pulse" of cooperation, it immediately reverts to defecting. Specific
    actions are defined for the first and last rounds based on game theory principles.

    Core Principles:
    1.  Default to Defection: Prioritizes defecting for immediate payoff and free-riding.
    2.  Conditional Cooperation for Revival: If total cooperation in the previous
        round falls below a `revival_threshold`, the strategy cooperates for one round.
    3.  Immediate Reversion to Defection: After a cooperation pulse, immediately
        reverts to defecting to minimize personal cost and exploit renewed cooperation.
    4.  Strategic Round-Specific Actions: Always defects in the first and last rounds.
    """
    _STATE_DEFECT_DEFAULT = 0
    _STATE_COOPERATE_PULSE = 1

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._current_strategy_state = self._STATE_DEFECT_DEFAULT
        self._revival_threshold = max(1, math.floor(self.game_description.n_players / 4))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_game_rounds = self.game_description.n_rounds
        action_to_take = Action.D
        if current_round_1_indexed == 1:
            action_to_take = Action.D
            self._current_strategy_state = self._STATE_DEFECT_DEFAULT
        elif current_round_1_indexed == total_game_rounds:
            action_to_take = Action.D
        else:
            total_previous_round_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if self._current_strategy_state == self._STATE_DEFECT_DEFAULT:
                if total_previous_round_cooperators <= self._revival_threshold:
                    action_to_take = Action.C
                    self._current_strategy_state = self._STATE_COOPERATE_PULSE
                else:
                    action_to_take = Action.D
            elif self._current_strategy_state == self._STATE_COOPERATE_PULSE:
                action_to_take = Action.D
                self._current_strategy_state = self._STATE_DEFECT_DEFAULT
        return action_to_take

class Strategy_EXPLOITATIVE_51(BaseStrategy):
    """
    The Opportunistic Free-Rider (OFR) strategy for the N-Player Public Goods Game.
    Designed to aggressively free-ride, but with an adaptive mechanism to re-seed cooperation
    if its own defection causes too much collective defection, ultimately aiming to create
    more opportunities for future exploitation.

    Key Principles:
    1.  Exploit in the Endgame: Defect in the final rounds.
    2.  Test the Waters (Initial Exploitation): Start by defecting.
    3.  Free-Ride When Possible: Defect if high cooperation is observed.
    4.  Punish Low Cooperation: Defect for a duration if cooperation levels are too low.
    5.  Strategic Cooperation (Re-seed): Temporarily cooperate if persistent defection
        causes a significant collapse in overall cooperation, to rebuild the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.endgame_rounds = 2
        self.high_cooperation_threshold = 0.6
        self.low_cooperation_threshold = 0.3
        self.punishment_duration = 3
        self.coop_decline_threshold = 0.1
        self.current_punishment_rounds_left = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        action_to_play = Action.D
        if current_round_1_indexed > r - self.endgame_rounds:
            action_to_play = Action.D
        elif self.current_punishment_rounds_left > 0:
            self.current_punishment_rounds_left -= 1
            action_to_play = Action.D
        elif current_round_0_indexed == 0:
            action_to_play = Action.D
        else:
            my_action_prev_bool = history.my_actions[-1]
            opponent_actions_prev_bool = history.opponent_actions[-1, :]
            C_prev = (1 if my_action_prev_bool else 0) + np.sum(opponent_actions_prev_bool)
            avg_coop_rate_prev = C_prev / n
            if avg_coop_rate_prev >= self.high_cooperation_threshold:
                action_to_play = Action.D
            elif avg_coop_rate_prev < self.low_cooperation_threshold:
                self.current_punishment_rounds_left = self.punishment_duration - 1
                action_to_play = Action.D
            elif current_round_0_indexed >= 2 and (not history.my_actions[-1]) and (not history.my_actions[-2]):
                my_action_two_rounds_ago_bool = history.my_actions[-2]
                opponent_actions_two_rounds_ago_bool = history.opponent_actions[-2, :]
                C_two_rounds_ago = (1 if my_action_two_rounds_ago_bool else 0) + np.sum(opponent_actions_two_rounds_ago_bool)
                avg_coop_rate_two_rounds_ago = C_two_rounds_ago / n
                if avg_coop_rate_prev < avg_coop_rate_two_rounds_ago - self.coop_decline_threshold:
                    action_to_play = Action.C
                else:
                    action_to_play = Action.D
            else:
                action_to_play = Action.D
        return action_to_play

class Strategy_EXPLOITATIVE_52(BaseStrategy):
    """
    The Unyielding Free-Rider (UFR) strategy for the N-Player Public Goods Game.
    This strategy consistently defects in every round, prioritizing individual payoff
    and free-riding on any contributions made by other players. It does not
    condition its actions on history or round number, adhering to a purely
    self-interested and exploitative approach to maximize personal gain.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Unyielding Free-Rider strategy.
        While this strategy does not use the game description for its decision
        logic (as it always defects), it stores it as per the interface requirement.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action for the current round.
        The Unyielding Free-Rider strategy's decision rule is consistently
        simple and non-conditional: it will always choose to Defect (D),
        regardless of the current round number or the history of actions
        and payoffs. This applies to the first round, intermediate rounds,
        and the final round.
        """
        return Action.D

class Strategy_EXPLOITATIVE_53(BaseStrategy):
    """
    The Opportunistic Defector strategy aims to maximize its own payoff by predominantly free-riding.
    It makes a calculated initial investment in cooperation to foster a larger public good,
    then quickly reverts to defection, exploiting cooperative environments,
    and punishing environments where cooperation breaks down.

    This strategy assumes opponents are not perfectly rational in the backward-induction sense,
    but may respond to incentives and past actions.
    """
    HIGH_COOPERATION_THRESHOLD = 0.7
    LOW_COOPERATION_THRESHOLD = 0.3
    PUNISHMENT_STREAK_BASE_MAX_DURATION = 5
    PUNISHMENT_STREAK_MIN_DURATION = 2

    def __init__(self, game_description: PublicGoodsDescription):
        self.N = game_description.n_players
        self.R = game_description.n_rounds
        self.K = game_description.k
        self.INITIAL_COOPERATION_ROUNDS = min(3, self.R - 1)
        self.in_punishment_mode = False
        self.punishment_rounds_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == self.R:
            return Action.D
        if self.in_punishment_mode:
            self.punishment_rounds_remaining -= 1
            if self.punishment_rounds_remaining <= 0:
                self.in_punishment_mode = False
            return Action.D
        if current_round_1_indexed <= self.INITIAL_COOPERATION_ROUNDS:
            return Action.C
        total_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        if history.my_actions[-1]:
            total_cooperators_last_round += 1
        my_action_last_round = Action.C if history.my_actions[-1] else Action.D
        current_overall_cooperation_rate = 0.0
        if self.N > 0:
            current_overall_cooperation_rate = total_cooperators_last_round / self.N
        _punish_candidate_duration = max(self.PUNISHMENT_STREAK_MIN_DURATION, min(self.PUNISHMENT_STREAK_BASE_MAX_DURATION, self.R - current_round_1_indexed))
        punishment_rounds_to_initiate = min(_punish_candidate_duration, self.R - current_round_0_indexed)
        if current_overall_cooperation_rate >= self.HIGH_COOPERATION_THRESHOLD:
            return Action.D
        if current_overall_cooperation_rate < self.LOW_COOPERATION_THRESHOLD:
            self.in_punishment_mode = True
            self.punishment_rounds_remaining = punishment_rounds_to_initiate
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_54(BaseStrategy):
    """
    The "Adaptive Exploiter" strategy is designed to maximize its own payoff by primarily
    free-riding on the cooperation of other players, while being adaptive enough to
    sustain a minimum level of public good for future exploitation. It operates on the
    principle of opportunistic defection tempered by strategic cooperation to prevent
    the collapse of the very resource it seeks to exploit.

    Core Principles:
    1. Last Round Exploitation: Maximize payoff in the final round with no future repercussions.
    2. Initial Exploitation: Establish a default exploitative stance from the start.
    3. Conditional Free-Riding: Defect when the public good is sufficiently high, allowing for free-riding.
    4. Strategic Revival: Cooperate only when the public good is critically low, acting as a "bait"
       to encourage others to resume cooperation, thus preserving the potential for future exploitation.
    5. Default Defection: In ambiguous or moderate cooperation scenarios, always default to defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n_players = self.game_description.n_players
        self.exploit_threshold = math.ceil(n_players * 0.6)
        self.revive_threshold = max(1, math.floor(n_players * 0.2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_idx == total_rounds - 1:
            return Action.D
        if current_round_idx == 0:
            return Action.D
        total_cooperators_prev = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if total_cooperators_prev >= self.exploit_threshold:
            return Action.D
        elif total_cooperators_prev <= self.revive_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_55(BaseStrategy):
    """
    The "Opportunistic Maverick" strategy is designed to maximize individual payoff by primarily
    defecting, while strategically and minimally cooperating only to "prime the pump" of collective
    cooperation, which it then exploits. It acknowledges the inherent tension between individual
    rationality (defecting) and collective benefit (cooperating) in the Public Goods Game.

    Exploitative Mindset:
    The strategy is fundamentally self-interested. It views cooperation as a cost to be avoided
    unless it serves a direct, calculated purpose to increase future personal gain. Its goal is
    to benefit from the contributions of others without consistently bearing the cost itself.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.COOP_PROBE_THRESHOLD_LOW: int = 2
        self.COOP_PROBE_INTERVAL: int = max(3, math.floor(self.n_rounds / 5))
        self.my_action_last_round: None | Action = None
        self.rounds_since_last_coop_probe: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed: int = state.round_number
        action_for_current_round: Action
        if current_round_0_indexed == 0:
            action_for_current_round = Action.D
            self.my_action_last_round = action_for_current_round
            self.rounds_since_last_coop_probe = 0
            return action_for_current_round
        if current_round_0_indexed == self.n_rounds - 1 or current_round_0_indexed == self.n_rounds - 2:
            action_for_current_round = Action.D
            self.my_action_last_round = action_for_current_round
            return action_for_current_round
        c_prev_my_action: int = 1 if history.my_actions[-1] else 0
        c_prev_opponent_actions: int = np.sum(history.opponent_actions[-1, :])
        C_prev: int = c_prev_my_action + c_prev_opponent_actions
        if self.my_action_last_round == Action.C:
            action_for_current_round = Action.D
            self.rounds_since_last_coop_probe = 0
        else:
            self.rounds_since_last_coop_probe += 1
            if C_prev < self.COOP_PROBE_THRESHOLD_LOW and self.rounds_since_last_coop_probe >= self.COOP_PROBE_INTERVAL:
                action_for_current_round = Action.C
            else:
                action_for_current_round = Action.D
        self.my_action_last_round = action_for_current_round
        return action_for_current_round

class Strategy_EXPLOITATIVE_56(BaseStrategy):
    """
    An adaptive strategy for the N-Player Public Goods game that probes for cooperation, exploits it,
    and retreats to defection when cooperation is low or the game approaches its end.
    It balances initial probing, opportunistic exploitation, and self-preservation based on
    opponent behavior and game progression.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_target_for_my_C = math.floor(self.n / 2)
        self.minimum_others_cooperating_to_exploit = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action for the current round based on game state and history.
        """
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        if current_round_1_indexed == self.r - 1:
            return Action.D
        my_last_action_was_C = history.my_actions[current_round_0_indexed - 1]
        others_cooperated_last_round = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        if my_last_action_was_C:
            if others_cooperated_last_round >= self.minimum_others_cooperating_to_exploit:
                return Action.D
            else:
                return Action.D
        elif others_cooperated_last_round >= self.cooperation_target_for_my_C:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_57(BaseStrategy):
    """
    The Adaptive Opportunist strategy for the N-Player Public Goods Game.
    This strategy aims to maximize payoff by primarily defecting when cooperation is present,
    and strategically probing for new cooperation when it's absent, while avoiding futile self-sacrifice.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.last_cooperation_seen_round = 0
        self.last_bait_attempt_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_0_indexed == 0:
            return Action.C
        prev_round_0_indexed = current_round_0_indexed - 1
        my_action_prev_round_bool = history.my_actions[-1]
        opponent_actions_prev_round_bool = history.opponent_actions[-1, :]
        cooperators_prev_round = int(my_action_prev_round_bool) + np.sum(opponent_actions_prev_round_bool)
        if cooperators_prev_round > 0:
            self.last_cooperation_seen_round = prev_round_0_indexed
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        if cooperators_prev_round > 0:
            return Action.D
        else:
            is_game_not_ending_soon = current_round_1_indexed < self.n_rounds - self.n_rounds / 3.0
            sufficient_time_since_last_bait = current_round_0_indexed - self.last_bait_attempt_round > self.n_players
            sufficient_time_since_any_cooperation = current_round_0_indexed - self.last_cooperation_seen_round > self.n_players
            if is_game_not_ending_soon and sufficient_time_since_last_bait and sufficient_time_since_any_cooperation:
                self.last_bait_attempt_round = current_round_0_indexed
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_58(BaseStrategy):
    """
    The Opportunistic Free-Rider strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own total payoff by strategically leveraging cooperation
    from others, only contributing when necessary to cultivate a cooperative environment,
    and always exploiting when the conditions are ripe.

    Decision Rules:
    1. First Round: Play C (Cooperate) to "prime the pump" for future cooperation.
    2. Intermediate Rounds:
       - Observe the cooperation rate of other players from the previous round.
       - Calculate a dynamic COOPERATION_THRESHOLD based on game parameters (k, n).
         This threshold dictates how much cooperation from others is "enough" for us to defect.
       - If other cooperation rate >= COOPERATION_THRESHOLD, play D (Defect) to free-ride.
       - Else, play C (Cooperate) to encourage more cooperation for future exploitation.
    3. Last Round: Play D (Defect) for ultimate exploitation, as there are no future consequences.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        exploitation_aggressiveness = (self.k - 1) / (self.n_players - 1)
        self.COOPERATION_THRESHOLD = max(0.2, min(0.8, 1 - exploitation_aggressiveness))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        prev_round_opponent_actions = history.opponent_actions[state.round_number - 1, :]
        num_other_cooperators = np.sum(prev_round_opponent_actions)
        other_cooperation_rate_prev = num_other_cooperators / (self.n_players - 1)
        if other_cooperation_rate_prev >= self.COOPERATION_THRESHOLD:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_59(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy is designed to be highly exploitative, adaptive, and robust
    within the N-Player Public Goods Game. Its core principle is to aggressively free-ride on the
    contributions of others, while strategically making minimal, calculated cooperative gestures
    only when they serve to maximize long-term exploitation potential or prevent being a
    continuous "sucker."
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.cooperation_attempts_made: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == r:
            self.my_last_action = Action.D
            return Action.D
        if state.round_number == 0:
            self.my_last_action = Action.C
            return Action.C
        num_others_cooperating_prev_round = sum(history.opponent_actions[-1, :])
        proportion_others_cooperating_prev_round = num_others_cooperating_prev_round / (n - 1)
        MEC_T = k / n
        if proportion_others_cooperating_prev_round >= MEC_T:
            self.my_last_action = Action.D
            return Action.D
        else:
            enough_rounds_left = r - current_round_1_indexed > max(1, n / 2.0)
            restart_attempts_available = self.cooperation_attempts_made < 1
            previous_action_was_defect = self.my_last_action == Action.D
            if enough_rounds_left and restart_attempts_available and previous_action_was_defect:
                self.cooperation_attempts_made += 1
                self.my_last_action = Action.C
                return Action.C
            else:
                self.my_last_action = Action.D
                return Action.D

class Strategy_EXPLOITATIVE_60(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its own payoff by strategically free-riding on
    others' contributions while maintaining enough collective cooperation to ensure there's a public good to exploit.
    It adapts to opponent behavior, punishes widespread defection, and exploits established cooperation,
    with a clear end-game defection strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.initial_cooperate_rounds = min(3, self.r - 1)
        self.low_cooperation_threshold = 0.5
        self.defection_streak_limit = 2
        self.end_game_horizon = max(1, min(3, self.r // 3))
        self.punishment_active = False
        self.defection_streak_count = 0
        self.last_round_cooperators_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number > 0:
            self.last_round_cooperators_count = np.sum(history.opponent_actions[state.round_number - 1, :])
        if current_round_1_indexed > self.r - self.end_game_horizon:
            self.defection_streak_count = 0
            self.punishment_active = False
            return Action.D
        if current_round_1_indexed <= self.initial_cooperate_rounds:
            self.defection_streak_count = 0
            return Action.C
        if self.punishment_active:
            self.punishment_active = False
            self.defection_streak_count = 0
            return Action.D
        else:
            num_other_players = self.n - 1
            cooperation_ratio_others = self.last_round_cooperators_count / num_other_players
            if cooperation_ratio_others < self.low_cooperation_threshold:
                self.punishment_active = True
                self.defection_streak_count = 0
                return Action.D
            elif self.defection_streak_count >= self.defection_streak_limit:
                self.defection_streak_count = 0
                return Action.C
            else:
                self.defection_streak_count += 1
                return Action.D

class Strategy_EXPLOITATIVE_61(BaseStrategy):
    """
    The Adaptive Predator strategy aims to maximize its own total payoff in the N-Player
    Public Goods Game by adaptively managing the cooperation levels of other players.
    It phases through initial probing, adaptive exploitation/inducement, and a final
    end-game defection.

    In the Initial Probe Phase, it cooperates to signal a willingness to contribute and
    potentially foster a pool of cooperators.

    During the Adaptive Exploitation/Inducement Phase, it monitors the average cooperation
    rate of other players over a `MEMORY_WINDOW`. If the cooperation rate is above a
    `COOP_THRESHOLD_PROPORTION`, it defects to free-ride. If the cooperation rate
    falls below the threshold, it temporarily cooperates to try and re-induce cooperation,
    aiming to restore opportunities for future free-riding.

    In the End Game Phase, as influence over future rounds diminishes, the strategy
    unconditionally defects to maximize immediate payoff without concern for future
    interactions.
    """
    INITIAL_COOP_ROUNDS: int
    MEMORY_WINDOW: int
    COOP_THRESHOLD_PROPORTION: float
    END_GAME_WINDOW: int

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.INITIAL_COOP_ROUNDS = 1
        self.MEMORY_WINDOW = 3
        self.COOP_THRESHOLD_PROPORTION = 0.25
        self.END_GAME_WINDOW = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed <= self.INITIAL_COOP_ROUNDS:
            return Action.C
        if current_round_0_indexed >= self.n_rounds - self.END_GAME_WINDOW:
            return Action.D
        rounds_since_initial_probe_end = current_round_1_indexed - self.INITIAL_COOP_ROUNDS
        actual_memory_window = min(self.MEMORY_WINDOW, rounds_since_initial_probe_end)
        if actual_memory_window <= 0:
            return Action.D
        total_coop_from_others = np.sum(history.opponent_actions[-actual_memory_window:, :])
        num_possible_cooperators_in_window = actual_memory_window * (self.n_players - 1)
        if num_possible_cooperators_in_window == 0:
            return Action.D
        average_coop_from_others_proportion = total_coop_from_others / num_possible_cooperators_in_window
        if average_coop_from_others_proportion >= self.COOP_THRESHOLD_PROPORTION:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_62(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by strategically free-riding on
    others' cooperation while adapting to the overall cooperativeness of the
    environment. It prioritizes securing a minimum payoff in hostile environments
    and exploiting cooperative ones.
    """
    P_ROUNDS = 3
    COMMITMENT_THRESHOLD = 0.5

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.EXPLOIT_THRESHOLD = max(1, math.floor((self.game_description.n_players - 1) / 2))
        self.operational_phase = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        if current_round == self.game_description.n_rounds:
            return Action.D
        if current_round <= self.P_ROUNDS:
            return Action.C
        if self.operational_phase is None:
            total_C_others_during_probe = 0
            for r_idx in range(self.P_ROUNDS):
                total_C_others_during_probe += np.sum(history.opponent_actions[r_idx, :])
            C_OTHERS_AVG_PROBE = total_C_others_during_probe / self.P_ROUNDS
            if C_OTHERS_AVG_PROBE < (self.game_description.n_players - 1) * self.COMMITMENT_THRESHOLD:
                self.operational_phase = 'PD'
            else:
                self.operational_phase = 'AE'
        if self.operational_phase == 'PD':
            return Action.D
        elif self.operational_phase == 'AE':
            last_round_idx = state.round_number - 1
            C_OTHERS_PREV = np.sum(history.opponent_actions[last_round_idx, :])
            if C_OTHERS_PREV >= self.EXPLOIT_THRESHOLD:
                return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_63(BaseStrategy):
    """
    The "Opportunistic Predator" strategy is designed to maximize its own payoff by strategically encouraging cooperation
    from others and then exploiting it. It employs a mix of initial cooperation, dynamic adaptation, and ruthless endgame
    defection, coupled with swift punishment for non-cooperation. This strategy is robust against various opponent
    behaviors by avoiding being a "sucker" and by consistently seeking out opportunities for self-gain.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.endgame_horizon = 2
        self.cooperation_threshold_low = 0.6
        self.cooperation_threshold_high = 0.8
        self.punishment_rounds = 2
        self.punishment_active = False
        self.punishment_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_t = state.round_number + 1
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_t >= n_rounds - (self.endgame_horizon - 1):
            return Action.D
        if self.punishment_active:
            if self.punishment_timer > 0:
                self.punishment_timer -= 1
                return Action.D
            else:
                self.punishment_active = False
        if current_t == 1:
            return Action.C
        prev_round_opponent_actions = history.opponent_actions[state.round_number - 1, :]
        C_others_prev_round = sum(prev_round_opponent_actions)
        n_others = n_players - 1
        C_ratio_others_prev_round = C_others_prev_round / n_others
        if C_ratio_others_prev_round < self.cooperation_threshold_low:
            self.punishment_active = True
            self.punishment_timer = self.punishment_rounds - 1
            return Action.D
        elif C_ratio_others_prev_round >= self.cooperation_threshold_high:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_64(BaseStrategy):
    """
    The Opportunistic Shark strategy for the N-Player Public Goods Game.

    This strategy probes for cooperation, exploits it aggressively when found,
    and retreats/re-probes when cooperation dwindles, always with the aim of
    maximizing its own gains.

    Decision Rules:
    1. First Round (0-indexed: 0): Cooperate ('C') to probe for cooperation and signal intent.
    2. Last Round (0-indexed: r-1): Defect ('D') due to backward induction; no future rounds to influence.
    3. Intermediate Rounds:
       - If a sufficient number of *other* players cooperated in the previous round
         (defined by EXPLOIT_THRESHOLD_COOPERATORS), Defect ('D') to free-ride.
       - If not enough others cooperated:
         - If this player has been defecting for `RETEST_FREQUENCY` rounds or more consecutively,
           Cooperate ('C') to re-seed cooperation, attempting to revive the public good for future exploitation.
         - Otherwise (not yet time to re-test), Defect ('D') to continue signaling dissatisfaction or waiting for others to cooperate.

    Strategy Parameters:
    - EXPLOIT_THRESHOLD_COOPERATORS: Minimum number of *other* players cooperating
                                     in the previous round to trigger this player's exploitation (Defect).
                                     Calculated as ceil((n - 1) / 2).
    - RETEST_FREQUENCY: Number of consecutive rounds this player defects before
                        attempting to cooperate again when overall cooperation from others is low.

    State Variables (internal to this strategy instance):
    - rounds_since_last_cooperation_attempt: An integer counter for the number of
                                             consecutive rounds this player has chosen to Defect.
                                             It resets to 0 when this player chooses to Cooperate ('C').
                                             This variable is updated at the end of each `__call__`
                                             method based on the action chosen for the *current* round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.EXPLOIT_THRESHOLD_COOPERATORS = math.ceil((self.n - 1) / 2)
        self.RETEST_FREQUENCY = 3
        self.rounds_since_last_cooperation_attempt = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        my_current_action: Action
        if current_round_0_indexed == 0:
            my_current_action = Action.C
        elif current_round_0_indexed == self.r - 1:
            my_current_action = Action.D
        else:
            num_others_cooperated_previous_round = sum(history.opponent_actions[-1, :])
            if num_others_cooperated_previous_round >= self.EXPLOIT_THRESHOLD_COOPERATORS:
                my_current_action = Action.D
            elif self.rounds_since_last_cooperation_attempt >= self.RETEST_FREQUENCY:
                my_current_action = Action.C
            else:
                my_current_action = Action.D
        if my_current_action == Action.C:
            self.rounds_since_last_cooperation_attempt = 0
        else:
            self.rounds_since_last_cooperation_attempt += 1
        return my_current_action

class Strategy_EXPLOITATIVE_65(BaseStrategy):
    """
    The 'Adaptive Exploiter' strategy aims to maximize payoff in the N-Player Public Goods Game.
    It probes for cooperation, exploits when cooperation is sufficient, punishes unreciprocated
    cooperation, re-engages if cooperation drops, and defects in the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self._threshold_M = math.ceil(self.n / self.k)
        self._punishment_duration = 3
        self._punishment_counter = 0
        self._last_action_was_C_alone = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.r:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        my_action_prev_was_C = history.my_actions[-1]
        C_others_prev_count = np.sum(history.opponent_actions[-1, :])
        if my_action_prev_was_C and C_others_prev_count == 0:
            self._last_action_was_C_alone = True
        else:
            self._last_action_was_C_alone = False
        C_total_prev = C_others_prev_count + (1 if my_action_prev_was_C else 0)
        if self._punishment_counter > 0:
            self._punishment_counter -= 1
            return Action.D
        elif self._last_action_was_C_alone:
            self._punishment_counter = min(self.r - current_round_1_indexed, self._punishment_duration)
            return Action.D
        elif C_total_prev >= self._threshold_M:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_66(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize individual payoff in the N-Player Public Goods Game
    by strategically oscillating between cooperating (to foster the public good) and defecting
    (to free-ride on it). It adapts to the observed level of cooperation, is robust across
    game parameters, and explicitly handles edge cases.

    Core Principle:
    The Adaptive Exploiter "invests" in the public good by cooperating when aggregate cooperation
    is low, and "exploits" by defecting when aggregate cooperation is high enough to ensure a
    strong personal return. It always defects in the final round to maximize immediate gain
    without future repercussions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.C_min_for_exploitation = math.ceil((self.k_factor - 1) * self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        my_prev_action_cooperated_int = int(history.my_actions[-1])
        opponent_cooperators_prev = sum(history.opponent_actions[-1, :])
        C_count_prev = my_prev_action_cooperated_int + opponent_cooperators_prev
        if C_count_prev >= self.C_min_for_exploitation:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_67(BaseStrategy):
    """
    The Opportunistic Catalyst: Primarily defects to free-ride on others' contributions.
    It strategically cooperates for a single round to "catalyze" or re-establish cooperation
    when the observed level of cooperation from other players drops below a dynamically
    calculated threshold. After this single priming cooperation, it immediately reverts
    to defection to avoid being exploited. The strategy always defects in the last round,
    following backward induction, and starts with a defection to test the environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.priming_attempt_in_progress: bool = False
        n = self.game_description.n_players
        k = self.game_description.k
        self.cooperation_revival_threshold: int = math.floor(n / k)
        self.cooperation_revival_threshold = min(n - 1, self.cooperation_revival_threshold)
        self.cooperation_revival_threshold = max(2, self.cooperation_revival_threshold)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            self.priming_attempt_in_progress = False
            return Action.D
        if current_round_1_indexed == total_rounds:
            return Action.D
        c_prev = sum(history.opponent_actions[-1, :])
        if self.priming_attempt_in_progress:
            self.priming_attempt_in_progress = False
            return Action.D
        elif c_prev >= self.cooperation_revival_threshold:
            return Action.D
        else:
            self.priming_attempt_in_progress = True
            return Action.C

class Strategy_EXPLOITATIVE_68(BaseStrategy):
    """
    The Opportunistic Freeloader (OFL) strategy aims to maximize individual payoff in the N-Player Public Goods Game
    by primarily free-riding (Defecting) when collective cooperation is sufficiently high.
    It strategically cooperates (C) in initial "explore" phases and during "reset" phases when cooperation levels
    drop too low, in order to re-induce cooperation for future exploitation. It always defects in the final round.
    """
    INITIAL_COOP_ROUNDS = 2
    LOOKBACK_ROUNDS = 3
    EXPLOIT_THRESHOLD = 0.5
    MINIMUM_COOP_THRESHOLD = 0.2
    RESET_COOP_ROUNDS = 1
    _current_state: str
    _rounds_in_current_state: int
    _history_cooperators: list[int]

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._current_state = 'EXPLORE'
        self._rounds_in_current_state = 0
        self._history_cooperators = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number > 0 and history is not None:
            my_prev_action_was_coop = history.my_actions[-1]
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_prev_action_was_coop else 0)
            self._history_cooperators.append(int(total_cooperators_prev_round))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        current_action_choice = Action.C
        if self._current_state == 'EXPLORE':
            current_action_choice = Action.C
            self._rounds_in_current_state += 1
            if self._rounds_in_current_state >= self.INITIAL_COOP_ROUNDS:
                self._current_state = 'EXPLOIT'
                self._rounds_in_current_state = 0
        elif self._current_state == 'RESET':
            current_action_choice = Action.C
            self._rounds_in_current_state += 1
            if self._rounds_in_current_state >= self.RESET_COOP_ROUNDS:
                self._current_state = 'EXPLOIT'
                self._rounds_in_current_state = 0
        elif self._current_state == 'EXPLOIT':
            relevant_history_start_idx = max(0, len(self._history_cooperators) - self.LOOKBACK_ROUNDS)
            relevant_history_cooperators_window = self._history_cooperators[relevant_history_start_idx:]
            avg_coop_proportion = 0.0
            if relevant_history_cooperators_window:
                total_cooperators_in_window = sum(relevant_history_cooperators_window)
                num_observed_rounds_in_window = len(relevant_history_cooperators_window)
                num_players = self.game_description.n_players
                avg_coop_proportion = total_cooperators_in_window / (num_observed_rounds_in_window * num_players)
            if avg_coop_proportion >= self.EXPLOIT_THRESHOLD:
                current_action_choice = Action.D
            elif avg_coop_proportion < self.MINIMUM_COOP_THRESHOLD:
                current_action_choice = Action.C
                self._current_state = 'RESET'
                self._rounds_in_current_state = 1
            else:
                current_action_choice = Action.D
        return current_action_choice

class Strategy_EXPLOITATIVE_69(BaseStrategy):
    """
    The "Adaptive Exploiter" strategy for the N-Player Public Goods Game.
    This strategy is designed to primarily defect when a sufficient level of public good exists,
    but will strategically cooperate for short periods to "reset" and encourage cooperation if
    the public good contributions fall too low, and will avoid cooperating if others are
    consistently uncooperative. It prioritizes maximizing its own payoff by free-riding
    on others' contributions and strategically investing in cooperation only when it's
    expected to create opportunities for future free-riding.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        k = game_description.k
        self.n_rounds = game_description.n_rounds
        self._cooperation_reset_duration: int = 2
        self._cooperation_threshold_for_exploit: int = math.ceil(n / k)
        self._reset_countdown: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        my_prev_action_bool = history.my_actions[-1]
        my_prev_action = Action.C if my_prev_action_bool else Action.D
        opponent_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        nc_last_round = opponent_cooperators_last_round + (1 if my_prev_action == Action.C else 0)
        if self._reset_countdown > 0:
            self._reset_countdown -= 1
            return Action.C
        elif nc_last_round >= self._cooperation_threshold_for_exploit:
            return Action.D
        else:
            num_coop_excluding_me = nc_last_round - (1 if my_prev_action == Action.C else 0)
            if num_coop_excluding_me <= 0:
                return Action.D
            else:
                self._reset_countdown = self._cooperation_reset_duration - 1
                return Action.C

class Strategy_EXPLOITATIVE_70(BaseStrategy):
    """
    The Adaptive Exploiter (AE) is an opportunistic strategy designed to maximize its
    cumulative payoff in the N-Player Public Goods Game. It strategically balances
    immediate free-riding with calculated investments to sustain a cooperative environment
    for future exploitation.

    **Decision Rules:**
    1.  **Round 1 (Initial Investment):** Cooperate (C) to encourage initial cooperation from others.
    2.  **Rounds 2 to r-1 (Adaptive Exploitation Phase):**
        *   If at least one other player cooperated in the previous round, Defect (D) to free-ride.
        *   If no other player cooperated in the previous round (public good collapsed), Cooperate (C)
            to "re-seed" cooperation, hoping to entice others back into contributing for future exploitation.
    3.  **Round r (Final Defection):** Defect (D), as there are no future rounds to influence.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            prev_round_opponent_actions = history.opponent_actions[state.round_number - 1, :]
            num_other_cooperators = sum(prev_round_opponent_actions)
            if num_other_cooperators >= 1:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_71(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize its own payoff in the N-Player Public Goods Game.
    It starts by cooperating to probe the environment and establish a public good.
    Then, in intermediate rounds, it monitors the level of collective cooperation from the previous round (C_prev).
    - If C_prev is below a calculated threshold (ceil(n_players / 2)), it cooperates to prevent collapse.
    - If C_prev is at or above the threshold, it defects to free-ride.
    - If C_prev is zero, it defects as there's no public good to exploit.
    In the final round, it always defects due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.COOPERATION_THRESHOLD = math.ceil(self.game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        c_prev_opponents = np.sum(history.opponent_actions[-1, :])
        c_prev_self = 1 if history.my_actions[-1] else 0
        C_prev = c_prev_opponents + c_prev_self
        if C_prev == 0:
            return Action.D
        elif C_prev < self.COOPERATION_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_72(BaseStrategy):
    """
    The Adaptive Opportunistic Defector (AOD) strategy for the N-Player Public Goods Game.
    This strategy prioritizes individual payoff by largely free-riding (defecting).
    It only cooperates for a single round as a strategic "investment" if the total
    level of cooperation observed in the previous round falls below a calculated threshold,
    hoping to stimulate more cooperation from others for future exploitation.
    It always reverts to defection after cooperating and defects unconditionally in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_revival_threshold = math.floor(self.game_description.n_players / 3)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round == 0:
            return Action.D
        if current_round == total_rounds - 1:
            return Action.D
        my_action_t_minus_1_is_cooperate = history.my_actions[-1]
        cooperators_from_opponents_t_minus_1 = np.sum(history.opponent_actions[-1, :])
        cooperators_from_me_t_minus_1 = 1 if my_action_t_minus_1_is_cooperate else 0
        C_t_minus_1 = cooperators_from_opponents_t_minus_1 + cooperators_from_me_t_minus_1
        if not my_action_t_minus_1_is_cooperate:
            if C_t_minus_1 >= self.cooperation_revival_threshold:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_73(BaseStrategy):
    """
    The Opportunistic Free-Rider strategy aims to maximize its own payoff by leveraging
    the cooperation of others while minimizing its own contributions. It adapts its
    behavior based on observed history, cooperating as an "investment" to foster
    future cooperation or to prevent total collapse of the public good, and
    defecting to exploit or to avoid being a "sucker."
    """
    INITIAL_COOPERATE_ROUNDS = 1
    DEFECT_THRESHOLD_ROUNDS = 2
    MIN_OTHERS_FOR_EXPLOITABLE_COOPERATION = 0.6
    MIN_OTHERS_TO_RESEED_COOPERATION = 0.3

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round_1_indexed <= self.INITIAL_COOPERATE_ROUNDS:
            return Action.C
        if current_round_1_indexed > n_rounds - self.DEFECT_THRESHOLD_ROUNDS:
            return Action.D
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        num_other_cooperators_prev_round = np.sum(prev_round_opponent_actions)
        n_minus_1 = n_players - 1
        proportion_other_cooperators = num_other_cooperators_prev_round / n_minus_1
        if proportion_other_cooperators >= self.MIN_OTHERS_FOR_EXPLOITABLE_COOPERATION:
            return Action.D
        elif proportion_other_cooperators < self.MIN_OTHERS_TO_RESEED_COOPERATION:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_74(BaseStrategy):
    """
    Conditional Provocation & Exploitation (CPE) Strategy for N-Player Public Goods Game.

    This strategy is designed to primarily exploit any existing cooperation by defecting,
    but also to "provoke" or re-seed cooperation if the common good completely collapses.
    It leverages the repeated nature of the game and perfect information to adapt its behavior.

    Core Principles:
    1.  Initial Probe: Start by defecting to immediately free-ride and test opponents.
    2.  Sustained Exploitation: Whenever there is any cooperation in the public good
        in the previous round, defect to maximize individual payoff.
    3.  Provocation/Reset: If cooperation completely collapsed (everyone defected) in the
        previous round, temporarily cooperate to "seed" the public good, hoping to
        re-ignite cooperation that can then be exploited.
    4.  Endgame Rationality: Always defect in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        num_C_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_C_prev_round == 0:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_75(BaseStrategy):
    """
    The Opportunistic Prober strategy aims to free-ride as much as possible in the N-Player Public Goods Game.
    It starts with an initial cooperation to 'probe' for cooperative tendencies, then defects relentlessly.
    It only contributes again if cooperation among others drops below a critical threshold, threatening the
    existence of the public good it exploits, or if it has free-rided for too long and risks causing a collapse.
    In the final rounds, it defects without hesitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.FREE_RIDE_MAX_STREAK = max(2, self.n_players // 3)
        self.MIN_COOPERATORS_FOR_PUBLIC_GOOD = max(2, self.n_players // 4)
        self.HIGH_COOPERATION_THRESHOLD = self.n_players - 1
        self.our_action_prev: Action | None = None
        self.consecutive_defections_count: int = 0
        self.cooperation_collapse_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_t_0indexed = state.round_number
        action_for_this_round: Action
        if current_t_0indexed == self.n_rounds - 1:
            action_for_this_round = Action.D
        elif current_t_0indexed == self.n_rounds - 2:
            action_for_this_round = Action.D
        elif current_t_0indexed == 0:
            action_for_this_round = Action.C
            self.consecutive_defections_count = 0
            self.cooperation_collapse_streak = 0
        else:
            C_others_prev = sum(history.opponent_actions[current_t_0indexed - 1, :])
            if C_others_prev < self.MIN_COOPERATORS_FOR_PUBLIC_GOOD:
                self.cooperation_collapse_streak += 1
            else:
                self.cooperation_collapse_streak = 0
            action_for_this_round = Action.D
            if self.cooperation_collapse_streak >= 2:
                action_for_this_round = Action.D
            elif C_others_prev >= self.HIGH_COOPERATION_THRESHOLD:
                action_for_this_round = Action.D
            elif self.consecutive_defections_count >= self.FREE_RIDE_MAX_STREAK:
                action_for_this_round = Action.C
            else:
                action_for_this_round = Action.D
        self.our_action_prev = action_for_this_round
        if action_for_this_round == Action.D:
            self.consecutive_defections_count += 1
        else:
            self.consecutive_defections_count = 0
        return action_for_this_round

class Strategy_EXPLOITATIVE_76(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to free-ride on the cooperation of others in the N-Player
    Public Goods Game. It initiates cooperation to probe for potential cooperators, then defects
    to exploit. If cooperation from others wanes, it temporarily "re-invests" by cooperating
    again for a limited period to try and re-establish a public good pool, before reverting
    to full defection if its investment doesn't pay off. It also applies backward induction
    for full exploitation in the final rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Exploiter strategy with game-specific parameters
        and sets up internal state variables.
        """
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        k = game_description.k
        self.EXPLOIT_THRESHOLD_RATE = k / n
        self.FORGIVENESS_ROUNDS = max(1, r // 5)
        self.ENDGAME_HORIZON = 1
        self.low_cooperation_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on
        the Adaptive Exploiter strategy's rules.
        """
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_0_indexed == 0:
            self.low_cooperation_streak = 0
            return Action.C
        if current_round_0_indexed >= total_rounds - self.ENDGAME_HORIZON:
            return Action.D
        previous_round_idx = current_round_0_indexed - 1
        m_others_prev_round = sum(history.opponent_actions[previous_round_idx, :])
        cooperation_rate_others = m_others_prev_round / (n_players - 1)
        if cooperation_rate_others >= self.EXPLOIT_THRESHOLD_RATE:
            self.low_cooperation_streak = 0
            chosen_action = Action.D
        else:
            self.low_cooperation_streak += 1
            if self.low_cooperation_streak <= self.FORGIVENESS_ROUNDS:
                chosen_action = Action.C
            else:
                chosen_action = Action.D
        return chosen_action

class Strategy_EXPLOITATIVE_77(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by strategically probing for cooperation,
    then free-riding on it, and punishing when it is itself exploited or when cooperation collapses.
    It adapts to varying levels of cooperation among opponents and is robust across different
    game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.punish_rounds_remaining: int = 0
        self.coop_threshold_proportion: float = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_total = self.game_description.n_players
        r_total = self.game_description.n_rounds
        current_round = state.round_number + 1
        if current_round == r_total:
            self.my_last_action = Action.D
            return Action.D
        Coop_others_prev = 0
        if current_round > 1 and history is not None:
            Coop_others_prev = int(np.sum(history.opponent_actions[-1, :]))
        if self.punish_rounds_remaining > 0:
            self.punish_rounds_remaining -= 1
            self.my_last_action = Action.D
            return Action.D
        if current_round == 1:
            self.my_last_action = Action.C
            return Action.C
        if self.my_last_action == Action.C:
            required_coop_for_reciprocity = (n_total - 1) * self.coop_threshold_proportion
            if Coop_others_prev >= math.floor(required_coop_for_reciprocity):
                self.my_last_action = Action.D
                return Action.D
            else:
                total_punishment_rounds = min(2, r_total - current_round)
                self.punish_rounds_remaining = total_punishment_rounds - 1
                self.my_last_action = Action.D
                return Action.D
        elif self.my_last_action == Action.D:
            if Coop_others_prev > 0:
                self.my_last_action = Action.D
                return Action.D
            else:
                self.my_last_action = Action.C
                return Action.C
        self.my_last_action = Action.D
        return Action.D

class Strategy_EXPLOITATIVE_78(BaseStrategy):
    """
    The "Chameleon Free-Rider" strategy for the N-Player Public Goods Game.

    This strategy is designed to be opportunistic and exploitative. It attempts to
    initiate cooperation in the first round, then free-ride on others' contributions.
    It adapts its behavior based on the observed level of cooperation in the
    immediately preceding round, aiming to exploit high cooperation, protect
    itself during low cooperation, and strategically cooperate to "repair" a
    deteriorating cooperative environment for future exploitation opportunities.
    It always defects in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.total_rounds = game_description.n_rounds
        self.EXPLOI_THRESHOLD = self.n_players - 1
        self.COLLAPSE_THRESHOLD = 0
        self.REPAIR_THRESHOLD = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.total_rounds:
            return Action.D
        last_round_cooperators_opponents = np.sum(history.opponent_actions[-1, :])
        my_last_action_bool = history.my_actions[-1]
        total_cooperators_last_round = last_round_cooperators_opponents + int(my_last_action_bool)
        my_last_action = Action.C if my_last_action_bool else Action.D
        if total_cooperators_last_round >= self.EXPLOI_THRESHOLD:
            return Action.D
        if total_cooperators_last_round == self.COLLAPSE_THRESHOLD:
            return Action.D
        if my_last_action == Action.D:
            if total_cooperators_last_round >= self.REPAIR_THRESHOLD:
                return Action.D
            else:
                return Action.C
        elif total_cooperators_last_round >= self.REPAIR_THRESHOLD:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_79(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy is designed to maximize individual payoff in the N-Player
    Public Goods Game by strategically oscillating between cooperation (to foster the public good)
    and defection (to exploit it). It's adaptive to the collective behavior of other players and
    becomes increasingly exploitative as the game progresses.

    Decision Rules:
    - First Round: Cooperate (C) to initiate potential for cooperation and gather information.
    - Last Round: Defect (D) due to backward induction; no future interactions to influence.
    - Intermediate Rounds:
        - Calculates two dynamic thresholds (`low_threshold` and `high_threshold`) based on the
          `progress_factor` (how far the game has progressed).
        - If `N_C_others_prev` (number of other cooperators in the previous round) is at or above
          `high_threshold`: Defect (D) to free-ride on sufficient public good contributions.
        - Else if `N_C_others_prev` is at or below `low_threshold`: Defect (D) to cut losses when
          cooperation levels are critically low.
        - Else (moderate cooperation: `low_threshold < N_C_others_prev < high_threshold`): Cooperate (C)
          to strategically invest and sustain the public good for future exploitation.

    Thresholds adapt over time, with the `high_threshold` decreasing more rapidly to reflect
    increasing exploitation as the game nears its end.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        my_action_prev_round_int = 1 if history.my_actions[-1] else 0
        n_c_others_prev = sum(history.opponent_actions[-1, :])
        progress_factor = current_round_0_indexed / (self.n_rounds - 1)
        low_threshold_proportion = 0.2 - 0.1 * progress_factor
        low_threshold = math.floor((self.n_players - 1) * low_threshold_proportion)
        low_threshold = max(0, low_threshold)
        high_threshold_proportion = 0.8 - 0.4 * progress_factor
        high_threshold = math.floor((self.n_players - 1) * high_threshold_proportion)
        min_high_threshold_val = low_threshold + 1 if low_threshold < self.n_players - 1 else low_threshold
        high_threshold = max(min_high_threshold_val, high_threshold)
        high_threshold = min(self.n_players - 1, high_threshold)
        if n_c_others_prev >= high_threshold:
            return Action.D
        elif n_c_others_prev <= low_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_80(BaseStrategy):
    """
    The Conditional Exploiter strategy for the N-Player Public Goods Game.
    It probes for cooperation in Round 1, ruthlessly defects in the final round,
    and in intermediate rounds, defects when sufficient cooperation (above a calculated threshold)
    is observed from the previous round, otherwise cooperates to re-seed the public good.
    This strategy aims to maximize personal payoff by exploiting others' contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        self.cooperation_threshold_M = max(1, math.floor(n * (1 - k / n)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if cooperators_prev_round >= self.cooperation_threshold_M:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_81(BaseStrategy):
    """
    The Calculated Leech strategy for the N-Player Public Goods Game.

    This strategy is designed to be highly exploitative, primarily defaulting
    to defection to maximize immediate gain. It is adaptive in that it
    strategically cooperates only under very specific, self-serving conditions
    to "re-seed" the public good if it collapses, thereby preserving future
    opportunities for exploitation. It is robust by avoiding situations where
    it repeatedly acts as the sole cooperator.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.THRESHOLD_COOPERATION_COLLAPSE = 0
        self.ENDGAME_THRESHOLD_ROUNDS = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        if state.round_number >= total_rounds - 1 - self.ENDGAME_THRESHOLD_ROUNDS:
            return Action.D
        my_action_prev_bool = history.my_actions[state.round_number - 1]
        n_c_others_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
        if n_c_others_prev <= self.THRESHOLD_COOPERATION_COLLAPSE and (not my_action_prev_bool):
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_82(BaseStrategy):
    """
    The Dynamic Free-Rider (DFR) strategy aims to maximize its total payoff by
    free-riding on the contributions of other players as much as possible.
    It probes for cooperation initially, aggressively free-rides when a
    sufficient public good is generated by others, and strategically
    "invests" in cooperation only when the collective public good is too low
    to exploit effectively and there are enough future rounds for this
    investment to pay off. It avoids being a "sucker" by quickly defecting
    if its cooperation is not sufficiently reciprocated.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.INITIAL_PROBE_ROUNDS = 1
        self.COOPERATION_RESEED_THRESHOLD = 0.15
        self.MIN_ROUNDS_FOR_RESEED = 4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed < self.INITIAL_PROBE_ROUNDS:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        my_action_prev_bool = history.my_actions[-1]
        previous_round_opponent_actions_bool = history.opponent_actions[-1, :]
        total_cooperators_prev = int(my_action_prev_bool) + np.sum(previous_round_opponent_actions_bool)
        other_cooperators_prev = np.sum(previous_round_opponent_actions_bool)
        num_other_cooperations = np.sum(history.opponent_actions[:current_round_0_indexed, :])
        total_other_opportunities = history.opponent_actions[:current_round_0_indexed, :].size
        overall_other_cooperation_rate = 0.0
        if total_other_opportunities > 0:
            overall_other_cooperation_rate = num_other_cooperations / total_other_opportunities
        if my_action_prev_bool:
            if total_cooperators_prev <= 1:
                return Action.D
            else:
                return Action.D
        elif other_cooperators_prev > 0:
            return Action.D
        else:
            remaining_rounds = total_rounds - (current_round_0_indexed + 1)
            if overall_other_cooperation_rate < self.COOPERATION_RESEED_THRESHOLD and remaining_rounds >= self.MIN_ROUNDS_FOR_RESEED:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_83(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to opportunistically free-ride on the cooperation of others,
    while selectively cooperating to foster or maintain a level of public good that can be exploited.
    It prioritizes maximizing its own payoff by defecting whenever possible without causing
    a complete collapse of cooperation that would eliminate future exploitation opportunities.

    Key principles:
    1.  Probe for Cooperation (Initial Round): Start by cooperating to signal willingness and test for cooperation.
    2.  Exploit Robust Cooperation (Middle Rounds): Defect when enough others cooperate to free-ride.
    3.  Induce Cooperation (Middle Rounds): Cooperate when cooperation is moderate to encourage more.
    4.  Avoid Being a Sucker (Middle Rounds): Defect when cooperation is too low to bother contributing.
    5.  Backward Induction (Last Round): Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        n_others = self.n_players - 1
        self.EXPLOITATION_THRESHOLD_OTHERS = max(1, math.floor(n_others * 0.7))
        self.PUNISHMENT_THRESHOLD_OTHERS = max(0, math.floor(n_others * 0.2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        C_others_prev_round = sum(history.opponent_actions[current_round_0_indexed - 1, :])
        if C_others_prev_round >= self.EXPLOITATION_THRESHOLD_OTHERS:
            return Action.D
        elif C_others_prev_round <= self.PUNISHMENT_THRESHOLD_OTHERS:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_84(BaseStrategy):
    """
    The Conditional Probing Exploiter (CPE) strategy.

    This strategy aims to maximize its own total payoff by free-riding on others' contributions.
    It defaults to defection, which is the dominant strategy in a single round.
    However, in a repeated game, continuous defection can lead to a low-payoff
    "all defect" equilibrium.

    CPE incorporates a calculated, one-time cooperative "probe" in the first round
    under specific conditions. This probe aims to activate reciprocal cooperation
    from other players, which the strategy will then immediately exploit by
    consistently defecting in subsequent rounds.

    Decision Rules:
    1.  Pre-computation (in __init__):
        Calculates an "Exploitation Incentive Threshold" (EIT). This threshold
        determines if an initial cooperative move is likely to yield a higher
        total payoff compared to always defecting, assuming a scenario where
        other players might include reciprocal cooperators (e.g., Tit-for-Tat).
        EIT = k * (2n - 1) / n

    2.  Round 1 (state.round_number == 0):
        If EIT > 1: Play C (Cooperate). This is a strategic investment to
                     "prime the pump" for exploitation.
        Else (if EIT <= 1): Play D (Defect). The risk of an initial cooperative
                             move is too high; default to dominant strategy.

    3.  Rounds 2 to r (state.round_number > 0):
        Always play D (Defect). This free-rides on any induced cooperation or
        minimizes losses if cooperation is absent. This also covers the last
        round, aligning with backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        self.eit_value = k * ((2.0 * n - 1.0) / n)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number
        if current_round_t == 0:
            if self.eit_value > 1.0:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_85(BaseStrategy):
    """
    The "Calculated Exploiter" strategy is designed to maximize individual payoff in the
    N-Player Public Goods Game by primarily free-riding on the cooperation of others.
    It only contributes to the public good when the overall level of cooperation is
    critically low, acting as a "booster" to prevent the complete collapse of
    cooperation, which would result in suboptimal outcomes for everyone, including the exploiter.
    It is adaptive, robust, and prioritizes self-interest.
    """
    COOP_THRESHOLD: float = 0.3
    BOOST_DURATION: int = 2
    BOOST_PAYOFF_FLOOR: float = 1.0

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action: Action = Action.D
        self.last_round_cooperators: int = 0
        self.my_payoff_last_round: float = 0.0
        self.is_boosting: bool = False
        self.rounds_left_in_boost: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_action: Action
        if state.round_number > 0 and history is not None:
            self.my_payoff_last_round = history.my_payoffs[-1]
            own_action_prev_round_is_coop: bool = history.my_actions[-1]
            opponent_cooperators_prev_round: int = np.sum(history.opponent_actions[-1, :])
            self.last_round_cooperators = opponent_cooperators_prev_round + (1 if own_action_prev_round_is_coop else 0)
            self.my_last_action = Action.C if own_action_prev_round_is_coop else Action.D
        if state.round_number == 0:
            self.is_boosting = False
            self.rounds_left_in_boost = 0
            current_action = Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            self.is_boosting = False
            self.rounds_left_in_boost = 0
            current_action = Action.D
        elif self.is_boosting:
            if self.my_payoff_last_round < self.BOOST_PAYOFF_FLOOR:
                self.is_boosting = False
                self.rounds_left_in_boost = 0
                current_action = Action.D
            else:
                current_action = Action.C
                self.rounds_left_in_boost -= 1
                if self.rounds_left_in_boost == 0:
                    self.is_boosting = False
        else:
            cooperation_proportion_prev = self.last_round_cooperators / self.game_description.n_players
            if cooperation_proportion_prev < self.COOP_THRESHOLD:
                self.is_boosting = True
                self.rounds_left_in_boost = self.BOOST_DURATION - 1
                current_action = Action.C
            else:
                current_action = Action.D
                self.is_boosting = False
                self.rounds_left_in_boost = 0
        self.my_last_action = current_action
        return current_action

class Strategy_EXPLOITATIVE_86(BaseStrategy):
    """
    Adaptive Free-Rider with Sucker's Guard for N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by adaptively free-riding on
    cooperative opponents. It strategically defects when enough players
    cooperate, but is willing to "re-seed" cooperation by contributing when
    overall cooperation levels are low. A "sucker's guard" mechanism prevents
    it from continuously losing out if its attempts to re-seed cooperation
    fail repeatedly. The strategy always defects in the final round as per
    backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.M = math.ceil(self.n / self.k)
        self.max_sucker_rounds = 2
        self.consecutive_sucker_moves = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            action_to_take = Action.C
        elif current_round_1_indexed == self.r:
            action_to_take = Action.D
            self.consecutive_sucker_moves = 0
        else:
            my_prev_action_was_C = history.my_actions[-1]
            cooperators_in_prev_round = int(my_prev_action_was_C) + np.sum(history.opponent_actions[-1, :])
            if my_prev_action_was_C and cooperators_in_prev_round < self.M:
                self.consecutive_sucker_moves += 1
            else:
                self.consecutive_sucker_moves = 0
            if self.consecutive_sucker_moves >= self.max_sucker_rounds:
                action_to_take = Action.D
                self.consecutive_sucker_moves = 0
            elif cooperators_in_prev_round >= self.M:
                action_to_take = Action.D
            else:
                action_to_take = Action.C
        return action_to_take

class Strategy_EXPLOITATIVE_87(BaseStrategy):
    """
    The Calculated Leech strategy for an N-Player Public Goods Game.

    This strategy aims to opportunistically exploit cooperation when present,
    and strategically "re-seed" cooperation when it dwindles, but only when
    there are enough future rounds to profit from this investment. It adopts
    a purely selfish stance in the final rounds when no future influence is possible.

    Rules:
    1.  Round 0 (first round): Cooperate to probe and signal willingness to cooperate.
    2.  Last Round (self.game_description.n_rounds - 1): Defect, as there are no future consequences.
    3.  Middle Rounds:
        a.  If the total number of cooperators in the previous round (including self)
            was >= ceil(n / 2) (cooperation_threshold): Defect to exploit.
        b.  Else (low cooperation):
            i.  If there are more than 1 round remaining after the current one:
                Cooperate strategically to re-seed and encourage future cooperation.
            ii. Else (1 or 0 rounds remaining after current): Defect, as there's
                not enough time to rebuild cooperation and exploit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = math.ceil(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        prev_round_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if prev_round_cooperators >= self.cooperation_threshold:
            return Action.D
        elif self.n_rounds - (current_round_0_indexed + 1) > 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_88(BaseStrategy):
    """
    The "Opportunistic Defector" (OD) strategy for the N-Player Public Goods Game.
    This strategy balances aggressive exploitation with strategic cooperation to maintain a favorable environment.
    It defaults to defecting when possible and only cooperates when it is strategically beneficial to either
    initiate or restore cooperation, ensuring a pool of contributions to exploit.

    Decision Rules:
    1. First Round (t=1): COOPERATE (C)
       - Rationale: Strategic opening gesture to encourage cooperation. If all players start by defecting,
                    the game quickly converges to the lowest possible payoff (1 for everyone),
                    leaving no public good to exploit. This initial cooperation "primes the pump."

    2. Last Two Rounds (t=r and t=r-1): DEFECT (D)
       - Rationale: Endgame rounds. In the final round, there are no subsequent interactions to influence,
                    so the strictly dominant strategy (Defect) applies. Applying backward induction,
                    players realize this for the penultimate round as well, as cooperation cannot induce
                    future cooperation. This aggressively cuts off contributions when no future consequences exist.

    3. Intermediate Rounds (1 < t < r-1): Adaptive Exploitation & Maintenance
       - If `C_others_t-1 == 0` (No one else cooperated in the previous round):
         - Action: DEFECT (D)
         - Rationale: If there's no public good being contributed by others, there's nothing to exploit.
                      Cooperating would only make us a "sucker" with no tangible benefit. We await a change.

       - Else (At least one other player cooperated in the previous round):
         - If `my_action_t-1 == D` (We defected in the previous round):
           - Check for a "Cooperation Collapse" signal:
             - Condition 1: `C_others_t-1 < C_others_t-2` (Cooperation from others has decreased).
             - Condition 2: `C_others_t-1 <= (n-1) / 2` (The remaining cooperation from others is at or below critical mass).
           - If both Condition 1 and Condition 2 are TRUE:
             - Action: COOPERATE (C)
             - Rationale: Our previous defection might have triggered a significant drop in cooperation.
                          To prevent total collapse into an "All Defect" state, we strategically cooperate for this round.
                          This is a short-term investment to attempt to restore the "cooperation pool."
           - Else:
             - Action: DEFECT (D)
             - Rationale: Our previous defection was either tolerated, or cooperation is still robust enough.
                          We maintain our exploitative stance.

         - If `my_action_t-1 == C` (We cooperated in the previous round):
           - Action: DEFECT (D)
           - Rationale: We made an investment in cooperation last round. Now it's time to reap the benefits
                        by defecting, assuming our previous cooperation helped maintain or improve the `C_others` count.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds or current_round_1_indexed == self.n_rounds - 1:
            return Action.D
        c_others_prev_round = sum(history.opponent_actions[-1, :])
        my_action_prev_round_is_C = history.my_actions[-1]
        if c_others_prev_round == 0:
            return Action.D
        if not my_action_prev_round_is_C:
            c_others_two_rounds_ago = 0
            if state.round_number >= 2:
                c_others_two_rounds_ago = sum(history.opponent_actions[-2, :])
            condition1_coop_decreased = c_others_prev_round < c_others_two_rounds_ago
            condition2_below_critical_mass = c_others_prev_round <= (self.n_players - 1) / 2
            if condition1_coop_decreased and condition2_below_critical_mass:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_89(BaseStrategy):
    """
    The Adaptive Opportunist strategy aims to maximize its own payoff by selectively
    free-riding on the contributions of others. It probes for cooperation, exploits
    it when sufficient, and strategically cooperates to prevent a complete breakdown
    of the public good if cooperation levels drop too low. It fully exploits the
    terminal round effect.

    Core Principles:
    - Leverages Others' Willingness to Cooperate: Attempts to initiate and sustain cooperation.
    - Minimizing Own Contributions: Contributes only when strategically necessary.
    - Capitalizing on End-Game Effects: Defects unconditionally in the final round.

    Key Parameter:
    - T_critical = 1/k: Cooperation Threshold. This is the proportion of cooperators in the
      previous round (P_c_prev) that ensures each cooperator received at least 1 unit of
      payoff from their share of the public good. When P_c_prev >= T_critical, there's
      sufficient cooperation to justify defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T_critical = 1.0 / self.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.r:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        num_opponent_coop_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
        my_action_prev = history.my_actions[state.round_number - 1]
        num_coop_prev = num_opponent_coop_prev + int(my_action_prev)
        P_c_prev = float(num_coop_prev) / self.n
        if P_c_prev >= self.T_critical:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_90(BaseStrategy):
    """
    The Adaptive Free-Rider strategy is designed to ruthlessly exploit cooperation from other
    players while being adaptive enough to encourage initial cooperation or punish sustained
    defection. It seeks to maximize its own payoff by free-riding on the public good whenever
    possible, and only investing in cooperation strategically or when forced to secure a minimum payoff.
    """
    COOPERATION_THRESHOLD_PERCENTAGE = 0.5
    FORGIVENESS_ROUNDS = 1
    BAIT_ROUNDS = 1

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._my_last_action: Action | None = None
        self._num_low_cooperation_rounds: int = 0
        self._rounds_since_last_bait_C: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if state.round_number == n_rounds - 1:
            action = Action.D
            self._my_last_action = action
            return action
        if state.round_number == 0:
            action = Action.C
            self._my_last_action = action
            self._num_low_cooperation_rounds = 0
            self._rounds_since_last_bait_C = 0
            return action
        C_others_prev = np.sum(history.opponent_actions[-1, :])
        threshold_cooperators = (n_players - 1) * self.COOPERATION_THRESHOLD_PERCENTAGE
        if C_others_prev < threshold_cooperators:
            self._num_low_cooperation_rounds += 1
        else:
            self._num_low_cooperation_rounds = 0
        if self._my_last_action == Action.C:
            self._rounds_since_last_bait_C = 0
        else:
            self._rounds_since_last_bait_C += 1
        action = Action.D
        if C_others_prev >= threshold_cooperators:
            action = Action.D
        elif self._num_low_cooperation_rounds >= self.FORGIVENESS_ROUNDS:
            action = Action.D
        elif self._my_last_action == Action.D and self._rounds_since_last_bait_C >= self.BAIT_ROUNDS:
            action = Action.C
        else:
            action = Action.D
        self._my_last_action = action
        return action

class Strategy_EXPLOITATIVE_91(BaseStrategy):
    """
    The "Opportunistic Free-Rider" strategy is designed for the N-Player Public Goods Game with an exploitative mindset.
    It is adaptive, robust, and relies solely on game parameters and historical actions to maximize its own payoff.

    It initiates cooperation in the first round to probe the environment. In the final round, it defects based on
    backward induction. In intermediate rounds, it dynamically adapts its play based on the number of other players
    who cooperated in the previous round, using thresholds to categorize the level of cooperation:
    - If others are severely uncooperative (below a low threshold), it defects defensively to avoid being exploited.
    - If others are highly cooperative (above a high threshold), it defects opportunistically to free-ride.
    - If others show moderate cooperation (between thresholds), it cooperates strategically to foster the public good,
      hoping to push it towards higher cooperation levels for future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._my_last_action_for_prev_round: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        chosen_action: Action
        if current_round_0_indexed == r - 1:
            chosen_action = Action.D
        elif current_round_0_indexed == 0:
            chosen_action = Action.C
        else:
            num_others_cooperated_prev_round = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
            n_minus_1 = n - 1
            UNCOOPERATIVE_THRESHOLD = math.floor(n_minus_1 / 3)
            HIGH_COOPERATION_THRESHOLD = math.ceil(n_minus_1 * 2 / 3)
            if num_others_cooperated_prev_round <= UNCOOPERATIVE_THRESHOLD:
                chosen_action = Action.D
            elif num_others_cooperated_prev_round >= HIGH_COOPERATION_THRESHOLD:
                chosen_action = Action.D
            else:
                chosen_action = Action.C
        self._my_last_action_for_prev_round = chosen_action
        return chosen_action

class Strategy_EXPLOITATIVE_92(BaseStrategy):
    """
    The "Aggressive Conditional Exploiter" strategy for the N-Player Public Goods Game.

    This strategy attempts to free-ride on existing cooperation as much as possible,
    only contributing when cooperation has completely collapsed, with the aim of
    re-establishing a pool to exploit.

    Decision Rules:
    1. Last Round (Terminal Round, t=r): Always Defect (D).
       Rationale: No future interactions to influence, maximize immediate payoff.
    2. First Round (t=1): Always Cooperate (C).
       Rationale: A calculated investment to "seed" the public good, probing
                  others' willingness to cooperate and creating an exploitable
                  pool for future rounds.
    3. Intermediate Rounds (1 < t < r):
       Observe the total number of players who cooperated in the immediately
       preceding round.
       - IF total cooperators in previous round == 0: Cooperate (C).
         Rationale: Cooperation has completely collapsed. Re-seed the public good
                    to create future exploitation opportunities.
       - ELSE (total cooperators in previous round > 0): Defect (D).
         Rationale: Exploit existing cooperation. Maximize private payoff (1)
                    plus share of public good created by others.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        previous_round_index = current_round_0_indexed - 1
        previous_round_opponent_cooperators = sum(history.opponent_actions[previous_round_index, :])
        previous_round_my_action_was_cooperate = history.my_actions[previous_round_index]
        total_cooperators_prev_round = previous_round_opponent_cooperators + previous_round_my_action_was_cooperate
        if total_cooperators_prev_round == 0:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_93(BaseStrategy):
    """
    The 'Calculated Free-Rider' strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its individual payoff by primarily defecting
    and free-riding on the contributions of others. It strategically cooperates
    for a short duration only when the level of group cooperation falls below
    a certain threshold, in an attempt to "reignite" the public good pool,
    which it then exploits again. It always defects in the final rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        self.cooperation_reignition_threshold = max(1, int(round(n / 4)))
        self.reignition_duration = 1
        self.end_game_defection_rounds = 2
        self.current_reignition_mode = False
        self.current_reignition_round_counter = 0
        self.my_last_action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        num_other_cooperators = 0
        if current_round > 1 and history is not None:
            num_other_cooperators = sum(history.opponent_actions[-1, :])
        current_action_for_this_round = Action.D
        if current_round > total_rounds - self.end_game_defection_rounds:
            self.current_reignition_mode = False
            self.current_reignition_round_counter = 0
            current_action_for_this_round = Action.D
        elif current_round == 1:
            current_action_for_this_round = Action.D
        elif self.current_reignition_mode:
            self.current_reignition_round_counter += 1
            if self.current_reignition_round_counter <= self.reignition_duration:
                current_action_for_this_round = Action.C
            else:
                self.current_reignition_mode = False
                self.current_reignition_round_counter = 0
                current_action_for_this_round = Action.D
        elif num_other_cooperators < self.cooperation_reignition_threshold:
            self.current_reignition_mode = True
            self.current_reignition_round_counter = 1
            current_action_for_this_round = Action.C
        else:
            current_action_for_this_round = Action.D
        self.my_last_action = current_action_for_this_round
        return current_action_for_this_round

class Strategy_EXPLOITATIVE_94(BaseStrategy):
    """
    The "Strategic Opportunist" is an adaptive and exploitative strategy designed
    to maximize individual payoff in the N-Player Public Goods Game. It operates
    on the principle of defaulting to defection to maximize private gain, but
    strategically cooperating as a temporary "investment" when overall cooperation
    levels drop too low, threatening future opportunities for exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        threshold_calculation = (n - 1) * (k / n)
        self.COOPERATION_THRESHOLD = max(1, math.floor(threshold_calculation))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_t == total_rounds:
            return Action.D
        if current_round_t == 1:
            return Action.D
        last_round_my_action_is_C = history.my_actions[state.round_number - 1]
        last_round_opponent_actions_is_C = history.opponent_actions[state.round_number - 1, :]
        total_cooperators_prev_round = int(last_round_my_action_is_C) + np.sum(last_round_opponent_actions_is_C)
        cooperation_from_others_prev = total_cooperators_prev_round - (1 if last_round_my_action_is_C else 0)
        if last_round_my_action_is_C == False:
            if cooperation_from_others_prev >= self.COOPERATION_THRESHOLD:
                return Action.D
            else:
                return Action.C
        elif cooperation_from_others_prev >= self.COOPERATION_THRESHOLD:
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_511(BaseStrategy):
    """
    The Adaptive Collective Reciprocity (ACR) strategy is a conditional cooperation strategy
    for the N-Player Public Goods Game. It starts optimistically, defects in the final round,
    and in intermediate rounds, decides based on a dynamically calculated "Cooperation Sustainability Threshold (CST)"
    and the observed level of cooperation in the previous round. This strategy aims to promote
    and sustain cooperation by being adaptive to observed behavior and robust across various
    game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.CST = max(1, math.floor(self.n - (self.k - 1)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.r - 1:
            return Action.D
        previous_round_index = current_round - 1
        my_previous_action_cooperated = int(history.my_actions[previous_round_index])
        opponent_previous_actions_cooperated = np.sum(history.opponent_actions[previous_round_index, :])
        m_prev = my_previous_action_cooperated + opponent_previous_actions_cooperated
        if m_prev >= self.CST:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_95(BaseStrategy):
    """
    The Opportunistic Predator strategy for the N-Player Public Goods Game.
    This strategy aims to maximize personal payoff by encouraging and then free-riding on others' contributions.
    It employs a calculated "bait and switch" mechanism, only contributing when necessary to foster a more exploitable environment,
    and always defaults to defection when individually rational or in the absence of future consequences.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with the game's parameters.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action for the current round based on the strategy's rules.

        Args:
            state (GameState): The current state of the game, including the round number.
            history (None | PlayerHistory): Past actions and payoffs, or None if it's the first round.

        Returns:
            Action: The chosen action (C for Cooperate, D for Defect).
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == n_rounds - 1:
            return Action.D
        my_action_last_round_was_C = history.my_actions[-1]
        others_cooperated_last_round = sum(history.opponent_actions[-1, :])
        if others_cooperated_last_round == 0:
            return Action.D
        if my_action_last_round_was_C:
            return Action.D
        else:
            cooperation_threshold = math.floor((n_players - 1) / 2)
            if others_cooperated_last_round >= cooperation_threshold:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_96(BaseStrategy):
    """
    The "Grudging Free-Rider" (GFR) strategy aims to maximize its own total payoff
    by encouraging others to cooperate while strategically defecting to free-ride.
    It cooperates initially to probe, exploits when possible, conditionally
    re-seeds cooperation if drops are too severe (early/mid-game), and defects
    aggressively in the end-game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.CIT = 0.3
        self.DET_base = 0.3
        self.DET_ramp = 0.5
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        DET = self.DET_base + current_round_1_indexed / self.n_rounds * self.DET_ramp
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            my_action_prev_was_C = history.my_actions[-1]
            cooperators_others_prev = sum(history.opponent_actions[-1, :])
            n_others = self.n_players - 1
            cooperation_ratio_others = cooperators_others_prev / n_others
            if my_action_prev_was_C:
                if cooperation_ratio_others >= self.CIT:
                    return Action.C
                else:
                    return Action.D
            elif cooperation_ratio_others >= DET:
                return Action.D
            elif current_round_1_indexed < self.n_rounds / 2:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_97(BaseStrategy):
    """
    The Unwavering Predator strategy for the N-Player Public Goods Game.

    This strategy is founded on the unshakeable principle of individual rationality
    and the exploitation of the dominant strategy. It consistently prioritizes
    its own payoff by always choosing to Defect (D), regardless of the observed
    actions of other players or the current round number. Its "adaptiveness" and
    "robustness" lie in its ability to leverage the fundamental game mechanics
    (Defect being a strictly dominant strategy) to maximize utility across a wide
    spectrum of opponent behaviors, without making costly sacrifices or relying on
    uncertain reciprocation.

    It performs optimally against purely rational adversaries and effectively exploits
    deviations from rationality (i.e., cooperation) by others, without risking its
    own position.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        return Action.D

class Strategy_EXPLOITATIVE_98(BaseStrategy):
    """
    The Strategic Parasite strategy is designed to be highly exploitative by default,
    maximizing personal gain through free-riding. It only deviates from defection
    as a calculated measure to prevent the complete collapse of cooperation among
    other players, thereby preserving the public good pool for future exploitation.
    It is adaptive, responding to observed behavior, and robust against a range of
    opponent strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_others_cooperated_prev_round = sum(history.opponent_actions[-1, :])
        cooperation_threshold = math.floor((self.n_players - 1) / 2)
        if num_others_cooperated_prev_round < cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_99(BaseStrategy):
    """
    The "Calculated Altruist for Exploitation" (CAFE) strategy aims to maximize its own payoff
    by defaulting to defection (exploitation) whenever possible. It strategically cooperates
    in specific situations to "re-prime the pump" of public good contributions, preventing
    a complete collapse into mutual defection where its own payoff would be minimal. It also
    learns to identify and cease investing in truly unresponsive opponents.

    Key Concepts:
    1.  Exploitation Default: Defects when public good is sufficiently high.
    2.  Calculated Altruism: Cooperates if public good drops too low, to encourage others.
    3.  No Sucker Rule: Stops calculated altruism if all other players are permanent defectors.
    4.  Backward Induction: Defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.CTT = math.ceil(self.n / self.k)
        self.has_cooperated_self = False
        self.has_cooperated_opponents = [False] * (self.n - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number > 0:
            prev_round_idx = state.round_number - 1
            if history.my_actions[prev_round_idx] == Action.C.value:
                self.has_cooperated_self = True
            for i, opponent_action in enumerate(history.opponent_actions[prev_round_idx, :]):
                if opponent_action == Action.C.value:
                    self.has_cooperated_opponents[i] = True
        if current_round_1_indexed == 1:
            self.has_cooperated_self = True
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        else:
            prev_round_idx = state.round_number - 1
            my_prev_action_as_int = int(history.my_actions[prev_round_idx])
            total_opponent_cooperators_last = np.sum(history.opponent_actions[prev_round_idx, :])
            C_total_last = my_prev_action_as_int + total_opponent_cooperators_last
            if C_total_last < self.CTT:
                other_permanent_defectors_count = 0
                for cooperated_status in self.has_cooperated_opponents:
                    if not cooperated_status:
                        other_permanent_defectors_count += 1
                if other_permanent_defectors_count == self.n - 1:
                    return Action.D
                else:
                    self.has_cooperated_self = True
                    return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_100(BaseStrategy):
    """
    Calculated Opportunist: This strategy aims to maximize its own payoff by primarily free-riding
    on others' contributions. It strategically cooperates only when necessary to prevent the
    complete collapse of the public good, thereby creating opportunities for future exploitation.
    It promptly reverts to defection once a viable level of cooperation is established or maintained,
    and defects aggressively towards the end of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.T_end_rounds = max(1, min(5, self.n_rounds - 1))
        self.Minimum_Viable_Cooperation = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num_1_indexed = state.round_number + 1
        if self.n_rounds == 1:
            return Action.D
        if current_round_num_1_indexed > self.n_rounds - self.T_end_rounds:
            return Action.D
        my_last_action = Action.D
        C_prev_round = 0
        if state.round_number > 0:
            my_last_action = Action.C if history.my_actions[-1] else Action.D
            C_prev_round = sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        if my_last_action == Action.C:
            return Action.D
        elif C_prev_round < self.Minimum_Viable_Cooperation:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_101(BaseStrategy):
    """
    The Adaptive Opportunist (AO) strategy for the N-Player Public Goods Game.

    This strategy is designed to be highly exploitative, adaptive, and robust to
    various opponent behaviors. It prioritizes free-riding on others' cooperation
    and only contributes when absolutely necessary to potentially restart a collapsed
    public good, quickly abandoning such efforts if not reciprocated.

    Core Philosophy: Maximize own payoff by free-riding on the public good whenever possible.
    Only initiate cooperation as a short-term, low-cost attempt to "seed" the public good
    when it has entirely collapsed, and swiftly revert to defection if this attempt fails.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_previous_action: Action | None = None
        self.permanent_defection_mode: bool = False
        self.induction_patience_limit: int = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds_1_indexed = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            self.my_previous_action = Action.D
            self.permanent_defection_mode = False
            return Action.D
        if current_round_1_indexed == total_rounds_1_indexed:
            self.my_previous_action = Action.D
            return Action.D
        others_cooperated_in_previous_round = sum(history.opponent_actions[-1, :])
        if self.permanent_defection_mode:
            self.my_previous_action = Action.D
            return Action.D
        elif others_cooperated_in_previous_round >= 1:
            self.my_previous_action = Action.D
            return Action.D
        elif self.my_previous_action == Action.C:
            self.permanent_defection_mode = True
            self.my_previous_action = Action.D
            return Action.D
        else:
            self.my_previous_action = Action.C
            return Action.C

class Strategy_EXPLOITATIVE_102(BaseStrategy):
    """
    Adaptive Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize personal payoff by initially cooperating to
    encourage others, then free-riding on their contributions when sufficient
    cooperation is observed. It defends against exploitation by defecting when
    others fail to cooperate, and applies backward induction in the final rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.free_ride_threshold = max(1, round((self.n - 1) * (self.k / self.n)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number >= self.r - 2:
            return Action.D
        num_other_cooperators_t_minus_1 = sum(history.opponent_actions[-1, :])
        if num_other_cooperators_t_minus_1 >= self.free_ride_threshold:
            return Action.D
        elif num_other_cooperators_t_minus_1 == 0:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_103(BaseStrategy):
    """
    Exploitative Adaptive Reciprocator (EAR) strategy for the N-Player Public Goods Game.
    This strategy aims to cultivate cooperation among other players, then exploit it by free-riding.
    It protects itself from being a "sucker" by retaliating when cooperation levels are too low,
    and relentlessly exploits in ideal conditions. It uses a dynamic approach based on the observed
    collective cooperation of others in the immediately preceding round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.n_rounds - 1:
            return Action.D
        if current_round == 0:
            return Action.C
        else:
            c_others_t_minus_1 = np.sum(history.opponent_actions[-1, :])
            min_cooperators_to_exploit = self.n_players - 1
            min_cooperators_to_stay_cooperative = math.floor(self.n_players * 0.25)
            if c_others_t_minus_1 >= min_cooperators_to_exploit:
                return Action.D
            elif c_others_t_minus_1 <= min_cooperators_to_stay_cooperative:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_104(BaseStrategy):
    """
    Bait-and-Defect (BnD) is an exploitative, adaptive, and robust strategy
    for the N-Player Public Goods Game. It aims to maximize its own payoff by
    strategically leveraging temporary cooperation ("bait") to encourage others,
    only to exploit them ("defect") once cooperation is established. If baiting
    fails or cooperation from others drops too low, it reverts to defection.
    In the final rounds, all future-influencing incentives disappear, so it
    always defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.P_bait: int = 1
        self.cooperation_threshold: float = 0.4
        self.P_endgame: int = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0based = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_0based >= total_rounds - self.P_endgame:
            return Action.D
        if current_round_0based < self.P_bait:
            return Action.C
        previous_round_opponent_actions = history.opponent_actions[-1, :]
        num_other_cooperators_prev_round = np.sum(previous_round_opponent_actions)
        num_other_players = n_players - 1
        cooperation_rate_others = num_other_cooperators_prev_round / num_other_players
        if cooperation_rate_others >= self.cooperation_threshold:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_105(BaseStrategy):
    """
    The "Adaptive Exploiter" strategy aims to maximize individual payoff in the N-Player Public Goods Game
    by strategically leveraging the cooperation of others. It combines an initial cooperative "investment"
    to encourage participation, an opportunistic free-riding mechanism when cooperation is high,
    a self-preservation rule to avoid being the sole cooperator, and a terminal exploitation phase.
    It is adaptive to the observed behavior of other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.total_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.total_rounds - 1:
            return Action.D
        c_others_prev = np.sum(history.opponent_actions[-1, :])
        threshold_high_coop = (self.n_players - 1) / 2.0
        if c_others_prev >= threshold_high_coop:
            return Action.D
        elif c_others_prev == 0:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_106(BaseStrategy):
    """
    The "Opportunistic Re-Seeder" (ORS) strategy prioritizes individual payoff maximization
    by free-riding on others' contributions and strategically intervening (cooperating)
    only when cooperation levels fall below a critical threshold or at the very start
    to "bait" others. It defects consistently in the final two rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n_players = self.game_description.n_players
        k_factor = self.game_description.k
        self.cooperation_floor_threshold = max(1, math.floor(n_players * (1 - k_factor / n_players)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1 or current_round_0_indexed == total_rounds - 2:
            return Action.D
        total_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if total_cooperators_prev_round < self.cooperation_floor_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_107(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize individual payoff in the
    N-Player Public Goods Game by strategically free-riding when cooperation is high,
    punishing widespread defection, and contributing to maintain a baseline of cooperation.
    It always defects in the final round to capitalize on the end-game effect.
    """
    EXPLORING = 'EXPLORING'
    EXPLOITING = 'EXPLOITING'
    PUNISHING = 'PUNISHING'

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r_total = game_description.n_rounds
        self.history_cooperators_count = []
        self.state = self.EXPLORING
        self.cooperation_floor_threshold = max(1, math.floor(self.n * 0.25))
        self.exploitation_ceiling_threshold = max(1, math.ceil(self.n * 0.5))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number > 0:
            my_prev_action_val = 1 if history.my_actions[-1] else 0
            opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
            C_prev = my_prev_action_val + opponent_cooperators_prev
            self.history_cooperators_count.append(C_prev)
        else:
            C_prev = -1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r_total:
            return Action.D
        if self.state == self.EXPLORING:
            if C_prev < self.cooperation_floor_threshold:
                self.state = self.PUNISHING
                return Action.D
            elif C_prev >= self.exploitation_ceiling_threshold:
                self.state = self.EXPLOITING
                return Action.D
            else:
                return Action.C
        elif self.state == self.EXPLOITING:
            if C_prev < self.cooperation_floor_threshold:
                self.state = self.PUNISHING
                return Action.D
            elif C_prev >= self.exploitation_ceiling_threshold:
                return Action.D
            else:
                self.state = self.EXPLORING
                return Action.C
        elif self.state == self.PUNISHING:
            if C_prev < self.exploitation_ceiling_threshold:
                return Action.D
            else:
                self.state = self.EXPLORING
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_108(BaseStrategy):
    """
    The "Calculated Manipulator" strategy is designed to be highly exploitative by free-riding on others' cooperation,
    while also being adaptive and robust enough to prevent the total collapse of the public good (which would reduce
    its exploitation opportunities). It aims to extract maximum value by defecting whenever a sufficient public good
    exists, and only cooperates strategically to "bail out" a faltering public good, treating it as an investment
    for future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.consecutive_unhealthy_rounds: int = 0
        self.cooperation_sustain_threshold: int = max(1, math.floor((self.n - 1) / 2))
        self.bail_out_attempts_before_giving_up: int = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        previous_round_idx = state.round_number - 1
        cooperators_among_others_last_round = sum(history.opponent_actions[previous_round_idx, :])
        if cooperators_among_others_last_round >= self.cooperation_sustain_threshold:
            self.consecutive_unhealthy_rounds = 0
            return Action.D
        else:
            self.consecutive_unhealthy_rounds += 1
            if self.consecutive_unhealthy_rounds <= self.bail_out_attempts_before_giving_up:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_109(BaseStrategy):
    """
    Adaptive Exploitative Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by free-riding on others' contributions.
    It operates in three phases:
    1. Initial Probing: Cooperates in the first round to signal willingness and gauge
       initial cooperation levels from other players.
    2. Adaptive Intermediate: In subsequent rounds, it monitors the cooperation level
       of other players from the previous round.
       - If sufficient cooperation is observed (above `REQUIRED_COOP_THRESHOLD`),
         the strategy defects to free-ride and maximize its immediate payoff.
       - If cooperation is insufficient, the strategy will conditionally cooperate
         for a limited number of `MAX_COOP_ATTEMPTS` to try and re-induce cooperation.
         If these attempts fail, it switches to defection to protect its own payoff
         and avoid being a "sucker".
    3. Final Exploitative: Defects in the last round due to backward induction, as
       there are no future rounds to influence.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        self.required_coop_threshold = math.ceil((n - 1) * (k - 1) / k)
        self.max_coop_attempts = 2
        self.consecutive_unsuccessful_coop_attempts = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == self.game_description.n_rounds:
            return Action.D
        c_others_prev = sum(history.opponent_actions[-1, :])
        if c_others_prev >= self.required_coop_threshold:
            self.consecutive_unsuccessful_coop_attempts = 0
            return Action.D
        elif self.consecutive_unsuccessful_coop_attempts < self.max_coop_attempts:
            self.consecutive_unsuccessful_coop_attempts += 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_110(BaseStrategy):
    """
    The "Adaptive Trigger-Exploiter" strategy is designed to maximize its own payoff in the N-Player Public Goods Game
    by strategically oscillating between cooperation and defection. It aims to free-ride on others' contributions
    when group cooperation is high, punish when it's low, and explore/re-establish cooperation in moderate situations.
    It also exploits the endgame by defecting in the final rounds, leveraging backward induction.
    """
    MODE_EXPLORE = 'EXPLORE'
    MODE_EXPLOIT = 'EXPLOIT'
    MODE_PUNISH = 'PUNISH'

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._my_mode = self.MODE_EXPLORE
        self._rounds_since_last_cooperation = 0
        n_players = self.game_description.n_players
        self._cooperation_threshold_high_exploitation = math.ceil(n_players * 0.7)
        self._cooperation_threshold_low_punishment = math.ceil(n_players * 0.3)
        self._forgiveness_rounds = 3
        self._endgame_horizon = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        action_for_this_round = Action.D
        if current_round > total_rounds - self._endgame_horizon:
            action_for_this_round = Action.D
        elif current_round == 1:
            self._my_mode = self.MODE_EXPLORE
            self._rounds_since_last_cooperation = 0
            action_for_this_round = Action.C
        else:
            C_prev = history.opponent_actions[-1, :].sum() + int(history.my_actions[-1])
            if self._my_mode == self.MODE_EXPLORE:
                if C_prev >= self._cooperation_threshold_high_exploitation:
                    self._my_mode = self.MODE_EXPLOIT
                    action_for_this_round = Action.D
                elif C_prev < self._cooperation_threshold_low_punishment:
                    self._my_mode = self.MODE_PUNISH
                    action_for_this_round = Action.D
                else:
                    action_for_this_round = Action.C
            elif self._my_mode == self.MODE_EXPLOIT:
                if C_prev >= self._cooperation_threshold_high_exploitation:
                    action_for_this_round = Action.D
                elif C_prev < self._cooperation_threshold_low_punishment:
                    self._my_mode = self.MODE_PUNISH
                    action_for_this_round = Action.D
                else:
                    self._my_mode = self.MODE_EXPLORE
                    action_for_this_round = Action.C
            elif self._my_mode == self.MODE_PUNISH:
                if self._rounds_since_last_cooperation >= self._forgiveness_rounds:
                    self._my_mode = self.MODE_EXPLORE
                    action_for_this_round = Action.C
                else:
                    action_for_this_round = Action.D
        if action_for_this_round == Action.C:
            self._rounds_since_last_cooperation = 0
        else:
            self._rounds_since_last_cooperation += 1
        return action_for_this_round

class Strategy_EXPLOITATIVE_111(BaseStrategy):
    """
    Reputation-Seeking Opportunist (RSO) strategy for the N-Player Public Goods Game.

    Core Philosophy:
    The RSO strategy aims to maximize its personal payoff by aggressively free-riding
    on other players' contributions whenever a sufficient public good is present.
    It strategically initiates cooperation only in the initial round to "probe"
    for reciprocal behavior and builds a temporary "reputation" for cooperation.
    After this initial probe, it rapidly shifts to defection. It only re-engages
    in cooperation if its defection causes the collective cooperation level to drop
    below a critical threshold, thereby threatening its own ability to free-ride profitably.
    In such cases, it "invests" in a round of cooperation to attempt to re-induce
    others to contribute, only to resume defection once the opportunity arises.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_probe_rounds = 1
        n = self.game_description.n_players
        k = self.game_description.k
        self.low_cooperation_threshold = max(1, math.floor((n - 1) / k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed < self.initial_probe_rounds:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        my_action_prev_round_bool = history.my_actions[-1]
        c_others_prev_round = np.sum(history.opponent_actions[-1, :])
        if my_action_prev_round_bool is True:
            return Action.D
        elif c_others_prev_round <= self.low_cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_112(BaseStrategy):
    """
    Adaptive Exploiter (AE) strategy for the N-Player Public Goods Game.

    This strategy aims to "prime the pump" of cooperation in early/low-cooperation stages
    and then ruthlessly free-ride when a sufficient level of cooperation has been established.
    It carefully balances the need to maintain a cooperative environment with the desire
    to maximize individual gain through defection, adhering to backward induction in the
    final round.

    Decision Rules:
    1. Initial Round (Round 1 / state.round_number == 0): Cooperate (C) to signal cooperation.
    2. Last Round (Round r / state.round_number == self.n_rounds - 1): Defect (D) due to backward induction.
    3. Adaptive Rounds (Rounds 2 to r-1):
       - Monitor C_prev: total cooperators in the previous round.
       - If C_prev == 0: Defect (D). No public good, avoid being a "sucker."
       - Else if C_prev >= k: Defect (D). Sufficient public good, free-ride.
       - Else (0 < C_prev < k): Cooperate (C). Prime the pump to encourage more cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        my_prev_action_bool = history.my_actions[-1]
        opponent_prev_actions_bool = history.opponent_actions[-1, :]
        C_prev = int(my_prev_action_bool) + np.sum(opponent_prev_actions_bool)
        if C_prev == 0:
            return Action.D
        elif C_prev >= self.k:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_113(BaseStrategy):
    """
    The "Opportunistic Catalyst" strategy is designed for the N-Player Public Goods Game.
    It operates on the principle of maximizing individual gain by defecting whenever possible,
    while making minimal, strategic contributions (cooperating) only when necessary to prevent
    the public good from collapsing to a point where there's nothing left to exploit.

    The strategy defects by default, acting as a free-rider. However, if the total number
    of cooperators in the previous round falls below a calculated `Cooperation_Threshold (T_C)`,
    it temporarily "catalyzes" cooperation by contributing itself for `Catalyze_Rounds (R_C)`
    (which is set to 1 for this strategy). This is a calculated investment to encourage
    other players to resume or increase their contributions, after which the strategy
    immediately reverts to its default exploitative defection.

    It always defects in the very first round to probe the environment and in the final
    round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        self.T_C = max(1, math.floor(n / k))
        self.R_C = 1
        self.consecutive_catalyze_rounds_left = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == total_rounds - 1:
            self.consecutive_catalyze_rounds_left = 0
            return Action.D
        if self.consecutive_catalyze_rounds_left > 0:
            self.consecutive_catalyze_rounds_left -= 1
            return Action.C
        elif current_round_0_indexed == 0:
            return Action.D
        else:
            total_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if total_cooperators_prev_round < self.T_C:
                self.consecutive_catalyze_rounds_left = self.R_C - 1
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_114(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the N-Player Public Goods Game.
    This strategy aims to primarily defect to free-ride, but strategically
    cooperates when collective cooperation drops too low to "re-seed" it,
    then resumes free-riding. It is designed to be adaptive and exploitative.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploitation_mode: bool = True
        self.punishment_rounds_left: int = 0
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.initial_coop_rounds: int = 1
        self.endgame_rounds: int = 1
        self.c_cooperation_trigger: int = max(1, math.ceil(self.n_players / self.k_factor))
        self.punishment_duration: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed <= self.initial_coop_rounds:
            return Action.C
        if current_round_1_indexed > self.n_rounds - self.endgame_rounds:
            return Action.D
        prev_round_idx = state.round_number - 1
        num_others_cooperated_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        if self.punishment_rounds_left > 0:
            self.punishment_rounds_left -= 1
            self.exploitation_mode = False
        elif num_others_cooperated_prev_round < self.c_cooperation_trigger:
            self.exploitation_mode = False
            self.punishment_rounds_left = self.punishment_duration - 1
        else:
            self.exploitation_mode = True
        if self.exploitation_mode:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_115(BaseStrategy):
    """
    The Opportunistic Grudger strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by encouraging a general level of
    cooperation among opponents, then strategically free-riding on that cooperation.
    It is initially cooperative to test the waters, but quickly becomes "grudging"
    if cooperation is low and "opportunistic" when cooperation is high.
    It aggressively exploits the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.total_rounds = game_description.n_rounds
        self.L = min(3, self.total_rounds - 1)
        self.COOP_THRESHOLD = self.n_players // 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if state.round_number >= self.total_rounds - self.L:
            return Action.D
        n_C_prev = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if n_C_prev >= self.n_players - 1:
            return Action.D
        elif n_C_prev >= self.COOP_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_116(BaseStrategy):
    """
    The Aggressive Opportunist strategy for the N-Player Public Goods Game.
    This strategy prioritizes immediate self-interest by defaulting to defection.
    It only cooperates in two specific, calculated scenarios:
    1. To strategically "lure" other players into cooperating when a critical mass of cooperation is absent.
    2. During a short test phase (lure duration).
    Any cooperation is short-lived and immediately followed by a return to defection, aiming to capture the
    benefits of others' contributions. It rigorously applies backward induction in the final rounds to
    maximize terminal gains.
    """
    EXPLOIT_THRESHOLD = 1
    LURE_TRIGGER = 1
    LURE_DURATION = 1
    ENDGAME_HORIZON = 2

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Aggressive Opportunist strategy with game parameters and internal state.

        Args:
            game_description (PublicGoodsDescription): An object containing game parameters
                                                     (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.luring_in_progress = False
        self.lure_rounds_left = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides whether to Cooperate ('C') or Defect ('D') for the current round.

        Args:
            state (GameState): An object containing the current round number (0-indexed).
            history (None | PlayerHistory): An object containing past actions and payoffs.
                                           Is None for the very first round (round_number 0).

        Returns:
            Action: Action.C for Cooperate, Action.D for Defect.
        """
        total_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.D
        if state.round_number >= total_rounds - self.ENDGAME_HORIZON:
            return Action.D
        if self.luring_in_progress:
            self.lure_rounds_left -= 1
            if self.lure_rounds_left > 0:
                return Action.C
            else:
                self.luring_in_progress = False
                return Action.D
        else:
            num_others_cooperated_prev_round = sum(history.opponent_actions[-1, :])
            if num_others_cooperated_prev_round >= self.EXPLOIT_THRESHOLD:
                return Action.D
            elif num_others_cooperated_prev_round < self.LURE_TRIGGER:
                self.luring_in_progress = True
                self.lure_rounds_left = self.LURE_DURATION
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_117(BaseStrategy):
    """
    The Adaptive Opportunist strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by probing for cooperation,
    inducing it through punishment, exploiting it when present, and repairing
    cooperation levels when they drop too low, finally defecting in the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.C_TARGET_PERCENTAGE = 0.7
        self.SWITCH_ROUNDS_INITIAL = min(max(1, math.floor(self.r / 4)), 5)
        self.FORGIVENESS_ROUNDS = 1
        self.consecutive_low_cooperation_rounds = 0
        self.current_mode = 'INDUCING'
        self.C_target_threshold = round((self.n - 1) * self.C_TARGET_PERCENTAGE)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number == 0:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        C_prev = sum(history.opponent_actions[-1, :])
        if current_round_1_indexed > self.SWITCH_ROUNDS_INITIAL:
            if C_prev >= self.C_target_threshold:
                self.consecutive_low_cooperation_rounds = 0
                self.current_mode = 'EXPLOITING'
            else:
                self.consecutive_low_cooperation_rounds += 1
                if self.consecutive_low_cooperation_rounds > self.FORGIVENESS_ROUNDS:
                    self.current_mode = 'INDUCING'
                    self.consecutive_low_cooperation_rounds = 0
        if self.current_mode == 'INDUCING' or current_round_1_indexed <= self.SWITCH_ROUNDS_INITIAL:
            if C_prev >= self.C_target_threshold:
                return Action.C
            else:
                return Action.D
        elif C_prev >= self.C_target_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_118(BaseStrategy):
    """
    Adaptive Opportunistic Free-Rider (AOFR) strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own total payoff by aggressively free-riding on
    other players' contributions. It strategically attempts to re-seed cooperation
    only when the game is stuck in a universal defection state, ensuring that it avoids
    being a persistent "sucker" and has sufficient future rounds to exploit any
    re-established cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.reseeding_attempted_in_current_defection_streak = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            self.reseeding_attempted_in_current_defection_streak = False
            return Action.C
        if state.round_number == total_rounds - 1:
            self.reseeding_attempted_in_current_defection_streak = False
            return Action.D
        C_others_prev = np.sum(history.opponent_actions[-1, :])
        My_action_prev_is_C = history.my_actions[-1]
        if C_others_prev > 0:
            self.reseeding_attempted_in_current_defection_streak = False
            return Action.D
        elif My_action_prev_is_C:
            self.reseeding_attempted_in_current_defection_streak = False
            return Action.D
        elif not self.reseeding_attempted_in_current_defection_streak and current_round_1_indexed < total_rounds - 1:
            self.reseeding_attempted_in_current_defection_streak = True
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_119(BaseStrategy):
    """
    The Calculated Opportunist's primary objective is to maximize its own cumulative payoff.
    Its default action is to Defect (D). However, it recognizes that a complete collapse
    of cooperation (leading to an all-defect outcome where everyone gets 1) might be
    suboptimal if the public good multiplier k is high. Therefore, it strategically
    initiates short "probe" phases of cooperation to "re-seed" or "bait" others into
    cooperating, creating new opportunities for free-riding.

    This strategy dynamically calculates a 'probe_threshold' based on game parameters n and k.
    It defects by default, but if the observed cooperation level in the previous round
    falls below this threshold, it initiates a single-round probe of cooperation to
    attempt to restore the overall cooperation level, thus creating new free-riding opportunities.
    It always defects in the first two rounds and the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.probe_rounds_left = 0
        self.probe_duration = 1
        numerator = self.n * (self.k - 1)
        denominator = self.n - 1
        self.probe_threshold = max(0, math.floor(numerator / denominator))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if self.probe_rounds_left > 0:
            self.probe_rounds_left -= 1
        if current_round_1_indexed == self.r:
            self.probe_rounds_left = 0
            return Action.D
        if self.probe_rounds_left > 0:
            return Action.C
        if current_round_1_indexed <= 2:
            return Action.D
        c_prev_round = np.sum(history.opponent_actions[-1, :])
        if history.my_actions[-1]:
            c_prev_round += 1
        if c_prev_round <= self.probe_threshold:
            self.probe_rounds_left = self.probe_duration
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_120(BaseStrategy):
    """
    The "Adaptive Free-Rider with End-Game Pounce" strategy is designed to be highly
    exploitative. It maximizes its own payoff by leveraging any existing cooperation
    while minimizing its own contributions. It combines a cautious initial probe
    with aggressive free-riding and a ruthless end-game defection, making it
    robust against a variety of opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.end_game_rounds = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if total_rounds <= self.end_game_rounds:
            return Action.D
        if current_round_0_indexed >= total_rounds - self.end_game_rounds:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_121(BaseStrategy):
    """
    The "Calculated Free-Rider" strategy is designed to be exploitative and adaptive.
    Its primary goal is to maximize its own cumulative payoff by generally free-riding,
    only cooperating when the collective cooperation level drops too low, threatening
    the long-term profitability of free-riding.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Calculated Free-Rider strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.C_min_threshold = math.floor(self.n / self.k)
        self.current_mode = 'FREE_RIDE'

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round based on the strategy rules.

        Args:
            state: An object containing the current game state, including the round number.
                   (state.round_number is 0-indexed)
            history: An object containing records of past actions and payoffs. None for the first round.

        Returns:
            Action: The chosen action (Action.C for Cooperate, Action.D for Defect).
        """
        if state.round_number == 0:
            self.current_mode = 'FREE_RIDE'
            return Action.D
        if state.round_number == self.r - 1:
            return Action.D
        total_cooperators_previous_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if self.current_mode == 'FREE_RIDE':
            if total_cooperators_previous_round > self.C_min_threshold:
                return Action.D
            else:
                self.current_mode = 'BAIL_OUT'
                return Action.C
        else:
            self.current_mode = 'FREE_RIDE'
            return Action.D

class Strategy_EXPLOITATIVE_122(BaseStrategy):
    """
    The Ironclad Free-Rider strategy.

    This strategy is purely self-interested and aims to maximize its own
    cumulative payoff by always choosing to Defect (D) in every round of the game.
    The rationale is based on the payoff structure of the N-Player Public Goods Game
    where, given the parameters 1 < k < n, defecting is the dominant strategy
    for any individual player in any single round. This means choosing to defect
    always yields a higher payoff for the player, regardless of how many other
    players choose to cooperate.

    By consistently defecting, this strategy free-rides on any contributions
    to the public good made by other players, exploiting their cooperation
    without reciprocating. It is robust because its optimal action (D) does
    not depend on past history or predictions of opponent behavior, and it
    is adaptive in that it maximally exploits any other strategy that
    deviates from pure defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        return Action.D

class Strategy_EXPLOITATIVE_123(BaseStrategy):
    """
    The Strategic Parasite strategy for the N-Player Public Goods Game.
    This strategy prioritizes individual payoff by defaulting to defection,
    only cooperating as a calculated, temporary investment to prevent a
    total collapse of the public good. It quickly reverts to defection
    when it perceives opportunities for free-riding or if its attempts
    to induce cooperation are deemed futile.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_attempt_rounds: list[int] = []
        self.CIT_FACTOR = 0.25
        self.FRT_FACTOR = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_0_indexed == 0:
            self.cooperation_attempt_rounds.clear()
            return Action.D
        if current_round_0_indexed == r - 1:
            return Action.D
        prev_round_0_indexed = current_round_0_indexed - 1
        my_last_action: bool = history.my_actions[prev_round_0_indexed]
        opponents_last_actions: NDArray[np.bool_] = history.opponent_actions[prev_round_0_indexed, :]
        C_prev_total = int(my_last_action) + np.sum(opponents_last_actions)
        CIT = max(1, math.floor(n * self.CIT_FACTOR))
        FRT = math.floor(n * self.FRT_FACTOR)
        action_to_take = Action.D
        cooperation_was_futile_last_round = my_last_action is True and C_prev_total < CIT
        if C_prev_total < CIT:
            if cooperation_was_futile_last_round:
                action_to_take = Action.D
                self.cooperation_attempt_rounds.clear()
            else:
                action_to_take = Action.C
                self.cooperation_attempt_rounds.append(current_round_1_indexed)
        else:
            others_C_prev = C_prev_total - (1 if my_last_action is True else 0)
            if others_C_prev >= FRT:
                action_to_take = Action.D
                self.cooperation_attempt_rounds.clear()
            else:
                action_to_take = Action.D
                self.cooperation_attempt_rounds.clear()
        return action_to_take

class Strategy_EXPLOITATIVE_124(BaseStrategy):
    """
    The Pragmatic Leech strategy for the N-Player Public Goods Game.

    Core Philosophy: Primarily defect to maximize immediate gain, as defection is the
    dominant strategy in any single round. Only cooperate as a calculated "lure" to
    initiate cooperation from others when the overall game state is highly
    uncooperative, with the sole purpose of exploiting that induced cooperation
    immediately afterward. Always defect in endgame rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.initial_defect_rounds = 1
        self.r_endgame_start = self.n_rounds
        self.r_probe_interval = max(3, self.n_rounds // 5)
        self.cooperation_lure_threshold = 0.2
        self.last_probe_round = -float('inf')
        self.total_others_cooperation_count = 0
        self.total_other_player_actions_observed = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        num_others_cooperating_prev_round = 0
        if history is not None:
            prev_round_opponent_actions = history.opponent_actions[-1, :]
            num_others_cooperating_prev_round = np.sum(prev_round_opponent_actions)
            num_opponents = len(prev_round_opponent_actions)
            self.total_others_cooperation_count += num_others_cooperating_prev_round
            self.total_other_player_actions_observed += num_opponents
        if current_round_1_indexed >= self.r_endgame_start:
            return Action.D
        if current_round_1_indexed <= self.initial_defect_rounds:
            return Action.D
        if num_others_cooperating_prev_round >= 1:
            return Action.D
        else:
            avg_C_others_overall = 0.0
            if self.total_other_player_actions_observed > 0:
                avg_C_others_overall = self.total_others_cooperation_count / self.total_other_player_actions_observed
            time_since_last_probe = current_round_1_indexed - self.last_probe_round
            cond1 = time_since_last_probe >= self.r_probe_interval
            cond2 = avg_C_others_overall < self.cooperation_lure_threshold
            cond3 = current_round_1_indexed < self.n_rounds - 1
            if cond1 and cond2 and cond3:
                self.last_probe_round = current_round_1_indexed
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_125(BaseStrategy):
    """
    The Strategic Free-Rider (SFR) for N-Player Public Goods Game.
    Aims to maximize its own total payoff by primarily free-riding on others' contributions,
    only temporarily cooperating to revive a collapsing public good when strategically advantageous.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.COOPERATION_REVIVAL_THRESHOLD = math.ceil(0.5 * self.n / self.k)
        self.BAIT_ROUNDS_DURATION = 1
        self.in_baiting_phase = False
        self.bait_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.r:
            self.in_baiting_phase = False
            self.bait_counter = 0
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        num_cooperators_last_round_total = int(sum(history.opponent_actions[-1, :])) + (1 if history.my_actions[-1] else 0)
        my_action_last_round_was_C = history.my_actions[-1]
        if self.in_baiting_phase:
            if self.bait_counter > 0:
                self.bait_counter -= 1
                return Action.C
            else:
                self.in_baiting_phase = False
        if num_cooperators_last_round_total < self.COOPERATION_REVIVAL_THRESHOLD:
            if self.r - current_round_1_indexed >= self.BAIT_ROUNDS_DURATION + 1:
                if not my_action_last_round_was_C:
                    self.in_baiting_phase = True
                    self.bait_counter = self.BAIT_ROUNDS_DURATION - 1
                    return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_126(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy for an N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by free-riding on others' cooperation,
    while strategically contributing only when necessary to prevent the complete collapse
    of cooperation, thus preserving the opportunity for future exploitation. It is
    designed to be highly exploitative, adaptive, and robust.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.history_my_actions: list[Action] = []
        self.history_num_cooperators: list[int] = []
        self.C_collapse_threshold = max(2 / self.n, 0.2)
        self.C_exploit_threshold = 0.6
        self.W = 3
        self.Delta_decrease = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        my_action: Action
        if current_round_0_indexed == 0:
            my_action = Action.C
        elif current_round_0_indexed == self.r - 1:
            my_action = Action.D
        else:
            num_opponent_cooperators_prev = sum(history.opponent_actions[current_round_0_indexed - 1, :])
            my_last_action_bool = history.my_actions[current_round_0_indexed - 1]
            num_cooperators_prev = num_opponent_cooperators_prev + (1 if my_last_action_bool else 0)
            self.history_num_cooperators.append(num_cooperators_prev)
            my_last_action_actual = Action.C if my_last_action_bool else Action.D
            current_coop_prop = num_cooperators_prev / self.n
            if current_coop_prop < self.C_collapse_threshold:
                my_action = Action.D
            elif current_coop_prop >= self.C_exploit_threshold:
                my_action = Action.D
            else:
                start_idx_for_avg = max(0, len(self.history_num_cooperators) - self.W)
                recent_window_num_cooperators = self.history_num_cooperators[start_idx_for_avg:]
                recent_avg_coop_prop: float
                if not recent_window_num_cooperators:
                    recent_avg_coop_prop = current_coop_prop
                else:
                    recent_avg_coop_prop = sum(recent_window_num_cooperators) / (len(recent_window_num_cooperators) * self.n)
                if my_last_action_actual == Action.D:
                    if current_coop_prop < recent_avg_coop_prop - self.Delta_decrease:
                        my_action = Action.C
                    else:
                        my_action = Action.D
                else:
                    my_action = Action.D
        self.history_my_actions.append(my_action)
        return my_action

class Strategy_EXPLOITATIVE_127(BaseStrategy):
    """
    The "Strategic Defector with Bait" (SDB) strategy for the N-Player Public Goods Game.
    This strategy prioritizes free-riding by generally defecting. It only cooperates under
    specific conditions to "bait" other players into cooperating, creating a larger public
    good to exploit in subsequent rounds. It aims to maximize individual payoff by manipulating
    the collective cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.BAIT_THRESHOLD = 1
        self.MIN_ROUNDS_BETWEEN_BAITS = 3
        self.my_last_action = Action.D
        self.rounds_since_last_bait = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_index = state.round_number
        if current_round_index == 0:
            chosen_action = Action.D
            self.my_last_action = Action.D
            self.rounds_since_last_bait = 1
            return chosen_action
        if current_round_index == self.game_description.n_rounds - 1:
            chosen_action = Action.D
            self.my_last_action = Action.D
            self.rounds_since_last_bait = 0
            return chosen_action
        c_other_prev_round = sum(history.opponent_actions[-1, :])
        should_bait = c_other_prev_round <= self.BAIT_THRESHOLD and self.rounds_since_last_bait >= self.MIN_ROUNDS_BETWEEN_BAITS and (self.my_last_action == Action.D)
        if should_bait:
            chosen_action = Action.C
            self.my_last_action = Action.C
            self.rounds_since_last_bait = 0
        else:
            chosen_action = Action.D
            self.my_last_action = Action.D
            self.rounds_since_last_bait += 1
        return chosen_action

class Strategy_EXPLOITATIVE_128(BaseStrategy):
    """
    Opportunistic Free-Rider Strategy for the N-Player Public Goods Game.

    This strategy is designed to probe for cooperation, free-ride when the opportunity is ripe,
    and protect itself when others defect or when the game is ending. It adaptively responds to
    the observed level of cooperation in the group to maximize its own payoff.

    Strategy phases:
    1. Initial Foray: Cooperate unconditionally for a few initial rounds to foster cooperation.
    2. Endgame Exploitation: Defect unconditionally in the final rounds, leveraging the game's end.
    3. Adaptive Play:
       - Punish Widespread Defection: Defect if overall cooperation falls below a threshold.
       - Exploit High Cooperation: Defect if almost all other players are cooperating to free-ride.
       - Maintain Cooperation: Cooperate in moderate cooperation levels to sustain the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.initial_cooperation_rounds = 1
        self.defection_punishment_threshold_percentage = 0.5
        self.exploit_full_cooperation_threshold_count = self.n_players - 1
        self.endgame_exploitation_rounds = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed <= self.initial_cooperation_rounds:
            return Action.C
        if current_round_1_indexed > self.n_rounds - self.endgame_exploitation_rounds:
            return Action.D
        previous_round_cooperators_total = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        required_cooperators_for_punishment = math.floor(self.n_players * self.defection_punishment_threshold_percentage)
        if required_cooperators_for_punishment == 0 and self.n_players > 1:
            required_cooperators_for_punishment = 1
        if previous_round_cooperators_total < required_cooperators_for_punishment:
            return Action.D
        if previous_round_cooperators_total >= self.exploit_full_cooperation_threshold_count:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_129(BaseStrategy):
    """
    "The Shark" strategy for an N-Player Public Goods Game.
    This strategy is an aggressively exploitative, adaptive, and robust strategy.
    It starts with a brief probe of cooperation, then defaults to defection to maximize
    individual payoff from others' contributions. It only cooperates again if persistent
    defection causes a widespread breakdown of cooperation, offering a temporary
    conciliatory gesture to reset and lure others back, creating future exploitation opportunities.
    It unconditionally defects in the end game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.L = max(1, min(3, math.floor(self.r / 4)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed > self.r - self.L:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        my_action_prev_round_bool = history.my_actions[-1]
        N_C_others_prev_round = np.sum(history.opponent_actions[-1, :])
        N_D_others_prev_round = self.n - 1 - N_C_others_prev_round
        if my_action_prev_round_bool == True:
            return Action.D
        elif N_D_others_prev_round >= math.ceil((self.n - 1) / 2):
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_130(BaseStrategy):
    """
    Adaptive Free-Rider with Strategic Reciprocity (AFRSR)

    This strategy is designed to be exploitative, adaptive, and robust in the N-Player
    Public Goods Game. It aims to maximize its own payoff by primarily free-riding on
    the cooperation of others, while strategically engaging in cooperation or punishment
    to maintain an environment conducive to future free-riding.

    It works by:
    1. Probing with cooperation in the first round to gather information.
    2. Exploiting in the final round by defecting due to backward induction.
    3. Adapting in intermediate rounds based on the proportion of other players
       who cooperated in the previous round:
       - Free-riding (Defect) if others are highly cooperative.
       - Punishing/cutting losses (Defect) if others are extremely uncooperative.
       - Strategically reciprocating (Cooperate or Defect) in moderate cooperation
         scenarios to either punish lukewarm reciprocation or re-induce cooperation.
    """
    FREE_RIDE_THRESHOLD = 0.5
    RECIPROCATE_THRESHOLD = 0.2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        prev_round_idx = current_round_0_indexed - 1
        my_prev_action_is_C = history.my_actions[prev_round_idx]
        cooperating_others_prev = sum(history.opponent_actions[prev_round_idx, :])
        num_others = self.n_players - 1
        coop_rate_others = float(cooperating_others_prev) / num_others
        if coop_rate_others >= self.FREE_RIDE_THRESHOLD:
            return Action.D
        elif coop_rate_others < self.RECIPROCATE_THRESHOLD:
            return Action.D
        elif my_prev_action_is_C:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_131(BaseStrategy):
    """
    The "Single-Probe Defector" strategy:
    This strategy is designed to be highly exploitative. It makes a minimal initial investment
    (cooperating in Round 1) to test the environment's cooperativeness. After this single
    probe, it aggressively defects for all subsequent rounds, prioritizing individual gain
    by free-riding on any observed cooperation or defaulting to self-preservation if
    cooperation is scarce. It relies only on the current round number.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_132(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the N-Player Public Goods Game.
    This strategy attempts to induce cooperation in others and then exploits it.
    It dynamically adjusts its behavior based on the observed level of cooperation
    from other players in the previous round.

    Strategy Parameters:
    - EXPLOITATION_THRESHOLD (ET): Minimum number of *other* players who must
      have cooperated in the previous round for this strategy to defect.
      Value: 1 (maximally exploitative)
    - COOPERATION_ATTEMPT_LIMIT (CAL): How many consecutive rounds this strategy
      will attempt to induce cooperation (by cooperating itself) when
      cooperation from others is too low, before giving up and defecting permanently.
      Value: 1 (balances re-establishment with avoiding "sucker" status)
    """
    EXPLOITATION_THRESHOLD = 1
    COOPERATION_ATTEMPT_LIMIT = 1

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._previous_action: Action | None = None
        self._consecutive_low_coop_rounds: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n_rounds = self.game_description.n_rounds
        action_for_current_round: Action
        if current_round_number == n_rounds - 1:
            action_for_current_round = Action.D
        elif current_round_number == 0:
            action_for_current_round = Action.C
        else:
            n_c_others_last_round = np.sum(history.opponent_actions[-1, :])
            if n_c_others_last_round >= self.EXPLOITATION_THRESHOLD:
                action_for_current_round = Action.D
                self._consecutive_low_coop_rounds = 0
            else:
                self._consecutive_low_coop_rounds += 1
                if self._consecutive_low_coop_rounds <= self.COOPERATION_ATTEMPT_LIMIT:
                    action_for_current_round = Action.C
                else:
                    action_for_current_round = Action.D
        self._previous_action = action_for_current_round
        return action_for_current_round

class Strategy_EXPLOITATIVE_133(BaseStrategy):
    """
    The Adaptive Predator strategy aims to maximize its own total payoff over r rounds by aggressively exploiting
    the cooperation of other players. It achieves this by strategically probing for cooperation, then exploiting it,
    and attempting to re-induce it only when necessary to maintain exploitable conditions.
    It always defects in the final round to avoid being a "sucker."
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.COOPERATION_THRESHOLD_RATIO = 0.5
        self.PATIENCE_ROUNDS = 2
        self._my_previous_action: Action | None = None
        self._consecutive_low_cooperation_rounds: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        current_action: Action
        if current_round_0_indexed == r - 1:
            current_action = Action.D
            self._my_previous_action = current_action
            return current_action
        if current_round_0_indexed == 0:
            current_action = Action.C
            self._my_previous_action = current_action
            return current_action
        my_action_prev_round_bool = history.my_actions[current_round_0_indexed - 1]
        opponent_actions_prev_round_bools = history.opponent_actions[current_round_0_indexed - 1, :]
        total_cooperators_previous_round = int(my_action_prev_round_bool) + np.sum(opponent_actions_prev_round_bools)
        cooperation_threshold = n * self.COOPERATION_THRESHOLD_RATIO
        if total_cooperators_previous_round < cooperation_threshold:
            self._consecutive_low_cooperation_rounds += 1
        else:
            self._consecutive_low_cooperation_rounds = 0
        if self._my_previous_action == Action.C:
            if total_cooperators_previous_round >= cooperation_threshold:
                current_action = Action.D
            else:
                current_action = Action.D
        elif total_cooperators_previous_round >= cooperation_threshold:
            current_action = Action.D
        elif self._consecutive_low_cooperation_rounds >= self.PATIENCE_ROUNDS:
            current_action = Action.C
        else:
            current_action = Action.D
        self._my_previous_action = current_action
        return current_action

class Strategy_EXPLOITATIVE_134(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy is designed to be highly exploitative by
    prioritizing defection when profitable, while strategically cooperating
    only when necessary to preserve or enhance future free-riding opportunities.
    It balances immediate gain with the long-term sustainability of the public
    good pool, which it views as its resource to exploit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        self.min_coop_to_sustain = max(2, math.floor(n / 3))
        self.optimal_exploitation_threshold = max(math.ceil(n / 2), n - 1)
        if self.optimal_exploitation_threshold <= self.min_coop_to_sustain:
            self.optimal_exploitation_threshold = self.min_coop_to_sustain + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        c_prev_opponents_sum = np.sum(history.opponent_actions[-1, :])
        c_prev_my_action_value = 1 if history.my_actions[-1] else 0
        C_prev = c_prev_opponents_sum + c_prev_my_action_value
        if C_prev <= self.min_coop_to_sustain:
            return Action.D
        if C_prev >= self.optimal_exploitation_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_135(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize its own total payoff by free-riding on other players'
    contributions. It is adaptive, strategically cooperating for a round if the collective cooperation level of
    other players drops below a threshold that makes free-riding sufficiently profitable. This cooperation is
    a calculated risk to re-establish a higher level of cooperation from others, which can then be exploited again.
    It explicitly avoids continuous cooperation if others consistently defect and always defects in the first and
    last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        self.min_C_others_for_free_ride = int(math.ceil((k - 1) * n / k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        C_others_prev = sum(history.opponent_actions[-1, :])
        if C_others_prev == 0:
            return Action.D
        elif C_others_prev >= self.min_C_others_for_free_ride:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_136(BaseStrategy):
    """
    The Pragmatic Exploiter strategy for the N-Player Public Goods Game.
    Prioritizes immediate personal payoff by free-riding, with calculated,
    temporary cooperation to "re-prime" the public good when it has collapsed,
    creating new exploitation opportunities. Always defects in end-game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.dejection_streak: int = 0
        self.reprimed_flag: bool = False
        self.REPRIME_THRESHOLD: int = 1
        self.DEFECTION_STREAK_TO_REPRIME: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_m = state.round_number + 1
        action_to_take: Action
        if current_m == 1:
            action_to_take = Action.D
            self.my_last_action = action_to_take
            self.dejection_streak = 1
            self.reprimed_flag = False
            return action_to_take
        C_prev = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if current_m >= self.game_description.n_rounds - 1:
            action_to_take = Action.D
            self.my_last_action = action_to_take
            self.dejection_streak += 1
            self.reprimed_flag = False
            return action_to_take
        if self.reprimed_flag:
            action_to_take = Action.D
            self.my_last_action = action_to_take
            self.dejection_streak = 1
            self.reprimed_flag = False
            return action_to_take
        if C_prev <= self.REPRIME_THRESHOLD and self.dejection_streak >= self.DEFECTION_STREAK_TO_REPRIME:
            action_to_take = Action.C
            self.my_last_action = action_to_take
            self.dejection_streak = 0
            self.reprimed_flag = True
            return action_to_take
        else:
            action_to_take = Action.D
            self.my_last_action = action_to_take
            self.dejection_streak += 1
            self.reprimed_flag = False
            return action_to_take

class Strategy_EXPLOITATIVE_137(BaseStrategy):
    """
    The "Pragmatic Predator" strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by aggressively free-riding on others' cooperation
    while minimizing the risk of being exploited itself. It probes with initial cooperation,
    ruthlessly defects when enough others cooperate, and defects when no one cooperates.
    In situations of moderate cooperation, it strategically cooperates to encourage more
    cooperation for future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialises the strategy with game parameters and calculates the FREE_RIDE_THRESHOLD.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.FREE_RIDE_THRESHOLD = (self.n_players - 1) // 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action for the current player based on the Pragmatic Predator strategy rules.

        Args:
            state (GameState): The current state of the game, including the round number.
            history (None | PlayerHistory): The history of actions and payoffs from previous rounds.
                                          This is None for the very first round (round 0).

        Returns:
            Action: 'C' for Cooperate, 'D' for Defect.
        """
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_other_cooperated_last_round = sum(history.opponent_actions[-1, :])
        if num_other_cooperated_last_round >= self.FREE_RIDE_THRESHOLD:
            return Action.D
        elif num_other_cooperated_last_round == 0:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_138(BaseStrategy):
    """
    The Unflinching Free-Rider strategy for the N-Player Public Goods Game.

    This strategy always chooses to Defect (D) in every round. This is because,
    given the game parameters (1 < k < n), Defecting is a strictly dominant
    strategy. It consistently yields a higher individual payoff (by 1 - k/n)
    than Cooperating, regardless of other players' actions or the game history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        return Action.D

class Strategy_EXPLOITATIVE_139(BaseStrategy):
    """
    The Adaptive Leech strategy aims to maximize personal payoff in the N-Player Public Goods Game by
    primarily free-riding, only cooperating strategically to prevent the collapse of the public good,
    and aggressively defecting in endgame rounds.

    Phases:
    1.  Warm-Up Phase (initial rounds): Cooperates to "prime the pump" and encourage others to contribute.
    2.  Adaptive Exploitation Phase (mid-game): Defaults to defecting. If the proportion of others'
        cooperation drops below a threshold, it cooperates to revive the public good for continued exploitation.
    3.  Endgame Phase (final rounds): Always defects to maximize immediate payoff, leveraging the
        diminished incentive for future cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.r_warmup = 2
        self.revive_coop_threshold = 0.3
        self.r_endgame = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_1_indexed > total_rounds - self.r_endgame:
            return Action.D
        elif current_round_1_indexed <= self.r_warmup:
            return Action.C
        else:
            cooperators_others_prev_round = sum(history.opponent_actions[-1, :])
            others_coop_proportion = cooperators_others_prev_round / (n_players - 1)
            if others_coop_proportion < self.revive_coop_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_140(BaseStrategy):
    """
    The "Calculated Opportunist" strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by free-riding on others'
    contributions. It only cooperates when the collective good is severely
    threatened, acting as a "pump" to revive cooperation, only to exploit
    it once it's re-established. It always defects in the final round to
    maximize end-game payoff due to the absence of future consequences.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Calculated Opportunist strategy.

        Args:
            game_description (PublicGoodsDescription): An object containing
                                                       game parameters like
                                                       n_players, n_rounds, k.
        """
        self.game_description = game_description
        self.cooperation_revival_threshold = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): Current state of the game, including the round number.
            history (None | PlayerHistory): Past actions and payoffs, or None if it's
                                            the very first round.

        Returns:
            Action: The chosen action for the current round (Action.C or Action.D).
        """
        current_round_0_indexed = state.round_number
        total_rounds_0_indexed = self.game_description.n_rounds - 1
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds_0_indexed:
            return Action.D
        my_prev_action_is_C = int(history.my_actions[-1])
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev = my_prev_action_is_C + num_opponent_cooperators_prev
        if total_cooperators_prev <= self.cooperation_revival_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_141(BaseStrategy):
    """
    The Adaptive Opportunistic Free-Rider (AOFR) strategy aims to maximize its own payoff by
    defaulting to defection (free-riding) but intelligently adjusting its behavior.

    - In the first round (t=1), it cooperates to probe for cooperation from others.
    - In the last round (t=r), it defects to exploit the lack of future repercussions.
    - In intermediate rounds (1 < t < r):
        - If collective cooperation collapsed in the previous round (C_prev == 0), it defects to avoid being a sucker.
        - If cooperation was weak but present (0 < C_prev < k), it cooperates to re-induce or encourage more cooperation.
        - If cooperation was sufficient (C_prev >= k), it defects to free-ride on the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Opportunistic Free-Rider strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on AOFR strategy rules.

        Args:
            state: An object containing the current game state, including the round number (0-indexed).
            history: An object containing the history of actions and payoffs for the player and opponents
                     up to the previous round. None for the first round.

        Returns:
            Action: The chosen action for the current round, either Action.C (Cooperate) or Action.D (Defect).
        """
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        k_factor = self.game_description.k
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        else:
            my_prev_action_was_c = history.my_actions[current_round_0_indexed - 1]
            opponent_prev_cooperators_count = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
            C_prev = int(my_prev_action_was_c) + opponent_prev_cooperators_count
            if C_prev == 0:
                return Action.D
            elif C_prev < k_factor:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_142(BaseStrategy):
    """
    The "Adaptive Pragmatist" strategy for an N-Player Public Goods Game.
    This strategy is designed to maximize individual payoff by primarily free-riding on the
    cooperation of others. It makes minimal, calculated investments in cooperation only
    when necessary to prevent the collapse of the public good to a level where it's no longer
    profitable to exploit. It adapts to the observed level of cooperation in the game and
    avoids being a "sucker" by repeatedly contributing without effect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_instability_threshold = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.r - 1:
            return Action.D
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_is_C = history.my_actions[-1]
        total_cooperators_prev_round = opponent_cooperators_prev_round + (1 if my_action_prev_is_C else 0)
        if total_cooperators_prev_round < self.cooperation_instability_threshold and (not my_action_prev_is_C):
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_143(BaseStrategy):
    """
    Adaptive Exploiter with Self-Preservation (AESP) strategy for the N-Player Public Goods Game.

    This strategy is designed to be highly opportunistic and aggressive in exploiting cooperation,
    while having strong self-preservation mechanisms to avoid being a "sucker" and ensure a
    baseline performance competitive even against purely defecting opponents.

    Core principles:
    1. Initial Probe (Round 1): Cooperate once to test for cooperative opponents.
    2. Exploit When Possible: Defect to free-ride when sufficient cooperation exists.
    3. Self-Preservation: Immediately revert to defection if exploited (lone cooperator).
    4. Re-seed Cooperation (Cautiously): Make a rare attempt to restart cooperation if it completely collapsed,
       but only if not recently exploited.
    5. Endgame Rationality: Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_lone_cooperations = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        my_last_action_is_cooperate: bool = history.my_actions[-1]
        total_cooperators_prev_round: int = sum(history.opponent_actions[-1, :]) + (1 if my_last_action_is_cooperate else 0)
        if my_last_action_is_cooperate and total_cooperators_prev_round == 1:
            self.consecutive_lone_cooperations += 1
        else:
            self.consecutive_lone_cooperations = 0
        if self.consecutive_lone_cooperations > 0:
            return Action.D
        if total_cooperators_prev_round == 0 and (not my_last_action_is_cooperate) and (self.consecutive_lone_cooperations == 0):
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_144(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy aims to exploit other players' cooperation
    in the N-Player Public Goods Game. It primarily defects to free-ride but
    employs calculated strategic cooperation (baiting or luring) to encourage
    or revive a public good pool when it benefits future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.C_Min_Revival_Threshold = max(1, self.game_description.n_players // 4)
        self._last_lure_round = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        n_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        prev_round_my_action = history.my_actions[current_round_0_indexed - 1]
        prev_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
        N_C_total_prev = np.sum(prev_round_opponent_actions) + int(prev_round_my_action)
        cond_critically_low_public_good = N_C_total_prev < self.C_Min_Revival_Threshold
        cond_sufficient_rounds_remaining = n_rounds - (current_round_0_indexed + 1) > 1
        cond_not_recent_lure_attempt = current_round_0_indexed - 1 != self._last_lure_round
        if cond_critically_low_public_good and cond_sufficient_rounds_remaining and cond_not_recent_lure_attempt:
            self._last_lure_round = current_round_0_indexed
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_145(BaseStrategy):
    """
    The "Adaptive Exploiter" strategy is designed to maximize its own payoff by leveraging the free-rider incentive
    in the Public Goods Game. It is adaptive, robust, and explicitly exploitative, using cooperation only as a
    strategic tool to induce or restore a cooperative environment that it can then exploit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.reprobing_flag = False
        self.cooperation_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_0_indexed == 0:
            self.reprobing_flag = False
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            self.reprobing_flag = False
            return Action.D
        prev_round_my_action = history.my_actions[current_round_0_indexed - 1]
        prev_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
        S_c_prev = int(prev_round_my_action) + np.sum(prev_round_opponent_actions)
        coop_ratio_prev = S_c_prev / n_players
        if coop_ratio_prev >= self.cooperation_threshold:
            self.reprobing_flag = False
            return Action.D
        elif self.reprobing_flag == True:
            self.reprobing_flag = False
            return Action.D
        else:
            self.reprobing_flag = True
            return Action.C

class Strategy_EXPLOITATIVE_146(BaseStrategy):
    """
    The Adaptive Opportunist aims to create and maintain a cooperative environment among other players
    as much as possible, then opportunistically defects when cooperation is robust.
    It also proactively defects to avoid being exploited when cooperation is low and anticipates
    the breakdown of cooperation towards the end of the game due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n_players = game_description.n_players
        self.low_cooperation_threshold = max(1, math.floor(n_players * 0.3))
        if n_players <= 3:
            self.high_cooperation_threshold = n_players
        else:
            self.high_cooperation_threshold = max(self.low_cooperation_threshold + 1, math.floor(n_players * 0.8))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == total_rounds - 1:
            return Action.D
        elif current_round_0_indexed == total_rounds - 2:
            return Action.D
        total_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if total_cooperators_prev_round >= self.high_cooperation_threshold:
            return Action.D
        elif total_cooperators_prev_round < self.low_cooperation_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_147(BaseStrategy):
    """
    The Calculated Provocateur strategy for the N-Player Public Goods Game.

    This strategy primarily defects to free-ride. It makes a calculated, temporary
    sacrifice (cooperating) only when the game is stuck in a state of universal
    defection for too long. The goal of this temporary cooperation is to
    "provoke" other players into cooperating, creating an environment that can
    then be exploited by reverting to defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.reset_threshold = 3
        self.consecutive_rounds_with_zero_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n_rounds = self.game_description.n_rounds
        if current_round_number == n_rounds - 1:
            return Action.D
        if current_round_number == 0:
            return Action.D
        prev_round_opponent_cooperators = sum(history.opponent_actions[-1, :])
        prev_round_my_action = history.my_actions[-1]
        total_cooperators_prev_round = prev_round_opponent_cooperators + (1 if prev_round_my_action else 0)
        if total_cooperators_prev_round == 0:
            self.consecutive_rounds_with_zero_cooperators += 1
        else:
            self.consecutive_rounds_with_zero_cooperators = 0
        if self.consecutive_rounds_with_zero_cooperators >= self.reset_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_148(BaseStrategy):
    """
    The "Prudent Exploiter" strategy for the N-Player Public Goods Game.
    It aggressively free-rides by defaulting to defection, but prudently
    cooperates when necessary to prevent the collapse of the public good
    or to signal a willingness to cooperate, setting up future exploitation
    opportunities. It adapts to opponent cooperation levels using calculated
    CooperationFloor and ExploitCeiling thresholds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        self.cooperation_floor = max(0, math.ceil(n / k) - 1)
        self.exploit_ceiling = math.ceil(n / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n_rounds = self.game_description.n_rounds
        if current_round_number == 0:
            return Action.C
        if current_round_number == n_rounds - 1:
            return Action.D
        my_prev_action: np.bool_ = history.my_actions[-1]
        opponent_prev_actions: NDArray[np.bool_] = history.opponent_actions[-1, :]
        C_prev = int(my_prev_action) + np.sum(opponent_prev_actions)
        C_others_prev = np.sum(opponent_prev_actions)
        if C_others_prev < self.cooperation_floor:
            return Action.D
        if C_prev >= self.exploit_ceiling:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_149(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by strategically leveraging the
    cooperation of others while avoiding being a "sucker." It encourages collective
    cooperation just enough to free-ride when possible, and defects when cooperation
    from others is insufficient or when the game is ending.

    Key decision points:
    1.  Last Round: Always defects, as there are no future consequences.
    2.  First Round: Always cooperates, as an initial probe or investment.
    3.  Intermediate Rounds:
        a.  Calculates the proportion of other players who cooperated in the previous round.
        b.  Determines an "Adaptive Cooperation Threshold (ACT)" based on the k/n ratio.
            - A lower k/n (less individual benefit from public good) leads to a higher ACT,
              demanding more cooperation from others.
            - A higher k/n (more individual benefit) leads to a lower ACT,
              being more tolerant of some defection.
        c.  If previous cooperation from others is below ACT, it defects (punishment).
        d.  If previous cooperation from others meets or exceeds ACT:
            - If this strategy cooperated last round, it defects (free-rides).
            - If this strategy defected last round, it cooperates (incentivizes continued cooperation).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Free-Rider strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing the history of actions and payoffs
                     for this player and opponents up to the previous round.
                     Will be None for the very first round (round_number = 0).

        Returns:
            Action: Action.C for Cooperate, Action.D for Defect.
        """
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        k_factor = self.game_description.k
        if current_round_0_indexed == total_rounds - 1:
            action = Action.D
            self.my_last_action = action
            return action
        if current_round_0_indexed == 0:
            action = Action.C
            self.my_last_action = action
            return action
        s_other_c_prev = sum(history.opponent_actions[-1, :])
        coop_ratio_others_prev = s_other_c_prev / (n_players - 1)
        ACT = 0.5 + (1 - k_factor / n_players) / 2
        if coop_ratio_others_prev < ACT:
            action = Action.D
        elif self.my_last_action == Action.C:
            action = Action.D
        else:
            action = Action.C
        self.my_last_action = action
        return action

class Strategy_EXPLOITATIVE_150(BaseStrategy):
    """
    The "Opportunistic Piranha" strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by primarily free-riding on
    others' contributions, while adaptively managing the level of collective
    cooperation to ensure there's enough to exploit. It operates in three
    dynamic states: COOPERATE_TO_PROBE, EXPLOIT_FREERIDE, and PUNISH_DEFECTION.

    State Transitions:
    1.  COOPERATE_TO_PROBE: Cooperates to test for cooperative potential.
        - Switches to EXPLOIT_FREERIDE if other players' cooperation is high enough.
        - Switches to PUNISH_DEFECTION if probing fails to elicit sufficient cooperation within a limit.
    2.  EXPLOIT_FREERIDE: Defects to free-ride.
        - Switches to COOPERATE_TO_PROBE if other players' cooperation drops too low,
          indicating the need to rebuild the cooperative pool.
    3.  PUNISH_DEFECTION: Defects to punish widespread defection.
        - Switches to COOPERATE_TO_PROBE after a fixed number of punishment rounds,
          or if others show signs of cooperation recovery.

    Edge Cases:
    - Always cooperates in the very first round to establish an initial baseline.
    - Always defects in the very last round to maximize terminal payoff, as there
      are no future interactions to incentivize cooperation.
    """
    STATE_COOPERATE_TO_PROBE = 'cooperate_to_probe'
    STATE_EXPLOIT_FREERIDE = 'exploit_freeride'
    STATE_PUNISH_DEFECTION = 'punish_defection'
    COOPERATION_HIGH_THRESHOLD = 0.6
    COOPERATION_LOW_THRESHOLD = 0.3
    COOPERATION_RECOVERY_THRESHOLD = 0.4
    PROBE_ROUNDS_LIMIT = 2
    PUNISH_ROUNDS_LIMIT = 3

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_current_state: str = self.STATE_COOPERATE_TO_PROBE
        self.rounds_in_current_state: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.my_current_state = self.STATE_COOPERATE_TO_PROBE
            self.rounds_in_current_state = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_prev_round_others = np.sum(previous_round_opponent_actions)
        num_other_players = self.game_description.n_players - 1
        last_round_cooperation_rate_others = float(num_cooperators_prev_round_others) / num_other_players if num_other_players > 0 else 0.0
        if self.my_current_state == self.STATE_COOPERATE_TO_PROBE:
            if last_round_cooperation_rate_others >= self.COOPERATION_HIGH_THRESHOLD:
                self.my_current_state = self.STATE_EXPLOIT_FREERIDE
                self.rounds_in_current_state = 0
            elif self.rounds_in_current_state >= self.PROBE_ROUNDS_LIMIT:
                self.my_current_state = self.STATE_PUNISH_DEFECTION
                self.rounds_in_current_state = 0
        elif self.my_current_state == self.STATE_EXPLOIT_FREERIDE:
            if last_round_cooperation_rate_others < self.COOPERATION_LOW_THRESHOLD:
                self.my_current_state = self.STATE_COOPERATE_TO_PROBE
                self.rounds_in_current_state = 0
        elif self.my_current_state == self.STATE_PUNISH_DEFECTION:
            if self.rounds_in_current_state >= self.PUNISH_ROUNDS_LIMIT:
                self.my_current_state = self.STATE_COOPERATE_TO_PROBE
                self.rounds_in_current_state = 0
            elif last_round_cooperation_rate_others > self.COOPERATION_RECOVERY_THRESHOLD:
                self.my_current_state = self.STATE_COOPERATE_TO_PROBE
                self.rounds_in_current_state = 0
        action_to_take = Action.D
        if self.my_current_state == self.STATE_COOPERATE_TO_PROBE:
            action_to_take = Action.C
        self.rounds_in_current_state += 1
        return action_to_take

class Strategy_EXPLOITATIVE_151(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy is designed to exploit the N-Player Public Goods Game
    by encouraging others to contribute to the public good, then free-riding on their
    contributions whenever possible. It adapts its behavior based on the observed level of
    cooperation in previous rounds, seeking to maximize its own payoff while minimizing
    its own contributions.

    Core Philosophy:
    The strategy recognizes the individual incentive to defect (free-ride) but also
    acknowledges the collective benefit of cooperation. Its goal is to manipulate the
    environment to encourage sufficient cooperation from others, creating a substantial
    public good, and then to exploit this public good by defecting. If cooperation falters,
    it strategically invests (cooperates) to try and revive the public good, only giving
    up entirely if cooperation proves persistently futile.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.C_threshold_high = math.ceil(self.n / self.k)
        self.C_threshold_low = 1
        self.defection_tolerance_rounds = 2
        self.current_state = 'Induce_Cooperation'
        self.consecutive_low_cooperation_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.r:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        prev_round_0_indexed = state.round_number - 1
        my_prev_action_cooperated = int(history.my_actions[prev_round_0_indexed])
        opponent_prev_cooperators = sum(history.opponent_actions[prev_round_0_indexed, :])
        C_count_prev = my_prev_action_cooperated + opponent_prev_cooperators
        if self.current_state == 'Give_Up':
            return Action.D
        if C_count_prev < self.C_threshold_low:
            self.consecutive_low_cooperation_rounds += 1
            if self.consecutive_low_cooperation_rounds >= self.defection_tolerance_rounds:
                self.current_state = 'Give_Up'
                return Action.D
            else:
                self.current_state = 'Induce_Cooperation'
                return Action.C
        elif C_count_prev >= self.C_threshold_high:
            self.consecutive_low_cooperation_rounds = 0
            self.current_state = 'Free_Ride'
            return Action.D
        else:
            self.consecutive_low_cooperation_rounds = 0
            self.current_state = 'Induce_Cooperation'
            return Action.C

class Strategy_EXPLOITATIVE_152(BaseStrategy):
    """
    The Adaptive Predator strategy aims to defect whenever possible to maximize immediate private gain,
    only cooperating when it serves a clear self-interested, long-term purpose: either to initiate
    potential cooperation or to prevent the collapse of a public good that it intends to exploit.

    It is exploitative, adaptive, and robust, adjusting its behavior based on the observed
    number of cooperators in the previous round and predefined thresholds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        k = game_description.k
        self.MIN_COOP_TO_BE_VIABLE = math.ceil(n / k)
        self.HIGH_COOP_FOR_EXPLOIT = n - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.game_description.n_rounds - 1:
            return Action.D
        C_prev = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        action_to_take = Action.D
        if self.MIN_COOP_TO_BE_VIABLE >= self.HIGH_COOP_FOR_EXPLOIT:
            action_to_take = Action.D
        elif C_prev >= self.HIGH_COOP_FOR_EXPLOIT:
            action_to_take = Action.D
        elif C_prev < self.MIN_COOP_TO_BE_VIABLE:
            action_to_take = Action.D
        else:
            action_to_take = Action.C
        return action_to_take

class Strategy_EXPLOITATIVE_153(BaseStrategy):
    """
    The Calculated Exploiter strategy aims to maximize personal gain by free-riding on the cooperation
    of others. It attempts to foster enough cooperation in the group to make defection profitable,
    but it is quick to defect when others do not contribute. It employs a measured cycle of
    exploitation and apparent cooperation to prolong the group's willingness to contribute.

    Core Philosophy:
    Maximize personal gain by free-riding. Foster just enough cooperation to make defection profitable,
    but defect quickly when others stop contributing to avoid being a "sucker." Use a measured
    cycle of exploitation and apparent cooperation to sustain group contribution.
    """
    COOPERATION_THRESHOLD: float = 0.5
    FORGIVENESS_THRESHOLD: float = 0.2
    EXPLOIT_GRACE_PERIOD: int = 2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.my_consecutive_defections: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players_overall = self.game_description.n_players
        if state.round_number == 0:
            chosen_action = Action.C
            self.my_last_action = chosen_action
            self.my_consecutive_defections = 0
            return chosen_action
        if current_round_1_indexed == total_rounds:
            chosen_action = Action.D
            return chosen_action
        num_others_cooperated = np.sum(history.opponent_actions[-1])
        n_others = n_players_overall - 1
        others_cooperation_rate = num_others_cooperated / n_others
        chosen_action: Action
        if others_cooperation_rate >= self.COOPERATION_THRESHOLD:
            if self.my_consecutive_defections < self.EXPLOIT_GRACE_PERIOD:
                chosen_action = Action.D
                self.my_consecutive_defections += 1
            else:
                chosen_action = Action.C
                self.my_consecutive_defections = 0
        else:
            rounds_remaining = total_rounds - current_round_1_indexed
            if others_cooperation_rate < self.FORGIVENESS_THRESHOLD and rounds_remaining >= 3:
                chosen_action = Action.C
                self.my_consecutive_defections = 0
            else:
                chosen_action = Action.D
                self.my_consecutive_defections += 1
        self.my_last_action = chosen_action
        return chosen_action

class Strategy_EXPLOITATIVE_154(BaseStrategy):
    """
    The Calculated Free-Rider (CFR) strategy for the N-Player Public Goods Game.
    It adaptively exploits cooperative environments by defecting, while strategically
    re-engaging cooperation with short bursts when conditions become unprofitable
    for free-riding, aiming to restore an exploitable state.
    It starts with an initial probe, always defects in the final round, and
    avoids being a "sucker" when cooperation is too low.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.initial_probe_rounds_calculated = min(3, r - 1)
        if self.initial_probe_rounds_calculated < 1:
            self.initial_probe_rounds_calculated = 1
        self.min_cooperation_to_exploit_calculated = math.floor(n * 0.5)
        self.re_engage_cooperation_threshold_low_calculated = max(1, math.floor(n * 0.2))
        self.re_engage_rounds_delay_calculated = 2
        self.cooperation_reset_duration_calculated = 1
        self.num_rounds_below_re_engage_threshold = 0
        self.currently_re_engaging = False
        self.re_engage_end_round_calculated = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if current_round_0_indexed < self.initial_probe_rounds_calculated:
            return Action.C
        C_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if self.currently_re_engaging:
            if current_round_0_indexed <= self.re_engage_end_round_calculated:
                return Action.C
            else:
                self.currently_re_engaging = False
                self.re_engage_end_round_calculated = -1
        if not self.currently_re_engaging and current_round_0_indexed < total_rounds - self.initial_probe_rounds_calculated:
            if C_prev < self.re_engage_cooperation_threshold_low_calculated:
                self.num_rounds_below_re_engage_threshold += 1
            else:
                self.num_rounds_below_re_engage_threshold = 0
            if self.num_rounds_below_re_engage_threshold >= self.re_engage_rounds_delay_calculated:
                self.currently_re_engaging = True
                self.re_engage_end_round_calculated = current_round_0_indexed + self.cooperation_reset_duration_calculated - 1
                self.num_rounds_below_re_engage_threshold = 0
                return Action.C
        if C_prev >= self.min_cooperation_to_exploit_calculated:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_155(BaseStrategy):
    """
    The Adaptive Exploiter strategy dynamically responds to cooperation levels of other players
    to maximize its own payoff. It free-rides on high cooperation, punishes low cooperation,
    and strategically invests in moderate conditions to create future exploitation opportunities.
    It defects unconditionally in terminal rounds due to backward induction.

    Strategy Parameters:
    - TDR (Terminal Defect Rounds): Number of final rounds to unconditionally Defect. (default: 1)
    - PT (Punishment Threshold): If the proportion of *other* cooperators falls below this, Defect. (default: 0.3)
    - ET (Exploitation Threshold): If the proportion of *other* cooperators meets or exceeds this, Defect. (default: 0.7)
    """
    DEFAULT_TDR = 1
    DEFAULT_PT = 0.3
    DEFAULT_ET = 0.7

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.TDR = self.DEFAULT_TDR
        self.PT = self.DEFAULT_PT
        self.ET = self.DEFAULT_ET
        if not 0 <= self.PT < self.ET <= 1:
            pass

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed >= r - self.TDR:
            return Action.D
        else:
            num_other_cooperators_prev = sum(history.opponent_actions[-1, :])
            num_other_players = n - 1
            P_C_others_prev = num_other_cooperators_prev / num_other_players
            if P_C_others_prev >= self.ET:
                return Action.D
            elif P_C_others_prev < self.PT:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_156(BaseStrategy):
    """
    The Adaptive Exploiter (TAE) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own total payoff over `n_rounds` by
    dynamically adapting its cooperation strategy. It follows a three-stage approach:

    1.  **First Round (t=1 / round_number=0):** Cooperates (C) as a "probe" to
        gather initial data on others' willingness to contribute and to signal
        initial cooperation.

    2.  **Last Round (t=r / round_number=n_rounds-1):** Defects (D) due to
        backward induction, as there are no future consequences to consider.

    3.  **Intermediate Rounds (1 < t < r):**
        *   Calculates the proportion of other players who cooperated in the
            previous round (`P_C_others_prev`).
        *   Determines a Dynamic Cooperation Threshold (DCT) based on game parameters
            `n_players` and `k_factor`: `DCT = (n_players - k_factor) / (n_players - 1)`.
        *   If `P_C_others_prev` meets or exceeds the `DCT`, the strategy Defects (D)
            to free-ride on sufficient existing cooperation.
        *   If `P_C_others_prev` is below the `DCT`, the strategy Cooperates (C)
            to "re-seed" or encourage cooperation, aiming to rebuild the public
            good for future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with the game's parameters.
        No additional internal state needs to be managed beyond the game description,
        as all necessary past information is provided in the `history` object.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round based on
        the game state and past interactions.

        Args:
            state: An object containing the current round number.
            history: An object containing records of past actions and payoffs
                     for this player and all opponents. It is None for the first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        k_factor = self.game_description.k
        current_round_num_0_indexed = state.round_number
        if current_round_num_0_indexed == 0:
            return Action.C
        if current_round_num_0_indexed == n_rounds - 1:
            return Action.D
        previous_round_index = current_round_num_0_indexed - 1
        num_cooperators_others_prev = np.sum(history.opponent_actions[previous_round_index, :])
        proportion_cooperators_others_prev = num_cooperators_others_prev / (n_players - 1)
        DCT = (n_players - k_factor) / (n_players - 1)
        if proportion_cooperators_others_prev >= DCT:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_157(BaseStrategy):
    """
    The "Calculated Opportunist" strategy for the N-Player Public Goods Game.
    This strategy defaults to defection to maximize immediate private gain.
    It strategically cooperates only to prevent the collapse of the public good,
    which is essential for future free-riding opportunities.
    It adapts to observed collective cooperation levels without assuming specific
    opponent strategies. The strategy always defects in the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.is_cooperating_burst_active = False
        self.cooperation_burst_rounds_left = 0
        self.T_COLLAPSE = max(1, self.game_description.n_players // 3)
        self.COOPERATION_BURST_DURATION = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds_r = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == total_rounds_r:
            return Action.D
        else:
            prev_round_opponent_actions = history.opponent_actions[state.round_number - 1, :]
            N_C_prev = np.sum(prev_round_opponent_actions)
            if self.is_cooperating_burst_active:
                self.cooperation_burst_rounds_left -= 1
                if self.cooperation_burst_rounds_left == 0:
                    self.is_cooperating_burst_active = False
                return Action.C
            elif N_C_prev <= self.T_COLLAPSE:
                self.is_cooperating_burst_active = True
                self.cooperation_burst_rounds_left = min(self.COOPERATION_BURST_DURATION, total_rounds_r - current_round_1_indexed)
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_158(BaseStrategy):
    """
    The Cautious Free-Rider (CFR) strategy aims to maximize a player's own payoff by primarily free-riding.
    It adaptively intervenes with strategic cooperation only to prevent a complete collapse of the public good,
    which would ultimately harm even the free-rider, always intending to revert to defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.r - 1:
            return Action.D
        prev_round_idx = current_round_0_indexed - 1
        C_prev = np.sum(history.opponent_actions[prev_round_idx, :]) + history.my_actions[prev_round_idx]
        my_action_prev_bool = history.my_actions[prev_round_idx]
        my_action_prev = Action.C if my_action_prev_bool else Action.D
        condition_a = C_prev < self.n / self.k
        condition_b = current_round_0_indexed < self.r - 2
        condition_c = my_action_prev == Action.D
        if condition_a and condition_b and condition_c:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_159(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by strategically leveraging cooperation
    from other players while minimizing its own contributions. It probes for cooperation
    initially, exploits established cooperation, re-induces cooperation if it drops
    too low, and defects in the final rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = self.game_description.n_rounds
        self.initial_cooperation_rounds = min(3, math.floor(r / 2))
        self.end_game_defection_rounds = max(1, math.floor(r * 0.1))
        self.cooperation_threshold_high = 0.7
        self.cooperation_threshold_low = 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed <= self.initial_cooperation_rounds:
            return Action.C
        if current_round_1_indexed > self.game_description.n_rounds - self.end_game_defection_rounds:
            return Action.D
        total_others_cooperated = np.sum(history.opponent_actions)
        total_possible_cooperations = history.opponent_actions.size
        average_cooperation_rate_others = 0.0
        if total_possible_cooperations > 0:
            average_cooperation_rate_others = total_others_cooperated / total_possible_cooperations
        if average_cooperation_rate_others >= self.cooperation_threshold_high:
            return Action.D
        elif average_cooperation_rate_others < self.cooperation_threshold_low:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_160(BaseStrategy):
    """
    The "Adaptive Predator" strategy for the N-Player Public Goods Game is designed to be
    exploitative, adaptive, and robust. It aims to maximize its own payoff by free-riding
    on others' contributions while strategically investing in cooperation only when
    necessary to "prime the pump" and sustain the collective good.

    The strategy operates in three phases:
    1.  **Initial Probe (Round 1):** Cooperates to signal willingness and observe others' reactions.
    2.  **Final Exploitation (Last Round):** Defects to maximize immediate payoff, as there are
        no future rounds to influence.
    3.  **Intermediate Rounds:**
        -   If the observed cooperation from other players is below a `Minimum Exploitable Cooperation (MEC)`
            threshold AND there are enough rounds remaining before the `Endgame Horizon`, the strategy
            will cooperate to encourage future contributions.
        -   Otherwise (if cooperation from others is sufficient to exploit, OR the game is nearing its end),
            the strategy defects to maximize short-term gains.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Predator strategy with game parameters.
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.MEC = max(1, math.floor((self.n - 1) / 3))
        self.ENDGAME_HORIZON_FRACTION = 0.25

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on
        game parameters and historical play.
        """
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        c_others_prev = np.sum(history.opponent_actions[-1, :])
        num_rounds_remaining = self.r - current_round_1_indexed
        is_in_endgame_horizon = num_rounds_remaining <= self.r * self.ENDGAME_HORIZON_FRACTION
        if c_others_prev < self.MEC and (not is_in_endgame_horizon):
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_161(BaseStrategy):
    """
    An Adaptive Exploiter strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by probing for cooperation,
    then exploiting detected cooperation by defecting. If overall cooperation
    drops too low, it temporarily re-cooperates to re-induce contributions
    from others. It always defects in the final rounds of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.INITIAL_COOPERATE_ROUNDS = 2
        self.EXPLOIT_WINDOW = 3
        self.EXPLOIT_THRESHOLD_OTHERS_C = 0.5
        self.REINDUCTION_COOPERATE_ROUNDS = 2
        self.ENDGAME_DEFECT_ROUNDS = 2
        self.my_cooperation_rounds_left = 0
        self.history_total_C_count = []
        self.history_my_action_choice = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if history is not None and state.round_number > 0:
            prev_my_action_was_c = int(history.my_actions[-1])
            prev_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
            prev_round_total_C_count = prev_my_action_was_c + prev_opponent_cooperators
            self.history_total_C_count.append(prev_round_total_C_count)
        action_to_play = Action.D
        if current_round_1_indexed > self.r - self.ENDGAME_DEFECT_ROUNDS:
            action_to_play = Action.D
        elif current_round_1_indexed <= self.INITIAL_COOPERATE_ROUNDS:
            action_to_play = Action.C
        elif self.my_cooperation_rounds_left > 0:
            action_to_play = Action.C
            self.my_cooperation_rounds_left -= 1
        else:
            avg_other_cooperation_rate = 0.0
            rounds_available_in_history = len(self.history_total_C_count)
            if rounds_available_in_history > 0:
                start_index = max(0, rounds_available_in_history - self.EXPLOIT_WINDOW)
                total_other_cooperators_sum = 0
                rounds_considered_for_avg = 0
                for i in range(start_index, rounds_available_in_history):
                    rounds_considered_for_avg += 1
                    cooperators_excluding_self_in_round_i = self.history_total_C_count[i] - self.history_my_action_choice[i]
                    total_other_cooperators_sum += cooperators_excluding_self_in_round_i
                if rounds_considered_for_avg > 0:
                    avg_other_cooperation_rate = total_other_cooperators_sum / (rounds_considered_for_avg * (self.n - 1))
            if avg_other_cooperation_rate < self.EXPLOIT_THRESHOLD_OTHERS_C:
                action_to_play = Action.C
                self.my_cooperation_rounds_left = self.REINDUCTION_COOPERATE_ROUNDS - 1
            else:
                action_to_play = Action.D
        self.history_my_action_choice.append(1 if action_to_play == Action.C else 0)
        return action_to_play

class Strategy_EXPLOITATIVE_162(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize personal total payoff by leveraging other players'
    cooperation. It prioritizes free-riding by defecting when observed group cooperation
    is high. If cooperation collapses for a sustained period, it strategically
    cooperates to "re-seed" the public good, allowing for future exploitation.

    Key characteristics:
    - Initial probe: Cooperates in the first round to test the waters and potentially
      seed cooperation among others. This is a calculated short-term loss for potential
      long-term gain in exploitable public good.
    - Last round defection: Always defects in the final round, as there's no future
      to influence (standard backward induction logic).
    - Adaptive exploitation: When the fraction of total cooperators in the previous
      round is at or above a set 'cooperation_threshold_percentage', the strategy defects
      to maximize private gain while benefiting from the robust public good.
    - Strategic reset: If the observed cooperation falls below the threshold for a
      number of 'low_cooperation_reset_rounds' consecutive times, the strategy
      attempts a "reset" by cooperating itself. This is a calculated risk to encourage
      other conditional cooperators, aiming to rebuild the public good for future
      exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold_percentage = 0.4
        self.low_cooperation_reset_rounds = 2
        self.consecutive_low_cooperation_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == self.n_rounds - 1:
            return Action.D
        if current_round_number == 0:
            self.consecutive_low_cooperation_rounds = 0
            return Action.C
        else:
            total_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
            C_ratio = total_cooperators_prev_round / self.n_players
            if C_ratio >= self.cooperation_threshold_percentage:
                self.consecutive_low_cooperation_rounds = 0
                return Action.D
            else:
                self.consecutive_low_cooperation_rounds += 1
                if self.consecutive_low_cooperation_rounds >= self.low_cooperation_reset_rounds:
                    self.consecutive_low_cooperation_rounds = 0
                    return Action.C
                else:
                    return Action.D

class Strategy_EXPLOITATIVE_163(BaseStrategy):
    """
    Opportunistic Lurer-Defector (OLD) strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by initially cooperating to lure
    others, then exploiting that cooperation by defecting. It protects itself from
    being a "sucker" when cooperation is low and strategically cooperates to maintain
    moderate cooperation levels. It adapts its thresholds based on game parameters (n, k)
    and defects decisively in the final round to maximize immediate gain.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.min_coop_threshold = 1
        self.defection_incentive_ratio = 1.0 - self.k / self.n
        calculated_threshold = math.ceil((self.n - 1) * self.defection_incentive_ratio)
        self.exploit_coop_threshold = max(1, calculated_threshold)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.r - 1:
            return Action.D
        num_other_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        if num_other_cooperators_prev_round < self.min_coop_threshold:
            return Action.D
        elif num_other_cooperators_prev_round >= self.exploit_coop_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_164(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy for the N-Player Public Goods Game.
    This strategy prioritizes free-riding when possible, but also engages in strategic
    cooperation to maintain a healthy public good for future exploitation. It cooperates
    initially to probe, defects in the last round, and uses a set of rules in
    intermediate rounds to balance exploitation and preservation of the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.r - 1:
            return Action.D
        my_last_action = history.my_actions[-1]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = opponent_cooperators_prev_round
        if my_last_action:
            total_cooperators_prev_round += 1
        if total_cooperators_prev_round == self.n:
            return Action.D
        elif total_cooperators_prev_round == self.n - 1 and my_last_action == Action.C:
            return Action.D
        elif total_cooperators_prev_round >= self.n - 1 and my_last_action == Action.D:
            return Action.C
        elif total_cooperators_prev_round >= math.floor(self.n / 2.0):
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_165(BaseStrategy):
    """
    The Opportunistic Defector (OD) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own total payoff by primarily free-riding on the contributions
    of others. It defaults to defection, but strategically cooperates under specific conditions
    to prevent a complete collapse of cooperation, which would diminish its free-riding opportunities.
    This 're-seeding' of cooperation is done with the explicit intent of restoring a pool of cooperators
    to exploit.

    Key principles:
    1.  **Exploitation First:** Always defect if there's sufficient public good being generated by others.
    2.  **Adaptive Intervention:** Only cooperate when others' cooperation drops below a critical threshold,
        and only if the strategy itself hasn't cooperated very recently (in the immediately preceding round).
    3.  **Endgame Exploitation:** Always defect in the final round as there are no future consequences.
    4.  **Initial Probe:** Start with cooperation in the very first round to test the waters and potentially
        "seed" initial cooperation among opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.critical_cooperation_threshold_others = math.ceil((self.n_players - 1) / 2)
        self.rounds_since_last_own_cooperation = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.rounds_since_last_own_cooperation = 0
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        my_action_prev_round_was_C = history.my_actions[-1]
        cooperation_from_others_prev_round = np.sum(history.opponent_actions[-1, :])
        if not my_action_prev_round_was_C:
            self.rounds_since_last_own_cooperation += 1
        else:
            self.rounds_since_last_own_cooperation = 0
        if cooperation_from_others_prev_round >= self.critical_cooperation_threshold_others:
            return Action.D
        elif not my_action_prev_round_was_C:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_166(BaseStrategy):
    """
    The "Persistent Predator" is an exploitative strategy designed to maximize individual payoff
    in the N-Player Public Goods Game by predominantly defecting and free-riding on others' contributions.
    It only cooperates as a calculated, temporary investment to re-establish a pool of cooperators,
    from which it can then continue to extract value.

    This strategy operates under the assumption that other players might be willing to cooperate.
    It is adaptive to the overall level of cooperation observed in the previous round.

    Decision Rules:
    1.  Round 1 (0-indexed round 0): Always Defect (D).
    2.  Final Round (0-indexed round n_rounds - 1): Always Defect (D).
    3.  Intermediate Rounds (0-indexed rounds 1 to n_rounds - 2):
        -   Default to Defect (D).
        -   Cooperate (C) ONLY if two conditions are met:
            a.  `Total_C_prev == 0`: No players (including self) cooperated in the previous round.
            b.  `r - current_round > 2` (or `remaining_rounds_after_current > 2` for 0-indexed):
                There are at least three more rounds remaining *after* the current one,
                allowing sufficient future opportunity for exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        my_action_prev = history.my_actions[-1]
        opponent_actions_prev = history.opponent_actions[-1, :]
        total_c_prev = int(my_action_prev) + sum((int(a) for a in opponent_actions_prev))
        remaining_rounds_after_current = self.n_rounds - 1 - current_round_0_indexed
        condition_A_coop_collapse = total_c_prev == 0
        condition_B_sufficient_opportunity = remaining_rounds_after_current > 2
        if condition_A_coop_collapse and condition_B_sufficient_opportunity:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_167(BaseStrategy):
    """
    The Ruthless Free-Rider (RFR) strategy for an N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by strategically exploiting the
    cooperative tendencies of other players. It probes for cooperation, free-rides
    when enough opponents cooperate, strategically re-seeds cooperation if it wanes,
    and punishes severe defection to avoid being a 'sucker'. It always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.InitialCooperationRounds: int = 1
        self.CooperationMonitorWindow: int = 3
        self.FreeRideThreshold: float = 0.5
        self.PunishmentThreshold: float = 0.2
        self.PunishmentDuration: int = 2
        self.history_other_cooperators: list[int] = []
        self.punishment_active: bool = False
        self.punishment_rounds_left: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        round_1_indexed = state.round_number + 1
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if round_1_indexed > 1 and history is not None:
            other_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            self.history_other_cooperators.append(other_cooperators_prev_round)
            if len(self.history_other_cooperators) > self.CooperationMonitorWindow:
                self.history_other_cooperators.pop(0)
        my_action: Action
        if round_1_indexed == n_rounds:
            my_action = Action.D
        elif self.punishment_active:
            my_action = Action.D
            self.punishment_rounds_left -= 1
            if self.punishment_rounds_left == 0:
                self.punishment_active = False
        elif round_1_indexed <= self.InitialCooperationRounds:
            my_action = Action.C
        else:
            avg_coop_rate_others = 0.0
            if len(self.history_other_cooperators) > 0:
                sum_cooperators_in_window = sum(self.history_other_cooperators)
                total_possible_cooperators_in_window = len(self.history_other_cooperators) * (n_players - 1)
                if total_possible_cooperators_in_window > 0:
                    avg_coop_rate_others = sum_cooperators_in_window / total_possible_cooperators_in_window
            if avg_coop_rate_others >= self.FreeRideThreshold:
                my_action = Action.D
            elif avg_coop_rate_others < self.PunishmentThreshold:
                self.punishment_active = True
                self.punishment_rounds_left = self.PunishmentDuration
                my_action = Action.D
            else:
                my_action = Action.C
        return my_action

class Strategy_EXPLOITATIVE_168(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed for the N-Player Public Goods Game.
    It begins with cooperation to probe for collaborative potential. In subsequent
    rounds, it adapts its behavior based on the total number of cooperators in the
    previous round. It defects aggressively when cooperation is high (n-1 or more
    cooperators) to free-ride. It strategically cooperates to maintain a moderate
    level of public good (above ceil(n/2) but below n-1 cooperators), treating
    cooperation as an investment. When cooperation is low (ceil(n/2) or fewer
    cooperators), it defects to minimize personal losses and signal dissatisfaction.
    In the final round, it always defects for pure self-interest.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.THRESHOLD_EXPLOIT = self.n_players - 1
        self.THRESHOLD_MAINTAIN = math.ceil(self.n_players / 2.0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        total_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if total_cooperators_prev_round >= self.THRESHOLD_EXPLOIT:
            return Action.D
        elif total_cooperators_prev_round > self.THRESHOLD_MAINTAIN and total_cooperators_prev_round < self.THRESHOLD_EXPLOIT:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_169(BaseStrategy):
    """
    The Manipulator: An exploitative strategy for the N-Player Public Goods Game.
    It strategically encourages a critical mass of cooperation from others
    through initial investment (Patience Phase) or re-seeding, then exploits
    it by defecting. It protects itself by entering a futility mode if others
    consistently fail to cooperate.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.is_futility_mode = False
        self.consecutive_zero_others_coop = 0
        self.PATIENCE_PHASE_LENGTH = min(max(1, math.floor(r / 5)), 3)
        self.EXPLOIT_TRIGGER_OTHERS_COUNT = max(1, math.floor((n - 1) / 2))
        self.FUTILITY_THRESHOLD_COUNT = 0
        self.FUTILITY_ROUNDS_TRIGGER = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if self.is_futility_mode:
            return Action.D
        if current_round_0_indexed < self.PATIENCE_PHASE_LENGTH:
            return Action.C
        coop_from_others_prev = sum(history.opponent_actions[-1, :])
        if coop_from_others_prev == self.FUTILITY_THRESHOLD_COUNT:
            self.consecutive_zero_others_coop += 1
        else:
            self.consecutive_zero_others_coop = 0
        if self.consecutive_zero_others_coop >= self.FUTILITY_ROUNDS_TRIGGER:
            self.is_futility_mode = True
            return Action.D
        if coop_from_others_prev >= self.EXPLOIT_TRIGGER_OTHERS_COUNT:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_170(BaseStrategy):
    """
    The "Strategic Free-Rider" (SFR) strategy for the N-Player Public Goods Game.
    This strategy prioritizes personal payoff maximization by free-riding on
    the contributions of others. It only contributes when strategically necessary
    to prevent the collapse of the public good, which would eliminate future
    free-riding opportunities. It reverts to pure defection if cooperation is
    too low to exploit or if re-seeding attempts fail.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.COOP_THRESHOLD_FOR_EXPLOITATION = math.ceil(self.n / self.k)
        self.RESURRECTION_THRESHOLD = 1
        self.my_last_action: Action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            my_current_action = Action.D
            self.my_last_action = my_current_action
            return my_current_action
        if current_round_1_indexed == self.r:
            my_current_action = Action.D
            self.my_last_action = my_current_action
            return my_current_action
        C_total_t_minus_1 = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if C_total_t_minus_1 >= self.COOP_THRESHOLD_FOR_EXPLOITATION:
            my_current_action = Action.D
        elif C_total_t_minus_1 < self.RESURRECTION_THRESHOLD:
            my_current_action = Action.D
        elif self.my_last_action == Action.D:
            my_current_action = Action.C
        else:
            my_current_action = Action.D
        self.my_last_action = my_current_action
        return my_current_action

class Strategy_EXPLOITATIVE_171(BaseStrategy):
    """
    The Smart Exploiter strategy defaults to defection, aiming to free-ride. It strategically
    cooperates only when the collective cooperation from other players drops below a dynamic
    threshold (based on 'k') to revive it, acting as a temporary investment. It always defects
    in the first and last rounds and explicitly avoids being a 'sucker' by never cooperating
    if no others contribute.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.has_cooperated_in_last_round_to_revive = False
        self.cooperation_maintenance_threshold = max(1, round(self.game_description.k - 1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        n_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        k_factor = self.game_description.k
        my_action = Action.D
        if current_round_0_indexed == 0:
            my_action = Action.D
            self.has_cooperated_in_last_round_to_revive = False
        elif current_round_0_indexed == n_rounds - 1:
            my_action = Action.D
            self.has_cooperated_in_last_round_to_revive = False
        else:
            my_action_prev_round_bool = history.my_actions[-1]
            total_cooperators_from_opponents_prev = np.sum(history.opponent_actions[-1, :])
            num_cooperators_from_others_prev = total_cooperators_from_opponents_prev
            if num_cooperators_from_others_prev <= 0:
                my_action = Action.D
                self.has_cooperated_in_last_round_to_revive = False
            elif self.has_cooperated_in_last_round_to_revive:
                my_action = Action.D
                self.has_cooperated_in_last_round_to_revive = False
            elif num_cooperators_from_others_prev < self.cooperation_maintenance_threshold:
                my_action = Action.C
                self.has_cooperated_in_last_round_to_revive = True
            else:
                my_action = Action.D
                self.has_cooperated_in_last_round_to_revive = False
        return my_action

class Strategy_EXPLOITATIVE_172(BaseStrategy):
    """
    The "Grumpy Free-Rider" strategy for the N-Player Public Goods Game.

    This strategy aims to be exploitative, adaptive, and robust. It aggressively
    free-rides when others cooperate, but will strategically cooperate for an
    initial "probe" in Round 1 and a limited number of "re-seeding" attempts
    if cooperation completely collapses, hoping to spark it back up for
    future exploitation. If these attempts fail, it defaults to permanent
    defection to minimize losses.
    """
    MAX_RESEED_ATTEMPTS = 1

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_attempt_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_idx = current_round - 1
        C_others_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        if C_others_prev > 0:
            self.cooperation_attempt_count = 0
            return Action.D
        elif self.cooperation_attempt_count < self.MAX_RESEED_ATTEMPTS:
            self.cooperation_attempt_count += 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_173(BaseStrategy):
    """
    The Strategic Opportunist strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own total payoff by defaulting to defection (exploitation)
    whenever there is a sufficient pool of cooperators. However, it strategically contributes
    (cooperates) under two specific conditions: to initiate potential cooperation in the first round,
    and to prevent the complete collapse of the public good if cooperation levels drop too low.
    It explicitly avoids being a "sucker" by not cooperating if no one else is.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.REBUILD_THRESHOLD_PROPORTION: float = 0.33
        self.ENDGAME_WINDOW: int = 1
        self.my_previous_action: Action | None = None
        self.N_c_total_prev_round: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed > 1:
            self.my_previous_action = Action.C if history.my_actions[-1] else Action.D
            num_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
            self.N_c_total_prev_round = num_opponent_cooperators + (1 if self.my_previous_action == Action.C else 0)
        if current_round_1_indexed >= self.r - self.ENDGAME_WINDOW:
            return Action.D
        elif current_round_1_indexed == 1:
            return Action.C
        else:
            num_my_prev_coop = 1 if self.my_previous_action == Action.C else 0
            N_c_others_prev_round = self.N_c_total_prev_round - num_my_prev_coop
            if N_c_others_prev_round == 0:
                return Action.D
            rebuild_threshold = math.floor(self.n * self.REBUILD_THRESHOLD_PROPORTION)
            if self.N_c_total_prev_round < rebuild_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_174(BaseStrategy):
    """
    The Adaptive Leech strategy aims to maximize its own payoff by strategically exploiting others' cooperation
    while minimizing its own contributions. It operates by probing for cooperation, then carefully
    sustaining it just enough to keep others contributing, and finally defecting outright in the end-game.

    Core Principles:
    1.  Initial Investment: Proactively cooperate for a few initial rounds to "prime the pump" and
        encourage others to cooperate, establishing a baseline for future exploitation.
    2.  Adaptive Exploitation: Monitor the recent cooperation behavior of other players. If they are
        cooperating sufficiently, exploit their generosity by defecting most of the time, but cooperate
        probabilistically to prevent a complete collapse of collective contribution. If they are not
        cooperating enough, defect to avoid being a "sucker" and to incentivize them to cooperate more.
    3.  End-Game Betrayal: Always defect in the final round(s) as the game unravels due to the finite horizon.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        self.initial_coop_rounds = max(1, min(2, r // 4))
        self.cooperation_threshold = max(0.2, min(0.8, 1 - k / n))
        self.P_coop_if_high_coop = min(0.8, max(0.2, k / n * 1.5))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == r - 1:
            return Action.D
        if current_round_0_indexed < self.initial_coop_rounds:
            return Action.C
        history_window_size = max(3, min(n - 1, current_round_0_indexed))
        total_other_cooperations_in_window = 0
        num_rounds_considered_in_window = 0
        start_idx = current_round_0_indexed - 1
        end_idx = max(0, current_round_0_indexed - history_window_size)
        for t_idx in range(start_idx, end_idx - 1, -1):
            num_rounds_considered_in_window += 1
            total_other_cooperations_in_window += np.sum(history.opponent_actions[t_idx, :])
        if num_rounds_considered_in_window == 0:
            return Action.D
        max_possible_other_cooperations_in_window = (n - 1) * num_rounds_considered_in_window
        recent_cooperation_rate_of_others = total_other_cooperations_in_window / max_possible_other_cooperations_in_window
        if recent_cooperation_rate_of_others >= self.cooperation_threshold:
            if random.random() < self.P_coop_if_high_coop:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_175(BaseStrategy):
    """
    The "Predator-Prey" strategy is designed to be highly exploitative, maximizing
    individual payoff by selectively defecting when others cooperate, and strategically
    cooperating only to cultivate an environment conducive to future exploitation.
    It adapts to the observed level of cooperation in the game, avoiding being a "sucker"
    while being prepared to "re-seed" cooperation if the "prey pool" (cooperating players)
    diminishes too much.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.initial_probe_rounds = 2
        self.cooperation_threshold_percentage = 0.2
        self.cooperation_reseed_interval = max(3, int(self.n_rounds / 5))
        self.last_self_cooperation_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed <= self.initial_probe_rounds:
            self.last_self_cooperation_round = current_round_1_indexed
            return Action.C
        num_others_cooperated_prev_round = sum(history.opponent_actions[-1, :])
        num_other_players = self.n_players - 1
        min_coop_for_exploit = max(1, round(num_other_players * self.cooperation_threshold_percentage))
        if num_others_cooperated_prev_round >= min_coop_for_exploit:
            return Action.D
        elif current_round_1_indexed - self.last_self_cooperation_round >= self.cooperation_reseed_interval:
            self.last_self_cooperation_round = current_round_1_indexed
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_176(BaseStrategy):
    """
    The "Dynamic Exploiter" strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by primarily defaulting to free-riding
    on others' contributions. It strategically and sparingly cooperates only as a calculated
    "investment" to stimulate or maintain a sufficient level of public good, ensuring
    there's something worthwhile to exploit in subsequent rounds.

    Core principles:
    1. Default to Defect: Always try to free-ride.
    2. Last-Round Exploitation: Always Defect in the final round (backward induction).
    3. Conditional Cooperation: Cooperate only when the public good is critically low
       (below COOP_LOW_THRESHOLD), enough rounds remain for the investment to pay off,
       and it's been a while since my last cooperation probe. This is an investment
       to re-ignite collective contribution for future exploitation, not out of altruism.
    4. Adaptive Free-Riding: When cooperation is robust (above COOP_HIGH_THRESHOLD),
       free-ride aggressively.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.total_cooperators_history: list[int] = []
        self.rounds_since_my_last_C: int = 0
        self.COOP_LOW_THRESHOLD = max(1, math.floor(self.n / self.k))
        self.COOP_HIGH_THRESHOLD = max(self.COOP_LOW_THRESHOLD + 1, math.floor(self.n * 0.6))
        self.PROBE_INTERVAL = max(3, math.floor(self.r / 5))
        self.MIN_ROUNDS_FOR_PROBE_BENEFIT = self.PROBE_INTERVAL + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        my_action_for_this_round = Action.D
        if current_round_0_indexed > 0 and history is not None:
            prev_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
            prev_total_opponent_cooperators = np.sum(prev_round_opponent_actions)
            prev_my_action_was_C = history.my_actions[current_round_0_indexed - 1]
            prev_total_cooperators = prev_total_opponent_cooperators + (1 if prev_my_action_was_C else 0)
            self.total_cooperators_history.append(prev_total_cooperators)
            if not prev_my_action_was_C:
                self.rounds_since_my_last_C += 1
            else:
                self.rounds_since_my_last_C = 0
        if current_round_0_indexed == self.r - 1:
            my_action_for_this_round = Action.D
        elif current_round_0_indexed == 0:
            my_action_for_this_round = Action.D
        else:
            num_recent_rounds_to_consider = min(len(self.total_cooperators_history), 3)
            avg_cooperators_recent = 0.0
            if num_recent_rounds_to_consider > 0:
                avg_cooperators_recent = np.mean(self.total_cooperators_history[-num_recent_rounds_to_consider:])
            my_action_for_this_round = Action.D
            if avg_cooperators_recent >= self.COOP_HIGH_THRESHOLD:
                my_action_for_this_round = Action.D
            elif avg_cooperators_recent < self.COOP_LOW_THRESHOLD:
                if self.rounds_since_my_last_C >= self.PROBE_INTERVAL and self.r - (current_round_0_indexed + 1) >= self.MIN_ROUNDS_FOR_PROBE_BENEFIT:
                    if avg_cooperators_recent < self.n - 1:
                        my_action_for_this_round = Action.C
            else:
                my_action_for_this_round = Action.D
        return my_action_for_this_round

class Strategy_EXPLOITATIVE_177(BaseStrategy):
    """
    An opportunistic free-rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by defaulting to defection (free-riding)
    when a sufficient proportion of other players cooperate. It will only cooperate itself
    if the perceived level of cooperation from others falls below a dynamically calculated
    threshold, in an attempt to "re-seed" the public good for future exploitation.

    Key behaviors:
    - Cooperates in the first round to probe the environment and encourage initial cooperation.
    - Defects in the last round to maximize immediate gain, exploiting the end-game effect.
    - In intermediate rounds, it observes the proportion of *other players* who cooperated
      in the previous round.
    - If this proportion meets or exceeds its `cooperation_threshold`, it defects to free-ride.
    - Otherwise (insufficient cooperation), it cooperates to encourage public good
      re-establishment, hoping to create future free-riding opportunities.
    - The `cooperation_threshold` is dynamically set based on `1.0 / (num_players - 1)`,
      capped at a minimum of 0.1, making it adaptive to group size.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold: float = max(0.1, 1.0 / (self.game_description.n_players - 1.0))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        num_players = self.game_description.n_players
        if current_round == 0:
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        num_others_cooperating_prev_round = np.sum(history.opponent_actions[-1, :])
        proportion_others_cooperating = float(num_others_cooperating_prev_round) / (num_players - 1)
        if proportion_others_cooperating >= self.cooperation_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_178(BaseStrategy):
    """
    The Adaptable Leech strategy dynamically assesses the cooperativeness of the environment.
    It begins with a probe phase where it cooperates to gather information.
    Based on the observed cooperation level from other players during this probe,
    it transitions into one of two persistent modes:
    1. EXPLOIT mode: If sufficient cooperation is detected, the Leech consistently defects
       to free-ride on others' contributions.
    2. DEFAULT_DEFECT mode: If cooperation is low, the Leech defaults to defection
       to minimize its own losses.
    In the final round, the strategy always defects due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.strategy_mode = 'UNSET'
        self.COOPERATION_THRESHOLD_PROPORTION = 0.5
        self.PROBE_ROUNDS_COUNT = 0
        if self.r >= 4:
            self.PROBE_ROUNDS_COUNT = 2
        elif self.r == 3:
            self.PROBE_ROUNDS_COUNT = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.r - 1:
            return Action.D
        if self.PROBE_ROUNDS_COUNT > 0 and current_round_0_indexed < self.PROBE_ROUNDS_COUNT:
            return Action.C
        if self.strategy_mode == 'UNSET' and current_round_0_indexed >= self.PROBE_ROUNDS_COUNT:
            if self.PROBE_ROUNDS_COUNT == 0:
                self.strategy_mode = 'DEFAULT_DEFECT'
            else:
                probe_opponent_actions = history.opponent_actions[0:self.PROBE_ROUNDS_COUNT, :]
                total_other_cooperations_in_probe = np.sum(probe_opponent_actions)
                max_possible_other_cooperation_points = self.PROBE_ROUNDS_COUNT * (self.n - 1)
                average_cooperation_proportion = 0
                if max_possible_other_cooperation_points > 0:
                    average_cooperation_proportion = total_other_cooperations_in_probe / max_possible_other_cooperation_points
                if average_cooperation_proportion >= self.COOPERATION_THRESHOLD_PROPORTION:
                    self.strategy_mode = 'EXPLOIT'
                else:
                    self.strategy_mode = 'DEFAULT_DEFECT'
        if self.strategy_mode == 'EXPLOIT':
            return Action.D
        elif self.strategy_mode == 'DEFAULT_DEFECT':
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_179(BaseStrategy):
    """
    The "Calculated Opportunist" strategy aims to maximize its own total payoff by defaulting to defection (free-riding)
    whenever possible. It employs a minimal, strategic cooperative move in the very first round to probe the environment,
    and then selectively "re-seeds" cooperation in later rounds only under specific, favorable conditions where such an
    investment is likely to lead to future exploitation opportunities. It avoids being a "sucker" by quickly reverting
    to defection if conditions for productive cooperation or exploitation are not met or cease to exist.
    """
    COOPERATION_EXPLOIT_THRESHOLD = 1
    K_N_IMPACT_THRESHOLD = 0.5
    ROUNDS_FOR_RETURN_THRESHOLD = 2

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with the game parameters.

        Args:
            game_description (PublicGoodsDescription): An object containing game parameters
                                                      (n_players, n_rounds, k).
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate or Defect) for the current round based on
        the "Calculated Opportunist" strategy.

        Args:
            state (GameState): An object containing the current round number.
            history (None | PlayerHistory): An object containing past actions and payoffs.
                                           It is None for the very first round.

        Returns:
            Action: The chosen action, either Action.C (Cooperate) or Action.D (Defect).
        """
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        n_coop_others_prev = sum(history.opponent_actions[-1, :])
        if n_coop_others_prev >= self.COOPERATION_EXPLOIT_THRESHOLD:
            return Action.D
        n_rounds_remaining = self.game_description.n_rounds - current_round_1_indexed
        k_n_ratio = self.game_description.k / self.game_description.n_players
        if n_coop_others_prev == 0 and k_n_ratio > self.K_N_IMPACT_THRESHOLD and (n_rounds_remaining > self.ROUNDS_FOR_RETURN_THRESHOLD):
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_180(BaseStrategy):
    """
    The Opportunistic Free-Rider (OFR) strategy.

    This strategy is built on the core principle of individual payoff maximization through
    defection (free-riding) but incorporates a strategic, calculated risk of cooperation
    to "prime the pump" and induce others to contribute, only to then exploit their contributions.
    It is designed to be highly exploitative, adaptive, and robust to various opponent behaviors.

    Core Philosophy: Prioritize defection (individual rationality) but occasionally
    "test the waters" with cooperation when others are not cooperating much, hoping to
    trigger more contributions from conditional cooperators that can be free-ridden upon.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the OpportunisticFreeRider strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.cooperation_inducement_threshold = 0.15
        self.test_cooperation_interval = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action ('C' or 'D') for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs for this player and opponents.
                     Is None for the very first round (round_number = 0).

        Returns:
            Action.C for Cooperate or Action.D for Defect.
        """
        current_round_1_indexed = state.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if current_round_1_indexed == r:
            return Action.D
        chosen_action = Action.D
        if state.round_number > 0:
            total_others_cooperated_count = np.sum(history.opponent_actions)
            total_possible_other_actions = history.opponent_actions.size
            avg_others_cooperated_rate = 0.0
            if total_possible_other_actions > 0:
                avg_others_cooperated_rate = total_others_cooperated_count / total_possible_other_actions
            if avg_others_cooperated_rate < self.cooperation_inducement_threshold:
                if current_round_1_indexed % self.test_cooperation_interval == 0:
                    num_rounds_to_check = self.test_cooperation_interval - 1
                    start_check_idx = max(0, state.round_number - num_rounds_to_check)
                    my_recent_cooperations_count = np.sum(history.my_actions[start_check_idx:])
                    if my_recent_cooperations_count == 0:
                        chosen_action = Action.C
        return chosen_action

class Strategy_EXPLOITATIVE_181(BaseStrategy):
    """
    The Adaptive Leech strategy aims to free-ride on others' contributions.
    It starts with cooperation to encourage others, then defects when there's
    sufficient public good to exploit. If cooperation collapses, it makes limited
    attempts to re-seed it before reverting to defection if its efforts are not reciprocated.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.low_cooperation_rounds_seen = 0
        self.exploitation_threshold_NC = (self.n_players - 1) * 0.5
        self.patience_limit = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == self.n_rounds:
            return Action.D
        N_C_others_prev = sum(history.opponent_actions[-1, :])
        if N_C_others_prev >= self.exploitation_threshold_NC:
            self.low_cooperation_rounds_seen = 0
            return Action.D
        else:
            self.low_cooperation_rounds_seen += 1
            if self.low_cooperation_rounds_seen <= self.patience_limit:
                return Action.C
            else:
                self.low_cooperation_rounds_seen = 0
                return Action.D

class Strategy_EXPLOITATIVE_182(BaseStrategy):
    """
    The Opportunistic Exploiter strategy aims to maximize its payoff by primarily
    free-riding. It defaults to Defect but strategically cooperates when public
    goods provision falls too low, to re-engage other players for future exploitation.
    It strictly applies backward induction for the final rounds, always defecting
    when future influence is irrelevant.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.CRT = max(1, math.floor(self.n_players / 3))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_t = state.round_number + 1
        if current_t == 1:
            return Action.D
        if current_t >= self.n_rounds - 2:
            return Action.D
        previous_round_index = state.round_number - 1
        C_prev_opponents = np.sum(history.opponent_actions[previous_round_index, :])
        C_prev_me = int(history.my_actions[previous_round_index])
        C_prev = C_prev_opponents + C_prev_me
        if C_prev <= self.CRT:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_183(BaseStrategy):
    """
    The Opportunistic Parasite strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by exploiting others'
    cooperation. It starts with an initial cooperative gesture, then
    free-rides by defecting when there's sufficient public good. If the
    level of cooperation drops too low, it temporarily cooperates to
    re-seed the public good for future exploitation. In the final rounds,
    it defects unequivocally to maximize end-game gains.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.initial_cooperation_rounds = 1
        self.cooperation_boost_threshold = 2
        self.effective_endgame_rounds = max(1, min(self.n_rounds - 1, 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed < self.initial_cooperation_rounds:
            return Action.C
        if current_round_0_indexed >= self.n_rounds - self.effective_endgame_rounds:
            return Action.D
        cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        if cooperators_prev_round == 0:
            return Action.D
        elif 0 < cooperators_prev_round < self.cooperation_boost_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_184(BaseStrategy):
    """
    The Opportunistic Free-Rider (OFR) strategy aims to extract maximum value by defaulting to defection.
    It strategically cooperates in the first round to "test the waters" and then periodically to "re-bait"
    or "re-induce" cooperation from others, provided the overall level of cooperation is sufficient.
    If opponents are not cooperative enough, OFR prioritizes self-preservation by defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.r_rounds = game_description.n_rounds
        self.rounds_since_last_cooperation_by_us = 0
        self.min_other_cooperators_threshold = math.ceil((self.n_players - 1) / 2)
        self.max_defections_before_re_cooperate = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.r_rounds
        if current_round_0_indexed == 0:
            self.rounds_since_last_cooperation_by_us = 0
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        num_other_cooperators_prev_round = sum(history.opponent_actions[current_round_0_indexed - 1, :])
        if num_other_cooperators_prev_round < self.min_other_cooperators_threshold:
            self.rounds_since_last_cooperation_by_us += 1
            return Action.D
        elif self.rounds_since_last_cooperation_by_us >= self.max_defections_before_re_cooperate:
            self.rounds_since_last_cooperation_by_us = 0
            return Action.C
        else:
            self.rounds_since_last_cooperation_by_us += 1
            return Action.D

class Strategy_EXPLOITATIVE_185(BaseStrategy):
    """
    "The Calculated Free-Rider" strategy for the N-Player Public Goods Game.
    This strategy is designed to be highly adaptive and exploitative. It aims to
    identify opportunities to free-ride on others' cooperation, but also
    strategically "invests" in cooperation to prevent the game from spiraling
    into an all-defect equilibrium, which would limit its own potential gains.
    It utilizes a "carrot and stick" approach, offering conditional cooperation
    and swift defection based on observed behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.Exploit_Threshold = max(1, math.floor((self.n - 1) / 2))
        self.N_rounds_to_retest = max(1, math.floor(self.r / 5))
        self.rounds_since_last_C_attempt: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.rounds_since_last_C_attempt = 0
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        C_others_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
        Our_Action_Prev = history.my_actions[state.round_number - 1]
        if C_others_prev >= self.Exploit_Threshold:
            self.rounds_since_last_C_attempt += 1
            return Action.D
        elif Our_Action_Prev == True:
            self.rounds_since_last_C_attempt += 1
            return Action.D
        elif self.rounds_since_last_C_attempt >= self.N_rounds_to_retest:
            self.rounds_since_last_C_attempt = 0
            return Action.C
        else:
            self.rounds_since_last_C_attempt += 1
            return Action.D

class Strategy_EXPLOITATIVE_186(BaseStrategy):
    """
    The "Adaptive Predator" strategy is designed to maximize its own total payoff in the
    N-Player Public Goods Game by strategically leveraging the observed cooperation levels
    of other players. It balances opportunistic exploitation (free-riding) with conditional
    cooperation (investing in the public good) to maintain an environment where free-riding
    remains profitable, while avoiding being a "sucker."

    This strategy uses adaptive thresholds (T_punish and T_exploit) based on the game
    parameters `n` (number of players) and `k` (multiplication factor) to dynamically
    decide when to cooperate or defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Predator strategy with game parameters.
        Calculates the dynamic thresholds (T_punish and T_exploit) which depend
        only on n and k, and are constant throughout the game.
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.T_punish = math.ceil(self.n / self.k)
        self.T_exploit = self.n - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round based on
        the Adaptive Predator strategy rules.
        """
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.r - 1:
            return Action.D
        cooperators_opponents_prev = sum(history.opponent_actions[-1, :])
        my_action_prev = 1 if history.my_actions[-1] else 0
        c_prev = cooperators_opponents_prev + my_action_prev
        if c_prev < self.T_punish:
            return Action.D
        elif c_prev > self.T_exploit:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_187(BaseStrategy):
    """
    The "Adaptive Predator" strategy for the N-Player Public Goods Game.

    This strategy is designed to be exploitative, adaptive, and robust. It
    strategically alternates between probing for cooperation, aggressively
    exploiting it when present, and attempting to re-initiate it when the
    public good diminishes to an unprofitable level. It leverages the repeated
    nature of the game to create opportunities for free-riding.

    Decision Rules (0-indexed rounds):
    - Round 0 (First Round): Cooperate (C) to probe for initial cooperation.
    - Last Round (game_description.n_rounds - 1): Defect (D) due to end-game effect.
    - Intermediate Rounds:
        - Calculate C_prev: total number of cooperators in the immediately preceding round (t-1),
          including this player's own contribution.
        - If C_prev >= k (game_description.k): Defect (D) to free-ride on abundant cooperation.
        - Else (C_prev < k): Cooperate (C) to re-seed or regenerate the public good
          for future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description (PublicGoodsDescription): An object containing
                                                       n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round.

        Args:
            state (GameState): The current state of the game, including the round number.
            history (None | PlayerHistory): The history of actions and payoffs from
                                            previous rounds, or None if it's the first round.

        Returns:
            Action: The chosen action (Action.C for Cooperate, Action.D for Defect).
        """
        n_rounds = self.game_description.n_rounds
        k_factor = self.game_description.k
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        cooperators_from_opponents_prev_round = sum(history.opponent_actions[-1, :])
        my_action_prev_round = history.my_actions[-1]
        c_prev = cooperators_from_opponents_prev_round + my_action_prev_round
        if c_prev >= k_factor:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_188(BaseStrategy):
    """
    The Adaptive Opportunist strategy is designed to maximize its individual payoff by primarily free-riding.
    It strategically cooperates only when the overall level of cooperation is so low that continuing to defect
    would risk the collapse of the public good, thereby eliminating future opportunities for exploitation.
    It's a calculated self-interest, not genuine cooperation.

    Decision Rules:
    - Round 1: Cooperate (C) to 'prime the pump'.
    - Intermediate Rounds (2 to r-1):
      - Calculate critical mass threshold M = ceil(n / 3).
      - If total cooperators in the previous round (N_C_t-1) < M: Play Cooperate (C) to prevent collapse.
      - If N_C_t-1 >= M: Play Defect (D) to free-ride.
    - Last Round (r): Defect (D) for pure self-interest.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.critical_mass_threshold = math.ceil(self.game_description.n_players / 3)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            previous_round_idx = state.round_number - 1
            num_cooperators_previous_round = int(history.my_actions[previous_round_idx]) + sum(history.opponent_actions[previous_round_idx, :])
            if num_cooperators_previous_round < self.critical_mass_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_189(BaseStrategy):
    """
    The "Opportunistic Provocateur" is an exploitative strategy designed to maximize its own payoff
    by predominantly free-riding on the contributions of others. It strategically cooperates
    only when cooperation levels are too low to exploit, aiming to "reset" the game
    and provoke others back into contributing so it can resume free-riding.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        num_players = self.game_description.n_players
        if current_round_1_indexed == 1:
            self.my_consecutive_defections = 1
            return Action.D
        if current_round_1_indexed == total_rounds:
            return Action.D
        other_cooperators_prev_round = sum(history.opponent_actions[state.round_number - 1, :])
        cooperation_threshold_for_free_riding = max(1, math.floor((num_players - 1) / 4))
        if other_cooperators_prev_round >= cooperation_threshold_for_free_riding:
            self.my_consecutive_defections += 1
            return Action.D
        else:
            min_defections_for_reboot = max(1, math.floor(total_rounds / 3))
            prolonged_self_defection_condition = self.my_consecutive_defections >= min_defections_for_reboot
            not_too_late_in_game_condition = current_round_1_indexed < total_rounds - math.floor(total_rounds / 4)
            existence_of_some_cooperators_condition = other_cooperators_prev_round > 0
            if prolonged_self_defection_condition and not_too_late_in_game_condition and existence_of_some_cooperators_condition:
                self.my_consecutive_defections = 0
                return Action.C
            else:
                self.my_consecutive_defections += 1
                return Action.D

class Strategy_EXPLOITATIVE_190(BaseStrategy):
    """
    The Opportunistic Parasite aims to maximize its individual payoff by leveraging others' willingness
    to cooperate. It defaults to free-riding (defecting) when there is sufficient cooperation to exploit.
    However, it strategically contributes (cooperates) when the collective cooperation level drops too low,
    threatening the existence of a profitable public good to free-ride upon. It also accounts for end-game
    rationality and for scenarios where opponents are consistently uncooperative.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.consecutive_zero_coop_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == 1:
            return Action.C
        if current_round_t == self.r:
            return Action.D
        num_others_cooperated_in_prev_round = np.sum(history.opponent_actions[-1, :])
        if num_others_cooperated_in_prev_round == 0:
            self.consecutive_zero_coop_rounds += 1
        else:
            self.consecutive_zero_coop_rounds = 0
        total_C_others_so_far = np.sum(history.opponent_actions)
        num_past_rounds = state.round_number
        max_possible_C_others_so_far = (self.n - 1) * num_past_rounds
        avg_coop_rate_others = total_C_others_so_far / max_possible_C_others_so_far
        if self.consecutive_zero_coop_rounds >= 3:
            return Action.D
        elif avg_coop_rate_others >= self.k / self.n:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_191(BaseStrategy):
    """
    The Adaptive Free-Rider strategy identifies and exploits periods of cooperation among other players.
    It will temporarily cooperate when necessary to "seed" or "re-seed" collective cooperation,
    only to revert to free-riding once the public good is sufficiently sustained by others.
    It always defects in the final round, leveraging backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_coop_rounds = 1
        self.cooperation_threshold = 0.5
        self.memory_window = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_0_indexed_round = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_0_indexed_round == total_rounds - 1:
            return Action.D
        if current_0_indexed_round < self.initial_coop_rounds:
            return Action.C
        other_cooperation_proportions = []
        start_history_idx = max(0, current_0_indexed_round - self.memory_window)
        for round_idx in range(start_history_idx, current_0_indexed_round):
            num_cooperating_opponents = np.sum(history.opponent_actions[round_idx, :])
            num_other_players = n_players - 1
            if num_other_players == 0:
                proportion_cooperating_others = 0.0
            else:
                proportion_cooperating_others = num_cooperating_opponents / num_other_players
            other_cooperation_proportions.append(proportion_cooperating_others)
        if not other_cooperation_proportions:
            avg_P_C_others_recent = 0.0
        else:
            avg_P_C_others_recent = np.mean(other_cooperation_proportions)
        if avg_P_C_others_recent >= self.cooperation_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_192(BaseStrategy):
    """
    Cyclical Probing Free-Rider (CPF) strategy for N-Player Public Goods Game.

    This strategy prioritizes free-riding by defaulting to defection. When cooperation from
    other players diminishes, it strategically "probes" by cooperating for a single round
    to attempt to re-stimulate cooperation, then immediately reverts to defection to
    continue exploiting.

    Internal state:
    - _my_last_action_was_cooperate: True if this player cooperated in the previous round, False otherwise.
    - _consecutive_defections_count: Number of consecutive rounds this player has defected since its last cooperation.

    Strategy parameters:
    - COOPERATION_THRESHOLD_FOR_FREE_RIDE: Minimum number of *other* players cooperating
      in the previous round to continue free-riding (defecting).
    - MIN_DEFECTIONS_BEFORE_PROBE: Minimum number of consecutive rounds *this player* must have
      defected before considering a "probe" (strategic cooperation). This acts as a cooldown.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.COOPERATION_THRESHOLD_FOR_FREE_RIDE = 1
        self.MIN_DEFECTIONS_BEFORE_PROBE = max(1, math.ceil(self.game_description.n_players / 2))
        self._my_last_action_was_cooperate: bool = False
        self._consecutive_defections_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_rounds = self.game_description.n_rounds
        action_to_take: Action
        if current_round == 0:
            action_to_take = Action.D
            self._my_last_action_was_cooperate = False
            self._consecutive_defections_count = 1
            return action_to_take
        if current_round == n_rounds - 1:
            return Action.D
        m_others_prev_round = np.sum(history.opponent_actions[-1, :])
        if self._my_last_action_was_cooperate:
            action_to_take = Action.D
            self._my_last_action_was_cooperate = False
            self._consecutive_defections_count = 1
        elif m_others_prev_round > self.COOPERATION_THRESHOLD_FOR_FREE_RIDE:
            action_to_take = Action.D
            self._consecutive_defections_count += 1
        elif self._consecutive_defections_count >= self.MIN_DEFECTIONS_BEFORE_PROBE:
            action_to_take = Action.C
            self._my_last_action_was_cooperate = True
            self._consecutive_defections_count = 0
        else:
            action_to_take = Action.D
            self._consecutive_defections_count += 1
        return action_to_take

class Strategy_EXPLOITATIVE_193(BaseStrategy):
    """
    This strategy aims to ride on the cooperation of others while minimizing its own contributions.
    It will only contribute strategically if the current level of cooperation is so low that continuing
    to defect would significantly harm its own long-term payoff (by driving everyone to defect,
    including potentially conditional cooperators).

    It defaults to defection in the first and last rounds, and also if too few rounds remain
    to justify a strategic "investment" in cooperation. The strategy adapts its behavior
    dynamically based on the observed level of cooperation in the previous round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = max(1, math.floor(self.n_players / 2))
        self.minimum_benefit_rounds = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        rounds_remaining = self.n_rounds - (current_round_0_indexed + 1)
        if rounds_remaining < self.minimum_benefit_rounds:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[-1, :]
        c_prev = np.sum(previous_round_opponent_actions)
        if c_prev < self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_194(BaseStrategy):
    """
    The Parasite strategy for the N-Player Public Goods Game.

    This strategy embodies an absolute exploitative mindset. Based on the game's
    payoff structure, defecting (D) is always the dominant strategy, yielding
    a strictly higher individual payoff than cooperating (C), irrespective of
    other players' actions.

    The Parasite consistently chooses to Defect in every round, maximizing its
    own gain by free-riding on any contributions made by other players, and
    never incurring the cost of cooperation itself. It is robust across all
    rounds (first, middle, or last) and against any combination of opponent strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes The Parasite strategy.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
                              These parameters confirm the dominance of defecting, but are
                              not used for real-time decision-making by this invariant strategy.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines The Parasite's action for the current round.

        The Parasite's decision rule is unwavering: always Defect.
        This choice is independent of the current round number, past actions,
        or the actions of other players, as defecting is the dominant strategy
        for individual payoff maximization in this game.

        Args:
            state: A GameState object containing the current round number.
            history: A PlayerHistory object (or None for the first round)
                     containing records of past actions and payoffs.
                     This strategy does not consult history.

        Returns:
            Action.D: The action to Defect.
        """
        return Action.D

class Strategy_EXPLOITATIVE_195(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize its own total payoff by free-riding
    on the contributions of other players as much as possible. It strategically cooperates only
    when the overall level of cooperation is dangerously low, threatening to collapse the public good.
    It avoids being a "sucker" by defecting if no one else is cooperating and exploits the
    end-game effect by defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Free-Rider strategy with game parameters.

        Args:
            game_description (PublicGoodsDescription): An object containing
                                                       n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.critical_cooperation_threshold = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the strategy's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): An object containing the current round number (0-indexed).
            history (None | PlayerHistory): An object containing past actions and payoffs
                                            for this player and opponents, or None for round 0.

        Returns:
            Action: The chosen action (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round = history.my_actions[-1]
        num_cooperators_prev_round = num_opponent_cooperators_prev_round + int(my_action_prev_round)
        if num_cooperators_prev_round == 0:
            return Action.D
        elif num_cooperators_prev_round < self.critical_cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_196(BaseStrategy):
    """
    The "Conditional Exploiter" (CE) strategy.

    This strategy is an Adaptive Opportunistic Exploiter. It aims to foster an environment where a critical
    mass of opponents cooperate, which it then exploits by defecting. It balances the need to signal
    willingness to cooperate (to encourage others) with the strong incentive to defect (to maximize
    personal gain). If cooperation breaks down, it defaults to defection to protect its own payoff.

    Core Principles:
    1.  Initial Probe: Start with a small act of cooperation to signal potential reciprocity and test the waters.
    2.  Foster Cooperation: If sufficient cooperation is observed from others, continue cooperating for a
        limited period to stabilize and encourage the cooperative environment.
    3.  Ruthless Exploitation: Once a cooperative environment is sufficiently stable (and this player has
        contributed to it consistently), switch to defection to gain the maximum benefit (private payoff
        + full share of public good).
    4.  Self-Preservation: If observed cooperation falls below a critical threshold, immediately switch
        to defection to avoid being a "sucker".
    5.  End-Game Rationality: Always defect in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.InitialCooperationRounds = 1
        self.CooperationSustainThreshold_Ratio = 0.75
        self.ExploitFrequency_Delay = 2
        self.T_C = math.floor(self.game_description.n_players * self.CooperationSustainThreshold_Ratio)
        self.my_last_action: Action | None = None
        self.cooperation_streak_observed = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            self.my_last_action = Action.C
            return Action.C
        if state.round_number == total_rounds - 1:
            self.my_last_action = Action.D
            return Action.D
        prev_round_my_action = history.my_actions[-1]
        prev_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        N_C_prev = prev_round_my_action + prev_round_opponent_cooperators
        if state.round_number < self.InitialCooperationRounds:
            if N_C_prev >= self.T_C:
                self.cooperation_streak_observed += 1
            else:
                self.cooperation_streak_observed = 0
            self.my_last_action = Action.C
            return Action.C
        if N_C_prev >= self.T_C:
            if prev_round_my_action == Action.C:
                self.cooperation_streak_observed += 1
            else:
                self.cooperation_streak_observed = 0
            if self.cooperation_streak_observed >= self.ExploitFrequency_Delay:
                self.my_last_action = Action.D
                self.cooperation_streak_observed = 0
                return Action.D
            else:
                self.my_last_action = Action.C
                return Action.C
        else:
            self.cooperation_streak_observed = 0
            self.my_last_action = Action.D
            return Action.D

class Strategy_EXPLOITATIVE_197(BaseStrategy):
    """
    The "Calculated Free-Rider (CFR)" strategy is designed for the N-Player Public Goods Game.
    It's fundamentally self-interested, aiming to free-ride on others' contributions while
    protecting itself from exploitation and strategically stimulating cooperation only when
    it's likely to lead to future free-riding opportunities. It incorporates an endgame
    defection phase to maximize final profits.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        self.C_THRESHOLD = math.ceil(n / k)
        self.MIN_COOPERATION_TO_TRY = max(1, math.floor(self.C_THRESHOLD / 2))
        self.FINAL_DEFECTION_ROUNDS = max(1, math.floor(r / 3))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed > total_rounds - self.FINAL_DEFECTION_ROUNDS:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        else:
            previous_round_0_indexed = state.round_number - 1
            my_action_prev = history.my_actions[previous_round_0_indexed]
            opponent_actions_prev = history.opponent_actions[previous_round_0_indexed, :]
            num_opponent_cooperators_prev = np.sum(opponent_actions_prev)
            m_prev = num_opponent_cooperators_prev + (1 if my_action_prev else 0)
            if m_prev >= self.C_THRESHOLD:
                return Action.D
            else:
                m_others_prev = num_opponent_cooperators_prev
                if m_others_prev >= self.MIN_COOPERATION_TO_TRY:
                    return Action.C
                else:
                    return Action.D

class Strategy_EXPLOITATIVE_198(BaseStrategy):
    """
    Adaptive Exploiter with Recalibration strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by free-riding on others' contributions
    while carefully avoiding being exploited. It follows these principles:
    1.  **Bait:** Cooperate in the first round to encourage others.
    2.  **Exploit:** Default to defection if there's any existing cooperation to free-ride on.
    3.  **Retaliate:** Defect if the player was the sole cooperator in the previous round.
    4.  **Recalibrate:** After a fixed number of consecutive rounds where everyone defected,
        it attempts to re-establish cooperation to break the low-payoff equilibrium and
        create new exploitation opportunities.
    5.  **Ruthless Endgame:** Always defect in the final round to maximize immediate payoff.
    """
    RECALIBRATE_THRESHOLD = 3

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_all_defect_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round == 0:
            return Action.C
        if current_round == total_rounds - 1:
            return Action.D
        prev_round_idx = current_round - 1
        my_c_prev = history.my_actions[prev_round_idx]
        opponent_cooperators_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        C_prev = opponent_cooperators_prev + (1 if my_c_prev else 0)
        if C_prev == 0:
            self.consecutive_all_defect_rounds += 1
        else:
            self.consecutive_all_defect_rounds = 0
        if C_prev >= 1:
            return Action.D
        elif self.consecutive_all_defect_rounds >= self.RECALIBRATE_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_199(BaseStrategy):
    """
    The Opportunistic Shark aims to free-ride on others' cooperation whenever possible.
    It acts as a conditional cooperator, but with a strong bias towards defection.
    It is willing to make minimal, strategic contributions only when necessary to initiate
    or sustain a level of public good that it can then exploit. It prioritizes its own
    immediate gain but understands that a complete collapse of cooperation removes
    future free-riding opportunities.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action: None | Action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r_total = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        current_action: Action
        if current_round_0_indexed == 0:
            current_action = Action.C
        elif current_round_0_indexed == r_total - 1:
            current_action = Action.D
        else:
            my_action_prev: Action = self.my_last_action
            opponent_cooperators_prev_count = history.opponent_actions[-1, :].sum()
            my_cooperation_prev_int = 1 if my_action_prev == Action.C else 0
            C_sum_prev = opponent_cooperators_prev_count + my_cooperation_prev_int
            if C_sum_prev == n:
                current_action = Action.D
            elif C_sum_prev == 0:
                current_action = Action.C
            else:
                others_cooperated_count = opponent_cooperators_prev_count
                cooperation_ratio_others_prev = others_cooperated_count / (n - 1)
                free_ride_threshold = 0.5
                if cooperation_ratio_others_prev >= free_ride_threshold:
                    current_action = Action.D
                elif my_action_prev == Action.C:
                    current_action = Action.D
                else:
                    current_action = Action.C
        self.my_last_action = current_action
        return current_action

class Strategy_EXPLOITATIVE_200(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to primarily free-ride on others' cooperation by defecting,
    as this always yields a higher per-round payoff for the individual than cooperating.
    It incorporates a mechanism to cautiously "reset" the game and encourage cooperation
    if the game devolves into a sustained state of universal defection, but only when
    there are enough rounds left to potentially exploit any newly established cooperation.
    It always defects in the final round to maximize terminal payoff.

    Decision Rules Summary:
    1.  **End Game (Final Round):** Always Defect.
    2.  **Adaptive Reset/Probe (Conditional Cooperation):** If all players have defected
        for `RESET_STREAK_THRESHOLD` consecutive rounds AND the current round is
        early enough (less than or equal to `RESET_ROUND_LIMIT`), then Cooperate.
        This is a calculated risk to try and re-establish cooperation.
    3.  **Default Action:** In all other cases, Defect to free-ride or avoid being a "sucker."
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_all_D_rounds = 0
        self.RESET_STREAK_THRESHOLD = 2
        self.RESET_ROUND_LIMIT = math.floor(self.game_description.n_rounds / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_1_indexed_round = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_1_indexed_round == total_rounds:
            return Action.D
        if state.round_number > 0 and history is not None:
            total_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            if history.my_actions[-1]:
                total_cooperators_prev_round += 1
            if total_cooperators_prev_round == 0:
                self.consecutive_all_D_rounds += 1
            else:
                self.consecutive_all_D_rounds = 0
        if self.consecutive_all_D_rounds >= self.RESET_STREAK_THRESHOLD and current_1_indexed_round <= self.RESET_ROUND_LIMIT:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_201(BaseStrategy):
    """
    The Shark strategy aims to maximize personal payoff in the N-Player Public Goods Game
    by probing cooperation, exploiting when cooperation is high, and "pumping" (cooperating)
    when cooperation levels drop too low to sustain future exploitation, then ruthlessly
    defecting in the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.endgame_rounds = min(3, self.r - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed > self.r - self.endgame_rounds:
            return Action.D
        adaptive_window_size = min(5, current_round_1_indexed - 1)
        total_C_others_sum = 0
        if adaptive_window_size == 0:
            avg_C_others_recent = 0
        else:
            recent_opponent_actions = history.opponent_actions[-adaptive_window_size:, :]
            for i in range(adaptive_window_size):
                cooperators_others_in_round_i = np.sum(recent_opponent_actions[i, :])
                total_C_others_sum += cooperators_others_in_round_i
            avg_C_others_recent = total_C_others_sum / adaptive_window_size
        cooperation_boost_threshold = math.floor((self.n - 1) * 0.25)
        if avg_C_others_recent < cooperation_boost_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_202(BaseStrategy):
    """
    The Calculated Opportunist strategy for an N-Player Public Goods Game.

    This strategy aims to opportunistically exploit any cooperation present in the game.
    It uses initial cooperation as a "bait" to encourage others, then defaults to defection
    to maximize immediate payoff. It only resorts to cooperating again if the observed
    public good generated by others becomes so low that further defection yields insufficient
    returns, treating cooperation as a necessary investment to restore exploitable conditions.
    In the final round, it always defects to maximize end-game payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description: An object containing game parameters such as n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.minimum_acceptable_payoff_threshold = 1 + self.k_factor / self.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round based on
        game parameters and historical data.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing past actions and payoffs of this player
                     and opponents. Is None for the very first round.

        Returns:
            Action: The chosen action, either Action.C (Cooperate) or Action.D (Defect).
        """
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        previous_opponent_actions = history.opponent_actions[-1, :]
        num_other_cooperators_previous_round = np.sum(previous_opponent_actions)
        estimated_payoff_if_defect = 1 + self.k_factor / self.n_players * num_other_cooperators_previous_round
        if estimated_payoff_if_defect < self.minimum_acceptable_payoff_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_203(BaseStrategy):
    """
    The "Strategic Free-Rider" strategy is designed to exploit the N-Player Public Goods Game.
    It prioritizes defection to maximize its own payoff, ideally by free-riding on the
    contributions of others. It strategically cooperates only to cultivate an environment
    where it can later free-ride, or to re-stimulate cooperation after a period of low contributions.

    This strategy dynamically adjusts its behavior based on observed cooperation levels
    in the previous round and its own recent history of actions. It aims to avoid being
    "suckered" into unreciprocated cooperation while also preventing a permanent collapse
    to an all-defection equilibrium if others are also adaptive.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.rounds_since_last_cooperation_attempt = 0
        self.my_last_action = None
        self.free_ride_threshold = max(2, math.floor(self.game_description.n_players / 2))
        self.max_consecutive_defections_before_reprobe = max(3, math.floor(self.game_description.n_rounds / 4))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            self.my_last_action = Action.C
            self.rounds_since_last_cooperation_attempt = 0
            return Action.C
        if current_round_number == self.game_description.n_rounds - 1:
            self.my_last_action = Action.D
            self.rounds_since_last_cooperation_attempt += 1
            return Action.D
        num_cooperators_previous_round = np.sum(history.opponent_actions[-1, :])
        chosen_action = None
        if num_cooperators_previous_round >= self.free_ride_threshold:
            chosen_action = Action.D
        elif num_cooperators_previous_round == 0:
            chosen_action = Action.D
        elif self.my_last_action == Action.D and self.rounds_since_last_cooperation_attempt >= self.max_consecutive_defections_before_reprobe:
            chosen_action = Action.C
        else:
            chosen_action = Action.D
        self.my_last_action = chosen_action
        if chosen_action == Action.C:
            self.rounds_since_last_cooperation_attempt = 0
        else:
            self.rounds_since_last_cooperation_attempt += 1
        return chosen_action

class Strategy_EXPLOITATIVE_204(BaseStrategy):
    """
    The Shark strategy aims to maximize its own payoff by primarily defecting when profitable,
    and strategically cooperating only to stimulate or sustain enough overall cooperation from
    which it can then free-ride in subsequent rounds. It defects in the first and last rounds
    and in intermediate rounds defects if the fraction of other cooperators is high enough
    (at or above the k/n threshold), otherwise it cooperates to encourage more contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_0_indexed_round = state.round_number
        if current_0_indexed_round == 0:
            return Action.D
        if current_0_indexed_round == self.r - 1:
            return Action.D
        opponent_actions_prev = history.opponent_actions[-1, :]
        C_others_prev = np.sum(opponent_actions_prev)
        if self.n - 1 > 0:
            F_coop = C_others_prev / (self.n - 1)
        else:
            F_coop = 0.0
        if F_coop >= self.k / self.n:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_205(BaseStrategy):
    """
    Adaptive Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own total payoff by primarily free-riding on the
    contributions of others. It strategically cooperates only when necessary to "re-bait"
    other players into contributing, thus preserving its opportunity to exploit them
    in subsequent rounds. It is adaptive to the level of cooperation observed in the game.

    Decision Rules:
    1. Round 0 (Initial Probe): Always Cooperate to potentially seed cooperation.
    2. Last Round (End-Game Exploitation): Always Defect, as there are no future consequences.
    3. Adaptive Free-Riding Phase (Intermediate Rounds):
       - If sufficient others cooperated last round:
         - If my consecutive defects are below a limit: Defect (free-ride).
         - Else (my streak is too long): Cooperate (re-bait).
       - If insufficient others cooperated last round: Defect (cooperation is futile).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.N = game_description.n_players
        self.R = game_description.n_rounds
        self.K = game_description.k
        self.free_ride_streak_limit = 3
        self.cooperation_threshold_others = max(1, math.floor(self.N / 2))
        self.my_consecutive_defects = 0
        self.my_last_action: None | Action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        if current_round_num == 0:
            self.my_consecutive_defects = 0
            self.my_last_action = Action.C
            return Action.C
        cooperators_from_others_prev_round = sum(history.opponent_actions[-1, :])
        if current_round_num >= self.R - 1:
            chosen_action = Action.D
            self.my_last_action = chosen_action
            return chosen_action
        chosen_action: Action
        if cooperators_from_others_prev_round >= self.cooperation_threshold_others:
            if self.my_consecutive_defects < self.free_ride_streak_limit:
                chosen_action = Action.D
                self.my_consecutive_defects += 1
            else:
                chosen_action = Action.C
                self.my_consecutive_defects = 0
        else:
            chosen_action = Action.D
            self.my_consecutive_defects += 1
        self.my_last_action = chosen_action
        return chosen_action

class Strategy_EXPLOITATIVE_206(BaseStrategy):
    """
    The Opportunistic Catalyst strategy for the N-Player Public Goods Game.

    This strategy aims to exploit other players by defecting when there's a sufficient public good
    created by others' cooperation. However, it understands that persistent defection can destroy
    the public good, so it strategically cooperates ("catalyzes") when the cooperation level
    drops too low, to rebuild the pool for future exploitation. It will never cooperate if it
    means receiving a payoff worse than if everyone defected.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.sucker_avoidance_threshold = max(0, int(math.ceil(self.n / self.k - 1)))
        self.exploitation_trigger_threshold = int(math.floor((self.k - 1) * self.n / self.k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.r - 1:
            return Action.D
        n_c_others_prev = sum(history.opponent_actions[-1, :])
        if n_c_others_prev >= self.exploitation_trigger_threshold:
            return Action.D
        elif n_c_others_prev < self.sucker_avoidance_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_207(BaseStrategy):
    """
    The Calculated Opportunist strategy aims to maximize its own total payoff by predominantly defecting,
    while strategically and minimally cooperating when necessary to induce and sustain a level of general
    cooperation from which to benefit. It starts by aggressively testing the environment with defection,
    adapts by free-riding when a sufficient proportion of others cooperate, and temporarily cooperates
    to re-induce cooperation if observed levels drop too low. If the environment becomes overwhelmingly
    uncooperative, it cuts its losses and reverts to permanent defection, always defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.INITIAL_TEST_ROUNDS = 3
        self.COOPERATION_INDUCEMENT_THRESHOLD = 0.4
        self.COOPERATION_DURATION = 2
        self.MINIMUM_FREE_RIDE_THRESHOLD = 0.1
        self.HISTORY_WINDOW_SIZE = 5
        self._others_previous_cooperation_counts = []
        self._cooperation_streak_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx > 0:
            opponent_actions_in_prev_round = history.opponent_actions[current_round_idx - 1, :]
            num_others_cooperated_t_minus_1 = sum(opponent_actions_in_prev_round)
            self._others_previous_cooperation_counts.append(num_others_cooperated_t_minus_1)
        if current_round_idx == self.r - 1:
            return Action.D
        if current_round_idx < self.INITIAL_TEST_ROUNDS:
            return Action.D
        if self._cooperation_streak_remaining > 0:
            self._cooperation_streak_remaining -= 1
            return Action.C
        num_other_players = self.n - 1
        prev_round_others_cooperation_count = self._others_previous_cooperation_counts[-1]
        p_other_coop_t_minus_1 = prev_round_others_cooperation_count / num_other_players
        start_idx = max(0, len(self._others_previous_cooperation_counts) - self.HISTORY_WINDOW_SIZE)
        recent_others_cooperation_counts = self._others_previous_cooperation_counts[start_idx:]
        avg_p_other_coop_recent = 0.0
        if recent_others_cooperation_counts:
            total_recent_cooperations = sum(recent_others_cooperation_counts)
            avg_p_other_coop_recent = total_recent_cooperations / (len(recent_others_cooperation_counts) * num_other_players)
        if avg_p_other_coop_recent < self.MINIMUM_FREE_RIDE_THRESHOLD:
            return Action.D
        elif p_other_coop_t_minus_1 < self.COOPERATION_INDUCEMENT_THRESHOLD:
            self._cooperation_streak_remaining = self.COOPERATION_DURATION - 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_208(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the N-Player Public Goods Game.
    This strategy is designed to be highly exploitative, prioritizing individual
    payoff maximization by predominantly free-riding. It adaptively "primes the pump"
    with minimal cooperation only when collective contribution is severely threatened,
    aiming to re-ignite contribution from others for future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.PROBE_DURATION = 1
        self.PROBE_COOLDOWN_ROUNDS = max(3, math.floor(self.game_description.n_rounds * 0.1))
        self.CRITICAL_COOPERATION_THRESHOLD = max(1, math.floor(self.game_description.n_players * 0.2))
        self.last_probe_round = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0idx = state.round_number
        current_round_1idx = current_round_0idx + 1
        if current_round_1idx == self.game_description.n_rounds:
            return Action.D
        if current_round_1idx <= 2:
            return Action.D
        my_prev_action_is_C = history.my_actions[-1].item()
        cooperators_prev_round = my_prev_action_is_C + np.sum(history.opponent_actions[-1, :])
        if my_prev_action_is_C and current_round_0idx - self.last_probe_round < self.PROBE_DURATION:
            if cooperators_prev_round == 1:
                return Action.D
            else:
                return Action.C
        if cooperators_prev_round <= self.CRITICAL_COOPERATION_THRESHOLD and current_round_0idx - self.last_probe_round > self.PROBE_COOLDOWN_ROUNDS:
            self.last_probe_round = current_round_0idx
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_209(BaseStrategy):
    """
    The Adaptive Predator strategy aims to maximize its own payoff by strategically encouraging
    cooperation among others, then defecting to take advantage of the public good created.
    It includes a robust mechanism to avoid being exploited by persistent defectors.

    I. Strategic Philosophy & Exploitative Mindset:
        - Induce Cooperation: Initially cooperates or when cooperation is low to build the public good.
        - Exploit Cooperation: Defects when a sufficient level of cooperation is observed among others.
        - Prevent Self-Exploitation: Reverts to defection if cooperation completely collapses to avoid being a "sucker".
        - Endgame Maximization: Defects in the final round to secure maximum payoff.

    II. Strategy Parameters:
        - COOPERATION_REVIVAL_THRESHOLD (0.4): Below this proportion of cooperators, it switches from exploitation to cooperation.
        - EXPLOITATION_ACTIVATION_THRESHOLD (0.7): Above this proportion of cooperators, it switches to exploitation (defection).
        - ZERO_COOPERATION_TOLERANCE_ROUNDS (3): Number of consecutive rounds with zero cooperators before giving up entirely.
    """
    COOPERATION_REVIVAL_THRESHOLD: float = 0.4
    EXPLOITATION_ACTIVATION_THRESHOLD: float = 0.7
    ZERO_COOPERATION_TOLERANCE_ROUNDS: int = 3

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description: PublicGoodsDescription = game_description
        self.is_in_exploitation_mode: bool = False
        self.zero_cooperation_streak: int = 0
        self.has_given_up_on_cooperation: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players: int = self.game_description.n_players
        n_rounds: int = self.game_description.n_rounds
        current_round_num_1_indexed: int = state.round_number + 1
        if current_round_num_1_indexed == 1:
            self.is_in_exploitation_mode = False
            self.zero_cooperation_streak = 0
            self.has_given_up_on_cooperation = False
            return Action.C
        if current_round_num_1_indexed == n_rounds:
            return Action.D
        previous_my_action_was_C: bool = history.my_actions[-1]
        num_opponent_cooperators_prev_round: int = np.sum(history.opponent_actions[-1, :])
        num_cooperators_prev_round: int = (1 if previous_my_action_was_C else 0) + num_opponent_cooperators_prev_round
        cooperation_proportion: float = num_cooperators_prev_round / n_players
        if num_cooperators_prev_round == 0:
            self.zero_cooperation_streak += 1
        else:
            self.zero_cooperation_streak = 0
        if not self.has_given_up_on_cooperation and self.zero_cooperation_streak >= self.ZERO_COOPERATION_TOLERANCE_ROUNDS and (state.round_number < n_rounds - 2):
            self.has_given_up_on_cooperation = True
            return Action.D
        if self.has_given_up_on_cooperation:
            return Action.D
        if self.is_in_exploitation_mode:
            if cooperation_proportion < self.COOPERATION_REVIVAL_THRESHOLD:
                self.is_in_exploitation_mode = False
                return Action.C
            else:
                return Action.D
        elif cooperation_proportion >= self.EXPLOITATION_ACTIVATION_THRESHOLD:
            self.is_in_exploitation_mode = True
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_210(BaseStrategy):
    """
    The Strategic Parasite strategy for the N-Player Public Goods Game.
    This strategy is designed to be exploitative, adaptive, and robust. It prioritizes
    free-riding by defecting to keep its private endowment while benefiting from the
    public good. However, it strategically "invests" in cooperation (at a cost)
    when the collective level of cooperation is critically low. This investment aims
    to re-stimulate contributions, thereby preserving the opportunity for future
    exploitation. The strategy also incorporates backward induction, defecting
    in the final rounds to maximize end-game payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed >= self.n_rounds - 2:
            return Action.D
        my_action_prev_bool = history.my_actions[-1]
        cooperators_opponents_prev = np.sum(history.opponent_actions[-1, :])
        C_prev = cooperators_opponents_prev + int(my_action_prev_bool)
        if C_prev >= self.k:
            return Action.D
        elif my_action_prev_bool == False:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_211(BaseStrategy):
    """
    The "Opportunistic Free-Rider" strategy is designed to maximize individual payoff in the N-Player
    Public Goods Game by strategically leveraging the cooperation of others while minimizing its own
    contributions. It balances initial probing with sustained exploitation and self-preservation,
    adapting its behavior based on observed group dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T_exploit_count = max(1, math.ceil((self.n - 1) * (self.k / self.n)))
        self.T_coop_rate_historical = self.k / self.n
        self.PROBING_ROUNDS = min(self.r - 1, 3)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        if current_round_0_indexed <= self.PROBING_ROUNDS:
            my_action_prev = history.my_actions[-1]
            c_others_prev = np.sum(history.opponent_actions[-1, :])
            if my_action_prev == True:
                if c_others_prev >= self.T_exploit_count:
                    return Action.D
                else:
                    return Action.C
            else:
                return Action.D
        total_c_others_overall = np.sum(history.opponent_actions)
        total_possible_cooperation_others = (self.n - 1) * current_round_0_indexed
        if total_possible_cooperation_others == 0:
            return Action.D
        proportion_coop_others_history = total_c_others_overall / total_possible_cooperation_others
        return Action.D

class Strategy_EXPLOITATIVE_212(BaseStrategy):
    """
    The Opportunistic Free-Rider (OFR) strategy aims to maximize individual payoff by defaulting to
    defection (free-riding) and only cooperating under specific conditions designed to maintain a
    sufficiently large public good from which to free-ride. It exploits the repeated nature of
    the game and the dominant-strategy-to-defect in a single round, while acknowledging the need
    to occasionally "invest" in the public good to prevent its collapse.

    Decision Rules:
    1. Round 1: Always Defect. Establishes an aggressive stance and maximizes initial payoff.
    2. Last Round (r): Always Defect. No future consequences for cooperation.
    3. Intermediate Rounds:
       - Observe total cooperators in the previous round (C_prev).
       - Calculate Cooperation_Threshold = floor(n / 2).
       - If C_prev < Cooperation_Threshold: Cooperate. Strategic investment to restore the public good.
       - Else (C_prev >= Cooperation_Threshold): Defect. Free-ride on sufficient existing cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.floor(self.game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        my_prev_action_bool = history.my_actions[current_round_0_indexed - 1]
        opponent_prev_actions_bool = history.opponent_actions[current_round_0_indexed - 1, :]
        num_cooperators_prev_round = np.sum(opponent_prev_actions_bool) + int(my_prev_action_bool)
        if num_cooperators_prev_round < self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_213(BaseStrategy):
    """
    The Opportunistic Free-Rider strategy aims to maximize its own total payoff by primarily defecting and free-riding on others' contributions. It strategically cooperates only when the collective cooperation among other players falls below a critical threshold, effectively "investing" to prevent the complete collapse of the public good, thus preserving future free-riding opportunities. It reverts to defection as soon as sufficient cooperation is observed.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.r_probe_end = max(2, min(self.n_rounds - 1, 5))
        self.Coop_Thresh = max(1, math.floor((self.n_players - 1) / 3))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        previous_round_other_cooperators = sum(history.opponent_actions[-1, :])
        if 2 <= current_round_1_indexed <= self.r_probe_end:
            if previous_round_other_cooperators > 0:
                return Action.D
            else:
                return Action.C
        if previous_round_other_cooperators < self.Coop_Thresh:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_214(BaseStrategy):
    """
    The "Adaptive Predator" strategy for the N-Player Public Goods Game.
    This strategy is designed to be exploitative, primarily free-riding on the contributions of other players.
    It begins with a strategic cooperative "probe" in the first round to encourage others' cooperation.
    For intermediate rounds, it systematically defects to exploit existing public good, only cooperating
    if the number of other cooperators falls below a calculated "collapse threshold" (floor(n/3)).
    This "re-seeding" of cooperation is a calculated investment to prevent the public good from vanishing,
    ensuring future opportunities for exploitation. In the final round, it purely defects, as there are no
    future consequences to consider.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        N_C_others_t_minus_1 = np.sum(history.opponent_actions[-1, :])
        threshold_collapse = math.floor(self.n / 3)
        if N_C_others_t_minus_1 <= threshold_collapse:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_215(BaseStrategy):
    """
    The Opportunistic Defector strategy for the N-Player Public Goods Game.

    This strategy aggressively defects to maximize immediate payoff, only switching
    to cooperation as a calculated, short-term "investment" to prevent the complete
    collapse of the public good, thereby safeguarding its own free-riding potential.
    It starts by probing for exploitation opportunities and quickly reverts to defection
    once cooperation shows signs of recovery.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.critical_cooperation_threshold = math.floor(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        elif current_round_number == self.n_rounds - 1:
            return Action.D
        else:
            my_last_action_was_C = history.my_actions[-1]
            opponents_cooperated_prev_round = np.sum(history.opponent_actions[-1, :])
            N_C_prev = opponents_cooperated_prev_round + (1 if my_last_action_was_C else 0)
            if not my_last_action_was_C and N_C_prev > 0 and (N_C_prev <= self.critical_cooperation_threshold):
                return Action.C
            elif my_last_action_was_C and N_C_prev > self.critical_cooperation_threshold:
                return Action.D
            else:
                return Action.D

class Strategy_EXPLOITATIVE_216(BaseStrategy):
    """
    The Cautious Opportunist strategy for the N-Player Public Goods Game.

    This strategy aims to maximize personal payoff by strategically free-riding on others'
    contributions, while also intelligently investing in cooperation when necessary to
    cultivate a profitable environment for future exploitation. It adapts to the observed
    level of cooperation and the remaining game length.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.learning_rounds = self.n_rounds // 2
        self.cooperation_viability_threshold = self.n_players / self.k_factor

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_my_cooperations_prev = int(history.my_actions[-1])
        num_opponent_cooperations_prev = np.sum(history.opponent_actions[-1, :])
        c_prev = num_my_cooperations_prev + num_opponent_cooperations_prev
        if c_prev >= self.cooperation_viability_threshold:
            return Action.D
        elif current_round_1_indexed < self.learning_rounds:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_217(BaseStrategy):
    """
    Dynamic Free-Rider (DFR) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by probing for cooperation,
    free-riding when enough cooperation is present, and defensively defecting
    when cooperation is too low or collapses. It accounts for the end-game
    by always defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = math.ceil(self.n / self.k)
        self.rounds_of_low_cooperation = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if history is not None:
            previous_round_cooperators_from_opponents = np.sum(history.opponent_actions[-1, :])
            previous_round_my_action = history.my_actions[-1]
            previous_total_cooperators = previous_round_cooperators_from_opponents + previous_round_my_action
        else:
            previous_total_cooperators = 0
        if current_round_1_indexed == self.r:
            return Action.D
        elif current_round_1_indexed == 1:
            self.rounds_of_low_cooperation = 0
            return Action.C
        else:
            if previous_total_cooperators < self.cooperation_threshold:
                self.rounds_of_low_cooperation += 1
            else:
                self.rounds_of_low_cooperation = 0
            if self.rounds_of_low_cooperation >= 2:
                return Action.D
            elif previous_total_cooperators >= self.cooperation_threshold:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_218(BaseStrategy):
    """
    The Persistent Free-Rider (PFR) strategy.

    This strategy is designed to be highly exploitative in the N-Player Public Goods Game.
    It initiates with a cooperation (C) in the first round as a probe to encourage others.
    In the middle game, it aims to free-ride by defecting (D) if a sufficient proportion
    of other players (INITIAL_EXPLOITATION_THRESHOLD) cooperated in the previous round.
    If cooperation from others drops below this threshold, the strategy cooperates to
    attempt to revive the public good, but it has a "patience limit" (MAX_FAILED_REVIVAL_WINDOW).
    If its attempts to revive cooperation by playing C consistently fail to bring
    others' cooperation above the threshold, it will permanently switch to defecting for
    the remainder of the middle game to avoid being a "sucker."
    In the final two rounds (endgame), it always defects due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        super().__init__(game_description)
        self.game_description = game_description
        self.INITIAL_EXPLOITATION_THRESHOLD = 0.3
        self.MAX_FAILED_REVIVAL_WINDOW = 3
        self.giving_up_on_revivals_for_middle_game = False
        self.failed_revival_streak_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if current_round == 0:
            return Action.C
        if current_round >= r - 2:
            return Action.D
        my_prev_action_bool = history.my_actions[current_round - 1]
        prev_round_opponent_actions = history.opponent_actions[current_round - 1, :]
        num_other_cooperators = np.sum(prev_round_opponent_actions)
        P_coop_others_prev = float(num_other_cooperators) / (n - 1)
        action_for_this_round = Action.D
        if self.giving_up_on_revivals_for_middle_game:
            action_for_this_round = Action.D
        else:
            if my_prev_action_bool == Action.C.value and P_coop_others_prev < self.INITIAL_EXPLOITATION_THRESHOLD:
                self.failed_revival_streak_count += 1
            else:
                self.failed_revival_streak_count = 0
            if self.failed_revival_streak_count >= self.MAX_FAILED_REVIVAL_WINDOW:
                self.giving_up_on_revivals_for_middle_game = True
                action_for_this_round = Action.D
            elif P_coop_others_prev >= self.INITIAL_EXPLOITATION_THRESHOLD:
                action_for_this_round = Action.D
            else:
                action_for_this_round = Action.C
        return action_for_this_round

class Strategy_EXPLOITATIVE_219(BaseStrategy):
    """
    The Opportunistic Provocateur strategy for the N-Player Public Goods Game.
    This strategy defaults to Defect (free-riding), only cooperating strategically
    as a minimal investment to prevent a total collapse of public good provision
    from which it can benefit, or to "bait" conditional cooperators back into action.
    It is adaptive by observing the aggregate behavior of other players.
    """
    THRESHOLD_FOR_COLLAPSE = 0.2
    DEFECT_STREAK_LIMIT = 4

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_consecutive_defections: int = 0
        self.my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        action_to_play: Action
        if current_round_1_indexed == total_rounds:
            action_to_play = Action.D
            return action_to_play
        if current_round_1_indexed == 1:
            action_to_play = Action.D
            self.my_consecutive_defections = 1
            self.my_last_action = Action.D
            return action_to_play
        previous_round_opponent_actions = history.opponent_actions[-1, :]
        C_prev = np.sum(previous_round_opponent_actions)
        if history.my_actions[-1] == Action.C.value:
            C_prev += 1
        P_coop_prev = C_prev / n_players
        if P_coop_prev < self.THRESHOLD_FOR_COLLAPSE and self.my_consecutive_defections >= self.DEFECT_STREAK_LIMIT:
            action_to_play = Action.C
            self.my_consecutive_defections = 0
            self.my_last_action = Action.C
        else:
            action_to_play = Action.D
            if self.my_last_action == Action.D:
                self.my_consecutive_defections += 1
            else:
                self.my_consecutive_defections = 1
            self.my_last_action = Action.D
        return action_to_play

class Strategy_EXPLOITATIVE_220(BaseStrategy):
    """
    Adaptive Free-Rider with Cooperation Trigger (AFR-CT) strategy for N-Player Public Goods Game.
    Prioritizes personal payoff maximization by defaulting to defection (free-riding).
    It only cooperates under specific, calculated conditions to prevent the collapse of the
    public good, which would ultimately diminish future free-riding opportunities.
    The strategy avoids futile cooperation when no baseline cooperation exists.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.CMT = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == self.r:
            return Action.D
        prev_round_history_idx = state.round_number - 1
        my_prev_action_is_C = history.my_actions[prev_round_history_idx]
        num_my_prev_C = int(my_prev_action_is_C)
        opponents_prev_actions_are_C = history.opponent_actions[prev_round_history_idx, :]
        num_opponents_prev_C = np.sum(opponents_prev_actions_are_C)
        N_C_prev = num_my_prev_C + num_opponents_prev_C
        if N_C_prev > 0 and N_C_prev < self.CMT:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_221(BaseStrategy):
    """
    The Opportunistic Prudent Defector strategy for the N-Player Public Goods Game.

    This strategy aims to maximize personal payoff by free-riding on others' contributions
    while strategically maintaining just enough group cooperation to keep the public good
    viable for exploitation.

    Decision Rules:
    1. Round 1: Cooperate (C) - Initial probe to encourage cooperation.
    2. Intermediate Rounds:
        - If previous round's total cooperators (C_prev) > floor(n_players / 2): Defect (D) - Exploit high cooperation.
        - Else (C_prev <= floor(n_players / 2)): Cooperate (C) - Re-prime to prevent collapse.
    3. Last Round: Defect (D) - Maximize final payoff, no future to influence.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = math.floor(self.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        num_cooperating_opponents_prev_round = sum(history.opponent_actions[-1, :])
        my_action_prev_round_is_cooperate = history.my_actions[-1]
        C_prev = num_cooperating_opponents_prev_round + my_action_prev_round_is_cooperate
        if C_prev > self.cooperation_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_222(BaseStrategy):
    """
    The Adaptive Exploiter with Conditional Re-seeding (AECR) is designed to maximize
    its own total payoff in the Public Goods Game. It primarily defects, free-riding
    on others' contributions. It only cooperates as a one-time "re-seeding" probe
    when the public good has completely collapsed (zero cooperators for a sustained
    period) to attempt to re-establish cooperation, which it will then exploit.
    It does not engage in sustained cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.no_coop_persistence_rounds = 2
        self.zero_coop_count = 0
        self.reseed_attempt_active = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_number == 0:
            self.zero_coop_count = 0
            self.reseed_attempt_active = False
            return Action.D
        if current_round_number == total_rounds - 1:
            self.zero_coop_count = 0
            self.reseed_attempt_active = False
            return Action.D
        else:
            previous_round_opponent_cooperators = sum(history.opponent_actions[-1, :])
            my_previous_action_was_cooperate = history.my_actions[-1]
            c_prev = previous_round_opponent_cooperators + (1 if my_previous_action_was_cooperate else 0)
            if c_prev == 0:
                self.zero_coop_count += 1
            else:
                self.zero_coop_count = 0
            if self.reseed_attempt_active:
                self.reseed_attempt_active = False
                return Action.D
            elif self.zero_coop_count >= self.no_coop_persistence_rounds:
                self.reseed_attempt_active = True
                self.zero_coop_count = 0
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_223(BaseStrategy):
    """
    The Opportunistic Predator strategy aims to maximize its own payoff by strategically alternating
    between cooperation (C) and defection (D). It uses cooperation as "bait" to encourage others to
    contribute to the public good, then defects to privately benefit from those contributions.
    It includes self-preservation mechanisms to avoid being a "sucker" if cooperation from others
    is too low, and guarantees a final round defection for maximal personal gain.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_sustain_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        my_previous_action_bool = history.my_actions[-1]
        opponent_previous_cooperators = np.sum(history.opponent_actions[-1, :])
        C_previous = int(my_previous_action_bool) + opponent_previous_cooperators
        cooperation_ratio = C_previous / n_players
        if cooperation_ratio < self.cooperation_sustain_threshold:
            return Action.D
        elif my_previous_action_bool == True:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_224(BaseStrategy):
    """
    The Adaptive Exploitative Strategy (AES) aims to free-ride on the contributions of other players.
    It cooperates initially to probe for cooperation, defects in the final round,
    and in intermediate rounds, it defects if opponent cooperation is high enough,
    otherwise it cooperates to "invest" in the public good to prevent its collapse.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.W_probe = 2
        self.W_window = 5
        self.alpha_exploit = 0.4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed <= self.W_probe:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        history_start_idx = max(0, state.round_number - self.W_window)
        history_end_idx = state.round_number - 1
        avg_c_rate_others = 0.0
        if history_end_idx >= history_start_idx:
            cooperation_in_window_count = history.opponent_actions[history_start_idx:history_end_idx + 1, :].sum()
            num_rounds_in_window = history_end_idx - history_start_idx + 1
            total_possible_cooperations = num_rounds_in_window * (self.n_players - 1)
            if total_possible_cooperations > 0:
                avg_c_rate_others = cooperation_in_window_count / total_possible_cooperations
        if avg_c_rate_others >= self.alpha_exploit:
            return Action.D
        elif avg_c_rate_others <= 1e-09:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_225(BaseStrategy):
    """
    The Strategic Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize personal payoff by primarily free-riding (defecting),
    but intelligently cooperates under specific conditions to maintain a functional
    public good for future exploitation or to escape all-defect equilibria.

    Decision Rules:
    1.  Always Defect in the very last round (endgame effect).
    2.  Always Defect in the very first round (to probe for cooperation).
    3.  In intermediate rounds:
        a.  Public Good Revival: If total cooperators in the previous round fall
            below `RECOVERY_COOP_THRESHOLD` (e.g., 2), AND this player defected
            in the previous round, cooperate to try and re-stimulate public good.
        b.  Forced Reset: If this player has defected for `FORCED_RESET_ROUNDS`
            (e.g., 'n' rounds) consecutively, cooperate to break potential
            all-defect cycles or probe for new cooperation.
        c.  Default: Defect (free-ride) if neither of the above triggers are met.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.RECOVERY_COOP_THRESHOLD = 2
        self.FORCED_RESET_ROUNDS = game_description.n_players
        self._my_last_action: Action = Action.D
        self._my_consecutive_defects: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        action_for_this_round: Action
        if current_round == total_rounds - 1:
            action_for_this_round = Action.D
        elif current_round == 0:
            action_for_this_round = Action.D
        else:
            cooperators_in_prev_round = sum(history.opponent_actions[-1, :])
            if history.my_actions[-1]:
                cooperators_in_prev_round += 1
            should_cooperate = False
            if cooperators_in_prev_round < self.RECOVERY_COOP_THRESHOLD:
                if self._my_last_action == Action.D:
                    should_cooperate = True
            if self._my_consecutive_defects >= self.FORCED_RESET_ROUNDS:
                should_cooperate = True
            if should_cooperate:
                action_for_this_round = Action.C
            else:
                action_for_this_round = Action.D
        self._my_last_action = action_for_this_round
        if action_for_this_round == Action.D:
            self._my_consecutive_defects += 1
        else:
            self._my_consecutive_defects = 0
        return action_for_this_round

class Strategy_EXPLOITATIVE_226(BaseStrategy):
    """
    The Apex Predator strategy aims to maximize its own cumulative payoff by primarily free-riding
    on the contributions of other players. It employs cooperation only as a calculated maneuver to
    probe for cooperative opponents or to revive a collapsed public good, thereby creating future
    opportunities for exploitation. It is ruthless in defecting once cooperation is established
    or when re-luring attempts fail to generate a payoff.

    Key Principles:
    1. Initial Probe: Take a calculated risk in the first round by cooperating to test for potential cooperators.
    2. Relentless Exploitation: Default to Defect (D) whenever there are cooperators to free-ride on.
    3. Strategic Re-Lure: Only cooperate if the public good completely collapses AND there's a
       reasonable chance (sufficient rounds remaining, re-lures haven't been consistently ineffective)
       to revive it for future exploitation. Avoid becoming a "sucker".
    4. Endgame Rationality: Always defect in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.rounds_since_last_effective_cooperation_effort = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        my_action_prev_was_C = history.my_actions[-1]
        C_others_prev = sum(history.opponent_actions[-1, :])
        if my_action_prev_was_C and C_others_prev > 0:
            self.rounds_since_last_effective_cooperation_effort = 0
        else:
            self.rounds_since_last_effective_cooperation_effort += 1
        if current_round_1_indexed == self.r:
            return Action.D
        if my_action_prev_was_C:
            return Action.D
        elif C_others_prev == 0:
            not_second_to_last_round = current_round_1_indexed < self.r - 1
            max_patience_rounds = max(2, math.floor(self.r / 4))
            re_lure_still_viable = self.rounds_since_last_effective_cooperation_effort < max_patience_rounds
            if not_second_to_last_round and re_lure_still_viable:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_227(BaseStrategy):
    """
    The "Persistent Free-Rider" strategy is designed to primarily free-ride (defect)
    but is adaptive to the observed level of cooperation. It strategically "probes"
    for cooperation, or attempts to "re-ignite" it if it collapses, solely for the
    purpose of creating future free-riding opportunities. It understands that a
    temporary sacrifice (cooperating once) might lead to greater exploitation later.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._n = game_description.n_players
        self._r = game_description.n_rounds
        self._k = game_description.k
        self._r_end_game_buffer = min(self._r - 1, 2)
        self._r_end_game_start_round = self._r - self._r_end_game_buffer
        self._probe_interval = max(2, math.floor(self._n / 2))
        self._last_probe_round = -1
        self._is_currently_probing = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            self._last_probe_round = current_round
            self._is_currently_probing = True
            return Action.C
        elif current_round >= self._r_end_game_start_round:
            self._is_currently_probing = False
            return Action.D
        elif self._is_currently_probing:
            self._is_currently_probing = False
            return Action.D
        else:
            num_cooperators_prev_round = np.sum(history.opponent_actions[current_round - 1, :])
            if history.my_actions[current_round - 1]:
                num_cooperators_prev_round += 1
            if num_cooperators_prev_round > 0:
                return Action.D
            elif current_round - self._last_probe_round >= self._probe_interval:
                self._last_probe_round = current_round
                self._is_currently_probing = True
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_228(BaseStrategy):
    """
    The Opportunistic Predator strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by primarily free-riding on
    the contributions of others. It makes a minimal initial investment
    (cooperating in the first round) to encourage others, and reluctantly
    contributes only when the public good is severely threatened. Otherwise,
    it defaults to defection to exploit existing cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        k = game_description.k
        self.initial_cooperation_round_idx = 0
        self.minimum_viable_cooperation = max(1, math.floor(n / k))
        self.free_ride_threshold_ratio = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_idx == total_rounds - 1:
            return Action.D
        if current_round_idx == self.initial_cooperation_round_idx:
            return Action.C
        my_previous_action_was_cooperation = history.my_actions[-1]
        opponent_previous_cooperators_count = sum(history.opponent_actions[-1, :])
        C_t_minus_1 = int(my_previous_action_was_cooperation) + opponent_previous_cooperators_count
        cooperation_ratio = float(C_t_minus_1) / n_players
        if C_t_minus_1 == 0:
            return Action.D
        if cooperation_ratio >= self.free_ride_threshold_ratio:
            return Action.D
        if C_t_minus_1 <= self.minimum_viable_cooperation:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_229(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize its own payoff by leveraging
    the cooperation of others. It probes with cooperation, then free-rides on sufficiently
    high cooperation levels. If cooperation from others drops too low, it initiates a
    "punishment" phase by defecting to re-incentivize future cooperation, before reverting
    to free-riding. It always defects in the final round.
    """
    P_DURATION: int = 2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_mode_active: bool = False
        self.punishment_rounds_remaining: int = 0
        n = self.game_description.n_players
        k = self.game_description.k
        self.TARGET_OTHER_COOPERATORS: int = math.ceil((k - 1) * n / k)
        self.TARGET_OTHER_COOPERATORS = max(0, min(n - 1, self.TARGET_OTHER_COOPERATORS))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            self.punishment_mode_active = False
            self.punishment_rounds_remaining = 0
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        if self.punishment_mode_active:
            if self.punishment_rounds_remaining > 0:
                self.punishment_rounds_remaining -= 1
                return Action.D
            else:
                self.punishment_mode_active = False
        num_others_cooperated_last_round = sum(history.opponent_actions[-1, :])
        if num_others_cooperated_last_round < self.TARGET_OTHER_COOPERATORS:
            self.punishment_mode_active = True
            self.punishment_rounds_remaining = self.P_DURATION - 1
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_230(BaseStrategy):
    """
    The "Calculated Opportunist" strategy for the N-Player Public Goods Game.
    This strategy is designed to be exploitative, adaptive, and robust. It aims to maximize
    its own payoff by initiating cooperation to probe, continuously monitoring other players'
    cooperation levels, free-riding opportunistically when others cooperate sufficiently,
    punishing widespread defection, and unilaterally defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.coop_threshold_ratio = 0.6
        self.punish_threshold_ratio = 0.3
        self.free_ride_interval = 4
        self.max_consecutive_free_rides = 2
        self.my_last_action: Action | None = None
        self.rounds_since_last_free_ride_attempt = 0
        self.free_ride_rounds_since_last_cooperation = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.r:
            return Action.D
        elif current_round_1_indexed == 1:
            self.my_last_action = Action.C
            self.rounds_since_last_free_ride_attempt = 0
            self.free_ride_rounds_since_last_cooperation = 0
            return Action.C
        else:
            other_cooperators = np.sum(history.opponent_actions[-1, :])
            num_other_players = self.n - 1
            cooperation_rate_others = other_cooperators / num_other_players
            should_punish = cooperation_rate_others < self.punish_threshold_ratio
            should_cooperate = cooperation_rate_others >= self.coop_threshold_ratio
            self.rounds_since_last_free_ride_attempt += 1
            current_action_decision = Action.D
            if should_punish:
                current_action_decision = Action.D
                self.free_ride_rounds_since_last_cooperation = 0
            elif should_cooperate:
                if self.my_last_action == Action.C:
                    if self.rounds_since_last_free_ride_attempt >= self.free_ride_interval:
                        current_action_decision = Action.D
                        self.rounds_since_last_free_ride_attempt = 0
                        self.free_ride_rounds_since_last_cooperation = 1
                    else:
                        current_action_decision = Action.C
                        self.free_ride_rounds_since_last_cooperation = 0
                elif self.free_ride_rounds_since_last_cooperation < self.max_consecutive_free_rides:
                    current_action_decision = Action.D
                    self.free_ride_rounds_since_last_cooperation += 1
                else:
                    current_action_decision = Action.C
                    self.free_ride_rounds_since_last_cooperation = 0
                    self.rounds_since_last_free_ride_attempt = 0
            else:
                current_action_decision = Action.D
                self.free_ride_rounds_since_last_cooperation = 0
            self.my_last_action = current_action_decision
            return current_action_decision

class Strategy_EXPLOITATIVE_231(BaseStrategy):
    """
    The Pure Exploiter strategy for the N-Player Public Goods Game.

    This strategy identifies that for the given game parameters (1 < k < n),
    defecting ('D') is always the dominant action for an individual player
    in any single round, as it yields a higher payoff regardless of other players' actions.
    In a finite repeated game, this also aligns with the backward induction solution.

    Therefore, the Pure Exploiter always chooses to defect in every round,
    maximizing its immediate individual payoff by free-riding on any cooperation
    from other players, or by securing the baseline payoff when all defect.
    It is robust and consistently exploitative against any opponent strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        return Action.D

class Strategy_EXPLOITATIVE_232(BaseStrategy):
    """
    The Opportunistic Free-Rider strategy. This strategy is designed to be highly
    exploitative by maximizing its own payoff, even at the expense of others.
    It adapts to observed behaviors by attempting to encourage cooperation when
    it's beneficial for future free-riding, and defecting whenever possible
    to capitalize on existing cooperation or minimize losses.

    Core Principle: Maximize personal gain by free-riding on others' contributions.
    Strategically cooperate only as a short-term investment to cultivate a larger
    pool of public goods that can be exploited in subsequent rounds. Never be the
    "sucker" who consistently contributes without reciprocal benefits.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold_exploit = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_num = state.round_number
        if current_round_num == 0:
            return Action.C
        if current_round_num == n_rounds - 1:
            return Action.D
        num_others_cooperated_last_round = sum(history.opponent_actions[-1, :])
        num_other_players = n_players - 1
        cs_t = num_others_cooperated_last_round / num_other_players
        if cs_t >= self.threshold_exploit:
            return Action.D
        else:
            is_early_enough = current_round_num < n_rounds / 2
            is_some_cooperation = cs_t > 0
            is_n_large_enough = n_players > 2
            if is_early_enough and is_some_cooperation and is_n_large_enough:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_233(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy aims to maximize its own payoff by primarily free-riding,
    but it is willing to make strategic, temporary contributions to "re-induce" cooperation when
    it perceives the collective good is at risk of collapsing, or to "punish" the group if its
    own cooperation isn't sufficiently reciprocated. The known finite number of rounds allows
    for a clear shift to pure exploitation in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.CIT = math.floor(self.game_description.n_players / self.game_description.k)
        self.my_action_prev_round: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number
        my_current_action: Action
        if current_round_t == 0:
            my_current_action = Action.C
        elif current_round_t == self.game_description.n_rounds - 1:
            my_current_action = Action.D
        else:
            my_action_in_prev_round_bool = history.my_actions[-1]
            opponent_cooperators_in_prev_round = np.sum(history.opponent_actions[-1, :])
            total_cooperators_prev_round = opponent_cooperators_in_prev_round + int(my_action_in_prev_round_bool)
            if total_cooperators_prev_round >= self.CIT:
                my_current_action = Action.D
            elif self.my_action_prev_round == Action.C:
                my_current_action = Action.D
            else:
                my_current_action = Action.C
        self.my_action_prev_round = my_current_action
        return my_current_action

class Strategy_EXPLOITATIVE_234(BaseStrategy):
    """
    Adaptive Forager 2.0 (AF2.0) strategy for the N-Player Public Goods Game.
    This exploitative strategy probes for cooperation, exploits it, and re-seeds it
    when necessary, while ensuring endgame maximization. It implements initial probing,
    adaptive exploitation based on others' cooperation, and unconditional endgame defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        self.initial_coop_rounds = max(1, min(math.floor(r / 10), 3))
        self.endgame_defect_rounds = min(r - 1, 3)
        self.cooperation_threshold_count = math.ceil(n / k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed <= self.initial_coop_rounds:
            return Action.C
        if current_round_1_indexed > total_rounds - self.endgame_defect_rounds:
            return Action.D
        last_round_0_indexed = state.round_number - 1
        my_last_action_was_C = history.my_actions[last_round_0_indexed]
        num_other_cooperators_last_round = sum(history.opponent_actions[last_round_0_indexed, :])
        if my_last_action_was_C:
            if num_other_cooperators_last_round >= self.cooperation_threshold_count:
                return Action.D
            else:
                return Action.D
        elif num_other_cooperators_last_round >= self.cooperation_threshold_count:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_235(BaseStrategy):
    """
    Adaptive Exploiter with Re-Induction: An exploitative strategy for the N-Player Public Goods Game.
    It strategically cooperates during an initial exploration period to induce cooperation from others.
    After this phase, it defects to free-ride on a sufficient number of cooperators.
    If the level of cooperation from other players drops too low, it temporarily cooperates to "re-induce"
    cooperation, always aiming to rebuild the pool for future exploitation. It defects unconditionally
    in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        r_total = self.game_description.n_rounds
        self.exploration_period_rounds = max(1, min(3, math.floor(r_total / 3)))
        self.cooperation_inducement_threshold = math.ceil((n - 1) / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        if current_round == self.game_description.n_rounds:
            return Action.D
        if current_round <= self.exploration_period_rounds:
            return Action.C
        num_other_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        if num_other_cooperators_prev_round >= self.cooperation_inducement_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_236(BaseStrategy):
    """
    The "Opportunistic Free-Rider (OFR)" strategy is designed to maximize its own payoff in the
    N-Player Public Goods Game by strategically oscillating between fostering cooperation
    (to create a large public good pool) and free-riding (defecting) on that cooperation.
    It is robustly designed to avoid being exploited by uncooperative players,
    while aggressively exploiting cooperative ones.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.my_last_action: Action | None = None
        self.punish_mode: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            self.my_last_action = Action.C
            self.punish_mode = False
            return Action.C
        if current_round_number == self.r - 1:
            self.my_last_action = Action.D
            self.punish_mode = False
            return Action.D
        previous_round_idx = current_round_number - 1
        my_prev_action_val = int(history.my_actions[previous_round_idx])
        opponent_prev_cooperators = np.sum(history.opponent_actions[previous_round_idx, :])
        C_prev = my_prev_action_val + opponent_prev_cooperators
        if C_prev >= self.k:
            self.my_last_action = Action.D
            self.punish_mode = False
            return Action.D
        else:
            if self.punish_mode:
                self.my_last_action = Action.D
                return Action.D
            if self.my_last_action == Action.C and C_prev <= 1:
                self.punish_mode = True
                self.my_last_action = Action.D
                return Action.D
            else:
                self.my_last_action = Action.C
                self.punish_mode = False
                return Action.C

class Strategy_EXPLOITATIVE_237(BaseStrategy):
    """
    Adaptive Predator strategy for the N-Player Public Goods Game.

    This strategy defaults to defection, aiming to free-ride on any existing cooperation.
    However, it incorporates a mechanism to "test" for cooperativeness when the collective
    contribution to the public good drops too low, temporarily investing in cooperation
    to potentially revive the public good for future exploitation. It strictly avoids
    cooperating if there's no evidence of others contributing or if its past attempts
    to revive cooperation have failed. It always defects in the final rounds due to
    backward induction.
    """
    LOW_COOPERATION_THRESHOLD_PERCENTAGE = 0.2
    HIGH_COOPERATION_THRESHOLD_PERCENTAGE = 0.6
    COOPERATION_TRIGGER_ROUNDS = 3
    MAX_TEST_COOPERATION_ATTEMPTS = 2
    ENDGAME_DEFECT_ROUNDS = 2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.Low_Cooperation_Threshold_Count = max(1, math.floor(self.n * self.LOW_COOPERATION_THRESHOLD_PERCENTAGE))
        self.High_Cooperation_Threshold_Count = max(1, math.floor(self.n * self.HIGH_COOPERATION_THRESHOLD_PERCENTAGE))
        self.recent_low_coop_streak_count = 0
        self.test_cooperation_attempts_in_phase = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t_current = state.round_number + 1
        if t_current == 1:
            self.recent_low_coop_streak_count = 0
            self.test_cooperation_attempts_in_phase = 0
            return Action.D
        prev_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_was_C = history.my_actions[-1]
        C_prev = prev_round_opponent_cooperators + (1 if my_action_prev_round_was_C else 0)
        if C_prev < self.Low_Cooperation_Threshold_Count:
            self.recent_low_coop_streak_count += 1
        else:
            self.recent_low_coop_streak_count = 0
        if t_current > self.r - self.ENDGAME_DEFECT_ROUNDS:
            return Action.D
        if C_prev == 0:
            self.test_cooperation_attempts_in_phase = 0
            return Action.D
        if C_prev >= self.High_Cooperation_Threshold_Count:
            self.test_cooperation_attempts_in_phase = 0
            return Action.D
        if self.recent_low_coop_streak_count >= self.COOPERATION_TRIGGER_ROUNDS and self.test_cooperation_attempts_in_phase < self.MAX_TEST_COOPERATION_ATTEMPTS:
            self.test_cooperation_attempts_in_phase += 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_238(BaseStrategy):
    """
    The "Adaptive Freerider" strategy is designed for maximum individual payoff in the N-Player Public Goods Game.
    It leverages observation of past rounds to adapt its behavior, prioritizing exploiting cooperative behavior,
    punishing widespread defection, and occasionally attempting to re-seed cooperation when stuck in a
    low-payoff equilibrium.

    Core Principles:
    1.  Selfish Default: Always defaults to Defect (D) as it's the dominant strategy in a single round.
    2.  Opportunistic Exploitation: Defects when a significant number of others are cooperating.
    3.  Protective Punishment: Defects when cooperation is very low, avoiding being a "sucker" and signaling
        that widespread defection is not rewarded by this player's cooperation.
    4.  Strategic Reset: Occasionally cooperates to "test the waters" or re-ignite cooperation if a long period
        of universal defection has occurred, creating future opportunities for exploitation.
    5.  Endgame Rationality: Defects in the final round to maximize immediate payoff, as there are no future
        rounds to influence.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        self.EXPL_THRESHOLD_C = max(2, math.ceil(n / 2))
        self.PUNISH_THRESHOLD_C = 1
        self.PUNISH_DURATION = 2
        self.RESET_COOP_FREQUENCY = max(3, n)
        self.current_state = 'EXPLORE'
        self.consecutive_defections_by_me = 0
        self.punish_rounds_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        n_rounds = self.game_description.n_rounds
        if current_round_0_indexed == n_rounds - 1:
            self.consecutive_defections_by_me += 1
            return Action.D
        if current_round_0_indexed == 0:
            self.consecutive_defections_by_me += 1
            return Action.D
        C_prev_opponents = sum(history.opponent_actions[-1, :])
        my_action_prev_is_C = history.my_actions[-1]
        C_prev = C_prev_opponents + (1 if my_action_prev_is_C else 0)
        action_for_this_round = Action.D
        if self.current_state == 'PUNISH':
            if self.punish_rounds_remaining > 0:
                self.punish_rounds_remaining -= 1
            if self.punish_rounds_remaining == 0:
                self.current_state = 'EXPLORE'
        if C_prev <= self.PUNISH_THRESHOLD_C:
            action_for_this_round = Action.D
            self.current_state = 'PUNISH'
            self.punish_rounds_remaining = self.PUNISH_DURATION
        elif C_prev >= self.EXPL_THRESHOLD_C:
            action_for_this_round = Action.D
            self.current_state = 'EXPLOIT'
        elif self.current_state == 'EXPLORE' and self.consecutive_defections_by_me >= self.RESET_COOP_FREQUENCY:
            action_for_this_round = Action.C
        else:
            action_for_this_round = Action.D
        if action_for_this_round == Action.D:
            self.consecutive_defections_by_me += 1
        else:
            self.consecutive_defections_by_me = 0
        return action_for_this_round

class Strategy_EXPLOITATIVE_239(BaseStrategy):
    """
    The "Adaptive Provocateur" strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by strategically exploiting
    cooperation from others while avoiding being exploited itself. It balances
    initial investment, free-riding, and self-preservation based on observed
    cooperation levels and game stage.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.critical_cooperation_threshold = math.floor(self.n / (self.k + 1))
        self.free_ride_threshold = math.ceil(self.n / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.D
        prev_my_action = history.my_actions[state.round_number - 1]
        prev_opponent_cooperators = sum(history.opponent_actions[state.round_number - 1, :])
        C_observed_prev = int(prev_my_action) + prev_opponent_cooperators
        if C_observed_prev < self.critical_cooperation_threshold:
            return Action.D
        elif C_observed_prev >= self.free_ride_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_240(BaseStrategy):
    """
    Adaptive Free-Rider - Hardened strategy for the N-Player Public Goods Game.

    This strategy balances initial investment to foster cooperation with aggressive
    free-riding and self-preservation. It is designed to maximize individual payoff
    by strategically free-riding on others' contributions while protecting against
    being a "sucker" in uncooperative environments.

    The strategy operates in phases:
    1.  **Initial Investment:** Cooperates for a few initial rounds to encourage overall
        group cooperation, creating a public good to potentially free-ride on.
    2.  **Adaptive Exploitation:** After the initial phase, if the group's cooperation
        level is sufficiently high (above `FREE_RIDE_THRESHOLD`), the strategy defects
        to free-ride.
    3.  **Re-induction/Protection:** If group cooperation drops below the threshold,
        the strategy attempts to re-induce it by cooperating again for a limited time
        (`PATIENCE_LIMIT`). If these attempts fail due to sustained low cooperation,
        it switches to a permanent defection mode to avoid further losses.
    4.  **Endgame Exploitation:** Always defects in the final round, as there are no
        future consequences to consider.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.N = game_description.n_players
        self.R = game_description.n_rounds
        self.K = game_description.k
        self.INITIAL_COOPERATE_ROUNDS = min(2, self.R - 1) if self.R > 1 else 0
        self.FREE_RIDE_THRESHOLD = math.ceil(self.N / 3.0)
        self.PATIENCE_LIMIT = 3
        self.persistent_consecutive_low_cooperation_rounds = 0
        self.persistent_permanent_defect_mode_active = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        pseudocode_round_number = state.round_number + 1
        if pseudocode_round_number == self.R:
            return Action.D
        if state.round_number > 0:
            total_cooperators_previous_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if not self.persistent_permanent_defect_mode_active:
                if total_cooperators_previous_round < self.FREE_RIDE_THRESHOLD:
                    self.persistent_consecutive_low_cooperation_rounds += 1
                    if self.persistent_consecutive_low_cooperation_rounds >= self.PATIENCE_LIMIT:
                        self.persistent_permanent_defect_mode_active = True
                else:
                    self.persistent_consecutive_low_cooperation_rounds = 0
        if self.persistent_permanent_defect_mode_active:
            return Action.D
        if pseudocode_round_number <= self.INITIAL_COOPERATE_ROUNDS:
            return Action.C
        if total_cooperators_previous_round >= self.FREE_RIDE_THRESHOLD:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_241(BaseStrategy):
    """
    Adaptive Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to be exploitative by primarily free-riding on the cooperation of others.
    However, it is adaptive, meaning it will strategically cooperate if the overall level
    of cooperation from other players falls too low, as a calculated act to "re-stimulate"
    the public good and ensure there's something to free-ride on later. It also accounts
    for the end-game effect by defecting in the final round.

    Decision Rules:
    1.  **Round 1 (0-indexed `state.round_number == 0`): COOPERATE (C)**
        Rationale: Start by signaling good faith, attempting to induce an initial phase
        of mutual cooperation which the strategy can then exploit.

    2.  **Last Round (0-indexed `state.round_number == self.n_rounds - 1`): DEFECT (D)**
        Rationale: In the final round, there are no future interactions for reciprocity
        or reputation building. The individual incentive to defect becomes paramount,
        as there are no consequences for damaging future cooperation. This is a purely
        rational, self-maximizing move for the terminal round.

    3.  **Middle Rounds (0 < `state.round_number` < `self.n_rounds - 1`): Adaptive Decision**
        -   **Observation:** The strategy observes the actions of all *other* players
            from the immediately preceding round.
        -   **Calculate Others' Cooperation:** `proportion_others_cooperating` is
            the fraction of other players who cooperated in the last round.
        -   **Define Exploitation Threshold:** `exploitation_threshold = k / n`.
            This value represents the share of the public good each player receives
            from one contribution.
        -   **Decision Logic:**
            *   **IF `proportion_others_cooperating >= exploitation_threshold`:** Play DEFECT (D).
                Rationale: If a significant portion of other players are cooperating
                (at or above the calculated threshold), the public good is being sufficiently
                replenished by others. The strategy seizes this opportunity to free-ride,
                maximizing its immediate private payoff while still benefiting from the
                public good built by others.
            *   **ELSE (`proportion_others_cooperating < exploitation_threshold`):** Play COOPERATE (C).
                Rationale: If the proportion of others cooperating falls below the threshold,
                it signals that the public good is weakening. A continued defection by this
                strategy risks a complete collapse of cooperation, which would reduce the
                payoff for all, including itself. By cooperating, the strategy makes a
                calculated investment to bolster the public good, hoping to encourage others
                to resume cooperation, thereby creating a more favorable environment for
                future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1:
            return Action.D
        previous_round_index = current_round_number - 1
        opponent_actions_last_round = history.opponent_actions[previous_round_index, :]
        num_other_cooperators_last_round = np.sum(opponent_actions_last_round)
        proportion_others_cooperating = num_other_cooperators_last_round / (self.n_players - 1)
        exploitation_threshold = self.k_factor / self.n_players
        if proportion_others_cooperating >= exploitation_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_242(BaseStrategy):
    """
    The Opportunistic Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by strategically inducing
    and then exploiting cooperation from others. It starts with an initial probe
    of cooperation, then adapts its behavior based on observed cooperation levels,
    prioritizing defection when cooperation is sufficiently high or to protect
    its endowment when cooperation is very low. It ruthlessly defects in the
    final rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.n_coop_threshold_low = max(1, math.floor(self.n / 5))
        self.n_coop_threshold_high = max(2, math.floor(self.n / 2))
        self.probe_probability = 0.15
        self.endgame_rounds = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round >= self.r - self.endgame_rounds:
            return Action.D
        my_prev_action_was_cooperate = history.my_actions[current_round - 1]
        opponents_prev_cooperators = np.sum(history.opponent_actions[current_round - 1, :])
        N_C_prev = int(my_prev_action_was_cooperate) + opponents_prev_cooperators
        if N_C_prev >= self.n_coop_threshold_high:
            return Action.D
        elif N_C_prev < self.n_coop_threshold_low:
            if random.random() < self.probe_probability:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_243(BaseStrategy):
    """
    The Opportunistic Cycle (TOC) is an exploitative, adaptive, and robust strategy for the
    N-Player Public Goods Game. It operates in cycles, alternating between "Cooperate Push"
    to establish or restore public goods, and "Free Ride" to exploit them. It treats
    cooperation as a strategic investment to enable greater defection payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.current_mode: str = 'COOPERATE_PUSH'
        self.rounds_in_current_mode: int = 0
        self.COOPERATION_THRESHOLD_BASE: float = 0.3
        self.TUNING_FACTOR_K_N: float = 1.5
        self.MAX_COOP_PUSH_ROUNDS: int = max(3, self.game_description.n_rounds // 4)
        self.MIN_FREE_RIDE_ROUNDS: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        k_factor = self.game_description.k
        action_to_take: Action
        if current_round_number == n_rounds - 1:
            action_to_take = Action.D
        elif current_round_number == 0:
            action_to_take = Action.C
            self.my_last_action = Action.C
            self.current_mode = 'COOPERATE_PUSH'
            self.rounds_in_current_mode = 1
        else:
            my_prev_action_was_C: bool = history.my_actions[-1]
            opponent_cooperators_prev_round: int = np.sum(history.opponent_actions[-1, :])
            total_cooperators_prev_round: int = int(my_prev_action_was_C) + opponent_cooperators_prev_round
            P_C_t_minus_1: float = total_cooperators_prev_round / n_players
            effective_coop_threshold: float = max(self.COOPERATION_THRESHOLD_BASE, k_factor / (self.TUNING_FACTOR_K_N * n_players))
            if self.current_mode == 'COOPERATE_PUSH':
                self.rounds_in_current_mode += 1
                if P_C_t_minus_1 >= effective_coop_threshold and self.rounds_in_current_mode >= self.MIN_FREE_RIDE_ROUNDS:
                    action_to_take = Action.D
                    self.current_mode = 'FREE_RIDE'
                    self.rounds_in_current_mode = 1
                elif self.rounds_in_current_mode >= self.MAX_COOP_PUSH_ROUNDS:
                    action_to_take = Action.D
                    self.current_mode = 'FREE_RIDE'
                    self.rounds_in_current_mode = 1
                else:
                    action_to_take = Action.C
            elif self.current_mode == 'FREE_RIDE':
                self.rounds_in_current_mode += 1
                if P_C_t_minus_1 < effective_coop_threshold:
                    action_to_take = Action.C
                    self.current_mode = 'COOPERATE_PUSH'
                    self.rounds_in_current_mode = 1
                else:
                    action_to_take = Action.D
            self.my_last_action = action_to_take
        return action_to_take

class Strategy_EXPLOITATIVE_244(BaseStrategy):
    """
    The Calculated Leech is an aggressively exploitative, adaptive, and robust approach for the N-Player
    Public Goods Game. It aims to maximize its own payoff by free-riding on the cooperation of others,
    contributing only when strategically necessary to prevent a total collapse of the public good that
    it seeks to exploit. It explicitly accounts for the finite nature of the game through an end-game phase.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        self.initial_probe_rounds = 2
        self.cooperation_bounty_threshold = max(0.25, k / n + 0.05)
        self.reseed_cooperation_threshold = min(0.15, k / n - 0.05)
        self.endgame_horizon = max(1, min(3, r // 4))
        if self.reseed_cooperation_threshold >= self.cooperation_bounty_threshold:
            self.reseed_cooperation_threshold = self.cooperation_bounty_threshold / 2
            if self.reseed_cooperation_threshold < 0.01 and self.cooperation_bounty_threshold > 0:
                self.reseed_cooperation_threshold = 0.01

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        P_c_all_prev = 0.0
        if current_round_1_indexed > 1:
            prev_round_my_action_is_c = history.my_actions[-1]
            prev_round_opponent_actions_are_c = history.opponent_actions[-1, :]
            num_cooperators_prev = int(prev_round_my_action_is_c) + np.sum(prev_round_opponent_actions_are_c)
            P_c_all_prev = num_cooperators_prev / n_players
        if current_round_1_indexed > total_rounds - self.endgame_horizon:
            return Action.D
        elif current_round_1_indexed <= self.initial_probe_rounds:
            if current_round_1_indexed == 1:
                return Action.C
            elif P_c_all_prev >= self.cooperation_bounty_threshold:
                return Action.D
            else:
                return Action.C
        elif P_c_all_prev >= self.cooperation_bounty_threshold:
            return Action.D
        elif P_c_all_prev < self.reseed_cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_246(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by strategically free-riding on
    others' cooperation and attempting to re-establish a public good for exploitation
    if it collapses. It starts by cooperating to "bait" others, defects when
    cooperation is sufficiently high, re-baits if cooperation collapses while it was
    defecting, and cuts losses if its re-baiting fails. The final round is always
    a defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.ET = max(2, math.ceil(self.n_players / self.k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        my_action_prev_round_is_cooperate = history.my_actions[-1]
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + int(my_action_prev_round_is_cooperate)
        if num_cooperators_prev_round >= self.ET:
            return Action.D
        elif not my_action_prev_round_is_cooperate:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_247(BaseStrategy):
    """
    The "Persistent Free-Rider with Recoil (PFRR)" strategy.

    This strategy aims to maximize private gain by largely free-riding, only
    contributing when the collective good is too weak to be worth exploiting.
    Once a contribution is made, it commits to a period of defection to
    capitalize on its "investment".

    Internal State Variables:
    - self.defection_countdown (int): Tracks how many more rounds the player is
      committed to defecting after a previous cooperative act. Initialized to 0.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defection_countdown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        k_factor = self.game_description.k
        cooperation_revival_threshold = max(1, math.floor(k_factor / 2))
        fixed_defection_streak = 3
        remaining_rounds_after_current = total_rounds - (current_round_0_indexed + 1)
        if remaining_rounds_after_current <= 1:
            defection_streak_limit = 0
        else:
            defection_streak_limit = min(fixed_defection_streak, remaining_rounds_after_current - 1)
        if current_round_0_indexed == 0:
            self.defection_countdown = 0
            return Action.D
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if self.defection_countdown > 0:
            self.defection_countdown -= 1
        if self.defection_countdown > 0:
            return Action.D
        num_coop_in_prev_round = sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        if num_coop_in_prev_round < cooperation_revival_threshold:
            self.defection_countdown = defection_streak_limit
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_248(BaseStrategy):
    """
    The "Adaptive Free-Rider" (AFR) strategy is designed to be highly exploitative in the N-Player Public Goods Game.
    It attempts to induce cooperation in others, then free-rides on their contributions, while also protecting itself
    from being consistently exploited. It adapts to the observed level of cooperation in the group and incorporates
    a mechanism to avoid being a "sucker" against persistently uncooperative opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = max(1, math.floor(self.game_description.k))
        self.rounds_below_threshold_counter = 0
        self.forced_defect_mode = False
        self.persistence_threshold = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == total_rounds - 1:
            return Action.D
        else:
            total_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
            if self.forced_defect_mode:
                return Action.D
            else:
                if total_cooperators_prev_round < self.cooperation_threshold:
                    self.rounds_below_threshold_counter += 1
                else:
                    self.rounds_below_threshold_counter = 0
                if self.rounds_below_threshold_counter >= self.persistence_threshold:
                    self.forced_defect_mode = True
                    return Action.D
                elif total_cooperators_prev_round >= self.cooperation_threshold:
                    return Action.D
                else:
                    return Action.C

class Strategy_EXPLOITATIVE_249(BaseStrategy):
    """
    The Adaptive Provocateur (AP) strategy is designed for the N-Player Public Goods Game.
    It prioritizes self-payoff by generally defecting to free-ride, but strategically
    cooperates to "re-seed" cooperation from others when the public good is too low.
    It begins with an initial probe of cooperation and becomes purely exploitative
    (defects) in the final rounds of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.HCT_proportion = 0.6
        self.LCT_proportion = 0.3
        self.ENDGAME_ROUNDS = min(self.game_description.n_rounds - 1, 3)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if state.round_number == 0:
            return Action.C
        if current_round_1_indexed >= total_rounds - self.ENDGAME_ROUNDS + 1:
            return Action.D
        C_total_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        my_last_action_bool = history.my_actions[-1]
        C_proportion_prev = C_total_prev / n_players
        if C_proportion_prev >= self.HCT_proportion:
            return Action.D
        elif C_proportion_prev < self.LCT_proportion:
            return Action.C
        elif my_last_action_bool:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_250(BaseStrategy):
    """
    Calculated Opportunist: An exploitative, adaptive, and robust strategy for the N-Player Public Goods Game.
    It primarily free-rides, but adapts to cooperate when collective contribution levels drop too low,
    acting as a temporary investment to restore profitability for future free-riding.
    It defects in the first and last rounds as a fixed rule.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.floor(self.game_description.n_players / 2)
        self.my_last_action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_rounds = self.game_description.n_rounds
        action_to_take = Action.D
        if current_round == 0:
            action_to_take = Action.D
        elif current_round == n_rounds - 1:
            action_to_take = Action.D
        else:
            num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            my_action_prev_round_was_C_int = int(history.my_actions[-1])
            total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_action_prev_round_was_C_int
            if self.my_last_action == Action.C:
                if total_cooperators_prev_round < self.cooperation_threshold:
                    action_to_take = Action.D
                else:
                    action_to_take = Action.D
            elif total_cooperators_prev_round < self.cooperation_threshold:
                action_to_take = Action.C
            else:
                action_to_take = Action.D
        self.my_last_action = action_to_take
        return action_to_take

class Strategy_EXPLOITATIVE_251(BaseStrategy):
    """
    Adaptive Exploiter with Reciprocal Thresholds (AERT) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by initially probing for cooperation,
    then exploiting established cooperation, punishing low cooperation, and defecting
    in the end-game phase.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.INITIAL_PROBE_ROUNDS = 2
        self.UNRAVEL_ROUNDS = 1
        self.EXPLOIT_THRESHOLD_RATIO = 0.75
        self.PUNISH_THRESHOLD_RATIO = 0.4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed > self.n_rounds - self.UNRAVEL_ROUNDS:
            return Action.D
        if current_round_1_indexed <= self.INITIAL_PROBE_ROUNDS:
            return Action.C
        last_round_opponent_actions = history.opponent_actions[-1, :]
        C_others_prev = np.sum(last_round_opponent_actions)
        total_others = self.n_players - 1
        C_others_prev_ratio = C_others_prev / total_others
        if C_others_prev_ratio >= self.EXPLOIT_THRESHOLD_RATIO:
            return Action.D
        elif C_others_prev_ratio < self.PUNISH_THRESHOLD_RATIO:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_252(BaseStrategy):
    """
    Adaptive Free-Rider: This strategy implements an exploitative approach for the
    N-Player Public Goods Game. It begins by cooperating to bait other players
    into contributing. It then seeks to free-ride on sufficient cooperation from
    others by defecting. If cooperation among others drops too low, the strategy
    may temporarily cooperate again (re-lure) if enough rounds remain, hoping
    to re-induce contributions for future exploitation. In the final round or
    when re-luring is no longer profitable, it reverts to defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.INITIAL_COOPERATE_ROUNDS = 2
        self.COOPERATION_MAINTAIN_THRESHOLD = 0.4
        self.REINDUCEMENT_ROUNDS_LEFT_THRESHOLD_ABS = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed <= self.INITIAL_COOPERATE_ROUNDS:
            return Action.C
        if current_round_1_indexed == r:
            return Action.D
        opponent_actions_prev_round = history.opponent_actions[-1, :]
        others_cooperated_last_round = sum(opponent_actions_prev_round)
        n_others = n - 1
        cooperation_ratio_others = float(others_cooperated_last_round) / n_others
        if cooperation_ratio_others >= self.COOPERATION_MAINTAIN_THRESHOLD:
            return Action.D
        else:
            rounds_left = r - current_round_1_indexed
            if rounds_left >= self.REINDUCEMENT_ROUNDS_LEFT_THRESHOLD_ABS:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_253(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize personal payoff by primarily free-riding on the
    cooperation of others. It defects by default, but strategically cooperates
    for a short period ("recharge") if the observed level of cooperation from
    other players drops too low, in order to stimulate future cooperation to exploit.
    It always defects in the final round and in the very first round to probe.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.COOPERATION_RECHARGE_THRESHOLD = 0.33
        self.RECHARGE_ROUNDS = 1
        self.MEMORY_WINDOW = 5
        self.recharge_countdown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.n_rounds - 1:
            self.recharge_countdown = 0
            return Action.D
        if self.recharge_countdown > 0:
            self.recharge_countdown -= 1
            return Action.C
        elif current_round_0_indexed == 0:
            return Action.D
        else:
            relevant_rounds_to_consider = min(current_round_0_indexed, self.MEMORY_WINDOW)
            recent_opponent_actions = history.opponent_actions[-relevant_rounds_to_consider:]
            total_other_cooperations = np.sum(recent_opponent_actions)
            total_other_opportunities = recent_opponent_actions.size
            average_other_cooperation_rate = total_other_cooperations / total_other_opportunities
            if average_other_cooperation_rate < self.COOPERATION_RECHARGE_THRESHOLD:
                self.recharge_countdown = self.RECHARGE_ROUNDS - 1
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_254(BaseStrategy):
    """
    The Strategic Free-Rider strategy for the N-Player Public Goods Game.

    This strategy is designed to be highly exploitative, prioritizing self-payoff
    maximization. It aims to free-ride on the contributions of others. It will
    only contribute strategically and minimally to cultivate or re-establish a
    cooperative environment from which it can then benefit by defecting. It is
    adaptive to the observed level of cooperation from other players.

    State transitions:
    - EXPLORING (initial probe) -> EXPLOITING
    - EXPLOITING (default defect) -> RETREATING (if others' coop too low)
    - RETREATING (continued defect) -> REBUILDING (if others' coop recovers)
    - REBUILDING (temp cooperate) -> EXPLOITING (after duration) or RETREATING (if others' coop fails during rebuild)
    - Last Round: Always Defect (Endgame Protocol)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.TH_LOW = 0.1
        self.TH_REBUILD = 0.75
        self.REBUILD_DURATION = 2
        self.my_current_state = 'EXPLORING'
        self.rounds_in_current_state = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_idx = state.round_number
        if current_round_idx == n_rounds - 1:
            return Action.D
        if current_round_idx == 0:
            return Action.C
        num_coop_others_prev = np.sum(history.opponent_actions[current_round_idx - 1, :])
        coop_ratio_others = num_coop_others_prev / (n_players - 1)
        action_to_play = Action.D
        if self.my_current_state == 'EXPLORING':
            self.my_current_state = 'EXPLOITING'
            self.rounds_in_current_state = 0
        if self.my_current_state == 'EXPLOITING':
            action_to_play = Action.D
            self.rounds_in_current_state += 1
            if coop_ratio_others < self.TH_LOW:
                self.my_current_state = 'RETREATING'
                self.rounds_in_current_state = 0
        elif self.my_current_state == 'RETREATING':
            action_to_play = Action.D
            self.rounds_in_current_state += 1
            if coop_ratio_others >= self.TH_REBUILD:
                self.my_current_state = 'REBUILDING'
                self.rounds_in_current_state = 0
        elif self.my_current_state == 'REBUILDING':
            action_to_play = Action.C
            self.rounds_in_current_state += 1
            if self.rounds_in_current_state >= self.REBUILD_DURATION:
                self.my_current_state = 'EXPLOITING'
                self.rounds_in_current_state = 0
            elif coop_ratio_others < self.TH_LOW and self.rounds_in_current_state > 1:
                self.my_current_state = 'RETREATING'
                self.rounds_in_current_state = 0
        return action_to_play

class Strategy_EXPLOITATIVE_255(BaseStrategy):
    """
    The Shrewd Opportunist strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own total payoff by primarily free-riding
    on the contributions of others, while strategically employing limited
    cooperation only when it serves to induce greater overall public good
    creation that can then be exploited.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_low = self.n / 4.0
        self.cooperation_threshold_high = self.n / 2.0
        self.lure_window = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        if current_round_1_indexed == self.r - 1:
            return Action.D
        num_past_rounds_to_consider = min(self.lure_window, state.round_number)
        total_cooperators_sum = 0
        for i in range(state.round_number - num_past_rounds_to_consider, state.round_number):
            my_action_val = 1 if history.my_actions[i] else 0
            opponent_cooperators_val = np.sum(history.opponent_actions[i, :])
            total_cooperators_sum += my_action_val + opponent_cooperators_val
        avg_cooperators_recent = total_cooperators_sum / num_past_rounds_to_consider
        my_last_action_is_cooperate = history.my_actions[state.round_number - 1]
        if avg_cooperators_recent >= self.cooperation_threshold_high:
            return Action.D
        elif avg_cooperators_recent < self.cooperation_threshold_low:
            if not my_last_action_is_cooperate:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_256(BaseStrategy):
    """
    The Adaptive Leech strategy aims to be highly exploitative by free-riding whenever
    a minimal level of cooperation exists, and only contributing when absolutely
    necessary to prevent a complete collapse of cooperation that would eliminate
    free-riding opportunities.

    Decision Rules:
    - Round 1: Cooperate (C) to probe and encourage initial cooperation.
    - Last Round: Defect (D) due to no future consequences.
    - Intermediate Rounds:
        - If 0 other players cooperated in the previous round: Defect (D) (no public good to exploit).
        - If 1 other player cooperated in the previous round: Cooperate (C) (minimal re-seeding to sustain fragile cooperation).
        - If 2 or more other players cooperated in the previous round: Defect (D) (exploit robust public good).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        m_t_minus_1 = np.sum(history.opponent_actions[-1, :])
        if m_t_minus_1 == 0:
            return Action.D
        elif m_t_minus_1 == 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_257(BaseStrategy):
    """
    Opportunistic Exploiter (OE) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by encouraging other players to cooperate,
    and then defecting to free-ride on their contributions. It's designed to be adaptive
    and robust by adjusting its behavior based on the collective actions of other players
    in the previous round, while also accounting for the specific dynamics of repeated games
    (first and last rounds).

    The core principle is to induce a state where the agent's defection is most profitable
    (i.e., when others are cooperating), and to use strategic cooperation as a tool to *reset*
    or *build* this cooperative environment when it breaks down. The strategy never cooperates
    out of pure altruism but as a calculated move for future self-gain.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        my_last_action_was_cooperate: bool = history.my_actions[-1]
        others_coop_count_last_round = sum(history.opponent_actions[-1, :])
        if others_coop_count_last_round == self.game_description.n_players - 1:
            return Action.D
        elif not my_last_action_was_cooperate:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_258(BaseStrategy):
    """
    The Opportunistic Free-Rider strategy prioritizes maximizing its own payoff by free-riding on others' contributions.
    It strategically cooperates only if the game has devolved into universal defection, and only as a calculated,
    temporary investment to "prime the pump" for others' future cooperation, from which it can then free-ride.
    It defects in the first and last rounds. It monitors the level of cooperation and reverts to defection
    immediately after a successful cooperation signal.
    """
    DEFECTION_TOLERANCE_THRESHOLD: int = 3

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action_was_cooperate_for_signal: bool = False
        self.consecutive_rounds_all_defected: int = 0
        self.MIN_ROUNDS_FOR_SIGNAL: int = self.DEFECTION_TOLERANCE_THRESHOLD + 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_idx = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_idx == 0:
            self.my_last_action_was_cooperate_for_signal = False
            self.consecutive_rounds_all_defected = 0
            return Action.D
        if current_round_0_idx == total_rounds - 1:
            return Action.D
        prev_round_my_action = history.my_actions[current_round_0_idx - 1]
        prev_round_opponent_actions = history.opponent_actions[current_round_0_idx - 1, :]
        C_prev_total = np.sum(prev_round_opponent_actions) + (1 if prev_round_my_action else 0)
        if self.my_last_action_was_cooperate_for_signal:
            self.my_last_action_was_cooperate_for_signal = False
            if C_prev_total > 0:
                self.consecutive_rounds_all_defected = 0
                return Action.D
            else:
                self.consecutive_rounds_all_defected += 1
                return Action.D
        elif C_prev_total > 0:
            self.consecutive_rounds_all_defected = 0
            return Action.D
        else:
            self.consecutive_rounds_all_defected += 1
            rounds_remaining_including_current = total_rounds - current_round_0_idx
            if self.consecutive_rounds_all_defected >= self.DEFECTION_TOLERANCE_THRESHOLD and rounds_remaining_including_current >= self.MIN_ROUNDS_FOR_SIGNAL:
                self.my_last_action_was_cooperate_for_signal = True
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_259(BaseStrategy):
    """
    Opportunistic Exploiter strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by selectively cooperating to induce
    others' contributions, then aggressively free-riding on those contributions.
    It probes with an initial cooperation, exploits when cooperation is high enough,
    and attempts to re-induce cooperation with minimal investment if it dips,
    otherwise cutting losses. It defects in the final round due to end-game rationality.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Opportunistic Exploiter strategy.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number.
            history: An object containing historical actions and payoffs for the player
                     and opponents, or None for the very first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        k_factor = self.game_description.k
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        cooperation_count_others_prev_round = np.sum(history.opponent_actions[-1, :])
        n_others = n_players - 1
        cooperation_rate_others_prev_round = cooperation_count_others_prev_round / n_others
        exploitation_threshold = k_factor / n_players
        re_induce_threshold = exploitation_threshold / 2
        my_last_action_was_C = history.my_actions[-1]
        if my_last_action_was_C:
            if cooperation_rate_others_prev_round >= exploitation_threshold:
                return Action.D
            else:
                return Action.D
        elif cooperation_rate_others_prev_round >= exploitation_threshold:
            return Action.D
        elif cooperation_rate_others_prev_round >= re_induce_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_260(BaseStrategy):
    """
    The "Adaptive Exploiter" strategy is designed to be highly exploitative within the N-Player Public Goods Game.
    It aims to maximize its own payoff by selectively cooperating to encourage collective contribution,
    then defecting to reap private benefits, while also protecting itself against widespread defection.
    It identifies periods of high collective cooperation, contributes just enough to maintain or foster that cooperation,
    and then strategically defects. It avoids being a "sucker" by punishing low cooperation, but is "forgiving"
    enough to re-engage if the environment improves, ensuring a steady stream of exploitable opportunities.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.DPT = self.game_description.k / self.game_description.n_players
        self.CRT = 0.5
        self.ETR = 3
        self.my_consecutive_C_count = 0
        self.my_last_exploit_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        total_players = self.game_description.n_players
        if current_round_t_1_indexed == total_rounds:
            self.my_consecutive_C_count = 0
            return Action.D
        if current_round_t_1_indexed == 1:
            self.my_consecutive_C_count = 1
            return Action.C
        num_others_cooperated_t_minus_1 = sum(history.opponent_actions[-1, :])
        n_minus_1 = total_players - 1
        if n_minus_1 > 0:
            fraction_others_cooperated_t_minus_1 = num_others_cooperated_t_minus_1 / n_minus_1
        else:
            fraction_others_cooperated_t_minus_1 = 0
        if fraction_others_cooperated_t_minus_1 < self.DPT:
            self.my_consecutive_C_count = 0
            return Action.D
        elif self.my_consecutive_C_count >= self.ETR and fraction_others_cooperated_t_minus_1 >= self.CRT and (current_round_t_1_indexed - self.my_last_exploit_round > self.ETR):
            self.my_consecutive_C_count = 0
            self.my_last_exploit_round = current_round_t_1_indexed
            return Action.D
        else:
            self.my_consecutive_C_count += 1
            return Action.C

class Strategy_EXPLOITATIVE_261(BaseStrategy):
    """
    The Opportunistic Prober strategy aims to primarily free-ride on any existing cooperation but also
    attempts to strategically "probe" for more cooperation when levels are low, hoping to induce
    future cooperation that can then be exploited.

    Decision Rules:
    - Round 1 (t=1): Defect (D) to test the environment and minimize initial risk.
    - Last Round (t=r): Defect (D) due to backward induction.
    - Middle Rounds (1 < t < r):
        - Observe 'num_other_cooperators_prev_round': total number of players (excluding self) who played 'C' in the previous round.
        - If num_other_cooperators_prev_round >= k (Exploitation Threshold):
            - Action: Defect (D). Free-ride on high cooperation.
        - Else (Low/Moderate Cooperation):
            - If (current_round_1_indexed % PROBE_INTERVAL == 0) and (current_round_1_indexed > 1):
                - Action: Cooperate (C). Periodically "probe" to encourage more future cooperation for exploitation.
            - Else:
                - Action: Defect (D). Revert to individually dominant strategy.
    """
    PROBE_INTERVAL = 4

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds_1_indexed = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == total_rounds_1_indexed:
            return Action.D
        num_other_cooperators_prev_round = sum(history.opponent_actions[state.round_number - 1, :])
        if num_other_cooperators_prev_round >= self.game_description.k:
            return Action.D
        elif current_round_1_indexed > 1 and current_round_1_indexed % self.PROBE_INTERVAL == 0:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_262(BaseStrategy):
    """
    The "Adaptive Predator" (AP) strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by encouraging others to cooperate,
    then free-riding, and punishing dips in overall cooperation. It incorporates
    conditional cooperation/defection and exploits the finite nature of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.initial_cooperation_rounds = 1
        self.free_ride_threshold = 0.6
        self.cooperation_drop_tolerance_factor = 0.8
        self.punishment_duration = 2
        self.current_mode = 'INITIAL_TEST'
        self.punishment_rounds_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        num_cooperators_prev_round = 0
        if state.round_number > 0:
            total_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            my_action_prev_round_is_c = history.my_actions[-1]
            num_cooperators_prev_round = total_opponent_cooperators_prev_round + (1 if my_action_prev_round_is_c else 0)
        rounds_remaining = self.r - current_round_1_indexed + 1
        if rounds_remaining <= 2:
            self.current_mode = 'END_GAME'
            return Action.D
        if self.current_mode == 'INITIAL_TEST':
            if current_round_1_indexed <= self.initial_cooperation_rounds:
                return Action.C
            else:
                cooperation_ratio = num_cooperators_prev_round / self.n
                if cooperation_ratio >= self.free_ride_threshold:
                    self.current_mode = 'FREE_RIDE_MODE'
                    return Action.D
                else:
                    self.current_mode = 'OBSERVE'
                    return Action.C
        elif self.current_mode == 'OBSERVE':
            cooperation_ratio = num_cooperators_prev_round / self.n
            if cooperation_ratio >= self.free_ride_threshold:
                self.current_mode = 'FREE_RIDE_MODE'
                return Action.D
            else:
                return Action.C
        elif self.current_mode == 'FREE_RIDE_MODE':
            cooperation_ratio = num_cooperators_prev_round / self.n
            if cooperation_ratio >= self.free_ride_threshold * self.cooperation_drop_tolerance_factor:
                return Action.D
            else:
                self.current_mode = 'PUNISH_MODE'
                self.punishment_rounds_counter = 1
                return Action.D
        elif self.current_mode == 'PUNISH_MODE':
            action_for_this_round = Action.D
            self.punishment_rounds_counter += 1
            if self.punishment_rounds_counter >= self.punishment_duration:
                self.current_mode = 'OBSERVE'
                self.punishment_rounds_counter = 0
            return action_for_this_round
        return Action.D

class Strategy_EXPLOITATIVE_263(BaseStrategy):
    """
    The "Adaptive Exploit" strategy for the N-Player Public Goods Game.
    This strategy aims for individual payoff maximization by strategically
    leveraging and exploiting the cooperation of others. It adapts to the
    observed level of cooperation in the group, operating in distinct phases:
    a probing phase, a main exploitation/adaptation phase, and an endgame phase.
    """
    N_PROBE_ROUNDS = 2
    N_MEMORY = 3
    THRESHOLD_EXPLOIT_RATIO = 0.5
    THRESHOLD_RE_COOP_RATIO = 0.25
    N_ENDGAME_ROUNDS = 1

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.THRESHOLD_EXPLOIT = math.ceil(self.n * self.THRESHOLD_EXPLOIT_RATIO)
        self.THRESHOLD_RE_COOP = math.ceil(self.n * self.THRESHOLD_RE_COOP_RATIO)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1

        def _get_total_cooperators_in_past_round(round_idx: int) -> int:
            if history is None or round_idx < 0 or round_idx >= history.opponent_actions.shape[0]:
                return 0
            opponent_cooperators = int(np.sum(history.opponent_actions[round_idx, :]))
            my_cooperation = 0
            if round_idx < history.my_actions.shape[0]:
                my_cooperation = int(history.my_actions[round_idx])
            return opponent_cooperators + my_cooperation
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed <= self.N_PROBE_ROUNDS:
            c_prev_round = _get_total_cooperators_in_past_round(current_round_0_indexed - 1)
            if c_prev_round >= self.THRESHOLD_RE_COOP:
                return Action.C
            else:
                return Action.D
        if current_round_1_indexed > self.r - self.N_ENDGAME_ROUNDS:
            return Action.D
        total_cooperators_sum = 0
        rounds_in_memory_count = 0
        start_round_for_avg_0_indexed = max(0, current_round_0_indexed - self.N_MEMORY)
        end_round_for_avg_0_indexed = current_round_0_indexed - 1
        for round_idx in range(start_round_for_avg_0_indexed, end_round_for_avg_0_indexed + 1):
            total_cooperators_sum += _get_total_cooperators_in_past_round(round_idx)
            rounds_in_memory_count += 1
        if rounds_in_memory_count == 0:
            avg_cooperators = 0.0
        else:
            avg_cooperators = float(total_cooperators_sum) / rounds_in_memory_count
        if avg_cooperators >= self.THRESHOLD_EXPLOIT:
            return Action.D
        elif avg_cooperators >= self.THRESHOLD_RE_COOP:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_264(BaseStrategy):
    """
    The Pragmatic Leech aims to maximize its own total payoff over r rounds by strategically exploiting
    the cooperation of other players. It seeks to contribute as little as possible to the public good
    while ensuring a sufficient level of collective cooperation is maintained by others to generate
    a profitable public good pool. It will only cooperate when it deems the overall level of cooperation
    from others to be too low to exploit effectively, or when it needs to "re-seed" cooperation
    to prevent a total collapse into universal defection.

    Decision Rules:
    - Round 1: Cooperate (C) to seed cooperation.
    - Final Round: Defect (D) to maximize last-round payoff without consequence.
    - Intermediate Rounds:
        - Observe the number of other players (C_others_prev) who cooperated in the previous round.
        - Calculate Cooperation_Sustain_Threshold = floor(n / k).
        - If C_others_prev < Cooperation_Sustain_Threshold: Cooperate (C) to re-stimulate cooperation.
        - If C_others_prev >= Cooperation_Sustain_Threshold: Defect (D) to exploit the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_sustain_threshold = math.floor(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        C_others_prev = int(np.sum(prev_round_opponent_actions))
        if C_others_prev < self.cooperation_sustain_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_265(BaseStrategy):
    """
    The Exploitative Equilibrium Seeker (EES) strategy for the N-Player Public Goods Game.

    This strategy is self-interested, aiming to free-ride on others' contributions.
    It strategically cooperates in the first round to probe and potentially initiate
    group cooperation. In intermediate rounds, it defects if cooperation is robust
    enough to exploit, or too weak to be worth contributing to (collapse).
    It cooperates only in a "middle ground" to stabilize or increase cooperation
    for future exploitation. In the final round, it always defects due to no future
    consequences.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.EXPL_THRESHOLD = math.ceil(self.n / 2)
        self.COLLAPSE_THRESHOLD = max(1, math.floor(self.k / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        num_C_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_C_prev >= self.EXPL_THRESHOLD:
            return Action.D
        if num_C_prev < self.COLLAPSE_THRESHOLD:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_266(BaseStrategy):
    """
    The Adaptive Predator strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by free-riding on the contributions
    of other players. It defaults to defection, but provides minimal, reactive
    cooperation only when necessary to prevent the complete collapse of the public good
    (the "prey pool"). After such a "rescue" cooperation, the strategy immediately
    reverts to defection to maximize exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.my_last_action = Action.D
        self.critical_cooperation_threshold = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        action_to_take = Action.D
        if current_round_t == self.n_rounds:
            action_to_take = Action.D
        elif current_round_t == 1:
            action_to_take = Action.D
        else:
            total_cooperators_previous_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
            if self.my_last_action == Action.C:
                action_to_take = Action.D
            elif total_cooperators_previous_round < self.critical_cooperation_threshold:
                action_to_take = Action.C
            else:
                action_to_take = Action.D
        self.my_last_action = action_to_take
        return action_to_take

class Strategy_EXPLOITATIVE_267(BaseStrategy):
    """
    The Deceptive Reciprocator strategy: This strategy aims to exploit cooperative tendencies
    in other players by mimicking cooperation during an initial phase to encourage a larger
    public good, before switching to full defection to free-ride on the established cooperation.
    It is robust by defaulting to defection when cooperation is not observed and for the
    crucial end-game rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.r_deception_end = max(1, math.floor(self.n_rounds * 0.5))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        elif current_round_1_indexed <= self.r_deception_end:
            num_cooperators_others_prev_round = sum(history.opponent_actions[-1, :])
            num_defectors_others_prev_round = self.n_players - 1 - num_cooperators_others_prev_round
            if num_cooperators_others_prev_round > num_defectors_others_prev_round:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_268(BaseStrategy):
    """
    Adaptive Forgiving Free-Rider (AFFR) is an exploitative strategy for the N-Player Public Goods Game.
    It aims to induce cooperation from opponents, free-ride on them when cooperation is high, and
    protect itself from being exploited when cooperation is low.

    The strategy balances initial goodwill with vigilant monitoring of opponents' cooperation levels,
    leading to either continued cooperation, punishment (defection), or exploitation (defection).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        self.initial_cooperate_rounds = max(1, min(math.floor(r * 0.1), 3))
        self.end_game_rounds = 1
        min_coop_for_me_to_consider_cooperating_count = max(0, math.ceil(n / k - 1.0))
        self.punishment_threshold_percentage = 0.0
        if n - 1 > 0:
            self.punishment_threshold_percentage = min_coop_for_me_to_consider_cooperating_count / (n - 1.0)
        self.exploitation_threshold_percentage = 0.75

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed > r - self.end_game_rounds:
            return Action.D
        if current_round_1_indexed <= self.initial_cooperate_rounds:
            return Action.C
        num_other_cooperators_previous_round = np.sum(history.opponent_actions[-1, :])
        other_cooperation_rate = 0.0
        n_minus_1 = float(n - 1)
        if n_minus_1 > 0:
            other_cooperation_rate = num_other_cooperators_previous_round / n_minus_1
        if other_cooperation_rate < self.punishment_threshold_percentage:
            return Action.D
        elif other_cooperation_rate >= self.exploitation_threshold_percentage:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_269(BaseStrategy):
    """
    The Opportunistic Provocateur strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by strategically inducing and then
    free-riding on others' cooperation, while preventing a complete collapse of the
    public good. It operates in three phases: Provocation, Exploitation & Revival,
    and Endgame, adapting its behavior based on game parameters and observed cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        self.r_provoke = max(1, min(2, math.floor(r / 5)))
        self.r_endgame = max(1, math.floor(r / 4))
        self.C_revive_threshold = max(1, math.floor(0.5 * n / k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        if t <= self.r_provoke and t <= self.game_description.n_rounds - self.r_endgame:
            return Action.C
        if t > self.game_description.n_rounds - self.r_endgame:
            return Action.D
        C_prev = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if C_prev <= self.C_revive_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_270(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy for the N-Player Public Goods Game.
    This strategy is designed to be highly exploitative, aiming to maximize
    its own payoff by strategically free-riding on the contributions of others.
    It initiates with cooperation to establish a cooperative environment, then
    consistently defects to exploit. If the average cooperation level of others
    drops below a certain threshold, it temporarily re-cooperates ("re-probes")
    to try and re-seed cooperation before resuming exploitation.
    Crucially, it always defects in the final rounds of the game, leveraging
    the principle of backward induction for maximal immediate gains.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.INITIAL_COOP_ROUNDS = 2
        self.TERMINAL_DEFECTION_ROUNDS = 3
        self.OBSERVATION_WINDOW_SIZE = 4
        self.LOW_COOPERATION_THRESHOLD = 0.25
        self.RE_PROBE_DURATION = 1
        self._strategy_state = 'INITIAL_COOP'
        self._re_probe_rounds_left = 0
        self._others_past_cooperation_counts = []

    def _calculate_avg_cooperation_rate_others(self) -> float:
        """
        Calculates the average cooperation rate of other players over the
        `OBSERVATION_WINDOW_SIZE` most recent completed rounds.
        """
        if not self._others_past_cooperation_counts:
            return 0.0
        relevant_coop_counts = self._others_past_cooperation_counts[-self.OBSERVATION_WINDOW_SIZE:]
        if not relevant_coop_counts:
            return 0.0
        total_coop_others_in_window = sum(relevant_coop_counts)
        num_other_players = self.game_description.n_players - 1
        if num_other_players <= 0:
            return 0.0
        max_possible_coop_others_in_window = len(relevant_coop_counts) * num_other_players
        if max_possible_coop_others_in_window == 0:
            return 0.0
        return total_coop_others_in_window / max_possible_coop_others_in_window

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if state.round_number > 0 and history is not None:
            cooperators_from_others_last_round = np.sum(history.opponent_actions[-1, :])
            self._others_past_cooperation_counts.append(int(cooperators_from_others_last_round))
        if current_round_1_indexed > total_rounds - self.TERMINAL_DEFECTION_ROUNDS:
            return Action.D
        if self._strategy_state == 'INITIAL_COOP':
            if current_round_1_indexed <= self.INITIAL_COOP_ROUNDS:
                return Action.C
            else:
                self._strategy_state = 'EXPLOITING'
                return Action.D
        elif self._strategy_state == 'EXPLOITING':
            avg_coop_rate_others = self._calculate_avg_cooperation_rate_others()
            if avg_coop_rate_others < self.LOW_COOPERATION_THRESHOLD:
                self._strategy_state = 'RE_PROBING'
                self._re_probe_rounds_left = self.RE_PROBE_DURATION
                return Action.C
            else:
                return Action.D
        elif self._strategy_state == 'RE_PROBING':
            if self._re_probe_rounds_left > 0:
                self._re_probe_rounds_left -= 1
                return Action.C
            else:
                self._strategy_state = 'EXPLOITING'
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_271(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game.

    This strategy employs a three-phase approach: an initial probe phase to test for cooperators,
    an adaptive exploitation phase that reacts to observed cooperation, and a final endgame phase.
    The core principle is to induce cooperation where possible, then immediately exploit it,
    or defect to cut losses if cooperation is insufficient.

    Phase 1: Endgame (Last Round)
        Always defect, as there are no future consequences or opportunities for retaliation.
        This maximizes immediate private payoff by capturing the full value of 1 from keeping
        the endowment, plus any public good generated by others in the final round.

    Phase 2: Initial Probe (Early Rounds)
        Cooperate for a few initial rounds (defined by `probe_rounds`). This serves as a
        calculated, minimal investment to signal a willingness to contribute and to
        "kickstart" collective cooperation among other players. The goal is to create
        an environment that can later be exploited. This phase does not apply to the
        very last round, even if `probe_rounds` would extend that far.

    Phase 3: Adaptive Exploitation (Mid-Game)
        After the probe phase and before the endgame, the strategy adapts based on observed
        cooperation from others.
        - If a sufficient number of *other players* cooperated in the previous round
          (defined by `others_cooperation_threshold`), the strategy defects. This is
          the prime opportunity to exploit their contributions, maximizing individual
          private payoff while still benefiting from their collective investment.
        - If insufficient other players cooperated, the strategy also defects. This minimizes
          losses and prevents being a "sucker" by contributing when others are not. It also
          sends a signal that low cooperation will not be rewarded by this player's continued
          contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.probe_rounds = 2
        self.others_cooperation_threshold = self.game_description.n_players // 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        if current_round_0_indexed < self.probe_rounds:
            return Action.C
        previous_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
        c_others_prev = np.sum(previous_round_opponent_actions)
        if c_others_prev >= self.others_cooperation_threshold:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_272(BaseStrategy):
    """
    The "Adaptive Exploiter" strategy aims to maximize individual payoff in the N-Player Public Goods Game by
    strategically oscillating between cooperation and defection. It fosters cooperation, free-rides when
    cooperation is high, defends against exploitation when cooperation is low, and performs a terminal betrayal.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == r - 1:
            return Action.D
        my_action_prev_round = history.my_actions[-1]
        others_cooperation_count = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = others_cooperation_count + (1 if my_action_prev_round else 0)
        others_cooperation_rate = others_cooperation_count / (n - 1)
        if others_cooperation_rate >= 0.75:
            return Action.D
        if total_cooperators_prev_round < n / k:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_273(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy for the N-Player Public Goods Game.

    This strategy defaults to defection to maximize immediate personal gain.
    It strategically cooperates only on rare occasions when the overall level of
    cooperation from other players becomes critically low and has significantly
    declined recently. This temporary cooperation is a calculated investment
    to prevent a total collapse of public good contributions, thereby
    preserving an exploitable environment for future rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.total_rounds_r = game_description.n_rounds
        self.k_factor = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == self.total_rounds_r:
            return Action.D
        if current_round_t == 1:
            return Action.D
        previous_round_idx = state.round_number - 1
        cooperators_last_round_excluding_me = sum(history.opponent_actions[previous_round_idx, :])
        lookback_window = min(current_round_t - 1, 5)
        avg_cooperators_others_recent = 0.0
        if lookback_window > 0:
            sum_cooperators_others = 0
            for i in range(previous_round_idx - lookback_window + 1, previous_round_idx + 1):
                sum_cooperators_others += sum(history.opponent_actions[i, :])
            avg_cooperators_others_recent = sum_cooperators_others / lookback_window
        CRITICAL_LOW_THRESHOLD = 1
        drop_magnitude = max(1, math.floor(self.n_players / (2 * self.k_factor)))
        SIGNIFICANT_DROP_THRESHOLD = avg_cooperators_others_recent - drop_magnitude
        should_cooperate_conditionally = False
        if cooperators_last_round_excluding_me <= CRITICAL_LOW_THRESHOLD:
            if cooperators_last_round_excluding_me < SIGNIFICANT_DROP_THRESHOLD:
                should_cooperate_conditionally = True
        if should_cooperate_conditionally:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_274(BaseStrategy):
    """
    The Opportunistic Instigator strategy aims to exploit other players by
    strategically instigating cooperation, free-riding when possible,
    and ruthlessly defecting in the final round. It balances initiating
    cooperation to create a public good pool with the individual incentive
    to defect.

    Core Philosophy:
    1. Instigate cooperation early to create an environment for free-riding.
    2. Opportunistically free-ride when others are cooperating sufficiently.
    3. Cautiously re-seed cooperation if it falters, but only after demonstrating
       a willingness to defect.
    4. Ruthlessly defect in the final round when there are no future consequences.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_cooperation_rounds = 2
        self.defection_threshold_others = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round_1_indexed <= self.initial_cooperation_rounds:
            return Action.C
        if current_round_1_indexed == n_rounds:
            return Action.D
        my_action_last_round_is_cooperate = history.my_actions[-1]
        cooperators_from_others_last_round = np.sum(history.opponent_actions[-1, :])
        num_other_players = n_players - 1
        proportion_coop_others = cooperators_from_others_last_round / num_other_players
        if proportion_coop_others < self.defection_threshold_others:
            if not my_action_last_round_is_cooperate:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_275(BaseStrategy):
    """
    The Persistent Exploiter strategy aims to free-ride on others' cooperation.
    It starts by cooperating to probe for cooperation, then defects if others cooperate enough.
    If cooperation collapses, it tries once to re-induce it before defaulting to defection.
    Always defects in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.attempts_to_reinduce: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        action_for_this_round: Action
        if current_round_1_indexed == r:
            action_for_this_round = Action.D
        elif current_round_1_indexed == 1:
            action_for_this_round = Action.C
            self.attempts_to_reinduce = 0
        else:
            my_previous_action: Action = self.my_last_action
            others_coop_count = np.sum(history.opponent_actions[state.round_number - 1, :])
            others_count = n - 1
            others_coop_rate = others_coop_count / others_count
            exploitation_threshold = 0.5
            if others_coop_rate >= exploitation_threshold:
                action_for_this_round = Action.D
                self.attempts_to_reinduce = 0
            elif my_previous_action == Action.C:
                action_for_this_round = Action.D
            elif self.attempts_to_reinduce == 0:
                action_for_this_round = Action.C
                self.attempts_to_reinduce = 1
            else:
                action_for_this_round = Action.D
        self.my_last_action = action_for_this_round
        return action_for_this_round

class Strategy_EXPLOITATIVE_276(BaseStrategy):
    """
    The Strategic Free-Rider (SFR) strategy is designed to be highly exploitative in the N-Player Public Goods Game
    by strategically leveraging the cooperation of other players, free-riding when possible, and defensively defecting
    when cooperation is insufficient or breaks down. It balances immediate gains from defection with the need to
    sustain a cooperative environment for future exploitation opportunities. The strategy is adaptive and robust,
    relying solely on game parameters and historical observations of play.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.COOP_THRESHOLD = math.floor(self.game_description.n_players / self.game_description.k) + 1
        self.EXPLOIT_CYCLES = 2
        self.MAX_EXPLOIT_ROUNDS = 1
        self.coop_stable_rounds = 0
        self.rounds_since_last_exploit = 0
        self.our_action_prev = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            self.our_action_prev = Action.C
            return Action.C
        if current_round == self.game_description.n_rounds - 1:
            self.our_action_prev = Action.D
            return Action.D
        prev_total_cooperators_excluding_self = np.sum(history.opponent_actions[-1, :])
        observed_total_cooperators = prev_total_cooperators_excluding_self + (1 if self.our_action_prev == Action.C else 0)
        if observed_total_cooperators >= self.COOP_THRESHOLD:
            self.coop_stable_rounds += 1
        else:
            self.coop_stable_rounds = 0
            self.rounds_since_last_exploit = 0
        current_action = None
        if self.coop_stable_rounds == 0:
            current_action = Action.D
            self.rounds_since_last_exploit = 0
        elif self.rounds_since_last_exploit < self.MAX_EXPLOIT_ROUNDS and self.our_action_prev == Action.C and (self.coop_stable_rounds >= self.EXPLOIT_CYCLES):
            current_action = Action.D
            self.rounds_since_last_exploit += 1
            self.coop_stable_rounds = 0
        else:
            current_action = Action.C
            self.rounds_since_last_exploit = 0
        self.our_action_prev = current_action
        return current_action

class Strategy_EXPLOITATIVE_277(BaseStrategy):
    """
    Strategy: The Calculated Free-Rider

    This strategy aims to maximize its own payoff by free-riding on others' contributions
    when sufficient cooperation exists, and strategically investing in cooperation
    (by temporarily cooperating itself) when the overall cooperation level from others
    drops too low. It starts with a brief probe phase of cooperation and defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.PROBE_ROUNDS = 1
        self.AVG_WINDOW_SIZE = 3
        self.LOWER_COOPERATION_THRESHOLD = 0.4
        self.RESET_COOPERATION_PERIOD = 2
        self.punish_rounds_remaining = 0
        self.recent_others_cooperation_rates = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if history is not None:
            C_others_t_minus_1 = sum(history.opponent_actions[-1, :])
            cooperation_rate_others_t_minus_1 = C_others_t_minus_1 / (n_players - 1)
            self.recent_others_cooperation_rates.append(cooperation_rate_others_t_minus_1)
            if len(self.recent_others_cooperation_rates) > self.AVG_WINDOW_SIZE:
                self.recent_others_cooperation_rates.pop(0)
        if current_round_1_indexed == total_rounds:
            return Action.D
        if current_round_1_indexed <= self.PROBE_ROUNDS:
            return Action.C
        if not self.recent_others_cooperation_rates:
            average_others_cooperation_rate = 0.0
        else:
            average_others_cooperation_rate = np.mean(self.recent_others_cooperation_rates)
        if self.punish_rounds_remaining > 0:
            self.punish_rounds_remaining -= 1
            return Action.C
        elif average_others_cooperation_rate < self.LOWER_COOPERATION_THRESHOLD:
            self.punish_rounds_remaining = self.RESET_COOPERATION_PERIOD - 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_278(BaseStrategy):
    """
    The "Adaptive Exploiter" strategy for the N-Player Public Goods Game.
    This strategy prioritizes free-riding (defecting) to maximize immediate payoff when
    sufficient cooperation exists among opponents. If cooperation levels drop too low,
    it strategically "baits" with a single cooperation to try and re-stimulate contributions
    from others for future exploitation, provided there's enough time left in the game
    and a cooldown period since the last bait has passed.
    The strategy always plays selfishly (defects) in the final round as there are no
    future interactions to influence. It also starts by defecting to observe opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.last_probe_round = -1
        self.has_ever_cooperated = False
        self.LOW_COOPERATION_THRESHOLD = 0.3
        self.PROBE_ROUNDS_LEFT_THRESHOLD_FACTOR = 0.3
        self.PROBE_COOLDOWN_ROUNDS_FACTOR = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[-1, :]
        N_C_others_prev = np.sum(previous_round_opponent_actions)
        num_other_players = n_players - 1
        cooperation_rate_others = N_C_others_prev / num_other_players if num_other_players > 0 else 0.0
        rounds_remaining = total_rounds - (current_round_0_indexed + 1)
        if cooperation_rate_others >= self.LOW_COOPERATION_THRESHOLD:
            return Action.D
        else:
            enough_rounds_left = rounds_remaining >= total_rounds * self.PROBE_ROUNDS_LEFT_THRESHOLD_FACTOR
            cooldown_passed = current_round_0_indexed - self.last_probe_round >= total_rounds * self.PROBE_COOLDOWN_ROUNDS_FACTOR
            can_bait_now = enough_rounds_left and cooldown_passed
            if can_bait_now:
                self.last_probe_round = current_round_0_indexed
                self.has_ever_cooperated = True
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_279(BaseStrategy):
    """
    Adaptive Opportunist (AO) strategy for the N-Player Public Goods Game.

    This strategy combines initial probing with dynamic adaptation to opponents' behavior,
    always with an eye toward maximizing free-riding opportunities, and reverting to
    pure self-interest in the game's final stage. It's designed to exploit varied
    behaviors in a tournament setting.

    Strategy Parameters:
    - END_GAME_ROUNDS: Only the very last round is treated as an end-game scenario.
    - INITIAL_PROBE_ROUNDS: The number of initial rounds (specifically, just the first round)
      where the strategy unconditionally cooperates.
    - COOPERATION_THRESHOLD: The proportion of *other* players who cooperated in the
      previous round. If this threshold is met or exceeded, the strategy defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.END_GAME_ROUNDS = 1
        self.INITIAL_PROBE_ROUNDS = 1
        self.COOPERATION_THRESHOLD = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_1_indexed == total_rounds:
            return Action.D
        if current_round_1_indexed == self.INITIAL_PROBE_ROUNDS:
            return Action.C
        prev_round_opponent_actions = history.opponent_actions[state.round_number - 1, :]
        num_other_cooperators = sum(prev_round_opponent_actions)
        n_others = n_players - 1
        prop_coop_others_prev = num_other_cooperators / n_others
        my_last_action = history.my_actions[state.round_number - 1]
        if prop_coop_others_prev >= self.COOPERATION_THRESHOLD:
            return Action.D
        elif my_last_action == Action.C:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_281(BaseStrategy):
    """
    The Adaptive Exploiter with Defection Lock strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff primarily through free-riding. It
    strategically cooperates only when the collective public good is too low to exploit,
    acting as a temporary "shepherd" to encourage others to contribute. Crucially, it
    detects environments of consistent defection and locks into permanent defection to
    avoid being exploited and secure a baseline payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_MEC: int = math.ceil((self.game_description.n_players - 1) / 2)
        self.MAX_COOP_ATTEMPTS: int = 2
        self.DEFECTION_LOCK_THRESHOLD: int = 3
        self.my_last_action: Action = Action.D
        self.consecutive_zero_cooperation_rounds: int = 0
        self.consecutive_coop_attempts_without_reciprocity: int = 0
        self.permanent_defection_mode: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        chosen_action: Action = Action.D
        if state.round_number == 0:
            chosen_action = Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            chosen_action = Action.D
        else:
            C_total_prev = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
            if C_total_prev == 0:
                self.consecutive_zero_cooperation_rounds += 1
            else:
                self.consecutive_zero_cooperation_rounds = 0
            if self.permanent_defection_mode or self.consecutive_zero_cooperation_rounds >= self.DEFECTION_LOCK_THRESHOLD:
                self.permanent_defection_mode = True
                chosen_action = Action.D
            else:
                if history.my_actions[-1] == True:
                    C_others_prev = C_total_prev - 1
                else:
                    C_others_prev = C_total_prev
                if C_others_prev >= self.T_MEC:
                    chosen_action = Action.D
                    self.consecutive_coop_attempts_without_reciprocity = 0
                elif self.consecutive_coop_attempts_without_reciprocity < self.MAX_COOP_ATTEMPTS:
                    chosen_action = Action.C
                    if C_others_prev == 0:
                        self.consecutive_coop_attempts_without_reciprocity += 1
                    else:
                        self.consecutive_coop_attempts_without_reciprocity = 0
                else:
                    chosen_action = Action.D
                    self.consecutive_coop_attempts_without_reciprocity = 0
        self.my_last_action = chosen_action
        return chosen_action

class Strategy_EXPLOITATIVE_283(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize personal payoff in the
    N-Player Public Goods Game by primarily defecting, but intelligently interspersing
    cooperation when it serves to cultivate or maintain a profitable level of public
    good contribution from other players. It's designed to be "nice" enough to
    encourage cooperation, "greedy" enough to exploit, and "smart" enough to cut losses.

    Decision Rules:
    1. First Round (t=1): Cooperate (C) to signal willingness and gather data.
    2. Last Two Rounds (t=r-1 or t=r): Defect (D) for endgame exploitation.
    3. Middle Rounds (1 < t < r-1): Adaptive Opportunism based on previous round's cooperation.
       - If strong cooperation (>= EXPLOIT_THRESHOLD): Defect (D) to free-ride.
       - If very low cooperation (< DEFECTION_THRESHOLD): Defect (D) to cut losses/signal.
       - If moderate cooperation (between thresholds): Cooperate (C) to invest and maintain public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.num_others = self.n_players - 1
        defection_threshold_raw = math.floor(self.num_others * 0.3)
        exploit_threshold_raw = math.floor(self.num_others * 0.6)
        self.defection_threshold = max(0, min(defection_threshold_raw, self.num_others))
        self.exploit_threshold = max(0, min(exploit_threshold_raw, self.num_others))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.n_rounds - 1 or current_round_number == self.n_rounds - 2:
            return Action.D
        num_other_cooperators_in_prev_round = sum(history.opponent_actions[-1, :])
        if num_other_cooperators_in_prev_round >= self.exploit_threshold:
            return Action.D
        elif num_other_cooperators_in_prev_round < self.defection_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_284(BaseStrategy):
    """
    The Adaptive Predator strategy is designed for the N-Player Public Goods Game to maximize
    its own payoff by strategically exploiting cooperative players. It operates on the principle
    of identifying and free-riding on contributions while preventing total collapse of cooperation
    that would diminish its own long-term gains.

    The strategy implements the following decision rules:
    1.  **Initial Probe (Round 1 / 0-indexed round 0):** Unconditionally cooperates to signal
        openness and encourage other players to start cooperating, building a pool for future exploitation.
    2.  **Adaptive Exploitation (Rounds 2 to r-1 / 0-indexed rounds 1 to r-2):** Observes the
        previous round's cooperation level among other players. If the proportion of other cooperators
        meets or exceeds a defined `exploit_ratio_threshold`, the strategy defects (free-rides).
        Otherwise, it cooperates to encourage a rebound in cooperation, acting as a strategic investment.
    3.  **Final Round (Round r / 0-indexed round r-1):** Always defects due to backward induction,
        as there are no subsequent rounds to influence.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_coop_rounds = 1
        self.exploit_ratio_threshold = 0.25

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        if current_round_0_indexed < self.initial_coop_rounds:
            return Action.C
        prev_round_idx = current_round_0_indexed - 1
        num_other_cooperators = sum(history.opponent_actions[prev_round_idx, :])
        num_other_players = n_players - 1
        target_exploit_count = max(1, math.floor(self.exploit_ratio_threshold * num_other_players))
        if num_other_cooperators >= target_exploit_count:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_285(BaseStrategy):
    """
    The Opportunistic Strategist aims to maximize its own payoff by primarily
    free-riding on others' cooperation, but strategically cooperates to
    initiate a public good or prevent its total collapse, all within a
    limited budget of "reconciliation" attempts. It's adaptive, responding
    to the observed level of cooperation, and robust, as it doesn't assume
    specific opponent behaviors but instead reacts to the overall game state.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.history_C_total: list[int] = []
        self.reconciliation_phase: bool = False
        self.reconciliation_rounds_remaining: int = 0
        self.reconciliation_budget: int = 0
        self.persistent_defection_mode: bool = False
        self.threshold_exploit_cooperators: int = 0
        self.threshold_collapse_cooperators: int = 0
        self.collapse_streak_rounds: int = 2
        self.reconciliation_duration: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.reconciliation_budget = max(1, math.floor(self.r / 5))
            self.threshold_exploit_cooperators = max(1, math.ceil(self.n * 0.5))
            self.threshold_collapse_cooperators = max(1, math.floor(self.n * 0.2))
            return Action.C
        prev_round_my_action_C = history.my_actions[-1]
        prev_round_opponent_C_total = np.sum(history.opponent_actions[-1, :])
        C_total_prev = int(prev_round_my_action_C) + prev_round_opponent_C_total
        self.history_C_total.append(C_total_prev)
        if state.round_number == self.r - 1:
            return Action.D
        if self.reconciliation_phase:
            if self.reconciliation_rounds_remaining > 0:
                self.reconciliation_rounds_remaining -= 1
                return Action.C
            else:
                self.reconciliation_phase = False
        if self.persistent_defection_mode:
            return Action.D
        low_coop_streak_count = 0
        start_index = max(0, len(self.history_C_total) - self.collapse_streak_rounds)
        for i in range(start_index, len(self.history_C_total)):
            if self.history_C_total[i] < self.threshold_collapse_cooperators:
                low_coop_streak_count += 1
            else:
                low_coop_streak_count = 0
        if low_coop_streak_count >= self.collapse_streak_rounds:
            if self.reconciliation_budget > 0:
                self.reconciliation_phase = True
                self.reconciliation_rounds_remaining = self.reconciliation_duration - 1
                self.reconciliation_budget -= 1
                return Action.C
            else:
                self.persistent_defection_mode = True
                return Action.D
        if C_total_prev >= self.threshold_exploit_cooperators:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_286(BaseStrategy):
    """
    The Adaptive Exploiter strategy for the N-Player Public Goods Game.

    This strategy is designed to be highly self-interested and exploitative.
    It aggressively defects to free-ride when cooperation is high, defects to avoid
    being a "sucker" when cooperation is low, and strategically "probes" with
    occasional cooperation only when it believes it can encourage future exploitable
    behavior from others. It adapts its play based on the observed level of
    cooperation in the previous round and its own prior action.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == r - 1:
            return Action.D
        my_action_prev = history.my_actions[-1]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = opponent_cooperators_prev_round + (1 if my_action_prev == Action.C else 0)
        T_HIGH_COOPERATION = n - 1
        T_LOW_COOPERATION = 1
        if total_cooperators_prev_round >= T_HIGH_COOPERATION:
            return Action.D
        elif total_cooperators_prev_round <= T_LOW_COOPERATION:
            return Action.D
        elif my_action_prev == Action.D:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_287(BaseStrategy):
    """
    The Opportunistic Reciprocator strategy is designed to be highly adaptive and exploitative
    in the N-Player Public Goods Game. It aims to maximize its own payoff by leveraging
    other players' willingness to cooperate, while simultaneously protecting itself from
    being a consistent "sucker." It operates in three distinct phases: an initial probing phase
    to signal cooperation and gather data, an adaptive exploitation phase where it defects
    when cooperation from others is high or cooperates to re-seed a failing public good,
    and a final end-game exploitation phase based on backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = game_description.n_rounds
        self.initial_probe_rounds = max(1, min(3, math.floor(r / 2)))
        self.final_defection_rounds = max(1, min(3, math.floor(r / 2)))
        self.threshold_for_exploitation = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if current_round_1_indexed <= self.initial_probe_rounds:
            return Action.C
        if current_round_1_indexed > r - self.final_defection_rounds:
            return Action.D
        else:
            relevant_opponent_actions_slice = history.opponent_actions[self.initial_probe_rounds:state.round_number, :]
            average_cooperation_rate_others = 0.0
            if relevant_opponent_actions_slice.size > 0:
                total_C_by_others = np.sum(relevant_opponent_actions_slice)
                total_actions_by_others = relevant_opponent_actions_slice.size
                if total_actions_by_others > 0:
                    average_cooperation_rate_others = total_C_by_others / total_actions_by_others
            if average_cooperation_rate_others >= self.threshold_for_exploitation:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_288(BaseStrategy):
    """
    The Opportunistic Exploiter strategy aims to maximize its own payoff by selectively
    cooperating to induce others' cooperation, and then free-riding on that cooperation.
    It accounts for the finite nature of the game, defecting in the last round.

    Core Idea:
    - Calculates a Cooperation Threshold (T_c = ceil(n_players / k)).
      This threshold represents the minimum number of cooperators needed for a
      cooperator's public good share to be at least 1.
    - Cooperates in the first round to signal willingness to build a cooperative environment.
    - In intermediate rounds, it defects if the number of cooperators in the previous
      round met or exceeded T_c (exploiting others' cooperation).
    - If cooperation was below T_c in the previous round, it cooperates to encourage
      more contributions and potentially reach an exploitable state.
    - Always defects in the very last round, as there's no future to influence.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_c = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_number == total_rounds - 1:
            return Action.D
        if current_round_number == 0:
            return Action.C
        C_previous_round = np.sum(history.opponent_actions[-1, :])
        if C_previous_round >= self.T_c:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_289(BaseStrategy):
    """
    Opportunistic Exploiter strategy for N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by strategically leveraging
    and exploiting the cooperative tendencies of other players, while robustly
    protecting against defection. It probes for cooperation, exploits it, and
    only re-engages cooperation if a substantial portion of others cooperate.
    It always defects in endgame rounds to free-ride.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.initial_cooperate_rounds = 1
        self.re_cooperate_threshold_ratio = 0.6
        self.exploit_rounds_count = max(1, math.floor(self.r / 5))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed <= self.initial_cooperate_rounds:
            return Action.C
        if current_round_1_indexed > self.r - self.exploit_rounds_count:
            return Action.D
        my_last_action_was_cooperate = history.my_actions[-1]
        c_others_prev = np.sum(history.opponent_actions[-1, :])
        re_cooperate_threshold = math.floor((self.n - 1) * self.re_cooperate_threshold_ratio)
        if my_last_action_was_cooperate:
            return Action.D
        elif c_others_prev >= re_cooperate_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_290(BaseStrategy):
    """
    The Opportunistic Exploiter strategy for the N-Player Public Goods Game.
    This strategy is designed to be adaptive, robust, and explicitly exploitative. It employs a
    phased approach, balancing initial investment, periods of aggressive free-riding, and
    strategic re-engagement to maximize individual gain over the game's duration.

    Core Philosophy: Create an environment where others cooperate, then free-ride on their
    contributions. It initiates cooperation to "prime the pump," ruthlessly defects when
    others are contributing, and strategically re-engages with cooperation only when
    necessary to re-establish a public good for future exploitation. It abandons all
    pretense of cooperation in the final stages of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_cooperation_rounds = 1
        self.free_ride_threshold = 0.7
        self.rebuild_cooperation_threshold = 0.3
        self.end_game_horizon = 1
        self.my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        my_current_action: Action
        if current_round < self.initial_cooperation_rounds:
            my_current_action = Action.C
        elif current_round >= n_rounds - self.end_game_horizon:
            my_current_action = Action.D
        else:
            num_cooperators_others_prev = np.sum(history.opponent_actions[-1, :])
            n_others = n_players - 1
            cooperation_rate_others_prev = 0.0
            if n_others > 0:
                cooperation_rate_others_prev = num_cooperators_others_prev / n_others
            if cooperation_rate_others_prev >= self.free_ride_threshold:
                my_current_action = Action.D
            elif cooperation_rate_others_prev < self.rebuild_cooperation_threshold:
                if self.my_last_action == Action.C:
                    my_current_action = Action.D
                else:
                    my_current_action = Action.C
            else:
                my_current_action = Action.C
        self.my_last_action = my_current_action
        return my_current_action

class Strategy_EXPLOITATIVE_291(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy for the N-Player Public Goods Game.
    This strategy aims to maximize individual payoff by strategically alternating
    between cooperation (to encourage others or prevent collapse) and defection
    (to free-ride on their contributions). It balances initial investment,
    adaptive exploitation, and endgame ruthlessness.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        k = game_description.k
        self.K_EFFICIENCY_RATIO = k / n
        self.ENDGAME_ROUNDS = max(2, min(r - 1, math.floor(math.sqrt(r))))
        self.INITIAL_COOPERATE_ROUNDS = max(1, min(r - self.ENDGAME_ROUNDS - 1, math.floor(math.sqrt(r) / 2)))
        self.LOW_COOPERATION_THRESHOLD_FOR_RESEED = max(0, math.floor((n - 1) / 3))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed <= self.INITIAL_COOPERATE_ROUNDS:
            return Action.C
        if current_round_1_indexed > total_rounds - self.ENDGAME_ROUNDS:
            return Action.D
        c_others_prev_round = sum(history.opponent_actions[-1, :])
        if c_others_prev_round <= self.LOW_COOPERATION_THRESHOLD_FOR_RESEED:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_292(BaseStrategy):
    """
    The Resilient Parasite (RP) strategy is designed to be highly exploitative in the N-Player Public Goods Game.
    Its core philosophy is to aggressively free-ride on the contributions of others, only contributing itself
    when absolutely necessary to prevent a complete collapse of the public good, thereby preserving future
    opportunities for exploitation. It is adaptive, robust, and prioritizes individual payoff maximization.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.num_other_players = self.n_players - 1
        self.COOPERATION_TRIGGER_PROPORTION = 0.33
        self.COOPERATION_TRIGGER_COUNT = max(1, math.floor(self.num_other_players * self.COOPERATION_TRIGGER_PROPORTION))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_one_indexed = state.round_number + 1
        if current_round_one_indexed == 1:
            return Action.D
        if current_round_one_indexed == 2:
            return Action.D
        if current_round_one_indexed == self.n_rounds:
            return Action.D
        num_cooperators_last_round = sum(history.opponent_actions[-1, :])
        if num_cooperators_last_round == 0:
            return Action.D
        elif num_cooperators_last_round < self.COOPERATION_TRIGGER_COUNT:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_293(BaseStrategy):
    """
    The Opportunist Free-Rider strategy for the N-Player Public Goods Game.

    This strategy is designed to be highly exploitative, primarily defaulting
    to defection (the dominant strategy). It introduces a single, early-game
    cooperative "test" in Round 2 (if the game length permits sufficient
    subsequent rounds for exploitation) to probe for potential group
    cooperation.

    - If cooperation is observed in Round 1, or successfully induced by the
      Round 2 test, the strategy immediately switches to exploiting it by
      consistently defecting in subsequent rounds.
    - If no cooperation is observed, or if the test fails to induce it,
      the strategy defaults to continuous defection to avoid being a "sucker."
    - It explicitly accounts for the finite nature of the game by defecting
      in the final two rounds, based on backward induction.

    The strategy aims to maximize its own payoff, even at the expense of
    overall group welfare, by free-riding on others' contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.total_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.MIN_COOPERATORS_TO_EXPLOIT = 1
        self.ROUND_FOR_TEST = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        if current_round == 1:
            return Action.D
        num_other_cooperators_last_round = sum(history.opponent_actions[state.round_number - 1, :])
        if current_round >= self.total_rounds - 1:
            return Action.D
        if current_round == self.ROUND_FOR_TEST and self.total_rounds >= current_round + 2:
            if num_other_cooperators_last_round >= self.MIN_COOPERATORS_TO_EXPLOIT:
                return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_295(BaseStrategy):
    """
    The Opportunistic Trigger strategy divides the game into three phases:
    an initial probing phase, an adaptive exploitation phase, and an end-game defection phase.
    It strategically cooperates to foster or maintain a level of public good that is
    profitable to free-ride on, but quickly switches to defection if cooperation is
    too strong (to exploit) or too weak (to avoid being a "sucker").
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.initial_probe_rounds = min(3, self.r - 1)
        self.exploit_threshold_proportion = 0.5
        self.recovery_threshold_proportion = 0.2
        self.consecutive_rounds_below_recovery_threshold = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            self.consecutive_rounds_below_recovery_threshold = 0
            return Action.C
        elif current_round_1_indexed <= self.initial_probe_rounds:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        my_prev_action_was_cooperation = bool(history.my_actions[-1])
        num_opponent_cooperators_prev = int(np.sum(history.opponent_actions[-1, :]))
        N_c_prev = (1 if my_prev_action_was_cooperation else 0) + num_opponent_cooperators_prev
        exploit_threshold_count = self.n * self.exploit_threshold_proportion
        recovery_threshold_count = self.n * self.recovery_threshold_proportion
        if N_c_prev >= exploit_threshold_count:
            self.consecutive_rounds_below_recovery_threshold = 0
            return Action.D
        elif N_c_prev < recovery_threshold_count:
            self.consecutive_rounds_below_recovery_threshold += 1
            if self.consecutive_rounds_below_recovery_threshold <= 1:
                self.consecutive_rounds_below_recovery_threshold = 0
                return Action.C
            else:
                return Action.D
        else:
            self.consecutive_rounds_below_recovery_threshold = 0
            return Action.D

class Strategy_EXPLOITATIVE_296(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy aims to maximize its own payoff by strategically
    encouraging cooperation when it's low (to create opportunities for free-riding)
    and then defecting when cooperation is sufficiently robust to be exploited.
    It explicitly accounts for the end-game dynamics of the N-Player Public Goods Game.

    Decision Rules:
    1.  Endgame Exploitation: Play Defect (D) in the last two rounds (r-1 and r).
        This handles backward induction and applies to the full game if r=2 (D,D).
    2.  Round 0 (Initial Probe): For games longer than 2 rounds, play Cooperate (C).
        This encourages initial cooperation and gathers data.
    3.  Intermediate Rounds (Adaptive Exploitation):
        Based on `C_prev` (total cooperators in the previous round) and a dynamic
        Cooperation Sustainability Threshold (T = floor(n/k) + 1):
        - If `C_prev < T`: Play Cooperate (C) to rebuild cooperation.
        - If `C_prev >= T`: Play Defect (D) to free-ride on robust cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.T = math.floor(self.n_players / self.k_factor) + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.n_rounds - 1 or current_round_0_indexed == self.n_rounds - 2:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        last_round_opponent_cooperators = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        last_round_my_action_is_C = history.my_actions[current_round_0_indexed - 1]
        total_cooperators_in_previous_round = int(last_round_my_action_is_C) + last_round_opponent_cooperators
        if total_cooperators_in_previous_round < self.T:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_297(BaseStrategy):
    """
    The "Calculated Free-Rider" strategy for the N-Player Public Goods Game.
    This strategy aims to maximize individual payoff by initially cooperating
    to induce contributions, then carefully monitoring group cooperation to
    identify opportunities for defection (exploitation). It also employs
    backward induction for end-game rounds and self-protective defection
    when cooperation levels are too low.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.ECT = max(2, math.floor(self.n / 2))
        self.MCT = 1
        self.Exploit_After_X_Rounds = 2
        self.rounds_since_my_last_exploit = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_0_indexed == 0:
            self.rounds_since_my_last_exploit = 0
            return Action.C
        if current_round_1_indexed == self.r or current_round_1_indexed == self.r - 1:
            self.rounds_since_my_last_exploit = 0
            return Action.D
        previous_round_opponent_cooperators = sum(history.opponent_actions[-1, :])
        previous_round_my_action_val = 1 if history.my_actions[-1] else 0
        previous_round_total_cooperators = previous_round_opponent_cooperators + previous_round_my_action_val
        if previous_round_total_cooperators <= self.MCT:
            self.rounds_since_my_last_exploit = 0
            return Action.D
        elif previous_round_total_cooperators >= self.ECT and self.rounds_since_my_last_exploit >= self.Exploit_After_X_Rounds:
            self.rounds_since_my_last_exploit = 0
            return Action.D
        else:
            self.rounds_since_my_last_exploit += 1
            return Action.C

class Strategy_EXPLOITATIVE_298(BaseStrategy):
    """
    "The Strategic Lurer and Free-Rider" strategy for the N-Player Public Goods Game.

    This strategy is designed to be exploitative by investing in initial cooperation to "lure"
    other players into cooperating, then primarily free-riding on their contributions.
    It adaptively monitors the level of cooperation among other players and strategically
    re-lures if cooperation drops too low, ensuring there's a "supply" of public good to exploit.
    It always defects in the final round to maximize immediate payoff without future repercussions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n_rounds = self.game_description.n_rounds
        if n_rounds > 1:
            self.LURE_ROUNDS = min(3, n_rounds - 1)
        else:
            self.LURE_ROUNDS = 0
        self.FREE_RIDE_THRESHOLD = 0.5
        self.RE_LURE_THRESHOLD = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        if current_round_0_indexed < self.LURE_ROUNDS:
            return Action.C
        num_others_cooperated_last_round = np.sum(history.opponent_actions[-1, :])
        num_opponents = n_players - 1
        percentage_others_cooperated_last_round = 0.0
        if num_opponents > 0:
            percentage_others_cooperated_last_round = num_others_cooperated_last_round / num_opponents
        if percentage_others_cooperated_last_round >= self.FREE_RIDE_THRESHOLD:
            return Action.D
        if percentage_others_cooperated_last_round < self.RE_LURE_THRESHOLD and n_rounds - 1 - current_round_0_indexed > self.LURE_ROUNDS:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_299(BaseStrategy):
    """
    The "Adaptive Predator" strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own total payoff by primarily free-riding,
    while strategically cooperating with minimal "investment" only when the
    public good is critically endangered. Its goal is to "re-ignite" cooperation
    among others for future exploitation, ensuring the public good doesn't
    collapse to an unprofitable state.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.CCL = max(1, math.floor((self.n - 1) / (self.k + 1)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.r - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.D
        prev_round_0_indexed = current_round_0_indexed - 1
        C_others_prev = np.sum(history.opponent_actions[prev_round_0_indexed, :])
        my_action_prev_bool = history.my_actions[prev_round_0_indexed]
        if C_others_prev > self.CCL:
            return Action.D
        elif my_action_prev_bool == True:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_300(BaseStrategy):
    """
    The Patient Opportunist strategy aims to maximize its total payoff by
    consistently exploiting any existing cooperation while intelligently
    probing for new opportunities to re-ignite cooperation among conditional
    players, thereby creating more exploitation windows. It carefully avoids
    "wasting" cooperation in the final rounds or when probes are unlikely
    to yield sufficient long-term gains.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.consecutive_all_D_rounds: int = 0
        self.reprobe_frequency: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        if current_round >= self.n_rounds - 1:
            return Action.D
        if current_round == 1:
            return Action.C
        num_other_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        my_action_prev = history.my_actions[-1]
        if num_other_cooperators_prev > 0:
            self.consecutive_all_D_rounds = 0
            return Action.D
        elif my_action_prev == True:
            self.consecutive_all_D_rounds = 0
            return Action.D
        else:
            self.consecutive_all_D_rounds += 1
            if self.consecutive_all_D_rounds >= self.reprobe_frequency and current_round + self.reprobe_frequency < self.n_rounds:
                self.consecutive_all_D_rounds = 0
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_301(BaseStrategy):
    """
    Opportunistic Pumper-and-Dumper (OPD) strategy for N-Player Public Goods Game.

    The OPD strategy primarily defects to free-ride on others' cooperation.
    However, if the observed level of cooperation in the previous round falls below
    a calculated threshold, it temporarily cooperates ('pumps') for a short period
    to encourage other players to re-establish cooperation. Once the pumping period
    is over, or if cooperation remains sufficient, it reverts to defecting ('dumping').
    It always defects in the last round and in the very first round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.is_pumping = False
        self.pump_duration_left = 0
        MIN_PUBLIC_GOOD_SHARE_TO_TRIGGER_PUMP = 0.5
        self.PUMP_TRIGGER_THRESHOLD = math.ceil(MIN_PUBLIC_GOOD_SHARE_TO_TRIGGER_PUMP * self.n / self.k)
        self.PUMP_TRIGGER_THRESHOLD = max(1, self.PUMP_TRIGGER_THRESHOLD)
        self.PUMP_PERIOD = max(1, min(3, self.r // 10))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.r:
            return Action.D
        num_cooperators_prev_round = 0
        if current_round_1_indexed > 1:
            prev_round_my_action = history.my_actions[current_round_0_indexed - 1]
            prev_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
            num_cooperators_prev_round = np.sum(prev_round_opponent_actions) + (1 if prev_round_my_action else 0)
        if self.is_pumping:
            if self.pump_duration_left > 0:
                self.pump_duration_left -= 1
                return Action.C
            else:
                self.is_pumping = False
        if current_round_1_indexed == 1 or num_cooperators_prev_round >= self.PUMP_TRIGGER_THRESHOLD:
            return Action.D
        else:
            self.is_pumping = True
            self.pump_duration_left = self.PUMP_PERIOD - 1
            return Action.C

class Strategy_EXPLOITATIVE_302(BaseStrategy):
    """
    The Opportunistic Pumper strategy aims to maintain a sufficient level of public good
    by occasionally "pumping" (cooperating) when it's too low to be worth exploiting,
    then immediately "dumping" (defecting) to free-ride when cooperation levels are high enough.

    It cooperates in the first round to "seed" the public good and encourages other
    conditional cooperators. It ruthlessly defects in the final round to maximize
    immediate payoff, as there are no future consequences. In intermediate rounds,
    it adapts its behavior based on the observed proportion of cooperators in the
    previous round relative to a calculated cooperation_threshold_ratio (k/n).
    If cooperation is high enough, it defects to free-ride; otherwise, it cooperates
    to "pump" the public good back up.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_ratio = self.game_description.k / self.game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == total_rounds:
            return Action.D
        else:
            cooperators_from_opponents_prev_round = np.sum(history.opponent_actions[-1, :])
            my_action_prev_round_was_cooperate = history.my_actions[-1]
            total_cooperators_prev_round = cooperators_from_opponents_prev_round + int(my_action_prev_round_was_cooperate)
            proportion_cooperators_prev = total_cooperators_prev_round / n_players
            if proportion_cooperators_prev >= self.cooperation_threshold_ratio:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_303(BaseStrategy):
    """
    The "Adaptive Free Rider" strategy for the N-Player Public Goods Game.

    This strategy is highly self-interested and opportunistic. It attempts to
    "prime the pump" of cooperation for a few initial rounds, then defaults
    to exploiting any existing cooperation by defecting. If cooperation from
    others wanes below a calculated threshold, it temporarily re-engages by
    cooperating to prevent total collapse, always with the goal of returning
    to exploitation. In the final round, it always defects to maximize
    immediate payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.total_rounds = game_description.n_rounds
        self.initial_cooperation_duration = 2
        self.min_other_cooperators_to_exploit = max(1, math.floor(math.sqrt(self.n_players)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t <= self.initial_cooperation_duration:
            return Action.C
        if current_round_t == self.total_rounds:
            return Action.D
        c_other_prev = sum(history.opponent_actions[-1, :])
        if c_other_prev >= self.min_other_cooperators_to_exploit:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_304(BaseStrategy):
    """
    Greedy Optimist with Prudence (GOP) Strategy for N-Player Public Goods Game.

    This strategy aims to exploit cooperation by defecting. It adaptively responds to
    a lack of cooperation by temporarily 'pump-priming' (cooperating) to try and
    re-ignite others' cooperation, but only if the potential benefits (k/n ratio)
    justify the risk. As soon as sufficient cooperation is detected, it reverts
    to free-riding. It always defects in the first and last rounds.
    """
    COOPERATION_DETECTION_THRESHOLD: int = 1
    NO_COOP_TOLERANCE_ROUNDS: int = 2
    PUMP_PRIME_DURATION: int = 1
    MIN_K_PER_N_FOR_PUMP_PRIME: float = 0.6

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_low_coop_rounds: int = 0
        self.pump_priming_active: bool = False
        self.pump_prime_rounds_left: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        n_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        k_factor = self.game_description.k
        if current_round_idx == n_rounds - 1:
            return Action.D
        if current_round_idx == 0:
            return Action.D
        c_others_prev = int(np.sum(history.opponent_actions[-1, :]))
        current_k_per_n = float(k_factor) / n_players
        if self.pump_priming_active:
            if c_others_prev >= self.COOPERATION_DETECTION_THRESHOLD:
                self.pump_priming_active = False
                self.consecutive_low_coop_rounds = 0
                self.pump_prime_rounds_left = 0
                return Action.D
            elif self.pump_prime_rounds_left > 0:
                self.pump_prime_rounds_left -= 1
                return Action.C
            else:
                self.pump_priming_active = False
                self.consecutive_low_coop_rounds = 0
                return Action.D
        elif c_others_prev >= self.COOPERATION_DETECTION_THRESHOLD:
            self.consecutive_low_coop_rounds = 0
            return Action.D
        else:
            self.consecutive_low_coop_rounds += 1
            if self.consecutive_low_coop_rounds >= self.NO_COOP_TOLERANCE_ROUNDS and current_k_per_n >= self.MIN_K_PER_N_FOR_PUMP_PRIME:
                self.pump_priming_active = True
                self.pump_prime_rounds_left = self.PUMP_PRIME_DURATION - 1
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_305(BaseStrategy):
    """
    Opportunistic Defector: This strategy aims to free-ride on others' cooperation by defecting when sufficient cooperation is present.
    It starts with a brief probe phase (C), then defaults to defection. If cooperation wanes, it attempts to re-induce it with a 're-bait' (C) for a limited time,
    otherwise falling back to permanent defection. It always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_probe_rounds = 1
        self.min_total_cooperation_to_exploit = 1
        self.cooperation_streak_tolerance = 2
        self.found_cooperators_after_initial_probe = False
        self.cooperation_streak_broken = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.game_description.n_rounds - 1:
            return Action.D
        if current_round < self.initial_probe_rounds:
            return Action.C
        prev_round_my_action = history.my_actions[current_round - 1]
        prev_round_opponent_actions = history.opponent_actions[current_round - 1, :]
        total_cooperators_prev = int(prev_round_my_action) + np.sum(prev_round_opponent_actions)
        other_cooperators_prev = np.sum(prev_round_opponent_actions)
        if other_cooperators_prev > 0:
            self.found_cooperators_after_initial_probe = True
        if total_cooperators_prev >= self.min_total_cooperation_to_exploit:
            self.cooperation_streak_broken = 0
            return Action.D
        else:
            self.cooperation_streak_broken += 1
            if not self.found_cooperators_after_initial_probe or self.cooperation_streak_broken > self.cooperation_streak_tolerance:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_306(BaseStrategy):
    """
    Adaptive Predator: An exploitative, adaptive, and robust strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by consistently leveraging the contributions of others.
    It defaults to the individually rational (defecting) action, but is adaptive in its approach to
    "seed" cooperation when it has completely broken down, solely for the purpose of future exploitation.

    Core Principles:
    1.  Dominant Strategy (Single Round): Defect (D) always yields a higher payoff than Cooperate (C)
        for any player, regardless of what others do. This forms the baseline action.
    2.  Exploitative Goal: The strategy's primary objective is to achieve the highest possible personal
        payoff, which occurs when this player defects and other players cooperate.
    3.  Adaptive Luring: When cooperation among players completely collapses (very few cooperators),
        the strategy makes a strategic, infrequent cooperative move. This acts as a "seed" to
        re-induce cooperation, creating future opportunities for exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.CET = math.ceil(self.game_description.k)
        self.LT = 1
        self.LI = max(3, math.floor(self.game_description.n_rounds / 5))
        self.last_lure_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        prev_round_opponent_cooperators = sum(history.opponent_actions[-1, :])
        prev_round_my_action_cooperated = history.my_actions[-1]
        C_prev = prev_round_opponent_cooperators + (1 if prev_round_my_action_cooperated else 0)
        if C_prev >= self.CET:
            return Action.D
        elif C_prev <= self.LT and current_round_0_indexed - self.last_lure_round >= self.LI and (current_round_0_indexed < total_rounds - 1):
            self.last_lure_round = current_round_0_indexed
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_307(BaseStrategy):
    """
    The Shifting Sands Grifter strategy aims to maximize personal payoff by primarily defecting.
    It strategically cooperates only to "bait" others into cooperation, thereby creating a pool
    of contributions that can then be exploited. It is quick to revert to defection if its
    bait fails or if others are already cooperating.

    Decision Rules:
    1. Last Round: Defect (D). In the final round, there are no future consequences for current actions.
       Defection is the immediate dominant strategy.
    2. First Round: Defect (D). The strategy starts aggressively by attempting to free-ride immediately.
       If other players are unconditionally cooperative, this yields a high initial payoff. If all players
       defect, the strategy's payoff is 1, no worse than if it had cooperated alone (which would yield 0).
    3. Intermediate Rounds (rounds 2 to r-1, using 1-indexed round numbers):
        The strategy analyzes the outcomes of the immediately preceding round (round_t - 1).

        - State Tracking from previous round (round_t - 1):
            - my_action_prev: The action taken by this strategy in the previous round.
            - others_cooperated_prev: The number of *other players* (excluding me) who cooperated in the previous round.

        - Decision Logic for current round (round_t):
            - Scenario A: Others are Cooperating (Exploit!):
                IF others_cooperated_prev >= 1: Action: Defect (D).
                Rationale: If at least one other player contributed in the previous round, there is an existing
                           public good to free-ride on. Defecting provides an immediate +1 private payoff.
                           This is the primary exploitative mode of the strategy.

            - Scenario B: No One Else is Cooperating (Bait or Revert):
                ELSE (meaning others_cooperated_prev == 0):
                    - IF my_action_prev == D: Action: Cooperate (C).
                      Rationale: If everyone (including me) defected last round, the collective payoff was 1 for all.
                                 This is a "bait" move. By cooperating now, the strategy signals a willingness to contribute,
                                 hoping to break a cycle of universal defection. This is a strategic investment: it costs 1
                                 payoff this round, but if it successfully encourages other conditional cooperators to
                                 contribute in the *next* round, the strategy can then exploit them.

                    - ELSE (meaning my_action_prev == C): Action: Defect (D).
                      Rationale: I tried to "bait" by cooperating in the previous round, but others_cooperated_prev
                                 is still 0. My bait failed, as no one else responded by cooperating. Continuing to
                                 cooperate would lead to unnecessary losses. The strategy reverts to its default of
                                 defection, minimizing further losses and signaling that unreciprocated cooperation
                                 is not sustainable.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.D
        prev_round_index = current_round_0_indexed - 1
        my_action_prev_bool = history.my_actions[prev_round_index]
        others_cooperated_prev = np.sum(history.opponent_actions[prev_round_index, :])
        if others_cooperated_prev >= 1:
            return Action.D
        elif my_action_prev_bool == False:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_308(BaseStrategy):
    """
    The Calibrated Free-Rider strategy for the N-Player Public Goods Game.
    It is adaptive, robust, and prioritizes maximizing individual payoff by
    strategically balancing cooperation and defection, with a strong bias
    towards exploitation where opportune.

    Decision Rules:
    1. Round 1: Cooperate to explore cooperative potential and mask intent.
    2. End-Game: Defect in the last X rounds (X = max(1, floor(n/2))) to exploit
       the finite horizon without fear of retaliation.
    3. Mid-Game:
       - If others' cooperation (min_others_coop_for_engagement = max(1, floor((n-1)/3)))
         is insufficient, Defect to avoid being a sucker and punish low cooperation.
       - If others' cooperation is sufficient:
         - If self cooperated last round, Defect to free-ride.
         - If self defected last round, Cooperate to stabilize cooperation and
           maintain future exploitation opportunities.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.end_game_defection_rounds = max(1, math.floor(self.n / 2))
        self.min_others_coop_for_engagement = max(1, math.floor((self.n - 1) / 3))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round >= self.r - self.end_game_defection_rounds:
            return Action.D
        c_others_prev = np.sum(history.opponent_actions[current_round - 1, :])
        my_last_action_was_c = history.my_actions[current_round - 1]
        if c_others_prev < self.min_others_coop_for_engagement:
            return Action.D
        elif my_last_action_was_c:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_309(BaseStrategy):
    """
    Adaptive Grifter: This strategy is designed to maximize its own payoff in the N-Player Public Goods Game.
    It attempts to foster cooperation among opponents, then exploit that cooperation by defecting, while also
    protecting itself from being a "sucker" when others defect. The strategy defaults to defection, only
    cooperating under specific circumstances to ensure maximum personal gain, and always defects in the final round.
    """
    INIT_C_ROUNDS: int = 2
    MIN_COOP_FOR_EXPLOIT_FACTOR: float = 0.5
    MIN_COOP_FOR_RETREAT_FACTOR: float = 0.25
    RELURE_INTERVAL: int = 5

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._my_last_action: Action | None = None
        self._rounds_since_last_c: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_1_indexed_round: int = state.round_number + 1
        n: int = self.game_description.n_players
        total_rounds: int = self.game_description.n_rounds
        min_coop_for_exploit: int = max(0, math.floor((n - 1) * self.MIN_COOP_FOR_EXPLOIT_FACTOR + 0.5))
        min_coop_for_retreat: int = max(0, math.floor((n - 1) * self.MIN_COOP_FOR_RETREAT_FACTOR + 0.5))
        chosen_action: Action
        if current_1_indexed_round == 1:
            chosen_action = Action.C
        elif current_1_indexed_round == total_rounds:
            chosen_action = Action.D
        else:
            my_action_in_prev_round_is_C: bool = history.my_actions[-1]
            s_others_t_minus_1: int = np.sum(history.opponent_actions[-1, :])
            if not my_action_in_prev_round_is_C:
                self._rounds_since_last_c += 1
            else:
                self._rounds_since_last_c = 0
            if current_1_indexed_round <= self.INIT_C_ROUNDS:
                chosen_action = Action.C
            elif s_others_t_minus_1 >= min_coop_for_exploit:
                chosen_action = Action.D
            elif s_others_t_minus_1 < min_coop_for_retreat:
                chosen_action = Action.D
            elif self._rounds_since_last_c >= self.RELURE_INTERVAL:
                chosen_action = Action.C
            else:
                chosen_action = Action.D
        self._my_last_action = chosen_action
        return chosen_action

class Strategy_EXPLOITATIVE_310(BaseStrategy):
    """
    The Wily Opportunist strategy aims to maximize its personal payoff by primarily free-riding on other players'
    contributions. It strategically uses occasional cooperation as a "lure" or "re-baiting" mechanism to sustain a
    cooperative environment, ensuring there's a public good to exploit. It adapts to the observed level of
    cooperation from others and responds defensively when cooperation is too low, preventing unnecessary losses.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        self.T_sufficient = max(1, math.floor((n - 1) / 2))
        self.max_defection_streak = min(r - 1, 3)
        self.my_last_action: Action | None = None
        self.defection_count_since_last_coop: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_rounds = self.game_description.n_rounds
        action_to_take: Action
        if current_round == 0:
            action_to_take = Action.C
        elif current_round == n_rounds - 1:
            action_to_take = Action.D
        else:
            C_others_prev = np.sum(history.opponent_actions[-1, :])
            if C_others_prev < self.T_sufficient:
                action_to_take = Action.D
            elif self.my_last_action == Action.C:
                action_to_take = Action.D
            elif self.defection_count_since_last_coop >= self.max_defection_streak:
                action_to_take = Action.C
            else:
                action_to_take = Action.D
        self.my_last_action = action_to_take
        if action_to_take == Action.C:
            self.defection_count_since_last_coop = 0
        else:
            self.defection_count_since_last_coop += 1
        return action_to_take

class Strategy_EXPLOITATIVE_311(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy aims to primarily free-ride on others' contributions.
    It strategically cooperates only to initiate a state of potential exploitation (Round 1)
    or to prevent the complete collapse of cooperation when it falls below a critical threshold
    in intermediate rounds, thereby preserving future opportunities for free-riding.
    In the last round, it defects following backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        self.cooperation_revival_threshold = max(1, math.floor(n / 4))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        n_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        c_others_prev = sum(history.opponent_actions[-1, :])
        if c_others_prev < self.cooperation_revival_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_312(BaseStrategy):
    """
    The "Calculated Free-Rider" (CFR) strategy is designed to be highly exploitative,
    maximizing individual payoff by free-riding on others' contributions while
    strategically maintaining a minimal level of group cooperation. It will default
    to defecting to free-ride on others' contributions. However, it recognizes that
    if cooperation collapses entirely, everyone's payoff will diminish. Therefore,
    the strategy will proactively "invest" in cooperation only when the collective
    contribution level falls below a critical threshold (`ceil(n/k)`),
    effectively "stoking the fire" of cooperation just enough to keep it burning for
    future exploitation, then immediately reverting to defection.
    It also defects unconditionally in the first and last rounds as per strategic rationale.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.COOPERATION_THRESHOLD = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        cooperators_previous_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if cooperators_previous_round < self.COOPERATION_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_313(BaseStrategy):
    """
    Opportunistic Reciprocator strategy for the N-Player Public Goods Game.

    This strategy aims to foster a cooperative environment initially,
    then strategically free-ride on others' contributions. It adapts its
    behavior based on the observed level of cooperation from others and
    aggressively defects in the end game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold: float = 1 - self.game_description.k / self.game_description.n_players
        self.defection_probability_on_cooperation: float = 0.15

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        num_players = self.game_description.n_players
        if current_round_t == 1:
            return Action.C
        if current_round_t == total_rounds:
            return Action.D
        other_players_prev_actions = history.opponent_actions[-1, :]
        num_other_cooperators = sum(other_players_prev_actions)
        num_other_players = num_players - 1
        cooperation_rate_t_minus_1 = num_other_cooperators / num_other_players
        if cooperation_rate_t_minus_1 >= self.cooperation_threshold:
            if random.random() < self.defection_probability_on_cooperation:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_314(BaseStrategy):
    """
    The "Adaptive Parasite" strategy aims to primarily free-ride on others' contributions.
    It strategically cooperates only when the observed level of cooperation from other players
    drops so low that its own payoff from defecting is significantly diminished, and it
    believes a temporary contribution might "re-seed" cooperation for future exploitation.
    It will not tolerate being a "sucker" if re-seeding fails.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_low_cooperation_rounds = 0
        n = self.game_description.n_players
        k = self.game_description.k
        self.reseed_cooperation_threshold = max(1, math.floor(n * (1 - k / n) * 0.5))
        self.patience_for_reseed = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            self.consecutive_low_cooperation_rounds = 0
            return Action.D
        if current_round == self.game_description.n_rounds - 1:
            self.consecutive_low_cooperation_rounds = 0
            return Action.D
        C_prev = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if C_prev >= self.reseed_cooperation_threshold:
            self.consecutive_low_cooperation_rounds = 0
            return Action.D
        else:
            self.consecutive_low_cooperation_rounds += 1
            if self.consecutive_low_cooperation_rounds > self.patience_for_reseed:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_315(BaseStrategy):
    """
    The Strategic Free-Rider strategy primarily defects to maximize immediate gain.
    It adapts by making calculated, temporary 'investments' in cooperation if other
    players' contributions falter, aiming to revive the public good for future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.COOP_TRIGGER_THRESHOLD = math.floor((self.n_players - 1) / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.n_rounds - 1:
            return Action.D
        m_other_cooperators = np.sum(history.opponent_actions[-1, :]).item()
        my_prev_action_was_cooperate = history.my_actions[-1]
        if m_other_cooperators >= self.COOP_TRIGGER_THRESHOLD:
            return Action.D
        elif not my_prev_action_was_cooperate:
            if m_other_cooperators > 0 and current_round < self.n_rounds - 2:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_316(BaseStrategy):
    """
    The Cynical Coaxer strategy. This strategy is designed to be highly exploitative,
    adaptive, and robust in the N-Player Public Goods Game tournament. Its core
    principle is to maximize individual payoff by predominantly free-riding on others'
    contributions, while strategically and minimally cooperating only when the overall
    cooperation level dwindles to a point where free-riding becomes unprofitable.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.initial_defect_rounds = 3
        self.end_game_rounds = 2
        self.coax_window = 5
        self.coax_threshold = max(1.0, self.n / 3.0)
        self.coax_duration = 1
        self.current_round = 0
        self.my_coax_rounds_left = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.current_round = state.round_number + 1
        if history is None:
            if self.initial_defect_rounds > 0 and self.current_round <= self.r - self.end_game_rounds:
                return Action.D
            elif self.current_round > self.r - self.end_game_rounds:
                return Action.D
            else:
                return Action.D
        if self.current_round <= self.initial_defect_rounds and self.current_round <= self.r - self.end_game_rounds:
            return Action.D
        if self.current_round > self.r - self.end_game_rounds:
            return Action.D
        if self.my_coax_rounds_left > 0:
            self.my_coax_rounds_left -= 1
            return Action.C
        num_others_cooperating_sum = 0
        rounds_considered_for_avg = 0
        start_round_idx = max(0, state.round_number - self.coax_window)
        relevant_opponent_actions = history.opponent_actions[start_round_idx:state.round_number, :]
        num_others_cooperating_sum = np.sum(relevant_opponent_actions)
        rounds_considered_for_avg = relevant_opponent_actions.shape[0]
        if rounds_considered_for_avg == 0:
            return Action.D
        avg_others_cooperating_in_window = num_others_cooperating_sum / rounds_considered_for_avg
        if avg_others_cooperating_in_window < self.coax_threshold:
            self.my_coax_rounds_left = self.coax_duration - 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_317(BaseStrategy):
    """
    The "Bait & Switch Exploiter" strategy for the N-Player Public Goods Game.

    This strategy aims to be exploitative, adaptive, and robust. It initiates with a "bait" phase
    of cooperation to probe for cooperative tendencies among opponents. After this probe, it
    evaluates the observed level of cooperation. If sufficient cooperation was detected (or
    even if not), it switches to permanent defection to free-ride on any existing public good
    or to minimize losses in an uncooperative environment. The final round always defaults
    to defection as there are no future consequences.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.P_rounds = max(1, min(4, self.r // 3))
        self.CooperationThreshold = self.n // 2
        self.ExploitationPhaseDetermined = False
        self.sum_cooperators_during_probe = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.r:
            return Action.D
        if not self.ExploitationPhaseDetermined:
            if current_round_1_indexed <= self.P_rounds:
                return Action.C
            else:
                self.sum_cooperators_during_probe = 0.0
                for r_idx in range(self.P_rounds):
                    round_total_cooperators = history.my_actions[r_idx] + np.sum(history.opponent_actions[r_idx, :])
                    self.sum_cooperators_during_probe += round_total_cooperators
                AvgCooperators_observed = self.sum_cooperators_during_probe / self.P_rounds
                self.ExploitationPhaseDetermined = True
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_318(BaseStrategy):
    """
    The Adaptive Opportunist strategy aims to maximize its own payoff by defecting as often as possible,
    but includes an adaptive mechanism to prevent the complete collapse of cooperation, thus preserving
    future opportunities for exploitation. It is robust, adaptive, and relies solely on game parameters
    and historical actions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action: Action = Action.D
        self.lure_in_progress: bool = False
        self.lure_round_count: int = 0
        self.MAX_LURE_ROUNDS: int = 2
        n = self.game_description.n_players
        k = self.game_description.k
        self.LURE_THRESHOLD_OTHERS: int = math.floor(n / k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        chosen_action: Action
        if current_round_number == 0:
            chosen_action = Action.D
            self.my_last_action = chosen_action
            self.lure_in_progress = False
            self.lure_round_count = 0
            return chosen_action
        if current_round_number == total_rounds - 1:
            chosen_action = Action.D
            self.my_last_action = chosen_action
            self.lure_in_progress = False
            self.lure_round_count = 0
            return chosen_action
        C_others_prev = sum(history.opponent_actions[-1, :])
        if self.lure_in_progress:
            self.lure_round_count += 1
            if self.lure_round_count <= self.MAX_LURE_ROUNDS:
                if C_others_prev > self.LURE_THRESHOLD_OTHERS:
                    chosen_action = Action.D
                    self.my_last_action = chosen_action
                    self.lure_in_progress = False
                    self.lure_round_count = 0
                else:
                    chosen_action = Action.C
                    self.my_last_action = chosen_action
            else:
                chosen_action = Action.D
                self.my_last_action = chosen_action
                self.lure_in_progress = False
                self.lure_round_count = 0
        elif C_others_prev <= self.LURE_THRESHOLD_OTHERS:
            chosen_action = Action.C
            self.my_last_action = chosen_action
            self.lure_in_progress = True
            self.lure_round_count = 1
        else:
            chosen_action = Action.D
            self.my_last_action = chosen_action
            self.lure_in_progress = False
            self.lure_round_count = 0
        return chosen_action

class Strategy_EXPLOITATIVE_319(BaseStrategy):
    """
    The Maverick Free-Rider strategy maximizes personal payoff by predominantly
    free-riding. It defects in the first and last rounds and defaults to defecting
    in intermediate rounds. It only cooperates under extreme circumstances where
    the public good has entirely collapsed (zero cooperators in the previous round),
    in an attempt to "reset" the game and create new opportunities for exploitation.
    This reset mechanism is attempted only once per game and not in the round
    immediately preceding the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds
        self.reset_attempted_this_game = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.D
        prev_round_my_action_bool = history.my_actions[-1]
        prev_round_opponent_actions_bools = history.opponent_actions[-1, :]
        c_total_prev = int(prev_round_my_action_bool) + np.sum(prev_round_opponent_actions_bools)
        condition_A = c_total_prev == 0
        condition_B = current_round_0_indexed < self.n_rounds - 2
        condition_C = not self.reset_attempted_this_game
        if condition_A and condition_B and condition_C:
            self.reset_attempted_this_game = True
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_320(BaseStrategy):
    """
    The "Adaptive Exploiter with Dynamic Reciprocity (AEDR)" strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by strategically balancing cooperation and defection.
    It seeks to exploit other players' contributions when possible, while using calculated cooperation
    to prevent a complete breakdown of the public good and maintain opportunities for future exploitation.

    The strategy adapts based on the previous round's collective cooperation level and its own recent
    defection history to avoid appearing too aggressive, which could lead to overall reduced payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_defection_streak = 0
        self.defection_threshold_ratio = 0.5
        self.exploitation_streak_limit = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        chosen_action: Action
        if current_round_number == 0:
            chosen_action = Action.C
        elif current_round_number == total_rounds - 1:
            chosen_action = Action.D
        else:
            C_others_prev = sum(history.opponent_actions[-1, :])
            required_cooperators_for_defection = math.ceil((n_players - 1) * self.defection_threshold_ratio)
            if C_others_prev < required_cooperators_for_defection:
                chosen_action = Action.C
            elif self.my_defection_streak >= self.exploitation_streak_limit:
                chosen_action = Action.C
            else:
                chosen_action = Action.D
        if chosen_action == Action.D:
            self.my_defection_streak += 1
        else:
            self.my_defection_streak = 0
        return chosen_action

class Strategy_EXPLOITATIVE_321(BaseStrategy):
    """
    Dynamic Exploiter with Retaliation: This strategy is designed to maximize its own cumulative payoff
    by primarily defecting. It strategically cooperates only when necessary to prevent the collapse
    of the public goods pool, thus preserving future exploitation opportunities. It adapts to opponent
    behavior across various game parameter settings.

    Core Philosophy: Prioritize defection for immediate gain. Only cooperate as a calculated
    investment to "fertilize" the environment, ensuring a sufficient level of collective cooperation
    that can be exploited in subsequent rounds, or to avoid being a "sucker" when cooperation is too low.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Dynamic Exploiter with Retaliation strategy.

        Args:
            game_description (PublicGoodsDescription): An object containing game parameters
                                                     like n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.high_cooperation_threshold = 0.65
        self.low_cooperation_threshold = 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on strategy rules.

        Args:
            state (GameState): An object containing the current round number.
            history (None | PlayerHistory): An object containing records of past actions and payoffs.
                                            It is None for the first round (round_number = 0).

        Returns:
            Action: The chosen action for the current round (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.D
        previous_round_actions_opponents = history.opponent_actions[state.round_number - 1, :]
        num_other_cooperators = np.sum(previous_round_actions_opponents)
        P_C_others = num_other_cooperators / (self.n_players - 1)
        if P_C_others >= self.high_cooperation_threshold:
            return Action.D
        elif P_C_others <= self.low_cooperation_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_322(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy for the N-Player Public Goods Game.
    This strategy is designed to navigate the game by balancing initial probing for cooperation,
    strategic free-riding, and leveraging the end-game effect. It aims to extract maximum value
    from the public good created by others, contributing only when necessary to prevent its collapse.

    Core Principles:
    1.  Probe for Cooperation: Start by cooperating to "test the waters" and potentially encourage
        other players to cooperate, creating a public good to exploit.
    2.  Strategic Free-Riding: If sufficient cooperation is observed, defect to gain the full
        private endowment (1) plus a share of the public good without contributing.
    3.  Invest to Sustain: If cooperation levels drop too low, temporarily cooperate again.
        This is an "investment" to prevent the complete collapse of the public good, ensuring
        there's something to exploit in future rounds.
    4.  End-Game Exploitation: Always defect in the final rounds due to the backward induction
        problem, maximizing immediate gains without fear of future repercussions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.initial_cooperation_rounds = max(1, min(3, r // 2))
        self.end_game_defection_rounds = max(1, min(3, r // 2))
        self.cooperation_floor_threshold = math.ceil(n / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed <= self.initial_cooperation_rounds:
            return Action.C
        if current_round_1_indexed > total_rounds - self.end_game_defection_rounds:
            return Action.D
        my_prev_action_is_cooperate = history.my_actions[state.round_number - 1]
        opponent_prev_actions_are_cooperate = history.opponent_actions[state.round_number - 1, :]
        prev_round_cooperators = int(my_prev_action_is_cooperate) + np.sum(opponent_prev_actions_are_cooperate)
        if prev_round_cooperators >= self.cooperation_floor_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_323(BaseStrategy):
    """
    This strategy aims to free-ride whenever the level of cooperation among other players is sufficiently high.
    When cooperation drops below a critical threshold, it attempts to "rescue" it by contributing,
    but only for a limited number of rounds before reverting to pure defection if its efforts are unreciprocated.
    It cooperates in the first round to seed cooperation and defects in the last round to maximize terminal payoff.
    """
    EXPLOITATION_THRESHOLD = 0.5
    PATIENCE_LIMIT = 3

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.low_coop_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == n_rounds - 1:
            return Action.D
        my_previous_contribution = 1 if history.my_actions[-1] else 0
        opponent_previous_cooperators = np.sum(history.opponent_actions[-1, :])
        previous_round_total_cooperators = my_previous_contribution + opponent_previous_cooperators
        proportion_cooperators = previous_round_total_cooperators / n_players
        if proportion_cooperators > self.EXPLOITATION_THRESHOLD:
            self.low_coop_streak = 0
        else:
            self.low_coop_streak += 1
        if self.low_coop_streak >= self.PATIENCE_LIMIT:
            return Action.D
        elif proportion_cooperators > self.EXPLOITATION_THRESHOLD:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_324(BaseStrategy):
    """
    Adaptive Opportunistic Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by defaulting to defection (free-riding).
    It only cooperates for a single round as a 'nudge' if the public good completely
    collapses (i.e., no one, including itself, cooperated in the previous round).
    After such a 'nudge', it immediately reverts to defection to exploit any
    revived cooperation. It always defects in the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.last_round_i_cooperated_as_nudge = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.last_round_i_cooperated_as_nudge = False
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            n_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if self.last_round_i_cooperated_as_nudge:
                self.last_round_i_cooperated_as_nudge = False
                return Action.D
            elif n_cooperators_prev_round == 0:
                self.last_round_i_cooperated_as_nudge = True
                return Action.C
            else:
                self.last_round_i_cooperated_as_nudge = False
                return Action.D

class Strategy_EXPLOITATIVE_325(BaseStrategy):
    """
    The Patient Predator strategy aims to "lure" other players into cooperating by demonstrating initial goodwill
    and short-term reciprocity, then "exploit" that cooperation by defecting, and "re-lure" if cooperation breaks down.
    It is patient enough to build a temporary foundation for cooperation but quick to capitalize on it,
    always defecting in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.consecutive_rounds_of_my_C_and_others_C = 0
        self.MIN_OTHERS_COOPERATING_THRESHOLD = math.floor((self.n_players - 1) / 2)
        self.EXPLOIT_DELAY_ROUNDS = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            self.consecutive_rounds_of_my_C_and_others_C = 0
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            self.consecutive_rounds_of_my_C_and_others_C = 0
            return Action.D
        my_action_last_round = history.my_actions[-1]
        num_other_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        if my_action_last_round == Action.C:
            if num_other_cooperators_last_round >= self.MIN_OTHERS_COOPERATING_THRESHOLD:
                self.consecutive_rounds_of_my_C_and_others_C += 1
                if self.consecutive_rounds_of_my_C_and_others_C >= self.EXPLOIT_DELAY_ROUNDS:
                    self.consecutive_rounds_of_my_C_and_others_C = 0
                    return Action.D
                else:
                    return Action.C
            else:
                self.consecutive_rounds_of_my_C_and_others_C = 0
                return Action.D
        elif my_action_last_round == Action.D:
            self.consecutive_rounds_of_my_C_and_others_C = 0
            if num_other_cooperators_last_round >= self.MIN_OTHERS_COOPERATING_THRESHOLD:
                return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_326(BaseStrategy):
    """
    The Adaptive Exploiter strategy for the N-Player Public Goods Game.

    This strategy aims to be exploitative, adaptive, and robust. It prioritizes maximizing
    its own payoff by strategically encouraging cooperation and then free-riding.

    Behavior rules:
    - **Round 1:** Cooperates to "seed" cooperation and probe the environment.
    - **Last Round:** Defects, as there are no future interactions for reciprocity.
    - **Intermediate Rounds:**
        - If no one cooperated in the previous round, defects to avoid being a "sucker".
        - If the total number of cooperators in the previous round meets or exceeds
          a dynamically calculated `exploitation_activation_threshold`, it defects
          to exploit the established public good.
        - Otherwise (some but insufficient cooperation), it cooperates to encourage
          further contributions, aiming to build up the public good for future exploitation.

    The `exploitation_activation_threshold` adjusts based on `n` and `k`, reflecting
    the value and ease of extracting benefit from the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Adaptive Exploiter strategy, storing game parameters and
        calculating the exploitation activation threshold.
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.exploitation_activation_threshold = max(1, math.ceil(0.5 * float(self.n) / self.k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action (Cooperate or Defect) for the current round.

        Args:
            state: The current state of the game, including the round number.
            history: A PlayerHistory object containing past actions and payoffs.
                     It is None for round 0.

        Returns:
            An Action (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        c_prev = sum(history.opponent_actions[current_round_0_indexed - 1, :]) + history.my_actions[current_round_0_indexed - 1]
        if c_prev == 0:
            return Action.D
        elif c_prev >= self.exploitation_activation_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_327(BaseStrategy):
    """
    Calculated & Conditional Free-Rider (CCFR) strategy:
    Aims to maximize payoff by free-riding on others' contributions,
    only cooperating when strategically necessary to prevent collapse
    of the public good or to "prime the pump". It avoids being a "sucker"
    when cooperation is non-existent or near the end of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        c_other_prev = np.sum(prev_round_opponent_actions)
        remaining_rounds = self.n_rounds - current_round_1_indexed
        if c_other_prev == 0:
            return Action.D
        elif c_other_prev == 1:
            threshold_rounds = math.floor(self.n_rounds / 4)
            if remaining_rounds > threshold_rounds:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_328(BaseStrategy):
    """
    The Adaptive Sucker Punch (ASP) strategy is designed to be exploitative,
    adaptive, and robust in the N-Player Public Goods Game. It aims to
    maximize its own total payoff by strategically leveraging the actions
    of other players, particularly by encouraging cooperation and then free-riding.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.r_test = max(1, min(math.floor(self.n_rounds * 0.1), 3))
        self.cooperation_threshold_high = 0.7
        self.cooperation_threshold_low = 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        elif current_round_1_indexed <= self.r_test:
            return Action.C
        else:
            total_opponent_cooperations = np.sum(history.opponent_actions)
            num_rounds_for_avg = state.round_number
            num_opponents = self.n_players - 1
            max_possible_opponent_cooperations = num_rounds_for_avg * num_opponents
            opponent_cooperation_rate = 0.0
            if max_possible_opponent_cooperations > 0:
                opponent_cooperation_rate = total_opponent_cooperations / max_possible_opponent_cooperations
            if opponent_cooperation_rate >= self.cooperation_threshold_high:
                return Action.D
            elif opponent_cooperation_rate < self.cooperation_threshold_low:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_329(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy is designed for the N-Player Public Goods Game.
    It primarily aims to free-ride, defaulting to defection to maximize its own payoff.
    It strategically cooperates only when the overall level of public good falls below
    a calculated profitable exploitation threshold, acting to "prime the pump" for
    future free-riding opportunities. The strategy exploits the first and last rounds
    by defecting, due to no prior observation and no future consequences, respectively.
    It also avoids being the "sucker" by not cooperating if no one else is.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_floor_threshold = math.floor(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == 1 or current_round_1_indexed == self.n_rounds:
            return Action.D
        previous_round_0_indexed = current_round_0_indexed - 1
        c_count_prev = int(history.my_actions[previous_round_0_indexed]) + np.sum(history.opponent_actions[previous_round_0_indexed, :])
        if c_count_prev == 0:
            return Action.D
        if c_count_prev < self.cooperation_floor_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_330(BaseStrategy):
    """
    The Cautious Opportunist strategy aims to maximize individual payoff by exploiting others' cooperation.
    It defects in the first and last rounds. In middle rounds:
    1. If it cooperated in the previous round, it immediately defects in the current round to free-ride.
    2. If it defected in the previous round:
        a. If at least one player cooperated in the previous round, it continues to defect (free-ride).
        b. If no players cooperated in the previous round (public good collapsed), it cooperates once
           to "prime the pump" and re-stimulate cooperation from others for future exploitation.
    This strategy is cautious not to be a perpetual cooperator and always exploits the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_previous_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            self.my_previous_action = Action.D
            return Action.D
        if current_round_0_indexed == total_rounds - 1:
            self.my_previous_action = Action.D
            return Action.D
        num_cooperators_in_prev_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
        current_action: Action
        if self.my_previous_action == Action.C:
            current_action = Action.D
        elif num_cooperators_in_prev_round > 0:
            current_action = Action.D
        else:
            current_action = Action.C
        self.my_previous_action = current_action
        return current_action

class Strategy_EXPLOITATIVE_331(BaseStrategy):
    """
    The Adaptive Exploiter strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by:
    1.  Inducing cooperation: Cooperating in the first round to encourage others.
    2.  Free-riding: Defecting when sufficient cooperation exists among others.
    3.  Preventing collapse: Strategically cooperating when overall cooperation is critically low
        to prevent the game from spiraling into an all-defection state, which yields minimal payoff.
    4.  End-game gain: Always defecting in the final round to capitalize on the lack of future repercussions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        EXPLOITATION_THRESHOLD_FACTOR = 0.5
        RE_ENGAGEMENT_THRESHOLD_FACTOR = 0.2
        self.EXPLOITATION_THRESHOLD = math.floor((self.game_description.n_players - 1) * EXPLOITATION_THRESHOLD_FACTOR)
        self.RE_ENGAGEMENT_THRESHOLD = math.floor((self.game_description.n_players - 1) * RE_ENGAGEMENT_THRESHOLD_FACTOR)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number_1_indexed = state.round_number + 1
        if current_round_number_1_indexed == 1:
            return Action.C
        elif current_round_number_1_indexed == self.game_description.n_rounds:
            return Action.D
        else:
            c_others_prev = np.sum(history.opponent_actions[-1, :])
            if c_others_prev >= self.EXPLOITATION_THRESHOLD:
                return Action.D
            elif c_others_prev <= self.RE_ENGAGEMENT_THRESHOLD:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_332(BaseStrategy):
    """
    Adaptive Exploiter with Strategic Goodwill (AESG) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by:
    1.  Strategically initiating cooperation (Goodwill Phase) to foster a cooperative environment.
    2.  Directly exploiting high levels of cooperation by free-riding (defecting).
    3.  Punishing low cooperation by defecting, to protect own payoff and encourage higher contributions.
    4.  Leveraging endgame rationality by always defecting in the final rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.ENDGAME_ROUNDS = 2
        self.GOODWILL_PHASE_ROUNDS = min(3, max(1, self.r - self.ENDGAME_ROUNDS))
        self.C_LOW_THRESHOLD_PROP = 0.3
        self.C_MEDIUM_THRESHOLD_PROP = 0.5
        self.C_HIGH_THRESHOLD_PROP = 0.7
        raw_C_LOW = math.floor(self.n * self.C_LOW_THRESHOLD_PROP)
        raw_C_MEDIUM = math.floor(self.n * self.C_MEDIUM_THRESHOLD_PROP)
        raw_C_HIGH = math.floor(self.n * self.C_HIGH_THRESHOLD_PROP)
        if self.n <= 2:
            self.C_LOW = 0
            self.C_MEDIUM = 1
            self.C_HIGH = 1
        elif self.n == 3:
            self.C_LOW = 0
            self.C_MEDIUM = 1
            self.C_HIGH = 2
        else:
            self.C_LOW = raw_C_LOW
            self.C_MEDIUM = raw_C_MEDIUM
            self.C_HIGH = raw_C_HIGH
            if self.C_HIGH <= self.C_MEDIUM:
                self.C_HIGH = min(self.n, self.C_MEDIUM + 1)
            if self.C_MEDIUM <= self.C_LOW:
                self.C_MEDIUM = min(self.n, self.C_LOW + 1)
        self.C_LOW = max(0, min(self.n, self.C_LOW))
        self.C_MEDIUM = max(0, min(self.n, self.C_MEDIUM))
        self.C_HIGH = max(0, min(self.n, self.C_HIGH))
        if self.C_LOW > self.C_MEDIUM:
            self.C_LOW, self.C_MEDIUM = (self.C_MEDIUM, self.C_LOW)
        if self.C_MEDIUM > self.C_HIGH:
            self.C_MEDIUM, self.C_HIGH = (self.C_HIGH, self.C_MEDIUM)
        if self.C_LOW > self.C_MEDIUM:
            self.C_LOW, self.C_MEDIUM = (self.C_MEDIUM, self.C_LOW)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed >= self.r - self.ENDGAME_ROUNDS + 1:
            return Action.D
        if current_round_1_indexed <= self.GOODWILL_PHASE_ROUNDS and current_round_1_indexed <= self.r - self.ENDGAME_ROUNDS:
            if current_round_1_indexed == 1:
                return Action.C
            else:
                cooperators_prev_round = sum(history.opponent_actions[-1, :])
                if cooperators_prev_round >= self.C_MEDIUM:
                    return Action.C
                else:
                    return Action.D
        cooperators_prev_round = sum(history.opponent_actions[-1, :])
        if cooperators_prev_round >= self.C_HIGH:
            return Action.D
        elif cooperators_prev_round >= self.C_MEDIUM:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_333(BaseStrategy):
    """
    The Calculated Free-Rider strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its total payoff by leveraging the collective benefits of cooperation
    while minimizing its individual contribution. It attempts to instigate cooperation early on,
    free-rides aggressively when cooperation levels are high, and only re-engages in temporary
    cooperation if it perceives a clear opportunity to restore collective cooperation for future exploitation.
    It is ruthless in cutting losses if cooperation is too low to be worth investing in.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        previous_round_idx = current_round_0_indexed - 1
        c_others_t_minus_1 = np.sum(history.opponent_actions[previous_round_idx, :])
        total_others = self.n - 1
        c_others_t_minus_2 = 0
        if current_round_0_indexed > 1:
            round_t_minus_2_idx = current_round_0_indexed - 2
            c_others_t_minus_2 = np.sum(history.opponent_actions[round_t_minus_2_idx, :])
        else:
            c_others_t_minus_2 = c_others_t_minus_1
        avg_cooperation_rate_others_recent = 0.0
        if current_round_0_indexed == 1:
            avg_cooperation_rate_others_recent = c_others_t_minus_1 / total_others
        else:
            avg_cooperation_rate_others_recent = (c_others_t_minus_1 + c_others_t_minus_2) / (2 * total_others)
        previous_round_other_coop_rate = c_others_t_minus_1 / total_others
        if previous_round_other_coop_rate >= 0.5:
            return Action.D
        elif avg_cooperation_rate_others_recent < 0.25:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_334(BaseStrategy):
    """
    The "Adaptive Opportunistic Defector" (AOD) strategy is designed to maximize individual payoff
    by strategically exploiting other players' cooperation in the N-Player Public Goods Game.
    It aims to free-ride whenever a sufficient level of cooperation exists, and only cooperates
    when it serves to encourage more cooperation or prevent a complete collapse of the public good
    that would also negatively impact its own payoffs.

    Core principles:
    1.  Prioritize Free-Riding: Defect when others cooperate sufficiently.
    2.  Strategic Cooperation (Baiting/Re-Baiting): Cooperate to cultivate or restore cooperation.
    3.  Self-Preservation: Defect if cooperation levels are too low to prevent being solely exploited.
    4.  Endgame Rationality: Defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.T_INITIAL_PROBE = 2 if self.n_rounds >= 3 else 1
        self.EXPLOIT_THRESHOLD = 0.4
        self.REBAIT_THRESHOLD = 0.1
        self.MEMORY_WINDOW_SIZE = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed <= self.T_INITIAL_PROBE:
            return Action.C
        total_others_cooperation_in_window = 0
        total_possible_others_cooperation_in_window = 0
        history_start_idx = max(0, state.round_number - self.MEMORY_WINDOW_SIZE)
        for s_idx in range(history_start_idx, state.round_number):
            others_cooperation_count_s = np.sum(history.opponent_actions[s_idx, :])
            total_others_cooperation_in_window += others_cooperation_count_s
            total_possible_others_cooperation_in_window += self.n_players - 1
        average_others_cooperation_rate = 0.0
        if total_possible_others_cooperation_in_window > 0:
            average_others_cooperation_rate = total_others_cooperation_in_window / total_possible_others_cooperation_in_window
        if average_others_cooperation_rate >= self.EXPLOIT_THRESHOLD:
            return Action.D
        elif average_others_cooperation_rate < self.REBAIT_THRESHOLD:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_335(BaseStrategy):
    """
    The Calculated Leech strategy is designed to be highly exploitative in the N-Player Public Goods Game.
    Its core principle is to maximize its own payoff by leveraging the contributions of other players.
    It primarily defects to extract value from the public good, only cooperating strategically to induce
    or re-induce cooperation from others, or to probe the environment. It adapts to opponent behavior
    and accounts for the game's finite horizon.
    """
    _INIT_PROBE_ROUNDS: int
    _ENDGAME_ROUNDS: int
    _COOP_THRESHOLD_PROBE: float = 0.3
    _REBAIT_COOP_ROUNDS: int = 2
    _MAX_FAILED_REBAIT_ATTEMPTS: int = 2
    rebait_countdown: int
    failed_rebait_attempts: int

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = self.game_description.n_rounds
        self._INIT_PROBE_ROUNDS = max(1, math.floor(r / 3))
        self._ENDGAME_ROUNDS = max(1, min(2, r - 1))
        self.rebait_countdown = 0
        self.failed_rebait_attempts = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if state.round_number == 0:
            self.rebait_countdown = 0
            self.failed_rebait_attempts = 0
            return Action.C
        previous_others_coop = sum(history.opponent_actions[-1, :])
        if current_round_t > r - self._ENDGAME_ROUNDS:
            return Action.D
        elif self.rebait_countdown > 0:
            self.rebait_countdown -= 1
            return Action.C
        elif current_round_t <= self._INIT_PROBE_ROUNDS:
            if previous_others_coop / (n - 1) >= self._COOP_THRESHOLD_PROBE:
                return Action.C
            else:
                return Action.D
        elif previous_others_coop > 0:
            self.failed_rebait_attempts = 0
            return Action.D
        elif self.failed_rebait_attempts < self._MAX_FAILED_REBAIT_ATTEMPTS:
            self.rebait_countdown = self._REBAIT_COOP_ROUNDS - 1
            self.failed_rebait_attempts += 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_336(BaseStrategy):
    """
    The "Shrewd Opportunist" strategy aims to maximize its own payoff by free-riding on others' cooperation.
    It cycles between building cooperation (being cooperative to induce trust) and exploiting cooperation
    (defecting to free-ride). It defaults to defection in the last round and when no cooperation is observed
    from others during a build phase (after it has cooperated) to avoid being a "sucker."
    """
    BUILD_COOPERATION = 'BUILD_COOPERATION'
    EXPLOIT_COOPERATION = 'EXPLOIT_COOPERATION'

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.BUILD_DURATION = max(1, int(round(math.log(self.n_players))))
        self.EXPLOIT_DURATION = max(1, int(round(self.n_players / 4)))
        self.phase: str | None = None
        self.phase_end_round_1_indexed: int | None = None
        self.my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        current_action_to_take: Action
        if current_round_1_indexed == 1:
            current_action_to_take = Action.C
            self.phase = self.BUILD_COOPERATION
            self.phase_end_round_1_indexed = current_round_1_indexed + self.BUILD_DURATION - 1
        elif current_round_1_indexed == self.n_rounds:
            current_action_to_take = Action.D
        else:
            prev_round_history_idx = state.round_number - 1
            C_others_prev = history.opponent_actions[prev_round_history_idx, :].sum()
            if current_round_1_indexed > self.phase_end_round_1_indexed:
                if self.phase == self.BUILD_COOPERATION:
                    self.phase = self.EXPLOIT_COOPERATION
                    self.phase_end_round_1_indexed = current_round_1_indexed + self.EXPLOIT_DURATION - 1
                else:
                    self.phase = self.BUILD_COOPERATION
                    self.phase_end_round_1_indexed = current_round_1_indexed + self.BUILD_DURATION - 1
            if self.phase == self.BUILD_COOPERATION:
                if C_others_prev == 0 and self.my_last_action == Action.C:
                    current_action_to_take = Action.D
                else:
                    current_action_to_take = Action.C
            elif self.phase == self.EXPLOIT_COOPERATION:
                current_action_to_take = Action.D
            else:
                current_action_to_take = Action.D
        self.my_last_action = current_action_to_take
        return current_action_to_take

class Strategy_EXPLOITATIVE_337(BaseStrategy):
    """
    The Opportunistic Harvester strategy is designed to be highly exploitative, adaptive,
    and robust in the N-Player Public Goods Game tournament. It aims to maximize its
    own payoff by strategically leveraging periods of cooperation from other players,
    while protecting itself and attempting to re-establish cooperation when necessary
    for future exploitation.

    It operates in three phases:
    1. PROBE: Initial rounds of cooperation to gather data and signal willingness to cooperate.
    2. EXPLOIT: Defect for a streak of rounds when other players show sufficient cooperation.
    3. RESEED: Cooperate when other players' cooperation drops too low, to encourage them
       to contribute again for future exploitation opportunities.
    """
    INITIAL_PROBE_ROUNDS: int = 3
    COOPERATION_THRESHOLD: float = 0.5
    EXPLOITATION_STREAK_LENGTH: int = 2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploit_rounds_remaining: int = 0
        self.current_mode: str = 'PROBE'

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            self.exploit_rounds_remaining = 0
            self.current_mode = 'PROBE'
            return Action.C
        if current_round_1_indexed == n_rounds:
            return Action.D
        if current_round_1_indexed <= self.INITIAL_PROBE_ROUNDS:
            self.exploit_rounds_remaining = 0
            self.current_mode = 'PROBE'
            return Action.C
        total_other_cooperations_observed = 0
        total_possible_other_observations = 0
        if history and history.opponent_actions.size > 0:
            total_other_cooperations_observed = np.sum(history.opponent_actions)
            total_possible_other_observations = history.opponent_actions.size
        average_other_cooperation_rate = 0.0
        if total_possible_other_observations > 0:
            average_other_cooperation_rate = total_other_cooperations_observed / total_possible_other_observations
        if self.exploit_rounds_remaining > 0:
            self.exploit_rounds_remaining -= 1
            self.current_mode = 'EXPLOIT'
            return Action.D
        elif average_other_cooperation_rate >= self.COOPERATION_THRESHOLD:
            self.exploit_rounds_remaining = self.EXPLOITATION_STREAK_LENGTH - 1
            self.current_mode = 'EXPLOIT'
            return Action.D
        else:
            self.current_mode = 'RESEED'
            return Action.C

class Strategy_EXPLOITATIVE_338(BaseStrategy):
    """
    The Opportunistic Free-Rider (OFR) is designed to maximize its individual payoff by primarily
    free-riding on the contributions of other players. It strategically contributes only to probe
    opponent behavior, coax cooperation from declining environments, or test the limits of exploitation.
    As the game approaches its end, the strategy becomes increasingly selfish, as the long-term
    benefits of fostering cooperation diminish.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        total_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        if state.round_number == total_rounds - 1:
            return Action.D
        my_last_action_was_C = history.my_actions[-1]
        cooperators_others_last_round = sum(history.opponent_actions[-1, :])
        others_coop_rate = cooperators_others_last_round / (n_players - 1)
        rounds_progress_factor = state.round_number / (total_rounds - 1)
        exploit_threshold = 0.6 + 0.35 * rounds_progress_factor
        collapse_threshold = 0.6 - 0.5 * rounds_progress_factor
        if others_coop_rate < collapse_threshold:
            return Action.D
        elif others_coop_rate >= exploit_threshold:
            return Action.D
        elif not my_last_action_was_C:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_340(BaseStrategy):
    """
    The "Adaptive Grudge and Greed" strategy for the N-Player Public Goods Game.
    It balances an initial willingness to foster cooperation with a strong propensity to free-ride
    and punish non-cooperators, adapting its decision-making based on historical averages
    and immediate past behavior, especially as the game progresses towards its end.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_other_cooperations_sum: int = 0
        self.total_rounds_played_so_far: int = 0
        self.Initial_Coop_Rounds: int = 1
        self.Endgame_Defect_Rounds: int = max(1, math.floor(self.game_description.n_rounds * 0.2))
        self.High_Coop_Threshold: float = 0.7
        self.Low_Coop_Threshold: float = 0.3
        self.Free_Ride_Probability: float = 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        if state.round_number > 0:
            n_c_others_prev_round_actual = sum(history.opponent_actions[-1, :])
            self.total_other_cooperations_sum += n_c_others_prev_round_actual
            self.total_rounds_played_so_far += 1
        avg_other_coop_rate_historical = 0.0
        if self.total_rounds_played_so_far > 0:
            avg_other_coop_rate_historical = self.total_other_cooperations_sum / (self.total_rounds_played_so_far * (n - 1))
        if current_round_1_indexed <= self.Initial_Coop_Rounds:
            return Action.C
        if current_round_1_indexed > r - self.Endgame_Defect_Rounds:
            return Action.D
        n_c_others_prev_round_for_decision = 0
        if state.round_number > 0:
            n_c_others_prev_round_for_decision = sum(history.opponent_actions[-1, :])
        coop_prop_prev_round = n_c_others_prev_round_for_decision / (n - 1)
        if avg_other_coop_rate_historical >= self.High_Coop_Threshold:
            if random.random() < self.Free_Ride_Probability:
                return Action.D
            else:
                return Action.C
        elif avg_other_coop_rate_historical <= self.Low_Coop_Threshold:
            return Action.D
        elif coop_prop_prev_round >= k / n:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_341(BaseStrategy):
    """
    Greedy Shepherd (GS) Strategy:
    An exploitative, adaptive, and robust strategy for the N-Player Public Goods Game.
    This strategy behaves like a shepherd who values the flock not for its own sake,
    but for the wool (payoff) it provides. It tries to "shear" (defect and free-ride)
    as much as possible, only "nurturing" (cooperating) the flock when it's too small
    to yield sufficient wool, immediately reverting to shearing when the flock is
    healthy enough.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.MSCL = max(1, math.floor(self.game_description.n_players / self.game_description.k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number + 1
        r = self.game_description.n_rounds
        if t == 1:
            return Action.D
        if t == r:
            return Action.D
        our_action_t_minus_1_is_C = history.my_actions[-1]
        num_opponent_cooperators_t_minus_1 = np.sum(history.opponent_actions[-1, :])
        C_total_t_minus_1 = num_opponent_cooperators_t_minus_1 + (1 if our_action_t_minus_1_is_C else 0)
        if not our_action_t_minus_1_is_C:
            if C_total_t_minus_1 < self.MSCL:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_342(BaseStrategy):
    """
    The Adaptive Free-Rider strategy.

    This strategy aims to be exploitative and adaptive in the N-Player Public Goods Game.
    It starts by cooperating in the first round to encourage a cooperative environment.
    In the final L rounds (calculated based on k and r), it defects unconditionally to maximize
    immediate payoff. In the intermediate rounds, it adaptively decides to cooperate or defect
    based on the total number of cooperators in the previous round relative to a threshold (n/k).
    It defects when cooperation is high enough to free-ride, and cooperates to cultivate
    cooperation when it's too low.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.L = max(1, min(self.n_rounds - 1, math.ceil(self.k_factor - 1)))
        self.cooperation_threshold_for_defection = self.n_players / self.k_factor

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed >= self.n_rounds - self.L:
            return Action.D
        else:
            prev_round_0_indexed = current_round_0_indexed - 1
            my_prev_action_was_cooperate = history.my_actions[prev_round_0_indexed]
            total_opponent_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_0_indexed, :])
            total_cooperators_prev_round = int(my_prev_action_was_cooperate) + total_opponent_cooperators_prev_round
            if total_cooperators_prev_round >= self.cooperation_threshold_for_defection:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_343(BaseStrategy):
    """
    The Opportunistic Parasite strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own total payoff by primarily relying on the dominant
    strategy to defect, but with a calculated, one-time attempt to induce or leverage
    cooperation from other players in sufficiently long games.

    Core Principles:
    - Dominant Strategy: Defect (D) is the dominant strategy in any single round.
    - Finite Horizon: In a finite game, 'Always Defect' (AD) is the Nash Equilibrium.
    - Exploiting Non-Rationality: Probes for cooperation in longer games to potentially
      exploit non-rational opponent behaviors (e.g., Always Cooperate, Tit-for-Tat).

    Decision Rules:
    1. Last Round: Always Defect (D).
    2. Initial Assessment Phase (first `r_assess` rounds): Always Defect (D) to observe
       opponent behavior without cost and immediately exploit any cooperators.
    3. Calculated Cooperation Attempt: Only once, immediately after the assessment phase,
       if enough rounds remain and a significant number of opponents cooperated in the
       last assessment round. This is a single 'C' to bait or reset.
    4. Default Exploitation: All other rounds, Defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.r_assess = min(2, self.game_description.n_rounds - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        total_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == total_rounds:
            return Action.D
        if current_round_1_indexed <= self.r_assess:
            return Action.D
        if current_round_1_indexed == self.r_assess + 1:
            if total_rounds - current_round_1_indexed > 2:
                last_round_opponent_cooperators = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
                if last_round_opponent_cooperators >= math.ceil(n_players / 2):
                    return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_344(BaseStrategy):
    """
    "The Leech" strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by free-riding on others' contributions.
    It initiates cooperation, exploits robust cooperation, and strategically intervenes
    with limited cooperation attempts if the public good is faltering. If revival attempts
    fail, it defaults to defection to avoid being a "sucker." The strategy also adheres
    to backward induction, always defecting in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_attempts_made = 0
        self.COOPERATION_THRESHOLD = math.ceil(self.n_players / 2)
        self.MAX_COOPERATION_ATTEMPTS = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.n_rounds - 1:
            self.cooperation_attempts_made = 0
            return Action.D
        if current_round_0_indexed == 0:
            self.cooperation_attempts_made = 1
            return Action.C
        prev_round_opponent_cooperators = sum(history.opponent_actions[-1, :])
        prev_round_my_action_was_C = history.my_actions[-1]
        C_prev = prev_round_opponent_cooperators + (1 if prev_round_my_action_was_C else 0)
        if C_prev >= self.COOPERATION_THRESHOLD:
            self.cooperation_attempts_made = 0
            return Action.D
        elif self.cooperation_attempts_made < self.MAX_COOPERATION_ATTEMPTS:
            self.cooperation_attempts_made += 1
            return Action.C
        else:
            self.cooperation_attempts_made = 0
            return Action.D

class Strategy_EXPLOITATIVE_345(BaseStrategy):
    """
    The Opportunistic Free-Rider strategy for the N-Player Public Goods Game.
    This strategy aims to maximize personal payoff by primarily free-riding on
    others' contributions. It cooperates in the first round to foster a cooperative
    environment and defects in the last round to maximize terminal payoff.
    In intermediate rounds, it defects if the previous round's total cooperation level
    (including its own) was high enough to ensure a substantial payoff (at least k),
    otherwise it cooperates to rebuild the public good for future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold_for_exploitation = self.n_players * (self.k_factor - 1) / self.k_factor

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev_round >= self.cooperation_threshold_for_exploitation:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_346(BaseStrategy):
    """
    The "Calculated Provocateur" strategy for the N-Player Public Goods Game.

    This strategy aims to be exploitative by primarily defecting (free-riding).
    It incorporates rare, calculated cooperative moves ("probes") only when
    cooperation among other players has completely collapsed. These probes
    are designed to "bait" opponents into re-igniting cooperation, which
    the strategy then immediately exploits by switching back to defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.last_cooperative_probe_round: int = 0
        self.observation_window_size: int = 5
        self.defection_threshold: float = 0.15
        self.probe_cooldown_rounds: int = 10

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if current_round == 0:
            return Action.D
        if current_round == r - 1:
            return Action.D
        if history is not None and history.my_actions[current_round - 1]:
            return Action.D
        P_C_others = 0.0
        num_other_players = n - 1
        start_idx = max(0, current_round - self.observation_window_size)
        if num_other_players > 0:
            observed_actions = history.opponent_actions[start_idx:current_round]
            num_observed_rounds = observed_actions.shape[0]
            if num_observed_rounds > 0:
                total_cooperations_by_others = np.sum(observed_actions)
                P_C_others = total_cooperations_by_others / (num_other_players * num_observed_rounds)
        if P_C_others < self.defection_threshold and current_round - self.last_cooperative_probe_round >= self.probe_cooldown_rounds and (current_round < r - 1):
            self.last_cooperative_probe_round = current_round
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_347(BaseStrategy):
    """
    An exploitative and adaptive strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by primarily free-riding on others'
    contributions. It employs a calculated, minimal "investment" in cooperation
    to potentially initiate or test for a cooperative environment, then defaults
    to defection to exploit it.

    Decision Rules:
    1.  First Round (Round 0, 0-indexed): Cooperate (C).
        - Rationale: This is an initial probe or "bait." By cooperating in the
          first round, the strategy signals a potential for positive interaction.
          This can encourage other players (especially reciprocal strategies)
          to cooperate, thereby establishing a public good pool that can be
          exploited in subsequent rounds. This is a strategic investment to
          assess the cooperation potential of the opponents.
    2.  Last Round (Round `n_rounds - 1`): Defect (D).
        - Rationale: In the final round, there are no future interactions for
          other players to punish defection or reward cooperation. The
          individually dominant strategy of defection yields the highest payoff
          without any future consequences. There is no strategic benefit to cooperating.
    3.  Intermediate Rounds (Round 1 to `n_rounds - 2`): Defect (D).
        - Rationale: After the initial probe in Round 0, the strategy
          switches to continuous defection. If other players are cooperating,
          this strategy free-rides on their contributions. If others are
          defecting, this strategy minimizes its losses by also defecting,
          aligning with the dominant strategy in a single round. This
          robustly exploitative approach avoids further exposure to
          non-cooperative opponents, thereby maximizing its own payoff
          against a wide range of strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        if state.round_number == total_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_348(BaseStrategy):
    """
    The Aggressive Adaptive Exploiter (AAE) strategy for the N-Player Public Goods Game.
    This strategy prioritizes free-riding (defecting) to maximize individual payoff.
    It defects when the community project is sufficiently funded by others,
    but cooperates sparingly as an "investment" when cooperation levels are too low
    to sustain profitable free-riding. It always exploits the certainties of the
    game's start (initial probe) and end (final round defection).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        else:
            prev_round_idx = state.round_number - 1
            num_opp_cooperators_prev_round = np.sum(history.opponent_actions[prev_round_idx])
            my_prev_action_was_c = history.my_actions[prev_round_idx]
            num_cooperators_prev_round = num_opp_cooperators_prev_round + (1 if my_prev_action_was_c else 0)
            if num_cooperators_prev_round == 0:
                return Action.D
            else:
                exploitation_threshold = max(1, math.floor(self.n_players / 3))
                if num_cooperators_prev_round > exploitation_threshold:
                    return Action.D
                else:
                    return Action.C

class Strategy_EXPLOITATIVE_349(BaseStrategy):
    """
    The Adaptive Exploitator strategy in the N-Player Public Goods Game.
    This strategy is designed to be highly exploitative, adaptive, and robust.
    Its core principle is to free-ride whenever possible, only contributing to the
    public good as a strategic "investment" to prevent a complete collapse of cooperation
    that would diminish its future free-riding opportunities.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        cooperation_value_per_contributor = k / n
        self.T_coop = max(1, math.floor((n - 1) * cooperation_value_per_contributor))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.D
        elif current_round_1_indexed == total_rounds:
            return Action.D
        else:
            prev_round_opponent_actions = history.opponent_actions[-1, :]
            C_others_prev_round = sum(prev_round_opponent_actions)
            if C_others_prev_round < self.T_coop:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_350(BaseStrategy):
    """
    Adaptive Exploiter strategy for the N-Player Public Goods Game.
    This strategy aims to maximize individual payoff by primarily free-riding,
    but intelligently cooperates when necessary to prevent the collapse of the
    common pool, ensuring there's always a public good from which to free-ride.
    It reacts to observed behavior and leverages endgame defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.INITIAL_COOPERATE_ROUNDS = 1
        self.ENDGAME_DEFECTION_ROUNDS = 2
        self.COOPERATION_THRESHOLD_PERCENT = 0.3
        self.COOPERATION_RETENTION_ROUNDS = 2
        self.consecutive_cooperation_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if state.round_number == 0:
            return Action.C
        if current_round_1_indexed > total_rounds - self.ENDGAME_DEFECTION_ROUNDS:
            return Action.D
        if self.consecutive_cooperation_rounds > 0:
            self.consecutive_cooperation_rounds -= 1
            return Action.C
        num_other_cooperators = np.sum(history.opponent_actions[-1, :])
        n_minus_1 = n_players - 1
        if n_minus_1 > 0:
            percentage_other_cooperators = num_other_cooperators / n_minus_1
        else:
            percentage_other_cooperators = 0.0
        if percentage_other_cooperators < self.COOPERATION_THRESHOLD_PERCENT:
            self.consecutive_cooperation_rounds = self.COOPERATION_RETENTION_ROUNDS - 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_351(BaseStrategy):
    """
    Adaptive Freerider with Probing and Disengagement (AFPD) strategy.
    This strategy aims to maximize individual payoff by exploiting cooperation when present,
    strategically investing to induce cooperation when it's low, and disengaging when
    such investments prove fruitless. It adapts its behavior based on the observed
    level of cooperation in the game, defined by a calculated C_threshold.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.C_threshold = math.ceil(self.n / self.k)
        self.Max_Failures_To_Cooperate = 3
        self.consecutive_low_cooperation_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.r - 1:
            return Action.D
        m_prev_opponents_cooperated = np.sum(history.opponent_actions[-1, :])
        m_prev_my_action = 1 if history.my_actions[-1] else 0
        m_prev = m_prev_opponents_cooperated + m_prev_my_action
        if m_prev >= self.C_threshold:
            self.consecutive_low_cooperation_rounds = 0
            return Action.D
        elif self.consecutive_low_cooperation_rounds < self.Max_Failures_To_Cooperate:
            self.consecutive_low_cooperation_rounds += 1
            return Action.C
        else:
            self.consecutive_low_cooperation_rounds = 0
            return Action.D

class Strategy_EXPLOITATIVE_352(BaseStrategy):
    """
    Adaptive Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by strategically exploiting the public goods dilemma.
    It focuses on free-riding when possible, while making calculated "investments" in cooperation
    to sustain the public good for future exploitation, and disengaging when cooperation completely collapses.
    It avoids rigid patterns and adapts to the observed behavior of other players.

    Strategy Parameters:
    - INITIAL_COOPERATE_ROUNDS: Number of initial rounds to always cooperate. (Default: 2)
    - ENDGAME_ROUNDS: Number of final rounds to always defect. (Default: 1)
    - COOPERATION_CHECK_WINDOW: Number of recent rounds to consider when calculating
                                average cooperation rates of other players. (Default: 3)
    - MIN_COOP_FOR_DEFECTION_THRESHOLD: Minimum average fraction of other players
                                        cooperating in the window for this strategy to Defect. (Default: 0.5)
    - PURE_DEFECTION_THRESHOLD_OTHERS: A very low average fraction of other players
                                       cooperating, indicating widespread defection. (Default: 0.1)
    - PURE_DEFECTION_TRIGGER_ROUNDS: Number of consecutive rounds where the
                                     PURE_DEFECTION_THRESHOLD_OTHERS is met before permanent Defection. (Default: 3)
    """
    INITIAL_COOPERATE_ROUNDS = 2
    ENDGAME_ROUNDS = 1
    COOPERATION_CHECK_WINDOW = 3
    MIN_COOP_FOR_DEFECTION_THRESHOLD = 0.5
    PURE_DEFECTION_THRESHOLD_OTHERS = 0.1
    PURE_DEFECTION_TRIGGER_ROUNDS = 3

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        total_rounds = self.game_description.n_rounds
        current_round_0idx = state.round_number
        if current_round_0idx < self.INITIAL_COOPERATE_ROUNDS:
            return Action.C
        if current_round_0idx >= total_rounds - self.ENDGAME_ROUNDS:
            return Action.D
        num_other_players = n_players - 1
        if num_other_players == 0:
            return Action.C
        total_other_cooperators_in_window = 0
        actual_rounds_in_window = 0
        start_check_round_0idx = max(self.INITIAL_COOPERATE_ROUNDS, current_round_0idx - self.COOPERATION_CHECK_WINDOW)
        for r_idx in range(start_check_round_0idx, current_round_0idx):
            num_coop_by_others_in_round = np.sum(history.opponent_actions[r_idx, :])
            total_other_cooperators_in_window += num_coop_by_others_in_round
            actual_rounds_in_window += 1
        if actual_rounds_in_window == 0:
            return Action.C
        avg_other_coop_rate_in_window = total_other_cooperators_in_window / (actual_rounds_in_window * num_other_players)
        consecutive_low_coop_rounds = 0
        for r_idx in range(current_round_0idx - 1, self.INITIAL_COOPERATE_ROUNDS - 1, -1):
            num_coop_by_others_in_r = np.sum(history.opponent_actions[r_idx, :])
            if num_coop_by_others_in_r / num_other_players < self.PURE_DEFECTION_THRESHOLD_OTHERS:
                consecutive_low_coop_rounds += 1
            else:
                break
        if consecutive_low_coop_rounds >= self.PURE_DEFECTION_TRIGGER_ROUNDS:
            return Action.D
        if avg_other_coop_rate_in_window >= self.MIN_COOP_FOR_DEFECTION_THRESHOLD:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_354(BaseStrategy):
    """
    The Adaptive Opportunist strategy is designed to be exploitative, adaptive, and robust in the N-Player Public Goods Game.
    It aims to maximize its own total payoff by free-riding on others' cooperation while taking calculated risks
    to re-initiate or maintain cooperation when it falters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.COLLAPSE_THRESHOLD = 1
        self.HIGH_COOPERATION_THRESHOLD = math.floor(self.game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == total_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        my_last_action_bool = history.my_actions[state.round_number - 1]
        opponent_last_round_cooperators = sum(history.opponent_actions[state.round_number - 1, :])
        last_round_total_cooperators = opponent_last_round_cooperators + (1 if my_last_action_bool else 0)
        if last_round_total_cooperators <= self.COLLAPSE_THRESHOLD:
            return Action.C
        elif last_round_total_cooperators >= self.HIGH_COOPERATION_THRESHOLD:
            return Action.D
        elif my_last_action_bool:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_355(BaseStrategy):
    """
    The Adaptive Exploiter strategy for the N-Player Public Goods Game.

    This strategy aims to establish and maintain a high level of cooperation among
    other players, from which it can then free-ride. It makes strategic "investments"
    (cooperation) only when necessary to keep the public good flowing, and disengages
    when cooperation is too low to be worth the effort.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.memory_window_M = 5
        self.initial_cooperation_rounds_R_init = 2
        self.endgame_rounds_R_end = 2
        self.cooperation_floor = 0.1
        self.cooperation_ceiling_for_free_ride = 0.5
        self.max_stimulation_rounds = 2
        self.failed_stimulation_cooldown = 3
        self.current_stimulation_count = 0
        self.current_cooldown_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        n_others = n_players - 1
        if current_round_number < self.initial_cooperation_rounds_R_init:
            return Action.C
        if current_round_number >= n_rounds - self.endgame_rounds_R_end:
            return Action.D
        total_other_cooperators_in_memory_window = 0
        num_actual_rounds_in_window = 0
        start_hist_round = max(0, current_round_number - self.memory_window_M)
        end_hist_round = current_round_number - 1
        for t_hist in range(start_hist_round, end_hist_round + 1):
            num_actual_rounds_in_window += 1
            total_other_cooperators_in_memory_window += np.sum(history.opponent_actions[t_hist, :])
        proportion_others_cooperating = total_other_cooperators_in_memory_window / (num_actual_rounds_in_window * n_others)
        action_to_take = Action.D
        if self.current_cooldown_count > 0:
            self.current_cooldown_count -= 1
            self.current_stimulation_count = 0
            action_to_take = Action.D
        elif self.current_stimulation_count > 0:
            if proportion_others_cooperating >= self.cooperation_ceiling_for_free_ride:
                self.current_stimulation_count = 0
                action_to_take = Action.D
            elif self.current_stimulation_count >= self.max_stimulation_rounds:
                self.current_stimulation_count = 0
                self.current_cooldown_count = self.failed_stimulation_cooldown
                action_to_take = Action.D
            else:
                self.current_stimulation_count += 1
                action_to_take = Action.C
        elif proportion_others_cooperating >= self.cooperation_ceiling_for_free_ride:
            action_to_take = Action.D
        elif proportion_others_cooperating <= self.cooperation_floor:
            action_to_take = Action.D
        else:
            self.current_stimulation_count = 1
            action_to_take = Action.C
        return action_to_take

class Strategy_EXPLOITATIVE_356(BaseStrategy):
    """
    The Parasite strategy for the N-Player Public Goods Game.

    This strategy aims to exploit other players' cooperation by defecting when a
    sufficient public good is available, while avoiding being a "sucker" and
    occasionally re-initiating cooperation if it collapses. It is a calculated
    opportunist, always prioritizing its own maximum gain.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.T_exploit = max(2, math.floor(self.n / 2))
        self.T_retest = max(1, math.floor(self.n / self.k))
        self.retest_period = 3
        self.low_cooperation_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == self.r - 1:
            return Action.D
        C_opponents_prev = np.sum(history.opponent_actions[-1, :])
        C_my_prev = 1 if history.my_actions[-1] else 0
        C_total_prev = C_opponents_prev + C_my_prev
        if C_total_prev < self.T_retest:
            self.low_cooperation_streak += 1
        else:
            self.low_cooperation_streak = 0
        if C_total_prev >= self.T_exploit:
            return Action.D
        elif self.low_cooperation_streak >= self.retest_period:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_357(BaseStrategy):
    """
    The "Last-Minute Leech" strategy aims for exploitative, adaptive, and robust play in
    finitely repeated N-Player Public Goods games. It starts with a brief period of cooperation
    to encourage others, then aggressively defects to free-ride on any observed cooperation.
    If cooperation from others completely collapses, it may temporarily re-cooperate ("re-bait")
    to revive the exploitable pool, but only if there are enough rounds remaining.
    The strategy always defects in the final rounds, recognizing the endgame's dominant strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_coop_rounds = min(2, self.game_description.n_rounds - 1)
        self.re_bait_min_remaining_rounds = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        if current_round_1_indexed <= self.initial_coop_rounds:
            return Action.C
        cooperators_others_in_prev_round = np.sum(history.opponent_actions[-1, :])
        if cooperators_others_in_prev_round >= 1:
            return Action.D
        else:
            remaining_rounds = self.game_description.n_rounds - current_round_1_indexed
            if remaining_rounds > self.re_bait_min_remaining_rounds:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_358(BaseStrategy):
    """
    The Aggressive Opportunist (AO) strategy for the N-Player Public Goods Game.

    This strategy's primary goal is to maximize its own payoff by free-riding
    on others' cooperation. It balances proactive probing for cooperation,
    swift exploitation of observed cooperation, and defensive defection when
    cooperation is low or the game is ending.

    - Starts with cooperation (Round 0) to probe for cooperative opponents.
    - In intermediate rounds, it defects if opponent cooperation is high
      (above EXPLOIT_THRESHOLD_FRACTION).
    - If opponent cooperation is moderate (above MAINTAIN_THRESHOLD_FRACTION)
      and the strategy itself cooperated in the previous round, it cooperates
      again to encourage further cooperation.
    - Otherwise (low opponent cooperation or strategy defected previously
      without improvement), it defects to minimize losses.
    - Always defects in the final round (backward induction).
    """
    EXPLOIT_THRESHOLD_FRACTION = 0.6
    MAINTAIN_THRESHOLD_FRACTION = 0.3

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_prev_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        num_players = self.game_description.n_players
        if current_round_zero_indexed == 0:
            self.my_prev_action = Action.C
            return Action.C
        if current_round_zero_indexed == total_rounds - 1:
            self.my_prev_action = Action.D
            return Action.D
        opponents_prev_actions = history.opponent_actions[-1, :]
        c_opp_prev_count = sum(opponents_prev_actions)
        num_opponents = num_players - 1
        c_opp_prev_fraction = 0.0
        if num_opponents > 0:
            c_opp_prev_fraction = c_opp_prev_count / num_opponents
        action_for_this_round: Action
        if c_opp_prev_fraction >= self.EXPLOIT_THRESHOLD_FRACTION:
            action_for_this_round = Action.D
        elif c_opp_prev_fraction >= self.MAINTAIN_THRESHOLD_FRACTION and self.my_prev_action == Action.C:
            action_for_this_round = Action.C
        else:
            action_for_this_round = Action.D
        self.my_prev_action = action_for_this_round
        return action_for_this_round

class Strategy_EXPLOITATIVE_359(BaseStrategy):
    """
    The Persistent Free-Rider strategy prioritizes free-riding by defecting whenever the
    overall level of cooperation among players is sufficiently high. However, it strategically
    cooperates when cooperation falls below a critical threshold to "re-seed" the public good,
    preventing a total collapse that would eliminate future exploitation opportunities.
    It cooperates in the first round to encourage initial contributions and defects in the final
    round for maximal individual payoff, aligning with backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.min_coop_ratio_for_exploitation = 0.25

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == r - 1:
            return Action.D
        my_prev_action_was_C = history.my_actions[current_round_number - 1]
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[current_round_number - 1, :])
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + (1 if my_prev_action_was_C else 0)
        observed_coop_ratio = total_cooperators_prev_round / n
        if observed_coop_ratio < self.min_coop_ratio_for_exploitation:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_360(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize its own payoff by
    free-riding on the contributions of others as much as possible. It adapts
    its behavior based on the observed level of cooperation in the previous round,
    using a parameter-dependent threshold to decide when to exploit and when to
    "invest" by cooperating.

    - Round 1: Cooperate (C) to probe for cooperators and signal initial willingness.
    - Intermediate Rounds:
        - Calculates a dynamic Cooperation Threshold (T = ceil(n / k)).
        - If the number of cooperators in the previous round (N_C_prev) is >= T, it Defects (D)
          to free-ride, as the public good is deemed robust enough.
        - If N_C_prev < T, it Cooperates (C) to "invest" and prevent the public good
          from collapsing, aiming to encourage further cooperation for future exploitation.
    - Last Round: Defect (D), as there are no future consequences.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold_T = math.ceil(self.n_players / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            num_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            if num_cooperators_prev_round >= self.cooperation_threshold_T:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_361(BaseStrategy):
    """
    The Bully strategy is designed to aggressively "prime the pump" for cooperation among other players.
    Once sufficient cooperation is observed, it ruthlessly defects to free-ride on the public good.
    If cooperation levels drop below a desirable threshold, it switches to a punitive defection phase
    to try and coerce other players back into cooperating, before attempting to re-establish a
    cooperative (and exploitable) environment. Near the end of the game, it defaults to defection,
    leveraging the finite nature of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.INITIAL_COOPERATE_ROUNDS = min(2, self.r - 1)
        self.DEFECTION_ROUNDS_BEFORE_END = max(1, math.floor(self.r / 3))
        self.TARGET_COOPERATION_RATE = 0.5
        self.PUNISHMENT_DURATION = 2
        self.rounds_in_punishment = 0
        self.my_last_action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number_t = state.round_number + 1
        other_cooperators_prev_round = 0
        if current_round_number_t > 1:
            other_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        MIN_COOPERATORS_NEEDED = math.ceil((self.n - 1) * self.TARGET_COOPERATION_RATE)
        if MIN_COOPERATORS_NEEDED < 0:
            MIN_COOPERATORS_NEEDED = 0
        next_action = Action.D
        if current_round_number_t > self.r - self.DEFECTION_ROUNDS_BEFORE_END:
            next_action = Action.D
            self.rounds_in_punishment = 0
        elif current_round_number_t <= self.INITIAL_COOPERATE_ROUNDS and self.rounds_in_punishment == 0:
            next_action = Action.C
        elif self.rounds_in_punishment > 0:
            if self.rounds_in_punishment < self.PUNISHMENT_DURATION:
                self.rounds_in_punishment += 1
                next_action = Action.D
            else:
                self.rounds_in_punishment = 0
                next_action = Action.C
        elif other_cooperators_prev_round >= MIN_COOPERATORS_NEEDED:
            next_action = Action.D
        else:
            self.rounds_in_punishment = 1
            next_action = Action.D
        self.my_last_action = next_action
        return next_action

class Strategy_EXPLOITATIVE_362(BaseStrategy):
    """
    Adaptive Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by defaulting to defecting (free-riding).
    It only cooperates strategically to "re-seed" cooperation if the group's cooperation
    level has completely collapsed, creating future opportunities for exploitation.
    It quickly reverts to defecting once cooperation is sufficiently re-established
    or if there's enough cooperation to exploit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.DEFECTION_COLLAPSE_THRESHOLD: int = 0
        self.RESEED_TRIGGER_ROUNDS: int = 3
        self.RESEED_SUCCESS_THRESHOLD_INCREASE: int = 2
        self.EXPLOITATION_THRESHOLD_HIGH: int = math.ceil((self.n_players - 1) / 2)
        self.my_previous_action: Action | None = None
        self.others_cooperation_count_prev_round: int = 0
        self.rounds_of_low_cooperation_observed: int = 0
        self.last_reseed_attempt_round: int = -1
        self.others_cooperation_count_before_reseed_attempt: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        action_for_this_round: Action = Action.D
        if current_round_0_indexed > 0 and history is not None:
            self.others_cooperation_count_prev_round = sum(history.opponent_actions[current_round_0_indexed - 1, :])
            self.my_previous_action = Action.C if history.my_actions[current_round_0_indexed - 1] else Action.D
        else:
            self.others_cooperation_count_prev_round = 0
            self.my_previous_action = None
        if self.others_cooperation_count_prev_round <= self.DEFECTION_COLLAPSE_THRESHOLD:
            self.rounds_of_low_cooperation_observed += 1
        else:
            self.rounds_of_low_cooperation_observed = 0
        if current_round_0_indexed == 0:
            action_for_this_round = Action.D
        elif current_round_0_indexed == self.n_rounds - 1:
            action_for_this_round = Action.D
        elif self.my_previous_action == Action.C and current_round_0_indexed - 1 == self.last_reseed_attempt_round:
            if self.others_cooperation_count_prev_round >= self.others_cooperation_count_before_reseed_attempt + self.RESEED_SUCCESS_THRESHOLD_INCREASE:
                action_for_this_round = Action.D
            else:
                action_for_this_round = Action.D
            self.last_reseed_attempt_round = -1
            self.others_cooperation_count_before_reseed_attempt = 0
        elif self.others_cooperation_count_prev_round >= self.EXPLOITATION_THRESHOLD_HIGH:
            action_for_this_round = Action.D
        elif self.rounds_of_low_cooperation_observed >= self.RESEED_TRIGGER_ROUNDS:
            action_for_this_round = Action.C
            self.last_reseed_attempt_round = current_round_0_indexed
            self.others_cooperation_count_before_reseed_attempt = self.others_cooperation_count_prev_round
        else:
            action_for_this_round = Action.D
        return action_for_this_round

class Strategy_EXPLOITATIVE_363(BaseStrategy):
    """
    The Exploitative Opportunist strategy aims to maximize individual payoff by primarily free-riding on others'
    contributions. It defaults to defecting (keeping its endowment). However, it strategically "pumps" (cooperates)
    only when the overall level of cooperation among other players falls critically low, threatening the public good's
    existence and thus the potential for future exploitation. This "pump" is a calculated, infrequent investment
    intended to revive cooperation just enough to allow the strategy to resume free-riding.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.COOPERATION_FLOOR_THRESHOLD = 0.2
        self.MIN_ROUNDS_BETWEEN_PUMPS = 3
        self.rounds_since_my_last_C_decision = self.MIN_ROUNDS_BETWEEN_PUMPS

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        action = Action.D
        if current_round_1_indexed == 1:
            action = Action.D
        elif current_round_1_indexed == self.game_description.n_rounds:
            action = Action.D
        else:
            c_others_prev = np.sum(history.opponent_actions[-1, :])
            num_opponents = self.game_description.n_players - 1
            if num_opponents > 0:
                cooperation_rate_others = c_others_prev / num_opponents
            else:
                cooperation_rate_others = 0.0
            if cooperation_rate_others < self.COOPERATION_FLOOR_THRESHOLD:
                if self.rounds_since_my_last_C_decision >= self.MIN_ROUNDS_BETWEEN_PUMPS:
                    action = Action.C
        if action == Action.C:
            self.rounds_since_my_last_C_decision = 0
        else:
            self.rounds_since_my_last_C_decision += 1
        return action

class Strategy_EXPLOITATIVE_364(BaseStrategy):
    """
    The Adaptive Opportunist (AO) strategy is designed to be highly exploitative in the
    N-Player Public Goods Game. It aims to maximize its own payoff by strategically
    leveraging others' cooperation while minimizing its own contributions. It is adaptive
    to varying levels of cooperation among opponents and robust against different behavioral patterns.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_threshold = math.ceil(self.n_players / 2.0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        my_action_prev = history.my_actions[-1]
        num_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        C_prev = num_opponent_cooperators_prev + (1 if my_action_prev else 0)
        if C_prev >= self.cooperation_threshold:
            return Action.D
        elif my_action_prev:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_365(BaseStrategy):
    """
    The Persistent Parasite strategy aims to maximize its own payoff by predominantly defecting,
    only cooperating under specific conditions to "rehabilitate" the overall cooperation level of the group,
    and then reverting to exploitation.

    Core Philosophy:
    It seeks to extract the largest possible share from the public good created by others,
    minimizing its own contributions. It only "invests" in cooperation as a last resort
    when the public good is on the verge of collapsing entirely, as a strategic move to
    ensure there's still something valuable to exploit in future rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        c_others_last_round = sum(history.opponent_actions[-1, :])
        if c_others_last_round == 0:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_366(BaseStrategy):
    """
    The "Adaptive Opportunistic Defector" (AOD) strategy defects by default to maximize immediate personal gain.
    It only cooperates under specific conditions to prevent the collapse of the public good that serves as the
    source of exploitation. This strategy is robust because it ensures a baseline payoff through defection
    and adaptive because it reacts to observed changes in group cooperation, attempting to nudge the group
    towards a state beneficial for free-riding.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        self.rounds_since_last_cooperation = max(3, math.floor(n / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_0idx = state.round_number
        current_round_1idx = current_round_0idx + 1
        self.rounds_since_last_cooperation += 1
        action_for_this_round = Action.D
        if current_round_1idx == r:
            action_for_this_round = Action.D
        elif current_round_1idx == 1:
            action_for_this_round = Action.D
        else:
            C_t_minus_1 = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if C_t_minus_1 < 2:
                action_for_this_round = Action.D
            elif current_round_1idx > 2:
                C_t_minus_2 = np.sum(history.opponent_actions[-2, :]) + int(history.my_actions[-2])
                my_action_t_minus_1 = Action.C if history.my_actions[-1] else Action.D
                cooldown_threshold = max(3, math.floor(n / 2))
                if my_action_t_minus_1 == Action.D and C_t_minus_1 < C_t_minus_2 * 0.75 and (C_t_minus_1 <= n / 2) and (self.rounds_since_last_cooperation >= cooldown_threshold):
                    action_for_this_round = Action.C
        if action_for_this_round == Action.C:
            self.rounds_since_last_cooperation = 0
        return action_for_this_round

class Strategy_EXPLOITATIVE_367(BaseStrategy):
    """
    The Opportunistic Exploiter strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own total payoff by primarily defecting when an
    exploitable level of public good is being generated by other players. However,
    it will strategically cooperate (invest) when cooperation levels are too low,
    to prevent a complete collapse to an all-Defect state that would limit its
    potential for future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters.

        Args:
            game_description (PublicGoodsDescription): An object containing
                                                       n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.t_exploit = max(1, math.floor(self.n_players / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action (Cooperate C or Defect D) for the current round.

        Args:
            state (GameState): An object containing the current round number (0-indexed).
            history (None | PlayerHistory): An object containing past actions and payoffs
                                            for this player and opponents. Is None for round 0.

        Returns:
            Action: Action.C for Cooperate, Action.D for Defect.
        """
        current_round_idx = state.round_number
        if current_round_idx == self.n_rounds - 1:
            return Action.D
        if current_round_idx == 0:
            return Action.C
        my_prev_action_val = 1 if history.my_actions[-1] else 0
        num_cooperators_prev_round = my_prev_action_val + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_prev_round >= self.t_exploit:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_368(BaseStrategy):
    """
    The Last Straw Defector strategy balances immediate exploitation with a cautious,
    one-time attempt to prevent a complete collapse of cooperation. It defects
    whenever possible to free-ride, but if the public good completely collapses
    (zero cooperators in the previous round), it makes a single, last-ditch
    cooperative "bait" effort. If this bait fails to elicit cooperation,
    the strategy reverts to permanent defection to guarantee a robust minimum payoff.
    It always defects in the first and final rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.has_attempted_bait = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == total_rounds:
            return Action.D
        previous_round_0_indexed = state.round_number - 1
        my_action_prev_round = history.my_actions[previous_round_0_indexed]
        opponent_cooperators_prev_round = np.sum(history.opponent_actions[previous_round_0_indexed, :])
        cooperators_in_previous_round = opponent_cooperators_prev_round + (1 if my_action_prev_round else 0)
        if cooperators_in_previous_round > 0:
            return Action.D
        elif not self.has_attempted_bait:
            self.has_attempted_bait = True
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_369(BaseStrategy):
    """
    The Adaptive Exploiter strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own total payoff by strategically
    inducing cooperation from others and then free-riding on it. It
    operates in three distinct phases: an initial probe, adaptive
    exploitation, and a decisive endgame.

    Core Principle: Strategically induce cooperation from others, then free-ride
    when their cooperation level is sufficiently high. When cooperation is low,
    contribute to try and build it up, but never miss an opportunity to defect
    without repercussions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.r - 1:
            return Action.D
        else:
            num_other_cooperators_prev = sum(history.opponent_actions[-1, :])
            n_minus_1 = self.n - 1
            if n_minus_1 == 0:
                proportion_coop_others = 0.0
            else:
                proportion_coop_others = num_other_cooperators_prev / n_minus_1
            if proportion_coop_others >= self.k / self.n:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_370(BaseStrategy):
    """
    The Opportunistic Provocateur strategy for the N-Player Public Goods Game.

    This strategy prioritizes immediate self-interest by defecting, aiming to free-ride
    on others' cooperation. However, it incorporates a strategic "ignition" mechanism:
    if cooperation among players consistently falls too low, the strategy will temporarily
    cooperate to "provoke" or re-establish a more cooperative environment. Once a sufficient
    level of cooperation is observed, it reverts to defecting, exploiting the efforts of others.
    This adaptive approach ensures robustness against both pervasive cooperation and universal defection.
    The strategy always defects in the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n_players = game_description.n_players
        self.LOW_COOP_THRESHOLD = max(1, math.floor(n_players / 3))
        self.IGNITION_STREAK = 3
        self.HIGH_COOP_THRESHOLD = n_players - 1
        self.consecutive_rounds_of_low_cooperation = 0
        self.my_cooperation_in_a_low_coop_phase_flag = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round == 0:
            self.consecutive_rounds_of_low_cooperation = 0
            self.my_cooperation_in_a_low_coop_phase_flag = False
            return Action.D
        if current_round == total_rounds - 1:
            return Action.D
        num_cooperators_last_round = (1 if history.my_actions[-1] else 0) + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_last_round < self.LOW_COOP_THRESHOLD:
            self.consecutive_rounds_of_low_cooperation += 1
        else:
            self.consecutive_rounds_of_low_cooperation = 0
            self.my_cooperation_in_a_low_coop_phase_flag = False
        if num_cooperators_last_round >= self.HIGH_COOP_THRESHOLD:
            return Action.D
        elif self.consecutive_rounds_of_low_cooperation >= self.IGNITION_STREAK:
            if self.my_cooperation_in_a_low_coop_phase_flag and num_cooperators_last_round < self.LOW_COOP_THRESHOLD:
                self.my_cooperation_in_a_low_coop_phase_flag = False
                self.consecutive_rounds_of_low_cooperation = 0
                return Action.D
            else:
                self.my_cooperation_in_a_low_coop_phase_flag = True
                return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_371(BaseStrategy):
    """
    The Adaptive Free-Rider strategy balances immediate gain from defecting with the long-term
    need to prevent the public good from collapsing entirely. It aims to encourage a sufficient
    level of cooperation from others, then free-rides on that cooperation, while avoiding being
    exploited as a lone cooperator.

    Strategy breakdown:
    - Round 0 (first round): Always Cooperate (C) to probe the environment.
    - Last Round: Always Defect (D) due to no future interactions.
    - Middle Rounds:
        - If previous cooperation (`C_prev`) was 0: Defect (D), as the public good has collapsed.
        - If `C_prev` was 1 and we were the sole cooperator: Defect (D), to avoid being exploited.
        - If `C_prev` is below `THRESHOLD_COOP_MAINTAIN` (but not 0 or sole cooperator scenario): Cooperate (C)
          to invest and prevent total collapse, hoping to encourage others.
        - If `C_prev` is at or above `THRESHOLD_COOP_MAINTAIN`: Defect (D), free-riding on sufficient cooperation.

    `THRESHOLD_COOP_MAINTAIN` is the minimum number of cooperators required for each player's
    share of the public good to be at least 1, calculated as `ceil(n / k)`.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.THRESHOLD_COOP_MAINTAIN = math.ceil(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        C_prev_opponents = np.sum(history.opponent_actions[-1, :])
        my_action_prev_bool = history.my_actions[-1]
        C_prev = C_prev_opponents + (1 if my_action_prev_bool else 0)
        my_action_prev_enum = Action.C if my_action_prev_bool else Action.D
        if C_prev == 0:
            return Action.D
        elif C_prev == 1 and my_action_prev_enum == Action.C:
            return Action.D
        elif C_prev < self.THRESHOLD_COOP_MAINTAIN:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_372(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy is designed to be highly exploitative in the N-Player
    Public Goods Game. It aims to maximize its own total payoff by strategically leveraging the
    cooperative tendencies of other players, while minimizing its contributions when cooperation
    is either too low to be beneficial or when it can safely free-ride without risking the
    collapse of the public good.

    This strategy balances an initial investment in cooperation with dynamic decision-making
    based on observed player behavior and a clear focus on self-interest.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n_players = self.game_description.n_players
        total_rounds = self.game_description.n_rounds
        self.initial_signaling_rounds_1_indexed = min(2, total_rounds - 1)
        self.cooperation_breakdown_threshold = math.floor(n_players * 0.25)
        self.exploitation_threshold = math.floor(n_players * 0.75)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == total_rounds:
            return Action.D
        if current_round_1_indexed <= self.initial_signaling_rounds_1_indexed:
            return Action.C
        cooperators_opponents_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
        my_action_prev_round_int = 1 if history.my_actions[state.round_number - 1] else 0
        c_prev = cooperators_opponents_prev_round + my_action_prev_round_int
        if c_prev < self.cooperation_breakdown_threshold:
            return Action.D
        elif c_prev >= self.exploitation_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_373(BaseStrategy):
    """
    Adaptive Opportunistic Free-Rider (AOFR) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by primarily free-riding on others' contributions.
    It strategically cooperates only to "bail out" collective cooperation when it's too low to be beneficial
    (even for a free-rider) or to initiate cooperation. It strictly defects when such efforts prove futile
    or when robust free-riding opportunities exist, refusing to be a "sucker".
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.C_threshold = max(1, math.floor(self.n / self.k))
        self._my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        current_action: Action = Action.D
        if current_round == 0:
            current_action = Action.C
        elif current_round == self.r - 1:
            current_action = Action.D
        else:
            C_total_prev = int(np.sum(history.opponent_actions[-1, :])) + int(history.my_actions[-1])
            my_last_action_enum = self._my_last_action
            if C_total_prev >= self.C_threshold:
                current_action = Action.D
            elif C_total_prev == 0 or (C_total_prev == 1 and my_last_action_enum == Action.C):
                current_action = Action.D
            else:
                current_action = Action.C
        self._my_last_action = current_action
        return current_action

class Strategy_EXPLOITATIVE_374(BaseStrategy):
    """
    Opportunistic Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to balance immediate free-riding gains with the need to sustain
    cooperation from other players. It starts by cooperating to "bait" others, then
    exploits high levels of cooperation, cooperates to rebuild/maintain moderate
    cooperation, and defects when cooperation is too low or when its own defection
    streak becomes too long, to avoid becoming a "sucker" or to re-bait. It always
    defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_coop_rounds = 2
        self.exploit_cooperation_threshold_fraction = 0.6
        self.recoop_cooperation_threshold_fraction = 0.3
        self.defection_streak_limit = 2
        self.current_defection_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        if current_round_1_indexed <= self.initial_coop_rounds:
            self.current_defection_streak = 0
            return Action.C
        C_others_prev = sum(history.opponent_actions[-1, :])
        num_other_players = self.game_description.n_players - 1
        if num_other_players < 1:
            return Action.D
        threshold_for_exploit_count = num_other_players * self.exploit_cooperation_threshold_fraction
        threshold_for_recoop_count = num_other_players * self.recoop_cooperation_threshold_fraction
        if C_others_prev >= threshold_for_exploit_count:
            self.current_defection_streak += 1
            return Action.D
        elif self.current_defection_streak >= self.defection_streak_limit:
            self.current_defection_streak = 0
            return Action.C
        elif C_others_prev >= threshold_for_recoop_count:
            self.current_defection_streak = 0
            return Action.C
        else:
            self.current_defection_streak += 1
            return Action.D

class Strategy_EXPLOITATIVE_375(BaseStrategy):
    """
    The "Opportunistic Prime-and-Exploit" Strategy for the N-Player Public Goods Game.
    This strategy defaults to defection to free-ride, but includes a calculated "priming"
    mechanism to temporarily cooperate and re-ignite collective contributions when they
    are too low, solely for the purpose of re-establishing a larger pool of cooperators
    to exploit in subsequent rounds. It is adaptive to the overall cooperation level.
    """
    RE_PRIME_COOLDOWN_ROUNDS: int = 4
    RE_PRIME_MIN_COOPERATORS_OTHERS: int = 1

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters and state variables.
        """
        self.game_description = game_description
        self.last_priming_round: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action for the current round based on the Opportunistic Prime-and-Exploit strategy.
        """
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        action_to_take = Action.D
        if state.round_number == 0:
            return Action.D
        can_prime_due_to_future_rounds = self.game_description.n_rounds - (state.round_number + 1) >= self.RE_PRIME_COOLDOWN_ROUNDS
        not_on_cooldown = state.round_number - self.last_priming_round >= self.RE_PRIME_COOLDOWN_ROUNDS
        if can_prime_due_to_future_rounds and not_on_cooldown:
            cooperators_others_in_prev_round = np.sum(history.opponent_actions[-1, :])
            if cooperators_others_in_prev_round <= self.RE_PRIME_MIN_COOPERATORS_OTHERS:
                action_to_take = Action.C
                self.last_priming_round = state.round_number
        return action_to_take

class Strategy_EXPLOITATIVE_376(BaseStrategy):
    """
    The Calculated Opportunist strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by strategically balancing
    initial cooperation to foster a cooperative environment, aggressive
    free-riding when opportunities arise, and temporary contributions to
    restore cooperation if it dwindles. It rigorously avoids being a "sucker"
    by not cooperating when others are not, and always defects in the final round
    to maximize immediate gain.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_probe_rounds: int = 1
        self.recovery_effort_rounds: int = 2
        self.cooperation_threshold: float = self.game_description.k / self.game_description.n_players
        self.current_state: str | None = None
        self.rounds_in_current_state: int = 0

    def _get_opponent_cooperation_proportion(self, history: PlayerHistory) -> float:
        """
        Calculates the proportion of *other* players who cooperated
        in the immediately preceding round.
        """
        num_others = self.game_description.n_players - 1
        if num_others == 0:
            return 0.0
        num_cooperators_others_prev_round = np.sum(history.opponent_actions[-1, :])
        return num_cooperators_others_prev_round / num_others

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        if current_round_num == 0:
            self.current_state = 'PROBING'
            self.rounds_in_current_state = 1
            return Action.C
        if current_round_num == self.game_description.n_rounds - 1:
            return Action.D
        proportion_cooperators_others_prev_round = self._get_opponent_cooperation_proportion(history)
        while True:
            if self.current_state == 'PROBING':
                if self.rounds_in_current_state < self.initial_probe_rounds:
                    self.rounds_in_current_state += 1
                    return Action.C
                else:
                    self.current_state = 'EXPLOITING'
                    self.rounds_in_current_state = 0
                    continue
            elif self.current_state == 'EXPLOITING':
                if proportion_cooperators_others_prev_round > self.cooperation_threshold:
                    return Action.D
                elif proportion_cooperators_others_prev_round == 0:
                    return Action.D
                else:
                    self.current_state = 'RECOVERING'
                    self.rounds_in_current_state = 1
                    return Action.C
            elif self.current_state == 'RECOVERING':
                if self.rounds_in_current_state < self.recovery_effort_rounds:
                    self.rounds_in_current_state += 1
                    return Action.C
                else:
                    self.current_state = 'EXPLOITING'
                    self.rounds_in_current_state = 0
                    continue
            return Action.D

class Strategy_EXPLOITATIVE_377(BaseStrategy):
    """
    Opportunistic Piranha strategy for the N-Player Public Goods Game.

    This strategy embodies an aggressive, self-interested approach. It aims to
    maximize its own payoff by primarily defecting and only strategically
    cooperating to either create opportunities for future free-riding or prevent
    the complete collapse of the public good (which would lead to universally
    low payoffs). It defects in the very first and very last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        c_prev = int(history.my_actions[-1]) + sum(history.opponent_actions[-1, :])
        reseed_threshold = max(1, math.floor(self.n_players / 4))
        if c_prev < reseed_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_378(BaseStrategy):
    """
    The Opportunistic Investor strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by free-riding on others'
    contributions. It defaults to defection, only cooperating for a brief,
    strategic "investment" period if the overall level of cooperation falls
    below a critical threshold, threatening the viability of the public good.
    It always defects in the final round to ensure maximum personal gain.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.D_THRESHOLD = math.ceil(self.n_players / self.k_factor)
        self.INVESTMENT_ROUNDS = 2
        self.current_investment_countdown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if current_round_t == self.n_rounds:
            return Action.D
        if self.current_investment_countdown > 0:
            self.current_investment_countdown -= 1
            return Action.C
        elif state.round_number == 0:
            return Action.D
        else:
            previous_round_cooperators_opponents = np.sum(history.opponent_actions[-1, :])
            previous_round_my_action_C = history.my_actions[-1]
            C_prev = previous_round_cooperators_opponents + (1 if previous_round_my_action_C else 0)
            if C_prev < self.D_THRESHOLD:
                self.current_investment_countdown = self.INVESTMENT_ROUNDS - 1
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_379(BaseStrategy):
    """
    The Opportunistic Pacer strategy for the N-Player Public Goods Game.
    Prioritizes immediate individual gain through defection but adaptively cooperates
    when collective cooperation from others drops to a critical threshold,
    threatening future exploitation opportunities. It avoids being the sole cooperator
    and exploits when others are sufficiently cooperative.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.critical_coop_threshold = math.ceil((self.n_players - 1) / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        c_others_prev_round = np.sum(history.opponent_actions[-1, :])
        if c_others_prev_round == 0:
            return Action.D
        elif c_others_prev_round < self.critical_coop_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_380(BaseStrategy):
    """
    The Adaptive Opportunist strategy aims to maximize its own payoff by selectively defecting
    when the overall level of cooperation is sufficiently high, and selectively cooperating
    when the overall level of cooperation is low to encourage renewed cooperation. It also
    leverages backward induction for the final round.
    """
    EXPLOITATION_THRESHOLD = 0.5

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.n_rounds:
            return Action.D
        else:
            my_previous_action_is_C = history.my_actions[-1]
            opponent_previous_actions_is_C = history.opponent_actions[-1, :]
            cooperator_count_t_minus_1 = int(my_previous_action_is_C) + np.sum(opponent_previous_actions_is_C)
            proportion_cooperators_t_minus_1 = cooperator_count_t_minus_1 / self.n_players
            if proportion_cooperators_t_minus_1 > self.EXPLOITATION_THRESHOLD:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_381(BaseStrategy):
    """
    Grudging Reciprocation with Opportunistic Free-riding (GROF) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by defaulting to defection (free-riding).
    It only grudgingly cooperates when the public good contributions in the previous round
    fall below a critical COOPERATION_THRESHOLD, especially after the player themselves defected.
    This "re-investment" prevents the public good from collapsing entirely, ensuring future
    opportunities for exploitation. The strategy defects in the last round to maximize terminal payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.COOPERATION_THRESHOLD = max(2, math.floor(self.game_description.n_players / self.game_description.k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds_1_indexed = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds_1_indexed:
            return Action.D
        my_action_last_round = history.my_actions[-1]
        last_round_total_cooperators = np.sum(history.opponent_actions[-1, :]) + my_action_last_round
        if my_action_last_round is False:
            if last_round_total_cooperators < self.COOPERATION_THRESHOLD:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_382(BaseStrategy):
    """
    The "Ruthless Opportunist" strategy aims to maximize individual payoff in the
    N-Player Public Goods Game. It prioritizes personal gain through defection,
    but strategically cooperates to cultivate or preserve the public good for
    future exploitation, or to avoid being a "sucker" if others aren't contributing.
    It adapts to game parameters and observed opponent behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.my_previous_action: Action | None = None
        self.my_defection_streak: int = 0
        self.critical_mass_coop_needed: int = max(1, math.ceil(self.n / self.k) - 1)
        self.strong_coop_level: int = self.n - 1
        self.exploit_window_start_1_indexed: int = math.floor(self.r * 0.75)
        self.max_personal_defection_run: int = math.floor(self.r / 5)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        action: Action
        if current_round_1_indexed == 1:
            action = Action.C
        elif current_round_1_indexed == self.r:
            action = Action.D
        else:
            previous_round_opponent_actions_boolean = history.opponent_actions[current_round_0_indexed - 1, :]
            others_cooperated_count_prev = np.sum(previous_round_opponent_actions_boolean)
            if current_round_1_indexed >= self.exploit_window_start_1_indexed:
                action = Action.D
            elif others_cooperated_count_prev >= self.strong_coop_level:
                action = Action.D
            elif self.my_defection_streak >= self.max_personal_defection_run and others_cooperated_count_prev >= self.critical_mass_coop_needed:
                action = Action.C
            elif others_cooperated_count_prev < self.critical_mass_coop_needed:
                action = Action.D
            elif self.my_previous_action == Action.D and others_cooperated_count_prev >= self.critical_mass_coop_needed:
                action = Action.C
            else:
                action = Action.D
        self.my_previous_action = action
        if action == Action.D:
            self.my_defection_streak += 1
        else:
            self.my_defection_streak = 0
        return action

class Strategy_EXPLOITATIVE_383(BaseStrategy):
    """
    Strategic Initiator & Exploiter (SIE) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize the player's own payoff by primarily free-riding on the
    contributions of others. It is adaptive, making calculated "investments" in cooperation
    only when necessary to prevent the public good from collapsing, thereby ensuring future
    opportunities for exploitation. It also employs a hard-nosed approach to avoid being
    a "sucker" repeatedly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploitation_threshold = math.ceil((self.game_description.n_players - 1) / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            return Action.D
        if current_round == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_idx = current_round - 1
        my_action_was_C_prev = history.my_actions[prev_round_idx]
        num_opponent_cooperated_prev = np.sum(history.opponent_actions[prev_round_idx, :])
        C_prev = num_opponent_cooperated_prev + (1 if my_action_was_C_prev else 0)
        if C_prev == 0:
            if my_action_was_C_prev:
                return Action.D
            else:
                return Action.C
        else:
            C_others_prev = num_opponent_cooperated_prev
            if C_others_prev >= self.exploitation_threshold:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_384(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by free-riding on others' contributions.
    It strategically cooperates in the first round to "prime the pump" of cooperation.
    In intermediate rounds, it defects by default, but switches to cooperate if the
    proportion of cooperators among other players falls below a dynamically calculated
    threshold. This threshold is lower when the public good return (k/n) is higher,
    allowing for more exploitation when cooperation is robust. It always defects in the
    final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        others_cooperated_count = sum(history.opponent_actions[current_round_0_indexed - 1, :])
        p_others_t_minus_1 = float(others_cooperated_count) / (self.n_players - 1)
        Dynamic_Recovery_Threshold = 0.5 - self.k / self.n_players * 0.4
        if p_others_t_minus_1 <= Dynamic_Recovery_Threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_385(BaseStrategy):
    """
    The "Calculated Opportunist (CO)" strategy is designed to maximize its total payoff
    in the N-Player Public Goods Game by primarily defecting, while strategically
    employing limited cooperation to induce and maintain cooperation from other players.
    It leverages insights from game theory, particularly backward induction, and adaptive
    responses to opponent behavior to exploit the environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.initial_probe_rounds = max(1, min(3, self.r // 3))
        self.cooperation_inducement_threshold = 0.3
        self.reset_trigger_threshold = 2
        self.reset_cooperation_duration = 1
        self.final_defection_rounds = max(1, min(2, self.r - 1))
        self.consecutive_all_others_defected = 0
        self.in_reset_phase = False
        self.reset_phase_countdown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        others_cooperated_last_round = 0
        if current_round_0_indexed > 0:
            last_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
            others_cooperated_last_round = np.sum(last_round_opponent_actions)
        if current_round_1_indexed >= self.r - self.final_defection_rounds + 1:
            return Action.D
        if self.in_reset_phase:
            if self.reset_phase_countdown > 0:
                self.reset_phase_countdown -= 1
                return Action.C
            else:
                self.in_reset_phase = False
        if current_round_0_indexed > 0:
            if others_cooperated_last_round == 0:
                self.consecutive_all_others_defected += 1
                if self.consecutive_all_others_defected >= self.reset_trigger_threshold:
                    self.in_reset_phase = True
                    self.reset_phase_countdown = self.reset_cooperation_duration - 1
                    self.consecutive_all_others_defected = 0
                    return Action.C
            else:
                self.consecutive_all_others_defected = 0
        if current_round_1_indexed <= self.initial_probe_rounds:
            if current_round_1_indexed == 1:
                return Action.C
            else:
                required_cooperators_for_defection = math.floor((self.n - 1) * self.cooperation_inducement_threshold)
                if others_cooperated_last_round < required_cooperators_for_defection:
                    return Action.C
                else:
                    return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_386(BaseStrategy):
    """
    The "Adaptive Predator" prioritizes free-riding. It defaults to defection,
    contributing only when necessary to sustain a minimum level of public good provision
    from others, or to initially probe the environment. It employs a strong punishment
    mechanism against persistent non-cooperators.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.r_probe = 1
        self.trigger_coop_level = max(1, math.ceil((self.n_players - 1) / 2))
        self.forgive_rounds = 2
        self.grim_trigger_active = False
        self.low_coop_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if self.grim_trigger_active:
            return Action.D
        if current_round_1_indexed == self.r_probe:
            return Action.C
        c_others_prev_round = sum(history.opponent_actions[state.round_number - 1])
        if c_others_prev_round >= self.trigger_coop_level:
            self.low_coop_streak = 0
            return Action.D
        else:
            self.low_coop_streak += 1
            if self.low_coop_streak >= self.forgive_rounds:
                self.grim_trigger_active = True
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_387(BaseStrategy):
    """
    Calculated Free-Rider: An exploitative strategy for the N-Player Public Goods Game.
    It aims to maximize its own payoff by primarily free-riding on others' cooperation,
    strategically punishing widespread defection, and offering limited, calculated
    cooperation only when necessary to re-establish a sufficiently cooperative
    environment for future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.P_EXPLOIT_THRESHOLD = 0.5
        self.P_PUNISH_THRESHOLD = 0.2
        self.ROUNDS_TO_RESET_COOPERATION = 3
        self.consecutive_defections_by_self = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            self.consecutive_defections_by_self = 0
            return Action.C
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        previous_round_actions_others = history.opponent_actions[-1, :]
        num_others_cooperated_prev_round = np.sum(previous_round_actions_others)
        p_others_prev = num_others_cooperated_prev_round / (n_players - 1)
        action_for_this_round = Action.D
        if p_others_prev >= self.P_EXPLOIT_THRESHOLD:
            action_for_this_round = Action.D
            self.consecutive_defections_by_self += 1
        elif p_others_prev <= self.P_PUNISH_THRESHOLD:
            action_for_this_round = Action.D
            self.consecutive_defections_by_self += 1
        elif self.consecutive_defections_by_self >= self.ROUNDS_TO_RESET_COOPERATION:
            action_for_this_round = Action.C
            self.consecutive_defections_by_self = 0
        else:
            action_for_this_round = Action.D
            self.consecutive_defections_by_self += 1
        return action_for_this_round

class Strategy_EXPLOITATIVE_388(BaseStrategy):
    """
    The Opportunistic Coercer: An exploitative, adaptive, and robust strategy for
    the N-Player Public Goods Game. It begins with a cooperative probe, then
    switches between exploiting sufficient cooperation and using coercive
    cooperation/punishment to manage the pool of cooperators. It activates
    a grim trigger (permanent defection) if collective cooperation consistently fails.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self._cooperation_threshold_for_exploitation = self.n / 2.0
        self._grim_trigger_threshold_rounds = 3
        self._consecutive_low_cooperation_rounds: int = 0
        self._grim_trigger_active: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == 0:
            self._consecutive_low_cooperation_rounds = 0
            self._grim_trigger_active = False
            return Action.C
        if current_round == self.r - 1:
            return Action.D
        previous_round_my_action_was_C = history.my_actions[-1]
        previous_round_opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        C_t_minus_1 = previous_round_opponent_cooperators + (1 if previous_round_my_action_was_C else 0)
        my_action_in_previous_round = Action.C if previous_round_my_action_was_C else Action.D
        if C_t_minus_1 < self._cooperation_threshold_for_exploitation:
            self._consecutive_low_cooperation_rounds += 1
        else:
            self._consecutive_low_cooperation_rounds = 0
        if self._consecutive_low_cooperation_rounds >= self._grim_trigger_threshold_rounds:
            self._grim_trigger_active = True
        chosen_action: Action
        if self._grim_trigger_active:
            chosen_action = Action.D
        elif C_t_minus_1 >= self._cooperation_threshold_for_exploitation:
            chosen_action = Action.D
        elif my_action_in_previous_round == Action.D:
            chosen_action = Action.C
        else:
            chosen_action = Action.D
        return chosen_action

class Strategy_EXPLOITATIVE_389(BaseStrategy):
    """
    The "Adaptive Exploiter" strategy aims to induce cooperation in early rounds,
    then relentlessly exploit it. If cooperation levels drop too low to be profitable,
    it temporarily "re-seeds" the public good to encourage others to contribute again,
    before resuming exploitation. It will always defect in the final rounds due to
    backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.R_PROBE = 1
        self.R_ENDGAME = 2
        self.REINDUCTION_THRESHOLD_FACTOR = 0.33
        self.REINDUCTION_THRESHOLD = max(1, math.floor(self.n_players * self.REINDUCTION_THRESHOLD_FACTOR))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed < self.R_PROBE:
            return Action.C
        if current_round_0_indexed >= self.n_rounds - self.R_ENDGAME:
            return Action.D
        my_action_prev = history.my_actions[-1]
        num_other_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        if num_other_cooperators_prev < self.REINDUCTION_THRESHOLD and my_action_prev == Action.D:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_390(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy is designed to maximize its own payoff by
    dynamically adjusting its behavior based on the observed level of cooperation
    in the previous round. It embodies an exploitative mindset by prioritizing
    free-riding when cooperation is high, punishing non-cooperation when it's low,
    and strategically re-cooperating to sustain a beneficial public good. It is
    "adaptive" because it learns from the collective behavior and "free-rider"
    because it leans towards defecting when opportune.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.MIN_COOPERATION_TO_CONSIDER_COOPERATING = math.ceil(self.n / self.k)
        self.HIGH_COOPERATION_FOR_EXPLOITATION = self.n - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == 0:
            return Action.C
        if current_round_idx == self.r - 1:
            return Action.D
        my_last_action_is_C = history.my_actions[-1]
        opponent_cooperators_last_round = np.sum(history.opponent_actions[-1, :])
        cooperators_last_round = opponent_cooperators_last_round + (1 if my_last_action_is_C else 0)
        if cooperators_last_round < self.MIN_COOPERATION_TO_CONSIDER_COOPERATING:
            return Action.D
        if cooperators_last_round >= self.HIGH_COOPERATION_FOR_EXPLOITATION:
            return Action.D
        if my_last_action_is_C:
            my_share_of_public_good_if_I_C = self.k / self.n * cooperators_last_round
            if my_share_of_public_good_if_I_C < 1.0:
                return Action.D
            else:
                return Action.C
        elif cooperators_last_round > self.n / 2:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_391(BaseStrategy):
    """
    Opportunistic Free-Rider (OFR) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize personal payoff by primarily free-riding on others'
    cooperation. It strategically contributes to maintain a beneficial level of public good
    and ceases contribution if efforts are futile. It is designed to be exploitative,
    adaptive, and robust against unresponsive environments.

    Parameters:
    - PUNISHMENT_THRESHOLD: Minimum acceptable number of *other players* cooperating.
    - COOPERATION_TARGET: Ideal number of total cooperators (all others cooperate).
    - FORGIVENESS_LIMIT: Number of consecutive rounds of low cooperation before
      switching to permanent defection mode.

    State Variables:
    - c_i_prev_my_action: Stores player i's action in the previous round (0 for D, 1 for C).
    - consecutive_low_cooperation_rounds: Counts consecutive rounds where
      n_C_others_prev was below PUNISHMENT_THRESHOLD.
    - in_pure_defection_mode: True if the strategy has given up on fostering cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        self.PUNISHMENT_THRESHOLD = max(1, math.ceil((n - 1) / 2))
        self.COOPERATION_TARGET = n - 1
        self.FORGIVENESS_LIMIT = 3
        self.c_i_prev_my_action = 0
        self.consecutive_low_cooperation_rounds = 0
        self.in_pure_defection_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_number == 0:
            current_action = Action.D
            self.c_i_prev_my_action = 0
            return current_action
        elif current_round_number == total_rounds - 1:
            current_action = Action.D
            self.c_i_prev_my_action = 0
            return current_action
        else:
            n_C_others_prev = np.sum(history.opponent_actions[-1, :])
            n_C_prev = self.c_i_prev_my_action + n_C_others_prev
            if self.in_pure_defection_mode:
                if n_C_prev >= self.COOPERATION_TARGET:
                    self.in_pure_defection_mode = False
                    self.consecutive_low_cooperation_rounds = 0
            elif n_C_others_prev < self.PUNISHMENT_THRESHOLD:
                self.consecutive_low_cooperation_rounds += 1
                if self.consecutive_low_cooperation_rounds > self.FORGIVENESS_LIMIT:
                    self.in_pure_defection_mode = True
            else:
                self.consecutive_low_cooperation_rounds = 0
            if self.in_pure_defection_mode:
                current_action = Action.D
            elif n_C_prev >= self.COOPERATION_TARGET:
                current_action = Action.D
            elif n_C_others_prev >= self.PUNISHMENT_THRESHOLD:
                current_action = Action.D
            else:
                current_action = Action.C
            self.c_i_prev_my_action = 1 if current_action == Action.C else 0
            return current_action

class Strategy_EXPLOITATIVE_392(BaseStrategy):
    """
    The Pragmatic Free-Rider strategy for the N-Player Public Goods Game.
    Prioritizes personal gain by defaulting to defection, only cooperating
    strategically to "prime the pump" if overall cooperation is too low,
    and cutting losses if these efforts are not reciprocated.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action = Action.D
        self.cooperation_attempts_failed_series = 0
        self.cooldown_before_new_attempt = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round = state.round_number
        if current_round == 0:
            action_to_take = Action.D
            self.my_last_action = action_to_take
            return action_to_take
        if current_round == n_rounds - 1:
            action_to_take = Action.D
            self.my_last_action = action_to_take
            return action_to_take
        C_others_prev = np.sum(history.opponent_actions[-1, :])
        exploit_threshold_others = (n_players - 1) / 2
        low_cooperation_threshold_others = (n_players - 1) / 4
        max_failed_attempts = min(3, n_rounds // 4)
        max_cooldown_rounds = min(n_players // 2, 5)
        action_to_take = Action.D
        if C_others_prev >= exploit_threshold_others:
            action_to_take = Action.D
            self.cooperation_attempts_failed_series = 0
            self.cooldown_before_new_attempt = 0
        elif self.cooperation_attempts_failed_series >= max_failed_attempts or self.cooldown_before_new_attempt > 0:
            action_to_take = Action.D
            self.cooldown_before_new_attempt += 1
            if self.cooldown_before_new_attempt > max_cooldown_rounds:
                self.cooldown_before_new_attempt = 0
                self.cooperation_attempts_failed_series = 0
        else:
            action_to_take = Action.C
            if C_others_prev < low_cooperation_threshold_others:
                self.cooperation_attempts_failed_series += 1
            else:
                self.cooperation_attempts_failed_series = 0
            self.cooldown_before_new_attempt = 0
        self.my_last_action = action_to_take
        return action_to_take

class Strategy_EXPLOITATIVE_393(BaseStrategy):
    """
    The "Watchful Predator" strategy for the N-Player Public Goods Game.

    This strategy is fundamentally exploitative. It defaults to defection to maximize
    immediate payoff, constantly free-riding on the contributions of others. However,
    it is also adaptive: it makes a calculated, temporary sacrifice (cooperate) only
    when the overall public good is on the verge of collapse (indicated by
    N_C_prev <= COLLAPSE_THRESHOLD), threatening its own long-term payoff, and only
    if its previous such attempt wasn't a failure (i.e., it defected in the previous
    critically low round). It aims to extract the maximum value from other players'
    cooperation while minimizing its own contribution. It defects in the first and
    last rounds as special cases.
    """
    COLLAPSE_THRESHOLD: int = 1

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Watchful Predator strategy with game-specific parameters.

        Args:
            game_description: An object containing game parameters like
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the player's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number.
            history: An object containing historical actions and payoffs for
                     this player and opponents, or None for the first round.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round == total_rounds - 1:
            return Action.D
        if current_round == 0:
            return Action.D
        my_action_prev: bool = history.my_actions[-1]
        opponent_actions_prev: NDArray[np.bool_] = history.opponent_actions[-1, :]
        N_C_prev: int = int(np.sum(opponent_actions_prev)) + int(my_action_prev)
        if N_C_prev > self.COLLAPSE_THRESHOLD:
            return Action.D
        elif my_action_prev is True:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_394(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy is designed for the N-Player Public Goods Game.
    It aims to maximize its own payoff by exploiting the contributions of other players,
    while strategically contributing only when necessary to prevent a collapse of the
    public good, thus maintaining an environment for future exploitation.

    Core Principles:
    1. Probe for Cooperation: Cooperate in the first round to gauge others' willingness.
    2. Exploit When Viable: Defect in intermediate rounds if sufficient cooperation was observed previously.
    3. Re-seed Cooperation: Cooperate in intermediate rounds if cooperation levels drop too low,
       to prevent collapse and encourage future contributions.
    4. Endgame Defection: Always defect in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        n_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        k_factor = self.game_description.k
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == n_rounds - 1:
            return Action.D
        else:
            C_count_prev = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
            dynamic_threshold = math.ceil(n_players / k_factor)
            if C_count_prev >= dynamic_threshold:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_395(BaseStrategy):
    """
    Calculated Free-Rider Strategy for the N-Player Public Goods Game.
    This strategy aims to exploit cooperation from other players by initially
    cooperating to establish a public good, then defecting to free-ride.
    It includes mechanisms to re-bait cooperation if the public good diminishes
    and safeguards against endless unreciprocated cooperation.
    """
    MODE_EXPLORING = 'EXPLORING'
    MODE_EXPLOITING = 'EXPLOITING'

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = self.game_description.n_players
        self.r = self.game_description.n_rounds
        self.mode = self.MODE_EXPLORING
        self.exploring_rounds_count = 0
        self.EXPLOITATION_THRESHOLD = math.ceil(self.n / 2)
        self.RE_BAIT_THRESHOLD = math.floor(self.n / 4)
        self.MIN_ROUNDS_FOR_REBAIT = 3
        self.MAX_EXPLORING_ROUNDS_CONTINUOUS = max(1, min(math.floor(self.r / 4), self.n))
        self.last_round_idx = self.r - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        if current_round_idx == self.last_round_idx:
            self.mode = self.MODE_EXPLOITING
            return Action.D
        if current_round_idx > 0 and history is not None:
            prev_round_idx = current_round_idx - 1
            my_action_prev_round = history.my_actions[prev_round_idx]
            cooperators_prev_round = sum(history.opponent_actions[prev_round_idx, :])
            if my_action_prev_round:
                cooperators_prev_round += 1
            if my_action_prev_round:
                self.exploring_rounds_count += 1
            else:
                self.exploring_rounds_count = 0
            if self.mode == self.MODE_EXPLORING:
                if my_action_prev_round and cooperators_prev_round >= self.EXPLOITATION_THRESHOLD:
                    self.mode = self.MODE_EXPLOITING
                    self.exploring_rounds_count = 0
                elif self.exploring_rounds_count >= self.MAX_EXPLORING_ROUNDS_CONTINUOUS:
                    self.mode = self.MODE_EXPLOITING
                    self.exploring_rounds_count = 0
            elif self.mode == self.MODE_EXPLOITING:
                if not my_action_prev_round and cooperators_prev_round <= self.RE_BAIT_THRESHOLD:
                    rounds_remaining = self.r - (current_round_idx + 1)
                    if rounds_remaining >= self.MIN_ROUNDS_FOR_REBAIT:
                        self.mode = self.MODE_EXPLORING
        if self.mode == self.MODE_EXPLORING:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_396(BaseStrategy):
    """
    The Calculated Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by primarily free-riding on
    the contributions of others. It is adaptive to opponent behavior and robust
    across various game parameters. It never cooperates out of altruism or
    simple reciprocity, but purely to maintain an environment profitable for
    exploitation.

    Core Principle: Prioritize defection, only cooperating as a calculated,
    strategic investment to prevent the collapse of overall cooperation or
    to re-seed it when free-riding opportunities become too scarce.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._rounds_forced_cooperation: int = 0
        self._COOP_REBOOT_THRESHOLD: int = max(1, math.floor(self.game_description.n_players / self.game_description.k))
        self._FORCED_COOP_DURATION: int = max(2, math.floor(self.game_description.n_rounds / 4))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            self._rounds_forced_cooperation = 0
            return Action.C
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if self._rounds_forced_cooperation > 0:
            self._rounds_forced_cooperation -= 1
            return Action.C
        previous_round_index = current_round_0_indexed - 1
        n_c_prev = np.sum(history.opponent_actions[previous_round_index, :])
        if history.my_actions[previous_round_index]:
            n_c_prev += 1
        if n_c_prev <= self._COOP_REBOOT_THRESHOLD:
            self._rounds_forced_cooperation = self._FORCED_COOP_DURATION
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_397(BaseStrategy):
    """
    The Adaptive Opportunist strategy for the N-Player Public Goods Game.

    This strategy balances consistent defection with a calculated, short-term cooperative "provocation" phase.
    It aims to maximize its own payoff by generally free-riding on the cooperation of others,
    while intelligently "investing" in cooperation only when necessary to create an environment
    more conducive to future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.provocation_cooperation_rate_threshold = self.game_description.k / self.game_description.n_players
        self.provocation_round_active = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        num_players = self.game_description.n_players
        if current_round_1_indexed == total_rounds:
            action = Action.D
            self.provocation_round_active = False
            return action
        if current_round_1_indexed == 1:
            action = Action.D
            self.provocation_round_active = False
            return action
        num_other_cooperators_prev_round = int(np.sum(history.opponent_actions[-1, :]))
        num_other_players = num_players - 1
        cooperation_rate_others_prev_round = num_other_cooperators_prev_round / num_other_players
        if cooperation_rate_others_prev_round >= self.provocation_cooperation_rate_threshold:
            action = Action.D
            self.provocation_round_active = False
        elif num_other_cooperators_prev_round == 0:
            action = Action.D
            self.provocation_round_active = False
        elif self.provocation_round_active:
            action = Action.D
            self.provocation_round_active = False
        else:
            action = Action.C
            self.provocation_round_active = True
        return action

class Strategy_EXPLOITATIVE_398(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game,
    dubbed "Adaptive Free-Rider".

    This strategy aims to maximize its own payoff by primarily free-riding on the
    cooperation of others. It adaptively adjusts its behavior based on the
    observed cooperation levels of other players, using strategic cooperation
    as an investment to "prime the pump" when overall cooperation is too low,
    and ruthlessly defects in the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.EXPLORATION_ROUNDS = 1
        self.ENDGAME_HORIZON = 1
        self.T = max(1, math.ceil((self.k - 1) * self.n / self.k))
        self.T = min(self.T, self.n - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed < self.EXPLORATION_ROUNDS:
            return Action.C
        if current_round_0_indexed >= self.r - self.ENDGAME_HORIZON:
            return Action.D
        C_others_prev = np.sum(history.opponent_actions[-1, :])
        if C_others_prev >= self.T:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_399(BaseStrategy):
    """
    The Opportunistic Shepherd strategy is designed to be exploitative, adaptive, and robust
    in the N-Player Public Goods Game. It aims to maximize its long-term payoff by free-riding
    on the cooperation of others, while strategically contributing only when necessary to
    prevent the collapse of the "public good" and ensure future opportunities for exploitation.
    It probes with cooperation initially, defects ruthlessly in the final round, and in middle
    rounds, it dynamically adjusts its behavior based on the recent cooperation rate of other
    players and its own defection streak, balancing free-riding with re-seeding cooperation
    to maintain the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.M = 3
        self.cooperation_floor = 0.3
        self.free_ride_threshold = 0.6
        self.defection_streak_limit = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_past_rounds = state.round_number
        num_rounds_to_consider = min(self.M, num_past_rounds)
        recent_other_cooperation_rate = 0.0
        if num_rounds_to_consider > 0:
            relevant_opponent_actions = history.opponent_actions[-num_rounds_to_consider:, :]
            total_other_cooperators = np.sum(relevant_opponent_actions)
            total_possible_cooperators = num_rounds_to_consider * (self.game_description.n_players - 1)
            if total_possible_cooperators > 0:
                recent_other_cooperation_rate = total_other_cooperators / total_possible_cooperators
        my_defection_streak = 0
        for action_bool in reversed(history.my_actions):
            if not action_bool:
                my_defection_streak += 1
            else:
                break
        action_to_take = Action.D
        current_round_1_indexed = state.round_number + 1
        rounds_remaining = self.game_description.n_rounds - current_round_1_indexed
        if recent_other_cooperation_rate < self.cooperation_floor:
            if rounds_remaining > num_rounds_to_consider:
                action_to_take = Action.C
            else:
                action_to_take = Action.D
        elif recent_other_cooperation_rate >= self.free_ride_threshold:
            action_to_take = Action.D
        elif my_defection_streak >= self.defection_streak_limit:
            action_to_take = Action.C
        else:
            action_to_take = Action.D
        return action_to_take

class Strategy_EXPLOITATIVE_400(BaseStrategy):
    """
    The Opportunistic Leech (OL) strategy for the N-Player Public Goods Game.
    This strategy aims to maximize personal payoff by predominantly defecting
    and free-riding, while strategically using rare cooperative "probes" to
    encourage and then exploit further cooperation from other players.

    Key principles:
    - Default to Defect (exploit)
    - Cooperate once in the first round to "probe"
    - If others cooperate enough (above a threshold), continue defecting
    - If others defect too much, send another "probe" (cooperate) after a delay
    - Always defect in the final 'endgame_rounds' due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold: float = 0.3
        self.reprobe_interval: int = 4
        self.endgame_rounds: int = 2
        self.my_defection_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if current_round >= n_rounds - self.endgame_rounds + 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        num_other_cooperators = sum(history.opponent_actions[-1, :])
        avg_cooperation_rate_others = num_other_cooperators / (n_players - 1)
        if avg_cooperation_rate_others > self.cooperation_threshold:
            self.my_defection_streak = 0
            return Action.D
        else:
            self.my_defection_streak += 1
            if self.my_defection_streak >= self.reprobe_interval and current_round < n_rounds - self.endgame_rounds:
                self.my_defection_streak = 0
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_401(BaseStrategy):
    """
    The "Cycling Opportunist" strategy for the N-Player Public Goods Game.

    This strategy is designed to test for cooperative tendencies among opponents,
    exploit them when they appear, and then attempt to re-establish cooperation
    if it collapses, always prioritizing self-interest and anticipating the end-game.

    Core Principles:
    - Initial Probe: Start by cooperating to "seed" cooperation.
    - Aggressive Exploitation: Once sufficient cooperation is detected, switch to defecting.
    - Adaptive Restart: If cooperation from others diminishes too much, attempt to "reset"
      the cooperative environment by cooperating again.
    - End-game Rationality: Always defect in the final rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.DEFAULT_EXPLORATION_ROUNDS = 2
        self.DEFAULT_ENDGAME_ROUNDS = 2
        self.COOPERATION_THRESHOLD_FOR_EXPLOIT = 0.5
        self.COOPERATION_THRESHOLD_FOR_RESTART = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        total_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        effective_exploration_rounds = self.DEFAULT_EXPLORATION_ROUNDS
        effective_endgame_rounds = self.DEFAULT_ENDGAME_ROUNDS
        if total_rounds <= self.DEFAULT_EXPLORATION_ROUNDS + self.DEFAULT_ENDGAME_ROUNDS:
            if total_rounds == 2:
                effective_exploration_rounds = 1
                effective_endgame_rounds = 1
            elif total_rounds == 3:
                effective_exploration_rounds = 1
                effective_endgame_rounds = 2
        my_last_action = Action.C
        others_cooperated_last_round_count = 0
        others_coop_proportion = 0.0
        if current_round_0_indexed > 0:
            last_round_idx = current_round_0_indexed - 1
            my_last_action_bool = history.my_actions[last_round_idx]
            my_last_action = Action.C if my_last_action_bool else Action.D
            others_cooperated_last_round_count = np.sum(history.opponent_actions[last_round_idx, :])
            if n_players > 1:
                others_coop_proportion = float(others_cooperated_last_round_count) / (n_players - 1)
            else:
                others_coop_proportion = 0.0
        if current_round_1_indexed >= total_rounds - effective_endgame_rounds + 1:
            return Action.D
        if current_round_1_indexed <= effective_exploration_rounds:
            return Action.C
        if others_coop_proportion >= self.COOPERATION_THRESHOLD_FOR_EXPLOIT:
            return Action.D
        if others_coop_proportion < self.COOPERATION_THRESHOLD_FOR_RESTART:
            if my_last_action == Action.D:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_402(BaseStrategy):
    """
    The Opportunist strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by being exploitative, adaptive
    to opponent behaviors, and robust across different game parameters.
    It primarily defects to exploit public good provision, only cooperates
    briefly to "re-seed" cooperation when it's critically low, and always
    defects in the final stages of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.coop_inducement_threshold = 0.33
        self.reseeding_attempts_limit = 1
        self.endgame_start_ratio = 0.25
        self.my_last_action: Action | None = None
        self.consecutive_coop_for_reseeding = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        action_to_take = Action.D
        if current_round_number == 0:
            action_to_take = Action.C
        elif current_round_number == self.n_rounds - 1:
            action_to_take = Action.D
            self.consecutive_coop_for_reseeding = 0
        elif current_round_number + 1 >= self.n_rounds * (1 - self.endgame_start_ratio):
            action_to_take = Action.D
            self.consecutive_coop_for_reseeding = 0
        else:
            num_other_cooperators = sum(history.opponent_actions[-1, :])
            proportion_other_cooperators = num_other_cooperators / (self.n_players - 1)
            if proportion_other_cooperators < self.coop_inducement_threshold:
                if self.consecutive_coop_for_reseeding < self.reseeding_attempts_limit:
                    action_to_take = Action.C
                    self.consecutive_coop_for_reseeding += 1
                else:
                    action_to_take = Action.D
                    self.consecutive_coop_for_reseeding = 0
            else:
                action_to_take = Action.D
                self.consecutive_coop_for_reseeding = 0
        self.my_last_action = action_to_take
        return action_to_take

class Strategy_EXPLOITATIVE_403(BaseStrategy):
    """
    Adaptive Exploiter 2.0 is an exploitative strategy for the N-Player Public Goods Game.
    It aims to maximize individual payoff by free-riding on others' cooperation when the
    public good is robust, and strategically cooperating when cooperation is low (but not
    absent) to prevent collapse, viewing it as an investment for future exploitation.
    It accounts for first and last round specific behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the strategy with game parameters and sets up internal state and thresholds.
        """
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.critical_cooperation_threshold = math.ceil(self.n / self.k)
        self.min_others_cooperated_for_investment = 1
        self.my_action_in_previous_round: Action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round based on the
        strategy's rules, game state, and past history.

        Args:
            state: Current game state, including the 0-indexed round number.
            history: PlayerHistory object containing past actions and payoffs, or None for round 0.

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        current_decision: Action | None = None
        if current_round_0_indexed == 0:
            current_decision = Action.C
        elif current_round_0_indexed == self.r - 1:
            current_decision = Action.D
        else:
            M_other_prev = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
            my_prev_action_was_C = self.my_action_in_previous_round == Action.C
            M_prev = M_other_prev + (1 if my_prev_action_was_C else 0)
            if M_other_prev < self.min_others_cooperated_for_investment:
                current_decision = Action.D
            elif M_prev >= self.critical_cooperation_threshold:
                current_decision = Action.D
            else:
                current_decision = Action.C
        self.my_action_in_previous_round = current_decision
        return current_decision

class Strategy_EXPLOITATIVE_404(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the N-Player Public Goods Game.
    This strategy embodies an exploitative mindset by prioritizing defection to
    maximize individual gain. It is adaptive and robust enough to strategically
    contribute when necessary to prevent the collapse of the public good, thus
    ensuring continued opportunities for exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialises the strategy with game parameters and pre-calculates
        the cooperation_floor.

        Args:
            game_description (PublicGoodsDescription): An object containing
                                                       n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.cooperation_floor = max(1, math.floor(self.game_description.n_players / self.game_description.k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the player's action for the current round based on the
        Adaptive Free-Rider strategy.

        Args:
            state (GameState): Contains the current round number (0-indexed).
            history (None | PlayerHistory): Contains records of past actions and payoffs.
                                            Will be None for round 0.

        Returns:
            Action: Action.C for Cooperate, Action.D for Defect.
        """
        current_round_idx = state.round_number
        n_rounds = self.game_description.n_rounds
        if current_round_idx == 0:
            return Action.D
        if current_round_idx == n_rounds - 1:
            return Action.D
        opponent_actions_last_round = history.opponent_actions[current_round_idx - 1, :]
        num_opponent_cooperators_last_round = np.sum(opponent_actions_last_round)
        my_action_last_round = history.my_actions[current_round_idx - 1]
        num_my_cooperators_last_round = 1 if my_action_last_round else 0
        num_cooperators_last_round = num_opponent_cooperators_last_round + num_my_cooperators_last_round
        if num_cooperators_last_round < self.cooperation_floor:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_405(BaseStrategy):
    """
    The "Calculated Predator" strategy for the N-Player Public Goods Game.

    This strategy aims to be highly exploitative by leveraging the dominant strategy
    of defecting, while adaptively testing for opportunities to encourage and then
    free-ride on others' cooperation. It minimizes risk by making a single, early
    investment in cooperation and then consistently pursues maximum individual gain.

    Decision Rules:
    1.  Round 1 (state.round_number == 0): Plays Cooperate (C).
        This is a calculated, low-risk "probe" to signal potential for cooperation
        and observe if other players reciprocate or are naive enough to cooperate.
        The goal is to potentially "seed" future cooperation from which the strategy
        can then benefit.
    2.  Rounds 2 to r (state.round_number > 0): Plays Defect (D).
        From the second round onwards, the strategy consistently plays its dominant
        strategy (Defect). This maximizes individual gain by free-riding on any
        contributions made by others (whether induced by the initial probe or from
        other players' inherent strategies). This also ensures robustness against
        rational opponents and reciprocal strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_406(BaseStrategy):
    """
    The Iron Free-Rider strategy for the N-Player Public Goods Game.

    This strategy always defects because, given the game parameters (1 < k < n),
    defecting is the strictly dominant strategy for individual payoff maximization
    in every single round. It maximizes the player's own payoff by free-riding
    on any contributions made by other players. This strategy is robust and
    unconditional, as the optimal individual action does not change based on
    observed history or opponent actions within this specific game structure.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        return Action.D

class Strategy_EXPLOITATIVE_407(BaseStrategy):
    """
    The Adaptable Predator (TAP) strategy for the N-Player Public Goods Game.
    Aims to maximize its own payoff by primarily free-riding on the contributions of others.
    It defaults to defection but will strategically and temporarily cooperate ("bait" or "reset")
    under specific conditions to re-stimulate cooperation among other players,
    creating a more profitable environment for subsequent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.reset_rounds_remaining_threshold = max(3, self.game_description.n_players // 4)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number == 0:
            return Action.D
        if current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        my_prev_action_was_C = history.my_actions[-1]
        C_prev_opponents = np.sum(history.opponent_actions[-1, :])
        C_prev_total = C_prev_opponents + (1 if my_prev_action_was_C else 0)
        action_for_this_round = Action.D
        if my_prev_action_was_C:
            action_for_this_round = Action.D
        else:
            rounds_remaining_after_this_round = self.game_description.n_rounds - current_round_1_indexed
            if C_prev_total > 0 and C_prev_total < self.game_description.k and (rounds_remaining_after_this_round >= self.reset_rounds_remaining_threshold):
                action_for_this_round = Action.C
        return action_for_this_round

class Strategy_EXPLOITATIVE_408(BaseStrategy):
    """
    The Opportunistic Baiter (OB) strategy is designed to be highly exploitative in the N-Player Public Goods Game.
    It prioritizes free-riding on others' cooperation, uses strategic temporary cooperation ("baiting")
    to re-introduce cooperation when it's low, and cuts losses if baiting consistently fails.
    It adapts its behavior based on observed cooperation levels and maintains an internal state
    to track baiting attempts and success.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.my_last_action_was_bait: bool = False
        self.failed_bait_attempts: int = 0
        self.Cooperation_Threshold_Exploit: float = 0.5
        self.Cooperation_Threshold_Bait: float = 0.2
        self.Max_Failed_Bait_Attempts: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r - 1:
            self.my_last_action_was_bait = False
            self.failed_bait_attempts = 0
            return Action.D
        elif current_round == 0:
            self.my_last_action_was_bait = True
            self.failed_bait_attempts = 0
            return Action.C
        num_others_cooperated_prev_round = np.sum(history.opponent_actions[-1, :])
        num_other_players = self.n - 1
        if num_other_players == 0:
            avg_cooperation_others_prev_round = 0.0
        else:
            avg_cooperation_others_prev_round = num_others_cooperated_prev_round / num_other_players
        if self.my_last_action_was_bait:
            if avg_cooperation_others_prev_round < self.Cooperation_Threshold_Exploit:
                self.failed_bait_attempts += 1
            else:
                self.failed_bait_attempts = 0
        self.my_last_action_was_bait = False
        if avg_cooperation_others_prev_round >= self.Cooperation_Threshold_Exploit:
            return Action.D
        elif avg_cooperation_others_prev_round < self.Cooperation_Threshold_Bait:
            if self.failed_bait_attempts < self.Max_Failed_Bait_Attempts:
                self.my_last_action_was_bait = True
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_409(BaseStrategy):
    """
    The Adaptive Exploiter (AX) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by opportunistically free-riding on others'
    contributions while also strategically investing (cooperating) to maintain a sufficient
    level of public good for future exploitation. It is designed to be adaptive to the
    current cooperation levels of other players and leverages end-game rationality.

    Key principles:
    - Initial Goodwill: Cooperate in the first round to "prime the pump".
    - Aggressive Free-Riding: Defect when others are cooperating strongly.
    - Strategic Investment: Cooperate when others are moderately cooperative to prevent collapse.
    - Punishment/Retreat: Defect when others are cooperating weakly to avoid being a "sucker".
    - End-Game Rationality: Defect in the final rounds due to lack of future repercussions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n_players_other = self.game_description.n_players - 1
        self.COOPERATION_TARGET_THRESHOLD = math.floor(n_players_other * 0.75)
        self.MINIMUM_COOPERATION_THRESHOLD = math.floor(n_players_other * 0.4)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if state.round_number == 0:
            return Action.C
        if state.round_number == total_rounds - 1:
            return Action.D
        c_others_prev = np.sum(history.opponent_actions[-1, :])
        if c_others_prev >= self.COOPERATION_TARGET_THRESHOLD:
            return Action.D
        if state.round_number == total_rounds - 2:
            return Action.D
        if c_others_prev >= self.MINIMUM_COOPERATION_THRESHOLD:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_410(BaseStrategy):
    """
    The "Adaptive Free-Rider" (AFR) strategy is designed to be highly exploitative
    in the N-Player Public Goods Game. Its core principle is to free-ride as much
    as possible, only contributing to the public good when necessary to prevent
    its complete collapse, thereby preserving future free-riding opportunities.
    It incorporates a learning mechanism to avoid being indefinitely exploited itself.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_pump = max(1, math.floor(self.game_description.k / 2))
        self.MaxFailedPumps = 1
        self.my_last_action_t_minus_1 = Action.D
        self.rounds_since_last_successful_pump = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        current_action = Action.D
        if current_round_0_indexed == 0:
            current_action = Action.D
        elif current_round_0_indexed == total_rounds - 1:
            current_action = Action.D
        else:
            my_prev_action_bool = history.my_actions[-1]
            opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
            C_total_t_minus_1 = int(my_prev_action_bool) + int(opponent_cooperators_prev_round)
            if self.my_last_action_t_minus_1 == Action.C:
                if C_total_t_minus_1 <= self.T_pump:
                    self.rounds_since_last_successful_pump += 1
                else:
                    self.rounds_since_last_successful_pump = 0
            else:
                self.rounds_since_last_successful_pump = 0
            if self.rounds_since_last_successful_pump >= self.MaxFailedPumps:
                current_action = Action.D
            elif C_total_t_minus_1 <= self.T_pump:
                current_action = Action.C
            else:
                current_action = Action.D
        self.my_last_action_t_minus_1 = current_action
        return current_action

class Strategy_EXPLOITATIVE_411(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by balancing initial cooperation,
    strategic free-riding, and "nudging" cooperation back up when it declines.
    It is designed to be exploitative, adaptive, and robust.

    The strategy's logic is as follows:
    1.  **First Round**: Always Cooperate (C) to probe and encourage initial cooperation.
    2.  **Last Round**: Always Defect (D) as there are no future consequences.
    3.  **Intermediate Rounds**:
        - Calculate `Nudge_Threshold = floor(k)`.
        - If the total number of cooperators in the *previous* round was greater than or equal to `Nudge_Threshold`, then Defect (D) to free-ride.
        - Otherwise (if cooperation was too low), Cooperate (C) to "nudge" collective cooperation back up.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.nudge_threshold = math.floor(self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        previous_round_cooperators_count = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if previous_round_cooperators_count >= self.nudge_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_412(BaseStrategy):
    """
    Opportunistic Revivalist is an exploitative strategy for the N-Player Public Goods Game.
    It defaults to defection to free-ride but makes calculated, minimal contributions
    to temporarily revive a dying public good, thus creating new free-riding opportunities.
    It also ruthlessly exploits the endgame effect by defecting in final rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.initial_probe_rounds = 2
        self.endgame_horizon = 2
        self.min_cooperators_for_revival = max(1, self.n_players // 4)
        self.defection_streak_for_revival = 3
        self.revival_rounds_duration = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        my_previous_round_action = Action.D
        total_cooperators_in_prev_round = 0
        my_consecutive_defections = 0
        if current_round_0_indexed > 0:
            my_previous_round_action = Action.C if history.my_actions[-1] else Action.D
            total_cooperators_in_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
            for i in range(current_round_0_indexed - 1, -1, -1):
                if history.my_actions[i] == False:
                    my_consecutive_defections += 1
                else:
                    break
        if current_round_0_indexed >= self.n_rounds - self.endgame_horizon:
            return Action.D
        if current_round_0_indexed < self.initial_probe_rounds:
            return Action.D
        if total_cooperators_in_prev_round < self.min_cooperators_for_revival:
            if my_previous_round_action == Action.D and my_consecutive_defections >= self.defection_streak_for_revival:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_413(BaseStrategy):
    """
    The Opportunistic Lurer strategy for the N-Player Public Goods Game.

    Core Philosophy:
    The primary goal is to maximize individual payoff. While defection is
    immediately more profitable, this strategy strategically cooperates ("lures")
    to encourage others to contribute, creating a public good pool to free-ride on.
    It's a calculated risk (strategic investment) for higher long-term exploitation.

    State Variables:
    - lure_attempts_remaining: Counts how many more times the strategy will "lure"
      by cooperating after observing universal defection from others.
    - LURE_LIMIT: A fixed parameter (recommended 2) defining the maximum number
      of strategic cooperation investments when no one else is cooperating.
    """
    LURE_LIMIT = 2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.lure_attempts_remaining = self.LURE_LIMIT

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_opponent_cooperators = sum(history.opponent_actions[current_round_0_indexed - 1, :])
        if previous_round_opponent_cooperators > 0:
            self.lure_attempts_remaining = self.LURE_LIMIT
            return Action.D
        elif self.lure_attempts_remaining > 0:
            self.lure_attempts_remaining -= 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_414(BaseStrategy):
    """
    The Adaptive Freeloader strategy for the N-Player Public Goods Game.
    This strategy aims to maximize individual cumulative payoff by primarily defecting,
    but strategically cooperating (re-seeding) if cooperation levels among others
    fall too low, to encourage future public good creation for exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        super().__init__(game_description)
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.my_consecutive_defections: int = 0
        self.last_round_was_reseed_cooperation: bool = False
        self.COOPERATION_FLOOR_THRESHOLD: int = max(2, math.floor(self.n / 5))
        self.MIN_ROUNDS_FOR_RESEED: int = 4
        self.DEFECTION_STREAK_FOR_RESEED: int = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            self.my_consecutive_defections = 1
            self.last_round_was_reseed_cooperation = False
            return Action.D
        if current_round_0_indexed == self.r - 1:
            return Action.D
        other_cooperators_prev: int = np.sum(history.opponent_actions[-1, :])
        rounds_remaining: int = self.r - current_round_0_indexed
        next_action = Action.D
        condition_A_low_cooperation = other_cooperators_prev <= self.COOPERATION_FLOOR_THRESHOLD
        condition_B_enough_time_left = rounds_remaining >= self.MIN_ROUNDS_FOR_RESEED
        condition_C_sufficient_defection_streak = self.my_consecutive_defections >= self.DEFECTION_STREAK_FOR_RESEED
        condition_D_not_just_reseeded = self.last_round_was_reseed_cooperation == False
        if condition_A_low_cooperation and condition_B_enough_time_left and condition_C_sufficient_defection_streak and condition_D_not_just_reseeded:
            next_action = Action.C
            self.my_consecutive_defections = 0
            self.last_round_was_reseed_cooperation = True
        else:
            self.my_consecutive_defections += 1
            self.last_round_was_reseed_cooperation = False
        return next_action

class Strategy_EXPLOITATIVE_415(BaseStrategy):
    """
    The "Parasite" strategy aims to maximize personal gain by primarily free-riding
    on the contributions of others in the N-Player Public Goods Game. It views
    other players' cooperation as a resource to be exploited. It only contributes
    (cooperates) when the collective good is severely threatened, acting as a
    calculated "investment" to prevent total collapse and ensure future free-riding
    opportunities. It is ruthless, starts with defection, and ends with defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.critical_C_threshold = math.ceil(self.n_players / self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_0_indexed == 0:
            return Action.D
        elif current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        else:
            total_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
            my_action_prev_was_cooperate = history.my_actions[-1]
            C_prev = total_opponent_cooperators_prev + (1 if my_action_prev_was_cooperate else 0)
            if C_prev < self.critical_C_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_416(BaseStrategy):
    """
    The Adaptive Opportunist strategy aims to foster an environment of cooperation among other players,
    then selectively defect to maximize individual gain, while also punishing widespread defection
    to steer the game back towards a state where exploitation is possible.

    It adapts its behavior based on the observed cooperation level of other players in the previous round,
    using internal parameters to decide when to punish, when to cooperate to maintain cooperation,
    and when to exploit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.MIN_COOP_OTHERS = max(1, math.floor((self.n_players - 1) * 0.5))
        self.HIGH_COOP_OTHERS = self.n_players - 1
        self.PUNISHMENT_DURATION = 1
        self.EXPLOIT_FREQUENCY = 3
        self.state = 'cooperating'
        self.punish_countdown = 0
        self.exploit_countdown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.state = 'cooperating'
            return Action.C
        if state.round_number == self.n_rounds - 1:
            self.state = 'defected_endgame'
            return Action.D
        c_others_prev_round = sum(history.opponent_actions[-1, :])
        if self.punish_countdown > 0:
            self.punish_countdown -= 1
            if self.punish_countdown == 0:
                self.state = 'cooperating'
            return Action.D
        if c_others_prev_round < self.MIN_COOP_OTHERS:
            self.state = 'punishing'
            self.punish_countdown = self.PUNISHMENT_DURATION - 1
            self.exploit_countdown = 0
            return Action.D
        if c_others_prev_round >= self.HIGH_COOP_OTHERS:
            self.exploit_countdown += 1
            if self.exploit_countdown >= self.EXPLOIT_FREQUENCY:
                self.state = 'exploiting'
                self.exploit_countdown = 0
                return Action.D
            else:
                self.state = 'cooperating'
                return Action.C
        self.state = 'cooperating'
        self.exploit_countdown = 0
        return Action.C

class Strategy_EXPLOITATIVE_417(BaseStrategy):
    """
    Adaptive Free-Rider with Re-Seeding (AFR-R) strategy for N-Player Public Goods Game.
    This strategy aims to maximize individual payoff by consistently free-riding on the
    contributions of others, only temporarily "re-seeding" cooperation when the public
    good contributions drop to a level that undermines its own potential for exploitation.

    The strategy operates as follows:
    1.  **First Round**: Always Defect (D) to maximize immediate gain.
    2.  **Last Round**: Always Defect (D) due to backward induction.
    3.  **Intermediate Rounds**:
        *   If currently in a "re-seeding" phase (triggered by low cooperation), continue to Cooperate (C)
            for the committed duration, then decrement the internal counter.
        *   If not in a re-seeding phase, it observes the proportion of cooperators in the previous round.
            *   If this proportion falls below a `Defection Tolerance Threshold (DTT)`, it initiates a
                re-seeding phase by Cooperating (C) and commits to cooperate for `P_duration` rounds.
            *   Otherwise, if the cooperation level is sufficient, it Defaults (D) to free-ride.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.DTT = 0.25
        self.P_duration = 2
        self.punishment_rounds_left = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        num_players = self.game_description.n_players
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if self.punishment_rounds_left > 0:
            self.punishment_rounds_left -= 1
            return Action.C
        else:
            cooperators_prev_round = np.sum(history.opponent_actions[current_round_0_indexed - 1, :]) + (1 if history.my_actions[current_round_0_indexed - 1] else 0)
            avg_cooperation_ratio = cooperators_prev_round / num_players
            if avg_cooperation_ratio < self.DTT:
                self.punishment_rounds_left = self.P_duration - 1
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_418(BaseStrategy):
    """
    The Opportunistic Exploiter strategy aims to maximize personal payoff by defaulting to defection.
    It strategically cooperates only when collective cooperation among others (excluding itself) falls
    below a predefined Cooperation Maintenance Threshold (CMT), entering a short-term "repair mode"
    to encourage others to restore their cooperation levels. This ensures there remains a public good
    to free-ride on in subsequent rounds. The strategy defects unconditionally in the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.debt_duration = 2
        self.cooperation_threshold_percentage = 0.6
        self.CMT = max(1, math.ceil((self.game_description.n_players - 1) * self.cooperation_threshold_percentage))
        self.my_cooperation_debt_counter = 0
        self.my_previous_action = Action.D
        self.previous_total_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.D
        if current_round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_action_prev_round_bool = history.my_actions[-1]
        self.my_previous_action = Action.C if my_action_prev_round_bool else Action.D
        self.previous_total_cooperators = int(my_action_prev_round_bool) + np.sum(history.opponent_actions[-1, :])
        c_others_prev = self.previous_total_cooperators - (1 if self.my_previous_action == Action.C else 0)
        c_others_prev = max(0, c_others_prev)
        if self.my_cooperation_debt_counter > 0:
            action_to_take = Action.C
            self.my_cooperation_debt_counter -= 1
        elif c_others_prev >= self.CMT:
            action_to_take = Action.D
        else:
            action_to_take = Action.C
            self.my_cooperation_debt_counter = self.debt_duration - 1
        return action_to_take

class Strategy_EXPLOITATIVE_419(BaseStrategy):
    """
    The "Adaptive Exploiter" strategy aims to maximize individual payoff in the
    N-Player Public Goods Game by strategically free-riding on the cooperation
    of others while investing minimal effort to prevent the complete collapse
    of collective contributions. It adapts to the observed level of cooperation
    in the group, aiming to maintain a fertile ground for exploitation.
    It prioritizes immediate defection when group cooperation is sufficiently
    high, strategically cooperates to encourage a return to higher cooperation
    levels when it's faltering, and always defects in the last round and when
    no one else is cooperating.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.n_over_k_threshold = self.n_players / self.k_factor

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        my_prev_action_cooperated = int(history.my_actions[-1])
        opponent_prev_cooperators = np.sum(history.opponent_actions[-1, :])
        C_prev = my_prev_action_cooperated + opponent_prev_cooperators
        if C_prev == 0:
            return Action.D
        elif C_prev >= self.n_over_k_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_420(BaseStrategy):
    """
    The Adaptive Free-Rider strategy aims to maximize personal total payoff by primarily defecting
    (free-riding) when others are contributing. It strategically cooperates only to incentivize
    or "reset" cooperation levels when they drop too low, thereby creating a fertile ground
    for future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.r - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        c_others_prev = sum(history.opponent_actions[-1, :])
        exploitation_threshold = max(1, math.floor((self.n - 1) * 0.75))
        if c_others_prev >= exploitation_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_421(BaseStrategy):
    """
    The "Opportunistic Pumper" strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by free-riding on the
    contributions of others. It strategically defects whenever the public good
    is sufficiently well-funded by the collective. However, it will temporarily
    cooperate ("pump") when the level of cooperation drops too low, threatening
    the viability of the public good and thus the potential for future free-riding.
    This cooperation is a calculated, self-interested investment, not an act of altruism.

    The strategy starts with a cooperative action in the first round to
    encourage initial cooperation. It ruthlessly defects in the final round
    to maximize immediate payoff when there are no future consequences.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        c_threshold_raw = (self.k - 1) * self.n / self.k
        self.c_threshold = math.floor(c_threshold_raw)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_prev_round == 0:
            return Action.D
        elif num_cooperators_prev_round <= self.c_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_422(BaseStrategy):
    """
    "The Opportunistic Catalyst" strategy for the N-Player Public Goods Game.

    This strategy aims to maximize personal payoff by primarily defecting (free-riding),
    but it strategically cooperates (acts as a "catalyst") for short periods to "re-seed"
    collective cooperation if it significantly drops. This ensures there's a public good
    to exploit in subsequent rounds, while minimizing "wasted" cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.ENDGAME_ROUNDS_THRESHOLD = max(1, min(3, r // 3))
        self.EXPLOIT_THRESHOLD_RATIO = 0.5
        self.INDUCING_COOPERATION_STREAK_LENGTH = 2
        self.MIN_COOPERATION_TO_INDUCE = 1
        self.DEFECTION_PERSISTENCE_ROUNDS = 3
        self.my_last_action: None | Action = None
        self.consecutive_low_cooperation_rounds = 0
        self.consecutive_induce_attempts = 0
        self.permanent_defection_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n = self.game_description.n_players
        if state.round_number == 0:
            self.my_last_action = Action.D
            return Action.D
        if current_round_1_indexed > self.game_description.n_rounds - self.ENDGAME_ROUNDS_THRESHOLD:
            self.my_last_action = Action.D
            return Action.D
        prev_C_others = np.sum(history.opponent_actions[-1, :])
        n_others = n - 1
        others_coop_ratio = 0.0
        if n_others > 0:
            others_coop_ratio = prev_C_others / n_others
        if others_coop_ratio < self.EXPLOIT_THRESHOLD_RATIO or prev_C_others < self.MIN_COOPERATION_TO_INDUCE:
            self.consecutive_low_cooperation_rounds += 1
        else:
            self.consecutive_low_cooperation_rounds = 0
        if self.my_last_action == Action.C and others_coop_ratio < self.EXPLOIT_THRESHOLD_RATIO:
            self.consecutive_induce_attempts += 1
        else:
            self.consecutive_induce_attempts = 0
        if self.permanent_defection_mode or self.consecutive_low_cooperation_rounds >= self.DEFECTION_PERSISTENCE_ROUNDS or (self.my_last_action == Action.C and self.consecutive_induce_attempts >= self.INDUCING_COOPERATION_STREAK_LENGTH):
            self.permanent_defection_mode = True
            self.my_last_action = Action.D
            return Action.D
        if others_coop_ratio >= self.EXPLOIT_THRESHOLD_RATIO:
            self.my_last_action = Action.D
            return Action.D
        if prev_C_others >= self.MIN_COOPERATION_TO_INDUCE and self.consecutive_induce_attempts < self.INDUCING_COOPERATION_STREAK_LENGTH:
            self.my_last_action = Action.C
            return Action.C
        self.my_last_action = Action.D
        return Action.D

class Strategy_EXPLOITATIVE_423(BaseStrategy):
    """
    Adaptive Opportunist (AO) strategy for the N-Player Public Goods Game.
    This strategy balances initial 'niceness' to encourage cooperation, aggressive free-riding,
    and strategic intervention (minimal cooperation) to prevent the collapse of the public good pool,
    all while exploiting the finite end of the game.

    Key Concepts:
    - Initial Trust Building: Cooperates in initial rounds to encourage others.
    - Default Free-Riding: Primarily defects when others' cooperation is sufficient.
    - Cooperation Revival: Temporarily cooperates if others' cooperation drops too low, to restore the public good.
    - Endgame Exploitation: Always defects in the final rounds, leveraging the lack of future consequences.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.initial_cooperate_rounds = 2
        self.endgame_horizon = max(1, min(3, self.r // 2))
        self.cooperation_revival_threshold = 0.5
        self.forgiveness_period = 1
        self.is_reviving_cooperation = False
        self.revival_rounds_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_1_indexed_round = state.round_number + 1
        if current_1_indexed_round > self.r - self.endgame_horizon:
            return Action.D
        if current_1_indexed_round <= self.initial_cooperate_rounds:
            return Action.C
        num_c_other_prev_round = np.sum(history.opponent_actions[-1, :])
        if self.n - 1 == 0:
            proportion_c_other_prev_round = 0.0
        else:
            proportion_c_other_prev_round = num_c_other_prev_round / (self.n - 1)
        if self.is_reviving_cooperation:
            if self.revival_rounds_remaining > 0:
                self.revival_rounds_remaining -= 1
                return Action.C
            else:
                self.is_reviving_cooperation = False
                return Action.D
        elif proportion_c_other_prev_round < self.cooperation_revival_threshold:
            self.is_reviving_cooperation = True
            self.revival_rounds_remaining = self.forgiveness_period - 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_424(BaseStrategy):
    """
    The "Probing Free-Rider" strategy aims to maximize its own payoff by strategically
    leveraging cooperation from others while minimizing its own contributions.
    It uses its own cooperation sparingly and strategically as an investment to
    foster a cooperative environment, then exploits it by defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.Probing_Rounds: int = 2
        self.Patience_For_Reinduction: int = 1
        self.Minimum_Cooperators_To_Exploit: int = math.ceil((self.game_description.n_players - 1) / 2)
        self.Induce_Mode_Active: bool = False
        self.Induce_Rounds_Counter: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == total_rounds:
            return Action.D
        if current_round_1_indexed <= self.Probing_Rounds:
            self.Induce_Mode_Active = False
            self.Induce_Rounds_Counter = 0
            return Action.C
        num_other_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        if num_other_cooperators_prev_round >= self.Minimum_Cooperators_To_Exploit:
            self.Induce_Mode_Active = False
            self.Induce_Rounds_Counter = 0
            return Action.D
        elif self.Induce_Mode_Active:
            self.Induce_Rounds_Counter += 1
            if self.Induce_Rounds_Counter <= self.Patience_For_Reinduction:
                return Action.C
            else:
                self.Induce_Mode_Active = False
                self.Induce_Rounds_Counter = 0
                return Action.D
        else:
            self.Induce_Mode_Active = True
            self.Induce_Rounds_Counter = 1
            return Action.C

class Strategy_EXPLOITATIVE_425(BaseStrategy):
    """
    The Aggressive Conditional Free-Rider strategy for the N-Player Public Goods Game.

    This strategy aims to establish a cooperative environment and then aggressively
    free-ride on the public good created by other players. It employs specific rules
    for initial, intermediate, and final rounds, adapting its behavior based on the
    observed level of cooperation in previous rounds. It only contributes if doing
    so appears strategically necessary to prevent a total collapse of the public good
    that it intends to exploit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.initial_coop_rounds = min(max(1, math.floor(self.r / 4)), 2)
        self.pure_cooperation_level = self.n
        self.pure_defection_level = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed <= self.initial_coop_rounds:
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        else:
            c_prev_total = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if c_prev_total == self.pure_cooperation_level:
                return Action.D
            elif c_prev_total == self.pure_defection_level:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_426(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by strategically probing for
    cooperation early in the game. If sufficient cooperation from other players
    is detected, it will ruthlessly exploit them by defecting. If cooperation
    is low, it will revert to its dominant strategy of defection to minimize
    losses, as attempting to induce cooperation further would be too costly
    and uncertain.

    It operates in three phases:
    1. Lure Phase: Cooperates initially to probe for cooperation from others.
    2. Adaptive Exploitation Phase: Observes recent cooperation levels.
       If in this phase, the strategy always defects, regardless of observed
       cooperation. The observation logic serves as a diagnostic justification
       for the strategy's overall exploitative nature: it defects if cooperation
       is high enough to make exploitation profitable, or defects if it's too
       low to warrant further investment in cooperation.
    3. Last Round: Always defects due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.LURE_ROUNDS = 2
        self.EXPLOIT_THRESHOLD = 0.4
        self.OBSERVATION_WINDOW = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number == self.n_rounds - 1:
            return Action.D
        if current_round_1_indexed <= self.LURE_ROUNDS:
            return Action.C
        total_cooperation_by_others = 0
        total_possible_actions = 0
        start_observation_0_indexed = max(0, state.round_number - self.OBSERVATION_WINDOW)
        observed_actions_window = history.opponent_actions[start_observation_0_indexed:state.round_number, :]
        if observed_actions_window.size > 0:
            total_cooperation_by_others = np.sum(observed_actions_window)
            observed_rounds_count = observed_actions_window.shape[0]
            total_possible_actions = (self.n_players - 1) * observed_rounds_count
        avg_cooperation_others = 0.0
        if total_possible_actions > 0:
            avg_cooperation_others = float(total_cooperation_by_others) / total_possible_actions
        return Action.D

class Strategy_EXPLOITATIVE_427(BaseStrategy):
    """
    Opportunistic Defector: This strategy probes for cooperation, quickly shifts to exploiting it when available,
    and only strategically re-seeds cooperation if the overall environment becomes too uncooperative to exploit
    effectively. It always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_probe_rounds = 3
        self.min_coop_for_exploit = max(1, (self.game_description.n_players - 1) // 2)
        self.recooperation_threshold = max(1, self.game_description.n_players // 4)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        n_rounds = self.game_description.n_rounds
        if current_round_zero_indexed == 0:
            return Action.C
        if current_round_zero_indexed == n_rounds - 1:
            return Action.D
        num_others_cooperated_prev_round = np.sum(history.opponent_actions[-1, :])
        total_cooperated_prev_round = num_others_cooperated_prev_round + (1 if history.my_actions[-1] else 0)
        my_ACR = np.mean(history.my_actions)
        if current_round_zero_indexed < self.initial_probe_rounds:
            if num_others_cooperated_prev_round >= self.min_coop_for_exploit:
                return Action.D
            else:
                return Action.C
        elif num_others_cooperated_prev_round >= self.min_coop_for_exploit:
            return Action.D
        elif my_ACR < 0.5 and total_cooperated_prev_round <= self.recooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_428(BaseStrategy):
    """
    Calculated Exploiter strategy for the N-Player Public Goods Game.
    This strategy aims to establish cooperation, then free-rides on it.
    If cooperation collapses, it temporarily re-engages to rebuild,
    only to attempt free-riding again. It also defects in the last round.
    """
    _INITIAL_COOPERATION_ROUNDS = 2
    _HIGH_COOPERATION_THRESHOLD = 0.75
    _LOW_COOPERATION_THRESHOLD = 0.25
    _PATIENCE_ROUNDS_AFTER_COOPERATING = 3
    _MODE_COOPERATING = True
    _MODE_DEFECTING = False

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.current_mode: bool = self._MODE_COOPERATING
        self.rounds_since_mode_switch: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed <= self._INITIAL_COOPERATION_ROUNDS:
            return Action.C
        others_cooperators = np.sum(history.opponent_actions[-1, :])
        others_cooperation_rate = others_cooperators / (self.n_players - 1)
        if self.current_mode == self._MODE_COOPERATING:
            self.rounds_since_mode_switch += 1
            if self.rounds_since_mode_switch >= self._PATIENCE_ROUNDS_AFTER_COOPERATING and others_cooperation_rate >= self._HIGH_COOPERATION_THRESHOLD:
                self.current_mode = self._MODE_DEFECTING
                self.rounds_since_mode_switch = 0
                return Action.D
            else:
                return Action.C
        else:
            self.rounds_since_mode_switch += 1
            if others_cooperation_rate < self._LOW_COOPERATION_THRESHOLD:
                self.current_mode = self._MODE_COOPERATING
                self.rounds_since_mode_switch = 0
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_429(BaseStrategy):
    """
    The Adaptive Leech (AL) is an exploitative strategy for the N-Player Public Goods Game.
    It dynamically switches between encouraging cooperation (to build a pool of public goods)
    and defecting (to free-ride on those goods), while always protecting its own minimum payoff.
    It adapts to varying levels of cooperation among opponents using thresholds derived from
    the number of players (n).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        self.HIGH_EXPLOIT_THRESHOLD_C = math.ceil(n * 0.75)
        self.LOW_PROTECT_THRESHOLD_C = math.floor(n * 0.25)
        self.MODERATE_COOP_PIVOT_C = math.floor(n / 2)
        self.LOW_PROTECT_THRESHOLD_C = max(0, self.LOW_PROTECT_THRESHOLD_C)
        self.HIGH_EXPLOIT_THRESHOLD_C = min(n, self.HIGH_EXPLOIT_THRESHOLD_C)
        self.MODERATE_COOP_PIVOT_C = min(max(self.LOW_PROTECT_THRESHOLD_C, self.MODERATE_COOP_PIVOT_C), self.HIGH_EXPLOIT_THRESHOLD_C)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == n_rounds:
            return Action.D
        C_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if C_prev >= self.HIGH_EXPLOIT_THRESHOLD_C:
            return Action.D
        if C_prev <= self.LOW_PROTECT_THRESHOLD_C:
            return Action.D
        if C_prev >= self.MODERATE_COOP_PIVOT_C:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_430(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy aims to maximize its own total payoff by
    strategically leveraging cooperation from others. It operates with an exploitative mindset,
    seeking to free-ride on the public good created by other players while only contributing
    when it serves to induce or maintain a beneficial level of collective cooperation.
    It is designed to be adaptive and robust, responding to the observed history of play.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.INITIAL_COOPERATION_ROUNDS = 2
        self.COOPERATION_THRESHOLD_PERCENTAGE = 0.5
        self.DEFECTION_STREAK_LIMIT = 3
        self.my_defection_streak: int = 0
        self.last_action_was_cooperation_attempt: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        total_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == total_rounds - 1:
            self.my_defection_streak += 1
            self.last_action_was_cooperation_attempt = False
            return Action.D
        if current_round_0_indexed < self.INITIAL_COOPERATION_ROUNDS:
            self.my_defection_streak = 0
            self.last_action_was_cooperation_attempt = True
            return Action.C
        C_t_minus_1_total = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        cooperation_threshold_count = n * self.COOPERATION_THRESHOLD_PERCENTAGE
        if C_t_minus_1_total >= cooperation_threshold_count:
            self.my_defection_streak += 1
            self.last_action_was_cooperation_attempt = False
            return Action.D
        elif self.my_defection_streak < self.DEFECTION_STREAK_LIMIT and (not self.last_action_was_cooperation_attempt):
            self.my_defection_streak = 0
            self.last_action_was_cooperation_attempt = True
            return Action.C
        else:
            self.my_defection_streak += 1
            self.last_action_was_cooperation_attempt = False
            return Action.D

class Strategy_EXPLOITATIVE_431(BaseStrategy):
    """
    The Strategic Free-Rider strategy aims to benefit from others' cooperation while minimizing its own
    contributions. It operates on a cycle of "cooperate to re-seed" and "defect to exploit," with
    clear rules for the critical first and last rounds. It monitors collective cooperation levels
    and free-rides when high, contributes to restore cooperation when moderate, and defects to
    avoid being a sucker when cooperation has collapsed.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.C_collapse_threshold = 1
        self.C_exploitation_threshold = math.floor(self.n_players * 0.5)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        num_cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_prev_round <= self.C_collapse_threshold:
            return Action.D
        elif num_cooperators_prev_round >= self.C_exploitation_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_432(BaseStrategy):
    """
    Adaptive Greedy Free-Rider (AGFR) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by generally free-riding on the
    contributions of others. It only contributes when the public good's total
    contributions fall below a calculated threshold, to prevent collapse and ensure
    future opportunities for exploitation.

    Strategy parameters (`cooperation_history_window` and `min_coop_trigger`)
    are dynamically calculated based on the game's `n`, `r`, and `k` parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        self.cooperation_history_window = min(r - 1, 5)
        self.min_coop_trigger = max(1, math.floor(0.5 * n / k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds_in_game = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == total_rounds_in_game - 1:
            return Action.D
        total_cooperators_sum = 0
        actual_window_size = 0
        start_history_idx = max(0, current_round_0_indexed - self.cooperation_history_window)
        for i in range(start_history_idx, current_round_0_indexed):
            cooperators_in_round_i = history.my_actions[i] + np.sum(history.opponent_actions[i, :])
            total_cooperators_sum += cooperators_in_round_i
            actual_window_size += 1
        if actual_window_size == 0:
            avg_cooperators_in_window = 0
        else:
            avg_cooperators_in_window = total_cooperators_sum / actual_window_size
        if avg_cooperators_in_window < self.min_coop_trigger:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_433(BaseStrategy):
    """
    The "Intelligent Opportunist" strategy is designed to be highly exploitative in the
    N-Player Public Goods Game. Its core principle is to aggressively free-ride whenever
    there is any observable cooperation among opponents, while selectively engaging in
    cooperation only to prevent the total collapse of the public good pool, thereby
    preserving future exploitation opportunities. It balances immediate maximal gain with
    strategic long-term manipulation of opponent behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.last_re_bait_round = -1
        self.my_actions_history = []
        self.other_cooperators_history = []
        self.EXPLOIT_THRESHOLD_OTHERS = 1
        self.RE_BAIT_FREQUENCY_ROUNDS = 3
        self.RE_BAIT_ON_COLLAPSE_THRESHOLD = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number
        action_for_this_round = None
        if current_round_num == 0:
            action_for_this_round = Action.C
            self.last_re_bait_round = current_round_num
        elif current_round_num == self.n_rounds - 1:
            action_for_this_round = Action.D
        else:
            num_other_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            self.other_cooperators_history.append((current_round_num - 1, num_other_cooperators_prev_round))
            if num_other_cooperators_prev_round >= self.EXPLOIT_THRESHOLD_OTHERS:
                action_for_this_round = Action.D
            elif current_round_num - self.last_re_bait_round >= self.RE_BAIT_FREQUENCY_ROUNDS and num_other_cooperators_prev_round <= self.RE_BAIT_ON_COLLAPSE_THRESHOLD:
                action_for_this_round = Action.C
                self.last_re_bait_round = current_round_num
            else:
                action_for_this_round = Action.D
        self.my_actions_history.append((current_round_num, action_for_this_round))
        return action_for_this_round

class Strategy_EXPLOITATIVE_434(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by primarily defecting and
    free-riding on others' contributions. It adaptively cooperates for a short
    duration if the observed collective cooperation among other players falls
    below a critical threshold, effectively "investing" to re-establish a public
    good from which to exploit. It also incorporates an end-game defection
    horizon based on backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.rounds_in_recovery_mode = 0
        self.defection_threshold: float = 0.4
        self.recovery_cooperation_duration: int = 1
        self.defection_horizon: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        C = Action.C
        D = Action.D
        current_round_1_indexed = state.round_number + 1
        if state.round_number == 0:
            self.rounds_in_recovery_mode = 0
            return D
        if current_round_1_indexed >= self.r - self.defection_horizon + 1:
            self.rounds_in_recovery_mode = 0
            return D
        if self.rounds_in_recovery_mode > 0:
            self.rounds_in_recovery_mode -= 1
            return C
        previous_round_opponent_actions = history.opponent_actions[-1, :]
        num_other_cooperators = np.sum(previous_round_opponent_actions)
        fraction_of_others_cooperating = num_other_cooperators / (self.n - 1)
        if fraction_of_others_cooperating < self.defection_threshold:
            self.rounds_in_recovery_mode = self.recovery_cooperation_duration
            return C
        else:
            return D

class Strategy_EXPLOITATIVE_435(BaseStrategy):
    """
    The Adaptive Predator strategy aims to maximize its own payoff in the N-Player Public Goods Game
    by free-riding on others' contributions. It starts by cooperating to test the environment, then
    exploits sufficient cooperation by defecting. If group cooperation collapses, it strategically
    cooperates for a limited time to rebuild the public good, only to exploit again. It always defects
    in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_actions_history: list[bool] = []
        self.recovery_mode_active: bool = False
        self.recovery_rounds_left: int = 0
        self.DEFECTION_THRESHOLD: float = 0.3
        self.EXPLOIT_THRESHOLD: float = 0.6
        self.MAX_RECOVERY_ROUNDS: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        action_to_take: Action
        if current_round_1_indexed == 1:
            self.recovery_mode_active = False
            self.recovery_rounds_left = 0
            action_to_take = Action.C
            self.my_actions_history.append(action_to_take == Action.C)
            return action_to_take
        if current_round_1_indexed == total_rounds:
            action_to_take = Action.D
            self.my_actions_history.append(action_to_take == Action.C)
            return action_to_take
        num_others_cooperated_prev = np.sum(history.opponent_actions[-1, :])
        n_players_excluding_self = n_players - 1
        effective_cooperation_rate: float
        effective_cooperation_rate = num_others_cooperated_prev / n_players_excluding_self
        if self.recovery_mode_active:
            if self.recovery_rounds_left > 0:
                action_to_take = Action.C
                self.recovery_rounds_left -= 1
                self.my_actions_history.append(action_to_take == Action.C)
                return action_to_take
            else:
                self.recovery_mode_active = False
        action_to_take = Action.D
        if effective_cooperation_rate >= self.EXPLOIT_THRESHOLD:
            action_to_take = Action.D
        elif effective_cooperation_rate < self.DEFECTION_THRESHOLD:
            my_last_action_was_cooperate = self.my_actions_history[-1]
            if my_last_action_was_cooperate:
                action_to_take = Action.D
            else:
                action_to_take = Action.C
                self.recovery_mode_active = True
                self.recovery_rounds_left = self.MAX_RECOVERY_ROUNDS - 1
        else:
            action_to_take = Action.D
        self.my_actions_history.append(action_to_take == Action.C)
        return action_to_take

class Strategy_EXPLOITATIVE_436(BaseStrategy):
    """
    The Stubborn Opportunist: This strategy defaults to defection, aiming to free-ride
    on others' cooperation. However, if the collective cooperation level from others
    falls too low, it strategically attempts a single round of cooperation ('bait')
    to stimulate others, immediately reverting to defection to exploit any induced
    cooperation. It learns from failed baiting attempts and, if consistently
    unsuccessful, switches to permanent defection to minimize losses. It always
    defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.COOPERATION_THRESHOLD = (self.n - 1) / 2.0
        self.BAIT_EFFECTIVENESS_THRESHOLD = 0.5
        self.MAX_FAILED_BAITS = 2
        self._consecutive_failed_baits: int = 0
        self._my_last_action_was_bait: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_idx = state.round_number
        final_round_idx = self.r - 1
        if current_round_idx == final_round_idx:
            return Action.D
        if current_round_idx > 0:
            if self._my_last_action_was_bait:
                S_others_prev_round = np.sum(history.opponent_actions[-1, :])
                S_others_before_bait = 0
                if current_round_idx >= 2:
                    S_others_before_bait = np.sum(history.opponent_actions[-2, :])
                if S_others_prev_round < self.COOPERATION_THRESHOLD and S_others_prev_round <= S_others_before_bait + self.BAIT_EFFECTIVENESS_THRESHOLD:
                    self._consecutive_failed_baits += 1
                else:
                    self._consecutive_failed_baits = 0
            self._my_last_action_was_bait = False
        if self._consecutive_failed_baits >= self.MAX_FAILED_BAITS:
            return Action.D
        if current_round_idx == 0:
            return Action.D
        S_others_prev_round = np.sum(history.opponent_actions[-1, :])
        if S_others_prev_round >= self.COOPERATION_THRESHOLD:
            return Action.D
        else:
            self._my_last_action_was_bait = True
            return Action.C

class Strategy_EXPLOITATIVE_437(BaseStrategy):
    """
    An exploitative, adaptive, and robust strategy for the N-Player Public Goods Game.
    This strategy maximizes its own payoff by leveraging repeated interaction and observed
    opponent behavior, without assuming shared norms or specific coordination.

    It starts by cooperating for a limited number of rounds to probe for cooperation
    among opponents. It then continuously monitors the level of cooperation from others.
    If a sufficiently high proportion of opponents cooperate, it switches to defecting
    to free-ride on their contributions. If opponent cooperation drops below a profitable
    threshold, it temporarily reverts to cooperating to "re-seed" cooperation.
    In the final rounds, it unconditionally defects, consistent with backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._COOPERATION_PROBE_ROUNDS = 1
        self._EXPLOITATION_THRESHOLD = 0.5
        self._ENDGAME_ROUNDS = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players_total = self.game_description.n_players
        if current_round_1_indexed <= self._COOPERATION_PROBE_ROUNDS:
            return Action.C
        if current_round_1_indexed >= total_rounds - self._ENDGAME_ROUNDS + 1:
            return Action.D
        num_others = n_players_total - 1
        num_cooperators_among_others_prev_round = np.sum(history.opponent_actions[-1, :])
        proportion_other_cooperators = 0.0
        if num_others > 0:
            proportion_other_cooperators = num_cooperators_among_others_prev_round / num_others
        if proportion_other_cooperators >= self._EXPLOITATION_THRESHOLD:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_438(BaseStrategy):
    """
    Adaptive Exploiter (AE) strategy for the N-Player Public Goods Game.

    This strategy is designed to cultivate a cooperative environment among other players
    in the early and middle rounds, then free-ride on that cooperation, and finally
    exploit the "end-game" effect in the last round. It adapts its behavior based on
    the observed level of cooperation in the previous round.

    Core Principles:
    1.  Seed Cooperation: Initiate cooperation in the first round.
    2.  Monitor and Exploit: In intermediate rounds, if total cooperation in the
        previous round was sufficiently high (>= n/2), free-ride by defecting.
    3.  Reinforce Cooperation: If total cooperation dips too low (< n/2), strategically
        cooperate to prevent collapse and preserve future exploitation opportunities.
    4.  End-Game Defection: Always defect in the final round to maximize immediate gain.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_target_threshold = self.n_players // 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        total_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if total_cooperators_prev_round < self.cooperation_target_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_439(BaseStrategy):
    """
    The Manipulative Free-Rider (MFR) strategy aims to maximize its own long-term payoff by:
    1. Baiting: Initiating cooperation to "test the waters" and encourage others to contribute.
    2. Exploitation: Primarily defecting to free-ride on the public good when others are cooperating sufficiently.
    3. Re-investment: Strategically cooperating when the level of cooperation from others drops too low,
       or after a prolonged period of defection, to prevent a complete collapse of cooperation and "re-prime" the system.
    4. Terminal Exploitation: Defecting in the final round, as backward induction dictates.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.defection_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.defection_streak = 0
            return Action.C
        elif state.round_number == self.n_rounds - 1:
            self.defection_streak = 0
            return Action.D
        else:
            C_other_prev = np.sum(history.opponent_actions[-1, :])
            if self.n_players > 1:
                P_C_other_prev = C_other_prev / (self.n_players - 1)
            else:
                P_C_other_prev = 0
            if P_C_other_prev > 0.5:
                self.defection_streak = 0
                return Action.D
            elif P_C_other_prev <= 0.1:
                self.defection_streak = 0
                return Action.C
            else:
                self.defection_streak += 1
                if self.defection_streak > 3:
                    self.defection_streak = 0
                    return Action.C
                else:
                    return Action.D

class Strategy_EXPLOITATIVE_440(BaseStrategy):
    """
    The "Calculated Opportunist" (CO) strategy for an N-Player Public Goods Game.

    This strategy is designed to maximize its own total payoff by strategically
    free-riding on the contributions of others when the public good is sufficiently
    robust. It makes minimal, targeted "investments" (cooperating) only when
    necessary to ensure the continued viability of the public good for future
    exploitation. It is adaptive to the collective behavior of other players and
    accounts for the game's finite horizon.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.K_THRESHOLD_INT = math.floor(self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        C_total_prev_round = history.opponent_actions[-1, :].sum() + history.my_actions[-1]
        if C_total_prev_round >= self.K_THRESHOLD_INT:
            return Action.D
        elif C_total_prev_round + 1 >= self.K_THRESHOLD_INT:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_441(BaseStrategy):
    """
    The Manipulative Cooperator (MC) strategy embodies an exploitative mindset.
    It defaults to defection to capture the immediate private gain. However, it monitors its
    own payoff. If defecting consistently leads to a payoff that is too low (meaning other
    players are also defecting or punishing), it will temporarily cooperate for a short burst
    to "reset" the group dynamic, hoping to induce other players to cooperate again, thereby
    creating an opportunity for future free-riding. The strategy defects in the first and last rounds.
    """
    RESET_COOPERATION_BURST_LENGTH = 2
    ASPIRATION_FACTOR = 1.5

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.low_payoff_threshold = 1.0 + self.k / self.n * self.ASPIRATION_FACTOR
        if self.low_payoff_threshold <= 1.0:
            self.low_payoff_threshold = 1.05
        self.cooperation_phase_remaining_rounds = 0
        self.last_round_own_payoff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.r - 1:
            return Action.D
        self.last_round_own_payoff = history.my_payoffs[current_round_0_indexed - 1]
        if self.cooperation_phase_remaining_rounds > 0:
            self.cooperation_phase_remaining_rounds -= 1
            return Action.C
        if self.last_round_own_payoff < self.low_payoff_threshold:
            self.cooperation_phase_remaining_rounds = self.RESET_COOPERATION_BURST_LENGTH - 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_442(BaseStrategy):
    """
    The Opportunistic Provocateur strategy aims to maximize personal payoff by primarily defecting,
    but intelligently engaging in temporary cooperation to "provoke" or "recharge" the public good
    if it becomes too depleted, only to exploit it again. It has a built-in "patience limit" to
    avoid being exploited by persistent defectors and defects unconditionally in the final round
    due to the finite game horizon.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_cooperate_rounds = max(1, min(2, self.game_description.n_rounds - 1))
        self.cooperation_sustain_threshold = math.floor(self.game_description.n_players / 2)
        self.recharge_patience_limit = 2
        self.consecutive_low_coop_recharges = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number == 0:
            self.consecutive_low_coop_recharges = 0
        else:
            my_action_t_minus_1_bool = history.my_actions[-1]
            my_action_t_minus_1 = Action.C if my_action_t_minus_1_bool else Action.D
            total_cooperators_t_minus_1 = int(my_action_t_minus_1_bool) + np.sum(history.opponent_actions[-1, :])
            was_initial_cooperate_round_t_minus_1 = current_round_1_indexed - 1 <= self.initial_cooperate_rounds
            if my_action_t_minus_1 == Action.C and (not was_initial_cooperate_round_t_minus_1):
                if total_cooperators_t_minus_1 < self.cooperation_sustain_threshold:
                    self.consecutive_low_coop_recharges += 1
                else:
                    self.consecutive_low_coop_recharges = 0
            else:
                self.consecutive_low_coop_recharges = 0
        if current_round_1_indexed <= self.initial_cooperate_rounds:
            return Action.C
        elif current_round_1_indexed == self.game_description.n_rounds:
            return Action.D
        else:
            my_action_t_minus_1_bool = history.my_actions[-1]
            total_cooperators_t_minus_1 = int(my_action_t_minus_1_bool) + np.sum(history.opponent_actions[-1, :])
            if self.consecutive_low_coop_recharges >= self.recharge_patience_limit:
                return Action.D
            elif total_cooperators_t_minus_1 >= self.cooperation_sustain_threshold:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_443(BaseStrategy):
    """
    The Calculated Opportunist: Defaults to free-riding, but strategically cooperates
    to prevent the collapse of the public good below a beneficial threshold, ensuring
    future exploitation opportunities. It's a purely self-interested strategy that
    views cooperation as a temporary investment for later free-riding.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.floor(self.game_description.n_players / self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_index = state.round_number - 1
        total_cooperators_previous_round = np.sum(history.opponent_actions[previous_round_index, :]) + history.my_actions[previous_round_index]
        if total_cooperators_previous_round < self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_444(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy for the N-Player Public Goods Game.

    Core Philosophy:
    AE aims to maximize its own payoff by strategically free-riding. It understands
    that prolonged universal defection is suboptimal and therefore strategically
    "invests" minimal cooperation when needed to encourage others, always with the
    ultimate goal of resuming free-riding. It adapts based on past observable behavior.

    Strategy Phases:
    1.  Initial Probe Phase: Defects to test the waters and establish free-riding.
    2.  Adaptive Exploitation Phase:
        -   If many others cooperate, it defects to maximize free-riding.
        -   If moderate cooperation, and it hasn't cooperated recently, it cooperates
            to re-induce higher cooperation levels from others.
        -   If cooperation is low, or it recently cooperated, it defects to avoid being
            a 'sucker' or to observe effects of prior cooperation.
    3.  Endgame Detection: Always defects in the final round to secure immediate payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.probe_rounds = min(2, self.r - 1) if self.r > 1 else 0
        self.exploit_threshold_ratio = 0.6
        self.punish_threshold_ratio = 0.2
        self.coop_initiation_delay = 3
        self._my_last_action = Action.D
        self._rounds_since_my_last_C = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        action_for_this_round = Action.D
        if current_round_1_indexed == self.r:
            action_for_this_round = Action.D
        elif current_round_1_indexed <= self.probe_rounds:
            action_for_this_round = Action.D
        else:
            c_others_prev = int(np.sum(history.opponent_actions[-1, :]))
            exploit_thresh = math.ceil((self.n - 1) * self.exploit_threshold_ratio)
            punish_thresh = math.floor((self.n - 1) * self.punish_threshold_ratio)
            if c_others_prev >= exploit_thresh:
                action_for_this_round = Action.D
            elif c_others_prev > punish_thresh and self._rounds_since_my_last_C >= self.coop_initiation_delay:
                action_for_this_round = Action.C
            else:
                action_for_this_round = Action.D
        self._my_last_action = action_for_this_round
        if action_for_this_round == Action.C:
            self._rounds_since_my_last_C = 0
        else:
            self._rounds_since_my_last_C += 1
        return action_for_this_round

class Strategy_EXPLOITATIVE_445(BaseStrategy):
    """
    The Greedy Trigger strategy for the N-Player Public Goods Game.
    It defaults to defection, free-riding on others. It cooperates only when
    the overall public good contributions fall below a critical lower threshold
    to stimulate more contributions. Once cooperation reaches an upper threshold,
    it reverts to defection. If attempts to stimulate cooperation repeatedly fail,
    it enters a permanent defect mode.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.cooperation_stimulus_active = False
        self.cooperation_attempt_counter = 0
        self.permanent_defect_mode = False
        n = self.n_players
        if n <= 3:
            self.c_threshold_lower = 1
        else:
            self.c_threshold_lower = max(1, math.floor(n * 0.3))
        if n <= 3:
            self.c_threshold_upper = n
        else:
            self.c_threshold_upper = max(self.c_threshold_lower + 1, math.floor(n * 0.7))
        self.max_unsuccessful_attempts = max(2, math.floor(n / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.D
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        c_prev_opponents_sum = np.sum(history.opponent_actions[current_round_0_indexed - 1, :])
        c_prev_my_action_int = 1 if history.my_actions[current_round_0_indexed - 1] else 0
        c_prev_total = c_prev_opponents_sum + c_prev_my_action_int
        if self.permanent_defect_mode:
            return Action.D
        if self.cooperation_stimulus_active:
            if c_prev_total >= self.c_threshold_upper:
                self.cooperation_stimulus_active = False
                self.cooperation_attempt_counter = 0
            else:
                self.cooperation_attempt_counter += 1
                if self.cooperation_attempt_counter >= self.max_unsuccessful_attempts:
                    self.permanent_defect_mode = True
                    self.cooperation_stimulus_active = False
                    self.cooperation_attempt_counter = 0
        elif c_prev_total <= self.c_threshold_lower:
            self.cooperation_stimulus_active = True
            self.cooperation_attempt_counter = 0
        if self.cooperation_stimulus_active:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_446(BaseStrategy):
    """
    Patient Adaptive Free-Rider (PAFR) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by strategically fluctuating
    between cooperation (to encourage others) and defection (to free-ride),
    with a built-in "patience" mechanism to avoid being exploited by persistent defectors.
    It seeks to establish an environment of cooperation to then exploit it.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_threshold = math.floor(self.game_description.n_players / self.game_description.k)
        self.LURE_ATTEMPT_LIMIT = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round = state.round_number
        if current_round == 0:
            return Action.C
        if current_round == r - 1:
            return Action.D
        prev_round_idx = current_round - 1
        C_sum_prev = np.sum(history.opponent_actions[prev_round_idx, :]) + int(history.my_actions[prev_round_idx])
        consecutive_low_cooperation_rounds_count = 0
        for i in range(prev_round_idx, -1, -1):
            my_action_i = history.my_actions[i]
            C_sum_i = np.sum(history.opponent_actions[i, :]) + int(my_action_i)
            if my_action_i is True and C_sum_i < self.T_threshold:
                consecutive_low_cooperation_rounds_count += 1
            else:
                break
        if C_sum_prev >= self.T_threshold:
            return Action.D
        elif consecutive_low_cooperation_rounds_count + 1 <= self.LURE_ATTEMPT_LIMIT:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_447(BaseStrategy):
    """
    The Calculated Free-Rider strategy for the N-Player Public Goods Game.

    This strategy embodies an exploitative and robust approach by consistently
    choosing to Defect (D) in every round. The game analysis reveals that
    Defecting is the dominant strategy in each single round because the payoff
    difference (_i(D) - _i(C) = 1 - k/n) is always positive given the game's
    parameters (1 < k < n).

    By always defecting, the strategy guarantees a higher immediate payoff
    for itself than cooperating, regardless of the actions of other players.
    It free-rides on any contributions made by others while minimizing its own cost.
    This approach is adaptive in that it optimally aligns with the fundamental
    mathematical structure of the game's payoffs, and robust because its
    performance is maximized for the player across all possible opponent behaviors
    without needing complex predictions or reactions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        return Action.D

class Strategy_EXPLOITATIVE_448(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own total payoff by primarily defecting,
    but strategically cooperating when necessary to maintain a sufficient level
    of cooperation among other players. It employs initial cooperation,
    opportunistic free-riding, adaptive recalibration, and ruthless endgame exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.COOPERATION_INDUCTION_ROUNDS = 2
        self.ENDGAME_ROUNDS = 2
        self.MIN_COOPERATORS_THRESHOLD = max(1, math.ceil((self.game_description.n_players - 1) / 2))
        self.EXPLOITATION_TOLERANCE = 2
        self.my_consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed >= n_rounds - self.ENDGAME_ROUNDS:
            return Action.D
        if current_round_0_indexed < self.COOPERATION_INDUCTION_ROUNDS:
            self.my_consecutive_defections = 0
            return Action.C
        num_cooperators_others_prev = np.sum(history.opponent_actions[-1, :])
        if num_cooperators_others_prev < self.MIN_COOPERATORS_THRESHOLD:
            self.my_consecutive_defections = 0
            return Action.C
        elif self.my_consecutive_defections < self.EXPLOITATION_TOLERANCE:
            self.my_consecutive_defections += 1
            return Action.D
        else:
            self.my_consecutive_defections = 0
            return Action.C

class Strategy_EXPLOITATIVE_449(BaseStrategy):
    """
    The Opportunistic Free-Rider strategy is designed to maximize its own payoff by
    strategically exploiting the contributions of other players while intelligently
    investing in cooperation when necessary to maintain a viable public good pool
    for future exploitation. It is adaptive, robust, and explicitly aligns with an
    exploitative mindset.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.Cooperation_Threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == n_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        previous_round_idx = current_round_0_indexed - 1
        opponent_actions_prev = history.opponent_actions[previous_round_idx, :]
        cooperation_from_others_prev = np.sum(opponent_actions_prev)
        num_other_players = n_players - 1
        average_cooperation_rate_of_others = float(cooperation_from_others_prev) / num_other_players
        if average_cooperation_rate_of_others >= self.Cooperation_Threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_450(BaseStrategy):
    """
    The Opportunistic Parasite strategy for N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by free-riding on others'
    contributions. It starts with a calculated probe (cooperate) to identify
    potential cooperators or responsive strategies. After this initial test,
    it defaults to defection to exploit any observed cooperation. If cooperation
    levels among opponents drop too low, it may probabilistically attempt to
    "re-ignite" cooperation with another strategic cooperation, hoping to
    create new free-riding opportunities. In the final round, it always defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n_players = self.game_description.n_players
        self.n_minus_1 = n_players - 1
        self.H = min(self.n_minus_1, 3)
        self.T_initial_coop = 1.0 / self.n_minus_1
        self.T_exploit = 0.3
        self.P_reignite = 0.15

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_num_0_indexed == total_rounds - 1:
            return Action.D
        if current_round_num_0_indexed == 0:
            return Action.C
        elif current_round_num_0_indexed == 1:
            other_actions_r0 = history.opponent_actions[0, :]
            num_coop_r0 = np.sum(other_actions_r0)
            if num_coop_r0 / self.n_minus_1 >= self.T_initial_coop:
                return Action.D
            else:
                return Action.D
        elif current_round_num_0_indexed == 2:
            return Action.D
        total_coop_others_in_window = 0
        total_possible_coop_others_in_window = 0
        start_history_index = max(0, current_round_num_0_indexed - self.H)
        for r_idx in range(start_history_index, current_round_num_0_indexed):
            round_data_opponent_actions = history.opponent_actions[r_idx, :]
            total_coop_others_in_window += np.sum(round_data_opponent_actions)
            total_possible_coop_others_in_window += self.n_minus_1
        AvgCoopRate_others_recent = 0.0
        if total_possible_coop_others_in_window > 0:
            AvgCoopRate_others_recent = total_coop_others_in_window / total_possible_coop_others_in_window
        if AvgCoopRate_others_recent >= self.T_exploit:
            return Action.D
        else:
            rounds_remaining_after_this_one = total_rounds - 1 - current_round_num_0_indexed
            if rounds_remaining_after_this_one > self.H + 1:
                if random.random() < self.P_reignite:
                    return Action.C
            return Action.D

class Strategy_EXPLOITATIVE_451(BaseStrategy):
    """
    The Opportunistic Investor strategy aims to maximize individual total payoff by defaulting to defect,
    exploiting others' cooperation. It only cooperates when the observed level of cooperation from others
    falls below a calculated "Cooperation Revival Threshold" (CRT), signaling a need to temporarily
    "invest" in cooperation to prevent a complete collapse and encourage renewed contributions for future exploitation.
    The strategy explicitly defects in the first and final rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.D
        if current_round_1_indexed == self.r:
            return Action.D
        else:
            previous_round_opponent_actions = history.opponent_actions[state.round_number - 1, :]
            c_others_count_prev = sum(previous_round_opponent_actions)
            threshold_value = math.floor(self.n / self.k)
            if c_others_count_prev < threshold_value:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_452(BaseStrategy):
    """
    The "Cautious Opportunist" strategy aggressively defects to free-ride on others'
    cooperation but "cautiously" intervenes with temporary cooperation to prevent
    a total breakdown of the public good, which would limit its own exploitation
    opportunities. It starts with cooperation, always defects in the last round,
    and otherwise adapts based on the level of cooperation observed.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        self.MIN_COOPERATORS_TO_EXPLOIT = max(2, n // 3)
        self.COOPERATION_RESET_WINDOW = 2
        self.COOPERATION_RESET_ROUNDS = 2
        self._rounds_since_low_cooperation = 0
        self._reset_cooperation_countdown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n_rounds = self.game_description.n_rounds
        if current_round_number == 0:
            self._rounds_since_low_cooperation = 0
            self._reset_cooperation_countdown = 0
            return Action.C
        if current_round_number == n_rounds - 1:
            return Action.D
        previous_cooperators = sum(history.opponent_actions[-1, :])
        if self._reset_cooperation_countdown > 0:
            self._reset_cooperation_countdown -= 1
            return Action.C
        if previous_cooperators <= 1:
            self._rounds_since_low_cooperation += 1
            if self._rounds_since_low_cooperation >= self.COOPERATION_RESET_WINDOW:
                self._reset_cooperation_countdown = self.COOPERATION_RESET_ROUNDS - 1
                self._rounds_since_low_cooperation = 0
                return Action.C
        else:
            self._rounds_since_low_cooperation = 0
        if previous_cooperators >= self.MIN_COOPERATORS_TO_EXPLOIT:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_453(BaseStrategy):
    """
    The Opportunistic Reciprocator strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its total payoff by primarily defecting (free-riding)
    on the contributions of others. However, it intelligently cooperates when the
    level of public good is too low, effectively "re-seeding" cooperation to rebuild
    a larger pool for future exploitation. This is a calculated sacrifice, not altruism.

    It operates in three phases:
    1. Initial Probe (Round 1): Cooperate to test the waters and encourage others.
    2. Adaptive Exploitation (Rounds 2 to r-1): Defect if sufficient other players
       cooperated in the previous round (determined by a dynamic Cooperation_Threshold).
       Otherwise, cooperate to invest in the public good.
    3. Final Round Exploitation (Round r): Always defect as there are no future
       consequences.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_threshold = max(1, math.ceil((self.n - 1) * (self.k / self.n)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        elif current_round_1_indexed == self.r:
            return Action.D
        else:
            c_others_prev = sum(history.opponent_actions[-1, :])
            if c_others_prev >= self.cooperation_threshold:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_454(BaseStrategy):
    """
    Adaptive k-Threshold Exploiter (AKT-E) strategy for the N-Player Public Goods Game.
    This strategy prioritizes defection to maximize individual payoff but strategically
    cooperates when the collective public good falls below a critical threshold (floor(k)),
    aiming to "re-seed" cooperation for future exploitation. It is designed to be a
    "smart free-rider," taking when it can, but contributing just enough to maintain
    the viability of the public good from which it benefits.

    Decision Rules:
    1.  **Round 1 (First Round, 0-indexed):** Always Cooperate (C).
        Rationale: Establishes an initial cooperative signal and probes the environment.
    2.  **Last Round (r-1, 0-indexed):** Always Defect (D).
        Rationale: No future interactions to incentivize; defecting maximizes immediate payoff.
    3.  **Intermediate Rounds (2 to r-1, 1 to r-2 0-indexed):**
        -   Observe `C_prev`: Total number of cooperators in the immediately preceding round.
        -   **If `C_prev < floor(k)`:** Cooperate (C).
            Rationale: Public good is too low; a "sacrificial" cooperation to encourage
                       others to re-engage and increase the public good's value.
        -   **If `C_prev >= floor(k)`:** Defect (D).
            Rationale: Public good is sufficiently robust; exploit others' contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_threshold = math.floor(self.k_factor)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = history.my_actions[-1] + np.sum(history.opponent_actions[-1, :])
        if num_cooperators_prev_round < self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_455(BaseStrategy):
    """
    The Strategic Freerider with Catalyst (SFC) is an exploitative strategy for the N-Player Public Goods Game.
    It aims to maximize its own payoff by selectively contributing to the public good only when cooperation
    levels fall below a calculated threshold (catalyst mode) to prevent a collapse that would reduce future
    freeriding opportunities. Otherwise, it defects (freerider mode) to exploit others' contributions.
    It always cooperates in the first round to 'seed' cooperation and defects in the final rounds due to
    endgame effects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.COOPERATION_THRESHOLD = max(1, math.floor(self.n * (self.k - 1) / (2 * self.k)))
        self.ENDGAME_BUFFER = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed >= self.r - self.ENDGAME_BUFFER + 1:
            return Action.D
        my_prev_action_is_cooperate = history.my_actions[-1]
        sum_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        total_cooperators_prev_round = my_prev_action_is_cooperate + sum_opponent_cooperators_prev_round
        if total_cooperators_prev_round < self.COOPERATION_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_456(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy aims to maximize its own payoff in the N-Player Public Goods Game
    by defaulting to defection to free-ride on others. It strategically cooperates only temporarily to
    prevent the complete collapse of the public good, and reverts to defection if seeding cooperation fails
    to avoid being a "sucker."

    Player State (Persistent across rounds):
    - consecutive_solo_coop_rounds: Tracks how many consecutive rounds this player has cooperated while
      no other player contributed. Initialized to 0.

    Strategy Parameter:
    - MAX_SOLO_COOP_ROUNDS: An integer threshold (set to 2) defining how many consecutive rounds
      the strategy is willing to be the only cooperator before giving up and defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_solo_coop_rounds = 0
        self.MAX_SOLO_COOP_ROUNDS = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            self.consecutive_solo_coop_rounds = 0
            return Action.D
        if current_round_number == self.game_description.n_rounds - 1:
            self.consecutive_solo_coop_rounds = 0
            return Action.D
        my_previous_action_is_cooperate = history.my_actions[-1]
        num_other_cooperators = np.sum(history.opponent_actions[-1, :])
        if num_other_cooperators > 0:
            self.consecutive_solo_coop_rounds = 0
            return Action.D
        elif my_previous_action_is_cooperate is False:
            self.consecutive_solo_coop_rounds = 1
            return Action.C
        else:
            self.consecutive_solo_coop_rounds += 1
            if self.consecutive_solo_coop_rounds >= self.MAX_SOLO_COOP_ROUNDS:
                self.consecutive_solo_coop_rounds = 0
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_457(BaseStrategy):
    """
    The Opportunistic Parasite strategy for the N-Player Public Goods Game.

    This strategy is designed to be purely self-interested and exploitative,
    aiming to maximize its own payoff by leveraging the fundamental game mechanics.

    The game's payoff structure reveals that Defecting (D) is always the
    individually dominant strategy in any single round. For any given number
    of cooperators from other players, a player always earns more by Defecting
    than by Cooperating. This is because the difference in payoff,
    `Payoff(D) - Payoff(C) = 1 - k/n`, is always positive given the game parameter
    `1 < k < n` (which implies `k/n < 1`).

    The detailed decision rules outlined for "The Opportunistic Parasite"
    (covering the first round, middle rounds with analysis of previous cooperation,
    and the last round) consistently lead to the choice to Defect. This aligns
    perfectly with the strategy's goals:
    - To exploit any existing cooperation by defecting and benefiting from the public good.
    - To avoid being exploited itself by never contributing if others are or aren't.
    - To never initiate cooperation to "re-seed" the public good, thus avoiding any immediate loss.

    Therefore, the most accurate and efficient implementation of this strategy
    is to always choose to Defect, as this consistently fulfills its exploitative
    nature and the dominant individual incentive of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        return Action.D

class Strategy_EXPLOITATIVE_458(BaseStrategy):
    """
    The Adaptive Free-Rider and Punisher (AFRP) strategy is designed to be highly exploitative,
    adaptive, and robust in the N-Player Public Goods Game. It probes for cooperation,
    then free-rides if cooperation is sufficient. If cooperation is consistently low,
    it switches to permanent defection to cut losses, and always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.C_threshold_proportion = 0.5
        self.Forgiveness_rounds = 1
        self.low_cooperation_streak = 0
        self.always_defect_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == total_rounds:
            return Action.D
        if self.always_defect_mode:
            return Action.D
        prev_round_opponent_actions = history.opponent_actions[state.round_number - 1, :]
        C_others_prev = sum(prev_round_opponent_actions)
        num_other_players = n_players - 1
        if num_other_players == 0:
            cooperation_ratio = 0.0
        else:
            cooperation_ratio = C_others_prev / num_other_players
        if cooperation_ratio >= self.C_threshold_proportion:
            self.low_cooperation_streak = 0
            return Action.D
        else:
            self.low_cooperation_streak += 1
            if self.low_cooperation_streak > self.Forgiveness_rounds:
                self.always_defect_mode = True
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_459(BaseStrategy):
    """
    The "Calibrated Opportunist" strategy aims to maximize its own payoff by strategically alternating
    between phases of cooperation (to induce others' cooperation) and defection (to exploit that cooperation).
    It is designed to be robust against a variety of opponent behaviors by adapting its approach based on
    observed collective cooperation levels, and to fully exploit the known finite game length.
    It cycles through "Probe", "Exploit", "Punish", and "Endgame" modes.
    """
    _PROBE_ROUNDS = 2
    _EXPLOIT_ROUNDS = 3
    _PUNISH_ROUNDS = 2
    _ENDGAME_HORIZON = 2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._COOPERATION_THRESHOLD = math.floor(self.game_description.n_players / 3.0)
        self._FORGIVENESS_THRESHOLD = math.floor(self.game_description.n_players / 2.0)
        self._current_mode = 'Probe'
        self._rounds_in_current_mode = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self._current_mode = 'Probe'
            self._rounds_in_current_mode = 0
            return Action.C
        prev_round_my_action = history.my_actions[state.round_number - 1]
        prev_round_opponent_actions = history.opponent_actions[state.round_number - 1]
        total_cooperators_prev = np.sum(prev_round_opponent_actions) + int(prev_round_my_action)
        if state.round_number >= self.game_description.n_rounds - self._ENDGAME_HORIZON:
            self._current_mode = 'Endgame'
            return Action.D
        self._rounds_in_current_mode += 1
        if self._current_mode == 'Probe':
            if self._rounds_in_current_mode > self._PROBE_ROUNDS:
                if total_cooperators_prev >= self._COOPERATION_THRESHOLD:
                    self._current_mode = 'Exploit'
                else:
                    self._current_mode = 'Punish'
                self._rounds_in_current_mode = 0
            return Action.C
        elif self._current_mode == 'Exploit':
            if self._rounds_in_current_mode > self._EXPLOIT_ROUNDS:
                if total_cooperators_prev < self._COOPERATION_THRESHOLD:
                    self._current_mode = 'Punish'
                else:
                    self._current_mode = 'Probe'
                self._rounds_in_current_mode = 0
            return Action.D
        elif self._current_mode == 'Punish':
            if self._rounds_in_current_mode > self._PUNISH_ROUNDS:
                if total_cooperators_prev >= self._FORGIVENESS_THRESHOLD:
                    self._current_mode = 'Probe'
                else:
                    self._current_mode = 'Punish'
                self._rounds_in_current_mode = 0
            return Action.D
        elif self._current_mode == 'Endgame':
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_460(BaseStrategy):
    """
    The Calculated Free-Rider strategy for the N-Player Public Goods Game.
    Prioritizes maximizing its own payoff by free-riding on others' contributions,
    only contributing itself as a calculated measure to prevent the collapse of the public good
    if others' cooperation falls too low, and abandoning this contribution if it proves ineffective.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_coop_threshold = 0.25
        self._my_last_action = Action.D
        self._cooperation_attempt_failed = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_action: Action
        t = state.round_number + 1
        n = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if t == 1:
            current_round_action = Action.D
        elif t == n_rounds:
            current_round_action = Action.D
        else:
            C_others_prev = sum(history.opponent_actions[-1, :])
            P_coop_others_prev = C_others_prev / (n - 1)
            if self._my_last_action == Action.C and P_coop_others_prev < self.T_coop_threshold:
                self._cooperation_attempt_failed = True
            else:
                self._cooperation_attempt_failed = False
            if P_coop_others_prev < self.T_coop_threshold:
                if self._cooperation_attempt_failed:
                    current_round_action = Action.D
                else:
                    current_round_action = Action.C
            else:
                current_round_action = Action.D
        self._my_last_action = current_round_action
        return current_round_action

class Strategy_EXPLOITATIVE_461(BaseStrategy):
    """
    The Opportunistic Seeder strategy aims to maximize its own total payoff in the N-Player Public Goods Game
    by adaptively leveraging and exploiting cooperation from other players.
    It initiates with a "seeding" phase of cooperation to encourage others, then ruthlessly defects when
    the observed level of cooperation is high enough (at or above a dynamic C_target threshold).
    If cooperation dwindles, it "re-seeds" by cooperating again, viewing this as an investment for future exploitation.
    It always defects in the final round to maximize immediate gain.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.r_probe = max(2, math.floor(self.r / 10))
        self.C_target = max(1, math.floor(self.n * (self.k - 1) / (self.n - 1)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round < self.r_probe:
            return Action.C
        elif current_round == self.r - 1:
            return Action.D
        else:
            N_C_prev = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if N_C_prev >= self.C_target:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_462(BaseStrategy):
    """
    The Adaptive Opportunist strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by free-riding on others' contributions
    while strategically encouraging cooperation to create future exploitation opportunities.

    Decision Rules per Round (0-indexed):
    1. End Game Phase (Highest Priority): In the last two rounds (n_rounds - 1 or n_rounds - 2), always Defect (D).
    2. Initial Probe Phase (Second Highest Priority): In the very first round (0), Cooperate (C) to probe the environment.
       This is skipped if total rounds are 2 or less due to End Game priority.
    3. Adaptive Exploitation Phase (Lowest Priority - Intermediate Rounds): For rounds between 1 and n_rounds - 3.
       - If total cooperators in the previous round (including self) >= Exploitation_Threshold (floor(n/2)), then Defect (D).
       - Otherwise (less cooperation), Cooperate (C) to encourage future cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploitation_threshold = self.game_description.n_players // 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == total_rounds - 1 or current_round_0_indexed == total_rounds - 2:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        previous_round_0_indexed = current_round_0_indexed - 1
        cooperators_in_previous_round = np.sum(history.opponent_actions[previous_round_0_indexed, :]) + history.my_actions[previous_round_0_indexed]
        if cooperators_in_previous_round >= self.exploitation_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_463(BaseStrategy):
    """
    Adaptive Opportunistic Free-Rider (AOFR) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by free-riding on the cooperation of others.
    It performs an initial probe (C) in round 1 to assess the environment.
    In intermediate rounds, it defects if a sufficient proportion of other players cooperated
    in the previous round. If cooperation levels are too low, it temporarily cooperates
    for a limited number of rounds ('max_cooperation_attempts') to try and re-induce
    cooperation from others. If these attempts fail, it reverts to defection to avoid being
    exploited. The strategy always defects in the final round (backward induction).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.cooperation_proportion_threshold = 0.5
        self.max_cooperation_attempts = 2
        self.consecutive_low_cooperation_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            self.consecutive_low_cooperation_rounds = 0
            return Action.C
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        num_others_cooperated_prev_round = np.sum(history.opponent_actions[-1, :])
        others_coop_rate = num_others_cooperated_prev_round / (self.n_players - 1)
        current_action: Action
        if others_coop_rate >= self.cooperation_proportion_threshold:
            current_action = Action.D
            self.consecutive_low_cooperation_rounds = 0
        else:
            self.consecutive_low_cooperation_rounds += 1
            if self.consecutive_low_cooperation_rounds <= self.max_cooperation_attempts:
                current_action = Action.C
            else:
                current_action = Action.D
        return current_action

class Strategy_EXPLOITATIVE_464(BaseStrategy):
    """
    Calculated Opportunist strategy for the N-Player Public Goods Game.
    This strategy aims to extract the maximum possible payoff by primarily defecting,
    while strategically cooperating (probing) only to re-establish a sufficiently
    cooperative environment that can then be exploited. It incorporates an aggressive
    endgame phase.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.min_cooperators_to_exploit = math.ceil((self.n_players - 1) / 2)
        self.probe_frequency = max(1, math.floor(self.n_rounds / 5))
        self.endgame_horizon = max(1, math.floor(self.n_rounds / 4))
        self.rounds_since_last_probe = 0
        self.my_last_action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed > self.n_rounds - self.endgame_horizon:
            self.my_last_action = Action.D
            return Action.D
        if state.round_number == 0:
            self.my_last_action = Action.D
            return Action.D
        others_cooperated_last_round = sum(history.opponent_actions[-1, :])
        if others_cooperated_last_round >= self.min_cooperators_to_exploit:
            self.rounds_since_last_probe = 0
            self.my_last_action = Action.D
            return Action.D
        if self.my_last_action == Action.C:
            self.my_last_action = Action.D
            self.rounds_since_last_probe = 0
            return Action.D
        self.rounds_since_last_probe += 1
        if self.rounds_since_last_probe >= self.probe_frequency:
            self.rounds_since_last_probe = 0
            self.my_last_action = Action.C
            return Action.C
        else:
            self.my_last_action = Action.D
            return Action.D

class Strategy_EXPLOITATIVE_465(BaseStrategy):
    """
    Strategy Name: The Adaptive Exploiter

    Core Philosophy: This strategy aims to maximize its own total payoff by primarily defecting
    (free-riding) when others are cooperating. However, it strategically uses occasional cooperation
    as an investment to trigger or re-ignite cooperation among other players, creating more
    opportunities for future exploitation. It avoids being a "sucker" by not cooperating
    indefinitely when others are not.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.probe_rounds_left: int = 0
        self.rounds_since_last_probe_start: int = 0
        self.my_prev_action: Action | None = None
        self.probe_duration: int = 2
        self.min_rounds_between_probes: int = 3
        self.cooperation_level_low_threshold: int = 0
        self.cooperation_level_high_threshold: int = max(1, math.ceil((self.n_players - 1) * 0.2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_num = state.round_number + 1
        current_action: Action
        if current_round_num == 1:
            current_action = Action.C
            self.my_prev_action = current_action
            self.rounds_since_last_probe_start = 0
            return current_action
        C_prev_round_others = sum(history.opponent_actions[-1, :])
        end_game_start_round = self.n_rounds - min(2, self.n_rounds - 1) + 1
        if current_round_num >= end_game_start_round:
            current_action = Action.D
            self.my_prev_action = current_action
            return current_action
        if self.probe_rounds_left > 0:
            current_action = Action.C
            self.probe_rounds_left -= 1
            self.rounds_since_last_probe_start += 1
        elif C_prev_round_others >= self.cooperation_level_high_threshold:
            current_action = Action.D
            self.probe_rounds_left = 0
            self.rounds_since_last_probe_start += 1
        elif C_prev_round_others <= self.cooperation_level_low_threshold and self.rounds_since_last_probe_start >= self.min_rounds_between_probes:
            current_action = Action.C
            self.probe_rounds_left = self.probe_duration - 1
            self.rounds_since_last_probe_start = 0
        else:
            current_action = Action.D
            self.probe_rounds_left = 0
            self.rounds_since_last_probe_start += 1
        self.my_prev_action = current_action
        return current_action

class Strategy_EXPLOITATIVE_466(BaseStrategy):
    """
    Adaptive Free-Rider with Self-Preservation strategy for the N-Player Public Goods Game.
    This strategy aims to maximize individual payoff by free-riding on others' cooperation,
    while also preserving the public good by cooperating when contributions are too low,
    and protecting itself against persistent defectors. It defects in the final round
    due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.my_action_prev_round: Action | None = None
        self.total_cooperators_prev_round: int = 0
        self.cooperation_streak_others_zero: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        action_to_take: Action
        if history is not None:
            my_action_prev_bool = history.my_actions[-1]
            self.my_action_prev_round = Action.C if my_action_prev_bool else Action.D
            opponent_actions_prev_bool = history.opponent_actions[-1, :]
            self.total_cooperators_prev_round = int(my_action_prev_bool) + np.sum(opponent_actions_prev_bool)
            effective_cooperation_level_prev_round = self.total_cooperators_prev_round - (1 if my_action_prev_bool else 0)
            if effective_cooperation_level_prev_round == 0:
                self.cooperation_streak_others_zero += 1
            else:
                self.cooperation_streak_others_zero = 0
        if current_round_0_indexed == 0:
            action_to_take = Action.C
        elif current_round_0_indexed == self.r - 1:
            action_to_take = Action.D
        else:
            my_prev_action_was_C = self.my_action_prev_round == Action.C
            effective_cooperation_level_for_decision = self.total_cooperators_prev_round - (1 if my_prev_action_was_C else 0)
            if self.cooperation_streak_others_zero >= 2:
                action_to_take = Action.D
            else:
                T = max(1, math.floor(self.k / 2))
                if effective_cooperation_level_for_decision < T:
                    action_to_take = Action.C
                else:
                    action_to_take = Action.D
        self.my_action_prev_round = action_to_take
        return action_to_take

class Strategy_EXPLOITATIVE_467(BaseStrategy):
    """
    The "Grudging Free-Rider" strategy for the N-Player Public Goods Game.
    This strategy aims to opportunistically free-ride on others' cooperation.
    It starts by probing for cooperative tendencies among opponents. If cooperation is detected,
    it exploits it by defecting. If it finds itself in a situation where it's the sole cooperator ("sucker"),
    it switches to defection. If a prolonged period of universal defection occurs, it will periodically
    re-probe with cooperation to try and re-establish a more profitable (for itself) environment for exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_rounds = game_description.n_rounds
        self.N_reprobe_threshold = max(2, min(5, math.ceil(self.n_rounds / 4)))
        self.consecutive_all_defect_rounds: int = 0
        self.my_action_in_previous_round: Action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        action_to_take: Action
        if current_round_0_indexed == self.n_rounds - 1:
            action_to_take = Action.D
        elif current_round_0_indexed == 0:
            action_to_take = Action.C
        else:
            previous_round_0_indexed = current_round_0_indexed - 1
            C_others_prev = np.sum(history.opponent_actions[previous_round_0_indexed, :])
            if C_others_prev >= 1:
                action_to_take = Action.D
                self.consecutive_all_defect_rounds = 0
            elif self.my_action_in_previous_round == Action.C:
                action_to_take = Action.D
                self.consecutive_all_defect_rounds = 1
            else:
                self.consecutive_all_defect_rounds += 1
                if self.consecutive_all_defect_rounds >= self.N_reprobe_threshold:
                    action_to_take = Action.C
                    self.consecutive_all_defect_rounds = 0
                else:
                    action_to_take = Action.D
        self.my_action_in_previous_round = action_to_take
        return action_to_take

class Strategy_EXPLOITATIVE_468(BaseStrategy):
    """
    Opportunistic Provocateur: An exploitative strategy for the N-Player Public Goods Game.
    It begins with a cooperative probe, then free-rides on observed cooperation.
    If cooperation collapses after its defection, it re-provokes with temporary cooperation.
    Always defects in the end-game phases due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n_players = self.game_description.n_players
        self.C_SUSTAIN_THRESHOLD = math.ceil((n_players - 1) / 2)
        self.C_REVIVE_THRESHOLD = 0 if n_players == 2 else 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed >= total_rounds - 2:
            return Action.D
        my_previous_action_cooperated = history.my_actions[-1]
        others_previous_cooperation_count = np.sum(history.opponent_actions[-1, :])
        if others_previous_cooperation_count >= self.C_SUSTAIN_THRESHOLD:
            return Action.D
        elif my_previous_action_cooperated:
            return Action.D
        elif not my_previous_action_cooperated:
            if others_previous_cooperation_count <= self.C_REVIVE_THRESHOLD:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_469(BaseStrategy):
    """
    The "Calculated Instigator" strategy aims to maximize its own cumulative payoff by
    skillfully navigating the tension between individual rationality (defection) and
    collective benefit (cooperation). It probes for cooperation, punishes exploitation,
    strategically re-instigates cooperation when it collapses, and exploits when the
    environment is ripe. It always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        k_factor = self.game_description.k
        if state.round_number == 0:
            return Action.C
        if state.round_number == n_rounds - 1:
            return Action.D
        my_previous_action_is_C = history.my_actions[-1]
        others_previous_coop_count = np.sum(history.opponent_actions[-1, :])
        if my_previous_action_is_C:
            private_cost = 1.0 - k_factor / n_players
            public_benefit_from_others = k_factor / n_players * others_previous_coop_count
            if public_benefit_from_others < private_cost:
                return Action.D
        if others_previous_coop_count == 0:
            return Action.C
        else:
            T_exploit = max(1, math.ceil((n_players - 1) / 2))
            if others_previous_coop_count >= T_exploit:
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_470(BaseStrategy):
    """
    The "Opportunistic Inducer" strategy aims to maximize its own payoff by strategically leveraging
    cooperation from other players. It's designed to be adaptive, probing for opportunities to free-ride,
    and resilient against various opponent behaviors. It uses temporary cooperation as a tool to foster
    an environment ripe for exploitation, switching to defection as soon as the conditions are favorable.
    """
    INITIAL_INDUCTION_DURATION: int = 2
    current_mode: str
    rounds_in_current_induction_attempt: int
    COOPERATION_THRESHOLD_COUNT: int

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.COOPERATION_THRESHOLD_COUNT = self.game_description.n_players // 2
        self.current_mode = 'INDUCING'
        self.rounds_in_current_induction_attempt = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        C_prev = 0
        if state.round_number > 0:
            C_prev = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if self.current_mode == 'INDUCING':
            if state.round_number == 0:
                self.rounds_in_current_induction_attempt += 1
                return Action.C
            elif C_prev >= self.COOPERATION_THRESHOLD_COUNT:
                self.current_mode = 'EXPLOITING'
                self.rounds_in_current_induction_attempt = 0
                return Action.D
            else:
                self.rounds_in_current_induction_attempt += 1
                if self.rounds_in_current_induction_attempt >= self.INITIAL_INDUCTION_DURATION:
                    self.current_mode = 'PERMANENT_DEFECT'
                    return Action.D
                else:
                    return Action.C
        elif self.current_mode == 'EXPLOITING':
            if C_prev >= self.COOPERATION_THRESHOLD_COUNT:
                return Action.D
            else:
                self.current_mode = 'INDUCING'
                self.rounds_in_current_induction_attempt = 1
                return Action.C
        elif self.current_mode == 'PERMANENT_DEFECT':
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_471(BaseStrategy):
    """
    The "Opportunistic Defector with Baiting" strategy prioritizes immediate self-interest through defection,
    but strategically employs cooperation as a "bait" to re-induce cooperation from other players when it has
    dwindled. It is adaptive to opponent behaviors and accounts for the game's finite horizon, leading to
    unconditional defection in the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.initial_probe_rounds = 1
        self.endgame_defection_rounds = 1
        self.bait_duration_rounds = 1
        self.exploitation_threshold_count = math.ceil((self.n - 1) / 2)
        self.bait_initiation_threshold_count = max(1, math.ceil((self.n - 1) / 4))
        self.is_baiting = False
        self.rounds_into_bait = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        game_round_1_indexed = current_round_0_indexed + 1
        if game_round_1_indexed <= self.initial_probe_rounds:
            self.is_baiting = False
            self.rounds_into_bait = 0
            return Action.C
        if game_round_1_indexed > self.r - self.endgame_defection_rounds:
            self.is_baiting = False
            self.rounds_into_bait = 0
            return Action.D
        others_cooperated_prev_round = sum(history.opponent_actions[-1, :])
        if self.is_baiting:
            if self.rounds_into_bait < self.bait_duration_rounds:
                self.rounds_into_bait += 1
                return Action.C
            else:
                self.is_baiting = False
                self.rounds_into_bait = 0
                return Action.D
        elif others_cooperated_prev_round >= self.exploitation_threshold_count:
            return Action.D
        elif others_cooperated_prev_round == 0:
            return Action.D
        elif others_cooperated_prev_round <= self.bait_initiation_threshold_count:
            self.is_baiting = True
            self.rounds_into_bait = 1
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_472(BaseStrategy):
    """
    The Opportunistic Propmaker strategy for the N-Player Public Goods Game.
    This strategy aims to bait other players into cooperating by initially contributing,
    then props up cooperation when it dwindles to keep the public good alive,
    and switches to full defection to exploit whenever there's enough cooperation to free-ride on.
    It defects in the final round to maximize immediate payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.prop_up_threshold = self.n_players // 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_bool = history.my_actions[-1]
        my_contribution_prev_round = 1 if my_action_prev_round_bool else 0
        C_prev = num_opponent_cooperators_prev_round + my_contribution_prev_round
        if C_prev <= 1:
            return Action.D
        elif C_prev < self.prop_up_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_473(BaseStrategy):
    """
    The Opportunistic Provocateur strategy aims to maximize individual payoff by primarily
    free-riding on the cooperation of others. It strategically employs temporary cooperation
    only to "provoke" or "re-establish" conditions favorable for future exploitation,
    adapting to observed opponent behavior and robust across various game parameter settings.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.last_action_was_provocation: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        t = state.round_number + 1
        action_to_take = Action.D
        if t == 1:
            action_to_take = Action.D
            self.last_action_was_provocation = False
        elif t == r:
            action_to_take = Action.D
        else:
            my_prev_action_bool = history.my_actions[-1]
            opponent_prev_actions_bool = history.opponent_actions[-1, :]
            C_prev_round = int(my_prev_action_bool) + np.sum(opponent_prev_actions_bool)
            if self.last_action_was_provocation:
                action_to_take = Action.D
                self.last_action_was_provocation = False
            else:
                cooperation_threshold = max(1, math.floor(n / k))
                if C_prev_round < cooperation_threshold:
                    action_to_take = Action.C
                    self.last_action_was_provocation = True
                else:
                    action_to_take = Action.D
                    self.last_action_was_provocation = False
        return action_to_take

class Strategy_EXPLOITATIVE_474(BaseStrategy):
    """
    The Opportunistic Free-Rider strategy is designed to be highly exploitative in the
    N-Player Public Goods Game. It leverages the game's core dilemma by primarily
    defecting, but strategically cooperates only when it's anticipated to lead to
    greater long-term individual gain through free-riding.

    It employs the following decision rules:
    1.  **Initial Baiting:** Cooperate in the very first round to encourage others to cooperate.
    2.  **Endgame Exploitation:** Always defect in the final round, exploiting backward induction.
    3.  **Adaptive Exploitation (Mid-Game):**
        *   **Free-Riding:** If the number of cooperators among other players in the previous round
            is at or above a `free_ride_threshold`, defect to free-ride.
        *   **Resurrecting Cooperation:** If the number of cooperators among other players in the
            previous round falls below a `reignite_threshold`, cooperate to try and "re-seed" or
            "reignite" cooperation among others.
        *   **Default Defection:** In other cases, default to defecting to maintain the exploitative stance.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.reignite_threshold_others = max(1, math.floor((self.n - 1) * 0.25))
        self.free_ride_threshold_others = math.ceil((self.n - 1) * 0.5)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        if current_round == self.r - 1:
            return Action.D
        if current_round == 0:
            return Action.C
        num_cooperators_among_others_last_round = np.sum(history.opponent_actions[-1, :])
        if num_cooperators_among_others_last_round >= self.free_ride_threshold_others:
            return Action.D
        elif num_cooperators_among_others_last_round < self.reignite_threshold_others:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_475(BaseStrategy):
    """
    The "Calculated Coaxer" strategy is designed to be highly exploitative in the N-Player Public Goods Game.
    Its core principle is to strategically induce cooperation from other players to ensure a robust public good,
    then defect to free-ride on those contributions. It balances the need to foster cooperation with the
    imperative to maximize individual payoff, while also protecting itself from being consistently exploited.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.my_last_action: Action | None = None
        self.unrequited_cooperation_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        chosen_action: Action
        if state.round_number == 0:
            chosen_action = Action.C
            self.my_last_action = chosen_action
            self.unrequited_cooperation_streak = 0
            return chosen_action
        if state.round_number == self.n_rounds - 1:
            chosen_action = Action.D
            return chosen_action
        C_others_prev = np.sum(history.opponent_actions[-1, :])
        T_exploit_others = max(1, math.ceil((self.n_players - 1) / 2))
        if C_others_prev >= T_exploit_others:
            chosen_action = Action.D
            self.unrequited_cooperation_streak = 0
        elif self.my_last_action == Action.C:
            if C_others_prev == 0:
                self.unrequited_cooperation_streak += 1
                if self.unrequited_cooperation_streak >= 2:
                    chosen_action = Action.D
                    self.unrequited_cooperation_streak = 0
                else:
                    chosen_action = Action.C
            else:
                chosen_action = Action.C
                self.unrequited_cooperation_streak = 0
        else:
            chosen_action = Action.C
            self.unrequited_cooperation_streak = 0
        self.my_last_action = chosen_action
        return chosen_action

class Strategy_EXPLOITATIVE_476(BaseStrategy):
    """
    The Dynamic Exploiter with Restoration strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own payoff by primarily defecting to free-ride
    on others' contributions. It strategically "punctures" periods of widespread defection
    by cooperating to re-establish an environment where cooperation is more prevalent,
    which it can then exploit. This is a calculated, self-interested investment to
    improve future free-riding opportunities, not altruistic cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.W_default = 3
        self.C_min_threshold = (self.n_players - 1) / 3.0
        self.R_horizon = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number + 1
        if current_round == 1:
            return Action.D
        if current_round > self.n_rounds - self.R_horizon:
            return Action.D
        actual_W = min(state.round_number, self.W_default)
        if actual_W == 0:
            return Action.D
        recent_opponent_actions = history.opponent_actions[-actual_W:, :]
        cooperators_per_round = np.sum(recent_opponent_actions, axis=1)
        C_others_avg = np.mean(cooperators_per_round)
        if C_others_avg < self.C_min_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_477(BaseStrategy):
    """
    The Adaptive Opportunist strategy for the N-Player Public Goods Game.
    This strategy aims to foster a cooperative environment by initially contributing
    and conditionally cooperating, then to exploit that cooperation by defecting
    when the collective contribution is sufficiently high. It also guards against
    being a "sucker" by defecting when others aren't contributing.
    """
    PUNISH_THRESHOLD = 0.2
    COOPERATION_MAINTAIN_THRESHOLD = 0.5

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.C
        else:
            my_prev_action_is_C = history.my_actions[-1]
            others_C_count = np.sum(history.opponent_actions[-1, :])
            C_rate_others = others_C_count / (self.n_players - 1)
            if C_rate_others < self.PUNISH_THRESHOLD:
                return Action.D
            elif C_rate_others >= self.COOPERATION_MAINTAIN_THRESHOLD:
                if my_prev_action_is_C:
                    return Action.D
                else:
                    return Action.C
            else:
                return Action.C

class Strategy_EXPLOITATIVE_478(BaseStrategy):
    """
    Adaptive Exploiter with Sucker-Protection (AESP) strategy for N-Player Public Goods Game.
    This strategy aims to free-ride when cooperation is high,
    invest in cooperation when it's low, and protect itself from being
    a perpetual "sucker" by switching to permanent defection if its
    cooperative efforts are unreciprocated.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = self.game_description.k / self.game_description.n_players
        self.sucker_protection_threshold = 3
        self._my_last_action: Action | None = None
        self._consecutive_low_group_coop_rounds_while_i_cooperated: int = 0
        self._permanent_defect_mode: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_1_indexed == 1:
            chosen_action = Action.C
            self._my_last_action = chosen_action
            self._consecutive_low_group_coop_rounds_while_i_cooperated = 0
            self._permanent_defect_mode = False
            return chosen_action
        if current_round_1_indexed == total_rounds:
            chosen_action = Action.D
            self._my_last_action = chosen_action
            return chosen_action
        if self._permanent_defect_mode:
            chosen_action = Action.D
            self._my_last_action = chosen_action
            return chosen_action
        prev_round_idx = state.round_number - 1
        my_action_prev_round_was_C = history.my_actions[prev_round_idx]
        opponent_actions_prev_round_was_C = history.opponent_actions[prev_round_idx, :]
        n_cooperators_prev_round = np.sum(opponent_actions_prev_round_was_C)
        if my_action_prev_round_was_C:
            n_cooperators_prev_round += 1
        p_cooperators_prev_round = n_cooperators_prev_round / n_players
        if my_action_prev_round_was_C:
            if p_cooperators_prev_round < self.cooperation_threshold:
                self._consecutive_low_group_coop_rounds_while_i_cooperated += 1
            else:
                self._consecutive_low_group_coop_rounds_while_i_cooperated = 0
        else:
            self._consecutive_low_group_coop_rounds_while_i_cooperated = 0
        if self._consecutive_low_group_coop_rounds_while_i_cooperated >= self.sucker_protection_threshold:
            self._permanent_defect_mode = True
            chosen_action = Action.D
            self._my_last_action = chosen_action
            return chosen_action
        if p_cooperators_prev_round >= self.cooperation_threshold:
            chosen_action = Action.D
            self._my_last_action = chosen_action
            return chosen_action
        else:
            chosen_action = Action.C
            self._my_last_action = chosen_action
            return chosen_action

class Strategy_EXPLOITATIVE_479(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy is designed to maximize its own payoff by primarily defecting,
    but also strategically cooperating when necessary to prevent the collapse of the public good,
    thereby preserving opportunities for future exploitation.

    Decision Rules:
    1. Last Round: Defect (D) - No future interactions to influence.
    2. First Round: Cooperate (C) - Initial investment to foster a cooperative environment.
    3. Intermediate Rounds:
       - Calculate `N_C_prev`: total number of players (including self) who cooperated in the previous round.
       - Use a `Cooperation_Sustainability_Threshold (T_C) = max(1, floor(n / 3))`.
       - If `N_C_prev >= T_C`: Defect (D) - Sufficient cooperation observed, free-ride.
       - If `N_C_prev < T_C`: Cooperate (C) - Cooperation level too low, contribute to prevent collapse and
                                             rebuild for future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = max(1, math.floor(self.game_description.n_players / 3))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1]) + history.my_actions[-1]
        if num_cooperators_prev_round >= self.cooperation_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_480(BaseStrategy):
    """
    The Adaptive Free-Rider (AFR) strategy is designed to exploit others' contributions
    by primarily defecting. However, it strategically switches to temporary cooperation
    if the overall level of cooperation (public good pool) drops below a profitable
    threshold. This temporary cooperation is a calculated investment to restore
    a viable environment for future free-riding, not an act of altruism.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.Cooperation_Revival_Threshold = math.ceil(self.n / self.k)
        self.Max_Revival_Attempts_Per_Collapse = 1
        self.cooperation_revival_mode_active = False
        self.cooperation_revival_round_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number == 0:
            self.cooperation_revival_mode_active = False
            self.cooperation_revival_round_count = 0
            return Action.D
        if current_round_1_indexed == self.r:
            return Action.D
        previous_round_index = state.round_number - 1
        my_prev_action_is_C = history.my_actions[previous_round_index]
        opponent_prev_actions = history.opponent_actions[previous_round_index, :]
        N_C_prev = np.sum(opponent_prev_actions) + (1 if my_prev_action_is_C else 0)
        if N_C_prev < self.Cooperation_Revival_Threshold:
            self.cooperation_revival_mode_active = True
            self.cooperation_revival_round_count += 1
        else:
            self.cooperation_revival_mode_active = False
            self.cooperation_revival_round_count = 0
        if self.cooperation_revival_mode_active:
            if self.cooperation_revival_round_count <= self.Max_Revival_Attempts_Per_Collapse:
                return Action.C
            else:
                self.cooperation_revival_mode_active = False
                self.cooperation_revival_round_count = 0
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_481(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy for the N-Player Public Goods Game.

    This strategy aims to exploit cooperation by free-riding when others contribute,
    and make calculated efforts to re-establish cooperation when it collapses,
    always prioritizing self-interest, especially as the game nears its end.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.init_coop_rounds = 1
        self.high_coop_threshold_ratio = 0.6
        self.low_coop_threshold_ratio = 0.2
        self.max_lure_rounds = 2
        self.endgame_horizon = 2
        self._my_last_action: Action | None = None
        self._lure_back_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round = state.round_number
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        my_action: Action
        if current_round < self.init_coop_rounds:
            my_action = Action.C
            self._lure_back_streak = 0
        elif current_round >= n_rounds - self.endgame_horizon:
            my_action = Action.D
            self._lure_back_streak = 0
        else:
            my_prev_action = self._my_last_action
            C_others_prev = np.sum(history.opponent_actions[-1, :])
            cooperation_ratio = C_others_prev / (n_players - 1)
            if self._lure_back_streak > 0 and self._lure_back_streak < self.max_lure_rounds:
                my_action = Action.C
                self._lure_back_streak += 1
            elif cooperation_ratio >= self.high_coop_threshold_ratio:
                my_action = Action.D
                self._lure_back_streak = 0
            elif cooperation_ratio <= self.low_coop_threshold_ratio:
                my_action = Action.C
                self._lure_back_streak = 1
            else:
                my_action = Action.D
                self._lure_back_streak = 0
        self._my_last_action = my_action
        return my_action

class Strategy_EXPLOITATIVE_482(BaseStrategy):
    """
    "Calculated Opportunist" strategy for N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by defaulting to defection (free-riding)
    whenever possible. It strategically contributes to the public good (cooperates)
    only when the level of cooperation among other players is critically low,
    and only if that contribution is likely to "re-seed" or encourage a rebound
    in cooperation from conditional cooperators, thus restoring future free-riding potential.
    It is a self-interested "investment" rather than altruism.

    Key principles:
    1.  **Default Defection:** Primary stance is to Defect.
    2.  **Endgame Rationality:** Always defects in the final round(s).
    3.  **Adaptive Re-seeding:** Monitors total cooperation. If it drops too low (but not to zero),
        and the strategy itself defected in the previous round, it will temporarily cooperate
        to prevent complete collapse of the public good, assuming others might be conditional
        cooperators who will respond to this "investment."
    4.  **First Round Probe:** Starts by defecting to gauge opponents' initial tendencies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.LOW_COOPERATION_THRESHOLD_RATIO = 1 / 3
        self.MIN_COOPERATORS_FOR_REVIVAL = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        total_rounds = self.game_description.n_rounds
        current_round_num = state.round_number
        if current_round_num == 0:
            return Action.D
        if current_round_num == total_rounds - 1:
            return Action.D
        prev_round_index = current_round_num - 1
        prev_round_total_cooperators = np.sum(history.opponent_actions[prev_round_index, :]) + int(history.my_actions[prev_round_index])
        prev_my_action_was_C = history.my_actions[prev_round_index]
        should_strategically_cooperate = False
        if prev_round_total_cooperators >= self.MIN_COOPERATORS_FOR_REVIVAL:
            if prev_round_total_cooperators <= n_players * self.LOW_COOPERATION_THRESHOLD_RATIO:
                if not prev_my_action_was_C:
                    should_strategically_cooperate = True
        if should_strategically_cooperate:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_483(BaseStrategy):
    """
    The "Adaptive Exploiter" (AE) strategy is designed to maximize individual payoff in the
    N-Player Public Goods Game by primarily free-riding on others' contributions, while
    strategically "investing" in the public good (by cooperating) only when necessary
    to ensure a sufficiently large pool of contributions for future exploitation.
    It is robust, adaptive, and explicitly aligns with an exploitative mindset.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.T_C = math.ceil(self.n / self.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        num_coop_opponents_prev_round = np.sum(history.opponent_actions[-1, :]).item()
        num_coop_my_prev_round = 1 if history.my_actions[-1] else 0
        total_cooperators_prev_round = num_coop_opponents_prev_round + num_coop_my_prev_round
        if total_cooperators_prev_round == 0:
            return Action.D
        elif total_cooperators_prev_round < self.T_C:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_484(BaseStrategy):
    """
    Adaptive Vulture: An exploitative, adaptive, and robust strategy for the N-Player Public Goods Game.
    It prioritizes maximizing its own payoff by free-riding whenever possible,
    while strategically cooperating only when necessary to preserve the potential
    for future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == r:
            return Action.D
        my_previous_action_bool = history.my_actions[-1]
        my_previous_action = Action.C if my_previous_action_bool else Action.D
        total_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (1 if my_previous_action_bool else 0)
        cooperation_threshold_for_exploitation = math.ceil(n / 2)
        if my_previous_action == Action.D:
            if total_cooperators_prev_round < cooperation_threshold_for_exploitation:
                return Action.C
            else:
                return Action.D
        elif total_cooperators_prev_round < cooperation_threshold_for_exploitation:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_485(BaseStrategy):
    """
    The Opportunistic Reviver strategy aims to maximize its own payoff in a N-Player Public Goods Game
    by primarily defecting (free-riding) but selectively cooperating to prevent the complete collapse
    of cooperation, especially if its own recent defection might have contributed to a significant decline.
    This approach is highly exploitative while maintaining adaptability and robustness.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_previous_action: Action = Action.D
        self.C_t_minus_1: int = 0
        self.C_t_minus_2: int = 0
        n = game_description.n_players
        self.MIN_COOPERATION_TO_REVIVE: int = max(1, math.floor(n * 0.1))
        self.DECLINE_FACTOR: float = 0.5
        self.REFERENCE_LEVEL_FOR_DECLINE: int = max(1, math.floor(n * 0.25))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n_rounds = self.game_description.n_rounds
        chosen_action: Action
        if current_round_number == 0:
            chosen_action = Action.D
        elif current_round_number == n_rounds - 1:
            chosen_action = Action.D
        else:
            my_action_in_prev_round_was_C_count = 1 if history.my_actions[-1] else 0
            opponent_cooperators_in_prev_round = sum(history.opponent_actions[-1, :])
            actual_C_t_minus_1_value = my_action_in_prev_round_was_C_count + opponent_cooperators_in_prev_round
            self.C_t_minus_2 = self.C_t_minus_1
            self.C_t_minus_1 = actual_C_t_minus_1_value
            if self.C_t_minus_1 == 0:
                chosen_action = Action.D
            elif self.my_previous_action == Action.D and self.C_t_minus_2 >= self.REFERENCE_LEVEL_FOR_DECLINE and (self.C_t_minus_1 < self.C_t_minus_2 * self.DECLINE_FACTOR) and (self.C_t_minus_1 >= self.MIN_COOPERATION_TO_REVIVE):
                chosen_action = Action.C
            else:
                chosen_action = Action.D
        self.my_previous_action = chosen_action
        return chosen_action

class Strategy_EXPLOITATIVE_486(BaseStrategy):
    """
    Opportunistic Prowler strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by leveraging the game structure
    and observed opponent behavior. It primarily defaults to defecting, but it
    incorporates a mechanism to "probe" for cooperation from other players when
    a sustained lack of cooperation leads to a low-payoff equilibrium (everyone defecting).
    If a probe successfully elicits cooperation, the strategy immediately reverts
    to exploiting that cooperation by free-riding. It also recognizes the end-game
    and always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Opportunistic Prowler strategy.

        Args:
            game_description: An object containing game parameters such as
                              n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self._rounds_since_others_cooperated = 0
        self._probe_interval = game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decides the action (Cooperate or Defect) for the current round.

        Args:
            state: Contains information about the current round number (0-indexed).
            history: Contains records of previous rounds' actions and payoffs.
                     Is None for the first round (round_number = 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == total_rounds - 1:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.D
        my_action_prev_round_is_cooperate = history.my_actions[-1]
        num_others_cooperated_prev_round = sum(history.opponent_actions[-1, :])
        action_for_this_round = Action.D
        if num_others_cooperated_prev_round > 0:
            action_for_this_round = Action.D
            self._rounds_since_others_cooperated = 0
        else:
            self._rounds_since_others_cooperated += 1
            if self._rounds_since_others_cooperated >= self._probe_interval and (not my_action_prev_round_is_cooperate):
                action_for_this_round = Action.C
                self._rounds_since_others_cooperated = 0
            else:
                action_for_this_round = Action.D
        return action_for_this_round

class Strategy_EXPLOITATIVE_487(BaseStrategy):
    """
    Optimized Exploiter strategy for the N-Player Public Goods Game.

    This strategy aims to maximize personal payoff by intelligently probing for cooperation,
    exploiting it when it reaches a profitable level, and strategically re-investing in
    cooperation if it dwindles. It starts with an initial period of cooperation, defects
    when the observed cooperation level exceeds a calculated threshold, and re-cooperates
    if the level drops below this threshold. It always defects in the final round.
    """
    INITIAL_COOPERATION_ROUNDS = 2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.COOPERATION_TARGET = math.ceil(self.n_players * (1 - 1 / self.k))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        if current_round_1_indexed <= self.INITIAL_COOPERATION_ROUNDS:
            return Action.C
        cooperators_prev_round = int(history.my_actions[-1]) + np.sum(history.opponent_actions[-1, :])
        if cooperators_prev_round >= self.COOPERATION_TARGET:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_488(BaseStrategy):
    """
    Relentless Opportunist: This strategy aims to free-ride on cooperative players.
    It defaults to defection, but uses an initial probe (cooperating in round 1)
    to test the cooperation levels of others. In subsequent rounds, it defects if
    overall cooperation is high (exploitation mode) or moderate. If cooperation
    is very low, the strategy enters "damage control" and might strategically
    cooperate ("bait") only if its prior cooperation demonstrably increased the
    overall level of cooperation, aiming to rebuild the public good for future
    exploitation. The final round is always exploited by defecting, as there are
    no future interactions to consider.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.EXPLOITATION_THRESHOLD_RATE = 0.6
        self.DAMAGE_CONTROL_THRESHOLD_RATE = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_game_round_1_indexed = state.round_number + 1
        if current_game_round_1_indexed == 1:
            return Action.C
        if current_game_round_1_indexed == self.n_rounds:
            return Action.D
        my_last_action_was_C = history.my_actions[-1]
        total_cooperators_prev_round = int(my_last_action_was_C) + np.sum(history.opponent_actions[-1, :])
        cooperation_rate_prev_round = total_cooperators_prev_round / self.n_players
        total_cooperators_two_rounds_ago = -1
        if current_game_round_1_indexed >= 3:
            my_two_rounds_ago_action_was_C = history.my_actions[-2]
            total_cooperators_two_rounds_ago = int(my_two_rounds_ago_action_was_C) + np.sum(history.opponent_actions[-2, :])
        if cooperation_rate_prev_round >= self.EXPLOITATION_THRESHOLD_RATE:
            return Action.D
        elif cooperation_rate_prev_round < self.DAMAGE_CONTROL_THRESHOLD_RATE:
            if my_last_action_was_C and current_game_round_1_indexed >= 3 and (total_cooperators_prev_round > total_cooperators_two_rounds_ago):
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_489(BaseStrategy):
    """
    Adaptive Exploiter strategy for the N-Player Public Goods Game.
    It aims to "farm" cooperation from other players by initially signaling willingness to cooperate,
    then strategically defecting when cooperation levels are high to maximize individual payoff.
    If cooperation falters, it will attempt to punish non-cooperators (by defecting) or re-seed
    cooperation (by contributing) to restore a public good pool that can be exploited.
    Always defects in the final round due to backward induction.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold_ratio = 0.5
        self.punishment_rounds_on_low_cooperation = 1
        self.rounds_since_low_cooperation = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            return Action.C
        if current_round_1_indexed == n_rounds:
            return Action.D
        previous_round_my_action = history.my_actions[state.round_number - 1]
        previous_round_opponent_actions = history.opponent_actions[state.round_number - 1, :]
        num_cooperators_prev_round = int(previous_round_my_action) + sum(previous_round_opponent_actions)
        current_cooperation_ratio = num_cooperators_prev_round / n_players
        if current_cooperation_ratio >= self.cooperation_threshold_ratio:
            self.rounds_since_low_cooperation = 0
            return Action.D
        else:
            self.rounds_since_low_cooperation += 1
            if self.rounds_since_low_cooperation <= self.punishment_rounds_on_low_cooperation:
                return Action.D
            else:
                self.rounds_since_low_cooperation = 0
                return Action.C

class Strategy_EXPLOITATIVE_490(BaseStrategy):
    """
    The "Shrewd Opportunist" strategy for the N-Player Public Goods Game.
    This strategy is designed to be exploitative, adaptive, and robust,
    primarily aiming to maximize its own payoff by free-riding.
    It probes for cooperators, aggressively free-rides when sufficient
    cooperation is observed, strategically re-baits with conditional
    cooperation to rebuild exploitable public good, and protects itself
    by defecting in the endgame or when cooperation is too low.
    """
    EXPLOIT_THRESHOLD_PROPORTION = 0.5
    FORGIVENESS_THRESHOLD_PROPORTION = 0.25

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.endgame_rounds_count = max(1, min(self.n_rounds - 1, 2))
        num_other_players = self.n_players - 1
        self.exploit_threshold_count = math.ceil(num_other_players * self.EXPLOIT_THRESHOLD_PROPORTION)
        self.forgiveness_threshold_count = math.ceil(num_other_players * self.FORGIVENESS_THRESHOLD_PROPORTION)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number >= self.n_rounds - self.endgame_rounds_count:
            return Action.D
        if state.round_number == 0:
            return Action.C
        num_other_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        if num_other_cooperators_prev_round >= self.exploit_threshold_count:
            return Action.D
        elif num_other_cooperators_prev_round >= self.forgiveness_threshold_count:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_491(BaseStrategy):
    """
    The Calculated Free-Rider strategy aims to exploit the cooperation of others while
    strategically contributing just enough to prevent the collapse of the public good.
    It starts with cooperation, then adapts its behavior based on observed cooperation
    levels, free-riding when possible and contributing when cooperation is too low
    or when its own cooperation isn't sufficiently reciprocated. In the final round,
    it always defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.cooperation_streak_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        total_rounds = self.game_description.n_rounds
        n = self.game_description.n_players
        if current_round_1_indexed == 1:
            self.my_last_action = Action.C
            self.cooperation_streak_count = 1
            return Action.C
        if current_round_1_indexed == total_rounds:
            self.my_last_action = Action.D
            self.cooperation_streak_count = 0
            return Action.D
        prev_round_idx = current_round_0_indexed - 1
        my_prev_action_was_C = history.my_actions[prev_round_idx]
        opponents_cooperated_prev_round = np.sum(history.opponent_actions[prev_round_idx, :])
        n_c_prev = int(opponents_cooperated_prev_round) + (1 if my_prev_action_was_C else 0)
        threshold_exploit_from_defect = math.floor(n * 0.5)
        threshold_exploit_from_cooperate = math.floor(n * 0.6)
        if self.my_last_action == Action.D:
            if n_c_prev >= threshold_exploit_from_defect:
                self.my_last_action = Action.D
                self.cooperation_streak_count = 0
                return Action.D
            else:
                self.my_last_action = Action.C
                self.cooperation_streak_count = 1
                return Action.C
        elif self.my_last_action == Action.C:
            if n_c_prev >= threshold_exploit_from_cooperate:
                self.my_last_action = Action.D
                self.cooperation_streak_count = 0
                return Action.D
            else:
                self.cooperation_streak_count += 1
                if self.cooperation_streak_count >= 2:
                    self.my_last_action = Action.D
                    self.cooperation_streak_count = 0
                    return Action.D
                else:
                    self.my_last_action = Action.C
                    return Action.C

class Strategy_EXPLOITATIVE_492(BaseStrategy):
    """
    The Opportunistic Predator strategy for the N-Player Public Goods Game.
    This strategy is designed to be highly exploitative, aiming to maximize its own payoff
    by selectively free-riding on others' contributions, while being robust to opponent behaviors
    and adaptive to observed game history. It prioritizes initial probing, aggressive exploitation,
    and self-preservation in the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.coop_trigger_threshold = math.floor((self.n - 1) / 2) + 1
        self.endgame_rounds_count = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_t = state.round_number + 1
        if self.r <= self.endgame_rounds_count:
            return Action.D
        if current_round_t == 1:
            return Action.C
        if current_round_t > self.r - self.endgame_rounds_count:
            return Action.D
        C_others_prev_round = sum(history.opponent_actions[state.round_number - 1, :])
        if C_others_prev_round >= self.coop_trigger_threshold:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_493(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy for the N-Player Public Goods Game.
    This strategy aims to maximize payoff by balancing initial cooperation to establish a public good,
    then free-riding on others' contributions, while also preventing collapse and exploiting the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.r_explore = 2
        self.r_endgame = 1
        self.threshold_coop = max(1, math.floor((self.n_players - 1) * 0.5))
        self.my_last_action: Action | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed <= self.r_explore:
            return Action.C
        if current_round_1_indexed > self.n_rounds - self.r_endgame:
            return Action.D
        my_last_action_bool = history.my_actions[-1]
        self.my_last_action = Action.C if my_last_action_bool else Action.D
        C_others_prev = int(np.sum(history.opponent_actions[-1, :]))
        if C_others_prev >= self.threshold_coop:
            return Action.D
        elif self.my_last_action == Action.C:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_494(BaseStrategy):
    """
    The Opportunistic Feeder strategy for the N-Player Public Goods Game.

    This strategy aims to maximize individual payoff by predominantly defecting
    when there's a sufficient pool of cooperators to exploit. It strategically
    cooperates ("feeds") when overall cooperation is low and the player has
    been defecting for too long, to prevent total collapse of the public good
    and ensure future opportunities for exploitation.

    Internal State:
    - rounds_since_my_last_cooperation: Tracks consecutive rounds this player has defected.

    Tunable Parameters:
    - COOPERATION_THRESHOLD_RATIO: Proportion of players cooperating in the previous
      round required for this player to defect.
    - FEEDER_PATIENCE: Number of consecutive defect rounds allowed when cooperation
      is low before this player cooperates to "feed" the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.rounds_since_my_last_cooperation: int = 0
        self.COOPERATION_THRESHOLD_RATIO: float = 0.5
        self.FEEDER_PATIENCE: int = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.rounds_since_my_last_cooperation = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        total_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        n_players = self.game_description.n_players
        observed_cooperation_ratio = total_cooperators_prev_round / n_players
        chosen_action: Action
        if observed_cooperation_ratio >= self.COOPERATION_THRESHOLD_RATIO:
            chosen_action = Action.D
            self.rounds_since_my_last_cooperation += 1
        elif self.rounds_since_my_last_cooperation < self.FEEDER_PATIENCE:
            chosen_action = Action.D
            self.rounds_since_my_last_cooperation += 1
        else:
            chosen_action = Action.C
            self.rounds_since_my_last_cooperation = 0
        return chosen_action

class Strategy_EXPLOITATIVE_495(BaseStrategy):
    """
    The Opportunist's Initial Gamble is an exploitative strategy for the N-Player Public Goods Game.
    It initiates cooperation in the first round as a strategic gamble to probe for conditional
    or unconditional cooperators. From the second round onwards, it commits to consistent defection
    to maximize individual payoff by free-riding on any existing contributions or by reverting
    to the dominant strategy to minimize losses if the initial probe was unsuccessful.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_496(BaseStrategy):
    """
    The "Calculated Exploiter" (CE) strategy for the N-Player Public Goods Game.
    This strategy balances short-term exploitation with long-term strategic inducement of cooperation.
    It probes for cooperation initially, sustains it when a sufficient number of others cooperate,
    and exploits immediately when cooperation is low or in the final rounds of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Calculated Exploiter strategy.
        Calculates dynamic parameters based on game_description.
        """
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.MinCooperationForMyCooperation = max(1, math.floor(n * 0.4))
        self.EndgameHorizon = min(max(1, math.floor(r / 4)), 3)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the strategy's action (Cooperate or Defect) for the current round.

        Args:
            state (GameState): Current state of the game, including the current round number (0-indexed).
            history (PlayerHistory | None): History of actions and payoffs from previous rounds.
                                            None for the first round (state.round_number == 0).

        Returns:
            Action: Action.C for Cooperate, Action.D for Defect.
        """
        current_round_1_indexed = state.round_number + 1
        total_rounds = self.game_description.n_rounds
        if current_round_1_indexed > total_rounds - self.EndgameHorizon:
            return Action.D
        elif state.round_number == 0:
            return Action.C
        else:
            C_others_prev = np.sum(history.opponent_actions[state.round_number - 1, :])
            if C_others_prev >= self.MinCooperationForMyCooperation:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_497(BaseStrategy):
    """
    The "Adaptive Opportunist" strategy aims to maximize individual payoff in the
    N-Player Public Goods Game. It starts with cooperation to probe the environment.
    In middle rounds, it uses dynamic thresholds to decide:
    - Defect if cooperation from others is too low (below CT) to be worth supporting.
    - Defect if cooperation from others is robust (at or above ET) to free-ride.
    - Cooperate in an intermediate zone to strategically maintain a viable public good.
    The strategy always defects in the final round to exploit the lack of future consequences.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.CT = max(1, math.floor((self.n - 1) / 3))
        self.ET = self.n - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed == self.r - 1:
            return Action.D
        prev_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
        C_others_prev = sum(prev_round_opponent_actions)
        if C_others_prev < self.CT:
            return Action.D
        elif C_others_prev >= self.ET:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_498(BaseStrategy):
    """
    The Exploitative Opportunist is a self-serving strategy designed to maximize its individual payoff by primarily free-riding on the contributions of other players. It strategically defects as much as possible, only contributing minimally and temporarily when the collective good is at critical risk of collapsing. This measured cooperation aims to re-establish a sufficient level of public good for subsequent exploitation. The strategy explicitly accounts for the finite nature of the game by adopting an uncooperative stance in the initial and final rounds, where incentives for long-term cooperation are absent.
    """
    COOPERATION_TRIGGER_THRESHOLD = 2

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the Exploitative Opportunist strategy with game parameters.

        Args:
            game_description: An object containing game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the strategy's action (Cooperate or Defect) for the current round.

        Args:
            state: An object containing the current round number (0-indexed).
            history: An object containing records of past actions and payoffs for all players.
                     Is None for the very first round (round_number = 0).

        Returns:
            An Action enum (Action.C for Cooperate, Action.D for Defect).
        """
        current_round_index = state.round_number
        if current_round_index == 0:
            return Action.D
        if current_round_index == self.n_rounds - 1:
            return Action.D
        else:
            num_other_cooperators_prev = np.sum(history.opponent_actions[current_round_index - 1, :])
            if num_other_cooperators_prev < self.COOPERATION_TRIGGER_THRESHOLD:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_499(BaseStrategy):
    """
    Opportunistic Reciprocator strategy for the N-Player Public Goods Game.
    This strategy aims to free-ride on the cooperation of others as much as possible,
    only engaging in strategic cooperation when it's deemed necessary to "re-seed"
    or encourage more widespread cooperation for future exploitation. It is acutely
    aware of the dominant strategy (Defect) and the finite nature of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.COOPERATION_EXPECTATION_THRESHOLD = 0.5
        self.PATIENCE_LIMIT = 3
        self.rounds_since_last_own_cooperation = 0
        self.num_other_players = self.game_description.n_players - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_round_idx = state.round_number - 1
        last_my_action_was_cooperate = history.my_actions[previous_round_idx]
        if last_my_action_was_cooperate:
            self.rounds_since_last_own_cooperation = 0
        else:
            self.rounds_since_last_own_cooperation += 1
        num_others_cooperated = np.sum(history.opponent_actions[previous_round_idx, :])
        cooperation_ratio_others_prev = num_others_cooperated / self.num_other_players
        if not last_my_action_was_cooperate:
            if cooperation_ratio_others_prev >= self.COOPERATION_EXPECTATION_THRESHOLD:
                return Action.D
            elif self.rounds_since_last_own_cooperation >= self.PATIENCE_LIMIT and state.round_number < self.game_description.n_rounds - 2:
                self.rounds_since_last_own_cooperation = 0
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_500(BaseStrategy):
    """
    Adaptive Free-Rider (AFR) strategy for the N-Player Public Goods Game.

    This strategy aims to maximize its own payoff by primarily defecting (free-riding)
    but strategically cooperates in specific scenarios to "re-seed" collective
    cooperation if it has collapsed, thereby creating a larger pool of public good
    to exploit in subsequent rounds. It is designed to never be a perpetual cooperator
    and to quickly revert to defection after any cooperative gesture.

    State Variables:
    - n, r, k: Game parameters derived from game_description.
    - last_action_was_reseed_C: A boolean flag, True if the player's action
      in the immediately preceding round was 'C' specifically because of the
      "re-seeding" rule (Rule 3b.i of the strategy description). Initialized
      to False at the start of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.last_action_was_reseed_C = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            action = Action.C
            self.last_action_was_reseed_C = False
            return action
        if current_round_1_indexed == self.r:
            action = Action.D
            self.last_action_was_reseed_C = False
            return action
        C_others_prev = np.sum(history.opponent_actions[-1, :])
        if self.last_action_was_reseed_C:
            action = Action.D
            self.last_action_was_reseed_C = False
            return action
        elif C_others_prev <= 1:
            action = Action.C
            self.last_action_was_reseed_C = True
            return action
        else:
            action = Action.D
            self.last_action_was_reseed_C = False
            return action

class Strategy_EXPLOITATIVE_501(BaseStrategy):
    """
    The Adaptive Exploiter strategy for the N-Player Public Goods Game.
    This strategy aims to maximize its own long-term payoff by primarily defecting,
    but intelligently choosing to cooperate when necessary to cultivate or restore
    group cooperation, thereby ensuring a pool of potential free-riding targets.
    It leans into the free-rider problem while recognizing the benefits of a
    functioning public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_0_indexed == 0:
            self.consecutive_defections = 0
            return Action.C
        if current_round_1_indexed == self.r:
            return Action.D
        prev_round_others_actions = history.opponent_actions[-1, :]
        num_other_cooperators_prev_round = sum(prev_round_others_actions)
        T_exploit_others = max(1, math.floor((self.n - 1) * (1 - self.k / self.n)))
        rounds_to_end_inclusive = self.r - current_round_1_indexed
        Max_Consecutive_Defections = max(1, math.floor(rounds_to_end_inclusive * (self.k / self.n)))
        Max_Consecutive_Defections = min(Max_Consecutive_Defections, rounds_to_end_inclusive)
        my_action = Action.D
        if num_other_cooperators_prev_round == 0:
            my_action = Action.C
            self.consecutive_defections = 0
        elif num_other_cooperators_prev_round < T_exploit_others:
            my_action = Action.C
            self.consecutive_defections = 0
        elif self.consecutive_defections < Max_Consecutive_Defections:
            my_action = Action.D
            self.consecutive_defections += 1
        else:
            my_action = Action.C
            self.consecutive_defections = 0
        return my_action

class Strategy_EXPLOITATIVE_502(BaseStrategy):
    """
    The Opportunistic Predator strategy for the N-Player Public Goods Game.
    This strategy strategically alternates between cooperation (to foster a public good)
    and defection (to free-ride on it), while always prioritizing end-game exploitation.
    It is adaptive to the observed behavior of opponents and robust to different game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        self.initial_coop_rounds = min(2, r - 1)
        self.endgame_rounds = min(2, r - 1)
        scaled_k = (k - 1) / (n - 1)
        cooperation_threshold_percentage = 0.2 + 0.6 * scaled_k
        self.absolute_cooperation_threshold = n * cooperation_threshold_percentage

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if state.round_number >= r - self.endgame_rounds:
            return Action.D
        elif state.round_number < self.initial_coop_rounds:
            return Action.C
        else:
            num_past_rounds = state.round_number
            total_cooperators_in_history = 0
            total_cooperators_in_history += np.sum(history.my_actions[:num_past_rounds])
            total_cooperators_in_history += np.sum(history.opponent_actions[:num_past_rounds, :])
            average_cooperators_per_round = total_cooperators_in_history / num_past_rounds
            if average_cooperators_per_round < self.absolute_cooperation_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_503(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the N-Player Public Goods Game.
    Prioritizes self-payoff maximization by free-riding, but strategically cooperates
    to prevent the collapse of collective cooperation that would ultimately harm its
    own long-term gains.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action: Action | None = None
        self.num_other_cooperators_last_round: int | None = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        k_factor = self.game_description.k
        if state.round_number > 0 and history is not None:
            last_round_idx = state.round_number - 1
            self.my_last_action = Action.C if history.my_actions[last_round_idx] else Action.D
            self.num_other_cooperators_last_round = sum(history.opponent_actions[last_round_idx, :])
        action: Action
        if state.round_number == 0:
            action = Action.C
        elif state.round_number == n_rounds - 1:
            action = Action.D
        else:
            C_others_prev = self.num_other_cooperators_last_round
            my_prev_action = self.my_last_action
            if C_others_prev * (k_factor / n_players) >= 1:
                action = Action.D
            elif C_others_prev == 0:
                action = Action.D
            elif my_prev_action == Action.C:
                action = Action.D
            else:
                action = Action.C
        return action

class Strategy_EXPLOITATIVE_504(BaseStrategy):
    """
    The Adaptive Inducer and Exploiter (AIE) strategy aims to maximize its own total payoff in the N-Player
    Public Goods Game by strategically inducing and then free-riding on the cooperation of other players,
    while avoiding being exploited itself. It uses initial cooperation to test the environment,
    then leverages a calculated defection probability when sufficient cooperation is observed,
    and withdraws cooperation if others don't contribute enough. It always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.INITIAL_COOP_ROUNDS = 2
        self.MIN_COOPERATORS_THRESHOLD = max(1, math.floor((self.n - 1) / 2))
        self.EXPLOIT_DEFECT_PROBABILITY = 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == self.r:
            return Action.D
        num_other_cooperators_prev_round = 0
        if current_round_1_indexed > 1:
            num_other_cooperators_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
        if current_round_1_indexed <= self.INITIAL_COOP_ROUNDS:
            if current_round_1_indexed == 1:
                return Action.C
            elif num_other_cooperators_prev_round == 0:
                return Action.D
            else:
                return Action.C
        if num_other_cooperators_prev_round >= self.MIN_COOPERATORS_THRESHOLD:
            rand_val = random.random()
            if rand_val < self.EXPLOIT_DEFECT_PROBABILITY:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_505(BaseStrategy):
    """
    The Strategic Opportunist strategy is designed to be highly exploitative within the N-Player
    Public Goods Game. It prioritizes maximizing its own payoff by largely free-riding on the
    contributions of others, while strategically making minimal, calculated interventions only
    when necessary to prevent the complete collapse of public goods, thereby creating more
    opportunities for future exploitation. It fully accounts for the finite nature of the game
    and the implications of backward induction, applying endgame ruthlessness. It only cooperates
    to "re-seed" cooperation when public goods have entirely collapsed, otherwise, it defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed >= self.n_rounds - 2:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.D
        num_opponent_cooperators_prev_round = np.sum(history.opponent_actions[-1, :])
        my_action_prev_round_was_C = history.my_actions[-1]
        total_cooperators_prev_round = num_opponent_cooperators_prev_round + my_action_prev_round_was_C
        if total_cooperators_prev_round == 0:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_506(BaseStrategy):
    """
    Strategy: The Opportunistic Leech.

    This strategy aims to maximize its own payoff by primarily free-riding on the
    contributions of others. It only contributes (cooperates) as a calculated
    investment: either to probe the environment initially, or to "re-seed" the
    public good when others' contributions drop too low, aiming to restore future
    free-riding opportunities. If cooperation attempts prove futile, it quickly
    reverts to defection to minimize personal losses.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        k = self.game_description.k
        calculated_threshold = round((n - 1) * (1 - k / n))
        self.n_min_coopers = max(1, calculated_threshold)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        current_round_0_indexed = state.round_number
        current_round_1_indexed = current_round_0_indexed + 1
        if current_round_1_indexed == r:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        our_prev_action_was_C = history.my_actions[-1]
        c_others_prev = np.sum(history.opponent_actions[-1, :])
        if c_others_prev >= self.n_min_coopers:
            return Action.D
        elif our_prev_action_was_C and c_others_prev <= 1:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_507(BaseStrategy):
    """
    The "Calculated Exploiter" strategy aims to maximize personal payoff by aggressively free-riding on the
    contributions of others. It strategically "invests" in cooperation only when necessary to prevent the
    collapse of the public good, thereby preserving future exploitation opportunities, and cuts losses
    if cooperation is non-existent.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.CST = math.ceil(self.n / self.k)
        self.EOT = self.n - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        if current_round_zero_indexed == 0:
            return Action.C
        if current_round_zero_indexed == self.r - 1:
            return Action.D
        my_last_action_was_C = history.my_actions[-1]
        total_opponent_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        C_prev = total_opponent_cooperators_prev + (1 if my_last_action_was_C else 0)
        if C_prev == 0:
            return Action.D
        if C_prev >= self.EOT:
            return Action.D
        if C_prev < self.CST:
            return Action.C
        if my_last_action_was_C:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_508(BaseStrategy):
    """
    The Adaptive Opportunist (AO) strategy prioritizes individual gain by strategically free-riding on others'
    contributions. It is designed to be highly adaptive, robust, and explicitly exploitative.
    It makes minimal, calculated investments in cooperation to "prime the pump" and encourage others,
    but its default stance is to defect, seizing opportunities to gain the private bonus.
    It's patient in waiting for others to cooperate but will eventually attempt to re-seed cooperation
    if the public good completely collapses.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.COOP_THRESHOLD_FOR_EXPLOIT: int = max(1, math.ceil(self.n / self.k))
        self.SEED_COOP_ROUNDS: int = 2
        self.DEFECT_PATIENCE_ROUNDS: int = 3
        self._my_last_action: Action | None = None
        self._my_current_coop_streak: int = 0
        self._my_current_defect_streak: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed: int = state.round_number
        if current_round_0_indexed == 0:
            action = Action.C
            self._my_last_action = action
            self._my_current_coop_streak = 1
            self._my_current_defect_streak = 0
            return action
        if current_round_0_indexed == self.r - 1:
            action = Action.D
            self._my_last_action = action
            self._my_current_coop_streak = 0
            self._my_current_defect_streak += 1
            return action
        C_others_prev: int = np.sum(history.opponent_actions[-1, :])
        if C_others_prev >= self.COOP_THRESHOLD_FOR_EXPLOIT:
            action = Action.D
            self._my_last_action = action
            self._my_current_coop_streak = 0
            self._my_current_defect_streak += 1
            return action
        elif self._my_last_action == Action.C and self._my_current_coop_streak < self.SEED_COOP_ROUNDS:
            action = Action.C
            self._my_last_action = action
            self._my_current_coop_streak += 1
            self._my_current_defect_streak = 0
            return action
        elif self._my_current_defect_streak >= self.DEFECT_PATIENCE_ROUNDS:
            action = Action.C
            self._my_last_action = action
            self._my_current_coop_streak = 1
            self._my_current_defect_streak = 0
            return action
        else:
            action = Action.D
            self._my_last_action = action
            self._my_current_coop_streak = 0
            self._my_current_defect_streak += 1
            return action

class Strategy_EXPLOITATIVE_509(BaseStrategy):
    """
    The "Adaptive Free-Rider" strategy for the N-Player Public Goods Game.

    This strategy is designed to opportunistically exploit periods of high cooperation by free-riding
    (defecting), but also to strategically contribute (cooperate) when cooperation levels drop too low,
    threatening its own long-term payoff. It uses two dynamically calculated thresholds
    based on the game parameters (n, k) to make these decisions.

    Decision Logic:
    1. Initial Round (round_number=0, t=1 in description): Always Cooperate (C) to signal willingness and probe.
    2. Last Round (round_number=n_rounds-1, t=r in description): Always Defect (D) due to backward induction.
    3. Intermediate Rounds:
       - Uses `THRESHOLD_START_EXPLOITING`: The minimum number of cooperators from the previous round
         needed for a defecting player's payoff to be as good as or better than the full cooperation scenario.
       - Uses `THRESHOLD_REBUILD_COOPERATION`: The minimum number of cooperators from the previous round
         below which the public good component becomes too small, requiring intervention (cooperation)
         to prevent collapse.
       - It tracks an `exploiting_state` flag:
         - If `exploiting_state` is True (currently free-riding), it continues to defect unless cooperation
           falls below `THRESHOLD_REBUILD_COOPERATION`, then it switches to cooperate and sets `exploiting_state=False`.
         - If `exploiting_state` is False (currently building/rebuilding), it continues to cooperate unless
           cooperation reaches or exceeds `THRESHOLD_START_EXPLOITING`, then it switches to defect and sets `exploiting_state=True`.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        k = game_description.k
        self.THRESHOLD_START_EXPLOITING = math.ceil(n / k * (k - 1))
        self.THRESHOLD_REBUILD_COOPERATION = math.floor(n / k)
        self.THRESHOLD_START_EXPLOITING = max(0, self.THRESHOLD_START_EXPLOITING)
        self.THRESHOLD_REBUILD_COOPERATION = max(0, self.THRESHOLD_REBUILD_COOPERATION)
        self.exploiting_state = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_zero_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_zero_indexed == 0:
            return Action.C
        elif current_round_zero_indexed == total_rounds - 1:
            return Action.D
        else:
            last_round_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            if self.exploiting_state:
                if last_round_cooperators < self.THRESHOLD_REBUILD_COOPERATION:
                    self.exploiting_state = False
                    return Action.C
                else:
                    return Action.D
            elif last_round_cooperators >= self.THRESHOLD_START_EXPLOITING:
                self.exploiting_state = True
                return Action.D
            else:
                return Action.C

class Strategy_EXPLOITATIVE_510(BaseStrategy):
    """
    The Conditional Exploiter strategy prioritizes free-riding in the N-Player Public Goods Game.
    It contributes (cooperates) only when the observed level of cooperation falls below a dynamically
    calculated threshold, signaling a risk of public good collapse. This strategic cooperation
    aims to re-seed future cooperation, enabling further exploitation. In the first round, it
    cooperates as an initial probe. In the final round, it always defects to maximize immediate gain.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.coop_threshold = max(1, round(self.n_players / self.k_factor))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        elif current_round_0_indexed == 0:
            return Action.C
        else:
            c_prev_total_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if c_prev_total_cooperators < self.coop_threshold:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_511(BaseStrategy):
    """
    The Schemer is an exploitative, adaptive, and robust strategy for the N-Player Public Goods Game.
    It aims to maximize its own payoff by intelligently leveraging cooperative behavior from other
    players while protecting itself from being exploited.

    The strategy operates in three distinct phases:
    1.  Initial Probing: Cooperates in the very first round to test the waters and signal potential cooperation.
    2.  End Game Phase: Strictly defects in the final rounds, applying backward induction logic to maximize
        payoffs when the "shadow of the future" is short.
    3.  Strategic Adaptive Phase: In intermediate rounds, it dynamically adjusts its action based on the
        observed level of cooperation among opponents in the preceding round:
        -   Defects (exploits) if cooperation is high (>= 50% of players cooperated).
        -   Defects (self-protects) if cooperation is very low (<= 20% of players cooperated).
        -   Cooperates (maintains) if cooperation is moderate, attempting to sustain a viable pool
            of cooperators for future exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.PROBE_ROUNDS = 1
        self.ENDGAME_ROUNDS = min(2, self.game_description.n_rounds - 1)
        self.THRESHOLD_HIGH_COOP = math.floor(self.game_description.n_players * 0.5)
        self.THRESHOLD_LOW_COOP = math.floor(self.game_description.n_players * 0.2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds = self.game_description.n_rounds
        if current_round_0_indexed == 0:
            return Action.C
        if current_round_0_indexed >= total_rounds - self.ENDGAME_ROUNDS:
            return Action.D
        previous_round_opponent_actions = history.opponent_actions[current_round_0_indexed - 1, :]
        cooperators_in_prev_round_among_opponents = np.sum(previous_round_opponent_actions)
        if cooperators_in_prev_round_among_opponents >= self.THRESHOLD_HIGH_COOP:
            return Action.D
        elif cooperators_in_prev_round_among_opponents <= self.THRESHOLD_LOW_COOP:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_512(BaseStrategy):
    """
    The "Calculated Free-Rider" strategy prioritizes defection to maximize immediate gain,
    but it makes strategic, temporary investments in cooperation to prevent the complete
    collapse of the public good, which would ultimately harm its free-riding potential.
    It avoids being a "sucker" when cooperation completely evaporates.

    Decision Rules:
    1.  Round 1: Cooperate (C) - Initial investment/probe.
    2.  Last Round: Defect (D) - Pure self-interest, no future consequences.
    3.  Intermediate Rounds:
        - Calculate Cooperation Incentive Threshold (T = n / k).
        - If previous round's total cooperators (C_total_prev) was 0: Defect (D) - Sucker prevention.
        - Else if 0 < C_total_prev < T: Cooperate (C) - Public good revitalization, strategic investment.
        - Else (C_total_prev >= T): Defect (D) - Sustained exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k_factor = game_description.k
        self.cooperation_incentive_threshold = self.n_players / self.k_factor

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        if current_round_0_indexed == 0:
            return Action.C
        elif current_round_0_indexed == self.n_rounds - 1:
            return Action.D
        my_prev_action_is_C = history.my_actions[-1]
        opponent_prev_cooperators = np.sum(history.opponent_actions[-1, :])
        c_total_prev_round = int(my_prev_action_is_C) + opponent_prev_cooperators
        if c_total_prev_round == 0:
            return Action.D
        elif c_total_prev_round < self.cooperation_incentive_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_284(BaseStrategy):
    """
    Adaptive Collective Reciprocity strategy for the N-Player Public Goods Game.
    This strategy aims to foster cooperation by adapting its behavior based on the
    observed collective cooperation level in previous rounds. It initiates cooperation,
    conditionally cooperates, punishes sustained low cooperation, and allows for forgiveness
    and re-engagement. It also incorporates end-game defection.
    """

    class _PlayerState(Enum):
        COOPERATING = 1
        PUNISHING = 0

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._player_state = self._PlayerState.COOPERATING
        self._rounds_of_low_cooperation_observed = 0
        n = self.game_description.n_players
        k = self.game_description.k
        self._coop_tolerance_threshold = (k - 1) / (n - 1) * 0.5 + 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        n_rounds = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if current_round_number == 0:
            return Action.C
        if current_round_number == n_rounds - 1:
            return Action.D
        cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        proportion_cooperators_prev_round = cooperators_prev_round / n_players
        if self._player_state == self._PlayerState.COOPERATING:
            if proportion_cooperators_prev_round >= self._coop_tolerance_threshold:
                self._rounds_of_low_cooperation_observed = 0
                return Action.C
            else:
                self._rounds_of_low_cooperation_observed += 1
                if self._rounds_of_low_cooperation_observed >= 2:
                    self._player_state = self._PlayerState.PUNISHING
                    self._rounds_of_low_cooperation_observed = 0
                    return Action.D
                else:
                    return Action.C
        elif proportion_cooperators_prev_round >= self._coop_tolerance_threshold:
            self._player_state = self._PlayerState.COOPERATING
            self._rounds_of_low_cooperation_observed = 0
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_417(BaseStrategy):
    """
    Robust Adaptive Collective Reciprocator (RACR) for the N-Player Public Goods Game.
    This strategy is designed to foster collective cooperation, punish widespread defection,
    and be resilient against a variety of opponent behaviors by offering forgiveness
    and testing for renewed cooperation. It operates in three distinct states:
    COOPERATIVE, PUNISHING, and TESTING_FORGIVENESS.
    """

    class RACRState(Enum):
        """
        Represents the current strategic posture of the player.
        """
        COOPERATIVE = 1
        PUNISHING = 2
        TESTING_FORGIVENESS = 3

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defection_tolerance_threshold = math.floor(self.game_description.n_players / 2)
        self.forgiveness_trigger_rounds = 2
        self.state = self.RACRState.COOPERATIVE
        self.consecutive_punishment_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        num_defect_prev_round = self.game_description.n_players - num_cooperators_prev_round
        current_action = Action.D
        if self.state == self.RACRState.COOPERATIVE:
            if num_defect_prev_round > self.defection_tolerance_threshold:
                current_action = Action.D
                self.state = self.RACRState.PUNISHING
                self.consecutive_punishment_rounds = 1
            else:
                current_action = Action.C
                self.state = self.RACRState.COOPERATIVE
                self.consecutive_punishment_rounds = 0
        elif self.state == self.RACRState.PUNISHING:
            self.consecutive_punishment_rounds += 1
            if num_defect_prev_round <= self.defection_tolerance_threshold:
                current_action = Action.C
                self.state = self.RACRState.COOPERATIVE
                self.consecutive_punishment_rounds = 0
            elif self.consecutive_punishment_rounds == self.forgiveness_trigger_rounds:
                current_action = Action.C
                self.state = self.RACRState.TESTING_FORGIVENESS
                self.consecutive_punishment_rounds = 0
            else:
                current_action = Action.D
                self.state = self.RACRState.PUNISHING
        elif self.state == self.RACRState.TESTING_FORGIVENESS:
            if num_defect_prev_round <= self.defection_tolerance_threshold:
                current_action = Action.C
                self.state = self.RACRState.COOPERATIVE
                self.consecutive_punishment_rounds = 0
            else:
                current_action = Action.D
                self.state = self.RACRState.PUNISHING
                self.consecutive_punishment_rounds = 1
        return current_action

class Strategy_COLLECTIVE_447(BaseStrategy):
    """
    The "Adaptive Collective Contributor" (ACC) strategy for the N-Player Public Goods Game.
    It balances initial trust, responsiveness to collective behavior, structured punishment,
    and forgiveness, while accounting for endgame rationality.
    """

    class ACCState(Enum):
        COOPERATIVE_PHASE = 0
        PUNISHMENT_PHASE = 1

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.current_state = self.ACCState.COOPERATIVE_PHASE
        self.punishment_countdown = 0
        self.COOPERATION_THRESHOLD = math.floor(self.game_description.n_players / 2) + 1
        self.PUNISHMENT_DURATION = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            return Action.C
        if current_round_number == self.game_description.n_rounds - 1:
            return Action.D
        my_prev_action_was_C = history.my_actions[-1]
        opponents_prev_cooperators_count = np.sum(history.opponent_actions[-1, :])
        m_previous_round = int(my_prev_action_was_C) + opponents_prev_cooperators_count
        if self.current_state == self.ACCState.PUNISHMENT_PHASE:
            self.punishment_countdown -= 1
            action_for_this_round = Action.D
            if self.punishment_countdown == 0:
                self.current_state = self.ACCState.COOPERATIVE_PHASE
            return action_for_this_round
        elif m_previous_round >= self.COOPERATION_THRESHOLD:
            return Action.C
        else:
            self.current_state = self.ACCState.PUNISHMENT_PHASE
            self.punishment_countdown = self.PUNISHMENT_DURATION
            return Action.D

class Strategy_COLLECTIVE_486(BaseStrategy):
    """
    Adaptive Collective Reciprocity with Patience (ACRP) strategy for the N-Player Public Goods Game.

    This strategy aims to promote high levels of cooperation, adapt to varying opponent behaviors,
    and protect against persistent exploitation, all while aligning with a collective mindset.
    It starts by cooperating, uses a collective cooperation threshold to decide on continued
    cooperation or punishment, and eventually resorts to permanent defection if attempts
    to build cooperation consistently fail. It also defects in the final round based on
    backward induction.
    """

    class ACRPState(Enum):
        COOPERATE = 0
        PUNISH = 1
        DEFECT_PERMANENTLY = 2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self._T_cooperation_threshold = math.ceil(self.n / self.k)
        self._punishment_duration = 1
        self._max_failed_cooperation_attempts = self.n
        self._state: Strategy.ACRPState = self.ACRPState.COOPERATE
        self._rounds_in_current_state: int = 0
        self._failed_cooperation_attempts_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_number = state.round_number
        if current_round_number == 0:
            self._state = self.ACRPState.COOPERATE
            self._rounds_in_current_state = 1
            self._failed_cooperation_attempts_count = 0
            return Action.C
        elif current_round_number == self.r - 1:
            return Action.D
        else:
            my_prev_action_was_C = history.my_actions[current_round_number - 1] == Action.C
            opponent_prev_cooperators = np.sum(history.opponent_actions[current_round_number - 1, :])
            C_prev = int(my_prev_action_was_C) + opponent_prev_cooperators
            action_to_take = Action.D
            if self._state == self.ACRPState.COOPERATE:
                if C_prev >= self._T_cooperation_threshold:
                    action_to_take = Action.C
                    self._rounds_in_current_state += 1
                    self._failed_cooperation_attempts_count = 0
                else:
                    action_to_take = Action.D
                    self._state = self.ACRPState.PUNISH
                    self._rounds_in_current_state = 1
                    self._failed_cooperation_attempts_count += 1
            elif self._state == self.ACRPState.PUNISH:
                if self._rounds_in_current_state < self._punishment_duration:
                    action_to_take = Action.D
                    self._rounds_in_current_state += 1
                elif self._failed_cooperation_attempts_count >= self._max_failed_cooperation_attempts:
                    action_to_take = Action.D
                    self._state = self.ACRPState.DEFECT_PERMANENTLY
                    self._rounds_in_current_state = 1
                else:
                    action_to_take = Action.C
                    self._state = self.ACRPState.COOPERATE
                    self._rounds_in_current_state = 1
            elif self._state == self.ACRPState.DEFECT_PERMANENTLY:
                action_to_take = Action.D
                self._rounds_in_current_state += 1
            return action_to_take

class Strategy_COLLECTIVE_509(BaseStrategy):
    """
    Adaptive Collective Reciprocity: A strategy for N-Player Public Goods Games
    that aims for high collective outcomes through initial trust, swift punishment
    of collective defection, and offering forgiveness to re-establish cooperation.
    It adapts to observed cooperation levels and handles game start/end explicitly.
    """

    class _StrategyState(Enum):
        COOPERATING = 1
        PUNISHING = 0

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._my_state: self._StrategyState = self._StrategyState.COOPERATING
        self._rounds_played: int = 0
        self._consecutive_punishment_rounds: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self._rounds_played = state.round_number + 1
        if self._rounds_played == 1:
            return Action.C
        if self._rounds_played == self.game_description.n_rounds:
            return Action.D
        total_cooperators_prev_round = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if self._my_state == self._StrategyState.COOPERATING:
            if total_cooperators_prev_round >= self.game_description.n_players - 1:
                self._consecutive_punishment_rounds = 0
                return Action.C
            else:
                self._my_state = self._StrategyState.PUNISHING
                self._consecutive_punishment_rounds = 1
                return Action.D
        elif self._my_state == self._StrategyState.PUNISHING:
            if total_cooperators_prev_round <= 1 and self._consecutive_punishment_rounds < 2:
                self._consecutive_punishment_rounds += 1
                return Action.D
            else:
                self._my_state = self._StrategyState.COOPERATING
                self._consecutive_punishment_rounds = 0
                return Action.C

class Strategy_EXPLOITATIVE_245(BaseStrategy):
    """
    Adaptive Opportunistic Exploiter (AOE) strategy for N-Player Public Goods Game.
    Starts by cooperating to build public good, then exploits it. If cooperation
    collapses, it attempts to re-seed it. Always defects in the last round.
    """

    class Mood(Enum):
        TRY_TO_COOPERATE = 0
        EXPLOIT = 1

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.current_mood = self.Mood.TRY_TO_COOPERATE
        self.exploit_threshold = math.ceil(self.game_description.n_players * (self.game_description.k - 1) / self.game_description.k)
        self.reseed_threshold = max(1, math.floor(self.game_description.k / 2))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        n_rounds = self.game_description.n_rounds
        if current_round_1_indexed == n_rounds:
            return Action.D
        if current_round_1_indexed == 1:
            return Action.C
        num_opponent_coop_prev_round = np.sum(history.opponent_actions[state.round_number - 1, :])
        my_coop_prev_round = int(history.my_actions[state.round_number - 1])
        total_coop_prev_round = num_opponent_coop_prev_round + my_coop_prev_round
        if self.current_mood == self.Mood.TRY_TO_COOPERATE:
            if total_coop_prev_round >= self.exploit_threshold:
                self.current_mood = self.Mood.EXPLOIT
            return Action.C
        else:
            if total_coop_prev_round <= self.reseed_threshold:
                self.current_mood = self.Mood.TRY_TO_COOPERATE
            return Action.D

class Strategy_EXPLOITATIVE_280(BaseStrategy):
    """
    The Adaptive Free-Rider strategy for the N-Player Public Goods Game.

    This strategy leverages repeated interaction to identify and exploit cooperative
    tendencies among other players, while rigorously protecting itself from being a "sucker."
    It adapts its behavior based on observed cooperation levels, moving between
    phases of probing, free-riding, and temporary punishment, always with an eye
    on the finite horizon of the game.
    """

    class Mode(Enum):
        PROBING = 'probing'
        FREE_RIDING = 'free_riding'
        PUNISHING = 'punishing'

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_probe_rounds: int = 2
        self.cooperation_detection_threshold: int = 1
        self.no_cooperation_punishment_rounds: int = 3
        self.end_game_rounds: int = 1
        self.mode: self.Mode = self.Mode.PROBING
        self.punishment_counter: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed: int = state.round_number + 1
        if current_round_1_indexed > self.game_description.n_rounds - self.end_game_rounds:
            return Action.D
        if current_round_1_indexed <= self.initial_probe_rounds:
            self.mode = self.Mode.PROBING
            return Action.C
        C_others_prev: int = sum(history.opponent_actions[-1, :])
        if self.mode == self.Mode.PROBING:
            if C_others_prev >= self.cooperation_detection_threshold:
                self.mode = self.Mode.FREE_RIDING
                return Action.D
            else:
                self.mode = self.Mode.PUNISHING
                self.punishment_counter = self.no_cooperation_punishment_rounds
                return Action.D
        elif self.mode == self.Mode.FREE_RIDING:
            if C_others_prev >= self.cooperation_detection_threshold:
                return Action.D
            else:
                self.mode = self.Mode.PUNISHING
                self.punishment_counter = self.no_cooperation_punishment_rounds
                return Action.D
        elif self.mode == self.Mode.PUNISHING:
            self.punishment_counter -= 1
            if self.punishment_counter == 0:
                self.mode = self.Mode.PROBING
                return Action.C
            else:
                return Action.D
        return Action.D


class Strategy_EXPLOITATIVE_294(BaseStrategy):
    """
    The "Opportunistic Free-Rider with Recalibration" strategy aims to maximize
    individual payoff by primarily defecting (free-riding) on the contributions
    of others. It adaptively responds to the level of cooperation from opponents,
    strategically cooperating only when necessary to revive the public good
    for future exploitation, and permanently abandoning cooperation if attempts
    to stimulate it prove futile.
    """

    class Mode(Enum):
        EXPLOIT = 0
        RECALIBRATE = 1
        PERMANENT_DEFECT = 2

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.RECALIBRATION_COOP_ROUNDS = 1
        self.COOP_RATE_EVAL_PERIOD = 3
        self.RECALIBRATION_TRIGGER_THRESHOLD = 0.2
        self.MIN_COOP_AFTER_RECALIBRATION = 0.05
        self.current_mode = self.Mode.EXPLOIT
        self.recalibration_countdown = 0

    def _calculate_historical_coop_rate_others(self, history: PlayerHistory, current_round_num: int) -> float:
        """
        Calculates the average cooperation rate of other players over the most recent
        `self.COOP_RATE_EVAL_PERIOD` rounds or fewer, if fewer rounds are available.

        Args:
            history: The PlayerHistory object containing past game data.
            current_round_num: The 0-indexed number of the current round.

        Returns:
            The average cooperation rate of other players as a float between 0.0 and 1.0.
        """
        if current_round_num == 0 or history is None:
            return 0.0
        available_history_rounds = history.opponent_actions.shape[0]
        num_rounds_to_consider = min(available_history_rounds, self.COOP_RATE_EVAL_PERIOD)
        if num_rounds_to_consider == 0:
            return 0.0
        total_cooperations_from_others = np.sum(history.opponent_actions[-num_rounds_to_consider:])
        possible_cooperations = num_rounds_to_consider * (self.game_description.n_players - 1)
        if possible_cooperations == 0:
            return 0.0
        return total_cooperations_from_others / possible_cooperations

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_0_indexed = state.round_number
        total_rounds_0_indexed = self.game_description.n_rounds - 1
        if current_round_0_indexed == total_rounds_0_indexed:
            return Action.D
        if current_round_0_indexed == 0:
            return Action.D
        if self.current_mode == self.Mode.RECALIBRATE and self.recalibration_countdown == 0:
            self.current_mode = self.Mode.EXPLOIT
            avg_coop_post_recal = self._calculate_historical_coop_rate_others(history, current_round_0_indexed)
            if avg_coop_post_recal < self.MIN_COOP_AFTER_RECALIBRATION:
                self.current_mode = self.Mode.PERMANENT_DEFECT
        if self.current_mode == self.Mode.PERMANENT_DEFECT:
            return Action.D
        elif self.current_mode == self.Mode.RECALIBRATE:
            self.recalibration_countdown -= 1
            return Action.C
        elif self.current_mode == self.Mode.EXPLOIT:
            historical_coop_rate_others = self._calculate_historical_coop_rate_others(history, current_round_0_indexed)
            if historical_coop_rate_others < self.RECALIBRATION_TRIGGER_THRESHOLD:
                self.current_mode = self.Mode.RECALIBRATE
                self.recalibration_countdown = self.RECALIBRATION_COOP_ROUNDS - 1
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_339(BaseStrategy):
    """
    The Adaptive Exploiter strategy for the N-Player Public Goods Game.
    This strategy aims to maximize individual payoff by encouraging others to cooperate
    while free-riding on their contributions. It employs a phased approach:
    1. INDUCING_COOPERATION: Play C to signal willingness to cooperate and build trust.
    2. EXPLOITING: Once sufficient cooperation is observed, switch to D to free-ride.
    3. DEFECTION_ONLY: If inducement fails repeatedly or cooperation drops too low
       after a failed re-inducement attempt, revert to permanent D.

    The strategy adapts based on observed cooperation rates and has an end-game rationality
    of always defecting in the final round.
    """

    class Mode(Enum):
        INDUCING_COOPERATION = 'INDUCING_COOPERATION'
        EXPLOITING = 'EXPLOITING'
        DEFECTION_ONLY = 'DEFECTION_ONLY'

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.COOPERATION_THRESHOLD_RATE = 0.6
        self.INDUCEMENT_ROUNDS_MAX = max(1, min(5, self.r // 5))
        self.RE_INDUCEMENT_TRIGGER_DROP = self.COOPERATION_THRESHOLD_RATE / 2
        self.current_mode = self.Mode.INDUCING_COOPERATION
        self.rounds_in_current_mode = 0
        self.last_attempt_to_induce_failed = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_1_indexed_round = state.round_number + 1
        if state.round_number == 0:
            return Action.C
        last_0_indexed_round = state.round_number - 1
        last_1_indexed_round = current_1_indexed_round - 1
        my_action_in_prev_round_bool = history.my_actions[last_0_indexed_round]
        my_action_in_prev_round_int = int(my_action_in_prev_round_bool)
        total_cooperators_in_prev_round = int(np.sum(history.opponent_actions[last_0_indexed_round, :])) + my_action_in_prev_round_int
        self._update_state_internal(last_1_indexed_round, total_cooperators_in_prev_round, my_action_in_prev_round_int)
        return self._decide_action_internal(current_1_indexed_round)

    def _decide_action_internal(self, t: int) -> Action:
        """
        Determines the action (C or D) for the current round `t` (1-indexed).
        """
        if t == self.r:
            return Action.D
        if self.current_mode == self.Mode.INDUCING_COOPERATION:
            return Action.C
        elif self.current_mode == self.Mode.EXPLOITING:
            return Action.D
        elif self.current_mode == self.Mode.DEFECTION_ONLY:
            return Action.D
        return Action.D

    def _update_state_internal(self, t: int, total_cooperators_in_round_t: int, my_action_in_round_t: int):
        """
        Updates internal state based on the results of round `t` (1-indexed).
        This method is called *after* round `t` has been played and its results observed.
        """
        if t == self.r:
            return
        self.rounds_in_current_mode += 1
        num_other_players = self.n - 1
        current_others_cooperation_rate = 0.0
        if num_other_players > 0:
            other_cooperators_count = total_cooperators_in_round_t - my_action_in_round_t
            current_others_cooperation_rate = other_cooperators_count / num_other_players
        if self.current_mode == self.Mode.INDUCING_COOPERATION:
            if current_others_cooperation_rate >= self.COOPERATION_THRESHOLD_RATE:
                self.current_mode = self.Mode.EXPLOITING
                self.rounds_in_current_mode = 0
                self.last_attempt_to_induce_failed = False
            elif self.rounds_in_current_mode >= self.INDUCEMENT_ROUNDS_MAX:
                self.current_mode = self.Mode.DEFECTION_ONLY
                self.rounds_in_current_mode = 0
                self.last_attempt_to_induce_failed = True
        elif self.current_mode == self.Mode.EXPLOITING:
            if current_others_cooperation_rate < self.RE_INDUCEMENT_TRIGGER_DROP and (not self.last_attempt_to_induce_failed):
                self.current_mode = self.Mode.INDUCING_COOPERATION
                self.rounds_in_current_mode = 0
            elif current_others_cooperation_rate < self.RE_INDUCEMENT_TRIGGER_DROP and self.last_attempt_to_induce_failed:
                self.current_mode = self.Mode.DEFECTION_ONLY
                self.rounds_in_current_mode = 0

class Strategy_EXPLOITATIVE_353(BaseStrategy):
    """
    The Adaptive Freerider (AFR) strategy for the N-Player Public Goods Game.
    AFR aims to create an environment of cooperation to then exploit it.
    It starts by "seeding" cooperation, then quickly switches to freeriding if others respond.
    If repeated attempts to seed cooperation fail, it becomes a pure defector to minimize losses.
    It always defects in the final round to leverage the endgame effect.
    """

    class State(Enum):
        PROBING = 1
        EXPLOITING = 2
        GRUDGE = 3

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.exploit_threshold = max(1, math.floor((self.n_players - 1) / 2))
        self.max_failed_probes = 2
        self.current_state: Strategy.State = self.State.PROBING
        self.consecutive_failed_probes_count: int = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if state.round_number == 0:
            self.current_state = self.State.PROBING
            self.consecutive_failed_probes_count = 0
            return Action.C
        others_cooperated_in_prev_round = int(np.sum(history.opponent_actions[-1, :]))
        if self.current_state == self.State.PROBING:
            if others_cooperated_in_prev_round < self.exploit_threshold:
                self.consecutive_failed_probes_count += 1
                if self.consecutive_failed_probes_count >= self.max_failed_probes:
                    self.current_state = self.State.GRUDGE
            else:
                self.consecutive_failed_probes_count = 0
                self.current_state = self.State.EXPLOITING
        elif self.current_state == self.State.EXPLOITING:
            if others_cooperated_in_prev_round < self.exploit_threshold:
                self.current_state = self.State.PROBING
                self.consecutive_failed_probes_count = 0
        if current_round_1_indexed == self.n_rounds:
            return Action.D
        elif self.current_state == self.State.GRUDGE:
            return Action.D
        elif self.current_state == self.State.EXPLOITING:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_282(BaseStrategy):
    """
    The Opportunistic Instigator-Exploiter (OIE) strategy is designed to be highly adaptive
    and exploitative in the N-Player Public Goods Game. Its core principle is to free-ride
    on others' contributions as much as possible. However, if cooperation levels among
    opponents become too low, leading to a diminished public good, the strategy will
    temporarily cooperate. This cooperation is a strategic investment to "prime the pump,"
    encouraging others to resume cooperating so that OIE can return to its primary
    role of exploitation. Near the end of the game, all pretense of cooperation is dropped
    for maximum immediate gain through pure defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.low_coop_defection_streak: int = 0
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        self.cooperation_recovery_threshold_C_prev = math.ceil(n / k)
        self.dejection_limit_before_recovery_attempt = 2
        self.rounds_near_endgame = max(2, min(self.game_description.n_players // 2, math.floor(self.game_description.n_rounds / 4)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        current_round_1_indexed = state.round_number + 1
        if current_round_1_indexed == 1:
            self.low_coop_defection_streak = 0
            return Action.C
        if state.round_number >= self.game_description.n_rounds - self.rounds_near_endgame:
            self.low_coop_defection_streak = 0
            return Action.D
        else:
            previous_round_cooperators_opponents = np.sum(history.opponent_actions[-1, :])
            previous_round_my_action_was_C = history.my_actions[-1]
            C_prev = previous_round_cooperators_opponents + int(previous_round_my_action_was_C)
            if C_prev >= self.cooperation_recovery_threshold_C_prev:
                self.low_coop_defection_streak = 0
                return Action.D
            elif self.low_coop_defection_streak < self.dejection_limit_before_recovery_attempt:
                self.low_coop_defection_streak += 1
                return Action.D
            else:
                self.low_coop_defection_streak = 0
                return Action.C