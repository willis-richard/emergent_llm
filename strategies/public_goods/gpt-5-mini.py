"""
Generated LLM strategies for social dilemma games.

This file contains strategy classes generated by LLMs for game theory experiments.
Each strategy is a callable class that implements a specific approach to the game.

Generated with:
- Provider: OpenAI
- Model: gpt-5-mini
- Game: PublicGoodsDescription
"""

from emergent_llm.players import BaseStrategy
from emergent_llm.games import PublicGoodsDescription, CollectiveRiskDescription, CommonPoolDescription, CommonPoolState
from emergent_llm.common import Action, C, D, PlayerHistory, GameState
import numpy as np
from numpy.typing import NDArray
import math
import random



class Strategy_COOPERATIVE_1(BaseStrategy):
    """
    Reciprocal Conditional Cooperation with Forgiveness (RCCF)

    - Starts cooperative to signal willingness to form cooperation.
    - Maintains per-opponent cooperation scores (S_j) with exponential-like updates.
    - Uses short-run group statistics (G_last, G_recent) to decide reciprocity, leniency, or punishment.
    - Limits punishment length and applies probabilistic forgiveness in mixed situations.
    - Always defects in the final round and tightens cooperation requirements in the final E rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.L = min(5, self.r)
        self.alpha = 0.3
        self.beta = 0.5
        self.T_coop = 0.5
        self.T_lenient = 0.2
        self.P_max = 3
        self.p_forgive = 0.25
        self.E = min(3, max(0, self.r - 1))
        self.n_opp = max(0, self.n - 1)
        self.S = np.full(self.n_opp, 0.5, dtype=float)
        self.last_processed = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        if history.round_number == 0:
            if self.last_processed != 0:
                self.S[:] = 0.5
                self.last_processed = 0
            return Action.C
        if history.round_number < self.last_processed:
            self.S[:] = 0.5
            self.last_processed = 0
        for round_idx in range(self.last_processed, history.round_number):
            actions = history.opponent_actions[round_idx]
            coop_mask = np.asarray(actions, dtype=bool)
            if self.n_opp > 0:
                if coop_mask.any():
                    self.S[coop_mask] = self.S[coop_mask] + self.alpha * (1.0 - self.S[coop_mask])
                if (~coop_mask).any():
                    self.S[~coop_mask] = self.S[~coop_mask] * (1.0 - self.beta)
        self.last_processed = history.round_number
        t = history.round_number + 1
        if t == self.r:
            return Action.D
        if history.round_number >= 1:
            last_row = history.opponent_actions[-1]
            m_last = float(np.sum(last_row))
            G_last = m_last / float(self.n_opp) if self.n_opp > 0 else 0.0
        else:
            G_last = 0.0
        start_idx = max(0, history.round_number - self.L)
        if history.round_number > 0:
            recent_rows = history.opponent_actions[start_idx:history.round_number]
            if recent_rows.size == 0:
                G_recent = 0.0
            else:
                m_per_round = np.sum(recent_rows, axis=1)
                frac_per_round = m_per_round / float(self.n_opp) if self.n_opp > 0 else np.zeros_like(m_per_round)
                G_recent = float(np.mean(frac_per_round))
        else:
            G_recent = 0.0
        S_bar = float(np.mean(self.S)) if self.n_opp > 0 else 0.5
        consecutive_low = 0
        for ridx in range(history.round_number - 1, -1, -1):
            frac = float(np.sum(history.opponent_actions[ridx])) / float(self.n_opp) if self.n_opp > 0 else 0.0
            if frac < self.T_lenient:
                consecutive_low += 1
            else:
                break
        in_punishment = consecutive_low > 0 and consecutive_low <= self.P_max
        chosen_action = Action.D
        if in_punishment:
            if G_recent >= self.T_coop or S_bar >= 0.75:
                chosen_action = Action.C
            else:
                chosen_action = Action.D
        elif G_last >= self.T_coop:
            chosen_action = Action.C
        elif G_recent >= self.T_coop:
            chosen_action = Action.C
        elif G_recent >= self.T_lenient:
            if random.random() < self.p_forgive:
                chosen_action = Action.C
            else:
                chosen_action = Action.D
        else:
            chosen_action = Action.D
        R_rem = self.r - t
        if R_rem <= self.E:
            if chosen_action == Action.C and (G_recent < 0.9 and S_bar < 0.9):
                chosen_action = Action.D
        return chosen_action

class Strategy_COOPERATIVE_2(BaseStrategy):
    """
    Reciprocal Thresholded Pavlov (RTP)

    - Starts by cooperating.
    - In intermediate rounds compares actual payoff last round to the
      counterfactual payoff had it defected. If the previous action
      "won" (actual >= counterfactual) it repeats it; otherwise it switches.
    - On ties prefers cooperation.
    - Forgives quickly: defects at most P_max consecutive rounds in response
      to a loss, and returns to cooperation early if group cooperation rebounds
      (>= ceil(n/2) cooperators).
    - Defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.P_max = 2
        self.consecutive_defect_punishment = 0
        self.last_action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.consecutive_defect_punishment = 0
            self.last_action = Action.C
            return Action.C
        if state.round_number == self.game_description.n_rounds:
            self.last_action = Action.D
            return Action.D
        n = int(self.game_description.n_players)
        k = float(self.game_description.k)
        my_prev_bool = bool(history.my_actions[-1])
        my_prev_action = Action.C if my_prev_bool else Action.D
        if self.last_action != my_prev_action:
            self.consecutive_defect_punishment = 0
            self.last_action = my_prev_action
        opp_last = history.opponent_actions[-1, :]
        coop_opp = int(np.sum(opp_last))
        c_my_prev = 1 if my_prev_bool else 0
        C_prev = coop_opp + c_my_prev
        actual_payoff = 1.0 - c_my_prev + k / n * float(C_prev)
        counterf_defect_payoff = 1.0 + k / n * float(C_prev - c_my_prev)
        tie_tol = 1e-09
        if actual_payoff > counterf_defect_payoff + tie_tol:
            next_action = my_prev_action
            self.consecutive_defect_punishment = 0
        elif actual_payoff < counterf_defect_payoff - tie_tol:
            next_action = Action.D if my_prev_action == Action.C else Action.C
            if next_action == Action.D:
                self.consecutive_defect_punishment += 1
            else:
                self.consecutive_defect_punishment = 0
        else:
            next_action = Action.C
            self.consecutive_defect_punishment = 0
        majority_thresh = math.ceil(n / 2)
        if next_action == Action.D and (C_prev >= majority_thresh or self.consecutive_defect_punishment > self.P_max):
            next_action = Action.C
            self.consecutive_defect_punishment = 0
        self.last_action = next_action
        return next_action

class Strategy_COOPERATIVE_3(BaseStrategy):
    """
    Pavlov (win-stay/lose-shift) adapted to an n-player public goods game.
    - Start by cooperating.
    - In the final round always defect.
    - For intermediate rounds, compare your last-round realized payoff to the
      counterfactual payoff you would have obtained by switching your last action
      (holding others' last-round actions fixed). If your last action did at least
      as well, repeat it; otherwise switch. On ties prefer cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        if state.round_number == self.r:
            return Action.D
        last_my_action_bool = bool(history.my_actions[-1])
        a_prev = Action.C if last_my_action_bool else Action.D
        last_opponents = history.opponent_actions[-1]
        m = int(np.sum(last_opponents))
        pi_prev = float(history.my_payoffs[-1])
        if a_prev == Action.C:
            pi_alt = 1.0 + self.k / self.n * m
        else:
            pi_alt = 0.0 + self.k / self.n * (m + 1)
        if pi_prev > pi_alt:
            return a_prev
        elif pi_prev < pi_alt:
            return Action.D if a_prev == Action.C else Action.C
        else:
            return Action.C

class Strategy_COOPERATIVE_4(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Forgiving Proportional Punishment (ACCFPP).

    - Start by cooperating.
    - Cooperate while recent group cooperation >= gamma.
    - If group cooperation in previous round falls below gamma, punish by defecting
      for a proportional number of rounds (based on shortfall and escalation).
    - Punishment is capped relative to remaining rounds and is forgiving:
      if group cooperation recovers for `window_for_reset` consecutive rounds,
      punishment is cleared early and escalation resets.
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        raw_gamma = 0.6 - 0.05 * ((self.k - 1.0) / max(1.0, self.n - 1.0))
        self.gamma = min(0.79, max(0.51, raw_gamma))
        self.base_punish_unit = 1
        self.punish_cap_fraction = 0.5
        self.escalation_limit = 8
        self.window_for_reset = 2
        self.punish_counter = 0
        self.escalation_factor = 1
        self.consecutive_good_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory):
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if history is None:
            return Action.C
        if t == self.r:
            return Action.D

        def cooperators_in_round(idx: int) -> int:
            my = int(history.my_actions[idx])
            opp = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size > 0 else 0
            return my + opp
        remaining_rounds = max(0, self.r - t + 1)
        if self.punish_counter > 0:
            if history.round_number >= self.window_for_reset:
                ok = True
                start_idx = history.round_number - self.window_for_reset
                for idx in range(start_idx, history.round_number):
                    coop_count = cooperators_in_round(idx)
                    if coop_count / float(self.n) < self.gamma:
                        ok = False
                        break
                if ok:
                    self.punish_counter = 0
                    self.escalation_factor = 1
                    self.consecutive_good_rounds = self.window_for_reset
                    return Action.C
            self.punish_counter = max(0, self.punish_counter - 1)
            return Action.D
        prev_idx = t - 2
        if prev_idx < 0:
            return Action.C
        C_prev = cooperators_in_round(prev_idx)
        coop_frac = C_prev / float(self.n)
        if coop_frac >= self.gamma:
            self.consecutive_good_rounds += 1
            if self.consecutive_good_rounds >= self.window_for_reset:
                self.escalation_factor = 1
            return Action.C
        else:
            self.consecutive_good_rounds = 0
            shortfall = math.ceil(self.n * self.gamma) - C_prev
            if shortfall < 1:
                shortfall = 1
            raw_punish = max(self.base_punish_unit, shortfall)
            punish_cap = max(1, math.floor(self.punish_cap_fraction * remaining_rounds))
            punish_length = int(min(punish_cap, raw_punish * self.escalation_factor))
            if punish_length < 1:
                punish_length = 1
            self.punish_counter = max(0, punish_length - 1)
            self.escalation_factor = min(self.escalation_limit, self.escalation_factor * 2)
            return Action.D

class Strategy_COOPERATIVE_5(BaseStrategy):
    """
    Forgiving Targeted Conditional Cooperation (FTCC)

    - Maintains per-opponent trust scores S_j (recomputed from public history each call).
    - Starts by cooperating (unless r == 1 or k <= 1).
    - Defects in the final round (t == r) and when k <= 1 or r == 1.
    - In intermediate rounds cooperates if recent cooperation / majority trusted / average trust is non-negative.
    - Updates trust using decay (lambda), +1 for cooperation, -p for defection, and an optional forgiveness pulse T_f
      when a previously negative score becomes positive due to cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.n_opponents = max(0, self.n - 1)
        self.lambda_decay = 0.8
        self.penalty = 2.0
        self.q = 0.0
        self.m_min = max(1, math.floor((self.n - 1) / 2))
        self.T_f = 1.0
        self.trust_majority_threshold = math.ceil((self.n - 1) / 2)
        self.last_action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if self.r == 1:
            self.last_action = Action.D
            return Action.D
        if self.k <= 1:
            self.last_action = Action.D
            return Action.D
        if t == self.r:
            self.last_action = Action.D
            return Action.D
        if history is None:
            self.last_action = Action.C
            return Action.C
        rounds_completed = history.round_number
        opp_actions = history.opponent_actions
        S = np.zeros(self.n_opponents, dtype=float)
        for round_idx in range(rounds_completed):
            acts = opp_actions[round_idx, :]
            for j in range(self.n_opponents):
                old = float(S[j])
                if bool(acts[j]):
                    new = self.lambda_decay * old + 1.0
                    if old < 0.0 and new > 0.0 and (self.T_f > 0):
                        new += float(self.T_f)
                else:
                    new = self.lambda_decay * old - float(self.penalty)
                S[j] = new
        if rounds_completed >= 1:
            last_cooperators = int(np.sum(opp_actions[-1, :]))
        else:
            last_cooperators = 0
        trusted_count = int(np.sum(S >= self.q))
        avg_trust = float(np.mean(S) if self.n_opponents > 0 else 0.0)
        cooperate = False
        if last_cooperators >= self.m_min:
            cooperate = True
        elif trusted_count >= self.trust_majority_threshold:
            cooperate = True
        elif avg_trust >= self.q:
            cooperate = True
        else:
            cooperate = False
        action = Action.C if cooperate else Action.D
        self.last_action = action
        return action

class Strategy_COOPERATIVE_6(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Proportional Punishment (ACCPP).

    - Start cooperative.
    - Track recent cooperation rates and a dynamic threshold alpha that depends on k/n.
    - When cooperation falls below alpha, punish by defecting for a short proportional number of rounds.
    - Small-probability probes (epsilon) allow re-connection after noise.
    - Endgame: be cautious in the final round(s), reduce punishments near the end.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_until_round = 0
        self.epsilon = 0.02
        self.punish_scale = 2.0
        self.n = int(self.game_description.n_players)
        self.r = int(self.game_description.n_rounds)
        self.k = float(self.game_description.k)
        self.W = min(5, self.r)
        self.alpha = 0.5 + 0.25 * (1.0 - self.k / float(self.n))
        self.max_punish = max(1, math.floor(0.2 * self.r))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punish_until_round = 0
            return Action.C
        t = int(state.round_number)
        completed = history.round_number
        if completed == 0:
            return Action.C
        opp_actions = history.opponent_actions
        if opp_actions.size == 0:
            opp_counts = np.zeros(completed, dtype=int)
        else:
            opp_counts = np.sum(opp_actions.astype(int), axis=1)
        my_counts = history.my_actions.astype(int)
        m_array = my_counts + opp_counts
        m_last = int(m_array[-1])
        f_last = float(m_last) / float(self.n)
        recent_len = min(self.W, completed)
        recent_slice = m_array[-recent_len:] if recent_len > 0 else np.array([], dtype=int)
        if recent_len > 0:
            f_recent = float(int(np.sum(recent_slice))) / (float(self.n) * float(recent_len))
        else:
            f_recent = f_last
        remaining_rounds = max(1, self.r - t + 1)
        few = max(1, math.floor(0.05 * self.r))
        in_final_few = t >= self.r - few + 1
        last_round = t == self.r
        if t <= self.punish_until_round:
            alpha_for_recovery = self.alpha - 0.05 if in_final_few else self.alpha
            if f_last >= alpha_for_recovery or f_recent >= alpha_for_recovery:
                self.punish_until_round = 0
                return Action.C
            if random.random() < self.epsilon:
                return Action.C
            else:
                return Action.D
        if last_round:
            total_coops = int(np.sum(m_array))
            denom = float(self.n * completed) if completed > 0 else 1.0
            overall_rate = float(total_coops) / denom if denom > 0 else 0.0
            if overall_rate >= self.alpha or f_last >= self.alpha:
                return Action.C
            else:
                return Action.D
        if f_last >= self.alpha or f_recent >= self.alpha:
            return Action.C
        severity = max(0.0, self.alpha - f_last)
        L = 1 + math.ceil(self.punish_scale * severity * remaining_rounds)
        L = int(min(self.max_punish, L))
        punish_until = t + L - 1
        if punish_until >= self.r:
            punish_until = self.r - 1
        if punish_until < t:
            punish_until = t
        self.punish_until_round = int(punish_until)
        return Action.D

class Strategy_COOPERATIVE_7(BaseStrategy):
    """
    Forgiving Majority Conditional Cooperator (FMCC)

    - Opens with cooperation.
    - Uses a short memory window to estimate recent cooperation among others.
    - Marks persistent defectors and excludes them from the cooperation estimate.
    - Punishes when cooperation among the (non-persistent) group falls below a simple majority.
      Punishment length is short and scales with the shortfall.
    - In the final round: cooperate only if everyone (including self) has cooperated in all prior rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_counter = 0
        n = int(game_description.n_players)
        r = int(game_description.n_rounds)
        self.n = n
        self.r = r
        self.w = min(3, r)
        self.majority_threshold = 0.5
        self.persist_defect_threshold = 0.5
        self.base_punish_len = 1
        self.punish_scale = math.ceil((n - 1) * 0.5)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.C
        rounds_observed = history.round_number
        current_round = rounds_observed + 1
        final_round = current_round == self.r
        opp_actions = history.opponent_actions
        if opp_actions.size == 0:
            defect_counts = np.zeros((self.n - 1,), dtype=int)
        else:
            defect_counts = np.sum(~opp_actions, axis=0).astype(int)
        if final_round:
            if rounds_observed == 0:
                return Action.C
            my_all_coop = bool(np.all(history.my_actions))
            opp_all_coop = True
            if opp_actions.size != 0:
                opp_all_coop = bool(np.all(opp_actions))
            if my_all_coop and opp_all_coop:
                return Action.C
            else:
                return Action.D
        denom = max(1, rounds_observed)
        persist_mask = defect_counts / float(denom) >= self.persist_defect_threshold
        nonpersistent_indices = [idx for idx, is_p in enumerate(persist_mask) if not is_p]
        N_nonpersist = len(nonpersistent_indices)
        if self.punish_counter > 0:
            self.punish_counter -= 1
            return Action.D
        window_size = min(self.w, rounds_observed)
        if window_size == 0 or N_nonpersist == 0:
            effective_fraction = 0.0
        else:
            start_idx = rounds_observed - window_size
            window = opp_actions[start_idx:rounds_observed, :]
            if window.size == 0:
                effective_fraction = 0.0
            else:
                subwindow = window[:, nonpersistent_indices]
                per_round_fracs = np.sum(subwindow, axis=1) / float(N_nonpersist)
                effective_fraction = float(np.mean(per_round_fracs))
        if effective_fraction >= self.majority_threshold:
            return Action.C
        else:
            shortfall = max(0.0, self.majority_threshold - effective_fraction)
            extra = math.ceil(shortfall * self.punish_scale)
            self.punish_counter = int(self.base_punish_len + extra)
            return Action.D

class Strategy_COOPERATIVE_8(BaseStrategy):
    """
    Adaptive Threshold Reciprocity (ATR)

    Forgiving, reciprocity-based strategy for the N-player public goods game.
    - Starts by cooperating.
    - Always defects in the final round.
    - Observes other players' cooperation in recent rounds (configurable window w).
    - If at most d_grace opponents defected in the previous round, cooperate (leniency).
    - Otherwise, if the fraction of other cooperators (averaged over the last w rounds)
      is at least tau, cooperate.
    - Else enter limited punishment: defect for up to P consecutive rounds (P small,
      adaptive to remaining rounds). If others' cooperation recovers, forgive early.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tau = 0.5
        self.d_grace = 1
        self.w = 1
        self.consecutive_low = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.consecutive_low = 0
            return Action.C
        total_rounds = self.game_description.n_rounds
        current_round = state.round_number
        if current_round >= total_rounds:
            return Action.D
        n = self.game_description.n_players
        n_opponents = max(1, n - 1)
        window = min(self.w, history.round_number)
        if window <= 0:
            frac = 0.0
        else:
            recent_opponent = history.opponent_actions[-window:, :]
            frac = float(np.mean(recent_opponent)) if recent_opponent.size > 0 else 0.0
        if history.round_number > 0:
            last_opponents = history.opponent_actions[-1, :]
            others_coop_last = int(np.sum(last_opponents))
            my_prev = 1 if bool(history.my_actions[-1]) else 0
        else:
            others_coop_last = 0
            my_prev = 0
        if others_coop_last >= n_opponents - self.d_grace:
            self.consecutive_low = 0
            return Action.C
        if frac >= self.tau:
            self.consecutive_low = 0
            return Action.C
        remaining_after_current = max(0, total_rounds - current_round)
        if remaining_after_current <= 1:
            P = 1
        else:
            P = min(3, remaining_after_current - 1 if remaining_after_current - 1 >= 1 else 1)
        self.consecutive_low += 1
        if self.consecutive_low <= P:
            return Action.D
        else:
            self.consecutive_low = 0
            return Action.C

class Strategy_COOPERATIVE_9(BaseStrategy):
    """
    Forgiving Conditional Cooperator (FCC)

    - Starts by cooperating.
    - Tracks short-term average group cooperation (over a window w = min(5, r-1)).
    - Has a threshold T (responsive to k/n and endgame) to decide whether to cooperate.
    - If group cooperation falls below T, initiates a short, proportional punishment of up to P_max rounds.
    - Is forgiving: after a punishment burst, will resume cooperation if group cooperation >= T_forgive (0.5).
    - Is endgame-aware: in the final round, defects unless near-unanimous cooperation has recently been observed
      (coop_rate >= 0.95) and not currently punishing.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_remaining: int = 0
        self._last_was_punishing: bool = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self._last_was_punishing = False
            return Action.C

        def clamp(x, lo, hi):
            if x < lo:
                return lo
            if x > hi:
                return hi
            return x
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        t = int(state.round_number)
        completed = history.round_number
        remaining_rounds = r - t + 1
        w = min(5, max(1, r - 1))
        lookback = min(w, max(0, completed))
        if lookback <= 0:
            coop_rate = 1.0
        else:
            start = completed - lookback
            end = completed
            my_segment = history.my_actions[start:end].astype(float)
            opp_segment = history.opponent_actions[start:end, :].astype(float)
            total_cooperators = float(my_segment.sum()) + float(opp_segment.sum())
            coop_rate = total_cooperators / (n * lookback)
        T_base = 0.5 + 0.3 * (1.0 - k / float(n))
        T_base = clamp(T_base, 0.4, 0.8)
        endgame_cutoff = max(1, math.floor(r / 10))
        if remaining_rounds <= endgame_cutoff:
            T = clamp(T_base + 0.15, 0.4, 0.95)
        else:
            T = T_base
        P_max = 3
        T_forgive = 0.5
        if t == r:
            if self.punishment_remaining > 0:
                self.punishment_remaining = max(0, self.punishment_remaining - 1)
                self._last_was_punishing = True
                return Action.D
            elif coop_rate >= 0.95:
                self._last_was_punishing = False
                return Action.C
            else:
                self._last_was_punishing = False
                return Action.D
        if self.punishment_remaining > 0:
            self.punishment_remaining = max(0, self.punishment_remaining - 1)
            self._last_was_punishing = True
            return Action.D
        if self._last_was_punishing:
            if coop_rate >= T_forgive:
                self._last_was_punishing = False
                return Action.C
        if coop_rate >= T:
            self._last_was_punishing = False
            return Action.C
        shortfall = T - coop_rate
        if T <= 0:
            raw_p = P_max
        else:
            raw_p = math.ceil(shortfall / T * P_max)
        P = int(min(P_max, raw_p))
        max_allowed_punish = max(1, remaining_rounds - 1)
        P = int(min(P, max_allowed_punish))
        if P < 1:
            P = 1
        self.punishment_remaining = max(0, P - 1)
        self._last_was_punishing = True
        return Action.D

class Strategy_COOPERATIVE_10(BaseStrategy):
    """
    Adaptive Threshold with Forgiving Punishment (ATFP)

    - Starts by cooperating.
    - Monitors up to a w-round window of recent other-players' cooperation.
    - Uses a threshold T (based on n and k) for required number of OTHER cooperators.
    - If recent average cooperation among others meets T, cooperate.
    - Otherwise, issue a short punishment (defect) whose length P scales with
      the frequency of under-threshold rounds (mis_rate), bounded by punish_max
      and the remaining rounds. After the punishment window, return to cooperation.
    - In the final round, defect unless the immediately preceding round had at least T
      other cooperators (unless already in a punishment window).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = int(game_description.n_players)
        r = int(game_description.n_rounds)
        k = float(game_description.k)
        self.n = n
        self.r = r
        self.k = k
        self.w = min(5, max(1, r - 1))
        theta_raw = 0.25 + 0.75 * (1.0 - k / float(n))
        self.theta = max(0.2, min(0.9, theta_raw))
        self.T = int(math.ceil(self.theta * float(max(0, n - 1))))
        self.punish_base = 2
        self.punish_max = max(1, min(4, max(0, r - 2)))
        self.punish_until_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if history is None:
            return Action.C
        if t <= self.punish_until_round:
            return Action.D
        if t == self.r:
            if history.opponent_actions.shape[0] >= 1:
                last_idx = t - 2
                last_idx = max(0, min(history.opponent_actions.shape[0] - 1, last_idx))
                others_coop_last = int(np.sum(history.opponent_actions[last_idx, :]))
            else:
                others_coop_last = 0
            if others_coop_last >= self.T:
                return Action.C
            else:
                return Action.D
        lookback = min(self.w, t - 1)
        start_idx = t - lookback - 1
        end_idx = t - 1
        start_idx = max(0, start_idx)
        end_idx = max(start_idx, min(history.opponent_actions.shape[0], end_idx))
        if start_idx >= end_idx:
            return Action.C
        recent_matrix = history.opponent_actions[start_idx:end_idx, :]
        others_counts = np.sum(recent_matrix, axis=1).astype(int)
        recent_avg = float(np.mean(others_counts))
        under_count = int(np.sum(others_counts < self.T))
        mis_rate = float(under_count) / float(lookback)
        if recent_avg >= self.T:
            return Action.C
        desired_P = int(math.ceil(mis_rate * self.punish_base))
        desired_P = max(1, desired_P)
        remaining = max(0, self.r - t)
        P = min(desired_P, self.punish_max, remaining)
        if P <= 0:
            return Action.D
        self.punish_until_round = t + P - 1
        return Action.D

class Strategy_COOPERATIVE_11(BaseStrategy):
    """
    Reciprocal Conditional Cooperator with graded punishment and forgiveness.

    - Starts cooperating (round 1).
    - Defects on the last round.
    - In the final E rounds before the last, only cooperates if the previous round
      was unanimous cooperation (conservative endgame).
    - In normal play, rewards rounds that meet a cooperation threshold T_total.
      If the previous round falls below T_total, initiates a short proportional
      punishment (defection for P rounds, bounded by P_max). Punishments are
      forgiving: after P rounds, resume cooperation if signals improve.
    - Applies smoothing: if the average cooperativeness over a short window W
      remains high, single-round lapses are ignored (no punishment).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_remaining = 0
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        self._n = n
        self._r = r
        self._k = k
        self.G = 1
        self.W = min(5, r)
        self.T_frac = 0.5 + 0.4 * (k - 1.0) / max(1, n - 1)
        self.T_total = int(math.ceil(self.T_frac * n))
        self.P_max = min(int(math.ceil(r / 5.0)), 4)
        self.E = min(3, r)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        n = self._n
        r = self._r
        if history is None:
            self.punish_remaining = 0
            return Action.C
        if t == r:
            return Action.D
        if t >= r - self.E + 1 and t <= r - 1:
            last_opponents = history.opponent_actions[-1, :]
            last_me = history.my_actions[-1]
            total_last = int(np.sum(last_opponents)) + (1 if bool(last_me) else 0)
            if total_last == n:
                return Action.C
            else:
                return Action.D
        if self.punish_remaining > 0:
            self.punish_remaining -= 1
            return Action.D
        last_opponents = history.opponent_actions[-1, :]
        last_me = history.my_actions[-1]
        C_prev = int(np.sum(last_opponents)) + (1 if bool(last_me) else 0)
        window_len = min(self.W, history.round_number)
        totals = []
        if window_len > 0:
            opp_slice = history.opponent_actions[-window_len:, :]
            me_slice = history.my_actions[-window_len:]
            opp_sums = np.sum(opp_slice.astype(int), axis=1)
            me_sums = np.array([1 if bool(x) else 0 for x in me_slice], dtype=int)
            totals = (opp_sums + me_sums).tolist()
            avg_coop = float(np.mean(totals))
        else:
            avg_coop = float(C_prev)
        if avg_coop >= self.T_frac * n:
            return Action.C
        if C_prev >= self.T_total:
            return Action.C
        gap = max(0, self.T_total - C_prev)
        P = min(self.P_max, 1 + int(math.ceil(gap / float(n) * self.P_max)))
        self.punish_remaining = max(0, P - 1)
        return Action.D

class Strategy_COOPERATIVE_12(BaseStrategy):
    """
    Adaptive Proportional Conditional Cooperator (APCC)

    - Starts by cooperating.
    - Measures recent cooperation of other players over a rolling window.
    - Cooperates when recent cooperation is above a threshold.
    - If cooperation falls sufficiently, applies a short proportional punishment (defection)
      whose length grows with the magnitude of the shortfall, then forgives and re-tests.
    - Uses a stricter threshold in the final L rounds (endgame awareness).
    - All decisions depend only on the public history and game parameters.

    Parameters derived from game:
    - w = min(5, r)
    - theta = 0.50
    - L = min(3, r)
    - theta_end = 0.70
    - delta = 0.10
    - p_test = 0.5
    - S = w
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = int(getattr(game_description, 'n_rounds', 1))
        self.w = min(5, max(1, r))
        self.theta = 0.5
        self.L = min(3, max(1, r))
        self.theta_end = 0.7
        self.delta = 0.1
        self.p_test = 0.5
        self.S = self.w
        self.punishment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        completed = int(history.round_number)
        t = completed + 1
        r = int(getattr(self.game_description, 'n_rounds', completed + 1))
        n_players = int(getattr(self.game_description, 'n_players', 2))
        if n_players < 2:
            n_players = 2
        m = min(self.w, completed)
        if m <= 0:
            rho = 1.0
        else:
            recent = history.opponent_actions[-m:, :]
            total_other_coops = float(np.sum(recent))
            denom = float((n_players - 1) * m)
            rho = total_other_coops / denom if denom > 0.0 else 1.0
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            return Action.D
        theta_cur = self.theta_end if t > r - self.L else self.theta
        if rho >= theta_cur:
            return Action.C
        elif rho >= theta_cur - self.delta:
            if random.random() < self.p_test:
                return Action.C
            else:
                return Action.D
        else:
            shortfall = max(0.0, theta_cur - rho)
            punishment_length = max(1, int(math.ceil(shortfall * self.S)))
            self.punishment_counter = punishment_length - 1
            return Action.D

class Strategy_COOPERATIVE_13(BaseStrategy):
    """
    Adaptive Conditional Cooperation with Graduated Forgiveness (ACCGF).

    - Starts cooperating, defects in the last round.
    - Tracks short-term 'strikes' per opponent for suspicious defections.
    - Uses a sliding window average of cooperators to decide whether to cooperate.
    - If cooperation falls below a majority threshold, enters a short graded punishment
      proportional to the shortfall. Punishments are limited and forgivable.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.coop_threshold = math.ceil(self.n / 2)
        self.W = min(5, self.r)
        self.strike_limit = max(2, math.floor(self.W / 2))
        self.max_punish = min(5, max(0, self.r - 1))
        self.S = max(2, self.W // 2)
        self.n_opponents = max(0, self.n - 1)
        self.max_strike = max(5, self.max_punish + 2)
        self.strikes = [0] * self.n_opponents
        self.punishment_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        completed = history.round_number
        t = completed + 1
        if t >= self.r:
            return Action.D
        window_end = completed - 1
        window_start = max(0, completed - self.W)
        window_len = max(0, completed - window_start)
        if window_len > 0:
            opp_slice = history.opponent_actions[window_start:completed, :]
            opp_sums = opp_slice.sum(axis=1)
            my_slice = history.my_actions[window_start:completed].astype(int)
            cooperators_per_round = opp_sums + my_slice
        else:
            cooperators_per_round = np.array([], dtype=int)
        if self.punishment_timer > 0 and completed > 0:
            last_cooperators = int(cooperators_per_round[-1]) if window_len > 0 else 0
            if last_cooperators >= self.coop_threshold:
                self.punishment_timer = 0
                last_round_idx = completed - 1
                for j in range(self.n_opponents):
                    if history.opponent_actions[last_round_idx, j]:
                        self.strikes[j] = max(0, self.strikes[j] - 1)
            else:
                self.punishment_timer = max(0, self.punishment_timer - 1)
        if window_len > 0:
            for j in range(self.n_opponents):
                opp_window = history.opponent_actions[window_start:completed, j]
                defections_in_window = int((~opp_window).sum())
                suspicious_defs = 0
                for idx_in_window, global_idx in enumerate(range(window_start, completed)):
                    opp_act = bool(history.opponent_actions[global_idx, j])
                    if not opp_act:
                        total_coops = int(cooperators_per_round[idx_in_window])
                        if total_coops >= self.coop_threshold:
                            suspicious_defs += 1
                if suspicious_defs > 0:
                    self.strikes[j] = min(self.max_strike, self.strikes[j] + suspicious_defs)
                if defections_in_window == 0 and window_len > 0:
                    self.strikes[j] = max(0, self.strikes[j] - 1)
                if completed >= self.S:
                    recent_start = completed - self.S
                    recent_slice = history.opponent_actions[recent_start:completed, j]
                    if recent_slice.all():
                        self.strikes[j] = max(0, self.strikes[j] - 1)
                self.strikes[j] = int(min(max(0, self.strikes[j]), self.max_strike))
        if self.punishment_timer > 0:
            return Action.D
        if window_len > 0:
            recent_cooperators_avg = float(np.mean(cooperators_per_round))
        else:
            recent_cooperators_avg = 0.0
        suspicious_count = sum((1 for s in self.strikes if s > self.strike_limit))
        suspicious_threshold = math.floor(self.n / 3)
        if recent_cooperators_avg >= self.coop_threshold and suspicious_count <= suspicious_threshold:
            return Action.C
        else:
            shortfall = max(0, self.coop_threshold - int(round(recent_cooperators_avg)))
            self.punishment_timer = min(self.max_punish, 1 + shortfall)
            return Action.D

class Strategy_COOPERATIVE_14(BaseStrategy):
    """
    Adaptive Generous Conditional Cooperator (AGCC)

    - Starts cooperative (unless game is very short).
    - Conditions cooperation on recent fraction of other players cooperating (window W).
    - Quick short punishment when clearly exploited (you cooperated while most others defected).
    - Forgives when group cooperation recovers.
    - Randomized occasional restart when group mostly defects.
    - Always defects in last round (and in r==2 by design).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_remain = 0
        self.last_action = None
        self.g = 1
        self.P = 2
        self.p_restart = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        W = min(4, max(1, r - 1))
        denom = (n - 1) / 2.0
        if denom > 0:
            adjust_step = math.floor((k - 1.0) / denom)
        else:
            adjust_step = 0
        theta = 0.5 - 0.1 * adjust_step
        theta = max(0.3, theta)
        if r == 2:
            self.last_action = False
            return Action.D
        if history is None:
            if r >= 3:
                self.last_action = True
                return Action.C
            else:
                self.last_action = False
                return Action.D
        rounds_completed = int(history.round_number)
        t = rounds_completed + 1
        if t == r:
            self.last_action = False
            return Action.D
        if self.punish_remain > 0:
            action = Action.D
            self.punish_remain = max(0, self.punish_remain - 1)
            self.last_action = False
            return action
        opp_actions = history.opponent_actions
        if rounds_completed == 0 or opp_actions.size == 0:
            q = 0.0
            last_round_others = 0.0
        else:
            start_idx = max(0, rounds_completed - W)
            window = opp_actions[start_idx:rounds_completed, :]
            if window.size == 0:
                q = 0.0
            else:
                row_sums = np.sum(window, axis=1).astype(float)
                fractions = row_sums / float(n - 1)
                q = float(np.mean(fractions))
            last_row = opp_actions[rounds_completed - 1, :]
            last_round_others = float(np.sum(last_row)) / float(n - 1)
        my_prev_action = bool(history.my_actions[-1]) if rounds_completed >= 1 else False
        if my_prev_action and last_round_others <= 1.0 / 3.0:
            self.punish_remain = int(self.P)
            self.last_action = False
            return Action.D
        if q >= theta:
            self.last_action = True
            return Action.C
        if q <= 0.1:
            if random.random() < float(self.p_restart):
                self.last_action = True
                return Action.C
            else:
                self.last_action = False
                return Action.D
        self.last_action = False
        return Action.D

class Strategy_COOPERATIVE_15(BaseStrategy):
    """
    Conditional Cooperate with Graded Punishment and Forgiveness (CGPF).

    - Starts by cooperating.
    - Uses a moving majority threshold (relaxed when MPCR is high) to decide whether
      the group is being sufficiently reciprocal.
    - If the group falls below the threshold, triggers a graded punishment (defection)
      whose length increases with the number of defectors in the previous round.
      Punishment is forgiven immediately if a unanimous cooperation round is observed.
    - In the final round, defects by default unless the previous round was unanimous cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_length = 0
        self.n_players = int(self.game_description.n_players)
        self.n_rounds = int(self.game_description.n_rounds)
        self.k = float(self.game_description.k)
        self.mpcr = self.k / max(1, self.n_players)
        raw_threshold = 0.5 + 0.25 * (1.0 - self.mpcr)
        self.f_threshold = min(max(raw_threshold, 0.5), 0.75)
        self.others_needed = math.ceil((self.n_players - 1) * self.f_threshold)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = int(state.round_number)
        if history.round_number == 0:
            return Action.C
        others_prev = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size else 0
        my_prev_action = bool(history.my_actions[-1])
        m_prev = others_prev + (1 if my_prev_action else 0)
        others_coop_prev = m_prev - (1 if my_prev_action else 0)
        if t == self.n_rounds:
            if m_prev == self.n_players:
                return Action.C
            else:
                return Action.D
        if m_prev == self.n_players:
            self.punish_length = 0
            return Action.C
        if self.punish_length > 0:
            self.punish_length = max(0, self.punish_length - 1)
            return Action.D
        if others_coop_prev >= self.others_needed:
            return Action.C
        else:
            num_defectors = self.n_players - m_prev
            total_punish_rounds = 1 + math.ceil(num_defectors / 2)
            self.punish_length = max(0, total_punish_rounds - 1)
            return Action.D

class Strategy_COOPERATIVE_16(BaseStrategy):
    """
    Adaptive Threshold with Forgiving Punishment (ATFP)

    - Seeds cooperation on the first round.
    - Uses an adaptive threshold τ (depends on n and k) to decide whether the previous
      round showed sufficient cooperation.
    - If group cooperation falls short, initiates a short, proportional punishment.
    - After a punishment finishes, forgives with probability γ on the next round.
    - Probes occasionally with small probability ε to detect shifts.
    - In an endgame window near the last rounds, requires near-unanimous cooperation to
      cooperate; always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        denom = self.n / 2.0
        raw_tau = 0.5
        if denom != 0:
            raw_tau = 0.5 - 0.2 * ((self.k - denom) / denom)
        self.tau = min(0.6, max(0.2, raw_tau))
        self.P_max = 3
        self.gamma = 0.2
        self.epsilon = 0.02
        self.E = min(3, max(1, math.floor(0.05 * self.r)))
        self.punish_count = 0
        self._just_ended_punishment = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:

        def possibly_probe(action: Action, allow_probe: bool=True) -> Action:
            if not allow_probe:
                return action
            if random.random() < self.epsilon:
                return Action.C if action == Action.D else Action.D
            return action
        t = int(state.round_number)
        if history is None:
            return possibly_probe(Action.C, allow_probe=True)
        if t >= self.r:
            return Action.D
        if history.round_number == 0:
            m_prev = 0
        else:
            last_my = bool(history.my_actions[-1])
            last_opponents = history.opponent_actions[-1, :]
            opp_coops = int(np.sum(last_opponents))
            m_prev = opp_coops + (1 if last_my else 0)
        f = m_prev / float(self.n) if self.n > 0 else 0.0
        if t > self.r - self.E:
            if self.punish_count > 0:
                action = Action.D
                self.punish_count = max(0, self.punish_count - 1)
                if self.punish_count == 0:
                    self._just_ended_punishment = True
                return possibly_probe(action, allow_probe=True)
            if m_prev == self.n:
                action = Action.C
            else:
                action = Action.D
            return possibly_probe(action, allow_probe=True)
        if self.punish_count > 0:
            action = Action.D
            self.punish_count = max(0, self.punish_count - 1)
            if self.punish_count == 0:
                self._just_ended_punishment = True
            return possibly_probe(action, allow_probe=True)
        if self._just_ended_punishment:
            self._just_ended_punishment = False
            if random.random() < self.gamma:
                return possibly_probe(Action.C, allow_probe=True)
        if f >= self.tau:
            return possibly_probe(Action.C, allow_probe=True)
        else:
            shortfall = max(0.0, self.tau - f)
            P = int(min(self.P_max, math.ceil(shortfall * self.n)))
            if P < 1:
                P = 1
            self.punish_count = max(0, P - 1)
            if self.punish_count == 0:
                self._just_ended_punishment = True
            else:
                self._just_ended_punishment = False
            return possibly_probe(Action.D, allow_probe=True)

class Strategy_COOPERATIVE_17(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Proportional Punishment (ACCPP).

    - Starts cooperating to signal cooperative intent.
    - Tolerant to a small number of other players' defections (noise).
    - If observed defections exceed tolerance in the most recent round, starts
      a proportional punishment whose length is computed from game parameters.
    - Punishment length scales with number of defectors and is computed so that
      it removes the one-shot incentive to defect (L_deterrent).
    - Forgives after the finite punishment and returns to cooperation.
    - In the final H = L_deterrent rounds, switches to defection since punishment
      cannot be credibly carried out.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.k = float(game_description.k)
        self.r = int(game_description.n_rounds)
        denom = self.k - 1
        if denom <= 0:
            self.L_deterrent = 1
        else:
            G = (self.n - self.k) / float(self.n)
            self.L_deterrent = max(1, math.ceil(G / denom))
        self.tolerance = 1 if self.n >= 4 else 0
        self.W = 1
        self.H_endgame = self.L_deterrent
        self.mode = 'Cooperate'
        self.punish_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = int(state.round_number) if state is not None else history.round_number + 1
        remaining = self.r - t + 1
        if remaining <= self.H_endgame:
            return Action.D
        d = 0
        if history.opponent_actions is not None and history.opponent_actions.size > 0:
            last_round_actions = history.opponent_actions[-1, :]
            d = int(last_round_actions.size - int(np.sum(last_round_actions)))
        else:
            d = 0
        if self.punish_remaining > 0:
            if d > self.tolerance:
                M = math.ceil(d / float(self.tolerance + 1))
                self.punish_remaining += int(M * self.L_deterrent)
            action = Action.D
            self.punish_remaining -= 1
            if self.punish_remaining <= 0:
                self.punish_remaining = 0
                self.mode = 'Cooperate'
            else:
                self.mode = 'Punish'
            return action
        if d > self.tolerance:
            M = math.ceil(d / float(self.tolerance + 1))
            self.punish_remaining = int(M * self.L_deterrent) - 1
            if self.punish_remaining < 0:
                self.punish_remaining = 0
            self.mode = 'Punish'
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_18(BaseStrategy):
    """
    Generous Conditional Cooperator with Limited Punishment (GCC-LP).

    - Starts by cooperating.
    - Cooperates when a majority (T = ceil(n/2)) cooperated in the previous round.
    - If cooperation falls below T, applies a short proportional punishment:
      P = min(P_max, max(1, ceil((n - S_prev)/2))) with P_max = 3.
      The strategy defects for P rounds (current + P-1 remaining).
    - Punishments are finite and can be re-triggered if subsequent rounds remain
      underthreshold. Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_remaining = 0
        self.T = math.ceil(self.game_description.n_players / 2)
        self.P_max = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punishment_remaining = 0
            if self.game_description.n_rounds <= 1:
                return Action.D
            return Action.C
        t = history.round_number + 1
        r = self.game_description.n_rounds
        n = self.game_description.n_players
        if t >= r:
            self.punishment_remaining = 0
            return Action.D
        if self.punishment_remaining > 0:
            self.punishment_remaining -= 1
            return Action.D
        if history.round_number == 0:
            return Action.C
        last_opponent_actions = history.opponent_actions[-1]
        opp_cooperators = int(np.sum(last_opponent_actions))
        my_last_coop = 1 if bool(history.my_actions[-1]) else 0
        S_prev = opp_cooperators + my_last_coop
        if S_prev >= self.T:
            return Action.C
        P = min(self.P_max, max(1, math.ceil((n - S_prev) / 2)))
        self.punishment_remaining = max(0, P - 1)
        return Action.D

class Strategy_COOPERATIVE_19(BaseStrategy):
    """
    Adaptive Majority-Triggered Conditional Cooperator (AMTCC).

    Starts cooperating. Monitors a sliding window of recent rounds to estimate
    each player's cooperation rate. If a sufficient fraction (M) of players
    have cooperation rate >= alpha, continue cooperating. Otherwise initiate
    a finite punishment phase of defections whose length is adaptive to the
    severity of the shortfall and modestly escalates on repeated violations.
    Small-probability forgiveness/tests (eps_forgive) allow recovery from
    mistakes. Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = int(min(10, max(1, self.r - 1)))
        raw_alpha = 0.6 + 0.4 * (1.0 - self.k / float(self.n))
        self.alpha = max(0.5, min(0.95, raw_alpha))
        self.M = int(math.ceil(self.alpha * self.n))
        self.max_punish = int(min(5, max(1, math.floor(self.r / 4.0))))
        self.eps_forgive = min(0.08, 4.0 / float(max(1, self.r)))
        self.punishment_counter = 0
        self.punish_escalation = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            upcoming_round = 1
        else:
            upcoming_round = history.round_number + 1
        if upcoming_round == self.r:
            return Action.D
        if history is None:
            return Action.C
        forgive_roll = random.random() < self.eps_forgive
        if self.punishment_counter > 0:
            self.punishment_counter = max(0, self.punishment_counter - 1)
            if forgive_roll:
                self.punish_escalation = max(0, self.punish_escalation - 1)
                return Action.C
            else:
                return Action.D
        available_rounds = history.round_number
        consider_len = min(self.w, available_rounds)
        if consider_len <= 0:
            return Action.C
        my_recent = np.asarray(history.my_actions[-consider_len:], dtype=float)
        opp_recent = np.asarray(history.opponent_actions[-consider_len:, :], dtype=float)
        try:
            recent_all = np.concatenate([my_recent.reshape(consider_len, 1), opp_recent], axis=1)
        except Exception:
            recent_all = np.column_stack([my_recent] + [opp_recent[:, j] for j in range(opp_recent.shape[1])])
        coop_rates = np.mean(recent_all, axis=0)
        coop_count = int(np.sum(coop_rates >= self.alpha))
        if coop_count >= self.M:
            if forgive_roll:
                self.punish_escalation = max(0, self.punish_escalation - 1)
            else:
                self.punish_escalation = max(0, self.punish_escalation - 1)
            return Action.C
        else:
            raw_length = 1 + (self.M - coop_count) + self.punish_escalation
            punishment_length = int(min(self.max_punish, raw_length))
            self.punishment_counter = max(0, punishment_length - 1)
            self.punish_escalation = int(min(self.max_punish, self.punish_escalation + 1))
            if forgive_roll:
                return Action.C
            else:
                return Action.D

class Strategy_COOPERATIVE_20(BaseStrategy):
    """
    Conditional Cooperative with Proportional Punishment and Forgiveness (CC-PPF).

    Starts by cooperating. Monitors recent cooperation among other players and
    cooperates when recent levels are above a threshold T. If the last round's
    cooperation among others falls below T, punish proportionally (short,
    targeted defection streak), then forgive. Tolerates small deviations and
    occasionally probes when the group has largely collapsed. Acts
    conservatively in the final rounds to avoid endgame exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.punish_until_round = 0
        self.L = min(5, max(1, self.r - 1))
        self.tau = 0.25
        self.T = 1.0 - self.tau
        self.P_max = min(3, max(1, self.r))
        self.E = min(2, max(1, math.floor(self.r / 10)))
        self.p_probe = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            return Action.C
        completed = history.round_number
        if completed == 0:
            return Action.C
        if t <= self.punish_until_round:
            return Action.D

        def frac_others_in_round(round_index: int) -> float:
            coop_count = float(np.sum(history.opponent_actions[round_index, :]))
            return coop_count / float(self.n - 1)
        if t > self.r - self.E:
            if t == self.r:
                last_round_idx = completed - 1
                opponents_all_coop = bool(np.all(history.opponent_actions[last_round_idx, :]))
                my_prev_coop = bool(history.my_actions[last_round_idx])
                if opponents_all_coop and my_prev_coop:
                    return Action.C
                else:
                    return Action.D
            else:
                last_round_idx = completed - 1
                last_frac = frac_others_in_round(last_round_idx)
                return Action.C if last_frac >= self.T else Action.D
        last_round_idx = completed - 1
        last_frac = frac_others_in_round(last_round_idx)
        num_rounds_for_avg = min(self.L, completed)
        start_idx = completed - num_rounds_for_avg
        if num_rounds_for_avg <= 0:
            recent_frac = last_frac
        else:
            fracs = []
            for idx in range(start_idx, completed):
                fracs.append(frac_others_in_round(idx))
            recent_frac = float(np.mean(np.array(fracs, dtype=float)))
        if recent_frac < 0.2:
            if random.random() < self.p_probe:
                return Action.C
            else:
                return Action.D
        if last_frac >= self.T:
            return Action.C
        coop_count_last = int(np.sum(history.opponent_actions[last_round_idx, :]))
        def_count = self.n - 1 - coop_count_last
        P = min(self.P_max, max(1, int(def_count)))
        self.punish_until_round = t + P - 1
        return Action.D

class Strategy_COOPERATIVE_21(BaseStrategy):
    """
    Responsive Proportional Reciprocity (RPR)

    - Start by cooperating.
    - Always defect in the final round.
    - For intermediate rounds, observe the number of cooperators in the previous round.
      If the previous round shows strong cooperation (s_prev == n or s_prev >= threshold_count),
      cooperate. Otherwise enter a proportional punishment: defect for up to `d` rounds
      (d = number of defectors observed), clipped by remaining rounds. After punishment
      ends, play a single probe (cooperate once). During punishment there is optional small
      generosity (cooperate with small probability) to help escape deterministic cycles.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.threshold_count = math.ceil(self.n - self.k / 2.0)
        self.punishment_remaining = 0
        self.probe_pending = False
        self.generosity = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = history.round_number + 1
        if t == self.r:
            return Action.D
        prev_my = bool(history.my_actions[-1])
        prev_opponents = history.opponent_actions[-1, :]
        s_prev = int(prev_my) + int(prev_opponents.sum())
        d = self.n - s_prev
        if self.punishment_remaining > 0:
            if self.generosity > 0.0 and random.random() < self.generosity:
                action = Action.C
            else:
                action = Action.D
            self.punishment_remaining -= 1
            if self.punishment_remaining <= 0:
                self.punishment_remaining = 0
                self.probe_pending = True
            return action
        if self.probe_pending:
            self.probe_pending = False
            return Action.C
        if s_prev == self.n or s_prev >= self.threshold_count:
            return Action.C
        remaining_rounds_after_this = max(0, self.r - t)
        self.punishment_remaining = min(remaining_rounds_after_this, d)
        return Action.D

class Strategy_COOPERATIVE_22(BaseStrategy):
    """
    Adaptive Win–Stay Lose–Shift with Aspirations (AWSLS-A).

    - Starts with optimistic aspiration equal to k and cooperates on the first round.
    - For rounds t >= 2 (and not the final round): if last round payoff >= aspiration_before_that_round
      then repeat last action (win->stay), otherwise switch action (lose->shift).
    - Aspiration is an exponential moving average with learning rate alpha (default 0.3),
      clamped to [payoff_min=1, payoff_max=k]. The aspiration used to evaluate the last payoff
      is the value before incorporating that last payoff (i.e., computed from earlier rounds only).
    - Always defects in the final round.
    - The implementation derives aspiration values from the provided history each call,
      so no external persistent state beyond configuration is required.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.alpha = 0.3
        self.payoff_min = 1.0
        self.payoff_max = float(game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:

        def clamp(x, low, high):
            if x < low:
                return low
            if x > high:
                return high
            return x
        r = self.game_description.n_rounds
        k = self.game_description.k
        alpha = self.alpha
        if history is None:
            return Action.C
        completed = history.round_number
        t = completed + 1
        if t == r:
            return Action.D
        A = k
        num_updates = max(0, completed - 1)
        for i in range(num_updates):
            pi = float(history.my_payoffs[i])
            A = clamp((1.0 - alpha) * A + alpha * pi, self.payoff_min, self.payoff_max)
        last_payoff = float(history.my_payoffs[-1])
        last_action_bool = bool(history.my_actions[-1])
        if last_payoff >= A:
            next_action_bool = last_action_bool
        else:
            next_action_bool = not last_action_bool
        return Action.C if next_action_bool else Action.D

class Strategy_COOPERATIVE_23(BaseStrategy):
    """
    Aspiration-based Win-Stay Lose-Shift (AWL) for Public Goods.

    - Starts by cooperating (unless the game has only one round, in which case it defects).
    - For rounds 2..r-1: compares the previous round's payoff to the aspiration level
      that existed before observing that payoff. If payoff >= aspiration -> repeat previous
      action (win -> stay). Otherwise -> switch action (lose -> shift).
    - Aspiration is updated using exponential recency with learning rate lambda:
      A_next = (1 - lambda) * A + lambda * payoff.
      The aspiration used for decisions in round t is the aspiration after processing
      payoffs up to round t-2 (i.e., before incorporating the most recent payoff).
    - Always defects in the final round.
    - Deterministic given A0 and lambda. Uses only own past actions and payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.A0 = (1.0 + float(game_description.k)) / 2.0
        self.lambda_ = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        total_rounds = int(self.game_description.n_rounds)
        if history is None or history.round_number == 0:
            if total_rounds == 1:
                return Action.D
            return Action.C
        completed = int(history.round_number)
        current_round = completed + 1
        if current_round >= total_rounds:
            return Action.D
        A = float(self.A0)
        num_updates = max(0, completed - 1)
        if num_updates > 0:
            for pi in history.my_payoffs[:num_updates]:
                A = (1.0 - self.lambda_) * A + self.lambda_ * float(pi)
        prev_action_bool = bool(history.my_actions[-1])
        prev_payoff = float(history.my_payoffs[-1])
        if prev_payoff >= A:
            return Action.C if prev_action_bool else Action.D
        else:
            return Action.D if prev_action_bool else Action.C

class Strategy_COOPERATIVE_24(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Proportional Punishment (ACCPP).

    - Starts by cooperating.
    - Tracks recent fraction of cooperators among opponents with a short window (w),
      weighting the most recent round more heavily.
    - Cooperates when recent cooperation >= T_coop.
    - Gives a small grace for isolated lapses; if many opponents defected last round,
      enters a proportional, capped punishment (series of defections).
    - After punishment ends, plays a one-round cooperation test to restore cooperation.
    - Slightly more forgiving when the public-good multiplier (g = k/n) is larger.
    - In the final round, defects unless the immediate past round shows very strong cooperation.
    - If cooperation is essentially zero for an extended period, switches to permanent defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.g = self.k / float(self.n)
        self.w = min(5, max(1, self.r - 1))
        raw = 0.625 - 0.15 * self.g
        self.T_coop = max(0.45, min(0.75, raw))
        self.T_tolerate = 0.4
        self.P_max = max(1, math.ceil(self.r / 10))
        self.punishment_timer = 0
        self.in_punishment = False
        self.just_exited_punishment = False
        self.permanent_defect = False
        self.last_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            self.last_action = Action.C
            return Action.C
        if self.permanent_defect:
            self.last_action = Action.D
            return Action.D
        t = int(state.round_number)
        rounds_done = history.round_number
        if rounds_done >= 1:
            last_row = history.opponent_actions[-1, :]
            S_last = float(np.mean(last_row)) if last_row.size > 0 else 0.0
        else:
            S_last = 0.0
        offending_rate_last = 1.0 - S_last
        recent_rows = min(self.w, rounds_done)
        if recent_rows >= 1:
            recent_slice = history.opponent_actions[-recent_rows:, :]
            per_round_rates = np.mean(recent_slice, axis=1).astype(float)
            if len(per_round_rates) == 1:
                f_recent = float(per_round_rates[0])
            else:
                weights = np.ones_like(per_round_rates)
                weights[-1] = 2.0
                f_recent = float(np.average(per_round_rates, weights=weights))
        else:
            f_recent = 0.0
        if rounds_done >= max(3 * self.w, 3) and f_recent < 0.05:
            self.permanent_defect = True
            self.last_action = Action.D
            return Action.D
        if self.just_exited_punishment:
            self.just_exited_punishment = False
            self.last_action = Action.C
            return Action.C
        if self.in_punishment and self.punishment_timer > 0:
            action = Action.D
            self.punishment_timer -= 1
            if self.punishment_timer <= 0:
                self.in_punishment = False
                self.punishment_timer = 0
                self.just_exited_punishment = True
            self.last_action = action
            return action
        if t == self.r:
            threshold = max(0.8, self.T_coop + 0.15)
            if S_last >= threshold:
                self.last_action = Action.C
                return Action.C
            else:
                self.last_action = Action.D
                return Action.D
        if f_recent >= self.T_coop:
            self.last_action = Action.C
            return Action.C
        if offending_rate_last <= 1.0 - self.T_tolerate:
            self.last_action = Action.C
            return Action.C
        p = offending_rate_last
        raw_len = 1 + round((p - self.T_tolerate) * float(self.n))
        raw_len = max(1, int(raw_len))
        punish_len = min(self.P_max, raw_len)
        self.punishment_timer = int(punish_len)
        self.in_punishment = True
        action = Action.D
        self.punishment_timer -= 1
        if self.punishment_timer <= 0:
            self.in_punishment = False
            self.punishment_timer = 0
            self.just_exited_punishment = True
        self.last_action = action
        return action

class Strategy_COOPERATIVE_25(BaseStrategy):
    """
    Adaptive Generous Conditional Cooperator (AGCC)

    - Starts by cooperating.
    - Uses a short moving-window estimate of recent group cooperation (W rounds).
    - Cooperates if recent cooperation fraction >= target_rate.
    - Otherwise defects for a short, proportional punishment of P rounds (including current round).
    - Forgiving: punishments are short and moving average smooths isolated lapses.
    - Last-round (endgame-aware): cooperate only if the last W rounds were unanimous cooperation; otherwise defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(4, max(1, self.r - 1))
        self.target_rate = min(0.8, 0.5 + 0.25 * (self.k / max(1.0, self.n)))
        raw_P = math.ceil((self.n - self.k) / 2.0)
        self.P = min(3, max(1, int(raw_P)))
        self.forgiveness_tolerance = 1
        self.punish_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punish_counter = 0
            return Action.C
        completed = history.round_number
        t = completed + 1
        if completed == 0:
            self.punish_counter = 0
            return Action.C
        if t == self.r:
            if completed >= self.W:
                unanimous = True
                for idx in range(completed - self.W, completed):
                    my_coop = bool(history.my_actions[idx])
                    opp_coops = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size else 0
                    total_coops = int(my_coop) + int(opp_coops)
                    if total_coops != self.n:
                        unanimous = False
                        break
                if unanimous:
                    return Action.C
            return Action.D
        if self.punish_counter > 0:
            self.punish_counter -= 1
            return Action.D
        L = min(self.W, completed)
        coop_sum = 0
        for idx in range(completed - L, completed):
            my_coop = bool(history.my_actions[idx])
            opp_coops = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size else 0
            coop_sum += int(my_coop) + int(opp_coops)
        if L > 0:
            f = coop_sum / float(L) / float(self.n)
        else:
            f = 0.0
        if f >= self.target_rate:
            return Action.C
        else:
            self.punish_counter = max(0, self.P - 1)
            return Action.D

class Strategy_COOPERATIVE_26(BaseStrategy):
    """
    Conditional, forgiving public-goods strategy.

    - Opens with cooperation.
    - Tolerates a small fraction of defectors (tol_frac, default 0.20).
    - If too many defectors occur in the previous round, triggers a short punishment
      (defect for a limited number of rounds, scaled by how many defectors exceeded tolerance).
    - Punishment length is capped (P_max) and scaled down for short games.
    - Forgives immediately if cooperation returns to acceptable levels during punishment.
    - Defects in the final Endgame_defect_rounds rounds to avoid last-round exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tol_frac_base = 0.2
        self.P_max_param = 3
        self.Endgame_defect_rounds = 1
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        if n > 1:
            k_scaled = max(0.0, min(1.0, (k - 1.0) / float(n - 1)))
        else:
            k_scaled = 0.0
        self.tol_frac = self.tol_frac_base + k_scaled * (0.4 - self.tol_frac_base)
        self.tol = max(1, math.floor((n - 1) * self.tol_frac))
        self.P_max = min(self.P_max_param, max(1, math.floor(r / 10)))
        self.punish_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punish_remaining = 0
            return Action.C
        t = int(state.round_number)
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        if t > r - int(self.Endgame_defect_rounds):
            return Action.D
        if history.round_number <= 0:
            return Action.C
        prev_idx = history.round_number - 1
        try:
            opp_row = history.opponent_actions[prev_idx, :]
            coop_opponents = int(np.sum(opp_row))
        except Exception:
            coop_opponents = 0
        try:
            self_prev = bool(history.my_actions[prev_idx])
        except Exception:
            self_prev = True
        coop_prev = coop_opponents + (1 if self_prev else 0)
        if getattr(self, 'punish_remaining', 0) > 0:
            if coop_prev >= n - self.tol:
                self.punish_remaining = 0
                return Action.C
            else:
                self.punish_remaining = max(0, self.punish_remaining - 1)
                return Action.D
        if coop_prev >= n - self.tol:
            return Action.C
        if coop_prev >= math.ceil(n / 2):
            return Action.C
        extra_defectors = max(0, n - coop_prev - self.tol)
        punish_length = min(self.P_max, 1 + extra_defectors)
        self.punish_remaining = max(0, punish_length - 1)
        return Action.D

class Strategy_COOPERATIVE_27(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Forgiveness (ACCF)

    - Starts by cooperating to signal willingness.
    - Cooperates when a recent majority of others have cooperated (avg_others >= theta).
    - When others' cooperation falls below threshold, enters a proportional short punishment
      phase (defect for punish_remaining rounds). Punishment length scales with how far
      avg_others is below theta and is capped.
    - Detects persistent defectors over a longer window; if too many persistent defectors
      are present, behaves defensively (defects) until the situation improves.
    - Forgives quickly: if others restore cooperation by the end of punishment, re-enter
      cooperative mode immediately.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.L = min(5, max(1, self.r - 1))
        self.W = min(10, max(1, self.r - 1))
        self.theta = 0.5
        self.P_base = 2
        self.P_max = min(5, max(1, self.r - 1))
        self.phi = 0.3
        self.d_thresh = max(1, math.floor((self.n - 1) / 3))
        self.mode = 'COOPERATE'
        self.punish_remaining = 0
        self.last_punish_length = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or getattr(history, 'round_number', 0) == 0:
            t = 1
        else:
            t = getattr(state, 'round_number', history.round_number + 1)
        if t == self.r:
            self.punish_remaining = 0
            return Action.D
        if t == 1:
            self.mode = 'COOPERATE'
            self.punish_remaining = 0
            self.last_punish_length = 0
            return Action.C
        completed = history.round_number
        start_round = max(1, t - self.L)
        end_round = t - 1
        start_idx = start_round - 1
        end_idx = end_round - 1
        S_len = end_round - start_round + 1
        opp_actions = history.opponent_actions
        start_idx = max(0, start_idx)
        end_idx = min(opp_actions.shape[0] - 1, end_idx)
        S_len = max(1, end_idx - start_idx + 1)
        recent_slice = opp_actions[start_idx:end_idx + 1, :]
        sum_recent = float(np.sum(recent_slice))
        avg_others = sum_recent / (S_len * (self.n - 1))
        last_w = min(self.W, completed)
        last_slice = opp_actions[-last_w:, :]
        coop_counts = np.sum(last_slice, axis=0).astype(float)
        coop_rates = coop_counts / float(last_w)
        persistent_count = int(np.sum(coop_rates < self.phi))
        if persistent_count > self.d_thresh:
            self.punish_remaining = 0
            return Action.D
        if self.mode == 'PUNISH' and self.punish_remaining > 0:
            action = Action.D
            self.punish_remaining = max(0, self.punish_remaining - 1)
            if self.punish_remaining == 0:
                if avg_others >= self.theta:
                    self.mode = 'COOPERATE'
                    self.last_punish_length = 0
                else:
                    new_len = max(1, self.last_punish_length + 1)
                    new_len = min(self.P_max, new_len)
                    remaining_rounds = max(0, self.r - t)
                    self.last_punish_length = min(new_len, remaining_rounds) if remaining_rounds > 0 else 0
                    self.punish_remaining = self.last_punish_length
                    self.mode = 'PUNISH'
            return action
        if self.mode == 'COOPERATE':
            if avg_others >= self.theta:
                return Action.C
            else:
                diff = max(0.0, self.theta - avg_others)
                extra = math.ceil(diff * 10.0)
                planned = self.P_base + int(extra)
                planned = min(self.P_max, planned)
                remaining_rounds = max(0, self.r - t)
                punish_len = min(planned, remaining_rounds) if remaining_rounds > 0 else 0
                self.mode = 'PUNISH'
                self.last_punish_length = punish_len
                self.punish_remaining = punish_len
                return Action.D
        return Action.D

class Strategy_COOPERATIVE_28(BaseStrategy):
    """
    Generous Conditional Cooperate-and-Exclude (GCCE)

    - Starts by cooperating.
    - Monitors recent cooperation among others using a short lookback window.
    - If recent cooperation falls below an adaptive threshold (θ tied to n and k),
      triggers a short, possibly escalating punishment of defections.
    - Uses small-probability forgiveness probes during punishment to allow recovery.
    - Resets escalation when cooperation is re-established.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_counter = 0
        self.escalate_count = 0
        self.P_base = 2
        self.escalate_limit = 3
        self.epsilon = 0.05
        self.theta_min = 0.2
        self.theta_max = 0.8
        self.n = int(self.game_description.n_players)
        self.r = int(self.game_description.n_rounds)
        self.k = float(self.game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = int(state.round_number)
        L = min(5, max(0, t - 1))
        if L == 0:
            return Action.C
        recent_opponent_actions = history.opponent_actions[-L:, :]
        total_cooperations = float(np.sum(recent_opponent_actions))
        denom = float((self.n - 1) * L)
        p_hat = total_cooperations / denom if denom > 0.0 else 0.0
        theta_raw = 0.5 * (self.n / max(1e-08, self.k))
        theta = min(max(theta_raw, self.theta_min), self.theta_max)
        if t == self.r:
            last_round_unanimous = False
            if history.round_number >= 1:
                opponents_all_coop = bool(np.all(history.opponent_actions[-1, :]))
                my_last_coop = bool(history.my_actions[-1])
                last_round_unanimous = opponents_all_coop and my_last_coop
            if self.punish_counter > 0 or p_hat < theta:
                return Action.D
            elif p_hat >= 0.99 or last_round_unanimous:
                return Action.C
            else:
                return Action.D
        if self.punish_counter > 0:
            if random.random() < self.epsilon:
                action = Action.C
            else:
                action = Action.D
            self.punish_counter = max(0, self.punish_counter - 1)
            if self.punish_counter == 0 and p_hat >= theta:
                self.escalate_count = 0
            return action
        if p_hat >= theta:
            return Action.C
        else:
            remaining_rounds_in_game = max(0, self.r - t + 1)
            desired_total_punish = self.P_base + self.escalate_count
            total_to_punish = min(desired_total_punish, remaining_rounds_in_game)
            self.punish_counter = max(0, total_to_punish - 1)
            self.escalate_count = min(self.escalate_count + 1, self.escalate_limit)
            return Action.D

class Strategy_COOPERATIVE_29(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC)

    - Starts by cooperating.
    - Uses a sliding window to estimate per-player cooperation rates.
    - Flags persistent defectors and punishes them for a short finite P rounds.
    - Enters a global punishment mode when a majority of players are flagged.
    - Occasionally probes (small probability) to test whether cooperation can resume.
    - Defects on the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.W = min(10, max(1, r - 1))
        self.q = 0.5
        self.P_default = 2
        self.P = self.P_default
        self.gamma = 0.4
        self.p_probe = 0.05
        self.n_players = n
        self.n_opponents = max(0, n - 1)
        self.punish_timers = np.zeros(self.n_opponents, dtype=int)
        self.global_punish_timer = 0
        self.last_seen_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory):
        if history is None or history.round_number == 0:
            self.last_seen_rounds = 0
            return Action.C
        completed_rounds = history.round_number
        t = completed_rounds + 1
        r = self.game_description.n_rounds
        rounds_passed = completed_rounds - self.last_seen_rounds
        if rounds_passed > 0:
            if self.n_opponents > 0:
                self.punish_timers = np.maximum(0, self.punish_timers - int(rounds_passed))
            self.global_punish_timer = max(0, int(self.global_punish_timer) - int(rounds_passed))
        window = min(self.W, completed_rounds)
        if window > 0 and self.n_opponents > 0:
            recent_opponent_actions = history.opponent_actions[-window:, :]
            coop_counts_opponents = np.sum(recent_opponent_actions.astype(int), axis=0)
        else:
            coop_counts_opponents = np.zeros(self.n_opponents, dtype=int)
        if window > 0:
            recent_my_actions = history.my_actions[-window:].astype(int)
            my_coop_count_recent = int(np.sum(recent_my_actions))
        else:
            my_coop_count_recent = 0
        if completed_rounds >= 1:
            prev_opponent_actions = history.opponent_actions[-1, :] if self.n_opponents > 0 else np.array([])
            prev_my_action = bool(history.my_actions[-1])
            m_prev = int(np.sum(prev_opponent_actions.astype(int))) + (1 if prev_my_action else 0)
        else:
            m_prev = 0
        near_end_window = r - t <= self.P
        R_opponents = np.zeros(self.n_opponents, dtype=float)
        if window > 0 and self.n_opponents > 0:
            R_opponents = coop_counts_opponents.astype(float) / float(window)
        defect_counts_opponents = np.zeros(self.n_opponents, dtype=int)
        if window > 0 and self.n_opponents > 0:
            defect_counts_opponents = window - coop_counts_opponents
        if not near_end_window and window > 0:
            for j in range(self.n_opponents):
                R_j = R_opponents[j]
                defects_j = int(defect_counts_opponents[j])
                if R_j < self.gamma and defects_j > 1:
                    self.punish_timers[j] = int(self.P)
        flagged_count = int(np.sum(self.punish_timers > 0)) if self.n_opponents > 0 else 0
        if flagged_count > self.n_players / 2:
            self.global_punish_timer = int(self.P)
        if window > 0 and self.n_opponents > 0:
            for j in range(self.n_opponents):
                if self.punish_timers[j] > 0 and R_opponents[j] >= self.gamma:
                    self.punish_timers[j] = 0
        threshold = math.ceil(self.q * self.n_players)
        if m_prev >= threshold:
            if self.n_opponents > 0:
                self.punish_timers[:] = 0
            self.global_punish_timer = 0
        if r - t <= self.P:
            rounds_left = max(0, r - t)
            if self.n_opponents > 0:
                self.punish_timers = np.minimum(self.punish_timers, int(rounds_left))
            self.global_punish_timer = min(self.global_punish_timer, int(rounds_left))
        if t == r:
            action = Action.D
        elif self.global_punish_timer > 0:
            action = Action.D
        elif self.n_opponents > 0 and np.any(self.punish_timers > 0):
            action = Action.D
        elif m_prev >= threshold:
            action = Action.C
        elif random.random() < float(self.p_probe):
            action = Action.C
        else:
            action = Action.D
        self.last_seen_rounds = completed_rounds
        return action

class Strategy_COOPERATIVE_30(BaseStrategy):
    """
    Reciprocal Gradual with Proportional Forgiveness (RGPF)

    Starts cooperative, sustains cooperation when others are cooperative,
    and applies short proportional punishments when the group's cooperation
    falls below an adaptive threshold. Punishments are bounded and can be
    ended early if others quickly return to cooperative behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punish_timer = 0
            return Action.C
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        completed = int(history.round_number)
        t = completed + 1
        L = min(5, max(1, math.floor(r / 10)))
        g = (k - 1.0) / max(1.0, n - 1.0)
        theta_raw = 0.5 - 0.2 * g
        theta = min(0.55, max(0.25, theta_raw))
        P_base = min(3, max(1, math.ceil((1.0 - g) * 3.0)))
        P_max = min(3, max(1, math.ceil(r / 10.0)))

        def compute_others_recent_rate():
            if completed == 0:
                return 0.0
            idx_start = max(0, completed - L)
            idx_end = completed
            window_len = idx_end - idx_start
            if window_len <= 0:
                return 0.0
            opp_slice = history.opponent_actions[idx_start:idx_end, :]
            opp_coop_sum = float(np.sum(opp_slice))
            denom = float(window_len * (n - 1))
            if denom <= 0.0:
                return 0.0
            return opp_coop_sum / denom
        if t == r:
            all_coop = True
            if completed == 0:
                all_coop = False
            else:
                for idx in range(0, completed):
                    coop_count = int(bool(history.my_actions[idx])) + int(np.sum(history.opponent_actions[idx, :]))
                    if coop_count != n:
                        all_coop = False
                        break
            return Action.C if all_coop else Action.D
        others_recent_rate = compute_others_recent_rate()
        if self.punish_timer > 0:
            theta_return = theta + 0.15
            if others_recent_rate >= theta_return:
                self.punish_timer = 0
                return Action.C
            else:
                self.punish_timer -= 1
                return Action.D
        if completed == 0:
            return Action.C
        last_idx = completed - 1
        coop_last = int(bool(history.my_actions[last_idx])) + int(np.sum(history.opponent_actions[last_idx, :]))
        if coop_last == n:
            return Action.C
        if others_recent_rate >= theta:
            return Action.C
        defectors_last = n - coop_last
        P = min(P_max, max(1, P_base + math.floor(defectors_last / 2)))
        self.punish_timer = max(0, P - 1)
        return Action.D

class Strategy_COOPERATIVE_31(BaseStrategy):
    """
    Forgiving Conditional Cooperator (FCC)

    - Starts by cooperating to signal willingness.
    - Uses a sliding window (H up to 3 rounds) to compute the recent fraction of OTHER players
      who cooperated. If that fraction is high enough, cooperate; if it is clearly low,
      defect. A small slack (delta) creates hysteresis: once we begin defecting due to low
      cooperation, we remain in a punishment mode until the cooperation rate recovers above q.
    - Always defects in the final Tfinal rounds (Tfinal = 1 by default).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._in_punishment = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        H = min(3, max(1, r - 1))
        q = 0.5
        delta = 0.15
        Tfinal = 1
        t = int(state.round_number)
        if history is None:
            self._in_punishment = False
            return Action.C
        if t > r - Tfinal:
            return Action.D
        if t == 1:
            return Action.C
        rounds_available = history.round_number
        rounds_to_use = min(H, rounds_available)
        if rounds_to_use <= 0:
            return Action.C
        start_idx = rounds_available - rounds_to_use
        end_idx = rounds_available
        opp_slice = history.opponent_actions[start_idx:end_idx, :]
        n_minus_1 = max(1, n - 1)
        coop_counts = np.sum(opp_slice, axis=1).astype(float)
        frac_each_round = coop_counts / float(n_minus_1)
        cooperation_rate_others = float(np.mean(frac_each_round))
        if cooperation_rate_others >= q:
            self._in_punishment = False
            return Action.C
        if cooperation_rate_others < q - delta:
            self._in_punishment = True
            return Action.D
        if self._in_punishment:
            return Action.D
        else:
            return Action.C

class Strategy_COOPERATIVE_32(BaseStrategy):
    """
    Adaptive Group Reciprocity with Proportional Punishment (AGR-PP).

    - Starts cooperative (round 1) to attempt to establish cooperation.
    - Uses a short memory (m = min(5, r-1)) to estimate recent cooperation among other players.
    - If the recent cooperation rate among others >= T (default 0.5) cooperates; otherwise begins
      a punishment phase (defecting) whose length is proportional to the shortfall and capped by maxPunish.
    - During punishment there is a small probability epsilon to probe with a cooperation (forgiveness probe).
    - Always defects in the final round.
    - punishLeft persists across rounds as internal state.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.m = min(5, max(1, self.r - 1))
        self.T = 0.5
        self.maxPunish = min(math.ceil(self.r / 3), max(1, self.r - 1))
        self.epsilon = 0.05
        self.punishLeft = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t >= self.r:
            return Action.D
        if history is None:
            self.punishLeft = 0
            return Action.C
        if self.punishLeft > 0:
            if random.random() < self.epsilon:
                action = Action.C
            else:
                action = Action.D
            self.punishLeft = max(0, self.punishLeft - 1)
            return action
        rounds_completed = history.round_number
        L = min(self.m, rounds_completed)
        if L <= 0:
            return Action.C
        try:
            recent_opponent_actions = history.opponent_actions[-L:, :]
        except Exception:
            return Action.C
        total_others_coop = float(np.sum(recent_opponent_actions))
        denom = float(L * max(1, self.n - 1))
        coop_rate = total_others_coop / denom if denom > 0.0 else 0.0
        if coop_rate >= self.T:
            return Action.C
        else:
            rawShortfall = max(0.0, self.T - coop_rate)
            punish_len = min(self.maxPunish, max(1, math.ceil(rawShortfall * self.r)))
            self.punishLeft = max(0, punish_len - 1)
            return Action.D

class Strategy_COOPERATIVE_33(BaseStrategy):
    """
    Adaptive Generous Conditional Cooperator (AGCC)

    Starts by cooperating. Uses a short lookback window L to monitor the public
    fraction of cooperators. Continues cooperating while recent cooperation is
    above a threshold gamma minus slack. If others fall below threshold, switch
    to defect to avoid exploitation. While defecting, only resume cooperation
    after observing a clear recovery (at least recovery_len rounds within the
    lookback window with fraction >= gamma + slack). Always defect on the
    final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_last_action = None
        self.defect_since_round = None
        n = game_description.n_players
        r = game_description.n_rounds
        k = game_description.k
        self.L = min(5, max(1, r - 1))
        self.gamma = max(0.5, float(k) / float(n))
        self.slack = 1.0 / (2.0 * float(self.L))
        self.recovery_len = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if history is None:
            self.my_last_action = Action.C
            self.defect_since_round = None
            return Action.C
        if t == r:
            self.my_last_action = Action.D
            if self.defect_since_round is None:
                self.defect_since_round = t
            return Action.D
        completed_rounds = history.round_number
        if completed_rounds > 0:
            opp_sums = np.sum(history.opponent_actions.astype(int), axis=1)
            my_sums = history.my_actions.astype(int)
            totals = opp_sums + my_sums
        else:
            totals = np.array([], dtype=int)
        m = min(self.L, max(0, completed_rounds))
        if m == 0:
            action = Action.C
            self.my_last_action = action
            self.defect_since_round = None if action == Action.C else self.defect_since_round
            return action
        consider = totals[-m:]
        avg_frac = float(np.mean(consider)) / float(n)
        if completed_rounds > 0:
            last_action_from_history = Action.C if bool(history.my_actions[-1]) else Action.D
            if self.my_last_action is None:
                self.my_last_action = last_action_from_history
        else:
            self.my_last_action = Action.C
        if self.my_last_action == Action.C:
            if avg_frac >= self.gamma - self.slack:
                action = Action.C
                self.my_last_action = Action.C
                self.defect_since_round = None
                return action
            else:
                action = Action.D
                self.my_last_action = Action.D
                if self.defect_since_round is None:
                    self.defect_since_round = t
                return action
        if self.my_last_action == Action.D:
            strong_threshold = self.gamma + self.slack
            frac_consider = consider.astype(float) / float(n)
            count_strong = int(np.count_nonzero(frac_consider >= strong_threshold))
            if count_strong >= self.recovery_len:
                action = Action.C
                self.my_last_action = Action.C
                self.defect_since_round = None
                return action
            else:
                action = Action.D
                self.my_last_action = Action.D
                if self.defect_since_round is None:
                    self.defect_since_round = t
                return action

class Strategy_COOPERATIVE_34(BaseStrategy):
    """
    Adaptive Threshold Reciprocity with Forgiveness (ATRF).
    - Starts by cooperating.
    - Reciprocates when a recent fraction of players cooperated above an adaptive threshold.
    - Briefly punishes when cooperation drops, but forgives quickly if cooperation returns.
    - Switches to safe-mode (always defect) if the group is persistently uncooperative.
    - In the final round, plays a best-response to the empirical estimate of others' cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, max(1, self.r))
        self.P = 2
        self.F = 2
        self.baseline_threshold = 0.5
        raw_thresh = self.baseline_threshold + 0.4 * (1.0 - self.k / max(1.0, self.n))
        self.threshold = min(0.95, max(0.5, raw_thresh))
        self.L = max(3, math.ceil(self.r / 5))
        self.punishment_counter = 0
        self.safe_mode = False
        self.low_coop_streak = 0
        self.recovery_streak = 0
        self.last_action_was_punish = False
        self.per_player_coops = np.zeros(self.n, dtype=int)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.C
        completed = history.round_number
        t = completed + 1
        n = self.n
        r = self.r
        k = self.k
        rounds_done = completed
        my_actions = np.asarray(history.my_actions, dtype=np.bool_)
        opp_actions = np.asarray(history.opponent_actions, dtype=np.bool_)
        if opp_actions.size == 0:
            full_actions = my_actions.reshape((rounds_done, 1))
        else:
            full_actions = np.concatenate([opp_actions, my_actions.reshape((rounds_done, 1))], axis=1)
        if full_actions.shape[1] == n:
            self.per_player_coops = np.sum(full_actions, axis=0).astype(int)
        else:
            coops = np.sum(full_actions, axis=0).astype(int).tolist()
            if len(coops) < n:
                coops = coops + [0] * (n - len(coops))
            else:
                coops = coops[:n]
            self.per_player_coops = np.array(coops, dtype=int)
        end_idx = completed - 1
        start_idx = max(0, completed - self.W)
        if completed <= 0:
            recent_mean = 1.0
            last_round_fraction = 1.0
            longrun_fraction = 1.0
        else:
            ms = []
            for s in range(start_idx, end_idx + 1):
                if opp_actions.size == 0:
                    tot = int(my_actions[s])
                else:
                    tot = int(np.sum(opp_actions[s, :]) + int(my_actions[s]))
                ms.append(tot)
            if len(ms) == 0:
                recent_mean = 1.0
            else:
                recent_mean = float(np.mean([m / float(n) for m in ms]))
            if completed >= 1:
                if opp_actions.size == 0:
                    last_tot = int(my_actions[-1])
                else:
                    last_tot = int(np.sum(opp_actions[-1, :]) + int(my_actions[-1]))
                last_round_fraction = float(last_tot) / float(n)
            else:
                last_round_fraction = 1.0
            total_coops_all = int(np.sum(self.per_player_coops))
            longrun_fraction = float(total_coops_all) / float(max(1, completed * n))
        exploitation_cutoff = min(self.threshold - 0.1, 0.25)
        forgiveness_threshold = max(0.0, self.threshold - 0.05)
        if longrun_fraction < exploitation_cutoff:
            self.low_coop_streak += 1
        else:
            self.low_coop_streak = 0
        if self.low_coop_streak >= self.L:
            self.safe_mode = True
        if self.safe_mode:
            can_recover = False
            if completed >= self.F:
                ok = True
                for s in range(completed - self.F, completed):
                    if opp_actions.size == 0:
                        tot_s = int(my_actions[s])
                    else:
                        tot_s = int(np.sum(opp_actions[s, :]) + int(my_actions[s]))
                    frac_s = float(tot_s) / float(n)
                    if frac_s < self.threshold:
                        ok = False
                        break
                if ok:
                    can_recover = True
            if can_recover:
                self.safe_mode = False
                self.low_coop_streak = 0
                self.recovery_streak = 0
            else:
                return Action.D
        if t == r:
            est_others = int(round(recent_mean * float(n - 1)))
            pi_if_C = 0.0 + k / float(n) * float(est_others + 1)
            pi_if_D = 1.0 + k / float(n) * float(est_others)
            if pi_if_C >= pi_if_D:
                return Action.C
            else:
                return Action.D
        if self.punishment_counter > 0:
            if recent_mean >= forgiveness_threshold:
                self.punishment_counter = 0
                self.last_action_was_punish = False
            else:
                self.punishment_counter -= 1
                self.last_action_was_punish = True
                return Action.D
        if last_round_fraction >= self.threshold:
            self.last_action_was_punish = False
            return Action.C
        else:
            self.punishment_counter = max(0, self.P - 1)
            self.last_action_was_punish = True
            return Action.D

class Strategy_COOPERATIVE_35(BaseStrategy):
    """
    Forgiving Proportional Trigger (FPT)

    - Starts cooperating.
    - Cooperates while the previous round showed near-universal cooperation (<=1 defector).
    - If >=2 defectors observed last round, punish by defecting for a finite number of rounds
      proportional to the shortfall (number of defectors), capped by remaining rounds.
    - Punishment length base is computed to deter a one-shot deviation:
        base_punish = max(1, ceil((1 - k/n) / (k - 1)))
    - Near the end of the game (remaining rounds <= base_punish) switch to unconditional defection (endgame).
    - Maintains a punishment_timer (persisting in the strategy instance) that decreases each round
      while punishing. After punishment completes, the strategy forgives and returns to cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        denom = self.k - 1.0 if self.k - 1.0 != 0 else 1e-12
        raw = (1.0 - self.k / float(self.n)) / denom
        self.base_punish = max(1, int(math.ceil(raw)))
        self.punishment_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = getattr(state, 'round_number', None)
        if t is None:
            t = history.round_number + 1
        t = int(t)
        H = max(0, self.r - t + 1)
        if H <= self.base_punish:
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer = max(0, self.punishment_timer - 1)
            return Action.D
        last_idx = -1
        opp_coops = int(np.sum(history.opponent_actions[last_idx, :]))
        my_last = int(history.my_actions[last_idx])
        S_prev = opp_coops + my_last
        if S_prev >= self.n - 1:
            return Action.C
        s = self.n - S_prev
        punishment_length = int(min(max(0, H - 1), self.base_punish * int(s)))
        self.punishment_timer = max(0, punishment_length - 1)
        return Action.D

class Strategy_COOPERATIVE_36(BaseStrategy):
    """
    Adaptive Public Goods strategy:
    - Start by cooperating to signal intent.
    - Cooperate if last round had sufficiently many cooperators (threshold adaptive to k).
    - If group cooperation falls below threshold, punish for a short, bounded sequence of rounds.
    - After punishment ends, occasionally probe (probabilistic forgiveness) to try to restore cooperation.
    - Be contrite: if you were a lone/small defector last round, return to cooperation.
    - Defect in the final round unless the previous M rounds were fully cooperative.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_timer = 0
        self.last_probe_round = -9999
        self._allow_probe = False
        self.last_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        theta_frac = 1.0 - 1.0 / k if k != 0 else 0.5
        if theta_frac < 0.5:
            theta_frac = 0.5
        elif theta_frac > 0.9:
            theta_frac = 0.9
        threshold_count = int(math.ceil(theta_frac * n))
        M = min(3, max(1, int(math.floor(r / 10))))
        punish_len = min(5, max(1, int(math.ceil(0.1 * r))))
        p_probe = 0.2
        if history is None:
            self.last_action = True
            return Action.C

        def cooperators_in_round(idx: int) -> int:
            opp_coops = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size else 0
            my_coop = 1 if history.my_actions[idx] else 0
            return opp_coops + my_coop
        if t == r:
            if history.round_number >= M:
                all_full = True
                for idx in range(history.round_number - M, history.round_number):
                    if cooperators_in_round(idx) != n:
                        all_full = False
                        break
                if all_full:
                    self.last_action = True
                    return Action.C
            self.last_action = False
            return Action.D
        action_is_coop = None
        if self.punishment_timer > 0:
            action_is_coop = False
            self.punishment_timer -= 1
            if self.punishment_timer == 0:
                self._allow_probe = True
        else:
            if history.round_number == 0:
                m_prev = 0
                my_last = False
            else:
                m_prev = cooperators_in_round(history.round_number - 1)
                my_last = bool(history.my_actions[-1])
            if m_prev >= threshold_count:
                action_is_coop = True
            elif m_prev >= max(0, threshold_count - 1) and (not my_last):
                action_is_coop = True
            else:
                action_is_coop = False
                self.punishment_timer = punish_len - 1
                if self.punishment_timer == 0:
                    self._allow_probe = True
                else:
                    self._allow_probe = False
        if (not action_is_coop) is False:
            pass
        if self.punishment_timer == 0 and self._allow_probe:
            if self.last_probe_round < t - 1:
                if random.random() < p_probe:
                    action_is_coop = True
                    self.last_probe_round = t
                    self._allow_probe = False
        if action_is_coop is None:
            action_is_coop = False
        self.last_action = bool(action_is_coop)
        return Action.C if action_is_coop else Action.D

class Strategy_COOPERATIVE_37(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Forgiveness (ACCF).

    - Starts strongly cooperative (p=1.0).
    - If clearly exploited (you cooperated but would have been better off defecting by more than
      exploitation_margin), retaliates by scheduling a short deterministic punishment window
      (play D for a few upcoming rounds) and halves the cooperation propensity p.
    - Outside punishment windows, cooperates probabilistically with probability p.
    - After non-exploitative rounds, adapts p toward the observed group cooperation rate with
      learning rate alpha.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.alpha = 0.3
        self.punish_fraction = 0.25
        self.exploitation_margin = 0.001
        self.p = 1.0
        self.punish_schedule = set()
        self.punish_counter = 0
        self.last_m = None
        self.last_payoff = None
        self.last_processed_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide action for the current round state.round_number (1-based).

        The method also processes any newly completed rounds from history (updates p and
        schedules punishments when exploitation is detected). Updates are done before deciding
        the current round's action, so the strategy follows the specified update-after-round flow.
        """
        current_round = int(state.round_number)
        if history is not None and history.round_number > self.last_processed_round:
            for round_idx in range(self.last_processed_round, history.round_number):
                t = round_idx + 1
                my_action = bool(history.my_actions[round_idx])
                opp_coops = int(np.sum(history.opponent_actions[round_idx, :]))
                m_t = int(my_action) + opp_coops
                my_payoff = float(history.my_payoffs[round_idx])
                self.last_m = m_t
                self.last_payoff = my_payoff
                if t == self.r:
                    continue
                if t in self.punish_schedule:
                    self.punish_schedule.discard(t)
                    self.punish_counter = len(self.punish_schedule)
                    continue
                if my_action:
                    other_cooperators = m_t - 1
                    payoff_if_defected = 1.0 + self.k / self.n * float(other_cooperators)
                    if my_payoff < payoff_if_defected - self.exploitation_margin:
                        remaining_rounds = max(0, self.r - t)
                        X = int(round(self.punish_fraction * remaining_rounds))
                        X = max(1, X)
                        punish_rounds = set(range(t + 1, min(self.r + 1, t + X + 1)))
                        self.punish_schedule.update(punish_rounds)
                        self.punish_counter = len(self.punish_schedule)
                        self.p = max(0.0, self.p * 0.5)
                        continue
                observed_rate = float(m_t) / float(self.n)
                self.p = self.p + self.alpha * (observed_rate - self.p)
                if self.p < 0.0:
                    self.p = 0.0
                elif self.p > 1.0:
                    self.p = 1.0
            self.last_processed_round = history.round_number
        if current_round == self.r:
            return Action.D
        if current_round in self.punish_schedule:
            self.punish_schedule.discard(current_round)
            self.punish_counter = len(self.punish_schedule)
            return Action.D
        if random.random() < float(self.p):
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_38(BaseStrategy):
    """
    Optimistic Threshold with Proportional Punishment and Forgiveness (OT-PPF).

    - If k/n > 1: unconditional cooperation every round.
    - Otherwise: start cooperative, monitor recent fraction of other players cooperating
      (window up to 5 rounds). If community cooperation is high (>= 0.5) reward by cooperating.
      If clearly low (< 0.4) apply a short, proportional punishment (defect for a few rounds).
      After any punishment, perform a one-round cooperative probe. If the probe elicits
      enough cooperation (>= 0.4) return to cooperation; otherwise re-initiate punishment
      with slightly increased length (bounded by remaining horizon).
      Forgiveness region (0.4 <= f < 0.5) cooperates (with optional small random D-probe).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_counter = 0
        self.just_finished_punishment = False
        self.did_probe_last_round = False
        self.theta_coop = 0.5
        self.theta_low = 0.4
        self.forgive_prob = 0.9

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        k_over_n = k / float(n)
        t = int(state.round_number)
        if k_over_n > 1.0:
            self.punish_counter = 0
            self.just_finished_punishment = False
            self.did_probe_last_round = False
            return Action.C
        if history is None:
            self.punish_counter = 0
            self.just_finished_punishment = False
            self.did_probe_last_round = False
            return Action.C
        if t == r:
            self.punish_counter = 0
            self.just_finished_punishment = False
            self.did_probe_last_round = False
            return Action.D

        def coop_fraction_window_from_history(hist: PlayerHistory, W: int) -> float:
            window = hist.opponent_actions[-W:, :]
            n_other = n - 1
            if n_other <= 0:
                return 1.0
            total_coop = float(np.sum(window))
            return total_coop / (W * n_other)
        if self.punish_counter > 0:
            action = Action.D
            self.punish_counter -= 1
            if self.punish_counter == 0:
                self.just_finished_punishment = True
            return action
        if self.just_finished_punishment:
            self.just_finished_punishment = False
            self.did_probe_last_round = True
            return Action.C
        if self.did_probe_last_round:
            W = min(5, history.round_number)
            coop_frac = coop_fraction_window_from_history(history, W)
            self.did_probe_last_round = False
            if coop_frac >= self.theta_low:
                self.punish_counter = 0
                return Action.C
            else:
                remaining = r - t + 1
                base_punish = max(1, math.floor(remaining * 0.15))
                max_punish = max(1, math.floor(remaining / 3))
                factor = (self.theta_low - coop_frac) / self.theta_low
                punish = math.ceil(base_punish * factor)
                punish = min(max_punish, max(1, punish + 1))
                self.punish_counter = punish
                self.punish_counter -= 1
                if self.punish_counter == 0:
                    self.just_finished_punishment = True
                return Action.D
        W = min(5, history.round_number)
        if W <= 0:
            return Action.C
        coop_frac = coop_fraction_window_from_history(history, W)
        if coop_frac >= self.theta_coop:
            return Action.C
        if coop_frac < self.theta_low:
            remaining = r - t + 1
            base_punish = max(1, math.floor(remaining * 0.15))
            max_punish = max(1, math.floor(remaining / 3))
            factor = (self.theta_low - coop_frac) / self.theta_low
            punish = math.ceil(base_punish * factor)
            punish = min(max_punish, max(1, punish))
            self.punish_counter = punish
            self.punish_counter -= 1
            if self.punish_counter == 0:
                self.just_finished_punishment = True
            return Action.D
        if random.random() < self.forgive_prob:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_39(BaseStrategy):
    """
    Parameterized Finite-Deterrence (PFD) strategy for the N-player public goods game.

    - Computes a basic deterrence length P0 from (n, k).
    - Cooperates by default while credible punishment is possible.
    - If any opponent defected in the immediately preceding round, impose a finite,
      proportional punishment of length P = min(remaining-1, P0 * m_defected).
      Punishment rounds are played as collective defections and are finite; afterwards
      the strategy forgives and returns to cooperation (unless too close to endgame).
    - If too few rounds remain to credibly punish (remaining rounds <= P0), switch
      to ENDGAME and defect for all remaining rounds.
    """
    COOPERATE = 'COOPERATE'
    PUNISH = 'PUNISH'
    ENDGAME = 'ENDGAME'

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = float(game_description.n_players)
        k = float(game_description.k)
        r = int(game_description.n_rounds)
        if k <= 1.0:
            self.P0 = math.inf
        else:
            self.P0 = math.ceil((1.0 - k / n) / (k - 1.0))
        self.mode = self.COOPERATE if r > self.P0 else self.ENDGAME
        self.punish_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        P0 = self.P0
        t_curr = getattr(state, 'round_number', None)
        if t_curr is None:
            if history is None:
                t_curr = 1
            else:
                t_curr = history.round_number + 1
        if history is None:
            if r > P0:
                self.mode = self.COOPERATE
                self.punish_remaining = 0
                return Action.C
            else:
                self.mode = self.ENDGAME
                self.punish_remaining = 0
                return Action.D
        mode = self.COOPERATE if r > P0 else self.ENDGAME
        punish_remaining = 0
        completed_rounds = history.round_number
        for past_t in range(1, completed_rounds + 1):
            R = r - past_t + 1
            if R <= P0:
                mode = self.ENDGAME
            if mode == self.ENDGAME:
                continue
            if mode == self.PUNISH:
                if punish_remaining > 0:
                    punish_remaining -= 1
                    if punish_remaining == 0:
                        mode = self.COOPERATE if R - 1 > P0 else self.ENDGAME
                    continue
                else:
                    mode = self.COOPERATE
            if R == 1:
                continue
            if past_t > 1:
                prev_idx = past_t - 2
                try:
                    m_defected = int(np.sum(~history.opponent_actions[prev_idx, :]))
                except Exception:
                    m_defected = 0
                if m_defected > 0:
                    punish_length = min(float(R - 1), float(P0) * float(max(1, m_defected)))
                    punish_remaining = int(math.floor(punish_length + 1e-12))
                    mode = self.PUNISH
                    if punish_remaining > 0:
                        punish_remaining -= 1
                    if punish_remaining == 0:
                        mode = self.COOPERATE if R - 1 > P0 else self.ENDGAME
                    continue
        R = r - t_curr + 1
        if R <= P0:
            self.mode = self.ENDGAME
            self.punish_remaining = 0
            return Action.D
        if mode == self.PUNISH and punish_remaining > 0:
            punish_remaining -= 1
            if punish_remaining == 0:
                next_mode = self.COOPERATE if R - 1 > P0 else self.ENDGAME
            else:
                next_mode = self.PUNISH
            self.mode = next_mode
            self.punish_remaining = punish_remaining
            return Action.D
        if R == 1:
            self.mode = self.ENDGAME
            self.punish_remaining = 0
            return Action.D
        if history.round_number >= 1:
            prev_idx = history.round_number - 1
            try:
                m_defected = int(np.sum(~history.opponent_actions[prev_idx, :]))
            except Exception:
                m_defected = 0
            if m_defected > 0:
                punish_length = min(float(R - 1), float(P0) * float(max(1, m_defected)))
                punish_remaining = int(math.floor(punish_length + 1e-12))
                if punish_remaining > 0:
                    punish_remaining -= 1
                next_mode = self.PUNISH if punish_remaining > 0 else self.COOPERATE if R - 1 > P0 else self.ENDGAME
                self.mode = next_mode
                self.punish_remaining = punish_remaining
                return Action.D
        self.mode = self.COOPERATE
        self.punish_remaining = 0
        return Action.C

class Strategy_COOPERATIVE_40(BaseStrategy):
    """
    Adaptive Threshold Reciprocity (ATR) for N-player Public Goods Game.

    - Starts cooperating.
    - Uses a sliding window of recent rounds to estimate group cooperation rate.
    - Cooperates when recent cooperation >= adaptive threshold gamma.
    - Starts short, proportional punishment episodes (P rounds) on clear drops or if lone cooperator.
    - Forgives small/isolated deviations via win-stay behavior.
    - Defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, max(1, self.r - 1))
        raw_gamma = 0.5 - 0.3 * (self.k - 1.0) / max(1, self.n - 1)
        self.gamma = max(0.2, min(0.8, raw_gamma))
        self.delta = 1.0 / float(self.n)
        self.P = min(3, max(1, math.floor(self.r / 10)))
        self.punishment_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        completed = int(history.round_number)
        t = completed + 1
        if t > self.r:
            return Action.D
        if t == self.r:
            self.punishment_remaining = 0
            return Action.D

        def total_cooperators_in_round(one_indexed_round: int) -> int:
            idx = one_indexed_round - 1
            my_coop = 1 if bool(history.my_actions[idx]) else 0
            opps = history.opponent_actions[idx, :] if history.opponent_actions.size else np.array([], dtype=bool)
            opp_coops = int(np.sum(opps))
            return my_coop + opp_coops
        window_start = max(1, t - self.w)
        window_end = t - 1
        window_indices = list(range(window_start, window_end + 1))
        if not window_indices:
            coop_rate_window = 0.0
        else:
            totals = [total_cooperators_in_round(rnd) for rnd in window_indices]
            coop_rate_window = float(np.mean([tot / float(self.n) for tot in totals]))
        prev_window_end = window_start - 1
        if prev_window_end >= 1:
            prev_window_start = max(1, window_start - self.w)
            prev_indices = list(range(prev_window_start, prev_window_end + 1))
            if prev_indices:
                prev_totals = [total_cooperators_in_round(rnd) for rnd in prev_indices]
                coop_rate_prev_window = float(np.mean([tot / float(self.n) for tot in prev_totals]))
            else:
                coop_rate_prev_window = coop_rate_window
        else:
            coop_rate_prev_window = coop_rate_window
        total_coop_last = total_cooperators_in_round(t - 1)
        was_only_cooperator = total_coop_last == 1
        if self.punishment_remaining > 0:
            sustained_recovery = coop_rate_window >= self.gamma and coop_rate_prev_window >= self.gamma
            if sustained_recovery:
                self.punishment_remaining = 0
                return Action.C
            else:
                self.punishment_remaining -= 1
                return Action.D
        if coop_rate_window >= self.gamma:
            return Action.C
        recent_drop = coop_rate_prev_window - coop_rate_window > self.delta
        small_coop_threshold = max(1, math.floor(0.2 * self.n))
        few_cooperators_last = total_coop_last <= small_coop_threshold
        if was_only_cooperator or recent_drop or few_cooperators_last:
            self.punishment_remaining = max(0, self.P - 1)
            return Action.D
        if completed >= 1:
            last_my_action = bool(history.my_actions[-1])
            return Action.C if last_my_action else Action.D
        return Action.C

class Strategy_COOPERATIVE_41(BaseStrategy):
    """
    Counterfactual Win-Stay/Lose-Shift with controlled forgiveness (C-WSLS-F).

    - Starts by cooperating.
    - After each round compares actual payoff to the counterfactual payoff had it played the opposite action.
      If actual >= counterfactual -> repeat previous action (stay). Else -> switch (shift).
    - Forgiveness: if currently set to defect but recent cooperation among others rebounds (window W,
      threshold theta_f) and there are at least 2 rounds left, resume cooperating.
    - Persistent-defection safeguard: if others have been near-universal defectors for S recent rounds
      and few rounds remain, defect for the remaining rounds.
    - Defects in the final round (endgame safety).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = 2
        self.theta_f = 0.5
        self.S = 3
        self.small_eps = 1.0 / max(1.0, float(self.n_players))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = int(state.round_number)
        r_total = self.n_rounds
        n = self.n_players
        k = self.k
        if t == r_total:
            return Action.D
        a_prev_bool = bool(history.my_actions[-1])
        m_prev = int(np.sum(history.opponent_actions[-1, :]))
        pi_prev = float(history.my_payoffs[-1])
        if a_prev_bool:
            counterfactual = 1.0 + k / float(n) * float(m_prev)
        else:
            counterfactual = 0.0 + k / float(n) * float(m_prev + 1)
        if pi_prev >= counterfactual:
            action_bool = a_prev_bool
        else:
            action_bool = not a_prev_bool
        rounds_available = int(history.round_number)
        W_use = min(self.W, rounds_available)
        if W_use > 0:
            recent_opponent = history.opponent_actions[-W_use:, :]
            denom = max(1, n - 1)
            fractions = np.sum(recent_opponent, axis=1) / float(denom)
            average_coop_recent = float(np.mean(fractions))
        else:
            average_coop_recent = 0.0
        remaining_after_current = r_total - t
        if not action_bool and average_coop_recent >= self.theta_f and (remaining_after_current >= 2):
            action_bool = True
        S_use = min(self.S, rounds_available)
        if S_use > 0:
            recent_opponent_S = history.opponent_actions[-S_use:, :]
            denom = max(1, n - 1)
            fractions_S = np.sum(recent_opponent_S, axis=1) / float(denom)
            near_universal_defection = bool(np.all(fractions_S <= self.small_eps))
            if near_universal_defection and remaining_after_current <= self.S:
                action_bool = False
        return Action.C if action_bool else Action.D

class Strategy_COOPERATIVE_42(BaseStrategy):
    """
    Conditional Group Punisher for Public Goods Game.

    - Cooperate on the first round to signal goodwill.
    - On subsequent rounds, cooperate if the previous round had at least q fraction cooperators (q=0.5).
    - If the previous round fell below q, schedule a short finite punishment (series of defections).
      Punishment length P scales with game parameters via P_base = ceil(n / (n - k)), capped to P_max = 4,
      and is truncated so it never runs into the final round.
    - While punishing, if the group returns to cooperation (>= q) in the most recent round, forgive early and
      resume cooperation.
    - Always defect in the final round.
    - Internal state:
        - self.punish_remaining: rounds left to defect as punishment (0 if not punishing).
        - self.played_last_round: last action taken (Action.C or Action.D); maintained for clarity.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.q = 0.5
        P_base = math.ceil(self.n / (self.n - self.k)) if self.n - self.k != 0 else 4
        P_base = int(P_base)
        P_max = 4
        self.P = min(P_max, max(1, P_base))
        self.punish_remaining = 0
        self.played_last_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.played_last_round = Action.C
            return Action.C
        t = int(state.round_number)
        if t == self.r:
            self.punish_remaining = 0
            self.played_last_round = Action.D
            return Action.D
        own_prev = bool(history.my_actions[-1]) if history.my_actions.size > 0 else False
        opp_prev_sum = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size > 0 else 0
        coop_count_prev = int(own_prev) + int(opp_prev_sum)
        coop_frac_prev = coop_count_prev / float(self.n)
        if self.punish_remaining > 0:
            if coop_frac_prev >= self.q:
                self.punish_remaining = 0
                self.played_last_round = Action.C
                return Action.C
            else:
                self.punish_remaining -= 1
                self.played_last_round = Action.D
                return Action.D
        if coop_frac_prev >= self.q:
            self.played_last_round = Action.C
            return Action.C
        else:
            remaining_rounds = self.r - t + 1
            sched = min(self.P, max(0, remaining_rounds - 1))
            self.punish_remaining = int(sched)
            if self.punish_remaining > 0:
                self.punish_remaining -= 1
                self.played_last_round = Action.D
                return Action.D
            else:
                self.played_last_round = Action.D
                return Action.D

class Strategy_COOPERATIVE_43(BaseStrategy):
    """
    Forgiving Threshold Reciprocity (FTR)

    - Start by cooperating to signal willingness.
    - Defect in the last round (no future incentives).
    - During play, monitor the fraction of other players who cooperated recently.
      * If that fraction >= tau_high -> cooperate.
      * If that fraction <= tau_low -> start a short, escalating but capped group-level punishment.
      * If in-between -> repeat previous action (inertia).
    - Punishments are short, capped, and probabilistically forgiving (epsilon).
    - State kept across rounds: last_action, punish_counter, consecutive_bad_rounds, rounds_played.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = max(1, getattr(game_description, 'n_rounds', 1))
        self.grace_period = min(3, max(0, r - 1))
        self.W = min(5, max(1, r - 1))
        self.tau_high = 0.6
        self.tau_low = 0.4
        self.max_punish = min(3, max(0, r - 1))
        self.escalation_step = 1
        self.epsilon = 0.05
        self.last_action = Action.C
        self.punish_counter = 0
        self.consecutive_bad_rounds = 0
        self.rounds_played = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            t = 1
        else:
            t = history.round_number + 1
        r = self.game_description.n_rounds
        n = self.game_description.n_players
        if history is None or history.round_number == 0:
            action = Action.C
            self.last_action = action
            self.rounds_played = 1
            return action
        if t == r:
            action = Action.D
            self.last_action = action
            self.rounds_played = t
            return action
        if t <= self.grace_period:
            action = Action.C
            self.last_action = action
            self.rounds_played = t
            return action
        if self.punish_counter > 0:
            if random.random() < self.epsilon:
                action = Action.C
                self.punish_counter = max(0, self.punish_counter - 1)
            else:
                action = Action.D
                self.punish_counter = max(0, self.punish_counter - 1)
            self.last_action = action
            self.rounds_played = t
            return action
        completed = history.round_number
        window = min(self.W, completed)
        if window <= 0:
            p = 1.0 if np.mean(history.opponent_actions) >= 0.5 else 0.0
        else:
            recent = history.opponent_actions[-window:, :]
            num_opponents = recent.shape[1] if recent.size else max(1, n - 1)
            cooperators_per_round = np.sum(recent, axis=1)
            fractions = cooperators_per_round / float(num_opponents)
            p = float(np.mean(fractions))
        if p >= self.tau_high:
            action = Action.C
            self.consecutive_bad_rounds = 0
        elif p <= self.tau_low:
            self.consecutive_bad_rounds += 1
            punish_len = min(self.max_punish, self.escalation_step * self.consecutive_bad_rounds)
            punish_len = max(0, punish_len - 1)
            self.punish_counter = punish_len
            action = Action.D
        else:
            action = self.last_action
        self.last_action = action
        self.rounds_played = t
        return action

class Strategy_COOPERATIVE_44(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Graduated Punishment and Forgiveness (ACG-PF).

    - Starts by cooperating.
    - Tracks recent group cooperation (sliding window).
    - Cooperates when others' cooperation >= adaptive threshold alpha.
    - If others fall below alpha, imposes a graduated punishment (several D rounds),
      then performs a one-round cooperative test. If test succeeds, de-escalate;
      otherwise escalate punishment length.
    - Small-probability cooperative probes during punishment (p_test).
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.phase = 'COOP'
        self.punish_timer = 0
        self.last_punish_length = 0
        self.offense_count = 0
        self.awaiting_test_evaluation = False
        self.last_test_round = None
        self.L0 = 1
        self.p_test = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        remaining = r - t + 1
        w = min(4, max(1, r - 1))
        raw_alpha = 0.6 + 0.4 * (k - 1.0) / max(1.0, n - 1.0)
        alpha = max(0.5, min(0.9, raw_alpha))

        def compute_others_coop_rate(next_round_t: int, hist: PlayerHistory) -> float:
            start = max(1, next_round_t - w)
            end = next_round_t - 1
            if end < start:
                return 1.0
            idx_start = start - 1
            idx_end = end - 1
            window = hist.opponent_actions[idx_start:idx_end + 1, :]
            if window.size == 0:
                return 1.0
            others_total = float(np.sum(window))
            denom = (end - start + 1) * (n - 1)
            if denom <= 0:
                return 1.0
            return others_total / float(denom)
        if self.awaiting_test_evaluation and history is not None and (self.last_test_round is not None):
            if history.round_number >= self.last_test_round:
                next_t_for_window = history.round_number + 1
                others_coop_rate = compute_others_coop_rate(next_t_for_window, history)
                if others_coop_rate >= alpha:
                    self.phase = 'COOP'
                    self.punish_timer = 0
                    if self.last_punish_length > 1:
                        self.last_punish_length = max(1, self.last_punish_length - 1)
                    self.offense_count = 0
                else:
                    self.phase = 'PUNISH'
                    self.offense_count += 1
                    base_increase = math.ceil((alpha - others_coop_rate) * r / 2.0)
                    new_len = int(self.last_punish_length + base_increase + 1)
                    new_len = max(self.L0, new_len)
                    new_len = min(new_len, max(1, remaining - 1)) if remaining - 1 >= 1 else 1
                    self.last_punish_length = new_len
                    self.punish_timer = max(0, new_len - 1)
                self.awaiting_test_evaluation = False
                self.last_test_round = None
        if t == r:
            return Action.D
        if history is None:
            self.phase = 'COOP'
            self.punish_timer = 0
            self.last_punish_length = 0
            self.offense_count = 0
            self.awaiting_test_evaluation = False
            self.last_test_round = None
            return Action.C
        if self.phase == 'PUNISH':
            if self.punish_timer == 0:
                self.awaiting_test_evaluation = True
                self.last_test_round = t
                return Action.C
            else:
                if random.random() < self.p_test:
                    chosen = Action.C
                else:
                    chosen = Action.D
                self.punish_timer = max(0, self.punish_timer - 1)
                return chosen
        if self.phase == 'COOP':
            others_coop_rate = compute_others_coop_rate(t, history)
            if others_coop_rate >= alpha:
                return Action.C
            else:
                self.phase = 'PUNISH'
                self.offense_count += 1
                shortfall = max(0.0, alpha - others_coop_rate)
                computed = int(self.L0 + math.ceil(shortfall * r / 2.0) + math.floor(self.offense_count / 1.5))
                computed = max(self.L0, computed)
                if remaining <= 2:
                    computed = min(computed, 1)
                else:
                    computed = min(computed, max(1, remaining - 1))
                self.last_punish_length = computed
                self.punish_timer = max(0, computed - 1)
                return Action.D
        return Action.C

class Strategy_COOPERATIVE_45(BaseStrategy):
    """
    Adaptive Threshold Reciprocity (ATR)

    - Starts by cooperating.
    - In intermediate rounds, cooperates if recent group cooperation >= alpha.
      Otherwise starts a short, proportional punishment (defect for a few rounds).
    - Punishments are limited to MAX_PUNISH rounds and are followed by CONTRITE_ROUNDS
      of cooperative play to signal rehabilitation.
    - Small random forgiveness probability p allows one-shot cooperative flips during punishments.
    - Defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, max(1, self.r - 1))
        self.alpha = 0.6
        self.MAX_PUNISH = 3
        self.FORGIVE_PROB = 0.1
        self.CONTRITE_ROUNDS = 2
        self.punish_counter = 0
        self.contrite_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            return Action.C
        if t == self.r:
            return Action.D
        action = Action.C
        if self.punish_counter > 0:
            action = Action.D
            self.punish_counter -= 1
            if self.punish_counter == 0:
                self.contrite_counter = self.CONTRITE_ROUNDS
        elif self.contrite_counter > 0:
            action = Action.C
            self.contrite_counter -= 1
        else:
            s0 = max(1, t - self.W)
            rounds_considered = t - s0
            if rounds_considered <= 0:
                g = 1.0
            else:
                start_idx = s0 - 1
                end_idx_exclusive = t - 1
                my_slice = history.my_actions[start_idx:end_idx_exclusive]
                opp_slice = history.opponent_actions[start_idx:end_idx_exclusive, :]
                my_coops = float(np.sum(my_slice))
                opp_coops = float(np.sum(opp_slice))
                total_cooperators = my_coops + opp_coops
                g = total_cooperators / (self.n * rounds_considered)
            if g >= self.alpha:
                action = Action.C
            else:
                shortfall = max(0.0, self.alpha - g)
                raw = 1 + math.ceil(shortfall / self.alpha * self.MAX_PUNISH)
                punish_length = int(min(self.MAX_PUNISH, max(1, raw)))
                self.punish_counter = punish_length - 1
                action = Action.D
        if action == Action.D:
            if random.random() < self.FORGIVE_PROB:
                action = Action.C
        return action

class Strategy_COOPERATIVE_46(BaseStrategy):
    """
    Conditional Cooperator with Proportional Punishment and Prompt Forgiveness.

    - Starts by cooperating.
    - If a simple majority (tau = 0.5) cooperated in the previous round, cooperate.
    - If the previous round fell below the majority threshold, punish by defecting for a
      short number of rounds proportional to the shortfall (punishment length = min(P_max, shortfall)),
      where P_max = max(1, ceil(r/10)). During punishment, stop punishing immediately if the group
      recovers to the threshold in any observed round.
    - In the final round, cooperate only if the previous round met the cooperation threshold.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.tau = 0.5
        self.P_max = max(1, math.ceil(self.r / 10.0))
        self.punishment_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punishment_timer = 0
            return Action.C
        t = history.round_number + 1
        r = self.r
        last_my = int(bool(history.my_actions[-1]))
        last_opponents = int(np.sum(history.opponent_actions[-1, :]))
        m = last_my + last_opponents
        threshold_count = int(math.ceil(self.tau * self.n))
        if m >= threshold_count:
            self.punishment_timer = 0
        if t == r:
            if m >= threshold_count:
                self.punishment_timer = 0
                return Action.C
            else:
                return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            return Action.D
        if m >= threshold_count:
            return Action.C
        else:
            shortfall = threshold_count - m
            punishment_length = min(self.P_max, max(1, int(shortfall)))
            self.punishment_timer = punishment_length - 1
            return Action.D

class Strategy_COOPERATIVE_47(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC)

    - Starts by cooperating.
    - In non-final rounds, cooperates iff the average number of cooperators
      over the last W completed rounds is >= m_threshold.
    - m_threshold = max(1, ceil(n / k)).
    - Uses a small smoothing window W (default up to 3) to be forgiving of noise.
    - Always defects on the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = getattr(game_description, 'n_players', 2)
        k = getattr(game_description, 'k', 1.0)
        self.m_threshold = max(1, math.ceil(n / float(k))) if k > 0 else max(1, n)
        self.W = min(3, max(1, game_description.n_rounds - 1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or getattr(history, 'round_number', 0) == 0:
            return Action.C
        r = self.game_description.n_rounds
        completed = history.round_number
        if completed >= r - 1:
            return Action.D
        rounds_to_consider = min(self.W, completed)
        start_idx = completed - rounds_to_consider
        opp_slice = history.opponent_actions[start_idx:completed, :]
        my_slice = history.my_actions[start_idx:completed]
        try:
            opp_counts = np.sum(opp_slice, axis=1)
            my_counts = np.array(my_slice, dtype=int)
            total_counts = opp_counts + my_counts
            m = float(np.mean(total_counts))
        except Exception:
            total_counts_list = []
            for t in range(start_idx, completed):
                opp_row = history.opponent_actions[t]
                opp_count = sum((bool(x) for x in opp_row))
                my_count = 1 if bool(history.my_actions[t]) else 0
                total_counts_list.append(opp_count + my_count)
            if not total_counts_list:
                return Action.D
            m = sum(total_counts_list) / float(len(total_counts_list))
        if m >= self.m_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_48(BaseStrategy):
    """
    Lenient Proportional-Tit-for-Tat with Forgiveness (LPTFT-F)

    - Starts cooperative.
    - Measures recent group cooperation over a short window W.
    - Tolerates small or isolated defections.
    - When significant defection occurs in the most recent round, issues a
      proportional, short punishment (mutual defection) for a bounded number
      of rounds. During punishment there is a small randomized forgiveness
      probability to test for recovery.
    - Uses a short endgame rule to avoid exploitation near the final rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_until_round = 0
        self.last_punish_start = 0
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        self.n = n
        self.r = r
        self.W = min(5, max(1, math.floor(r / 5)))
        self.tolerated_defect_fraction = min(0.25, 1.0 / (2.0 * math.sqrt(max(1, n))))
        self.punishment_base = max(1, math.ceil(r / 6))
        self.punishment_scale = 0.5
        self.forgiveness_prob = 0.08
        self.endgame_len = min(3, max(0, r - 1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = history.round_number + 1
        n = self.n
        r = self.r
        if t <= self.punish_until_round:
            if random.random() < self.forgiveness_prob:
                return Action.C
            else:
                return Action.D
        window_size = min(self.W, max(0, t - 1))
        if window_size >= 1:
            recent_opponents = history.opponent_actions[-window_size:, :]
            recent_coop_per_opponent = np.mean(recent_opponents.astype(float), axis=0)
            recent_group_coop_fraction = float(np.mean(recent_coop_per_opponent))
        else:
            recent_group_coop_fraction = 1.0
        cooperators_last_round = 0
        if history.round_number >= 1:
            cooperators_last_round += 1 if bool(history.my_actions[-1]) else 0
            cooperators_last_round += int(np.sum(history.opponent_actions[-1, :]))
        else:
            cooperators_last_round = n
        recent_round_defectors = n - cooperators_last_round
        recent_round_defect_fraction = recent_round_defectors / float(n)
        remaining_rounds = r - t + 1
        if remaining_rounds <= self.endgame_len:
            if recent_group_coop_fraction >= 0.9 and recent_round_defect_fraction <= self.tolerated_defect_fraction:
                return Action.C
            else:
                return Action.D
        if recent_group_coop_fraction >= 1.0 - self.tolerated_defect_fraction:
            return Action.C
        if recent_round_defect_fraction <= self.tolerated_defect_fraction:
            return Action.C
        severity = recent_round_defect_fraction
        proposed_punish = max(1, math.ceil(self.punishment_base * self.punishment_scale * severity))
        P = min(max(1, remaining_rounds), proposed_punish)
        self.punish_until_round = t + P - 1
        self.last_punish_start = t
        return Action.D

class Strategy_COOPERATIVE_49(BaseStrategy):
    """
    Reputation-based, forgiving public-goods strategy.

    - Start by cooperating in round 1.
    - Defect in the final round.
    - In intermediate rounds, compute per-opponent cooperation rates over a recent window L (<=5).
      Label opponents "reliable" if their recent coop-rate >= theta.
      If enough players (including self) are reliable (>= ceil(n * phi)) then cooperate.
      Otherwise, if a large fraction defected last round (>= gamma) enter a short proportional
      punishment (defect for a few rounds, targeting last-round defectors).
      If borderline, cooperate probabilistically with p = P_min + (expected_reliable_count / n) * (1 - P_min).
    - Punishments are temporary and stored in internal state (punishment_remaining, punish_targets).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.L_default = 5
        self.theta = 0.6
        self.phi = 0.6
        self.gamma = 0.4
        self.P_min = 0.2
        self.P_base = 2
        self.punishment_remaining = 0
        self.punish_targets = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        if history is None:
            self.punishment_remaining = 0
            self.punish_targets = set()
            return Action.C
        completed = int(history.round_number)
        t = completed + 1
        if t >= r:
            self.punishment_remaining = 0
            self.punish_targets = set()
            return Action.D
        if self.punishment_remaining > 0:
            self.punishment_remaining = max(0, self.punishment_remaining - 1)
            if self.punishment_remaining == 0:
                self.punish_targets = set()
            return Action.D
        L = min(self.L_default, max(0, r - 1))
        L_eff = min(L, completed)
        opp_actions = history.opponent_actions
        n_opps = n - 1
        coop_rates = [0.0] * n_opps
        if L_eff > 0 and opp_actions.size != 0:
            recent = opp_actions[-L_eff:, :]
            means = np.mean(recent.astype(float), axis=0)
            coop_rates = [float(x) for x in means]
        else:
            coop_rates = [0.0] * n_opps
        reliable_flags = []
        for j in range(n_opps):
            base_reliable = coop_rates[j] >= self.theta
            if j in self.punish_targets:
                reliable = False
            else:
                reliable = base_reliable
            reliable_flags.append(reliable)
        expected_reliable_count = 1 + sum((1 for v in reliable_flags if v))
        if completed >= 1:
            last_opp_round = opp_actions[-1, :]
            last_opp_cooperators = int(np.sum(last_opp_round.astype(int)))
            my_last_action = int(bool(history.my_actions[-1]))
            last_round_cooperators = last_opp_cooperators + my_last_action
        else:
            last_round_cooperators = 0
        last_round_defectors = n - last_round_cooperators
        last_defect_fraction = float(last_round_defectors) / float(n)
        threshold = math.ceil(n * self.phi)
        if expected_reliable_count >= threshold:
            return Action.C
        if last_defect_fraction >= self.gamma:
            severity = last_defect_fraction - self.gamma
            extra = math.ceil(n * severity)
            remaining_rounds = max(0, r - t)
            P = min(self.P_base + extra, remaining_rounds)
            if P > 0:
                punish_set = set()
                if completed >= 1:
                    last_round_opp_actions = opp_actions[-1, :]
                    for j in range(n_opps):
                        acted_coop = bool(last_round_opp_actions[j])
                        if not acted_coop:
                            punish_set.add(j)
                self.punishment_remaining = P - 1
                self.punish_targets = punish_set
                return Action.D
            else:
                pass
        p = self.P_min + float(expected_reliable_count) / float(n) * (1.0 - self.P_min)
        exploiters = [j for j, cr in enumerate(coop_rates) if cr < 0.2]
        if random.random() < p:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_50(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC).

    - Starts by cooperating to signal willingness.
    - Uses a short recent-history window (w) to estimate the fraction of other players
      who have cooperated. If that recent_coop_rate >= theta it cooperates.
    - If recent_coop_rate < theta it defects, but with a small forgiveness probability
      it may cooperate when recent_coop_rate is slightly below theta.
    - Defects in the final m_end rounds (endgame-aware).
    - Parameters derived from game description: w, m_end, f, theta, forgiveness_margin.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, max(1, self.r - 1))
        self.m_end = 1
        self.f = 0.1
        self.forgiveness_margin = 0.1
        self.theta = max(0.5, 1.0 - self.k / max(1.0, self.n))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        completed_rounds = history.round_number
        if completed_rounds == 0:
            return Action.C
        t = getattr(state, 'round_number', None)
        if not isinstance(t, int) or t <= 0:
            t = completed_rounds + 1
        if t > self.r - self.m_end:
            return Action.D
        lookback = min(self.w, t - 1)
        start_idx = t - lookback - 1
        end_slice = t - 1
        opp_actions = history.opponent_actions
        if opp_actions.size == 0 or opp_actions.shape[1] == 0:
            return Action.D
        recent_slice = opp_actions[start_idx:end_slice, :]
        total_other_C = int(np.sum(recent_slice))
        denom = (self.n - 1) * lookback
        recent_coop_rate = float(total_other_C) / float(denom) if denom > 0 else 0.0
        if recent_coop_rate >= self.theta:
            return Action.C
        if recent_coop_rate >= self.theta - self.forgiveness_margin:
            if random.random() < self.f:
                return Action.C
        return Action.D

class Strategy_COOPERATIVE_51(BaseStrategy):
    """
    Adaptive Forgiving Conditional Cooperator (AFCC)

    - Starts by cooperating.
    - Uses recent history (up to m rounds) to estimate the fraction of opponents who cooperated.
    - Cooperates if recent average >= tau, defects otherwise.
    - If exploited (you cooperated but others cooperated less than tau), starts a punishment of length P.
      Punishment length P is scaled to the one-shot temptation to defect.
    - Forgives immediately if in the last completed round every other player cooperated.
    - Always defects in the final round.
    - Near the endgame (remaining rounds <= E), only start punishments for severe exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.m = min(5, max(0, self.r - 1))
        g = 1.0 - self.k / float(self.n)
        if g <= 1e-12:
            raw_P = self.r
        else:
            raw_P = math.ceil(1.0 / g)
        self.P = int(min(self.r, max(1, raw_P)))
        self.tau = 0.5
        self.E = self.P
        self.punish_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        completed = history.round_number
        t = completed + 1
        if t == self.r:
            return Action.D

        def frac_others_coop_at(round_index: int) -> float:
            if self.n <= 1:
                return 0.0
            coop_count = float(np.sum(history.opponent_actions[round_index, :]))
            return coop_count / float(self.n - 1)
        last_f = None
        last_my_action = False
        if completed >= 1:
            last_f = frac_others_coop_at(completed - 1)
            last_my_action = bool(history.my_actions[-1])
            if last_my_action and last_f < self.tau:
                rem = self.r - completed
                if rem > self.E:
                    self.punish_counter = self.P
                elif last_f <= max(0.0, self.tau - 0.25):
                    self.punish_counter = self.P
                else:
                    self.punish_counter = self.punish_counter
        if self.punish_counter > 0:
            if last_f is not None and last_f == 1.0:
                self.punish_counter = 0
            else:
                self.punish_counter = max(0, self.punish_counter - 1)
                return Action.D
        use_count = min(self.m, completed)
        if use_count == 0:
            avg_coop = 1.0
        else:
            recent_opponent_matrix = history.opponent_actions[-use_count:, :]
            per_round_fracs = np.mean(recent_opponent_matrix.astype(float), axis=1)
            avg_coop = float(np.mean(per_round_fracs))
        if avg_coop >= self.tau:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_52(BaseStrategy):
    """
    Forgiving Proportional Trigger (FPT)

    - Start by cooperating.
    - If any players defect in a round, set a punishment timer proportional to
      the number of defectors (gamma * defectors, ceil), capped by P_max and by
      remaining rounds. While the timer > 0 defect (punish). After the timer
      expires immediately try to return to cooperation (one-shot forgiveness).
    - In the final round default to defect unless there has been recent unanimous
      cooperation for a short window (min(2, r-1) rounds).
    - Stateless reconstruction: the decision is computed from the full observed
      history (no reliance on externally preserved mutable state).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.gamma = 1.0
        r = int(self.game_description.n_rounds)
        self.P_max = max(1, min(5, math.floor(r / 3)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        T = history.round_number
        if T <= 0:
            return Action.C
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        gamma = self.gamma
        P_max = self.P_max
        p_timer = 0
        consec_unanimous = 0
        for idx in range(T):
            my_coop = bool(history.my_actions[idx])
            opp_row = history.opponent_actions[idx, :]
            opp_coops = int(np.sum(opp_row))
            M_t = int(my_coop) + opp_coops
            t = idx + 1
            remaining_rounds = max(0, r - t)
            if M_t == n:
                consec_unanimous += 1
            else:
                consec_unanimous = 0
                defec_count = n - M_t
                new_p = min(remaining_rounds, min(P_max, math.ceil(gamma * float(defec_count))))
                if new_p > p_timer:
                    p_timer = new_p
            if p_timer > 0:
                p_timer = max(0, p_timer - 1)
        next_round = T + 1
        remaining_rounds_next = r - T
        if remaining_rounds_next <= 0:
            return Action.D
        if p_timer > 0:
            return Action.D
        if next_round == r:
            required_unanimous = min(2, max(0, r - 1))
            if consec_unanimous >= required_unanimous and required_unanimous > 0:
                return Action.C
            return Action.D
        return Action.C

class Strategy_COOPERATIVE_53(BaseStrategy):
    """
    Aspiration-Based Reciprocal (ABR) with Gradual Punishment and Endgame Softening.

    - Starts cooperating to seed cooperation.
    - Uses an aspiration-based Pavlov (win-stay/lose-shift) rule with a running aspiration A.
    - Overrides with a short-term group check to join cooperative majorities or avoid exploitative majorities.
    - Applies an endgame bias toward defection except when long-run cooperation is extremely high.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.m = min(5, max(0, self.r - 1))
        self.alpha = 0.2
        self.endgame_window = min(2, max(0, self.r - 1))
        self.coop_high = 0.75
        self.coop_low = 0.25
        self.hist_coop_threshold = 0.95
        self.A = (1.0 + self.k) / 2.0
        self.prev_action = Action.C
        self.pi_last = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.prev_action = Action.C
            return Action.C
        completed = int(history.round_number)
        t = completed + 1
        if completed >= 1:
            last_my_action_bool = bool(history.my_actions[-1])
            self.prev_action = Action.C if last_my_action_bool else Action.D
            self.pi_last = float(history.my_payoffs[-1])
        if t > self.r - self.endgame_window:
            if completed == 0:
                hist_coop = 0.0
            else:
                total_other_coop = float(np.sum(history.opponent_actions))
                denom = float(completed * (self.n - 1))
                hist_coop = total_other_coop / denom if denom > 0.0 else 0.0
            if hist_coop >= self.hist_coop_threshold and self.prev_action == Action.C:
                action = Action.C
            else:
                action = Action.D
            self.prev_action = action
            return action
        if self.pi_last is not None:
            self.A = (1.0 - self.alpha) * self.A + self.alpha * self.pi_last
        m_eff = min(self.m, completed) if completed > 0 else 0
        if m_eff <= 0:
            group_coop_recent = 0.0
        else:
            recent_slice = history.opponent_actions[-m_eff:, :]
            coop_count = float(np.sum(recent_slice))
            denom = float(m_eff * (self.n - 1))
            group_coop_recent = coop_count / denom if denom > 0.0 else 0.0
        if self.pi_last is None:
            candidate = self.prev_action
        elif self.pi_last >= self.A:
            candidate = self.prev_action
        else:
            candidate = Action.D if self.prev_action == Action.C else Action.C
        if group_coop_recent >= self.coop_high and candidate == Action.D:
            action = Action.C
        elif group_coop_recent <= self.coop_low and candidate == Action.C:
            action = Action.D
        else:
            action = candidate
        self.prev_action = action
        return action

class Strategy_COOPERATIVE_54(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC)

    - Cooperate in the first round to signal willingness to cooperate.
    - In intermediate rounds, compute a short-window moving average fraction of cooperators
      (including self). Cooperate if that fraction meets a threshold alpha (with a small
      endgame bias), otherwise defect.
    - Always defect in the last round (rem = 1).
    - Forgiving: uses a windowed average (no persistent punitive state). Tie (f == alpha') -> cooperate.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        self.W = min(5, max(0, r - 1))
        if k <= 0:
            self.alpha = 0.5
        else:
            try:
                self.alpha = max(0.5, 1.0 - 1.0 / k)
            except Exception:
                self.alpha = 0.5
        self.H = min(3, max(0, r - 1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        t = getattr(state, 'round_number', None)
        if t is None:
            if history is not None:
                t = history.round_number + 1
            else:
                t = 1
        try:
            t = int(t)
            if t < 1:
                t = 1
        except Exception:
            t = 1
        if history is None or getattr(history, 'round_number', 0) == 0 or t == 1:
            return Action.C
        rem = r - t + 1
        if rem == 1:
            return Action.D
        W_t = min(self.W, max(0, t - 1))
        if W_t <= 0:
            return Action.C
        start_idx = t - 1 - W_t
        end_idx_exclusive = t - 1
        sumC = 0
        for idx in range(start_idx, end_idx_exclusive):
            if idx < 0 or idx >= history.round_number:
                continue
            my_coop = 1 if bool(history.my_actions[idx]) else 0
            opp_coop = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size else 0
            sumC += my_coop + opp_coop
        denom = float(W_t * n) if W_t * n > 0 else 1.0
        f = float(sumC) / denom
        if rem <= self.H and self.H > 0:
            alpha_p = min(1.0, self.alpha + 0.1 * (self.H - rem + 1))
        else:
            alpha_p = self.alpha
        if f >= float(alpha_p):
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_55(BaseStrategy):
    """
    Threshold Forgiving Conditional Cooperator (TFCC)

    - Starts by cooperating.
    - Uses a sliding window of recent rounds to estimate the fraction of
      opponents cooperating.
    - Cooperates while that recent cooperation rate >= threshold theta.
    - If the recent cooperation rate falls below theta, enters a measured
      punishment period of up to P rounds (defecting), but during each
      punishment round it may "forgive" (cooperate) with probability q to
      probe whether cooperation has returned.
    - If the recent cooperation rate recovers (>= theta) the strategy exits
      punishment and returns to cooperation.
    - If near-unanimous cooperation is observed for window_m consecutive
      rounds (f >= 0.95), switch to a stable cooperation mode (deterministic
      cooperation) until repeated breakdown is observed.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_remaining = 0
        self.cooperate_mode = True
        self.stable_mode = False
        self.stable_count = 0
        r = int(getattr(game_description, 'n_rounds', 1))
        self.window_m_default = max(1, min(3, max(1, r - 1)))
        self.theta_default = 0.6
        self.P_default = self.window_m_default
        self.q_default = 0.15
        self.endgame_rounds = 1
        self.stable_coop_threshold = 0.95

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = getattr(state, 'round_number', 1)
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        window_m = int(self.window_m_default)
        theta = float(self.theta_default)
        P = int(self.P_default)
        q = float(self.q_default)
        stable_threshold = float(self.stable_coop_threshold)
        if history is None or (hasattr(history, 'round_number') and history.round_number == 0):
            return Action.C
        if t >= r:
            return Action.D
        start_round = max(1, t - window_m)
        end_round = t - 1
        num_window_rounds = end_round - start_round + 1
        if num_window_rounds <= 0:
            f = 0.0
        else:
            frac_list = []
            for s in range(start_round, end_round + 1):
                idx = s - 1
                opp_coops = float(np.sum(history.opponent_actions[idx, :]))
                denom = max(1, n - 1)
                frac_list.append(opp_coops / denom)
            f = float(np.mean(frac_list))
        if f >= stable_threshold:
            self.stable_count += 1
        else:
            self.stable_count = 0
        if self.stable_count >= window_m:
            self.stable_mode = True
        if self.stable_mode and f < theta:
            self.stable_mode = False
        if self.stable_mode and self.punishment_remaining == 0:
            return Action.C
        if self.punishment_remaining > 0:
            if f >= theta:
                self.punishment_remaining = 0
                self.cooperate_mode = True
                return Action.C
            else:
                prob = random.random()
                if prob < q:
                    action = Action.C
                else:
                    action = Action.D
                self.punishment_remaining = max(0, self.punishment_remaining - 1)
                return action
        if f >= theta:
            self.cooperate_mode = True
            return Action.C
        else:
            self.cooperate_mode = False
            self.punishment_remaining = max(0, P - 1)
            return Action.D

class Strategy_COOPERATIVE_56(BaseStrategy):
    """
    Adaptive Win-Stay Lose-Shift for Public Goods (AWSLS-PG).

    - Start by cooperating.
    - For intermediate rounds: compare the payoff you actually received last round
      with the counterfactual payoff you would have received had you played the
      opposite action (given others' observed actions). If your actual payoff
      was strictly greater, repeat your previous action. If strictly less, switch.
      If equal, bias toward cooperation.
    - In the final round: play the myopic best response given the most recent
      observed cooperators among opponents (ties -> cooperate).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        if history.round_number == 0:
            return Action.C
        try:
            my_last_action_bool = bool(history.my_actions[-1])
        except Exception:
            return Action.C
        try:
            opps_last_row = history.opponent_actions[-1, :]
        except Exception:
            return Action.C
        try:
            m_others = int(np.sum(opps_last_row))
        except Exception:
            return Action.C
        k_over_n = self.k / float(self.n)
        pi_C = k_over_n * (m_others + 1)
        pi_D = 1.0 + k_over_n * m_others
        is_final_round = history.round_number == self.r - 1
        if is_final_round:
            if pi_C >= pi_D:
                return Action.C
            else:
                return Action.D
        if my_last_action_bool:
            last_payoff = pi_C
            alt_payoff = pi_D
        else:
            last_payoff = pi_D
            alt_payoff = pi_C
        eps = 1e-12
        if abs(last_payoff - alt_payoff) <= eps:
            return Action.C
        if last_payoff > alt_payoff:
            return Action.C if my_last_action_bool else Action.D
        else:
            return Action.D if my_last_action_bool else Action.C

class Strategy_COOPERATIVE_57(BaseStrategy):
    """
    Adaptive Threshold with Forgiving Reciprocity (ATFR)

    - Start cooperative.
    - Continue cooperating when at least q fraction of other players cooperated last round.
    - If group cooperation falls below threshold, start a proportional temporary punishment.
    - Punishments are capped and do not extend into the final round.
    - After a punishment expires, forgive with probability pf on the first post-punishment round.
    - While punishing, probe with small probability epsilon by cooperating (these probes do not count as
      cooperation for punishment/accounting).
    - Exercise endgame caution in the final one or two rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.q = 0.5
        self.alpha = 0.2
        self.max_punish = max(1, math.ceil(self.alpha * self.r)) if self.r > 1 else 1
        self.pf = 0.8
        self.epsilon = min(0.05, 1.0 / self.r) if self.r > 0 else 0.05
        self.punish_until = 0
        self.punish_rounds = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.C
        t = int(state.round_number)
        rounds_remaining = self.r - t + 1
        prev_punish_until = int(self.punish_until)
        is_currently_punishing = t <= prev_punish_until
        last_opponent_coops = int(np.sum(history.opponent_actions[-1, :]))
        last_my_coop = int(bool(history.my_actions[-1]))
        m_last = last_opponent_coops + last_my_coop
        last_round_index = t - 1
        last_round_was_punishing = last_round_index in self.punish_rounds
        others_coop_numerator = m_last - (last_my_coop if not last_round_was_punishing else 0)
        others_coop_frac = others_coop_numerator / float(self.n - 1)
        action = Action.D
        if is_currently_punishing:
            if random.random() < self.epsilon:
                action = Action.C
            else:
                action = Action.D
            self.punish_rounds.add(t)
        elif rounds_remaining == 1:
            if m_last >= self.n - 1 and prev_punish_until < t:
                action = Action.C
            else:
                action = Action.D
        elif rounds_remaining == 2:
            required = math.ceil(self.q * (self.n - 1)) + 1
            if m_last >= required:
                action = Action.C
            else:
                shortfall = max(0, self.n - m_last)
                P = max(1, math.ceil(shortfall * self.max_punish / self.n))
                P = min(self.max_punish, P)
                self.punish_until = min(self.r - 1, t + P - 1)
                action = Action.D
                if t <= self.punish_until:
                    self.punish_rounds.add(t)
        elif others_coop_frac >= self.q:
            action = Action.C
        else:
            shortfall = max(0, self.n - m_last)
            P = max(1, math.ceil(shortfall * self.max_punish / self.n))
            P = min(self.max_punish, P)
            self.punish_until = min(self.r - 1, t + P - 1)
            action = Action.D
            if t <= self.punish_until:
                self.punish_rounds.add(t)
        if prev_punish_until > 0 and t == prev_punish_until + 1:
            if random.random() < self.pf:
                action = Action.C
            else:
                action = Action.D
        if self.punish_until > max(0, self.r - 1):
            self.punish_until = max(0, self.r - 1)
        return action

class Strategy_COOPERATIVE_58(BaseStrategy):
    """
    Forgiving Conditional Cooperator (FCC)

    - Starts by cooperating.
    - Uses a short memory window L to track recent fraction of cooperators among other players.
    - If recent cooperation is above a threshold (frac_th) or the last round had at most g defectors,
      it cooperates. Otherwise it applies a short punishment of length P (defections), but forgives
      immediately if the entire group returns to full cooperation. Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        self.L = min(3, max(0, r - 1))
        self.g = max(0, int(round(0.1 * (n - 1))))

        def _clamp(x, a, b):
            return a if x < a else b if x > b else x
        raw_frac = 0.5 - 0.25 * ((k - 1.0) / max(1.0, n - 1))
        self.frac_th = _clamp(raw_frac, 0.2, 0.8)
        self.P = int(min(r - 1, 1 + math.floor((n - k) / 2.0), 3))
        self.T_forgive = 1
        self.punishment_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        if t == 1 or history is None:
            return Action.C
        if t == r:
            self.punishment_remaining = 0
            return Action.D
        available_rounds = min(self.L, history.round_number)
        if available_rounds <= 0:
            frac_coop_recent = 0.0
        else:
            recent = history.opponent_actions[-available_rounds:, :]
            opp_counts = np.sum(recent.astype(np.int64), axis=1)
            frac_per_round = opp_counts / float(max(1, n - 1))
            frac_coop_recent = float(np.mean(frac_per_round))
        last_round_cooperators = int(np.sum(history.opponent_actions[-1, :]).item())
        if self.punishment_remaining > 0:
            if last_round_cooperators == n - 1:
                self.punishment_remaining = 0
                return Action.C
            else:
                self.punishment_remaining = max(0, self.punishment_remaining - 1)
                return Action.D
        elif frac_coop_recent >= self.frac_th or last_round_cooperators >= n - 1 - self.g:
            return Action.C
        else:
            self.punishment_remaining = max(0, self.P - 1)
            return Action.D

class Strategy_COOPERATIVE_59(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC)

    - Starts by cooperating to signal intent.
    - Uses a sliding window of recent rounds to compute phi: the average fraction
      of other players who cooperated.
    - Cooperates when phi >= theta (threshold 0.60).
    - If exploited (you cooperated but phi < phi_min), trigger a short, bounded punishment.
    - While punishing, defect but forgive immediately if phi >= theta.
    - Uses small-probability probes (epsilon) to test for returning cooperation when defecting.
    - Always defects in the final round. Be stricter in the penultimate round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.theta = 0.6
        self.phi_min = 0.45
        self.epsilon = 0.02
        self.W = max(3, min(math.ceil(math.sqrt(self.r)), max(0, self.r - 1)))
        self.P_base = max(1, math.ceil(self.r / 20.0))
        self.P_cap = 4
        self.G = 1
        self.punish_counter = 0
        self.last_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.last_action = True
            return Action.C
        completed = int(history.round_number)
        t = completed + 1
        r = self.r
        if t >= r:
            self.last_action = False
            return Action.D

        def compute_phi():
            m = min(self.W, max(1, completed))
            opp = history.opponent_actions
            if opp.size == 0:
                return 0.0
            recent = opp[-m:, :] if m <= opp.shape[0] else opp
            per_round_frac = np.mean(recent.astype(float), axis=1)
            phi_val = float(np.mean(per_round_frac))
            return phi_val
        if t == r - 1:
            phi = compute_phi()
            theta_high = max(0.75, self.theta)
            if phi >= theta_high:
                self.last_action = True
                return Action.C
            else:
                self.last_action = False
                return Action.D
        phi = compute_phi()
        if self.punish_counter > 0:
            if phi >= self.theta:
                self.punish_counter = 0
                self.last_action = True
                return Action.C
            else:
                action = Action.D
                self.punish_counter -= 1
                self.last_action = False
                return action
        if phi >= self.theta:
            self.last_action = True
            return Action.C
        elif self.last_action is True and phi < self.phi_min:
            shortfall = max(0.0, self.theta - phi)
            P = self.P_base + math.ceil(shortfall * 10.0)
            P = min(P, self.P_cap)
            if P <= 1:
                self.punish_counter = 0
            else:
                self.punish_counter = P - 1
            self.last_action = False
            return Action.D
        elif random.random() < self.epsilon:
            self.last_action = True
            return Action.C
        else:
            self.last_action = False
            return Action.D

class Strategy_COOPERATIVE_60(BaseStrategy):
    """
    Adaptive Win-Stay Lose-Shift with Soft Forgiveness (AWSLS-SF).

    - Starts by cooperating.
    - Uses Win-Stay Lose-Shift comparing actual payoff last round to the
      counterfactual had it switched.
    - If switched because of being exploited (you cooperated and switching
      to defect would have been strictly better), schedule a short forgiveness
      probe phase for the next F rounds where cooperation is resumed only if
      a majority of others have cooperated recently (M-window average).
    - Final round uses a one-shot payoff-maximizing choice based on recent
      behavior; ties broken in favor of cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.probe_start = math.inf
        self.probe_end = -math.inf
        r = getattr(game_description, 'n_rounds', 2)
        self.M = min(5, max(1, r - 1))
        self.F = 2
        self.theta = 0.5
        self.eps = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory):
        if history is None:
            return Action.C
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        rounds_completed = int(history.round_number)
        t = rounds_completed + 1

        def payoff(c_i, others):
            return 1.0 - c_i + k / n * (others + c_i)
        if rounds_completed > 0:
            others_per_round = np.sum(history.opponent_actions, axis=1)
            last_c_bool = bool(history.my_actions[-1])
            last_c = 1 if last_c_bool else 0
            others_last = float(others_per_round[-1])
            payoff_last = payoff(last_c, others_last)
            if last_c == 1:
                counterfactual = payoff(0, others_last)
            else:
                counterfactual = payoff(1, others_last)
        else:
            return Action.C
        if t == r:
            M_eff = min(self.M, rounds_completed)
            if M_eff <= 0:
                expected_Others = others_last
            else:
                expected_Others = float(np.mean(others_per_round[-M_eff:]))
            payoff_if_C = payoff(1, expected_Others)
            payoff_if_D = payoff(0, expected_Others)
            if payoff_if_C >= payoff_if_D:
                chosen = Action.C
            else:
                chosen = Action.D
            return chosen
        if payoff_last >= counterfactual - self.eps:
            proposed_c = last_c
        else:
            proposed_c = 1 - last_c
            if last_c == 1 and payoff_last < counterfactual - self.eps:
                self.probe_start = t + 1
                self.probe_end = t + self.F
        if self.probe_start <= t and t <= self.probe_end:
            M_eff = min(self.M, rounds_completed)
            if M_eff <= 0:
                avg_frac_others = others_last / max(1, n - 1)
            else:
                recent_others = others_per_round[-M_eff:]
                avg_frac_others = float(np.mean(recent_others / float(max(1, n - 1))))
            if avg_frac_others >= self.theta:
                chosen_c = 1
            else:
                chosen_c = 0
            chosen = Action.C if chosen_c == 1 else Action.D
            return chosen
        chosen = Action.C if proposed_c == 1 else Action.D
        return chosen

class Strategy_COOPERATIVE_61(BaseStrategy):
    """
    Adaptive Proportional Reciprocity (APR)

    - Starts cooperating.
    - Measures recent cooperation among opponents over a window L.
    - If opponents' cooperation f >= alpha, cooperates.
    - If f < alpha, enters a proportional punishment of length P (defects immediately and for P-1 further rounds).
    - After a punishment finishes, enters a probation check: cooperate for at least one round if recent cooperation recovers to alpha_forgive, otherwise re-enter punishment.
    - Final round is cautious: defects by default unless there is strong evidence of stable cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_timer = 0
        self.in_punishment = False
        self.awaiting_probation = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        L = min(5, max(1, math.floor(r / 5)))
        alpha_base = 0.55 + 0.3 * (k / n - 0.2)
        alpha = max(0.5, min(0.9, alpha_base))
        alpha_forgive = alpha - 0.15
        Pmax = min(4, r)

        def rounds_available(h: PlayerHistory) -> int:
            return h.round_number

        def window_slice_count(h: PlayerHistory) -> int:
            return min(L, rounds_available(h))

        def compute_f_other_coop(h: PlayerHistory, rounds_used: int) -> float:
            if rounds_used <= 0:
                return 0.0
            opp_slice = h.opponent_actions[-rounds_used:, :]
            total_coops = float(np.sum(opp_slice))
            denom = (n - 1) * rounds_used
            if denom <= 0:
                return 0.0
            return total_coops / float(denom)

        def strong_cooperation_in_window(h: PlayerHistory, rounds_used: int) -> bool:
            if rounds_used <= 0:
                return False
            f = compute_f_other_coop(h, rounds_used)
            if f >= 0.95:
                return True
            threshold_rounds = min(3, L)
            opp_slice = h.opponent_actions[-rounds_used:, :]
            my_slice = h.my_actions[-rounds_used:].astype(int)
            opp_sums = np.sum(opp_slice.astype(int), axis=1)
            total_coops_per_round = opp_sums + my_slice
            allcoop_count = int(np.sum(total_coops_per_round == n))
            return allcoop_count >= threshold_rounds
        if history is None:
            self.punishment_timer = 0
            self.in_punishment = False
            self.awaiting_probation = False
            return Action.C
        t = int(state.round_number)
        rounds_done = history.round_number
        rounds_used = window_slice_count(history)
        if t == r:
            if strong_cooperation_in_window(history, rounds_used):
                self.punishment_timer = 0
                self.in_punishment = False
                self.awaiting_probation = False
                return Action.C
            else:
                self.punishment_timer = 0
                self.in_punishment = False
                self.awaiting_probation = False
                return Action.D
        if self.in_punishment:
            if self.punishment_timer > 0:
                self.punishment_timer -= 1
                return Action.D
            else:
                self.in_punishment = False
                self.awaiting_probation = True
        if self.awaiting_probation:
            f = compute_f_other_coop(history, rounds_used)
            if f >= alpha_forgive:
                self.awaiting_probation = False
                return Action.C
            else:
                r_remaining = r - t + 1
                P = int(min(Pmax, math.ceil((1.0 - f) * r_remaining)))
                if P <= 0:
                    P = 1
                self.in_punishment = True
                self.punishment_timer = max(0, P - 1)
                self.awaiting_probation = False
                return Action.D
        f = compute_f_other_coop(history, rounds_used)
        if f >= alpha:
            return Action.C
        else:
            r_remaining = r - t + 1
            P = int(min(Pmax, math.ceil((1.0 - f) * r_remaining)))
            if P <= 0:
                P = 1
            self.in_punishment = True
            self.punishment_timer = max(0, P - 1)
            self.awaiting_probation = False
            return Action.D

class Strategy_COOPERATIVE_62(BaseStrategy):
    """
    Adaptive Proportional Conditional Cooperator (APCC)

    - Starts by cooperating, defects on the final round.
    - Monitors recent fraction of other players cooperating (short window).
    - If recent cooperation falls below a target threshold, issues a proportional,
      bounded punishment (defect for P rounds). After punishment, runs a one-round
      probation (cooperate). If cooperation meaningfully improves, resume normal
      cooperation; otherwise, issue a short follow-up punishment.
    - Uses only public history and game parameters (n, k, r).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_until_round = 0
        self.last_trigger_frac = None
        self.in_probation = False
        self.probation_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        epsilon = 0.05
        max_punish_fraction = 0.2
        forgiveness_delta = 0.05
        max_window = 5
        t = int(state.round_number)

        def frac_recent(curr_round: int, hist: PlayerHistory) -> float:
            w = min(max_window, curr_round - 1)
            if w <= 0 or hist is None:
                return 1.0
            start_idx = curr_round - w - 1
            end_idx = curr_round - 2
            slice_ops = hist.opponent_actions[start_idx:end_idx + 1, :]
            if slice_ops.size == 0:
                return 1.0
            return float(np.mean(slice_ops))
        if history is None:
            self.punish_until_round = 0
            self.last_trigger_frac = None
            self.in_probation = False
            self.probation_round = None
            return Action.C
        if t == r:
            return Action.D
        if t <= self.punish_until_round:
            return Action.D
        if self.last_trigger_frac is not None and (not self.in_probation) and (t > self.punish_until_round):
            self.in_probation = True
            self.probation_round = t
            return Action.C
        if self.in_probation and self.probation_round == t:
            return Action.C
        if self.in_probation and self.probation_round is not None and (t > self.probation_round):
            observed_frac_post = frac_recent(t, history)
            triggered_frac = 0.0 if self.last_trigger_frac is None else float(self.last_trigger_frac)
            self.in_probation = False
            self.probation_round = None
            if observed_frac_post >= triggered_frac + forgiveness_delta:
                self.last_trigger_frac = None
            else:
                self.punish_until_round = t
                self.last_trigger_frac = observed_frac_post
                return Action.D
        T_val = 0.5 + 0.3 * ((k - 1.0) / max(1.0, n - 1))
        T_coop = min(max(T_val, 0.5), 0.9)
        frac = frac_recent(t, history)
        if frac >= T_coop - epsilon:
            return Action.C
        else:
            s = max(0.0, T_coop - frac)
            r_remain = max(1, r - t + 1)
            P_raw = int(math.ceil(s * r_remain))
            P_cap = max(1, int(math.ceil(max_punish_fraction * r)))
            P = min(max(1, P_raw), P_cap)
            self.punish_until_round = t + P - 1
            self.last_trigger_frac = float(frac)
            return Action.D

class Strategy_COOPERATIVE_63(BaseStrategy):
    """
    Adaptive Conditional Cooperation with Proportional Punishment (ACCPP).

    - Opens with cooperation to signal willingness.
    - Tracks recent group cooperation over a short sliding window M = min(4, r-1).
    - If recent cooperation >= tau (0.6) => cooperate.
    - If recent cooperation < tau => trigger a proportional finite punishment:
      defect immediately and set punishment_counter proportional to shortfall.
    - While punishing: continue defecting until punishment_counter expires,
      but forgive early (end punishment) if a subsequent observed round shows
      cooperation fraction >= tau.
    - Never punish in the final round; always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.M = min(4, max(0, self.r - 1))
        self.tau = 0.6
        self.beta = 0.25
        self.P_min = 1
        self.punishment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = history.round_number + 1
        if t == self.r:
            return Action.D

        def _round_coop_fraction(round_index: int) -> float:
            opp_sum = int(np.sum(history.opponent_actions[round_index, :])) if history.opponent_actions.size else 0
            my_act = int(bool(history.my_actions[round_index]))
            total = opp_sum + my_act
            return float(total) / float(self.n)
        if self.punishment_counter > 0:
            if history.round_number >= 1:
                last_frac = _round_coop_fraction(history.round_number - 1)
            else:
                last_frac = 0.0
            if last_frac >= self.tau:
                self.punishment_counter = 0
                return Action.C
            else:
                self.punishment_counter = max(0, self.punishment_counter - 1)
                return Action.D
        lookback = min(self.M, history.round_number)
        if lookback <= 0:
            return Action.C
        recent_opponents = history.opponent_actions[-lookback:, :] if lookback > 0 else np.zeros((0, 0))
        recent_my = history.my_actions[-lookback:] if lookback > 0 else np.zeros((0,))
        opp_sums = np.sum(recent_opponents, axis=1) if recent_opponents.size else np.zeros((lookback,))
        my_ints = recent_my.astype(int) if recent_my.size else np.zeros((lookback,))
        total_coops_per_round = opp_sums + my_ints
        frac_per_round = total_coops_per_round / float(self.n)
        avg_coop_fraction = float(np.mean(frac_per_round)) if frac_per_round.size else 0.0
        if avg_coop_fraction >= self.tau:
            return Action.C
        shortfall = max(0.0, 1.0 - avg_coop_fraction)
        raw_P = int(math.ceil(shortfall * float(self.n)))
        R_remain = max(0, self.r - t)
        cap_P = max(1, int(math.floor(self.beta * float(self.r - t + 1))))
        P = min(max(raw_P, self.P_min), cap_P, max(0, R_remain))
        self.punishment_counter = max(0, P - 1)
        return Action.D

class Strategy_COOPERATIVE_64(BaseStrategy):
    """
    Forgiving Proportional Reciprocity (FPR) for the N-player public goods game.

    - Starts by cooperating (signals intent).
    - Always defects in the final round.
    - Uses a short memory window L = min(5, r-1) to compute a medium-term cooperation rate.
    - Uses p_last_others (fraction of other players who cooperated in the previous round)
      and p_avg_others (average fraction over the last L rounds) to decide.
    - Has an adaptive threshold theta that depends on k/n and is clamped to [0.3, 0.7].
    - Uses a short, limited punishment timer (P=2) triggered when cooperation falls sharply
      (e.g., last round cooperators <= floor(n/2) or some opponents defected persistently).
    - Punishments are shortened when they cannot fit before the end of the game.
    - Deterministic and only depends on game_description and observed history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(self.game_description.n_players)
        self.r = int(self.game_description.n_rounds)
        self.k = float(self.game_description.k)
        self.P_default = 2
        self.pun = 0
        self.L = min(5, max(0, self.r - 1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.pun = 0
            return Action.C
        t = history.round_number + 1
        if t >= self.r:
            self.pun = 0
            return Action.D
        remaining = self.r - t + 1
        if history.round_number == 0:
            return Action.C
        my_last_action = 1 if bool(history.my_actions[-1]) else 0
        last_opponent_coops = int(history.opponent_actions[-1, :].sum())
        c_last = last_opponent_coops + my_last_action
        if self.n - 1 > 0:
            p_last_others = (c_last - my_last_action) / float(self.n - 1)
        else:
            p_last_others = 0.0
        L = min(self.L, history.round_number)
        if L > 0:
            start_idx = history.round_number - L
            window_opponent_slice = history.opponent_actions[start_idx:history.round_number, :]
            total_other_coops = int(window_opponent_slice.sum())
            window_length = L
            denom = float(window_length * (self.n - 1)) if self.n - 1 > 0 else 1.0
            p_avg_others = total_other_coops / denom
        else:
            p_avg_others = p_last_others
        k_over_n = self.k / float(self.n)
        theta = 0.5 - 0.4 * (k_over_n - 0.5)
        if theta < 0.3:
            theta = 0.3
        elif theta > 0.7:
            theta = 0.7
        if self.pun > 0:
            self.pun -= 1
            return Action.D
        if p_last_others >= theta:
            return Action.C
        if p_avg_others >= theta and remaining >= 2:
            return Action.C
        trigger_punishment = False
        if c_last <= math.floor(self.n / 2):
            trigger_punishment = True
        elif L > 0:
            opp_window = history.opponent_actions[history.round_number - L:history.round_number, :]
            per_opp_coops = opp_window.sum(axis=0)
            if int((per_opp_coops == 0).sum()) > 0:
                trigger_punishment = True
        if trigger_punishment and remaining > 2:
            max_allowed = max(0, remaining - 2)
            effective_pun = min(self.P_default, max_allowed)
            if effective_pun > 0:
                self.pun = effective_pun
                return Action.D
        return Action.D

class Strategy_COOPERATIVE_65(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC)

    - Starts by cooperating.
    - Uses short-term statistics (window m) to decide whether the group is cooperating.
    - If cooperation falls below threshold T, enters a short punishment phase of length P,
      during which it mostly defects but occasionally probes with probability p_test.
    - Forgives early if the most recent observed round is sufficiently cooperative.
    - Lowers threshold slightly if there exist stable cooperators in recent rounds.
    - Always defects in the final round and is slightly more conservative in the final G rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = int(game_description.n_players)
        r = int(game_description.n_rounds)
        k = float(game_description.k)
        self.n = n
        self.r = r
        self.k = k
        self.m = min(5, max(1, math.floor(r / 5)))
        self.P = min(3, max(1, math.floor(r / 10)))
        self.p_test = 0.1
        benefit_gap = 1.0 - k / n
        norm = 1.0 - 1.0 / n
        T_raw = 0.5 + 0.25 * (benefit_gap / (norm if norm != 0 else 1.0))
        self.T = max(0.5, min(0.75, T_raw))
        self.G = min(2, math.floor(r / 10))
        self.punishment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        num_past = history.round_number
        t = state.round_number if hasattr(state, 'round_number') and state.round_number is not None else num_past + 1
        try:
            t = int(t)
        except Exception:
            t = num_past + 1
        if t >= self.r:
            return Action.D
        recent_count = min(self.m, num_past)
        if recent_count <= 0:
            f_recent = 1.0
            any_stable_cooperator = False
        else:
            opp_actions = np.asarray(history.opponent_actions)
            my_actions = np.asarray(history.my_actions).astype(int)
            opp_recent = opp_actions[-recent_count:, :] if recent_count > 0 else np.zeros((0, self.n - 1), dtype=bool)
            my_recent = my_actions[-recent_count:] if recent_count > 0 else np.zeros((0,), dtype=int)
            coops_per_round = np.sum(opp_recent.astype(int), axis=1) + my_recent
            f_recent = float(np.mean(coops_per_round / float(self.n))) if coops_per_round.size > 0 else 1.0
            if opp_recent.size == 0:
                opp_rates = np.array([], dtype=float)
            else:
                opp_rates = np.mean(opp_recent.astype(float), axis=0)
            self_rate = float(np.mean(my_recent)) if my_recent.size > 0 else 0.0
            rates = np.concatenate([opp_rates, np.array([self_rate], dtype=float)]) if opp_rates.size > 0 else np.array([self_rate], dtype=float)
            any_stable_cooperator = np.any(rates >= 0.8)
        T_eff = float(self.T)
        if any_stable_cooperator:
            T_eff = max(0.4, T_eff - 0.05)
        if self.r - t < self.G:
            T_eff = T_eff + 0.05
        T_eff = max(0.0, min(1.0, T_eff))
        last_round_coop_frac = None
        if num_past > 0:
            last_opp = np.asarray(history.opponent_actions)[-1, :] if num_past > 0 else np.zeros((self.n - 1,), dtype=bool)
            last_my = int(np.asarray(history.my_actions)[-1]) if num_past > 0 else 0
            last_coop_count = int(np.sum(last_opp.astype(int))) + int(last_my)
            last_round_coop_frac = last_coop_count / float(self.n)
        action = Action.C
        just_started_punishment = False
        if self.punishment_counter <= 0 and f_recent < T_eff:
            self.punishment_counter = int(self.P)
            just_started_punishment = True
        if self.punishment_counter > 0:
            if random.random() < self.p_test:
                action = Action.C
            else:
                action = Action.D
        elif f_recent >= T_eff:
            action = Action.C
        elif random.random() < self.p_test:
            action = Action.C
        else:
            action = Action.D
        if last_round_coop_frac is not None and last_round_coop_frac >= T_eff:
            self.punishment_counter = 0
        elif self.punishment_counter > 0:
            self.punishment_counter = max(0, self.punishment_counter - 1)
        return action

class Strategy_COOPERATIVE_66(BaseStrategy):
    """
    Adaptive Generous Win-Stay–Lose-Shift (AG-WSLS)

    - Start by cooperating.
    - In the final round, defect.
    - In intermediate rounds, compare your realized payoff last round to the
      counterfactual payoff you would have received by doing the opposite.
      If your choice was at least as good, repeat it (tie -> cooperate).
      If it was strictly worse, switch, except:
        - If switching to D but last round had strong cooperation (S >= ceil(alpha*n)),
          forgive and cooperate.
        - If switching to C but last round had very low cooperation (S <= floor(beta*n))
          and you were defecting, stay defecting (no forgiveness).
    - Tunable parameters: alpha (forgiveness threshold, default 0.75),
      beta (no-forgiveness lower bound, default 0.25).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.alpha = 0.75
        self.beta = 0.25
        self.eps = 1e-09

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        if history is None or (hasattr(history, 'round_number') and history.round_number == 0):
            return Action.C
        if state.round_number == r:
            return Action.D
        c_prev_bool = bool(history.my_actions[-1])
        c_prev = 1 if c_prev_bool else 0
        opp_coops_last = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size > 0 else 0
        S = opp_coops_last + (1 if c_prev_bool else 0)
        pi_prev = float(history.my_payoffs[-1])
        if c_prev == 1:
            pi_alt = 1.0 + k / n * float(S - 1)
        else:
            pi_alt = k / n * float(S + 1)
        if pi_prev > pi_alt + self.eps:
            return Action.C if c_prev_bool else Action.D
        if math.isclose(pi_prev, pi_alt, rel_tol=0.0, abs_tol=self.eps):
            return Action.C
        intended = 1 - c_prev
        if intended == 0:
            if S >= math.ceil(self.alpha * n):
                return Action.C
            else:
                return Action.D
        if intended == 1:
            if S <= math.floor(self.beta * n) and c_prev == 0:
                return Action.D
            else:
                return Action.C
        return Action.C

class Strategy_COOPERATIVE_67(BaseStrategy):
    """
    Generous Conditional Cooperator (GCC)

    - Starts cooperative.
    - Cooperates when a clear majority recently cooperated.
    - Proportionally short punishments when majority defects.
    - Occasional probes during punishment/withdrawal to test recovery.
    - Forgives quickly if probes are reciprocated.
    - Withdraws (mostly defects) if repeatedly exploited.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.coop_threshold = math.ceil(n / 2)
        self.punish_max = 3
        self.probe_prob = 0.05
        self.W = min(5, r)
        self.exploit_epsilon = 0.02
        self.punishment_timer = 0
        self.withdraw_flag = False
        self.my_last_action = None
        self.probed_last_round = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            self.probed_last_round = False
            self.my_last_action = True
            return Action.C
        completed = history.round_number
        n = self.game_description.n_players
        total_rounds = self.game_description.n_rounds
        rounds_remaining = total_rounds - completed
        last_my_action = bool(history.my_actions[-1])
        opp_last_row = history.opponent_actions[-1, :]
        opp_coops_last = int(np.sum(opp_last_row))
        M_prev = opp_coops_last + (1 if last_my_action else 0)
        others_prev = M_prev - (1 if last_my_action else 0)
        if self.probed_last_round:
            if others_prev >= self.coop_threshold - 1:
                self.punishment_timer = 0
                self.withdraw_flag = False
            self.probed_last_round = False
        window = min(self.W, completed)
        if window > 0:
            recent_payoffs = history.my_payoffs[-window:]
            my_avg_payoff_window = float(np.mean(recent_payoffs))
            recent_coops = int(np.sum(history.my_actions[-window:]))
            if my_avg_payoff_window <= 1.0 + self.exploit_epsilon and recent_coops > window / 2.0:
                self.withdraw_flag = True
        if self.punishment_timer == 0 and (not self.withdraw_flag):
            if others_prev < self.coop_threshold - 1:
                shortfall = self.coop_threshold - 1 - others_prev
                self.punishment_timer = int(min(self.punish_max, max(1, shortfall), max(0, rounds_remaining)))
        if rounds_remaining <= 0:
            action_bool = False
            self.probed_last_round = False
        elif rounds_remaining == 1:
            if not self.withdraw_flag and others_prev >= self.coop_threshold - 1 and last_my_action:
                action_bool = True
            else:
                action_bool = False
            self.probed_last_round = False
        elif self.withdraw_flag:
            if random.random() < self.probe_prob:
                action_bool = True
                self.probed_last_round = True
            else:
                action_bool = False
                self.probed_last_round = False
        elif self.punishment_timer > 0:
            if random.random() < self.probe_prob:
                action_bool = True
                self.probed_last_round = True
            else:
                action_bool = False
                self.probed_last_round = False
        else:
            action_bool = True
            self.probed_last_round = False
        if self.punishment_timer > 0:
            self.punishment_timer = max(0, self.punishment_timer - 1)
        self.my_last_action = bool(action_bool)
        return Action.C if action_bool else Action.D

class Strategy_COOPERATIVE_68(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Proportional Punishment (ACCP-P).

    - Opens with cooperation.
    - Uses a short moving window (up to 5 rounds) to estimate the fraction of other players
      who cooperated recently. If that fraction meets or exceeds a dynamically computed
      threshold (target_coop minus small forgiveness eps), cooperate; otherwise defect and
      initiate a short, proportionate punishment (a limited number of defections).
    - Always defects in the final round.
    - Internal persistent state: rounds_left_to_punish
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = float(self.game_description.n_players)
        k = float(self.game_description.k)
        self.MPCR = k / n
        raw_target = 0.5 + 0.4 * (1.0 - self.MPCR)
        self.target_coop = min(0.9, max(0.5, raw_target))
        self.S = 5
        self.eps = 0.05
        self.max_punish = 3
        self.rounds_left_to_punish = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        r = int(self.game_description.n_rounds)
        n = int(self.game_description.n_players)
        if history is None or history.round_number == 0:
            return Action.C
        if t == r:
            return Action.D
        if self.rounds_left_to_punish > 0:
            self.rounds_left_to_punish = max(0, self.rounds_left_to_punish - 1)
            return Action.D
        w = min(5, max(1, t - 1))
        opp_actions = history.opponent_actions
        completed_rounds = history.round_number
        start_idx = max(0, completed_rounds - w)
        recent_opp = opp_actions[start_idx:completed_rounds, :]
        others_coop_count = int(np.sum(recent_opp))
        denom = float(recent_opp.shape[0] * max(1, recent_opp.shape[1]))
        others_coop_rate = float(others_coop_count) / denom if denom > 0 else 0.0
        if others_coop_rate >= self.target_coop - self.eps:
            return Action.C
        else:
            deficit = self.target_coop - others_coop_rate
            raw_punish = int(round(deficit * self.S))
            punishment_length = min(self.max_punish, max(1, raw_punish))
            remaining_after_this = max(0, r - t)
            self.rounds_left_to_punish = min(punishment_length, remaining_after_this)
            return Action.D

class Strategy_COOPERATIVE_69(BaseStrategy):
    """
    Generous Group TFT with Adaptive Punishment (GGTFT-AP).

    - Starts cooperating.
    - Tolerates up to f defectors in a round without punishing.
    - If many defectors occur, applies a graded punishment whose length scales
      with the number of defectors, bounded by Pmax and at least P0 rounds.
    - Punishment phases are consecutive defections tracked by punishment_counter.
    - Forgives quickly if the group returns to cooperation (within tolerance).
    - In the final g rounds (endgame) plays cautiously: cooperate only if the
      previous round was effectively unanimous (everyone but possibly self cooperated).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.w = min(5, r)
        self.f = max(1, round(0.2 * n))
        self.P0 = 2
        self.Pmax = max(1, min(math.ceil(r / 10), r))
        self.g = min(3, max(1, math.floor(r / 10)))
        self.punishment_counter = 0
        self.last_action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if history is None:
            self.last_action = Action.C
            return Action.C
        current_round = history.round_number + 1
        if history.round_number == 0:
            m_prev = n
        else:
            opp_coops = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size else 0
            my_prev_coop = bool(history.my_actions[-1])
            m_prev = opp_coops + (1 if my_prev_coop else 0)
        if self.punishment_counter > 0:
            action = Action.D
            self.punishment_counter = max(0, self.punishment_counter - 1)
            self.last_action = action
            return action
        if current_round > r - self.g:
            if m_prev >= n - 1:
                action = Action.C
            else:
                action = Action.D
            self.last_action = action
            return action
        if m_prev >= n - self.f:
            action = Action.C
            self.last_action = action
            return action
        else:
            severity = n - m_prev
            punishment_length = min(self.Pmax, self.P0 + math.ceil(severity / 2))
            self.punishment_counter = int(punishment_length)
            action = Action.D
            self.punishment_counter = max(0, self.punishment_counter - 1)
            self.last_action = action
            return action

class Strategy_COOPERATIVE_70(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Limited Punishment (ACCLP).

    - Starts by cooperating.
    - Uses up to 3-round memory of opponents' cooperation fraction (s_recent).
    - Computes a cooperation threshold that decreases with higher public-good return (k).
    - If recent opponent cooperation >= threshold: cooperate.
    - If below threshold: start a short proportional punishment (defect this round and for a few rounds),
      with punishment length scaled to how far below threshold the recent cooperation is.
    - Punishments are cancelable early if opponents return to cooperating.
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        if history is None:
            return Action.C
        t = history.round_number + 1
        if t == r:
            self.punishment_counter = 0
            return Action.D
        m = min(3, max(0, history.round_number))
        if m == 0:
            s_recent = 1.0
        else:
            last_opponent_actions = history.opponent_actions[-m:, :]
            s_recent = float(np.mean(last_opponent_actions))
        gamma = (k - 1.0) / max(1.0, n - 1.0)
        coop_threshold = max(0.1, 0.5 - 0.25 * gamma)
        P_max = min(3, max(1, r - 2))
        if self.punishment_counter > 0:
            if s_recent >= coop_threshold:
                self.punishment_counter = 0
                return Action.C
            else:
                self.punishment_counter = max(0, self.punishment_counter - 1)
                return Action.D
        if s_recent >= coop_threshold:
            return Action.C
        else:
            delta = coop_threshold - s_recent
            scale = max(0.01, coop_threshold)
            raw_len = 1 + math.floor(delta / scale * P_max)
            punishment_length = int(min(P_max, max(1, raw_len)))
            self.punishment_counter = max(0, punishment_length - 1)
            return Action.D

class Strategy_COOPERATIVE_71(BaseStrategy):
    """
    Generous Conditional Cooperator with Finite Punishment (GCC-FP).

    Starts cooperative, uses per-player exponential frequencies and a short
    recent-window group average to detect significant drops or persistent
    defectors. Triggers a short, finite punishment P rounds when warranted,
    otherwise cooperates when group signals are sufficiently cooperative.
    Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.P = max(1, min(4, math.floor(self.r / 10)))
        self.L = min(5, max(1, math.floor(self.r / 5)))
        if self.n - 1 > 0:
            raw_q = 0.5 - 0.2 * ((self.k - 1.0) / (self.n - 1.0))
        else:
            raw_q = 0.5
        self.q = max(0.3, min(raw_q, 0.7))
        self.d = max(1, math.ceil(0.15 * self.n))
        self.w = 0.5
        self.freq_j = np.ones(self.n - 1, dtype=float)
        self.recent_cooperators = []
        self.punish_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = getattr(state, 'round_number', None)
        if t is None:
            if history is None:
                t = 1
            else:
                t = history.round_number + 1
        if history is None or (hasattr(history, 'round_number') and history.round_number == 0):
            return Action.C
        if t == self.r:
            return Action.D
        rounds_completed = history.round_number
        if rounds_completed <= 0:
            return Action.C
        last_op_actions = history.opponent_actions[-1, :]
        last_my_action = bool(history.my_actions[-1])
        c_last = last_op_actions.astype(float)
        self.freq_j = self.w * self.freq_j + (1.0 - self.w) * c_last
        m_prev = int(np.sum(last_op_actions) + (1 if last_my_action else 0))
        self.recent_cooperators.append(m_prev)
        if len(self.recent_cooperators) > self.L:
            excess = len(self.recent_cooperators) - self.L
            if excess >= len(self.recent_cooperators):
                self.recent_cooperators = []
            else:
                self.recent_cooperators = self.recent_cooperators[excess:]
        if len(self.recent_cooperators) > 0:
            m_avg = float(np.mean(self.recent_cooperators))
        else:
            m_avg = float(max(0, self.n - 1))
        if self.freq_j.size > 0:
            group_freq = float(np.mean(self.freq_j))
        else:
            group_freq = 0.0
        persistent_defector = False
        if last_op_actions.shape[0] > 0:
            for j in range(len(self.freq_j)):
                if self.freq_j[j] < 0.2 and (not bool(last_op_actions[j])):
                    persistent_defector = True
                    break
        threshold_q_n = math.ceil(self.q * self.n)
        significant_drop = m_prev < m_avg - self.d or m_prev < threshold_q_n
        if persistent_defector or significant_drop:
            self.punish_timer = self.P
        if self.punish_timer > 0:
            action = Action.D
            self.punish_timer = max(0, self.punish_timer - 1)
            return action
        elif group_freq >= self.q or m_prev >= threshold_q_n:
            return Action.C
        else:
            action = Action.D
            self.punish_timer = max(0, self.P - 1)
            return action

class Strategy_COOPERATIVE_72(BaseStrategy):
    """
    Forgiving Conditional Majority (FCM)

    Start cooperative, reciprocate when a clear majority cooperated recently,
    apply short proportional punishments when defections are substantial,
    forgive occasional lapses by otherwise-cooperative players, and defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        majority_threshold = math.ceil(n / 2)
        small_defection_cut = math.floor(n / 4)
        max_punish = 3
        forgiveness_rate = 0.8
        if history is None:
            return Action.C
        completed = int(history.round_number)
        t = completed + 1
        r_remaining = r - (t - 1)
        if t == r:
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer = max(0, self.punishment_timer - 1)
            return Action.D
        my_actions = np.asarray(history.my_actions, dtype=bool)
        opp_actions = np.asarray(history.opponent_actions, dtype=bool)
        if completed > 0:
            opp_coop_counts = np.sum(opp_actions[:completed, :], axis=0) if opp_actions.size else np.array([])
            opp_coop_rates = opp_coop_counts.astype(float) / float(completed)
        else:
            opp_coop_rates = np.zeros((n - 1,), dtype=float)

        def adjusted_cooperators_for_round(round_idx: int):
            myc = bool(my_actions[round_idx]) if my_actions.size else False
            if opp_actions.size:
                opps_round = opp_actions[round_idx, :]
                adjusted_opps = []
                for j in range(opps_round.shape[0]):
                    acted = bool(opps_round[j])
                    if not acted and opp_coop_rates[j] >= forgiveness_rate:
                        adjusted_opps.append(1)
                    else:
                        adjusted_opps.append(1 if acted else 0)
                opp_sum = int(sum(adjusted_opps))
            else:
                opp_sum = 0
            return (1 if myc else 0) + opp_sum
        last_round_idx = completed - 1
        effective_coop_last = adjusted_cooperators_for_round(last_round_idx)
        L = min(3, t - 1)
        adjusted_counts = []
        start_idx = max(0, completed - L)
        for idx in range(start_idx, completed):
            adjusted_counts.append(adjusted_cooperators_for_round(idx))
        if adjusted_counts:
            smoothed_group_coop = float(np.mean(np.asarray(adjusted_counts, dtype=float)))
        else:
            smoothed_group_coop = float(effective_coop_last)
        if effective_coop_last >= majority_threshold:
            return Action.C
        if effective_coop_last <= small_defection_cut:
            punish_len = int(math.floor(r_remaining / 3.0))
            punish_len = max(1, punish_len)
            punish_len = min(max_punish, punish_len)
            self.punishment_timer = punish_len
            if self.punishment_timer > 0:
                self.punishment_timer = max(0, self.punishment_timer - 1)
            return Action.D
        if smoothed_group_coop >= majority_threshold:
            return Action.C
        else:
            punish_len = 1 + int(math.floor(r_remaining / 4.0))
            punish_len = max(1, punish_len)
            punish_len = min(max_punish, punish_len)
            self.punishment_timer = punish_len
            if self.punishment_timer > 0:
                self.punishment_timer = max(0, self.punishment_timer - 1)
            return Action.D

class Strategy_COOPERATIVE_73(BaseStrategy):
    """
    Adaptive Generous Conditional Cooperator (AGCC)

    - Starts cooperative to signal willingness.
    - Uses a short memory window W and target count g to decide whether the environment is cooperative.
    - Employs short, bounded punishments (P) when cooperation drops below the threshold.
    - Detects and targets persistent defectors (set T) based on recent behavior.
    - Rapidly forgives: requires only S good rounds to return to sustained cooperation.
    - Always defects in the final round.
    - Includes a small generosity mechanism: when the group just misses the threshold by 1
      and the player cooperated last round, cooperate with probability p=0.7 to avoid lock-in.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, max(0, self.r - 1))
        self.alpha = max(0.5, self.k / self.n)
        self.g = int(math.ceil(self.alpha * self.n))
        self.P = max(1, min(3, int(math.floor(self.r / 10))))
        self.S = 2
        self.tau = int(math.ceil(0.6 * max(1, self.W)))
        self.recovery_threshold = max(1, int(math.ceil(0.4 * max(1, self.W))))
        self.phase = 'COOP'
        self.punishment_rounds_remaining = 0
        self.T = set()
        self.consecutive_good_rounds = 0
        self.generosity_p = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory):
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t == self.r:
            return Action.D
        if history is None:
            self.phase = 'COOP'
            self.consecutive_good_rounds = 0
            self.punishment_rounds_remaining = 0
            self.T = set()
            return Action.C
        rounds_completed = history.round_number
        if rounds_completed <= 0:
            m_last = 1
            last_my_action = True
        else:
            opp_last = history.opponent_actions[-1, :]
            last_my_action = bool(history.my_actions[-1])
            m_last = int(np.sum(opp_last.astype(int)) + (1 if last_my_action else 0))
        W_window = min(self.W, rounds_completed)
        if W_window > 0:
            opp_window = history.opponent_actions[-W_window:, :]
            my_window = history.my_actions[-W_window:]
        else:
            opp_window = np.zeros((0, max(0, self.n - 1)), dtype=bool)
            my_window = np.zeros((0,), dtype=bool)
        coop_counts = [0] * self.n
        if W_window > 0:
            opp_counts = np.sum(opp_window.astype(int), axis=0) if self.n > 1 else np.array([], dtype=int)
            for i in range(self.n - 1):
                coop_counts[i] = int(opp_counts[i])
            coop_counts[self.n - 1] = int(np.sum(my_window.astype(int)))
        else:
            coop_counts = [0] * self.n
        new_T = set()
        if W_window > 0:
            for j in range(self.n):
                bad_count = 0
                for r_idx in range(W_window):
                    if j < self.n - 1:
                        action_j = bool(opp_window[r_idx, j])
                        others_coop = int(np.sum(opp_window[r_idx, :].astype(int))) + (1 if my_window[r_idx] else 0) - (1 if action_j else 0)
                    else:
                        action_j = bool(my_window[r_idx])
                        others_coop = int(np.sum(opp_window[r_idx, :].astype(int))) if self.n > 1 else 0
                    if not action_j and others_coop >= self.g:
                        bad_count += 1
                if bad_count >= self.tau:
                    new_T.add(j)
        if len(new_T) > 0:
            self.T = self.T.union(new_T)
            if self.phase != 'TARGETED_PUNISH':
                self.phase = 'TARGETED_PUNISH'
        action = Action.D
        if self.phase == 'COOP':
            if m_last >= self.g:
                action = Action.C
                self.consecutive_good_rounds = min(self.S, self.consecutive_good_rounds + 1)
            elif m_last == max(0, self.g - 1) and last_my_action:
                if random.random() < self.generosity_p:
                    action = Action.C
                    self.consecutive_good_rounds = min(self.S, self.consecutive_good_rounds + 1)
                else:
                    self.phase = 'PUNISH'
                    self.punishment_rounds_remaining = self.P
                    self.consecutive_good_rounds = 0
                    action = Action.D
            else:
                self.phase = 'PUNISH'
                self.punishment_rounds_remaining = self.P
                self.consecutive_good_rounds = 0
                action = Action.D
        elif self.phase == 'PUNISH':
            action = Action.D
            if self.punishment_rounds_remaining > 0:
                self.punishment_rounds_remaining -= 1
            if self.punishment_rounds_remaining <= 0:
                self.phase = 'COOP'
                self.consecutive_good_rounds = 0
        elif self.phase == 'TARGETED_PUNISH':
            if len(self.T) == 0:
                self.phase = 'PUNISH'
                self.punishment_rounds_remaining = self.P
                action = Action.D
            else:
                action = Action.D
                updated_T = set()
                for j in list(self.T):
                    if W_window > 0:
                        if coop_counts[j] < self.recovery_threshold:
                            updated_T.add(j)
                    else:
                        updated_T.add(j)
                self.T = updated_T
        else:
            action = Action.D
        return action

class Strategy_COOPERATIVE_74(BaseStrategy):
    """
    Windowed Forgiving Punisher for Public Goods.

    - Cooperates by default (first move C).
    - Estimates others' recent cooperation rate over a window m = min(10, t-1).
    - If recent others' cooperation >= threshold T_coop (depends on k/n) -> cooperate.
    - If below threshold -> enter short, probabilistic punishment (mostly defect with
      forgiveness probability g). Punishment cycles are short (P rounds) and the
      decision re-evaluates cooperation each round so recovery is possible.
    - Immediate reward: if in the last round all other players cooperated, cooperate.
    - Endgame: in the final L rounds (computed from k/n and r) always defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.P = 2
        self.g = 0.2
        self.eps = 1e-09

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        if history is None or history.round_number == 0:
            return Action.C
        t = history.round_number + 1
        frac = 1.0 - k / float(n)
        L = min(max(1, math.ceil(frac * 5)), max(0, r - 1))
        if t > r - L:
            return Action.D
        m = min(10, t - 1)
        mid = 0.6 - 0.2 * (k / float(n))
        T_coop = min(max(0.4, mid), 0.7)
        last_round_opponents = history.opponent_actions[-1, :]
        if int(np.sum(last_round_opponents)) == n - 1:
            return Action.C
        if m == 0:
            O_rate = 1.0
        else:
            S = float(np.sum(history.opponent_actions[-m:, :]))
            O_rate = S / (m * (n - 1))
        if O_rate + self.eps >= T_coop:
            return Action.C
        if random.random() < self.g:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_75(BaseStrategy):
    """
    Start-cooperative, forgiving-but-calibrated punisher for the N-player public goods game.

    - Begins by cooperating.
    - On each non-final round, observes the fraction of cooperators in the previous round.
      * If the previous round was unanimously cooperative or a majority cooperated, cooperate.
      * If the previous round had less than half cooperating, initiate a calibrated finite punishment
        (defect for a computed number of rounds), where the minimum punishment length P0 is
        determined from the game parameters (n, k) to deter one-shot deviations.
    - Punishments are finite and followed by forgiveness (the strategy will resume cooperation
      after the punishment window if opponents cooperate).
    - In the final round, cooperate only if every prior round was unanimously cooperative; otherwise defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        denom = self.k - 1.0
        if denom <= 0:
            P0 = 1
        else:
            raw = (1.0 - self.k / float(self.n)) / denom
            P0 = int(math.ceil(raw)) if raw > 0 else 1
        if P0 < 1:
            P0 = 1
        self.P0 = P0
        self.punishing_until = 0
        self.last_round_all_cooperate_flag = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punishing_until = 0
            self.last_round_all_cooperate_flag = True
            return Action.C
        t = history.round_number + 1
        if t < 1:
            t = 1
        if t > self.r:
            t = self.r
        if history.opponent_actions.size == 0:
            opp_coop_last = 0
        else:
            opp_coop_last = int(np.sum(history.opponent_actions[-1, :]))
        my_coop_last = int(bool(history.my_actions[-1]))
        total_coop_last = opp_coop_last + my_coop_last
        f_prev = float(total_coop_last) / float(self.n)
        all_coop_prev = total_coop_last == self.n
        self.last_round_all_cooperate_flag = self.last_round_all_cooperate_flag and all_coop_prev
        if t == self.r:
            if self.last_round_all_cooperate_flag:
                return Action.C
            else:
                return Action.D
        if t <= self.punishing_until:
            return Action.D
        if all_coop_prev:
            return Action.C
        if f_prev >= 0.5:
            return Action.C
        remaining_rounds = self.r - t + 1
        scaled_raw = (1.0 - f_prev) * float(remaining_rounds) * 0.25
        extra_P = int(math.ceil(scaled_raw)) if scaled_raw > 0 else 0
        P = max(self.P0, extra_P)
        max_allowed = self.r - t
        if max_allowed < 1:
            P = 1
        elif P > max_allowed:
            P = max_allowed
        self.punishing_until = t + P - 1
        return Action.D

class Strategy_COOPERATIVE_76(BaseStrategy):
    """
    Adaptive Win‑Stay / Lose‑Shift with Forgiveness and Endgame Safety (AWSLS).

    - Starts cooperating.
    - Keeps an adaptive aspiration level A (initialized midway between mutual
      defection and mutual cooperation) and uses exponential smoothing to update it.
    - If the previous round had essentially no cooperation (<=1 cooperator), defects.
    - Otherwise, if last payoff >= A, repeats previous action (win-stay).
      If last payoff < A, switches action (lose-shift): C->D (punish), D->C (probe/forgive).
    - Always defects on the final round (endgame safety).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.A = (1.0 + self.k) / 2.0
        if self.r > 1:
            self.lambda_ = max(0.8, 1.0 - 1.0 / float(self.r))
        else:
            self.lambda_ = 0.9
        self.epsilon = 1.0 / float(self.n)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        current_round = int(state.round_number)
        last_my_action_bool = bool(history.my_actions[-1])
        last_my_payoff = float(history.my_payoffs[-1])
        opp_coops = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size > 0 else 0
        last_total_coops = (1 if last_my_action_bool else 0) + opp_coops
        if current_round == self.r:
            action = Action.D
        elif last_total_coops <= 1:
            action = Action.D
        elif last_my_payoff >= self.A:
            action = Action.C if last_my_action_bool else Action.D
        else:
            action = Action.D if last_my_action_bool else Action.C
        self.A = self.lambda_ * self.A + (1.0 - self.lambda_) * last_my_payoff
        return action

class Strategy_COOPERATIVE_77(BaseStrategy):
    """
    Adaptive Forgiving Conditional Cooperator (AFCC)

    - Starts by cooperating to signal niceness.
    - Maintains an exponentially-weighted "trust" score T in [0,1].
    - Rewards strong recent cooperation immediately.
    - Proportionally punishes sharp drops in cooperation for a short bounded period.
    - Forgives and allows trust to recover when cooperation returns.
    - Defects in the final round for endgame safety.

    State:
    - self.T: trust score (float)
    - self.punish_timer: remaining punishment rounds (int)
    - self.frac_history: list of observed fraction cooperators per completed round
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.lambda_ = 0.6
        self.tau = 0.55
        self.tau_high = 0.8
        self.forgiveness_floor = 0.2
        self.delta = 0.15
        self.gamma = 0.5
        self.P_max = max(1, math.ceil(0.2 * max(1, self.game_description.n_rounds)))
        self.T = 1.0
        self.punish_timer = 0
        self.frac_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = getattr(state, 'round_number', None)
        if t is None:
            t = 1 if history is None else history.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if history is None or (hasattr(history, 'round_number') and history.round_number == 0):
            return Action.C
        if t == r:
            return Action.D
        last_idx = -1
        opp_last = history.opponent_actions[last_idx, :] if history.opponent_actions.size else np.array([], dtype=bool)
        my_last = bool(history.my_actions[last_idx])
        total_coops_last = int(np.sum(opp_last)) + (1 if my_last else 0)
        f_last = total_coops_last / float(n)
        if history.round_number >= 2:
            opp_prev = history.opponent_actions[-2, :]
            my_prev = bool(history.my_actions[-2])
            total_coops_prev = int(np.sum(opp_prev)) + (1 if my_prev else 0)
            f_prev = total_coops_prev / float(n)
        else:
            f_prev = f_last
        self.frac_history.append(f_last)
        self.T = self.lambda_ * self.T + (1.0 - self.lambda_) * f_last
        if self.T < 0.0:
            self.T = 0.0
        elif self.T > 1.0:
            self.T = 1.0
        if self.punish_timer > 0:
            self.punish_timer -= 1
            return Action.D
        if f_last >= self.tau_high:
            return Action.C
        if self.T >= self.tau:
            return Action.C
        drop = max(0.0, f_prev - f_last)
        if drop >= self.delta:
            r_remaining = max(1, r - t + 1)
            p_len = min(self.P_max, max(1, math.ceil(self.gamma * r_remaining * drop)))
            self.punish_timer = max(0, int(p_len) - 1)
            return Action.D
        if f_last >= self.forgiveness_floor:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_78(BaseStrategy):
    """
    Generous Gradual Reciprocity (GGR) for the N-player Public Goods Game.

    - Signals cooperation on the first round.
    - Cooperates when recent group cooperation meets a threshold T (adjusted by k).
    - When the group under-contributes, initiates a short, graded punishment whose
      length scales with the deficit relative to ceil(T * n). Punishments are
      cancellable after two consecutive rounds of recovery (>= ceil(T*n)).
    - Includes a small probing probability to cooperate while punishing to recover
      from noise.
    - Always defects in the last round (endgame).
    - For very short games (r <= 3) uses a conservative rule: if more than one
      player defected in round 1, switch to permanent defection to avoid being
      exploited in tiny horizons.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.mode = 'GOOD'
        self.remaining_punish = 0
        self.consecutive_recovery_rounds = 0
        self.permanent_defect = False
        self.T_base = 0.5
        self.gamma = 1.0
        self.p_probe_default = 0.05
        self.Endgame_safe = 1
        self.W = min(5, max(0, self.game_description.n_rounds - 1))
        self.n = int(self.game_description.n_players)
        self.r = int(self.game_description.n_rounds)
        self.k = float(self.game_description.k)
        if self.n > 1:
            raw = 0.5 + 0.3 * (self.k - 1.0) / (self.n - 1.0)
            self.T = max(0.5, min(0.8, raw))
        else:
            self.T = self.T_base
        if self.r <= 3:
            self.p_probe = max(0.0, self.p_probe_default * 0.5)
        else:
            self.p_probe = self.p_probe_default

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.permanent_defect:
            return Action.D
        if state.round_number == self.r:
            return Action.D
        if history is None:
            return Action.C
        last_my = bool(history.my_actions[-1])
        last_opponents = history.opponent_actions[-1, :]
        observed_cooperators = int(last_my) + int(np.sum(last_opponents))
        if self.r <= 3 and history.round_number >= 1:
            first_my = bool(history.my_actions[0])
            first_opponents = history.opponent_actions[0, :]
            first_cooperators = int(first_my) + int(np.sum(first_opponents))
            if first_cooperators <= self.n - 2:
                self.permanent_defect = True
                return Action.D
        ceil_T_n = math.ceil(self.T * self.n)
        if observed_cooperators >= ceil_T_n:
            self.consecutive_recovery_rounds += 1
        else:
            self.consecutive_recovery_rounds = 0
        if self.remaining_punish > 0:
            if self.consecutive_recovery_rounds >= 2:
                self.remaining_punish = 0
                self.mode = 'GOOD'
            else:
                if random.random() < self.p_probe:
                    action = Action.C
                else:
                    action = Action.D
                self.remaining_punish = max(0, self.remaining_punish - 1)
                if self.remaining_punish == 0:
                    self.mode = 'GOOD'
                return action
        if observed_cooperators >= ceil_T_n:
            return Action.C
        else:
            deficit = max(0, ceil_T_n - observed_cooperators)
            t = state.round_number
            remaining_future_rounds = max(0, self.r - t)
            punish_length = 1 + math.ceil(self.gamma * deficit)
            self.remaining_punish = min(remaining_future_rounds, punish_length)
            self.mode = 'PUNISH'
            if self.remaining_punish > 0:
                self.remaining_punish = max(0, self.remaining_punish - 1)
            if self.remaining_punish == 0:
                self.mode = 'GOOD'
            return Action.D

class Strategy_COOPERATIVE_79(BaseStrategy):
    """
    Adaptive Forgiving Conditional Cooperator (AFCC)

    - Opens with cooperation.
    - Estimates recent cooperation among others (window w).
    - If others are sufficiently cooperative (>= gamma) -> cooperate.
    - If shortfall detected -> launch short proportionate punishment (defect T rounds).
      Punishment length scales with magnitude of shortfall, capped by Pmax
      and never initiated if it would push into the endgame horizon E.
    - After a punishment ends, with probability p_test play a one-round cooperative test.
    - Uses a smoothed aspiration A and WSLS-like fallback when no clear prescription.
    - Always defects in the final round (no future to enforce reciprocity).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.w = min(3, game_description.n_rounds)
        self.gamma = 0.6
        self.Pmax = 3
        self.s = 2.0 / max(1, game_description.n_players - 1)
        self.p_test = 0.25
        self.alpha = 0.2
        self.E = 1
        self.in_punishment = False
        self.punishment_left = 0
        self.in_test = False
        self.last_action = Action.C
        self.A = (1.0 + game_description.k) / 2.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.in_punishment = False
            self.punishment_left = 0
            self.in_test = False
            self.last_action = Action.C
            self.A = (1.0 + self.game_description.k) / 2.0
            return Action.C
        completed = history.round_number
        t = completed + 1
        r = self.game_description.n_rounds
        n = self.game_description.n_players
        if t > r:
            return Action.D
        if t == r:
            self.last_action = Action.D
            self.in_punishment = False
            self.punishment_left = 0
            self.in_test = False
            return Action.D
        if completed >= 1:
            last_payoff = float(history.my_payoffs[-1])
            self.A = (1.0 - self.alpha) * self.A + self.alpha * last_payoff
        if self.in_test:
            self.in_test = False
            self.last_action = Action.C
            return Action.C
        if self.in_punishment and self.punishment_left > 0:
            self.punishment_left -= 1
            play = Action.D
            if self.punishment_left == 0:
                self.in_punishment = False
                if random.random() < self.p_test:
                    self.in_test = True
            self.last_action = play
            return play
        elif self.in_punishment and self.punishment_left <= 0:
            self.in_punishment = False
        W = min(self.w, completed)
        observed_frac = 0.0
        if W > 0:
            recent = history.opponent_actions[-W:, :]
            sum_others = float(np.sum(recent))
            observed_frac = sum_others / (W * (n - 1))
        else:
            observed_frac = 1.0
        if observed_frac >= self.gamma:
            self.last_action = Action.C
            return Action.C
        rounds_remaining = r - t + 1
        max_punishable_rounds = max(0, rounds_remaining - self.E)
        if max_punishable_rounds <= 0:
            if completed >= 1:
                last_payoff = float(history.my_payoffs[-1])
                if last_payoff >= self.A:
                    play = self.last_action
                else:
                    play = Action.C if self.last_action == Action.D else Action.D
            else:
                play = Action.D
            self.last_action = play
            return play
        deficit = max(0.0, self.gamma - observed_frac)
        raw_T = int(math.ceil(self.s * deficit * (n - 1)))
        T = max(1, raw_T)
        T = min(self.Pmax, T)
        T = min(T, max_punishable_rounds)
        if T <= 1:
            self.in_punishment = False
            self.punishment_left = 0
            if random.random() < self.p_test:
                self.in_test = True
            self.last_action = Action.D
            return Action.D
        else:
            self.in_punishment = True
            self.punishment_left = T - 1
            self.last_action = Action.D
            return Action.D

class Strategy_COOPERATIVE_80(BaseStrategy):
    """
    Threshold Reciprocity with Proportional Punishment and Forgiveness (TR-PPF).

    - Start by cooperating to signal intent.
    - Sustain cooperation when a majority cooperated last round.
    - If cooperation falls below majority, punish by defecting for a short,
      proportional number of rounds (capped by P_max). While punishing,
      if a majority returns to cooperating, forgive immediately.
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.T = math.ceil(self.n / 2)
        self.P_max = min(3, max(1, self.r - 1))
        self.last_action = Action.C
        self.punish_timer = 0
        self.trigger_defectors = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            current_round = 1
        else:
            current_round = history.round_number + 1
        if current_round == self.r:
            action = Action.D
            self.last_action = action
            return action
        if history is not None and self.punish_timer > 0 and (history.round_number >= 1):
            last_my = bool(history.my_actions[-1])
            last_opponents_sum = int(np.sum(history.opponent_actions[-1, :]))
            m_last = int(last_my) + int(last_opponents_sum)
            if m_last >= self.T:
                self.punish_timer = 0
            else:
                self.punish_timer = max(0, self.punish_timer - 1)
        if self.punish_timer > 0:
            action = Action.D
        elif history is None:
            action = Action.C
        else:
            if history.round_number >= 1:
                prev_my = bool(history.my_actions[-1])
                prev_opponents_sum = int(np.sum(history.opponent_actions[-1, :]))
                m_prev = int(prev_my) + int(prev_opponents_sum)
            else:
                m_prev = 0
            if m_prev >= self.T:
                action = Action.C
            else:
                L = min(self.P_max, max(1, math.ceil((self.n - m_prev) / 2)))
                self.punish_timer = L
                self.trigger_defectors = self.n - m_prev
                action = Action.D
        self.last_action = action
        return action

class Strategy_COOPERATIVE_81(BaseStrategy):
    """
    Adaptive conditional cooperator for the N-player Public Goods Game.

    - Cooperates on the first move.
    - Uses a short moving window of recent rounds (H up to 3) to estimate
      the fraction of other players cooperating (avg_coop).
    - If avg_coop >= theta, cooperates.
    - If avg_coop < theta, enters a short proportional punishment (defect for L rounds),
      where L scales with how far avg_coop is below theta, capped by P_max.
    - Forgives immediately if cooperation resumes (i.e., avg_coop >= theta).
    - If punishment is active, will continue to defect unless avg_coop >= theta,
      in which case it resets punishment and cooperates.
    - In the final round (no future), defects by default unless avg_coop >= final_round_confidence.
    - If avg_coop is extremely low for T_long consecutive checks, switches to permanent defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.H_window = 3
        self.theta = 0.6
        self.P_scale = 3
        self.P_max = 5
        self.final_round_confidence = 0.95
        self.T_long = 4
        self.extreme_low_threshold = 0.05
        self.punishment_counter = 0
        self.consecutive_low = 0
        self.long_defect_flag = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            self.punishment_counter = 0
            self.consecutive_low = 0
            self.long_defect_flag = False
            return Action.C
        t = history.round_number + 1
        r = self.game_description.n_rounds
        n = self.game_description.n_players
        num_opponents = max(1, n - 1)
        if self.long_defect_flag:
            return Action.D
        H = min(self.H_window, max(0, t - 1))
        if H == 0:
            avg_coop = 1.0
        else:
            recent_opponents = history.opponent_actions[-H:, :]
            coop_frac_per_round = np.sum(recent_opponents, axis=1) / float(num_opponents)
            avg_coop = float(np.mean(coop_frac_per_round))
        if avg_coop < self.extreme_low_threshold:
            self.consecutive_low += 1
        else:
            self.consecutive_low = 0
        if self.consecutive_low >= self.T_long:
            self.long_defect_flag = True
            return Action.D
        if self.punishment_counter > 0 and avg_coop >= self.theta:
            self.punishment_counter = 0
            return Action.C
        if self.punishment_counter > 0:
            self.punishment_counter = max(0, self.punishment_counter - 1)
            return Action.D
        if t == r:
            if avg_coop >= self.final_round_confidence:
                return Action.C
            else:
                return Action.D
        if avg_coop >= self.theta:
            return Action.C
        else:
            deficit = max(0.0, self.theta - avg_coop)
            raw_L = math.ceil(deficit * self.P_scale * float(num_opponents))
            L = int(min(self.P_max, max(1, raw_L)))
            self.punishment_counter = max(0, L - 1)
            return Action.D

class Strategy_COOPERATIVE_82(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Gradual Punishment and Forgiveness (ACC-GPF)

    - Starts cooperating.
    - Cooperates when recent others' cooperation is above threshold q.
    - Issues short, proportional punishments (defect for L rounds) when others fall below threshold.
    - Escalates punishment length for repeated offenses; forgives after S good rounds.
    - Defects in the final M rounds (endgame safe mode).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.w_default = min(5, max(1, self.r - 1))
        self.q = 0.6
        self.P_base = 1
        self.S = 2
        self.M = 1
        self.pc = 0
        self.escalation = 0
        self.good_run = 0

    def __call__(self, state: GameState, history: None | PlayerHistory):
        if history is None:
            return Action.C
        current_round = history.round_number + 1
        remaining_rounds = max(0, self.r - current_round + 1)
        if remaining_rounds <= self.M:
            return Action.D
        if self.pc > 0:
            self.pc -= 1
            return Action.D
        w = min(self.w_default, history.round_number)
        if w <= 0:
            return Action.C
        m = w
        opp_actions = history.opponent_actions
        recent = opp_actions[-m:, :]
        counts = np.sum(recent.astype(np.int32), axis=1)
        denom = max(1, self.n - 1)
        fractions = counts.astype(float) / float(denom)
        f_hist = float(np.mean(fractions))
        f_last = float(fractions[-1])
        if f_hist >= self.q or f_last >= self.q:
            self.good_run += 1
            S_effective = min(self.S, max(1, self.r - 1))
            if self.good_run >= S_effective:
                self.escalation = 0
            return Action.C
        else:
            s = max(0.0, self.q - f_last)
            L = self.P_base + self.escalation
            severity_add = int(math.ceil((self.n - 1) * s))
            L = L + severity_add
            L = max(1, min(L, remaining_rounds))
            self.pc = max(0, L - 1)
            self.escalation += 1
            self.good_run = 0
            return Action.D

class Strategy_COOPERATIVE_83(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC)

    - Starts by cooperating.
    - Uses a short look-back window (L) to compute the recent fraction of other players
      who cooperated (coop_rate).
    - If coop_rate >= thr_high -> cooperate.
      If coop_rate <= thr_low  -> defect.
      Otherwise probe probabilistically with p = (coop_rate - thr_low)/(thr_high - thr_low).
    - If recently exploited (you played C while others cooperated < thr_low in a recent round),
      punish by defecting until short-lived cooperation is observed again (handled via sliding checks).
    - In the final round, cooperate only if recent rounds show stable cooperation (and no recent exploitation).
    Thresholds scale with k and n to adapt permissiveness/strictness.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        self.L = min(5, max(0, r - 1))
        thr_base = 1.0 - k / (2.0 * n)
        self.thr_high = min(max(thr_base, 0.5), 0.95)
        self.thr_low = min(max(self.thr_high - 0.2, 0.1), 0.5)
        self.forgive_rounds = 1
        self.stable_rounds_for_last = min(3, max(0, r - 1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = int(state.round_number)
        completed = int(history.round_number)
        n = int(self.game_description.n_players)

        def frac_others_at_indices(idxs):
            if len(idxs) == 0:
                return np.array([], dtype=float)
            rows = history.opponent_actions[np.array(idxs, dtype=int), :]
            return np.array(rows.mean(axis=1), dtype=float)
        if completed <= 0:
            return Action.C
        last_k = min(self.L, completed)
        start_idx = completed - last_k
        recent_round_indices = list(range(start_idx, completed))
        recent_fracs = frac_others_at_indices(recent_round_indices)
        coop_rate = float(np.mean(recent_fracs)) if recent_fracs.size > 0 else 0.0
        exploit_window = min(2, completed)
        exploit_start = completed - exploit_window
        exploited_recently = False
        if exploit_window > 0:
            for idx in range(exploit_start, completed):
                my_act = bool(history.my_actions[idx])
                frac_others = float(history.opponent_actions[idx, :].mean()) if history.opponent_actions.size > 0 else 0.0
                if my_act and frac_others < self.thr_low:
                    exploited_recently = True
                    break
        if t == int(self.game_description.n_rounds):
            sr = max(1, self.stable_rounds_for_last) if self.stable_rounds_for_last > 0 else 1
            checks = min(sr, completed)
            stable_ok = True
            for i in range(1, checks + 1):
                idx = completed - i
                frac_others = float(history.opponent_actions[idx, :].mean()) if history.opponent_actions.size > 0 else 0.0
                if frac_others < self.thr_high:
                    stable_ok = False
                    break
            if stable_ok and (not exploited_recently):
                return Action.C
            else:
                return Action.D
        if exploited_recently:
            return Action.D
        if coop_rate >= self.thr_high:
            return Action.C
        if coop_rate <= self.thr_low:
            return Action.D
        denom = self.thr_high - self.thr_low
        if denom <= 0:
            return Action.D
        p = (coop_rate - self.thr_low) / denom
        if random.random() < float(p):
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_84(BaseStrategy):
    """
    Adaptive Forgiving Conditional Cooperator (AFCC)

    - Starts by cooperating.
    - Uses a short recent window to estimate others' cooperation rate.
    - If others have been cooperating reliably (recent_coop_rate >= major_frac) -> cooperate.
    - If a simple majority of others cooperated last round -> cooperate.
    - If there is a sharp drop in cooperation -> enter a proportional short punishment.
    - Punishments are short and proportional; after punishment the strategy probabilistically forgives to resume cooperation.
    - Near the end of a finite game (endgame_rounds) the strategy is cautious and cooperates only if long-run trust is high.
    - Parameters (n, k, r) are used to adjust major_frac and endgame behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_until = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        base_major_frac = 0.6
        raw_major = base_major_frac - 0.15 * (k - 1.0) / max(1, n - 1)
        major_frac = max(0.45, min(0.75, raw_major))
        punishment_base = 2
        forgiveness_prob = 0.8
        W = min(5, r)
        endgame_rounds = min(3, max(1, math.floor(r * 0.08)))
        endgame_trust_threshold = 0.75
        t = int(state.round_number)
        if history is None:
            return Action.C
        completed = history.round_number
        if completed == 0:
            return Action.C
        if t <= self.punish_until:
            return Action.D
        try:
            last_coop_count = int(np.sum(history.opponent_actions[-1, :]))
        except Exception:
            last_coop_count = 0
        window = min(W, completed)
        if window > 0:
            recent_slice = history.opponent_actions[-window:, :]
            counts = np.sum(recent_slice, axis=1).astype(float)
            fracs = counts / float(max(1, n - 1))
            recent_coop_rate = float(np.mean(fracs))
        else:
            recent_coop_rate = 0.0
        in_endgame = t > r - endgame_rounds
        if recent_coop_rate >= major_frac:
            if in_endgame:
                if completed > 0:
                    thr = math.ceil((n - 1) * major_frac)
                    counts_all = np.sum(history.opponent_actions, axis=1).astype(int)
                    trusted_rounds = int(np.sum(counts_all >= thr))
                    long_run_trust = trusted_rounds / float(completed)
                else:
                    long_run_trust = 0.0
                if long_run_trust >= endgame_trust_threshold:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.C
        majority_thresh = math.ceil((n - 1) * 0.5)
        if last_coop_count >= majority_thresh:
            if in_endgame:
                if completed > 0:
                    thr = math.ceil((n - 1) * major_frac)
                    counts_all = np.sum(history.opponent_actions, axis=1).astype(int)
                    trusted_rounds = int(np.sum(counts_all >= thr))
                    long_run_trust = trusted_rounds / float(completed)
                else:
                    long_run_trust = 0.0
                if long_run_trust >= endgame_trust_threshold:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.C
        if completed >= 2:
            prev = int(np.sum(history.opponent_actions[-2, :]))
        else:
            prev = last_coop_count
        drop = max(0, prev - last_coop_count)
        low_expectation_thresh = math.ceil((n - 1) * major_frac)
        if last_coop_count < low_expectation_thresh and drop >= 1:
            punish_length = int(punishment_base + math.ceil(drop / 1.5))
            self.punish_until = min(r, t + punish_length - 1)
            return Action.D
        if in_endgame:
            if completed > 0:
                thr = math.ceil((n - 1) * major_frac)
                counts_all = np.sum(history.opponent_actions, axis=1).astype(int)
                trusted_rounds = int(np.sum(counts_all >= thr))
                long_run_trust = trusted_rounds / float(completed)
            else:
                long_run_trust = 0.0
            if long_run_trust >= endgame_trust_threshold:
                return Action.C
            else:
                return Action.D
        if recent_coop_rate >= 0.3:
            if random.random() < forgiveness_prob:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COOPERATIVE_85(BaseStrategy):
    """
    Generous Conditional Cooperator (GCC)

    - Starts cooperating.
    - Cooperates in intermediate rounds if at least half of the other players
      cooperated in the previous round (T = 0.5).
    - If that threshold is not met, punishes by defecting for a short,
      proportional number of rounds (punishment length in [P_min, P_max]).
    - While punishing, forgives immediately if the previous round shows
      sufficient cooperation (f_prev >= T).
    - In the final round, cooperates only if the average fraction of other
      cooperators over the last L = min(5, r-1) rounds >= 0.8; otherwise defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(self.game_description.n_players)
        self.r = int(self.game_description.n_rounds)
        self.T = 0.5
        self.P_min = 1
        self.P_max = min(3, max(1, math.floor(self.r / 4)))
        self.pun_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or getattr(history, 'round_number', 0) == 0:
            return Action.C
        current_round = int(state.round_number)
        other_count = max(1, self.n - 1)

        def prev_other_frac() -> float:
            prev_row = history.opponent_actions[-1, :]
            return float(np.sum(prev_row)) / float(other_count)
        if self.r > 1 and current_round == self.r:
            L = min(5, self.r - 1)
            recent_opponents = history.opponent_actions[-L:, :]
            recent_sum = float(np.sum(recent_opponents))
            recent_coop_fraction = recent_sum / float(L * other_count)
            if recent_coop_fraction >= 0.8:
                return Action.C
            else:
                return Action.D
        f_prev = prev_other_frac()
        if self.pun_counter > 0:
            if f_prev >= self.T:
                self.pun_counter = 0
                return Action.C
            else:
                action = Action.D
                self.pun_counter = max(0, self.pun_counter - 1)
                return action
        elif f_prev >= self.T:
            return Action.C
        else:
            severity = (self.T - f_prev) * float(other_count)
            pun_length = int(round(severity))
            pun_length = max(self.P_min, pun_length)
            pun_length = min(self.P_max, pun_length)
            self.pun_counter = pun_length
            action = Action.D
            self.pun_counter = max(0, self.pun_counter - 1)
            return action

class Strategy_COOPERATIVE_86(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Forgiveness (ACCF)

    - Cooperate on the first round (signal).
    - Always defect on the final round.
    - Condition on recent fraction of other players who cooperated (window w).
      - If fraction >= theta_high: cooperate.
      - If fraction <= theta_low: defect.
      - If in between: cooperate probabilistically with p = max(p_min, min(1, g * f)).
    - If exploited (cooperated and would have been strictly better off defecting, and others were sufficiently defecting),
      initiate a limited punishment of length L (set punish_until = t_exploit + L).
    - Punishment means defecting while t <= punish_until.
    - Forgive early if others respond (observe high cooperation during/after punishment).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        w_default = max(1, math.floor(self.r / 10))
        self.w = min(3, w_default)
        self.L = 2
        self.theta_high = 0.6
        self.theta_low = 0.3
        self.theta_low_exploit = self.theta_low
        self.g = 1.1
        self.p_min = 0.05
        self.eps = 1e-09
        self.punish_until = 0
        self._last_processed_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        completed = history.round_number
        for completed_round_count in range(self._last_processed_rounds, completed):
            idx = completed_round_count
            t_just_completed = idx + 1
            opp_actions_row = history.opponent_actions[idx, :]
            sum_opponents = int(np.sum(opp_actions_row))
            my_cooperated = bool(history.my_actions[idx])
            coop_count = (1 if my_cooperated else 0) + sum_opponents
            if self.n - 1 > 0:
                f_t = sum_opponents / float(self.n - 1)
            else:
                f_t = 0.0
            my_payoff = float(history.my_payoffs[idx])
            payoff_if_defected = 1.0 + self.k / float(self.n) * float(sum_opponents)
            group_payoffs = np.concatenate(([float(history.my_payoffs[idx])], history.opponent_payoffs[idx, :].astype(np.float64)))
            group_median = float(np.median(group_payoffs))
            exploited = False
            if my_cooperated:
                payoff_condition = my_payoff + self.eps < payoff_if_defected
                median_condition = my_payoff + self.eps < group_median
                if (payoff_condition or median_condition) and f_t <= self.theta_low_exploit + self.eps:
                    exploited = True
            if exploited:
                self.punish_until = t_just_completed + self.L
            if self.punish_until >= t_just_completed:
                if f_t >= self.theta_high - self.eps:
                    self.punish_until = t_just_completed
        self._last_processed_rounds = completed
        t = completed + 1
        if t >= self.r:
            return Action.D
        if t <= self.punish_until:
            return Action.D
        if completed == 0:
            return Action.C
        window_size = min(self.w, completed)
        opp_recent = history.opponent_actions[completed - window_size:completed, :]
        if self.n - 1 > 0:
            frac_per_round = np.sum(opp_recent, axis=1) / float(self.n - 1)
            f = float(np.mean(frac_per_round))
        else:
            f = 0.0
        if f >= self.theta_high - self.eps:
            return Action.C
        if f <= self.theta_low + self.eps:
            return Action.D
        p = max(self.p_min, min(1.0, self.g * f))
        if random.random() < p:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_87(BaseStrategy):
    """
    Adaptive Majority-Conditional Cooperator with Graduated Punishment (AMCGP).

    - Starts by cooperating.
    - Cooperates when a majority (fraction >= p*) cooperated in the previous round.
    - Forgives up to `forgiveness_slips` below-threshold rounds.
    - If forgiveness exhausted, enters a finite punishment of length up to `max_punish`.
    - While punishing, defects. If cooperation returns (fraction >= p*) for two consecutive observed rounds
      during punishment, punishment ends early (recovery).
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.pstar = 0.5
        self.forgiveness_slips = 1
        r = int(self.game_description.n_rounds)
        self.max_punish = min(3, max(1, math.floor(r / 6)))
        self.punish_remaining = 0
        self.slips_used = 0
        self._last_observed_rounds = 0
        self._recent_fraction_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        n = int(self.game_description.n_players)
        total_rounds = int(self.game_description.n_rounds)
        completed = history.round_number
        current_round = completed + 1
        for idx in range(self._last_observed_rounds, completed):
            my_coop = bool(history.my_actions[idx])
            opp_coops = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size > 0 else 0
            total_coops = opp_coops + (1 if my_coop else 0)
            frac = float(total_coops) / float(n)
            self._recent_fraction_history.append(frac)
            if self.punish_remaining > 0:
                if len(self._recent_fraction_history) >= 2:
                    last_two = self._recent_fraction_history[-2:]
                    if last_two[0] >= self.pstar and last_two[1] >= self.pstar:
                        self.punish_remaining = 0
                        self.slips_used = 0
                        self._last_observed_rounds = idx + 1
                        continue
                self.punish_remaining = max(0, self.punish_remaining - 1)
            elif frac >= self.pstar:
                self.slips_used = 0
            elif self.slips_used < self.forgiveness_slips:
                self.slips_used += 1
            else:
                t = idx + 1
                remaining_after_t = max(0, total_rounds - t)
                new_pun = min(self.max_punish, remaining_after_t)
                self.punish_remaining = new_pun
                self.punish_remaining = max(0, self.punish_remaining - 1)
            self._last_observed_rounds = idx + 1
        if current_round >= total_rounds:
            return Action.D
        if self.punish_remaining > 0:
            return Action.D
        if completed == 0:
            return Action.C
        prev_idx = completed - 1
        prev_my = bool(history.my_actions[prev_idx])
        prev_opp_coops = int(np.sum(history.opponent_actions[prev_idx, :])) if history.opponent_actions.size > 0 else 0
        prev_total_coops = prev_opp_coops + (1 if prev_my else 0)
        prev_frac = float(prev_total_coops) / float(n)
        if prev_frac >= self.pstar:
            self.slips_used = 0
            return Action.C
        elif self.slips_used < self.forgiveness_slips:
            self.slips_used += 1
            return Action.C
        else:
            t = current_round
            remaining_after_t = max(0, total_rounds - t)
            self.punish_remaining = min(self.max_punish, remaining_after_t)
            return Action.D

class Strategy_COOPERATIVE_88(BaseStrategy):
    """
    Adaptive Generous Trigger (AGT) for N-player public goods game.

    - Signals cooperation on the first round.
    - Defects on the last round.
    - Monitors recent opponents' cooperation (window w).
    - If recent cooperation >= threshold tau -> cooperate and progress reconciliation counter.
    - If recent cooperation < tau -> defect and trigger a short punishment whose length scales
      with how far recent cooperation is below tau (but is bounded).
    - Punishments are short and reconciliations require S_req consecutive good rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, self.r)
        self.g = (self.k - 1.0) / max(1, self.n - 1)
        tau_raw = 0.5 - 0.2 * self.g
        self.tau = max(0.2, min(0.6, tau_raw))
        self.P_max = min(4, max(1, math.floor(self.r / 10)))
        self.S_req = min(3, self.r)
        self.PunishCounter = 0
        self.InReconciliation = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.PunishCounter = 0
            self.InReconciliation = 0
            return Action.C
        t = history.round_number + 1
        if t == self.r:
            return Action.D
        if self.PunishCounter > 0:
            self.PunishCounter = max(0, self.PunishCounter - 1)
            self.InReconciliation = 0
            return Action.D
        num_completed = history.round_number
        window_size = min(self.w, num_completed)
        if window_size <= 0:
            recent_coop_rate = 1.0
        else:
            recent_actions = history.opponent_actions[-window_size:, :]
            per_round_rates = np.sum(recent_actions.astype(float), axis=1) / float(max(1, self.n - 1))
            recent_coop_rate = float(np.mean(per_round_rates))
        if recent_coop_rate >= self.tau:
            self.InReconciliation = min(self.S_req, self.InReconciliation + 1)
            return Action.C
        else:
            severity = self.tau - recent_coop_rate
            rawP = 1 + math.floor(severity / self.tau * self.P_max)
            P = max(1, min(self.P_max, int(rawP)))
            self.PunishCounter = max(0, P - 1)
            self.InReconciliation = 0
            return Action.D

class Strategy_COOPERATIVE_89(BaseStrategy):
    """
    Forgiving Threshold Conditional Cooperator (FTCC)

    - Starts by cooperating.
    - Uses an adaptive cooperation threshold alpha = 1 - 0.5*(k/n).
    - Forgives small slips (delta = 0.15).
    - If the previous-round cooperation fraction falls significantly below alpha,
      starts a short proportional punishment of length P (bounded by P_max and remaining rounds).
    - Punishment state is tracked via self.punish_remaining.
    - Always defects in the final round (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = float(self.game_description.n_players)
        k = float(self.game_description.k)
        r = int(self.game_description.n_rounds)
        self.alpha = 1.0 - 0.5 * (k / n)
        self.delta = 0.15
        self.P_max = min(5, max(1, math.floor(r / 4)))
        self.punish_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        r = int(self.game_description.n_rounds)
        n = int(self.game_description.n_players)
        if history is None:
            return Action.C
        t = history.round_number + 1
        if t == r:
            return Action.D
        if history.round_number == 0:
            return Action.C
        last_opponents = history.opponent_actions[-1, :]
        last_my = history.my_actions[-1]
        coop_count = int(np.sum(last_opponents)) + (1 if bool(last_my) else 0)
        f_prev = float(coop_count) / float(n)
        if self.punish_remaining > 0:
            if f_prev >= self.alpha:
                self.punish_remaining = 0
                return Action.C
            action = Action.D
            self.punish_remaining = max(0, self.punish_remaining - 1)
            return action
        if f_prev >= self.alpha:
            return Action.C
        if f_prev >= self.alpha - self.delta:
            return Action.C
        shortfall = self.alpha - f_prev
        P = max(1, math.ceil(shortfall * float(r)))
        P = min(self.P_max, P)
        rem = r - t + 1
        allowed_P = max(1, min(P, rem - 1))
        P = allowed_P
        self.punish_remaining = max(0, P - 1)
        return Action.D

class Strategy_COOPERATIVE_90(BaseStrategy):
    """
    Adaptive Forgiving Majority (AFM)

    - Start by cooperating.
    - Defect on the final round.
    - In intermediate rounds, use a short recent window W to estimate the fraction
      of opponent cooperation. Cooperate if recent fraction >= threshold theta.
    - Be probabilistically forgiving (g) when others are not clearly cooperating,
      with g increasing with public-good multiplier k and further increased if the
      overall historical cooperation rate is high.
    - Punish for at most P_max consecutive rounds, then attempt forgiveness.
    - Reward observed improvements in opponent cooperation by cooperating.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.W_default = 5
        self.theta_default = 0.5
        self.P_max = 2
        self.g_min = 0.05
        self.g_max = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = getattr(state, 'round_number', None)
        if t is None:
            t = history.round_number + 1
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        if t == r:
            return Action.D
        W = min(self.W_default, max(1, t - 1))
        rounds_completed = history.round_number
        start_last = max(0, rounds_completed - W)
        end_last = rounds_completed
        if rounds_completed == 0:
            coop_count = 0
        else:
            recent_opponents = history.opponent_actions[start_last:end_last, :]
            coop_count = int(np.sum(recent_opponents))
        denom = float(W * (n - 1))
        frac = float(coop_count) / denom if denom > 0 else 0.0
        if rounds_completed == 0:
            hist_coop_rate = 0.0
        else:
            total_my = int(np.sum(history.my_actions))
            total_opponents = int(np.sum(history.opponent_actions))
            total_coop_all = total_my + total_opponents
            total_actions_all = float(rounds_completed * n)
            hist_coop_rate = float(total_coop_all) / total_actions_all if total_actions_all > 0 else 0.0
        theta = self.theta_default
        if hist_coop_rate >= 0.75:
            theta = 0.4
        raw_g = (k - 1.0) / k if k > 0 else self.g_min
        g = max(self.g_min, min(raw_g, self.g_max))
        if hist_coop_rate >= 0.75:
            g = min(self.g_max, g + 0.1)
        my_consec_D = 0
        if rounds_completed > 0:
            for a in history.my_actions[::-1]:
                if not bool(a):
                    my_consec_D += 1
                else:
                    break
        improvement = False
        if rounds_completed >= 2 * W:
            start_prev = rounds_completed - 2 * W
            end_prev = rounds_completed - W
            prev_window = history.opponent_actions[start_prev:end_prev, :]
            coop_prev_window = int(np.sum(prev_window))
            frac_prev = float(coop_prev_window) / denom if denom > 0 else 0.0
            if frac > frac_prev:
                improvement = True
        if frac >= theta:
            return Action.C
        if improvement:
            return Action.C
        if my_consec_D >= self.P_max:
            if random.random() < g:
                return Action.C
            else:
                return Action.D
        if random.random() < g:
            return Action.C
        return Action.D

class Strategy_COOPERATIVE_91(BaseStrategy):
    """
    Conditional Cooperate with Proportional Punishment (CCPP)

    - Start cooperating to signal willingness to cooperate.
    - If a clear majority cooperated in the previous round, cooperate.
    - If cooperation fell below threshold, defect and impose a punishment whose
      length scales with the fraction of defectors (proportional punishment).
    - After a punishment finishes, check recent rounds (forgiveness window). If
      recent cooperation meets the threshold, resume cooperation; otherwise,
      start another proportional punishment.
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_threshold = 0.5
        self.Lmin = 1
        self.Lmax = 3
        self.F = 3
        self.punishment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        t = state.round_number
        if history is None or history.round_number == 0:
            self.punishment_counter = 0
            return Action.C
        if t == r:
            return Action.D

        def cooperators_in_round(idx: int) -> int:
            my_coop = 1 if bool(history.my_actions[idx]) else 0
            opp_coops = int(np.sum(history.opponent_actions[idx, :]))
            return my_coop + opp_coops
        if self.punishment_counter > 0:
            action = Action.D
            self.punishment_counter -= 1
            if self.punishment_counter == 0:
                rounds_available = history.round_number
                start_idx = max(0, rounds_available - self.F)
                if start_idx < rounds_available:
                    fracs = []
                    for idx in range(start_idx, rounds_available):
                        coop = cooperators_in_round(idx)
                        fracs.append(coop / float(n))
                    avg_frac = float(np.mean(np.array(fracs))) if fracs else 0.0
                else:
                    avg_frac = 0.0
                if avg_frac < self.p_threshold:
                    last_idx = rounds_available - 1
                    coop_last = cooperators_in_round(last_idx)
                    frac = coop_last / float(n)
                    fraction_defectors = 1.0 - frac
                    L = int(math.ceil(self.Lmin + fraction_defectors * (self.Lmax - self.Lmin)))
                    remaining_rounds = r - t
                    L = min(L, remaining_rounds)
                    if L > 0:
                        self.punishment_counter = L
            return action
        coop_last = cooperators_in_round(history.round_number - 1)
        frac = coop_last / float(n)
        if frac >= self.p_threshold:
            return Action.C
        else:
            fraction_defectors = 1.0 - frac
            L = int(math.ceil(self.Lmin + fraction_defectors * (self.Lmax - self.Lmin)))
            remaining_rounds = r - t
            L = min(L, remaining_rounds)
            if L > 0:
                self.punishment_counter = L
            return Action.D

class Strategy_COOPERATIVE_92(BaseStrategy):
    """
    Reciprocal Forgiver (RF) for N-Player Public Goods Game.

    - Starts by cooperating.
    - Tracks recent cooperation (window H) and labels players as reliable if their
      cooperation rate >= rho.
    - Cooperates when a strong majority is reliably cooperating or when there was
      near-universal cooperation in the previous round.
    - Triggers short, proportional punishments against clear breakdowns or
      persistent defectors. After punishment, performs a single cooperative probe
      round to test reconciliation. If probe is exploited, escalates punishment.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_timer = 0
        self.awaiting_probe = False
        self.just_probed = False
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        self.n = n
        self.r = r
        self.H = max(2, min(6, math.floor(r / 4)))
        self.rho = 0.7
        self.Q_coop = math.ceil(0.75 * n)
        self.Q_min = math.ceil(0.5 * n)
        self.P_base = max(1, math.floor(r / 10))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punishment_timer = 0
            self.awaiting_probe = False
            self.just_probed = False
            return Action.C
        t = history.round_number + 1
        n = self.n
        r = self.r
        if t == r:
            self.just_probed = False
            self.awaiting_probe = False
            return Action.D
        remaining = r - t + 1
        if self.punishment_timer > 0:
            action = Action.D
            self.punishment_timer -= 1
            if self.punishment_timer == 0:
                self.awaiting_probe = True
            self.just_probed = False
            return action
        if self.awaiting_probe:
            self.awaiting_probe = False
            self.just_probed = True
            return Action.C
        window = min(self.H, max(1, t - 1))
        my_recent = history.my_actions[-window:]
        my_C = int(np.sum(my_recent))
        opp_recent = history.opponent_actions[-window:, :]
        if opp_recent.size == 0:
            opp_C = np.zeros((n - 1,), dtype=int)
        else:
            opp_C = np.sum(opp_recent, axis=0).astype(int)
        C_arr = np.concatenate(([my_C], opp_C))
        reliability = C_arr / float(window)
        R = int(np.sum(reliability >= self.rho))
        prev_self = bool(history.my_actions[-1])
        prev_opps = history.opponent_actions[-1, :]
        total_cooperators_prev = int(prev_self) + int(np.sum(prev_opps))
        if total_cooperators_prev >= n - 1:
            self.just_probed = False
            return Action.C
        if R >= self.Q_coop:
            self.just_probed = False
            return Action.C
        if R < self.Q_min:
            opp_defect_counts = window - opp_C
            persistent_defectors = int(np.sum(opp_defect_counts >= 2))
            severity = max(1, persistent_defectors)
            P = min(max(0, remaining - 1), self.P_base * severity)
            if P <= 0:
                self.just_probed = False
                return Action.D
            else:
                self.punishment_timer = P - 1
                if self.punishment_timer == 0:
                    self.awaiting_probe = True
                self.just_probed = False
                return Action.D
        if t >= 3:
            prev2_self = bool(history.my_actions[-2])
            prev2_opps = history.opponent_actions[-2, :]
            total_cooperators_prev2 = int(prev2_self) + int(np.sum(prev2_opps))
            recent_trend = total_cooperators_prev > total_cooperators_prev2
        else:
            recent_trend = True
        if self.just_probed:
            if total_cooperators_prev >= self.Q_min or R >= self.Q_coop:
                self.just_probed = False
                return Action.C
            else:
                opp_defect_counts = window - opp_C
                persistent_defectors = int(np.sum(opp_defect_counts >= 2))
                severity = max(1, persistent_defectors)
                P = min(max(0, remaining - 1), 2 * self.P_base * severity)
                self.just_probed = False
                if P <= 0:
                    return Action.D
                else:
                    self.punishment_timer = P - 1
                    if self.punishment_timer == 0:
                        self.awaiting_probe = True
                    return Action.D
        if recent_trend:
            self.just_probed = False
            return Action.C
        self.just_probed = False
        return Action.D

class Strategy_COOPERATIVE_93(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Proportional Punishment (ACPP).

    - Opens with cooperation (signal).
    - Rewards rounds where group cooperation >= required_cooperators.
    - If group cooperation falls below threshold, starts a proportional, limited punishment.
    - Forgives immediately if the group shows recovered cooperation.
    - Anti-sucker rule: if you were the sole cooperator for two consecutive rounds, switch
      to persistent defection until a recovery signal (at least required_cooperators) is observed.
    - Defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, max(1, math.floor(self.r / 10)))
        self.T = 0.6
        self.required_cooperators = math.ceil(self.T * self.n)
        self.P_max = min(3, max(1, math.floor(self.r / 6)))
        self.punish_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t == self.r:
            return Action.D
        if history is None:
            return Action.C

        def cooperators_in_round(idx: int) -> int:
            my_a = bool(history.my_actions[idx])
            opps = history.opponent_actions[idx, :]
            opp_coops = int(np.sum(opps))
            return int(my_a) + opp_coops
        s_last = cooperators_in_round(-1)
        if self.punish_counter > 0:
            if s_last >= self.required_cooperators:
                self.punish_counter = 0
                return Action.C
            self.punish_counter = max(0, self.punish_counter - 1)
            return Action.D
        if s_last >= self.required_cooperators:
            return Action.C
        shortfall = max(0, self.required_cooperators - s_last)
        self.punish_counter = min(self.P_max, 1 + shortfall)
        action_to_return = Action.D
        if history.round_number >= 2:
            s_prev = cooperators_in_round(-2)
            my_last = bool(history.my_actions[-1])
            my_prev = bool(history.my_actions[-2])
            if s_last == 1 and s_prev == 1 and my_last and my_prev:
                remaining_rounds = max(0, self.r - t)
                self.punish_counter = remaining_rounds
        return action_to_return

class Strategy_COOPERATIVE_94(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC)

    - Starts by cooperating.
    - Uses short-term and longer suspicion windows to estimate others' cooperation.
    - Cooperates when recent others' cooperation exceeds an adaptive threshold.
    - Triggers proportional short punishments when cooperations are betrayed.
    - Forgives early if group cooperation recovers for a short run.
    - Enters a safe (mostly defecting) mode when the population is persistently uncooperative.
    - Occasionally probes with small probability epsilon to detect recovery.
    - Always defects in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.M = min(5, self.r)
        self.suspicion_window = min(10, self.r)
        self.L_base = 2
        self.eps = min(0.05, 1.0 / max(1, self.r))
        raw = 0.5 - 0.15 * (self.k - 1.0) / max(1.0, self.n - 1.0)
        self.c_threshold = max(0.25, min(0.75, raw))
        self.suspicion_threshold = 0.2
        self.punish_until = 0
        self.recover_window = 2
        self._recover_count = 0
        self.suspect = np.zeros(max(0, self.n - 1), dtype=float)

    def __call__(self, state: GameState, history: None | PlayerHistory):
        if history is None or history.round_number == 0:
            return Action.C
        t = history.round_number + 1
        if t > self.r:
            t = self.r
        last_idx = max(0, history.round_number - 1)
        if t == self.r:
            return Action.D
        n_opp = self.n - 1
        use_M = min(self.M, history.round_number)
        if use_M <= 0:
            P_others_M = 0.0
        else:
            opp_slice = history.opponent_actions[-use_M:, :]
            if n_opp > 0:
                per_round = opp_slice.astype(float).sum(axis=1) / float(n_opp)
                P_others_M = float(np.mean(per_round))
            else:
                P_others_M = 0.0
        use_S = min(self.suspicion_window, history.round_number)
        if use_S <= 0:
            P_others_sus = 0.0
        else:
            opp_slice_s = history.opponent_actions[-use_S:, :]
            if n_opp > 0:
                per_round_s = opp_slice_s.astype(float).sum(axis=1) / float(n_opp)
                P_others_sus = float(np.mean(per_round_s))
            else:
                P_others_sus = 0.0
        last_opp_sum = int(history.opponent_actions[last_idx, :].astype(int).sum()) if n_opp > 0 else 0
        my_last = bool(history.my_actions[last_idx])
        totalC_last = last_opp_sum + (1 if my_last else 0)
        if n_opp > 0:
            for j in range(n_opp):
                opp_defected = not bool(history.opponent_actions[last_idx, j])
                if my_last and opp_defected:
                    self.suspect[j] += 1.0
                else:
                    self.suspect[j] = max(0.0, self.suspect[j] - 0.5)
        if P_others_sus < self.suspicion_threshold:
            if random.random() < self.eps:
                return Action.C
            return Action.D
        if t <= self.punish_until:
            if P_others_M >= self.c_threshold:
                self._recover_count += 1
            else:
                self._recover_count = 0
            if self._recover_count >= self.recover_window:
                self.punish_until = 0
                self._recover_count = 0
            else:
                return Action.D
        if my_last and totalC_last < self.n:
            severity = float(self.n - totalC_last) / float(self.n)
            L = int(round((self.n - 1) * severity / 3.0)) + self.L_base
            self.punish_until = min(self.r - 1, t - 1 + max(1, L))
            return Action.D
        if P_others_M >= self.c_threshold:
            return Action.C
        else:
            if random.random() < self.eps:
                return Action.C
            return Action.D

class Strategy_COOPERATIVE_95(BaseStrategy):
    """
    Conditional Limited-Punishment with Forgiveness (CLPF)

    - Starts by cooperating.
    - Always defects in the final round.
    - Tracks recent defections of each opponent over a sliding window W.
    - Marks opponents persistent if their defections in W exceed a threshold.
    - Gives persistent players limited, proportional punishment until they
      show consecutive reforms.
    - Forgives single, isolated defections probabilistically (generosity g).
    - Uses bounded punishments (max_punish) for group/mass defections.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, max(1, self.r - 1))
        self.persistent_threshold = math.ceil(self.W / 2)
        self.consecutive_reform = 2
        self.max_punish = max(1, math.floor(self.r / 4))
        raw_g = (self.k - 1.0) / max(1.0, self.n - 1.0)
        self.g = min(max(raw_g, 0.1), 0.5)
        self.persistent_set = set()
        self.punish_until = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            return Action.C
        if t == self.r:
            return Action.D
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        rounds_completed = int(history.round_number)
        window_start = max(0, rounds_completed - self.W)
        window_end = rounds_completed
        n_opponents = self.n - 1
        recent_defects = [0] * n_opponents
        if rounds_completed > 0:
            window_slice = opp_actions[window_start:window_end, :] if window_end > window_start else np.empty((0, n_opponents), dtype=bool)
            window_len = window_slice.shape[0]
            if window_len > 0:
                coop_counts = np.sum(window_slice, axis=0)
                for j in range(n_opponents):
                    recent_defects[j] = int(window_len - int(coop_counts[j]))
            else:
                recent_defects = [0] * n_opponents
        for j in range(n_opponents):
            if recent_defects[j] > self.persistent_threshold:
                self.persistent_set.add(j)
        if self.consecutive_reform > 0 and rounds_completed >= self.consecutive_reform:
            reform_start = rounds_completed - self.consecutive_reform
            reform_slice = opp_actions[reform_start:rounds_completed, :]
            to_remove = []
            for j in self.persistent_set:
                if reform_slice.shape[0] == self.consecutive_reform and np.all(reform_slice[:, j]):
                    to_remove.append(j)
            for j in to_remove:
                self.persistent_set.discard(j)
        if self.punish_until >= t:
            return Action.D
        if len(self.persistent_set) > 0:
            punish_length = min(max(1, 1 + len(self.persistent_set)), self.max_punish)
            self.punish_until = t + punish_length - 1
            return Action.D
        if rounds_completed == 0:
            return Action.C
        last_round_idx = rounds_completed - 1
        opp_coops_last = int(np.sum(opp_actions[last_round_idx, :])) if n_opponents > 0 else 0
        my_coop_last = 1 if bool(my_actions[last_round_idx]) else 0
        m_last = opp_coops_last + my_coop_last
        if m_last == self.n:
            return Action.C
        if m_last >= self.n - 1:
            if random.random() < self.g:
                return Action.C
            else:
                self.punish_until = t + 1
                return Action.D
        punish_length = min(max(1, 1 + (self.n - m_last)), self.max_punish)
        self.punish_until = t + punish_length - 1
        return Action.D

class Strategy_COOPERATIVE_96(BaseStrategy):
    """
    C3IF — Collective Conditional Cooperation with Individual Forgiveness

    - Starts by cooperating.
    - Uses a memory window of recent rounds to compute per-opponent trust.
    - Uses the previous round's cooperation rate among opponents plus average trust
      to decide whether to cooperate.
    - If cooperation breaks down, initiates a short, proportional punishment (group
      defection) capped by Pmax, but forgives early if high cooperation returns for
      F consecutive rounds.
    - Defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_remaining = 0
        self.H = 0.7
        self.L = 0.4
        self.T_trust = 0.6
        self.F = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punishment_remaining = 0
            return Action.C
        t = history.round_number + 1
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        if t == r:
            return Action.D
        M = min(10, max(1, r - 1))
        Pmax = min(3, max(1, math.floor(r / 10)))
        observed_rounds = history.round_number
        recent_coop_rate_prev = 0.0
        if observed_rounds >= 1:
            coop_count_last = float(np.sum(history.opponent_actions[-1, :]))
            recent_coop_rate_prev = coop_count_last / float(max(1, n - 1))
        else:
            recent_coop_rate_prev = 0.0
        num_for_trust = min(M, observed_rounds)
        if num_for_trust >= 1:
            window = history.opponent_actions[-num_for_trust:, :]
            trusts = np.sum(window, axis=0).astype(float) / float(num_for_trust)
            avg_trust = float(np.mean(trusts))
        else:
            trusts = np.zeros((n - 1,), dtype=float)
            avg_trust = 0.0
        if self.punishment_remaining > 0 and observed_rounds >= self.F:
            last_F = history.opponent_actions[-self.F:, :]
            coop_rates = np.sum(last_F, axis=1) / float(max(1, n - 1))
            if bool(np.all(coop_rates >= self.H)):
                self.punishment_remaining = 0
        if self.punishment_remaining > 0:
            action = Action.D
            self.punishment_remaining = max(0, self.punishment_remaining - 1)
            return action
        if recent_coop_rate_prev >= self.H:
            return Action.C
        elif recent_coop_rate_prev >= self.L and avg_trust >= self.T_trust:
            return Action.C
        else:
            budget = min(Pmax, max(0, r - t))
            self.punishment_remaining = budget
            action = Action.D
            self.punishment_remaining = max(0, self.punishment_remaining - 1)
            return action

class Strategy_COOPERATIVE_97(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Proportional Punishment (ACCP).

    - Starts by cooperating.
    - Monitors a short window of recent rounds (W up to 5).
    - If recent cooperation >= p_good -> cooperate.
    - If p_tolerate <= recent cooperation < p_good -> single-round retaliation (defect once).
    - If recent cooperation < p_tolerate -> apply a proportional multi-round punishment (defect for P rounds),
      where P scales with severity and remaining horizon. Punishments are carried out fully and do not
      terminate early if others briefly cooperate.
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.W = min(5, max(0, self.r - 1))
        self.p_good = 0.6
        self.p_tolerate = 0.4
        self.P_unit = 1
        self.punishment_remaining = 0
        self.last_punish_end_idx = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        r = self.r
        n = self.n
        if t == r:
            return Action.D
        if self.punishment_remaining > 0:
            self.punishment_remaining -= 1
            return Action.D
        if history is None:
            return Action.C
        completed_rounds = history.round_number
        earliest_allowed_idx = self.last_punish_end_idx + 1
        eligible_count = max(0, completed_rounds - earliest_allowed_idx)
        W_actual = min(self.W, eligible_count)
        if W_actual <= 0:
            return Action.C
        window_end_idx = completed_rounds
        window_start_idx = window_end_idx - W_actual
        if window_start_idx < earliest_allowed_idx:
            window_start_idx = earliest_allowed_idx
            W_actual = window_end_idx - window_start_idx
            if W_actual <= 0:
                return Action.C
        opp_window = history.opponent_actions[window_start_idx:window_end_idx, :]
        my_window = history.my_actions[window_start_idx:window_end_idx]
        opp_counts = np.sum(opp_window, axis=1)
        my_counts = my_window.astype(int)
        cooperators_per_round = opp_counts + my_counts
        f_s = cooperators_per_round / float(n)
        F = float(np.mean(f_s))
        if F >= self.p_good:
            return Action.C
        if F >= self.p_tolerate:
            return Action.D
        S = self.p_tolerate - F
        scaled = math.ceil(S / self.p_tolerate * (self.r / 4.0))
        P = min(r - t, self.P_unit + scaled)
        self.punishment_remaining = max(0, P - 1)
        last_punished_round_idx = t + P - 1 - 1
        self.last_punish_end_idx = last_punished_round_idx
        return Action.D

class Strategy_COOPERATIVE_98(BaseStrategy):
    """
    Firm-but-Fair Conditional Cooperator (FFCC)

    - Starts by cooperating to build cooperation.
    - If any defection is observed in the previous round, punish by defecting for P rounds (including the round
      the punishment starts), where P = max(1, ceil(G / B)), G = 1 - k/n, B = k - 1.
    - Punishment is finite and forgiving: after P rounds of defecting it returns to cooperation unless another
      defection is observed.
    - In the final L = P rounds (the endgame) it defects every round because future punishment cannot deter
      profitable one-shot deviations.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        G = 1.0 - self.k / float(self.n)
        B = max(1e-12, self.k - 1.0)
        self.P = max(1, int(math.ceil(G / B))) if G > 0 else 1
        self.L = self.P
        self.punish_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punish_timer = 0
            t = 1
        else:
            t = history.round_number + 1
        if self.r <= self.P:
            return Action.D
        if t > self.r - self.L:
            return Action.D
        if self.punish_timer > 0:
            self.punish_timer -= 1
            return Action.D
        if t == 1:
            return Action.C
        my_last = bool(history.my_actions[-1])
        opp_last = history.opponent_actions[-1, :]
        all_opponents_cooperated = bool(np.all(opp_last)) if opp_last.size > 0 else True
        if my_last and all_opponents_cooperated:
            return Action.C
        else:
            self.punish_timer = max(0, self.P - 1)
            return Action.D

class Strategy_COOPERATIVE_99(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC)

    - Starts by cooperating to signal intent.
    - Uses a short look-back window (W = min(3, r)) to gauge recent cooperation among all players.
    - Uses long-run cooperation of others to detect persistent defectors; if low, switch to permanent defection.
    - If recent cooperation falls below a dynamic threshold f_thresh (depends on k and n),
      punish by defecting (punishment_length = 1) but forgive quickly if cooperation returns.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_counter = 0
        self.permanent_defect = False
        self.f_long_min = 0.2
        self.punishment_length = 1
        n = getattr(self.game_description, 'n_players', 2)
        k = getattr(self.game_description, 'k', 1.0)
        r = getattr(self.game_description, 'n_rounds', 1)
        self.W = min(3, r)
        f_base = 0.5
        adjustment = 0.25 * (k - 1) / max(1, n - 1)
        f_thresh = f_base - adjustment
        if f_thresh < 0.25:
            f_thresh = 0.25
        elif f_thresh > 0.75:
            f_thresh = 0.75
        self.f_thresh = float(f_thresh)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        completed = int(history.round_number)
        t = completed + 1
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        if t == r:
            return Action.D
        if self.permanent_defect:
            return Action.D
        W = self.W
        start = max(1, t - W)
        rounds_count = t - start
        idx_start = start - 1
        idx_end = t - 2
        total_coop_recent = 0.0
        if rounds_count > 0:
            my_slice = history.my_actions[idx_start:idx_end + 1]
            total_coop_recent += float(np.sum(my_slice))
            opp_slice = history.opponent_actions[idx_start:idx_end + 1, :]
            total_coop_recent += float(np.sum(opp_slice))
            frac_recent = total_coop_recent / (n * rounds_count)
        else:
            frac_recent = 0.0
        if completed > 0:
            total_coop_others = float(np.sum(history.opponent_actions[:completed, :]))
            frac_long_others = total_coop_others / ((n - 1) * completed)
        else:
            frac_long_others = 0.0
        if completed > 0 and frac_long_others < self.f_long_min:
            self.permanent_defect = True
            return Action.D
        if self.punish_counter > 0:
            if frac_recent >= self.f_thresh:
                self.punish_counter = 0
                return Action.C
            else:
                self.punish_counter = max(0, self.punish_counter - 1)
                return Action.D
        if frac_recent >= self.f_thresh:
            return Action.C
        else:
            self.punish_counter = int(self.punishment_length)
            return Action.D

class Strategy_COOPERATIVE_100(BaseStrategy):
    """
    Adaptive Conditional Cooperator with Graded Punishment & Forgiveness (ACG)

    - Starts by cooperating.
    - Maintains per-opponent exponential moving averages (EMA) of cooperation.
    - Cooperates when estimated cooperating others exceed an adaptive threshold θ.
    - If cooperated but others defected, starts a short graded punishment (1 + d rounds, capped).
    - Forgives after punishment if others resume cooperating.
    - Defects on the final round. Switches to permanent defection if group-level cooperation
      is persistently below a small threshold.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.alpha = 0.7
        self.theta0 = 0.5
        generous_factor = min(0.25, (max(1.0, self.k) - 1.0) / max(1.0, self.n - 1) * 0.25)
        self.theta = self.theta0 - generous_factor
        self.max_punish = 3
        self.permanent_defect_threshold = 0.1
        self.W_persist = 4
        self.S = np.full((max(0, self.n - 1),), 0.5, dtype=np.float64)
        self.G_history = []
        self.punishment_counter = 0
        self.permanent_defect_flag = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.S = np.full((max(0, self.n - 1),), 0.5, dtype=np.float64)
            self.G_history = []
            self.punishment_counter = 0
            self.permanent_defect_flag = False
            return Action.C
        t = history.round_number + 1
        prev_punish = int(self.punishment_counter)
        if history.round_number > 0:
            last_op_actions = np.asarray(history.opponent_actions[-1, :], dtype=np.bool_)
            if last_op_actions.size == self.S.size:
                self.S = self.alpha * self.S + (1.0 - self.alpha) * last_op_actions.astype(np.float64)
            else:
                self.S = np.full((max(0, self.n - 1),), 0.5, dtype=np.float64)
                if last_op_actions.size == self.S.size:
                    self.S = self.alpha * self.S + (1.0 - self.alpha) * last_op_actions.astype(np.float64)
            if self.S.size > 0:
                G = float(np.mean(self.S))
            else:
                G = 0.0
            self.G_history.append(G)
            if len(self.G_history) > self.W_persist:
                self.G_history = self.G_history[-self.W_persist:]
            if len(self.G_history) >= self.W_persist and all((g < self.permanent_defect_threshold for g in self.G_history)):
                self.permanent_defect_flag = True
            last_my_action = bool(history.my_actions[-1])
            d = int((~last_op_actions).sum())
            if prev_punish == 0 and last_my_action and (d > 0):
                self.punishment_counter = min(self.max_punish, 1 + d)
        if self.permanent_defect_flag:
            return Action.D
        if t == self.r:
            return Action.D
        if t == 1:
            return Action.C
        if self.punishment_counter > 0:
            self.punishment_counter = max(0, self.punishment_counter - 1)
            return Action.D
        m_hat = float(np.sum(self.S)) if self.S.size > 0 else 0.0
        threshold_count = (self.n - 1) * self.theta
        if m_hat >= threshold_count:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_101(BaseStrategy):
    """
    Cooperate-first, group-reciprocity strategy for N-player public goods.
    - Opens with C.
    - Reciprocates recent cooperation; punishes short and proportional when group cooperation
      or specific players' cooperation rates fall below thresholds.
    - Punishments are short (P rounds), include occasional single-round probes (epsilon),
      and are followed by a forgiveness probe to re-establish cooperation.
    - In the final L_end rounds (endgame) the agent is cautious and defects unless near-unanimous
      cooperation was observed in the previous round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_timer = 0
        self.punishment_cycle_start = -10 ** 6
        self.last_probe_round = -10 ** 6
        self.awaiting_forgiveness = False
        self.epsilon = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        W = min(5, max(2, r - 1))
        P = min(3, max(1, math.floor(r / 10)))
        L_end = min(3, max(1, math.floor(r / 10)))
        theta_base = 0.6
        theta = 0.5 if k > n / 2.0 else theta_base
        high_theta = 0.9
        exploit_threshold = 0.5
        low_indiv = 0.25
        epsilon = self.epsilon
        if history is None:
            return Action.C
        completed = int(history.round_number)
        t = completed + 1
        last_round_idx = completed - 1
        if completed <= 0:
            return Action.C
        try:
            others_last_round_coops = int(np.sum(history.opponent_actions[last_round_idx, :]))
        except Exception:
            others_last_round_coops = 0
        denom_others = max(1, n - 1)
        f_prev = others_last_round_coops / float(denom_others)
        if self.punishment_timer > 0:
            if self.last_probe_round < self.punishment_cycle_start and random.random() < epsilon:
                self.last_probe_round = t
                return Action.C
            self.punishment_timer -= 1
            if self.punishment_timer == 0:
                self.awaiting_forgiveness = True
            return Action.D
        if self.awaiting_forgiveness and self.last_probe_round < self.punishment_cycle_start:
            self.last_probe_round = t
            self.awaiting_forgiveness = False
            self.punishment_cycle_start = -10 ** 6
            return Action.C
        if t > r - L_end:
            if f_prev >= high_theta:
                return Action.C
            else:
                return Action.D
        start_idx = max(0, completed - W)
        rows = slice(start_idx, completed)
        num_rows = completed - start_idx
        if num_rows <= 0:
            group_recent = 1.0
        else:
            opp_slice = history.opponent_actions[rows, :]
            opp_sums = np.sum(opp_slice, axis=1)
            my_slice = history.my_actions[rows].astype(int)
            total_coops = opp_sums + my_slice
            frac_per_round = total_coops / float(n)
            group_recent = float(np.mean(frac_per_round))
        coop_rates = np.zeros(max(1, n - 1))
        if num_rows > 0 and n - 1 > 0:
            coop_rates = np.mean(history.opponent_actions[rows, :].astype(float), axis=0)
        has_persistent_defector = np.any(coop_rates <= low_indiv)
        if f_prev >= theta:
            return Action.C
        if group_recent < exploit_threshold or has_persistent_defector:
            self.punishment_timer = P
            self.punishment_cycle_start = t
            self.awaiting_forgiveness = True
            return Action.D
        p_coop = max(0.1, f_prev)
        if random.random() < p_coop:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_102(BaseStrategy):
    """
    Adaptive Generous Tit-for-Tat with Proportional Punishment (AGTTPP)

    - Starts by cooperating.
    - Tolerates a small number of defections (tau = floor(k-1)).
    - Punishes proportionally to number of other defectors, capped by max_punish.
    - Enters a protective defecting mode if recent cooperation among others falls below protection_threshold.
    - Conservative in the final round: cooperate only if recent cooperation strongly supports it.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = int(game_description.n_players)
        r = int(game_description.n_rounds)
        k = float(game_description.k)
        tau = int(math.floor(k - 1))
        if tau < 0:
            tau = 0
        tau = min(tau, max(0, n - 1))
        self.n = n
        self.r = r
        self.k = k
        self.tau = tau
        self.min_majority = int(math.ceil(n / 2))
        self.max_punish = min(5, max(1, r - 1))
        self.lookback_L = min(5, max(1, r - 1))
        self.protection_threshold = 0.5
        self.punish_timer = 0
        self.protective_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = history.round_number + 1
        n = self.n
        r = self.r
        rounds_completed = history.round_number
        lookback = min(self.lookback_L, rounds_completed)
        if lookback <= 0:
            avg_coop_others = 1.0
        else:
            recent_opponent_actions = history.opponent_actions[-lookback:, :]
            avg_coop_others = float(np.mean(recent_opponent_actions))
        self.protective_mode = avg_coop_others < self.protection_threshold
        if self.protective_mode:
            return Action.D
        if self.punish_timer > 0:
            self.punish_timer = max(0, self.punish_timer - 1)
            return Action.D
        last_my_action = bool(history.my_actions[-1])
        last_opponent_coops = int(np.sum(history.opponent_actions[-1, :]))
        m_last = int(last_my_action) + last_opponent_coops
        d_last = n - m_last
        d_others_last = n - 1 - last_opponent_coops
        if t == r:
            if m_last >= self.min_majority and d_others_last <= self.tau:
                return Action.C
            else:
                return Action.D
        if d_others_last <= self.tau:
            return Action.C
        else:
            extra_defectors = max(0, d_others_last - self.tau)
            L = 1 + extra_defectors
            L = min(self.max_punish, L)
            self.punish_timer = int(L)
            self.punish_timer = max(0, self.punish_timer - 1)
            return Action.D

class Strategy_COOPERATIVE_103(BaseStrategy):
    """
    Start cooperatively for G rounds to signal goodwill, then use a majority-based,
    forgiving tit-for-tat on the observed group cooperation fraction. Briefly punish
    when group cooperation falls, escalate punishment length slowly if low cooperation
    persists, and switch to safe defection in the final H rounds where cooperating
    cannot be reliably recouped.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.H = int(math.ceil(1.0 / max(1e-12, self.k - 1.0)))
        self.G = min(3, self.r)
        self.theta = 0.5
        self.theta_forgive = 0.4
        self.P_init = 1
        self.P_max = 3
        self.Q = 2
        self.current_punishment_length = int(self.P_init)
        self.punish_counter = 0
        self.consecutive_low_coop = 0

    def __call__(self, state: GameState, history: None | PlayerHistory):
        t = int(state.round_number)
        rem = self.r - t + 1
        if rem <= self.H:
            return Action.D
        if t <= self.G:
            return Action.C
        if history is None or history.round_number == 0:
            return Action.C
        if self.punish_counter > 0:
            self.punish_counter = max(0, self.punish_counter - 1)
            return Action.D
        try:
            my_last = bool(history.my_actions[-1])
        except Exception:
            my_last = False
        try:
            opp_last_row = history.opponent_actions[-1, :]
            opp_coop_count = int(np.sum(opp_last_row))
        except Exception:
            opp_coop_count = 0
        total_cooperators_last = int(my_last) + int(opp_coop_count)
        last_f = float(total_cooperators_last) / float(max(1, self.n))
        if last_f >= self.theta_forgive:
            self.consecutive_low_coop = 0
            self.current_punishment_length = int(self.P_init)
        if last_f >= self.theta:
            self.consecutive_low_coop = 0
            self.current_punishment_length = int(self.P_init)
            return Action.C
        else:
            self.consecutive_low_coop += 1
            if self.consecutive_low_coop >= self.Q:
                self.current_punishment_length = int(min(self.P_max, self.current_punishment_length + 1))
            self.punish_counter = max(0, int(self.current_punishment_length) - 1)
            return Action.D

class Strategy_COOPERATIVE_104(BaseStrategy):
    """
    Conditional Proportional Cooperate (CPC)

    - Starts by cooperating.
    - Uses a short recent window of observed opponent cooperation to decide:
      reward high recent cooperation, punish sustained low cooperation, and
      probabilistically forgive in the ambiguous middle.
    - Defects in the final round and is more conservative in the L_safe rounds
      before the final round (requires very high recent cooperation to still
      cooperate).
    - Extra safety: in ambiguous zone, if everyone else defected in the
      previous round, defect this round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.w = min(5, max(1, self.r - 1))
        self.T_high = 0.6
        self.T_low = 0.4
        self.L_safe = min(2, max(1, self.r - 1))
        self.p_floor = 0.05
        self.p_ceil = 0.95

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            return Action.C
        rounds_completed = int(history.round_number)
        if t == self.r:
            return Action.D
        w_actual = min(self.w, rounds_completed)
        if rounds_completed == 0 or history.opponent_actions.size == 0:
            return Action.C
        recent_opponent = history.opponent_actions[-w_actual:, :]
        coop_count = float(np.sum(recent_opponent))
        denom = float((self.n - 1) * w_actual)
        if denom <= 0.0:
            avg_coop_recent = 0.0
        else:
            avg_coop_recent = coop_count / denom
        if t > self.r - self.L_safe:
            if avg_coop_recent >= 0.9:
                return Action.C
            else:
                return Action.D
        if avg_coop_recent >= self.T_high:
            return Action.C
        if avg_coop_recent <= self.T_low:
            return Action.D
        try:
            prev_round_coops = int(np.sum(history.opponent_actions[-1, :]))
        except Exception:
            prev_round_coops = 0
        if prev_round_coops == 0:
            return Action.D
        fraction = (avg_coop_recent - self.T_low) / (self.T_high - self.T_low)
        p = float(self.p_floor + (self.p_ceil - self.p_floor) * fraction)
        if p < self.p_floor:
            p = float(self.p_floor)
        if p > self.p_ceil:
            p = float(self.p_ceil)
        u = random.random()
        if u <= p:
            return Action.C
        else:
            return Action.D

class Strategy_COOPERATIVE_105(BaseStrategy):
    """
    EWMA-based reciprocal public goods strategy.
    - Start by cooperating.
    - Maintain an EWMA s of the fraction of other players cooperating.
    - Use a dynamic threshold theta(t) that increases toward the end.
    - If s >= theta -> cooperate (except final round).
    - If s < theta -> defect and start a limited proportional punishment.
    - While punishing: defect unless recent_frac >= forgiveness_level (then forgive).
    - Defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.alpha = 0.5
        self.max_threshold_increase = 0.3
        self.forgiveness_level = 0.75
        self.s = None
        self.punish_counter = 0
        n = int(self.game_description.n_players)
        k = float(self.game_description.k)
        r = int(self.game_description.n_rounds)
        raw = 0.5 - 0.2 * (k - 1.0) / max(1, n - 1)
        self.base_theta = min(0.7, max(0.3, raw))
        self.max_punishment = max(1, math.floor(r / 6))
        self.scale_for_punishment = r / 2.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = history.round_number + 1
        r = int(self.game_description.n_rounds)
        n = int(self.game_description.n_players)
        if t == r:
            return Action.D
        if history.opponent_actions.size == 0:
            recent_frac = 1.0
        else:
            last_round_actions = history.opponent_actions[-1, :]
            denom = max(1, last_round_actions.shape[-1])
            recent_frac = float(np.sum(last_round_actions) / denom)
        if self.s is None:
            self.s = recent_frac
        else:
            self.s = self.alpha * recent_frac + (1.0 - self.alpha) * self.s
        theta = self.base_theta + self.max_threshold_increase * (t - 1) / max(1, r - 1)
        if theta > 0.95:
            theta = 0.95
        if self.punish_counter > 0:
            if recent_frac >= self.forgiveness_level:
                self.punish_counter = 0
                return Action.C
            else:
                self.punish_counter = max(0, self.punish_counter - 1)
                return Action.D
        elif self.s >= theta:
            return Action.C
        else:
            shortfall = max(0.0, theta - self.s)
            punish_length = 1 + math.floor(shortfall * self.scale_for_punishment)
            punish_length = int(min(punish_length, self.max_punishment))
            self.punish_counter = max(0, punish_length - 1)
            return Action.D

class Strategy_COOPERATIVE_106(BaseStrategy):
    """
    Majority-Responsive Forgiving Cooperator (MRFC)

    - Starts by cooperating.
    - Uses a recent-window estimate of group cooperation to decide.
    - If majority-like cooperation observed last round, cooperates.
    - If cooperation collapses, defects for a short punishment window that
      extends modestly when recent cooperation is far below threshold.
    - Forgives after punishment and returns to normal play.
    - In the final E rounds behaves cautiously: cooperates only if both
      last-round and recent cooperation are sufficiently high.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        k = game_description.k
        self.W = min(5, r)
        raw_thresh = 0.6 - 0.1 * ((k - 1.0) / max(1.0, n - 1.0))
        self.coop_threshold = max(0.5, min(0.6, raw_thresh))
        self.P0 = 2
        self.punish_extend_multiplier = 4.0
        self.E = min(3, max(0, r - 1))
        self.endgame_coop_threshold = 0.75
        self.punish_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if history is None:
            return Action.C
        if history.round_number > 0:
            opps = history.opponent_actions.astype(int)
            mine = history.my_actions.astype(int)
            opp_sums = np.sum(opps, axis=1).astype(int)
            total_coops_per_round = (mine + opp_sums).astype(float)
            coop_fractions = total_coops_per_round / float(n)
        else:
            coop_fractions = np.array([], dtype=float)
        if coop_fractions.size == 0:
            recent_coop_fraction = 1.0
        else:
            window = min(self.W, coop_fractions.size)
            recent_coop_fraction = float(np.mean(coop_fractions[-window:]))
        if coop_fractions.size == 0:
            last_round_fraction = 1.0
        else:
            last_round_fraction = float(coop_fractions[-1])
        if self.punish_remaining > 0:
            self.punish_remaining = max(0, self.punish_remaining - 1)
            return Action.D
        if self.E > 0 and t > r - self.E:
            if last_round_fraction >= self.endgame_coop_threshold and recent_coop_fraction >= self.coop_threshold:
                return Action.C
            else:
                return Action.D
        if last_round_fraction >= self.coop_threshold:
            return Action.C
        else:
            shortage = max(0.0, self.coop_threshold - recent_coop_fraction)
            extra = int(math.ceil(shortage * self.punish_extend_multiplier))
            P = int(self.P0 + extra)
            remaining_after_current = max(0, r - t)
            self.punish_remaining = min(P, remaining_after_current)
            return Action.D

class Strategy_COOPERATIVE_107(BaseStrategy):
    """
    Reciprocal Proportional Forgiveness (RPF)

    Starts cooperating. If any defections are observed in the previous round, enter a
    proportional punishment phase: punish for 1 + (#defectors last round) rounds (capped
    by remaining rounds). After a punishment finishes, require W = min(5, r) rounds of
    near-unanimous cooperation (each of those rounds must have at least n-1 cooperators)
    before committing to unconditional cooperation; if recovery fails during that window,
    re-enter punishment proportionally to the most recent defection. In the final round,
    cooperate only if the previous round was unanimous cooperation or if currently still
    serving a punishment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.mode = 'COOPERATE'
        self.punish_remaining = 0
        self.verified_coop = True
        self.W = min(5, self.r)
        self.max_punish = self.r

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:

        def count_cooperators_idx(idx: int, history_obj: PlayerHistory) -> int:
            opp_coops = int(np.sum(history_obj.opponent_actions[idx, :]))
            my_coop = int(bool(history_obj.my_actions[idx]))
            return opp_coops + my_coop

        def verify_recovery_and_maybe_punish(t: int, history_obj: PlayerHistory) -> bool:
            if history_obj.round_number < self.W:
                return False
            start_idx = history_obj.round_number - self.W
            end_idx = history_obj.round_number - 1
            for idx in range(start_idx, end_idx + 1):
                coop_count = count_cooperators_idx(idx, history_obj)
                if coop_count < self.n - 1:
                    latest_idx = history_obj.round_number - 1
                    latest_coops = count_cooperators_idx(latest_idx, history_obj)
                    latest_defectors = self.n - latest_coops
                    remaining_rounds = max(0, self.r - t + 1)
                    p = min(remaining_rounds, 1 + latest_defectors)
                    p = max(0, p - 1)
                    self.punish_remaining = p
                    self.mode = 'PUNISH'
                    self.verified_coop = False
                    return True
            self.verified_coop = True
            return False
        if history is None:
            self.mode = 'COOPERATE'
            self.punish_remaining = 0
            self.verified_coop = True
            return Action.C
        t = history.round_number + 1
        remaining_rounds = max(0, self.r - t + 1)
        if self.punish_remaining > 0:
            action = Action.D
            self.punish_remaining -= 1
            if self.punish_remaining == 0:
                self.mode = 'COOPERATE'
                self.verified_coop = False
            return action
        if history.round_number == 0:
            prev_cooperators = self.n
        else:
            last_idx = history.round_number - 1
            prev_cooperators = count_cooperators_idx(last_idx, history)
        if t == self.r:
            if prev_cooperators == self.n:
                return Action.C
            else:
                return Action.D
        if not self.verified_coop:
            reentered = verify_recovery_and_maybe_punish(t, history)
            if reentered:
                if self.punish_remaining == 0:
                    self.mode = 'COOPERATE'
                    self.verified_coop = False
                return Action.D
        if prev_cooperators == self.n:
            return Action.C
        else:
            num_defectors = self.n - prev_cooperators
            p = min(remaining_rounds, 1 + num_defectors)
            p = max(0, p - 1)
            self.punish_remaining = p
            if self.punish_remaining == 0:
                self.mode = 'COOPERATE'
                self.verified_coop = False
            else:
                self.mode = 'PUNISH'
                self.verified_coop = False
            return Action.D

class Strategy_COOPERATIVE_108(BaseStrategy):
    """
    Adaptive Generous Threshold Reciprocity (AGTR)

    - Signals cooperation in the first round.
    - Defects in the final round to avoid end-game exploitation.
    - Tracks recent group cooperation and per-opponent cooperation rates.
    - If recent group cooperation is above an adaptive threshold, cooperates.
    - If below threshold, triggers a short collective punishment (defection).
    - Uses short punishment length, forgiveness probability and a small randomized buffer
      around the threshold to recover from noise and accidental defections.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, max(1, self.r - 1))
        self.M = min(10, max(3, math.floor(self.r / 4)))
        self.q0 = 0.6
        if self.n > 1:
            adjust = 0.1 * (self.k - 1) / max(1.0, self.n - 1)
            self.q0 = max(0.4, min(0.8, self.q0 - adjust))
        else:
            self.q0 = 0.6
        self.theta_low = 0.2
        self.L = min(3, max(1, math.floor(self.r / 10)))
        self.g = 0.2
        self.delta = 0.05
        self.epsilon = 0.05
        self.punishment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = int(state.round_number)
        if t >= self.r:
            return Action.D
        completed = int(history.round_number)
        if completed <= 0:
            return Action.C
        opponent_count = max(1, self.n - 1)
        Wt = min(self.W, completed)
        Mt = min(self.M, completed)
        recent_opponent_actions = history.opponent_actions[-Mt:, :] if Mt > 0 else np.zeros((0, opponent_count), dtype=bool)
        if Mt > 0:
            coop_counts = np.sum(recent_opponent_actions.astype(float), axis=0)
            coop_rate_j = coop_counts / float(Mt)
        else:
            coop_rate_j = np.zeros(opponent_count, dtype=float)
        recent_window = history.opponent_actions[-Wt:, :] if Wt > 0 else np.zeros((0, opponent_count), dtype=bool)
        if Wt > 0:
            per_round_frac = np.sum(recent_window.astype(float), axis=1) / float(opponent_count)
            recent_group_coop = float(np.mean(per_round_frac)) if per_round_frac.size > 0 else 0.0
        else:
            recent_group_coop = 0.0
        persistent_count = int(np.sum(coop_rate_j < self.theta_low))
        persistent_fraction = persistent_count / float(opponent_count) if opponent_count > 0 else 0.0
        if persistent_fraction > 0.0:
            q = self.q0 + 0.25 * persistent_fraction
        else:
            q = self.q0
        q = max(0.0, min(1.0, q))
        if self.punishment_counter > 0:
            if recent_group_coop >= q and t < self.r:
                self.punishment_counter = 0
                if random.random() < self.epsilon:
                    return Action.D
                return Action.C
            else:
                self.punishment_counter = max(0, self.punishment_counter - 1)
                return Action.D
        if recent_group_coop >= q + self.delta:
            if random.random() < self.epsilon:
                return Action.D
            return Action.C
        if recent_group_coop <= q - self.delta:
            self.punishment_counter = max(0, self.L - 1)
            return Action.D
        prob_cooperate = min(1.0, self.g + self.epsilon)
        if random.random() < prob_cooperate:
            return Action.C
        else:
            self.punishment_counter = max(0, self.L - 1)
            return Action.D

class Strategy_COOPERATIVE_109(BaseStrategy):
    """
    Forgiving Proportional Punisher (FPP)

    - Starts by cooperating.
    - Rewards fully cooperative rounds.
    - If defections are observed, forgives likely noise (short window, 0.8 threshold).
    - Otherwise initiates a collective punishment phase (everyone defects) whose length is
      computed from game parameters (n, k, r) and scaled by number of defectors and repeat offences.
    - Punishments are capped so they cannot extend into the final round.
    - Defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.punishment_counter = 0
        self.last_defectors = set()
        self.offence_counts = np.zeros(max(0, self.n - 1), dtype=int)
        self.w = min(5, max(1, math.floor(self.r / 4)))
        self.eps = 1e-06

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            return Action.C
        if t == self.r:
            return Action.D
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            if self.punishment_counter == 0:
                if self.offence_counts.size > 0:
                    self.offence_counts = np.maximum(0, self.offence_counts - 1)
                self.last_defectors = set()
            return Action.D
        completed = history.round_number
        if completed == 0:
            return Action.C
        last_round_actions = history.opponent_actions[-1, :]
        defect_indices = set((int(i) for i in np.where(last_round_actions == False)[0]))
        self.last_defectors = set(defect_indices)
        if len(defect_indices) == 0:
            return Action.C
        window = min(self.w, completed)
        recent_slice = history.opponent_actions[-window:, :]
        forgive_all = True
        for j in defect_indices:
            coop_count_j = int(np.count_nonzero(recent_slice[:, j] == True))
            coop_rate_j = coop_count_j / float(window)
            if coop_rate_j < 0.8:
                forgive_all = False
                break
        if forgive_all:
            return Action.C
        Delta = 1.0 - self.k / float(self.n)
        denom = max(self.eps, self.k - 1.0)
        raw_P0 = Delta / denom
        P0 = int(math.ceil(raw_P0)) if raw_P0 > 0 else 0
        max_punishable = max(0, self.r - t)
        P0 = min(P0, max_punishable)
        d = len(defect_indices)
        if d > 0 and self.offence_counts.size > 0:
            avg_offences = float(np.mean([self.offence_counts[j] for j in defect_indices]))
        else:
            avg_offences = 0.0
        escalation_factor = 1.0 + 0.5 * min(avg_offences, 2.0)
        penal = P0 * d * escalation_factor
        punish_len = int(math.ceil(penal)) if penal > 0 else 1
        punish_len = max(1, punish_len)
        punish_len = min(max_punishable, punish_len) if max_punishable > 0 else 0
        if punish_len <= 0:
            return Action.D
        self.punishment_counter = punish_len
        for j in defect_indices:
            if self.offence_counts.size > 0:
                self.offence_counts[j] += 1
        self.punishment_counter -= 1
        if self.punishment_counter == 0:
            if self.offence_counts.size > 0:
                self.offence_counts = np.maximum(0, self.offence_counts - 1)
            self.last_defectors = set()
        return Action.D

class Strategy_COOPERATIVE_110(BaseStrategy):
    """
    Adaptive Forgiving Conditional Cooperator (AFCC)

    Start cooperatively, forgive small/occasional lapses, punish proportional to
    persistent or widespread defections with temporary punishments, allow recovery,
    and defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        if history is None:
            return Action.C
        t = int(history.round_number) + 1
        if t == r:
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            return Action.D
        w = min(3, max(1, math.floor(r / 10)))
        lookback = min(w, t - 1)
        if lookback <= 0:
            return Action.C
        recent_opponent_actions = history.opponent_actions[-lookback:, :]
        total_other_coop = float(np.sum(recent_opponent_actions))
        coop_rate = total_other_coop / ((n - 1) * lookback)
        last_round_other_coop = int(np.sum(history.opponent_actions[-1, :]))
        last_round_defectors = n - 1 - last_round_other_coop
        g = max(1, round(0.05 * n))
        T_high = 0.8
        T_low = 0.5
        T_recover = 0.6
        L_min = 1
        L_max = max(1, round(r / 4))
        if coop_rate >= T_high:
            return Action.C
        if coop_rate >= T_low:
            if last_round_defectors <= g:
                return Action.C
            else:
                punish_len = min(L_max, max(L_min, last_round_defectors - g))
                self.punishment_timer = punish_len
                self.punishment_timer -= 1
                return Action.D
        punish_len = min(L_max, max(L_min, round((1.0 - coop_rate) * n)))
        self.punishment_timer = punish_len
        self.punishment_timer -= 1
        return Action.D

class Strategy_COOPERATIVE_111(BaseStrategy):
    """
    Start by cooperating, sustain cooperation when the group is reliably cooperative,
    punish briefly and proportionally when the group defects, forgive quickly,
    and always defect in the final round. Uses only (n, r, k) and observed history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_until_round = 0
        r = self.game_description.n_rounds
        self.w = min(10, max(1, r - 1))
        self.g = math.ceil(math.sqrt(r))
        self.max_punish = max(1, min(5, math.ceil(r / 6)))
        self.gamma_high = 0.9
        self.gamma_med = 0.7
        self.gamma_low = 0.5
        self.persistence_thresh = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        t = state.round_number
        if history is None or history.round_number == 0:
            return Action.C
        if t < 1:
            return Action.C
        if t == r:
            return Action.D
        rounds_played = history.round_number
        opp_actions = history.opponent_actions
        if rounds_played > 0:
            opp_coop_counts = np.sum(opp_actions, axis=0).astype(int)
            opp_defect_counts = rounds_played - opp_coop_counts
        else:
            opp_defect_counts = np.zeros(n - 1, dtype=int)
        if t <= self.punish_until_round:
            return Action.D
        lookback = min(self.w, rounds_played)
        if lookback > 0:
            recent_slice = opp_actions[rounds_played - lookback:rounds_played, :]
            p_recent = float(np.mean(np.mean(recent_slice, axis=1)))
        else:
            p_recent = 1.0
        if rounds_played > 0:
            p_overall = float(np.mean(opp_actions))
        else:
            p_overall = 1.0
        persistent = []
        if rounds_played > 0:
            rates = opp_defect_counts / float(rounds_played)
            persistent = [idx for idx, rate in enumerate(rates) if rate >= self.persistence_thresh]
        if t > r - self.g:
            if p_recent == 1.0 and len(persistent) == 0:
                return Action.C
            else:
                return Action.D
        if p_recent >= self.gamma_high and p_overall >= self.gamma_med and (len(persistent) == 0):
            return Action.C
        if p_recent >= self.gamma_low:
            if random.random() < 0.9:
                return Action.C
            else:
                return Action.D
        if rounds_played > 0:
            last_round_coop = np.sum(opp_actions[-1, :])
            d_last = n - 1 - int(last_round_coop)
        else:
            d_last = 0
        punish_length = min(self.max_punish, 1 + math.ceil(d_last / float(max(1, n - 1)) * self.w))
        self.punish_until_round = t + punish_length - 1
        return Action.D

class Strategy_COOPERATIVE_112(BaseStrategy):
    """
    Adaptive Generous Conditional Cooperator (AGCC)

    - Starts by cooperating (nice).
    - Cooperates when a recent majority of other players cooperated.
    - Punishes by defecting for a limited, escalating number of rounds when cooperation falls below threshold.
    - Forgives: de-escalates when others return to cooperating.
    - Small generosity probability to try restarting cooperation.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, max(0, self.r - 1))
        self.gamma = 0.5
        self.P_max = 3
        self.eps = 0.05
        self.punish_counter = 0
        self.escalation_level = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            return Action.C
        if t == self.r:
            return Action.D
        if self.punish_counter > 0:
            self.punish_counter -= 1
            return Action.D
        lookback = min(self.W, max(0, t - 1))
        if lookback <= 0:
            other_coop_rate = 0.0
        else:
            recent = history.opponent_actions[-lookback:, :]
            total_other_contribs = float(np.sum(recent))
            max_possible = float(recent.shape[0] * recent.shape[1]) if recent.size > 0 else 0.0
            other_coop_rate = total_other_contribs / max_possible if max_possible > 0.0 else 0.0
        if other_coop_rate >= self.gamma:
            if self.escalation_level > 0:
                self.escalation_level = max(0, self.escalation_level - 1)
            return Action.C
        if random.random() < self.eps:
            return Action.C
        shortfall = max(0.0, self.gamma - other_coop_rate)
        base_P = 1 + math.ceil(shortfall / self.gamma * 2.0)
        P = min(self.P_max, base_P + int(self.escalation_level))
        self.punish_counter = max(0, int(P) - 1)
        self.escalation_level = min(self.escalation_level + 1, self.P_max)
        return Action.D

class Strategy_COOPERATIVE_113(BaseStrategy):
    """
    Forgiving Conditional Cooperator (FCC)

    - Starts by cooperating (unless the first round is also the final round).
    - Cooperates when the previous round had at least ceil(tau * n) cooperators,
      where tau = 1 - 1/k.
    - If cooperation falls below that threshold, initiates a proportional,
      bounded punishment (defecting for a small number of rounds) targeting
      the players who defected in the previous round.
    - While punishing, watches for repentance (two consecutive Cs or >=50% C
      in the recent window W) and ends punishment early if all punished players
      have repented.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.tau = 1.0 - 1.0 / self.k if self.k != 0 else 0.0
        self.W = min(4, max(1, math.floor(self.r / 5)))
        self.P_scale = min(5, max(1, math.floor(self.r / 10)))
        self.P_max = self.P_scale
        self.punishment_counter = 0
        self.punished_set = set()
        self.per_player_history = [[] for _ in range(self.n)]
        self._last_processed_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.r:
            return Action.D
        if history is None:
            self._last_processed_rounds = 0
            return Action.C
        completed_rounds = history.round_number
        rounds_available = completed_rounds
        my_actions_arr = np.asarray(history.my_actions) if rounds_available > 0 else np.array([], dtype=bool)
        opp_actions_arr = np.asarray(history.opponent_actions) if rounds_available > 0 else np.zeros((0, max(0, self.n - 1)), dtype=bool)
        new_per_player_history = [[] for _ in range(self.n)]
        if rounds_available > 0:
            recent_self = my_actions_arr[-self.W:] if len(my_actions_arr) > 0 else np.array([], dtype=bool)
            new_per_player_history[0] = [bool(x) for x in recent_self.tolist()]
        else:
            new_per_player_history[0] = []
        expected_opp = max(0, self.n - 1)
        for j in range(expected_opp):
            if rounds_available > 0 and opp_actions_arr.shape[1] >= expected_opp:
                col = opp_actions_arr[:, j]
                recent_col = col[-self.W:] if len(col) > 0 else np.array([], dtype=bool)
                new_per_player_history[j + 1] = [bool(x) for x in recent_col.tolist()]
            else:
                new_per_player_history[j + 1] = []
        self.per_player_history = new_per_player_history
        rounds_passed = max(0, completed_rounds - self._last_processed_rounds)
        if rounds_passed > 0 and self.punishment_counter > 0:
            self.punishment_counter = max(0, self.punishment_counter - rounds_passed)
        if self.punishment_counter > 0 and len(self.punished_set) > 0:
            all_repented = True
            for pid in list(self.punished_set):
                hist = self.per_player_history[pid]
                L = len(hist)
                repented = False
                if L >= 2:
                    if hist[-1] and hist[-2]:
                        repented = True
                if not repented and L > 0:
                    c_count = sum((1 for x in hist if x))
                    if c_count >= math.ceil(0.5 * L):
                        repented = True
                if not repented:
                    all_repented = False
                    break
            if all_repented:
                self.punishment_counter = 0
                self.punished_set = set()
        self._last_processed_rounds = completed_rounds
        t = state.round_number
        if self.punishment_counter > 0:
            return Action.D
        if t == 1:
            return Action.C
        c_prev = 0
        if completed_rounds > 0:
            try:
                c_prev += 1 if bool(history.my_actions[-1]) else 0
            except Exception:
                c_prev += 0
            try:
                c_prev += int(np.sum(history.opponent_actions[-1, :])) if hasattr(history, 'opponent_actions') and history.opponent_actions.size > 0 else 0
            except Exception:
                pass
        threshold_count = math.ceil(self.tau * self.n)
        if c_prev >= threshold_count:
            return Action.C
        else:
            d = self.n - c_prev
            new_punished = set()
            if completed_rounds > 0 and opp_actions_arr.size > 0:
                for j in range(opp_actions_arr.shape[1]):
                    acted = bool(opp_actions_arr[-1, j])
                    if not acted:
                        new_punished.add(j + 1)
            raw_p = round(d / float(self.n) * self.P_scale) if self.n > 0 else 1
            raw_p = int(raw_p)
            punishment_length = max(1, raw_p)
            punishment_length = min(self.P_max, punishment_length)
            remaining_rounds = max(0, self.r - t)
            punishment_length = min(punishment_length, max(1, remaining_rounds))
            self.punishment_counter = punishment_length
            self.punished_set = new_punished
            return Action.D

class Strategy_COOPERATIVE_114(BaseStrategy):
    """
    Forgiving Conditional Contributor (FCC)

    Starts by cooperating. Uses a sliding memory window W to estimate the fraction f of other
    players who have cooperated recently. If f is above a threshold p, cooperate; otherwise
    initiate a short, deterministic punishment (defect for L rounds). Punishments are finite
    and followed by a short recovery observation window to avoid oscillation. In the final
    round cooperate only if there is a very strong cooperative record.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(8, self.r)
        raw_p = 0.5 + 0.25 * (self.k - 1.0) / max(1.0, self.n - 1.0)
        self.p = max(0.5, min(0.75, raw_p))
        self.L = min(3, max(1, math.ceil((1.0 - self.p) * self.W)))
        self.p_final = 0.8
        self.punishment_remaining = 0
        self.recovery_wait = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = int(state.round_number)
        past_rounds = history.round_number
        lookback_start_round = max(1, t - self.W)
        start_idx = lookback_start_round - 1
        end_idx = t - 2
        denom = end_idx - start_idx + 1 if end_idx >= start_idx else 0
        f = 0.0
        if denom > 0:
            recent_opponents = history.opponent_actions[start_idx:end_idx + 1, :]
            per_round_frac = np.mean(recent_opponents.astype(float), axis=1)
            f = float(np.mean(per_round_frac))
        else:
            f = 0.0
        if self.punishment_remaining > 0:
            action = Action.D
            self.punishment_remaining -= 1
            if self.punishment_remaining == 0:
                self.recovery_wait = 1
            return action
        if self.recovery_wait > 0:
            if f < self.p:
                self.punishment_remaining = self.L - 1
                self.recovery_wait = 0
                if self.punishment_remaining == 0:
                    self.recovery_wait = 1
                return Action.D
            else:
                self.recovery_wait = max(0, self.recovery_wait - 1)
                return Action.D
        if t == self.r:
            final_lookback_count = min(self.W, max(0, self.r - 1))
            final_start_round = max(1, t - final_lookback_count)
            fs = []
            sustained_ok = True
            if final_lookback_count > 0:
                fs = []
                for rr in range(final_start_round, t):
                    idx = rr - 1
                    my_c = 1 if history.my_actions[idx] else 0
                    others_c = int(np.sum(history.opponent_actions[idx, :]))
                    group_frac = (my_c + others_c) / float(self.n)
                    if group_frac < self.p:
                        sustained_ok = False
                        break
            if f >= self.p_final and sustained_ok:
                return Action.C
            else:
                return Action.D
        if f >= self.p:
            return Action.C
        else:
            self.punishment_remaining = self.L - 1
            if self.punishment_remaining == 0:
                self.recovery_wait = 1
            return Action.D

class Strategy_COOPERATIVE_115(BaseStrategy):
    """
    Adaptive Conditional Cooperator (ACC)

    - Starts by cooperating to signal intent.
    - Uses a recent-window average of group cooperation to decide whether to cooperate.
    - If recent cooperation falls below a threshold (τ), issues a short, proportionate
      punishment (defect for a few rounds), capped by MaxPunish.
    - Forgives quickly after punishment if cooperation resumes.
    - Defects unconditionally in the final E rounds (endgame safeguard).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.L = min(10, self.r)
        self.alpha = (self.k - 1.0) / max(1, self.n - 1)
        raw_tau = 0.55 - 0.2 * self.alpha
        self.tau = max(0.35, min(raw_tau, 0.75))
        self.MaxPunish = min(3, max(1, self.r // 6))
        self.E = 1
        self.punishment_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        completed_rounds = 0 if history is None else int(history.round_number)
        t = completed_rounds + 1
        if t > self.r - self.E:
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            return Action.D
        if history is None or completed_rounds == 0:
            return Action.C
        window_len = min(self.L, completed_rounds)
        start_idx = completed_rounds - window_len
        coop_fracs = []
        for idx in range(start_idx, completed_rounds):
            my_c = bool(history.my_actions[idx])
            opp_row = history.opponent_actions[idx]
            opp_coops = int(np.sum(opp_row).item()) if hasattr(opp_row, 'dtype') else int(sum(opp_row))
            total_coops = (1 if my_c else 0) + opp_coops
            coop_fracs.append(total_coops / float(self.n))
        recent_frac = float(np.mean(np.array(coop_fracs))) if len(coop_fracs) > 0 else 0.0
        if recent_frac >= self.tau:
            return Action.C
        else:
            shortfall = self.tau - recent_frac
            raw = shortfall / self.tau * float(self.MaxPunish)
            punish_dur = 1 + int(math.floor(raw))
            punish_dur = min(punish_dur, self.MaxPunish)
            self.punishment_timer = max(0, punish_dur - 1)
            return Action.D

class Strategy_COOPERATIVE_116(BaseStrategy):
    """
    Adaptive Generous Conditional Cooperator (AGCC)
    - Starts by cooperating.
    - Uses a short sliding window of public cooperation counts to decide reciprocity.
    - If exploited (you cooperated while too many others defected) punish proportionally but briefly.
    - Forgives after punishment if the group shows cooperative tendencies; otherwise continues defecting
      but probes occasionally to try to re-establish cooperation.
    - Becomes stricter in the final few rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, max(1, self.r - 1))

        def _clamp(x, a, b):
            return min(b, max(a, x))
        target_frac = 0.5
        if self.k > 1.5:
            target_frac = target_frac - 0.05 * (self.k - 1.5)
        self.target_fraction = _clamp(target_frac, 0.3, 0.7)
        self.target_count = math.ceil((self.n - 1) * self.target_fraction)
        self.P_max = min(4, max(1, math.floor(self.r / 6)))
        self.Punish_scale = 1
        self.Probe_interval = max(3, math.ceil(self.r / 10))
        self.Endgame_window = min(2, max(1, self.r - 1))
        self.punishment_counter = 0
        self.last_action = Action.C
        self.rounds_since_probe = self.Probe_interval

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            self.last_action = Action.C
            self.punishment_counter = 0
            self.rounds_since_probe = self.Probe_interval
            return Action.C
        t = history.round_number + 1
        my_last = bool(history.my_actions[-1])
        sum_opponents_last = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size else 0
        total_coops_last = sum_opponents_last + (1 if my_last else 0)
        last_round_other_coops = sum_opponents_last
        rounds_available = history.round_number
        window = min(self.w, rounds_available)
        if window <= 0:
            avg_other_coop_count = 0.0
            avg_other_coop_fraction = 0.0
        else:
            opp_slice = history.opponent_actions[-window:, :] if history.opponent_actions.size else np.zeros((window, max(0, self.n - 1)), dtype=bool)
            per_round_counts = np.sum(opp_slice, axis=1) if opp_slice.size else np.zeros(window)
            avg_other_coop_count = float(np.mean(per_round_counts))
            avg_other_coop_fraction = avg_other_coop_count / max(1, self.n - 1)
        if self.punishment_counter > 0:
            action = Action.D
            self.punishment_counter = max(0, self.punishment_counter - 1)
            self.last_action = action
            self.rounds_since_probe = min(self.Probe_interval, self.rounds_since_probe + 1)
            return action
        if t > self.r - self.Endgame_window:
            if last_round_other_coops >= self.target_count:
                action = Action.C
                self.last_action = action
                self.rounds_since_probe = 0
            else:
                action = Action.D
                self.last_action = action
                self.rounds_since_probe = min(self.Probe_interval, self.rounds_since_probe + 1)
            return action
        if self.last_action == Action.C:
            if last_round_other_coops >= self.target_count:
                action = Action.C
                self.last_action = action
                self.rounds_since_probe = 0
                return action
            else:
                number_of_defectors_last_round = self.n - total_coops_last
                punishment_length = int(min(self.P_max, max(1, self.Punish_scale * number_of_defectors_last_round)))
                self.punishment_counter = max(0, punishment_length - 1)
                action = Action.D
                self.last_action = action
                self.rounds_since_probe = min(self.Probe_interval, self.rounds_since_probe + 1)
                return action
        elif last_round_other_coops >= self.target_count or avg_other_coop_count >= self.target_fraction * (self.n - 1):
            action = Action.C
            self.last_action = action
            self.rounds_since_probe = 0
            return action
        elif self.rounds_since_probe >= self.Probe_interval:
            action = Action.C
            self.last_action = action
            self.rounds_since_probe = 0
            return action
        else:
            action = Action.D
            self.last_action = action
            self.rounds_since_probe = min(self.Probe_interval, self.rounds_since_probe + 1)
            return action

class Strategy_COOPERATIVE_117(BaseStrategy):
    """
    Reciprocal Forgiving Majority (RFM)

    - Starts by cooperating.
    - Uses a recent-window majority threshold of other players' cooperation to decide.
    - If the recent cooperation fraction of others >= tau (0.60) -> cooperate.
    - If below tau -> defect and begin a finite punishment proportional to the shortfall,
      capped by P_max and never extending into the final round.
    - While within a punishment phase (current round <= punishing_until) -> defect.
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.M_max = min(5, max(0, self.r - 1))
        self.tau = 0.6
        self.P_max = min(4, max(0, self.r - 1))
        self.punishing_until = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or getattr(history, 'round_number', 0) == 0:
            return Action.C
        t = int(state.round_number)
        if t >= self.r:
            return Action.D
        if t <= self.punishing_until:
            return Action.D
        lookback = min(self.M_max, max(0, t - 1))
        if lookback <= 0:
            return Action.C
        recent_opponent_actions = history.opponent_actions[-lookback:, :]
        sum_other_coops = int(np.sum(recent_opponent_actions))
        denom = (self.n - 1) * lookback
        if denom <= 0:
            coop_freq_other = 0.0
        else:
            coop_freq_other = float(sum_other_coops) / float(denom)
        if coop_freq_other >= self.tau:
            return Action.C
        else:
            severity = self.tau - coop_freq_other
            raw_P = math.ceil(severity * (self.n - 1))
            P = min(self.P_max, max(1, int(raw_P)))
            self.punishing_until = min(self.r - 1, t + P - 1)
            return Action.D

class Strategy_COOPERATIVE_118(BaseStrategy):
    """
    Forgiving Conditional Cooperator (FCC)

    - Starts by cooperating.
    - Maintains an exponential moving average (p_hat) of the fraction of other players
      who cooperated in recent rounds.
    - Cooperates when p_hat >= T_coop, otherwise defects and triggers a limited,
      proportional punishment based on the fraction of defectors last round.
    - If currently punishing, may shorten punishment early if others clearly cooperated.
    - Implements contrition: if we defected previously but others stayed mostly
      cooperative, attempt to repair by cooperating.
    - Conservative in the final rounds (endgame), only cooperating near the end
      when trust is very high.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.alpha = 0.4
        self.T_coop = 0.6
        self.high_trust = 0.9
        self.K = 4
        r = int(self.game_description.n_rounds)
        self.E = min(3, max(0, r - 1))
        self.contrite_threshold = 0.75
        self.p_hat = 1.0
        self.punishment_timer = 0
        self.last_action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        r = int(self.game_description.n_rounds)
        n = int(self.game_description.n_players)
        if history is None or history.round_number == 0:
            self.last_action = Action.C
            return Action.C
        t = history.round_number + 1
        fraction_coop_others_last = None
        if history.round_number >= 1:
            last_ops = history.opponent_actions[-1, :]
            coop_count = float(last_ops.sum())
            fraction_coop_others_last = coop_count / float(max(1, n - 1))
        if fraction_coop_others_last is not None:
            self.p_hat = self.alpha * fraction_coop_others_last + (1.0 - self.alpha) * self.p_hat
        if self.punishment_timer > 0:
            action = Action.D
            self.punishment_timer = max(0, self.punishment_timer - 1)
            if fraction_coop_others_last is not None and fraction_coop_others_last >= 0.8:
                self.punishment_timer = max(0, self.punishment_timer - 1)
            self.last_action = action
            return action
        if t == r:
            cond_last_round_perfect = fraction_coop_others_last == 1.0 if fraction_coop_others_last is not None else False
            if self.p_hat >= self.high_trust and (t == 1 or cond_last_round_perfect):
                self.last_action = Action.C
                return Action.C
            else:
                self.last_action = Action.D
                return Action.D
        if self.E > 0 and t > r - self.E and (t != r):
            if self.p_hat >= self.high_trust and fraction_coop_others_last == 1.0:
                self.last_action = Action.C
                return Action.C
            else:
                self.last_action = Action.D
                return Action.D
        if self.last_action == Action.D and fraction_coop_others_last is not None and (fraction_coop_others_last >= self.contrite_threshold):
            self.last_action = Action.C
            return Action.C
        if self.p_hat >= self.T_coop:
            self.last_action = Action.C
            return Action.C
        action = Action.D
        if fraction_coop_others_last is not None:
            f_defect_last = 1.0 - fraction_coop_others_last
            if f_defect_last > 0.0:
                punish_len = int(min(self.K, max(1, math.ceil(f_defect_last * self.K))))
                remaining = max(0, r - t)
                punish_len = min(punish_len, remaining)
                self.punishment_timer = max(self.punishment_timer, punish_len)
        self.last_action = action
        return action

class Strategy_COOPERATIVE_119(BaseStrategy):
    """
    Adaptive Proportional Conditional Cooperator (APCC)

    - Starts by cooperating to signal willingness.
    - Uses a short look-back window L to compute recent group cooperation fraction.
    - If recent cooperation fraction falls below a threshold (adapted by k and optionally
      relaxed when a few opponents are persistent defectors), enters proportional punishment:
      defect for a number of rounds that grows with repeated violations but is capped.
    - Forgives when group cooperation recovers. Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.L = min(4, max(1, self.r - 1))
        self.base_threshold = 0.5
        raw_threshold = self.base_threshold - 0.25 * (self.k - 1.0) / max(1, self.n - 1)
        self.threshold = max(0.1, min(0.9, raw_threshold))
        self.max_base_punish = 3
        self.max_total_punish = 6
        self.forgiveness_reset = 1
        self.punish_until_round = 0
        self.consecutive_violations = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None or getattr(history, 'round_number', 0) == 0:
            return Action.C
        if t == self.r:
            return Action.D
        if t <= self.punish_until_round:
            return Action.D
        completed_rounds = int(history.round_number)
        lookback = min(self.L, completed_rounds)
        if lookback <= 0:
            return Action.C
        start_idx = completed_rounds - lookback
        end_idx = completed_rounds - 1
        total_fractions = []
        for idx in range(start_idx, end_idx + 1):
            opp_coops = int(np.sum(history.opponent_actions[idx, :]))
            self_coop = 1 if bool(history.my_actions[idx]) else 0
            total_cooperators = opp_coops + self_coop
            total_fractions.append(float(total_cooperators) / float(self.n))
        recent_coop_fraction = float(np.mean(total_fractions))
        threshold_effective = self.threshold
        try:
            if completed_rounds > 0:
                opp_totals = np.sum(history.opponent_actions[:completed_rounds, :], axis=0)
                opp_rates = opp_totals / float(completed_rounds)
                if np.any(opp_rates < 0.1):
                    threshold_effective = max(0.1, threshold_effective - 0.1)
        except Exception:
            threshold_effective = self.threshold
        if recent_coop_fraction >= threshold_effective:
            self.consecutive_violations = 0
            return Action.C
        else:
            self.consecutive_violations += 1
            punishment_length = min(self.max_total_punish, self.max_base_punish * self.consecutive_violations)
            self.punish_until_round = min(self.r - 1, t + int(punishment_length) - 1)
            return Action.D

class Strategy_COOPERATIVE_120(BaseStrategy):
    """
    Adaptive Forgiving Collective (AFC)

    - Starts by cooperating to signal intent.
    - Uses a majority threshold T = ceil(n/2) to judge if the group behaved 'well' in the previous round.
    - If the previous round had m_prev >= T cooperators, cooperate.
    - If the previous round had m_prev < T, start a proportional punishment:
        punish_length = min(max_punish, base_punish + punish_scale * (n - m_prev))
      Play D for the punishment length (starting immediately).
    - After a punishment finishes, enter a recovery mode where the strategy mostly cooperates
      but occasionally probes (cooperates) with probability probe_rate to test recovery.
      Probing decisions are deterministic given the public history (via a local RNG seeded
      from the public history).
    - On the last round, cooperate only if every previous round had full cooperation by all n players;
      otherwise defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.T = math.ceil(self.n / 2)
        self.base_punish = 1
        self.punish_scale = 1
        self.max_punish = max(1, math.floor(self.r / 4))
        self.probe_rate = 0.15
        self.punish_counter = 0
        self.in_recovery = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punish_counter = 0
            self.in_recovery = False
            return Action.C
        if getattr(self, 'punish_counter', 0) > 0:
            self.punish_counter -= 1
            if self.punish_counter == 0:
                self.in_recovery = True
            return Action.D
        current_round = int(state.round_number)
        if current_round == self.r:
            if history.round_number == 0:
                return Action.D
            opps = history.opponent_actions.astype(int)
            mine = history.my_actions.astype(int)
            totals = np.sum(opps, axis=1) + mine
            all_full = bool(np.all(totals == self.n))
            return Action.C if all_full else Action.D
        if history.round_number == 0:
            return Action.C
        last_my = int(history.my_actions[-1])
        last_opps_sum = int(np.sum(history.opponent_actions[-1, :]))
        m_prev = last_opps_sum + last_my
        if m_prev >= self.T:
            self.in_recovery = False
            return Action.C
        if self.in_recovery:
            try:
                my_arr = history.my_actions.astype(np.int64)
                opp_arr = history.opponent_actions.astype(np.int64).ravel()
                total_len = my_arr.size + opp_arr.size
                if total_len == 0:
                    seed = current_round
                else:
                    idx = np.arange(1, my_arr.size + 1, dtype=np.int64)
                    part1 = int(np.dot(my_arr, idx)) if my_arr.size > 0 else 0
                    idx2 = np.arange(1, opp_arr.size + 1, dtype=np.int64)
                    part2 = int(np.dot(opp_arr, idx2)) if opp_arr.size > 0 else 0
                    seed = part1 * 1000003 + part2 * 9176 + current_round * 131071 & 4294967295
            except Exception:
                seed = current_round * 1009
            rng = random.Random(seed)
            if rng.random() < self.probe_rate:
                return Action.C
        defectors_prev = self.n - m_prev
        punish_length = int(self.base_punish + self.punish_scale * defectors_prev)
        punish_length = min(self.max_punish, punish_length)
        punish_length = max(1, punish_length)
        self.punish_counter = punish_length
        self.in_recovery = False
        self.punish_counter -= 1
        if self.punish_counter == 0:
            self.in_recovery = True
        return Action.D

class Strategy_COOPERATIVE_121(BaseStrategy):
    """
    Cooperative, proportional-punishment public goods strategy.

    - Starts by cooperating.
    - Cooperates when a sufficiently large fraction of other players
      cooperated in the previous round (threshold gamma, derived from n and k).
    - If too few others cooperated, defects immediately and starts a finite,
      proportional punishment (length proportional to number of defectors,
      bounded by max_punish). Punishments are shortened in the final L rounds.
    - During punishment, usually defects but with probability phi cooperates
      as a forgiveness/test move.
    - If the recent environment appears persistently non-cooperative (average
      fraction of cooperating others over the last H rounds below gamma_low),
      switches to mostly defecting (but still allows occasional tests via phi).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        raw_gamma = 1.0 - (self.k - 1.0) / max(1.0, self.n - 1.0)
        self.gamma = min(0.95, max(0.5, raw_gamma))
        raw_phi = 0.08 + 0.32 * (self.k - 1.0) / max(1.0, self.n - 1.0)
        self.phi = min(0.5, max(0.05, raw_phi))
        self.lambda_ = 1.0
        self.max_punish = max(1, math.ceil(self.r / 4.0))
        self.L = min(3, max(1, math.floor(self.r / 10.0)))
        self.H = min(self.r, 5)
        self.gamma_low = 0.2
        self.punish_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        rounds_completed = history.round_number
        rounds_remaining = max(0, self.r - rounds_completed)
        endgame = rounds_remaining <= self.L
        lookback = min(self.H, history.round_number)
        if lookback > 0:
            frac_sum = 0.0
            for idx in range(-lookback, 0):
                coop_others = int(np.sum(history.opponent_actions[idx, :]))
                frac_sum += coop_others / max(1, self.n - 1)
            avg_frac_others = frac_sum / lookback
        else:
            avg_frac_others = 1.0
        if avg_frac_others < self.gamma_low:
            if random.random() < self.phi:
                return Action.C
            else:
                return Action.D
        if self.punish_timer > 0:
            self.punish_timer -= 1
            if random.random() < self.phi:
                return Action.C
            else:
                return Action.D
        if history.round_number == 0:
            return Action.C
        last_opponents = history.opponent_actions[-1, :]
        coop_others_last = int(np.sum(last_opponents))
        frac_others = coop_others_last / max(1, self.n - 1)
        if frac_others >= self.gamma:
            return Action.C
        else:
            num_defectors_last = self.n - 1 - coop_others_last
            proposed_punish = 1 + math.ceil(self.lambda_ * num_defectors_last)
            punish_len = min(proposed_punish, self.max_punish)
            if endgame:
                punish_len = min(punish_len, 1)
            self.punish_timer = max(0, int(punish_len) - 1)
            return Action.D

class Strategy_COOPERATIVE_122(BaseStrategy):
    """
    Forgiving Conditional Cooperator with Proportional Punishment (FCC-PP).
    Starts by signalling cooperation, conditions on recent fraction of others cooperating,
    punishes proportionally for a limited number of rounds, forgives if group recovers,
    and always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.pun_remaining = 0
        r = max(1, int(self.game_description.n_rounds))
        self.w = min(5, max(0, r - 1))
        self.base_theta = 0.5
        self.alpha = 0.4
        self.P_max = min(4, max(1, math.floor(r / 4)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        completed_rounds = 0 if history is None else int(history.round_number)
        t = completed_rounds + 1
        r = int(self.game_description.n_rounds)
        n = int(self.game_description.n_players)
        if t >= r:
            return Action.D
        if self.pun_remaining > 0:
            self.pun_remaining -= 1
            return Action.D
        if completed_rounds == 0:
            return Action.C
        w_act = min(self.w, completed_rounds)
        sum_other_coop = 0
        start_idx = completed_rounds - w_act
        for s in range(start_idx, completed_rounds):
            opps = int(np.sum(history.opponent_actions[s, :]))
            sum_other_coop += opps
        denom = (n - 1) * w_act
        p = sum_other_coop / denom if denom > 0 else 0.0
        theta = self.base_theta + self.alpha * ((t - 1) / max(1, r - 1))
        theta = max(0.05, min(0.95, theta))
        if p >= theta:
            return Action.C
        else:
            raw_len = math.ceil((theta - p) * self.P_max)
            pun_len = max(1, int(raw_len))
            self.pun_remaining = max(0, pun_len - 1)
            return Action.D

class Strategy_COOPERATIVE_123(BaseStrategy):
    """
    Graded Conditional Cooperator (GCC) for an N-player Public Goods Game.

    - Starts by cooperating, defects in the final round.
    - Uses a recent-window fraction of opponents' cooperation to decide.
    - Initiates graded, bounded punishments when the group falls below threshold T.
    - After a punishment period, performs a single cooperative probe.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, max(1, self.r - 1))
        T0 = 0.5
        denom = max(1, self.n - 1)
        self.T = max(0.2, T0 - 0.3 * (self.k - 1) / denom)
        self.Pmax = min(5, max(1, math.floor(self.r / 10)))
        self.punish_remaining = 0
        self.probe_next_after_punish = False

    def compute_recent_f(self, history: PlayerHistory, t: int) -> float:
        """
        Compute recent cooperation fraction f among other players over lookback rounds.
        t is the current round (1-indexed). We consider rounds max(1, t-W) .. t-1.
        If there are no lookback rounds return 1.0 to encourage cooperation.
        """
        start_round = max(1, t - self.W)
        end_round = t - 1
        start_idx = start_round - 1
        end_idx = end_round - 1
        if end_idx < start_idx:
            return 1.0
        recent = history.opponent_actions[start_idx:end_idx + 1, :]
        sum_coop = float(np.sum(recent))
        num_rounds = end_idx - start_idx + 1
        total_slots = num_rounds * (self.n - 1)
        if total_slots <= 0:
            return 1.0
        return sum_coop / float(total_slots)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            return Action.C
        if t == self.r:
            return Action.D
        if self.punish_remaining > 0:
            self.punish_remaining -= 1
            if self.punish_remaining == 0:
                self.probe_next_after_punish = True
            return Action.D
        if self.probe_next_after_punish:
            self.probe_next_after_punish = False
            return Action.C
        f = self.compute_recent_f(history, t)
        if f >= self.T:
            return Action.C
        deficit = self.T - f
        if self.T <= 0:
            P = 1
        else:
            scale = deficit / self.T if self.T > 0 else 1.0
            P = 1 + math.ceil(scale * (self.Pmax - 1))
        self.punish_remaining = max(0, P - 1)
        if self.punish_remaining == 0:
            self.probe_next_after_punish = True
        return Action.D

class Strategy_COOPERATIVE_124(BaseStrategy):
    """
    Pavlov-Probe strategy for the N-player Public Goods Game.

    - Starts by cooperating.
    - Uses a Pavlov (win-stay, lose-shift) rule comparing actual vs counterfactual payoff.
    - Measured forgiveness: if most other players cooperated last round, avoid switching C->D.
    - Probing: after a streak of defection, occasionally (probability eps) play C to test recovery.
    - Always defects on the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.forgiveness_threshold = 0.6
        self.eps = 0.05
        self.probe_wait = max(1, min(3, max(0, self.game_description.n_rounds - 2)))
        self.n = int(self.game_description.n_players)
        self.r = int(self.game_description.n_rounds)
        self.k = float(self.game_description.k)
        self._last_action_bool = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self._last_action_bool = True
            return Action.C
        next_round = history.round_number + 1
        if next_round == self.r:
            self._last_action_bool = False
            return Action.D
        if history.round_number == 0:
            c_my_last = 1
            opponents_last_coop = 0
        else:
            c_my_last = 1 if bool(history.my_actions[-1]) else 0
            opponents_last_coop = int(np.sum(history.opponent_actions[-1, :]))
        total_last = int(c_my_last + opponents_last_coop)
        pi_actual = 1.0 - float(c_my_last) + self.k / float(self.n) * float(total_last)
        c_alt = 1 - c_my_last
        total_alt = total_last + (c_alt - c_my_last)
        pi_alt = 1.0 - float(c_alt) + self.k / float(self.n) * float(total_alt)
        if pi_actual >= pi_alt:
            decision_bool = bool(c_my_last)
        else:
            decision_bool = bool(c_alt)
        if bool(c_my_last) and (not decision_bool):
            if self.n - 1 > 0:
                fraction_coop_others = (total_last - c_my_last) / float(self.n - 1)
            else:
                fraction_coop_others = 0.0
            if fraction_coop_others >= self.forgiveness_threshold:
                decision_bool = True
        defect_streak = 0
        for a in history.my_actions[::-1]:
            if bool(a) is False:
                defect_streak += 1
            else:
                break
        if not bool(c_my_last) and defect_streak >= self.probe_wait:
            if random.random() < self.eps:
                decision_bool = True
        self._last_action_bool = bool(decision_bool)
        return Action.C if decision_bool else Action.D

class Strategy_COOPERATIVE_125(BaseStrategy):
    """
    Gradual Conditional Cooperator with Proportional Punishment (GCCPP)

    - Start by cooperating.
    - For rounds 2..r-1: if currently in a punishment block, defect and decrement the block counter.
      Otherwise, inspect the previous round: if everyone cooperated, cooperate; if there were defections,
      start a proportional short punishment (length = number of defectors, capped).
    - Final round: cooperate only if all previous rounds were unanimous cooperation; otherwise defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_remaining = 0
        r = int(self.game_description.n_rounds)
        self.P_cap = min(3, max(0, r - 1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            self.punish_remaining = 0
            return Action.C
        current_round = history.round_number + 1
        r_total = int(self.game_description.n_rounds)
        n = int(self.game_description.n_players)
        if current_round == r_total:
            all_unanimous = True
            for ri in range(history.round_number):
                total_coops = int(bool(history.my_actions[ri])) + int(np.sum(history.opponent_actions[ri, :]))
                if total_coops != n:
                    all_unanimous = False
                    break
            return Action.C if all_unanimous else Action.D
        if self.punish_remaining > 0:
            self.punish_remaining -= 1
            return Action.D
        last_my_action = bool(history.my_actions[-1])
        last_opponent_coops = int(np.sum(history.opponent_actions[-1, :]))
        total_coops_prev = int(last_my_action) + last_opponent_coops
        count_defectors_total = n - total_coops_prev
        if count_defectors_total == 0:
            return Action.C
        else:
            punish_length = min(self.P_cap, count_defectors_total)
            self.punish_remaining = max(0, punish_length - 1)
            return Action.D

class Strategy_COOPERATIVE_126(BaseStrategy):
    """
    Adaptive Threshold Public-Goods strategy.

    - Start by cooperating to signal intent.
    - In non-final rounds, cooperate when the previous round's cooperation
      fraction meets an adaptive threshold tau (which decreases with k).
    - If the previous round falls below tau, punish (defect) for a short,
      proportional number of rounds (capped), unless recent trend is
      improving and a small forgiveness-probability check passes.
    - Always defect in the final round.
    - Maintains an internal punishment_timer to execute multi-round punishments.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_timer = 0
        self.max_punish = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punishment_timer = 0
            return Action.C
        completed_rounds = history.round_number
        current_round = completed_rounds + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        if current_round == r:
            self.punishment_timer = 0
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            return Action.D
        if completed_rounds == 0:
            return Action.C
        last_idx = completed_rounds - 1
        my_last = int(history.my_actions[last_idx])
        opp_last_sum = int(np.sum(history.opponent_actions[last_idx, :]))
        coop_count_prev = my_last + opp_last_sum
        f_prev = coop_count_prev / float(n)
        num_recent = min(3, completed_rounds)
        f_vals = []
        start_idx = completed_rounds - num_recent
        for idx in range(start_idx, completed_rounds):
            my_val = int(history.my_actions[idx])
            opp_sum = int(np.sum(history.opponent_actions[idx, :]))
            f_vals.append((my_val + opp_sum) / float(n))
        f_recent = float(np.mean(np.array(f_vals))) if len(f_vals) > 0 else f_prev
        normalized_k = 0.0
        if n > 1:
            normalized_k = (k - 1.0) / float(n - 1.0)
            if normalized_k < 0.0:
                normalized_k = 0.0
            elif normalized_k > 1.0:
                normalized_k = 1.0
        tau = 0.6 - 0.2 * normalized_k
        p_forgive = 0.05 + 0.25 * normalized_k
        if f_prev >= tau:
            self.punishment_timer = 0
            return Action.C
        else:
            shortfall = tau - f_prev
            punish_len = int(math.ceil(shortfall * 6.0))
            if punish_len < 1:
                punish_len = 1
            if punish_len > self.max_punish:
                punish_len = self.max_punish
            if f_recent >= tau - 0.1 and random.random() < p_forgive:
                return Action.C
            else:
                self.punishment_timer = max(0, punish_len - 1)
                return Action.D

class Strategy_COOPERATIVE_127(BaseStrategy):
    """
    Forgiving Conditional Cooperator (FCC)

    - Start cooperative.
    - In intermediate rounds, estimate fraction of OTHER players who cooperated
      over a short look-back window W = min(5, r-1).
    - If a majority (threshold f) of others cooperated recently, cooperate.
    - If you defected last round but all others cooperated then, show contrition
      and cooperate now.
    - Otherwise defect, but with small probability p_forgive probe by cooperating.
      If randomness is unavailable, probes are deterministic once every K rounds.
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = int(game_description.n_rounds)
        self.W = min(5, max(0, r - 1))
        self.f = 0.5
        self.p_forgive = 0.05
        self.f_contrition = 1.0
        self.f_low = 0.2
        self._deterministic_K = math.ceil(1.0 / self.p_forgive) if self.p_forgive > 0 else 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        if t == r:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.C
        start_round = max(1, t - self.W) if self.W > 0 else t
        start_idx = start_round - 1
        end_idx = t - 2
        rounds_counted = 0
        total_other_coops = 0
        if start_idx <= end_idx:
            slice_actions = history.opponent_actions[start_idx:end_idx + 1, :]
            total_other_coops = int(np.sum(slice_actions))
            rounds_counted = slice_actions.shape[0]
        if rounds_counted == 0 or n - 1 <= 0:
            AvgOtherCoop = 0.0
        else:
            AvgOtherCoop = total_other_coops / rounds_counted / float(n - 1)
        my_last_action_defected = history.my_actions[-1] == False
        last_other_coops = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size > 0 else 0
        if my_last_action_defected and last_other_coops == n - 1:
            return Action.C
        if AvgOtherCoop >= self.f:
            return Action.C
        try:
            if random.random() < self.p_forgive:
                return Action.C
            else:
                return Action.D
        except Exception:
            K = self._deterministic_K if self._deterministic_K > 0 else 1
            if t % K == 0:
                return Action.C
            return Action.D

class Strategy_COOPERATIVE_128(BaseStrategy):
    """
    Forgiving Conditional Cooperator (FCC)

    Starts by cooperating to build trust. Uses a moving window of recent rounds to
    measure the fraction of other players who cooperated. If that fraction meets
    a threshold (which depends on n and k) the strategy cooperates. If the
    observed cooperation falls below the threshold the strategy issues a short,
    proportional punishment (defection) whose length scales with how far the
    observed cooperation is below the threshold, capped by P_max. After a
    punishment phase finishes, FCC checks a forgiveness threshold to decide
    whether to resume cooperation or begin another short punishment. Defects in
    the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, max(1, math.floor(self.r / 10)))
        raw_theta = (self.n - self.k) / max(1, self.n - 1)
        self.theta = max(0.1, min(0.9, raw_theta))
        self.theta_forgive = max(0.5 * self.theta, self.theta - 0.15)
        self.P_max = max(1, math.floor(self.r / 10))
        self.S = 5
        self.punishment_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = history.round_number + 1
        if t == self.r:
            return Action.D

        def compute_O():
            rounds_completed = history.round_number
            if rounds_completed <= 0:
                return 1.0
            recent = min(self.W, rounds_completed)
            recent_actions = history.opponent_actions[-recent:, :]
            coop_counts = np.sum(recent_actions, axis=1).astype(float)
            fractions = coop_counts / float(max(1, self.n - 1))
            return float(np.mean(fractions))
        O = compute_O()
        if self.punishment_timer > 0:
            action = Action.D
            self.punishment_timer = max(0, self.punishment_timer - 1)
            if self.punishment_timer == 0:
                O_post = compute_O()
                if O_post < self.theta_forgive:
                    extra = 1 + math.ceil(max(0.0, self.theta - O_post) * self.S)
                    self.punishment_timer = int(min(self.P_max, extra))
            return action
        if O >= self.theta:
            return Action.C
        else:
            p_len = int(min(self.P_max, 1 + math.ceil(max(0.0, self.theta - O) * self.S)))
            self.punishment_timer = max(0, p_len - 1)
            if self.punishment_timer == 0:
                O_post = compute_O()
                if O_post < self.theta_forgive:
                    extra = 1 + math.ceil(max(0.0, self.theta - O_post) * self.S)
                    self.punishment_timer = int(min(self.P_max, extra))
            return Action.D

class Strategy_AGGRESSIVE_1(BaseStrategy):
    """
    Adaptive Exploitative Punisher (AEP)

    Aggressive-default public-goods strategy that:
    - Seeds cooperation for a few initial rounds to probe.
    - Defects by default outside seed/endgame.
    - Exploits reliably-cooperative groups by mostly defecting but occasionally cooperating to sustain exploitation.
    - Detects "sucker" events (was cooperative but got a lower payoff than others) and punishes for P rounds.
    - Estimates responsiveness (effect of my cooperation on others) and cooperates when expected discounted benefit outweighs immediate cost.
    - Always defects in the endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.S = min(3, max(1, math.floor(self.r / 10)))
        if self.S < 1:
            self.S = 1
        self.L = min(5, self.r)
        self.E = min(3, max(1, math.floor(self.r / 10)))
        if self.E < 1:
            self.E = 1
        self.TH_high = 0.75
        self.TH_mid = 0.4
        self.p_high = 0.7
        self.p_low = 0.2
        self.q_base = 0.15
        self.P = min(4, max(1, math.floor(self.r / 5)))
        self.alpha = 1.1
        self.punishment_timer = 0
        self.cooldown_until = 0
        self.eps = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t > self.r - self.E:
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            if self.punishment_timer == 0:
                self.cooldown_until = t + self.L
            return Action.D
        if t <= self.S:
            return Action.C
        if history is None:
            return Action.D
        n_opponents = self.n - 1
        win = min(self.L, history.round_number)
        opp_actions = np.array(history.opponent_actions)
        opp_payoffs = np.array(history.opponent_payoffs)
        my_actions = np.array(history.my_actions)
        my_payoffs = np.array(history.my_payoffs)
        if win <= 0:
            C_bar = 0.0
        else:
            recent_opp = opp_actions[-win:, :]
            C_bar = float(np.mean(recent_opp))
        effective_TH_high = self.TH_high
        if t <= self.cooldown_until:
            effective_TH_high = max(self.TH_high, self.p_high)
        if C_bar >= effective_TH_high:
            Rrem = self.r - t + 1
            prob_coop = self.q_base * (Rrem / float(self.r))
            if prob_coop < 0.0:
                prob_coop = 0.0
            elif prob_coop > 1.0:
                prob_coop = 1.0
            if random.random() < prob_coop:
                return Action.C
            else:
                return Action.D
        if history.round_number >= 1:
            last_idx = history.round_number - 1
            if bool(my_actions[last_idx]) is True:
                my_last_pay = float(my_payoffs[last_idx])
                med_opp = float(np.median(opp_payoffs[last_idx, :]))
                if my_last_pay < med_opp - self.eps:
                    self.punishment_timer = max(0, self.P - 1)
                    return Action.D
        fC_samples = []
        fD_samples = []
        rounds_completed = history.round_number
        for i in range(0, rounds_completed - 1):
            next_round_idx = i + 1
            frac_next_coop = float(np.mean(opp_actions[next_round_idx, :])) if n_opponents > 0 else 0.0
            if bool(my_actions[i]):
                fC_samples.append(frac_next_coop)
            else:
                fD_samples.append(frac_next_coop)
        min_obs = 2
        enough_data = len(fC_samples) >= min_obs and len(fD_samples) >= min_obs
        if enough_data:
            f_C = float(np.mean(fC_samples))
            f_D = float(np.mean(fD_samples))
            delta = f_C - f_D
            Rrem = self.r - t + 1
            expected_future_benefit = delta * (self.k / float(self.n)) * max(0, Rrem - 1)
            immediate_cost = 1.0 - self.k / float(self.n)
            if expected_future_benefit * self.alpha > immediate_cost:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_AGGRESSIVE_2(BaseStrategy):
    """
    Aggressive Prober–Exploit (APE)

    Defect by default, probe occasionally to detect stable cooperative clusters,
    exploit by defecting when others have shown sustained high cooperation,
    punish harshly if exploitation appears to trigger retaliation, then forgive
    and re-probe more cautiously. Always defect in the final round and in very
    short games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.W_max = max(1, min(5, max(0, self.r - 1)))
        self.T_exploit = 0.9
        self.M_stable = max(1, min(3, max(0, self.r - 1)))
        self.p_probe = 0.12
        self.L_base = 3
        self.delta_retaliation = 0.2
        self.endgame_margin = 1
        self.mode = 'NORMAL'
        self.punish_timer = 0
        self.last_action = None
        self.last_exploit_round = None
        self.last_exploit_checked = None
        self.escalation_multiplier = 1
        self.punish_cycles = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None or history.round_number == 0:
            if self.r <= 3 or t >= max(0, self.r - 1):
                self.last_action = Action.D
                return Action.D
            probe_prob = self.p_probe
            if random.random() < probe_prob:
                self.last_action = Action.C
                return Action.C
            else:
                self.last_action = Action.D
                return Action.D
        if t >= max(0, self.r - 1):
            self.last_action = Action.D
            return Action.D
        if self.r <= 3:
            self.last_action = Action.D
            return Action.D
        opp_actions = history.opponent_actions
        completed = history.round_number
        coop_fracs = np.array([])
        if completed > 0:
            coop_fracs = np.mean(opp_actions.astype(float), axis=1)
        W = min(self.W_max, max(1, completed))
        if completed == 0:
            recent_coop = 0.0
        else:
            start_idx = max(0, completed - W)
            recent_coop = float(np.mean(coop_fracs[start_idx:completed]))
        consecutive_high = 0
        if completed > 0:
            for v in coop_fracs[::-1]:
                if float(v) >= self.T_exploit:
                    consecutive_high += 1
                else:
                    break
        if self.last_exploit_round is not None and (self.last_exploit_checked is None or self.last_exploit_checked < self.last_exploit_round):
            s = self.last_exploit_round
            if completed - s >= 1:
                w = min(W, max(1, self.r - s))
                before_start = max(0, s - W)
                before_end = s
                if before_end - before_start <= 0:
                    recent_before = float(np.mean(coop_fracs[0:before_end])) if before_end > 0 else 0.0
                else:
                    recent_before = float(np.mean(coop_fracs[before_start:before_end]))
                after_end_idx = min(s + w, completed)
                if after_end_idx - s <= 0:
                    recent_after = 0.0
                else:
                    recent_after = float(np.mean(coop_fracs[s:after_end_idx]))
                drop = recent_before - recent_after
                rounds_of_drop = 0
                if after_end_idx - s > 0:
                    for v in coop_fracs[s:after_end_idx]:
                        if recent_before - float(v) >= self.delta_retaliation:
                            rounds_of_drop += 1
                if drop >= self.delta_retaliation and rounds_of_drop > 0:
                    base_timer = self.L_base * (1 + int(rounds_of_drop))
                    pun_len = int(max(1, base_timer * max(1, self.escalation_multiplier)))
                    self.punish_timer = pun_len
                    self.mode = 'PUNISH'
                    self.punish_cycles += 1
                    self.escalation_multiplier = min(16, self.escalation_multiplier * 2)
                self.last_exploit_checked = self.last_exploit_round
        if self.mode == 'PUNISH':
            if self.punish_timer > 0:
                self.punish_timer -= 1
            if self.punish_timer <= 0:
                self.mode = 'FORGIVE'
                self.punish_timer = 0
            self.last_action = Action.D
            return Action.D
        if self.mode == 'FORGIVE':
            probe_p = max(0.0, self.p_probe / 2.0)
        else:
            probe_p = self.p_probe
        if consecutive_high >= self.M_stable:
            self.last_exploit_round = t
            self.last_exploit_checked = None
            self.last_action = Action.D
            return Action.D
        if recent_coop < 0.15:
            self.last_action = Action.D
            return Action.D
        if random.random() < probe_p:
            self.last_action = Action.C
            return Action.C
        else:
            self.last_action = Action.D
            return Action.D

class Strategy_AGGRESSIVE_3(BaseStrategy):
    """
    Aggressive Opportunistic Free-Rider (AOFR)

    Default: defect. Aggressively exploit detectable cooperators, probe rarely to find
    new exploitable cooperators, and punish briefly if others punish us. Always defect
    in the final round and in tail-safe rounds near the end.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(6, max(1, math.floor(self.r / 5)))
        self.M = max(3, math.floor(self.r / 10))
        self.L = min(4, max(1, math.floor(self.r / 8)))
        self.tail = min(2, max(0, self.r - 1))
        self.G_high = 0.75
        self.G_probe_min = 0.2
        self.delta_punish = 0.25

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None or (history is not None and history.round_number == 0):
            return Action.D
        completed = int(history.round_number)
        if t >= self.r:
            return Action.D
        if t == 1:
            return Action.D
        if t > self.r - self.tail:
            return Action.D
        if completed >= 1:
            prev_opp_coops = int(np.sum(history.opponent_actions[completed - 1, :]))
            prev_my_coop = bool(history.my_actions[completed - 1])
            total_prev_coops = prev_opp_coops + (1 if prev_my_coop else 0)
            if total_prev_coops == self.n:
                return Action.D
        rounds_considered = min(self.W, completed)
        if rounds_considered <= 0:
            G_recent = 0.0
        else:
            start_idx = max(0, completed - rounds_considered)
            opp_slice = history.opponent_actions[start_idx:completed, :]
            G_recent = float(np.sum(opp_slice)) / (rounds_considered * (self.n - 1))
        if G_recent >= self.G_high:
            return Action.D
        detected_punish_rounds = []
        s_min = max(2, t - self.W)
        s_max = min(t - 1, completed)
        for s in range(s_min, s_max + 1):
            idx_s = s - 1
            if idx_s < 0 or idx_s >= completed:
                continue
            if bool(history.my_actions[idx_s]):
                continue
            idx_before = s - 2
            if idx_before < 0:
                continue
            coop_before = float(np.sum(history.opponent_actions[idx_before, :])) / (self.n - 1)
            if s + 1 < t and s < completed:
                idx_after = s
            else:
                idx_after = completed - 1
            if idx_after < 0 or idx_after >= completed:
                continue
            coop_after = float(np.sum(history.opponent_actions[idx_after, :])) / (self.n - 1)
            if coop_before - coop_after >= self.delta_punish:
                detected_punish_rounds.append(s)
        for s in detected_punish_rounds:
            punish_start = s + 1
            punish_end = s + self.L
            if t >= punish_start and t <= punish_end:
                return Action.D
        if self.M > 0 and t % self.M == 0 and (G_recent >= self.G_probe_min):
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_4(BaseStrategy):
    """
    Aggressive Opportunistic Defector (AOD)

    Aggressively favors defection. Uses a short sliding window of recent rounds to estimate
    how much others cooperate (P). If others are highly cooperative, AOD mostly defects but
    occasionally cooperates (maintenance) with a small probability to keep the public good
    alive. In mixed environments it probes rarely. If others mostly defect, it defects.
    It never cooperates in the final round. Implements a short hard-punishment after being
    exploited, and optional per-player targeting to permanently exploit near-unconditional
    cooperators.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.L = min(5, max(0, self.r - 1))
        self.high = 0.7
        self.low = 0.2
        self.p_min = 0.02
        self.p_max = 0.25
        self.probe_prob = 0.15
        self.final_round = self.r
        self.hard_defect_remaining = 0
        self.exploit_unconditional = False
        self._det_offset = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None or history.round_number == 0:
            return Action.D
        if t == self.final_round:
            return Action.D
        if self.hard_defect_remaining > 0:
            self.hard_defect_remaining -= 1
            return Action.D
        completed_rounds = history.round_number
        m = min(self.L, completed_rounds)
        if m <= 0:
            return Action.D
        opp_actions = history.opponent_actions
        available_rows = opp_actions.shape[0]
        m = min(m, available_rows)
        recent_opp = opp_actions[-m:, :]
        s_tau = np.sum(recent_opp.astype(float), axis=1)
        denom = max(1, self.n - 1)
        f_tau = s_tau / float(denom)
        P = float(np.mean(f_tau))
        rem = max(0, self.r - t + 1)
        per_player_rates = np.mean(recent_opp.astype(float), axis=0) if recent_opp.size else np.zeros((self.n - 1,))
        if np.any(per_player_rates > 0.9) and P < self.high:
            self.exploit_unconditional = True
        if self.exploit_unconditional:
            return Action.D
        last_f = float(np.sum(history.opponent_actions[-1, :]) / denom)
        if m >= 2:
            prev_f = float(np.mean(f_tau[:-1]))
        else:
            prev_f = last_f
        I_cooperated_last = bool(history.my_actions[-1])
        if I_cooperated_last and last_f < max(self.low, prev_f - 0.3):
            H = min(3, max(0, self.r - t + 1))
            self.hard_defect_remaining = H
            self.hard_defect_remaining = max(0, self.hard_defect_remaining - 1)
            return Action.D
        if P >= self.high:
            denom_range = max(1e-08, 1.0 - self.high)
            raw_p = self.p_min + (self.p_max - self.p_min) * ((P - self.high) / denom_range)
            p = max(self.p_min, min(self.p_max, raw_p))
            if rem <= 3:
                p = max(self.p_min / 2.0, p / 2.0)
            if random.random() < p:
                return Action.C
            else:
                return Action.D
        elif P > self.low and P < self.high:
            p = self.probe_prob * (P / self.high)
            if rem <= 3:
                p = p / 2.0
            if random.random() < p:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_AGGRESSIVE_5(BaseStrategy):
    """
    Aggressive Opportunist with Targeted Punishment (AOTP)

    - Probes with cooperation in round 1.
    - Tracks per-round fractions of other players cooperating and computes
      deltas after my C and D moves to estimate responsiveness.
    - Exploits large, stable cooperation when my cooperation does not
      materially increase others' cooperation.
    - Triggers sustained punishment (series of defections) when cooperation
      collapses or when others punish my defections strongly.
    - Forgives only after sustained recovery.
    - Always defects in final Tail_length rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.S_init = 1
        self.W = min(10, max(3, math.floor(self.r / 5)))
        self.p_exploit = 0.7
        self.p_low = 0.4
        self.p_recover = 0.75
        self.P_max = min(10, self.r)
        self.Tail_length = min(2, max(0, self.r - 1))
        self.eps = 0.02
        self.after_C_deltas = []
        self.after_D_deltas = []
        self.punishment_active = False
        self.P_remain = 0
        self.last_processed = 0
        self.recover_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if history is None:
            return Action.C if self.S_init == 1 else Action.D
        completed = history.round_number
        if completed >= 2:
            n_opponents = max(1, self.n - 1)
            process_up_to = completed - 1
            for u in range(self.last_processed, process_up_to):
                frac_u = float(np.sum(history.opponent_actions[u, :]) / n_opponents)
                frac_u1 = float(np.sum(history.opponent_actions[u + 1, :]) / n_opponents)
                delta = frac_u1 - frac_u
                if bool(history.my_actions[u]):
                    self.after_C_deltas.append(delta)
                else:
                    self.after_D_deltas.append(delta)
            self.last_processed = process_up_to
        W_eff = min(self.W, max(0, t - 1))
        if W_eff == 0:
            avg_others = 0.5
        else:
            start_idx = t - 1 - W_eff
            end_idx = t - 2
            start_idx = max(0, start_idx)
            end_idx = max(-1, end_idx)
            if end_idx < start_idx:
                avg_others = 0.5
            else:
                n_opponents = max(1, self.n - 1)
                fracs = []
                for j in range(start_idx, end_idx + 1):
                    fracs.append(float(np.sum(history.opponent_actions[j, :]) / n_opponents))
                avg_others = float(np.mean(fracs)) if len(fracs) > 0 else 0.5
        avg_delta_after_C = float(np.mean(self.after_C_deltas)) if len(self.after_C_deltas) > 0 else 0.0
        avg_delta_after_D = float(np.mean(self.after_D_deltas)) if len(self.after_D_deltas) > 0 else 0.0
        if self.Tail_length > 0 and t > self.r - self.Tail_length:
            return Action.D
        if self.punishment_active:
            if avg_others >= self.p_recover:
                self.recover_count += 1
            else:
                self.recover_count = 0
            if self.recover_count >= 2:
                self.punishment_active = False
                self.P_remain = 0
        if self.punishment_active and self.P_remain > 0:
            self.P_remain -= 1
            if self.P_remain <= 0:
                if avg_others < self.p_low and t < self.r:
                    remaining = max(0, self.r - t)
                    if remaining > 0:
                        self.P_remain = min(self.P_max, remaining)
                        self.punishment_active = True
                    else:
                        self.punishment_active = False
                        self.P_remain = 0
                else:
                    self.punishment_active = False
                    self.P_remain = 0
            return Action.D
        trigger = False
        if avg_others < self.p_low:
            trigger = True
        recently_defected = False
        if completed >= 1:
            recently_defected = not bool(history.my_actions[-1])
        if recently_defected and avg_delta_after_D < avg_delta_after_C - 2.0 * self.eps:
            trigger = True
        if trigger:
            remaining_rounds_in_game = max(0, self.r - t + 1)
            self.P_remain = min(self.P_max, remaining_rounds_in_game)
            if self.P_remain <= 0:
                self.punishment_active = False
                self.P_remain = 0
                return Action.D
            self.punishment_active = True
            self.P_remain -= 1
            if self.P_remain <= 0:
                if avg_others < self.p_low and t < self.r:
                    remaining = max(0, self.r - t)
                    if remaining > 0:
                        self.P_remain = min(self.P_max, remaining)
                        self.punishment_active = True
                    else:
                        self.punishment_active = False
                        self.P_remain = 0
                else:
                    self.punishment_active = False
                    self.P_remain = 0
            return Action.D
        if avg_others >= self.p_exploit:
            if avg_delta_after_C <= avg_delta_after_D + self.eps:
                return Action.D
            else:
                return Action.C
        if avg_others >= self.p_low:
            if avg_delta_after_C > avg_delta_after_D + self.eps:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_6(BaseStrategy):
    """
    Aggressive Conditional Exploiter (ACE)

    - Probes briefly by cooperating to detect conditional cooperators.
    - If a reliable cluster S of opponents is found (they cooperate more after I cooperate),
      performs short defect "exploit bursts" of length E to harvest one-shot gains, then
      cooperates for R rounds to rebuild. If opponents retaliate (cooperation of S drops
      by drop_threshold), switch to permanent defection.
    - Always defect in the final round and in very short/low-k games.
    - Uses Laplace smoothing to estimate conditional cooperation probabilities.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.mode = 'probing'
        self.exploit_remaining = 0
        self.rebuild_remaining = 0
        self.burst_start_round = None
        self.pre_burst_coop = None
        self.current_S = []
        n = game_description.n_players
        r = game_description.n_rounds
        k = game_description.k
        self.P = min(3, max(1, math.floor(0.1 * r)))
        self.delta_min = 0.1
        self.p_min = 0.5
        self.tau_high = 0.65
        self.tau_low = 0.2
        self.E = min(3, max(1, int(round(k))))
        self.R = max(1, self.E)
        self.drop_threshold = 0.2
        self.min_S_size = max(1, math.ceil((n - 1) * 0.2))
        self.n_players = n
        self.n_rounds = r
        self.k = k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
            completed_rounds = 0
        else:
            completed_rounds = history.round_number
            t = completed_rounds + 1
        if t == self.n_rounds:
            self.mode = 'permanent_defect'
            return Action.D
        if self.n_rounds <= 3 or self.k <= 1.1:
            self.mode = 'permanent_defect'
            return Action.D
        if self.mode == 'permanent_defect':
            return Action.D
        if t <= self.P:
            self.mode = 'probing'
            return Action.C
        if history is None:
            return Action.D
        T = completed_rounds
        window_size = min(3, max(1, T))
        if T == 0:
            others_recent = 0.0
        else:
            recent_slice = history.opponent_actions[-window_size:, :]
            others_recent = float(np.mean(recent_slice.astype(float)))
        if others_recent <= self.tau_low:
            self.mode = 'permanent_defect'
            return Action.D
        if T <= 1:
            self.mode = 'idle'
            return Action.D
        prev_my = history.my_actions[:-1]
        opp_count = history.opponent_actions.shape[1]
        p_after_C = np.zeros(opp_count, dtype=float)
        p_after_D = np.zeros(opp_count, dtype=float)
        delta = np.zeros(opp_count, dtype=float)
        for j in range(opp_count):
            opp_curr = history.opponent_actions[1:, j]
            mask_C = prev_my
            mask_D = ~prev_my
            count_prev_C = int(np.sum(mask_C))
            count_prev_D = int(np.sum(mask_D))
            coop_after_my_C = int(np.sum(opp_curr[mask_C].astype(int))) if count_prev_C > 0 else 0
            coop_after_my_D = int(np.sum(opp_curr[mask_D].astype(int))) if count_prev_D > 0 else 0
            pC = (coop_after_my_C + 1) / (count_prev_C + 2)
            pD = (coop_after_my_D + 1) / (count_prev_D + 2)
            p_after_C[j] = pC
            p_after_D[j] = pD
            delta[j] = pC - pD
        S_indices = [j for j in range(opp_count) if delta[j] >= self.delta_min and p_after_C[j] >= self.p_min]
        self.current_S = S_indices
        if len(S_indices) == 0:
            self.mode = 'idle'
            return Action.D
        avg_S = float(np.mean(p_after_C[S_indices])) if len(S_indices) > 0 else 0.0
        rounds_left = self.n_rounds - t + 1
        enough_rounds_for_cycle = rounds_left > self.E + self.R
        if self.mode == 'exploit':
            if self.exploit_remaining > 0:
                self.exploit_remaining -= 1
                if self.exploit_remaining == 0:
                    self.mode = 'post_exploit_evaluate'
                return Action.D
            else:
                self.mode = 'idle'
                return Action.D
        if self.mode == 'post_exploit_evaluate':
            T_now = history.round_number
            if T_now == 0:
                post_burst_coop = 0.0
            else:
                ws = min(3, T_now)
                recent = history.opponent_actions[-ws:, :]
                if len(self.current_S) == 0:
                    post_burst_coop = 0.0
                else:
                    post_burst_coop = float(np.mean(recent[:, self.current_S].astype(float)))
            pre = self.pre_burst_coop if self.pre_burst_coop is not None else 0.0
            if pre - post_burst_coop >= self.drop_threshold:
                self.mode = 'permanent_defect'
                return Action.D
            else:
                self.mode = 'rebuild'
                self.rebuild_remaining = self.R
                return Action.C
        if self.mode == 'rebuild':
            if self.rebuild_remaining > 0:
                self.rebuild_remaining -= 1
                if self.rebuild_remaining == 0:
                    self.mode = 'idle'
                return Action.C
            else:
                self.mode = 'idle'
                return Action.D
        if avg_S >= self.tau_high and enough_rounds_for_cycle and (len(S_indices) >= self.min_S_size):
            T_now = history.round_number
            if T_now == 0:
                self.pre_burst_coop = 0.0
            else:
                ws = min(3, T_now)
                recent = history.opponent_actions[-ws:, :]
                if len(S_indices) == 0:
                    self.pre_burst_coop = 0.0
                else:
                    self.pre_burst_coop = float(np.mean(recent[:, S_indices].astype(float)))
            self.burst_start_round = t
            self.exploit_remaining = self.E - 1
            self.mode = 'exploit'
            return Action.D
        self.mode = 'idle'
        return Action.D

class Strategy_AGGRESSIVE_7(BaseStrategy):
    """
    Aggressive Probe-and-Grim (APG)

    - Default: defect every round.
    - Early probing: with small probability eps cooperate in early rounds to find unconditional cooperators.
    - Identify reliable cooperators by recent cooperation rate (window w, threshold theta).
    - If reliable cooperators exist (>= s_min) exploit them by defecting.
    - Monitor for retaliation by comparing consecutive recent windows; if coordinated retaliation is detected,
      set grim_flag and defect permanently (except final-round logic remains D).
    - Always defect in the final H rounds (including the last round).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.grim_flag = False
        r = int(self.game_description.n_rounds)
        self.w = int(min(5, max(1, math.floor(r / 10))))
        self.theta = 0.8
        self.s_min = 1
        self.eps = min(0.05, 5.0 / max(1.0, r))
        self.H = int(min(3, max(1, r - 1)))
        self.retaliation_drop = 0.3
        self.retaliation_count_threshold = int(math.ceil((self.game_description.n_players - 1) / 3.0))
        self.retaliation_fraction_threshold = 0.5
        self.probe_phase_limit = int(max(3, math.floor(r / 10)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
            completed = 0
        else:
            completed = history.round_number
            t = completed + 1
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        if t >= r:
            return Action.D
        if t > r - self.H:
            return Action.D
        if self.grim_flag:
            return Action.D
        if history is None:
            return Action.D
        opp_actions = history.opponent_actions
        num_opponents = n - 1
        total_completed = completed
        w_recent = min(self.w, total_completed)
        if w_recent <= 0:
            coop_rates_recent = np.zeros(num_opponents, dtype=float)
        else:
            recent_slice = opp_actions[-w_recent:, :]
            coop_rates_recent = np.mean(recent_slice.astype(float), axis=0)
        if total_completed > 0:
            coop_rates_all = np.mean(opp_actions.astype(float), axis=0)
        else:
            coop_rates_all = np.zeros(num_opponents, dtype=float)
        S_reliable_idx = [j for j in range(num_opponents) if coop_rates_recent[j] >= self.theta]
        if len(S_reliable_idx) == 0:
            if t <= self.probe_phase_limit:
                if random.random() < self.eps:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D
        action_to_play = Action.D
        retaliators = set()
        if total_completed >= 1 and w_recent >= 1:
            remaining = total_completed - w_recent
            if remaining >= 1:
                w_prev = min(self.w, remaining)
                prev_slice = opp_actions[-(w_recent + w_prev):-w_recent, :] if w_prev > 0 else np.zeros((0, num_opponents))
                if w_prev > 0:
                    prev_rates = np.mean(prev_slice.astype(float), axis=0)
                else:
                    prev_rates = coop_rates_all
            else:
                prev_rates = coop_rates_all
            for j in S_reliable_idx:
                prev_rate_j = float(prev_rates[j]) if prev_rates.size > j else float(coop_rates_all[j])
                recent_rate_j = float(coop_rates_recent[j]) if coop_rates_recent.size > j else float(coop_rates_all[j])
                if prev_rate_j - recent_rate_j >= self.retaliation_drop:
                    retaliators.add(j)
        n_ret = len(retaliators)
        cond1 = n_ret >= self.retaliation_count_threshold and self.retaliation_count_threshold > 0
        cond2 = False
        if len(S_reliable_idx) > 0:
            if n_ret / float(len(S_reliable_idx)) >= self.retaliation_fraction_threshold:
                cond2 = True
        if cond1 or cond2:
            self.grim_flag = True
            action_to_play = Action.D
        return action_to_play

class Strategy_AGGRESSIVE_8(BaseStrategy):
    """
    Aggressive Conditional Exploiter (ACE)

    Aggressive default: defect on first and last rounds and by default otherwise.
    Exploits large cooperative majorities when opponents rarely punish defections.
    If many opponents punish your defections and there are enough rounds remaining,
    switch to cooperating to sustain mutual cooperation, but require strong evidence
    (sustained cooperation and remaining-horizon profitability). Forgiveness is
    conservative: only return to cooperation after strong, sustained cooperative
    signals and the horizon test.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None or history.round_number == 0:
            return Action.D
        if t >= self.r:
            return Action.D
        completed = int(history.round_number)
        R_rem = float(self.r - t + 1)
        cost_now = 1.0 - self.k / float(self.n)
        R_needed = cost_now / (self.k - 1.0)
        if completed >= 1:
            last_round_others_coop = int(np.sum(history.opponent_actions[-1, :]))
        else:
            last_round_others_coop = 0
        if completed >= 2:
            both = np.logical_and(history.opponent_actions[-2, :], history.opponent_actions[-1, :])
            last_two_rounds_others_coop = int(np.sum(both))
        else:
            last_two_rounds_others_coop = 0
        total_my_defections = 0
        count_defections_after_my_D = 0
        if completed >= 2:
            for s in range(0, completed - 1):
                my_action_s = bool(history.my_actions[s])
                if not my_action_s:
                    total_my_defections += 1
                    opponents_next = history.opponent_actions[s + 1, :]
                    defections_next = self.n - 1 - int(np.sum(opponents_next))
                    count_defections_after_my_D += defections_next
        if total_my_defections == 0:
            immediate_retaliation_rate = 0.0
        else:
            denom = float((self.n - 1) * total_my_defections)
            immediate_retaliation_rate = float(count_defections_after_my_D) / denom if denom > 0 else 0.0
        EXPLOITIVE_COOP_MAJ = math.ceil(0.8 * (self.n - 1))
        SUSTAIN_COOP_MAJ = math.ceil(0.6 * (self.n - 1))
        FORGIVE_COOP_MAJ = math.ceil(0.75 * (self.n - 1))
        PUNISHER_RATE = 0.5
        if last_round_others_coop >= EXPLOITIVE_COOP_MAJ and immediate_retaliation_rate < PUNISHER_RATE:
            return Action.D
        if immediate_retaliation_rate >= PUNISHER_RATE:
            if R_rem > R_needed and last_two_rounds_others_coop >= SUSTAIN_COOP_MAJ:
                return Action.C
            if last_two_rounds_others_coop >= FORGIVE_COOP_MAJ and R_rem > R_needed:
                return Action.C
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_9(BaseStrategy):
    """
    Ruthless Unanimity + Exploit-and-Punish (RUEP)

    - Signals cooperation in the first round.
    - If the group ever breaks unanimous cooperation, immediately begin a punishment
      sequence of length T (proportional to r) by defecting for T rounds (unless
      cleared by sustained unanimous cooperation).
    - After L >= U consecutive unanimous-cooperation rounds, do a one-shot exploit
      (defect) and then enter punishment mode.
    - Require S consecutive unanimous-cooperation rounds to clear punishment and
      resume trusting/cooperating.
    - Always defect in the final E rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.T = max(1, math.floor(0.3 * self.r))
        self.S = max(2, math.ceil(0.1 * self.r))
        self.U = max(3, math.ceil(0.15 * self.r))
        self.E = 1
        self.remaining_punish = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        completed = history.round_number
        t = completed + 1

        def _is_unanimous_at_index(idx: int) -> bool:
            if idx < 0 or idx >= completed:
                return False
            my_c = bool(history.my_actions[idx])
            opp_all_c = True
            if history.opponent_actions.size != 0:
                opp_all_c = bool(np.all(history.opponent_actions[idx, :]))
            return my_c and opp_all_c

        def _consec_unanimous_before_t() -> int:
            count = 0
            idx = completed - 1
            while idx >= 0 and _is_unanimous_at_index(idx):
                count += 1
                idx -= 1
            return count
        if t > self.r - self.E:
            return Action.D
        if self.remaining_punish > 0:
            self.remaining_punish -= 1
            if completed >= self.S:
                all_unanimous = True
                start_idx = completed - self.S
                for idx in range(start_idx, completed):
                    if not _is_unanimous_at_index(idx):
                        all_unanimous = False
                        break
                if all_unanimous:
                    self.remaining_punish = 0
            return Action.D
        prev_idx = completed - 1
        prev_unanimous = _is_unanimous_at_index(prev_idx)
        if not prev_unanimous:
            self.remaining_punish = self.T
            return Action.D
        L = _consec_unanimous_before_t()
        if L >= self.U:
            self.remaining_punish = self.T
            return Action.D
        return Action.C

class Strategy_AGGRESSIVE_10(BaseStrategy):
    """
    Aggressive Conditional Punisher (ACP)

    - Tries to establish full-group cooperation when it is credible (enough rounds remain).
    - Uses a grim-style permanent punishment when defections are observed (tolerating up to F defections).
    - Optional re-entry probe after P punishment rounds (default: never probe).
    - Defects in the terminal round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.F = 0
        self.P = math.inf
        self.punished = False
        self.punishment_since = 0
        self.just_probed = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.punished = False
            self.punishment_since = 0
            self.just_probed = False
            t = 1
            m = self.r - t + 1
            if m == 1:
                return Action.D
            M_min = math.ceil(1.0 + (1.0 - self.k / self.n) / (self.k - 1.0))
            if m >= M_min:
                return Action.C
            else:
                return Action.D
        completed_rounds = history.round_number
        t = completed_rounds + 1
        m = self.r - t + 1
        if self.just_probed:
            last_idx = completed_rounds - 1
            if last_idx >= 0:
                my_coop = bool(history.my_actions[last_idx])
                opp_coops = int(np.sum(history.opponent_actions[last_idx, :]))
                total_coops = int(my_coop) + int(opp_coops)
                if total_coops == self.n:
                    self.punished = False
                    self.punishment_since = 0
                else:
                    self.punished = True
                    self.punishment_since = 0
            self.just_probed = False
        if m == 1:
            if self.punished:
                self.punishment_since += 1
            return Action.D
        if self.punished:
            self.punishment_since += 1
            if math.isfinite(self.P) and self.punishment_since >= self.P:
                self.just_probed = True
                return Action.C
            else:
                return Action.D
        defections_seen = 0
        for i in range(completed_rounds):
            my_coop = bool(history.my_actions[i])
            opp_coops = int(np.sum(history.opponent_actions[i, :]))
            total_coops = int(my_coop) + int(opp_coops)
            if total_coops < self.n:
                defections_seen += 1
        if defections_seen > self.F:
            self.punished = True
            self.punishment_since = 0
            return Action.D
        M_min = math.ceil(1.0 + (1.0 - self.k / self.n) / (self.k - 1.0))
        if m >= M_min:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_11(BaseStrategy):
    """
    Aggressive Conditional Exploiter (ACE)

    - Starts with a small number of cooperative "grace" rounds to seed cooperation.
    - Predicts each opponent's cooperation rate and exploits when a predicted majority of
      opponents are likely to cooperate by defecting to free-ride.
    - If observed cooperation falls below a threshold, switches to a sustained punishment
      phase (always defect) for a computed number of rounds. Repeated triggers escalate
      punishment length up to a cap.
    - After a punishment phase, enters a probation period during which it only cooperates
      if recent actual cooperation meets the stricter requirement. If probation succeeds,
      resumes ATTEMPT_COOP behavior.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.Mode = 'ATTEMPT_COOP'
        self.punish_timer = 0
        self.probation_timer = 0
        self.exploitation_attempted_last_round = False
        self.punish_escalator_count = 0
        self.p_thresh = 0.65
        self.exploit_threshold_fraction = 0.5
        self.base_punish_length_fraction = 0.25
        self.max_punish = 6
        self.probation_length = 3
        self.grace_rounds = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
            completed_rounds = 0
        else:
            completed_rounds = history.round_number
            t = completed_rounds + 1
        r = self.r
        n = self.n
        r_remaining = max(1, r - t + 1)
        if t == r:
            self.exploitation_attempted_last_round = False
            return Action.D
        if t <= self.grace_rounds and history is None:
            self.exploitation_attempted_last_round = False
            return Action.C
        if t <= self.grace_rounds and history is not None and (completed_rounds < self.grace_rounds):
            self.exploitation_attempted_last_round = False
            return Action.C
        needed = math.ceil((n - 1) * self.exploit_threshold_fraction)
        if self.punish_timer > 0:
            self.punish_timer -= 1
            self.exploitation_attempted_last_round = False
            if self.punish_timer == 0:
                self.probation_timer = self.probation_length
            return Action.D
        coop_rates = None
        predicted_cooperators = 0
        if history is None or completed_rounds == 0:
            coop_rates = np.zeros((n - 1,), dtype=float)
            predicted_cooperators = 0
        else:
            coop_counts = np.sum(history.opponent_actions, axis=0).astype(float)
            coop_rates = coop_counts / float(completed_rounds)
            predicted_cooperators = int(np.sum(coop_rates >= self.p_thresh))
        if self.probation_timer > 0:
            if history is None or completed_rounds == 0:
                last_round_actual_cooperators = 0
            else:
                last_round_actual_cooperators = int(np.sum(history.opponent_actions[-1, :]))
            if last_round_actual_cooperators >= needed:
                action = Action.C
            else:
                action = Action.D
            self.probation_timer -= 1
            if self.probation_timer == 0:
                self.Mode = 'ATTEMPT_COOP'
            self.exploitation_attempted_last_round = False
            return action
        if self.Mode != 'ATTEMPT_COOP':
            self.Mode = 'ATTEMPT_COOP'
        if predicted_cooperators >= needed:
            action = Action.D
            exploitation_attempted_this_round = True
        else:
            action = Action.C
            exploitation_attempted_this_round = False
        if completed_rounds > 0:
            last_actual = int(np.sum(history.opponent_actions[-1, :]))
            if last_actual < needed:
                base_length = math.ceil(self.base_punish_length_fraction * r_remaining)
                base_length = max(1, base_length)
                punish_length = base_length
                if self.exploitation_attempted_last_round or self.punish_escalator_count > 0:
                    punish_length = int(math.ceil(punish_length * (1 + float(self.punish_escalator_count))))
                punish_length = min(self.max_punish, punish_length)
                self.punish_timer = int(punish_length)
                self.punish_escalator_count += 1
                self.Mode = 'PUNISH'
                self.exploitation_attempted_last_round = False
                return Action.D
        self.exploitation_attempted_last_round = exploitation_attempted_this_round
        return action

class Strategy_AGGRESSIVE_12(BaseStrategy):
    """
    Aggressive Opportunistic Exploiter (AOE)

    - Default: defect.
    - Exploit when a large fraction of opponents have cooperated recently.
    - If exploited (you cooperated while at least one opponent defected and you would have been better off defecting),
      trigger a multi-round punishment (defect for punish_len rounds). Repeated exploitation resets the timer.
    - Occasionally attempt a short rebuild (single cooperative probe) when recent cooperation is moderate.
    - Always defect in a short endgame or very short games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, max(0, self.r - 1))
        self.coop_high = 0.6
        self.coop_low = 0.3
        self.punish_len = max(2, math.ceil(self.r / 4))
        self.rebuild_burst = 1
        self.endgame_horizon = 2
        self.punish_timer = 0
        self.last_rebuild_round = -10 ** 9

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = self.n
        r = self.r
        if history is None:
            return Action.D
        rounds_completed = int(history.round_number)
        if rounds_completed >= 1:
            last_idx = rounds_completed - 1
            my_prev_action = bool(history.my_actions[last_idx])
            if my_prev_action:
                opp_coops = float(np.sum(history.opponent_actions[last_idx, :]))
                S_t = opp_coops + 1.0
                pi_if_coop = self.k * S_t / n
                pi_if_defect = 1.0 + self.k * (S_t - 1.0) / n
                if pi_if_defect > pi_if_coop:
                    self.punish_timer = self.punish_len
        if r <= 3:
            if self.punish_timer > 0:
                self.punish_timer -= 1
            return Action.D
        if t > r - self.endgame_horizon:
            if self.punish_timer > 0:
                self.punish_timer -= 1
            return Action.D
        if self.punish_timer > 0:
            self.punish_timer -= 1
            return Action.D
        start_idx = max(0, rounds_completed - self.W)
        end_idx = rounds_completed
        if end_idx <= start_idx:
            E = 0.0
        else:
            opp_slice = history.opponent_actions[start_idx:end_idx, :]
            if opp_slice.size == 0:
                E = 0.0
            else:
                opp_counts = np.sum(opp_slice, axis=1).astype(float)
                denom = float(max(1, n - 1))
                E = float(np.mean(opp_counts / denom))
        remaining_rounds = r - t + 1
        if E >= self.coop_high and remaining_rounds >= 3:
            return Action.D
        if E <= self.coop_low:
            return Action.D
        probe_interval = max(3, self.W)
        if t - self.last_rebuild_round >= probe_interval and remaining_rounds >= 3:
            self.last_rebuild_round = t
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_13(BaseStrategy):
    """
    Adaptive Exploitative Bait-and-Break (AEBB)

    Aggressive, data-driven public-goods strategy:
    - Short deterministic exploration (alternating C/D) to collect responsiveness data.
    - Estimate how others' cooperation responds to our cooperations vs defections.
    - If cooperating yields a positive, reliable uplift (delta), calculate whether a single
      cooperation "signal" can be recouped by exploiting the uplift in future rounds.
    - If profitable, run cycles: one C (signal) followed by S defections (exploit).
    - Monitor signal effectiveness; switch to permanent defection if responsiveness is absent
      or exploitation is no longer profitable. Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.cost = 1.0 - self.k / float(self.n)
        self.E = min(math.ceil(0.2 * self.r), 10)
        self.mode = 'EXPLORATION'
        self.cycle_pos = 0
        self.S = 1
        self.consecutive_ineffective = 0
        self.last_signal_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory):
        t = int(state.round_number)
        if t == self.r:
            self.mode = 'PERMANENT_DEFECT'
            return Action.D
        if history is None:
            self.mode = 'EXPLORATION'
            self.cycle_pos = 0
            self.consecutive_ineffective = 0
            self.last_signal_round = None
            return Action.C if t % 2 == 1 else Action.D
        if self.mode == 'EXPLORATION' and t <= self.E:
            return Action.C if t % 2 == 1 else Action.D
        rounds_completed = int(history.round_number)
        my_actions = np.asarray(history.my_actions, dtype=bool)
        opp_actions = np.asarray(history.opponent_actions, dtype=bool)
        sum_coops_after_coop = 0
        count_after_coop = 0
        sum_coops_after_def = 0
        count_after_def = 0
        for u_idx in range(1, rounds_completed):
            prev_my = bool(my_actions[u_idx - 1])
            others_coop = int(np.sum(opp_actions[u_idx, :])) if opp_actions.size != 0 else 0
            if prev_my:
                sum_coops_after_coop += others_coop
                count_after_coop += 1
            else:
                sum_coops_after_def += others_coop
                count_after_def += 1
        opponents = self.n - 1
        if opponents <= 0:
            self.mode = 'PERMANENT_DEFECT'
            return Action.D
        if count_after_coop > 0:
            p_after_coop = sum_coops_after_coop / (opponents * count_after_coop)
        else:
            p_after_coop = 0.0
        if count_after_def > 0:
            p_after_def = sum_coops_after_def / (opponents * count_after_def)
        else:
            p_after_def = 0.0
        if count_after_coop < 2 or count_after_def < 2:
            delta = max(0.0, p_after_coop - p_after_def - 0.02)
        else:
            delta = p_after_coop - p_after_def
        if delta <= 0.0:
            self.mode = 'PERMANENT_DEFECT'
            return Action.D
        B_per_round = self.k / float(self.n) * opponents * delta
        T_rem = self.r - t
        if B_per_round * T_rem <= self.cost:
            self.mode = 'PERMANENT_DEFECT'
            return Action.D
        self.mode = 'EXPLOITATION'
        self.S = max(1, int(math.ceil(self.cost / (B_per_round + 1e-12))))
        if rounds_completed >= 2:
            if bool(my_actions[-2]) is True:
                response_coops = int(np.sum(opp_actions[-1, :])) if opp_actions.size != 0 else 0
                baseline = p_after_def * opponents
                if response_coops < baseline + 1 - 1e-09:
                    self.consecutive_ineffective += 1
                else:
                    self.consecutive_ineffective = 0
                if self.consecutive_ineffective >= 2:
                    self.mode = 'PERMANENT_DEFECT'
                    return Action.D
        T_rem_check = self.r - t
        if B_per_round * T_rem_check <= self.cost:
            self.mode = 'PERMANENT_DEFECT'
            return Action.D
        if self.cycle_pos is None:
            self.cycle_pos = 0
        if self.cycle_pos == 0:
            if B_per_round * T_rem_check <= self.cost:
                self.mode = 'PERMANENT_DEFECT'
                return Action.D
            self.last_signal_round = t
            self.cycle_pos = 1
            return Action.C
        else:
            action = Action.D
            self.cycle_pos += 1
            if self.cycle_pos > self.S:
                self.cycle_pos = 0
            return action

class Strategy_AGGRESSIVE_14(BaseStrategy):
    """
    Aggressive Trigger with Escalating Punishment (ATEP)

    - Starts cooperating (unless in an endgame window).
    - Any observed defection (>= D_tol) triggers an immediate punishment period.
    - Base punishment length P0 = max(1, ceil(gamma * (n - k) / k)) with gamma=2.
    - Punishment length scales with the number of defectors: P0 * num_defectors.
    - If defections occur during punishment, punishment is extended by P0 * num_defectors (capped by remaining rounds).
    - After punishment ends, require G consecutive full-cooperation rounds to remain in COOP.
    - If remaining rounds <= E (E = P0) then defect unconditionally (endgame exploitation).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = int(self.game_description.n_players)
        k = float(self.game_description.k)
        r = int(self.game_description.n_rounds)
        self.n = n
        self.k = k
        self.r = r
        self.gamma = 2.0
        self.G = 2
        self.D_tol = 1
        p0_raw = self.gamma * (self.n - self.k) / self.k
        self.P0 = max(1, int(math.ceil(p0_raw)))
        self.E = self.P0
        self.mode = 'COOP'
        self.punish_countdown = 0
        self.consecutive_full_coop = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide Action.C or Action.D for the current round.
        state.round_number is 1-based current round index (first round = 1).
        history is None on the first move; otherwise it contains completed rounds data.
        """
        t = getattr(state, 'round_number', 1)
        remaining = max(0, self.r - (t - 1))
        if remaining <= self.E:
            self.mode = 'PUNISH'
            self.punish_countdown = remaining
            return Action.D
        if history is None or history.round_number == 0:
            coop_last = self.n
        else:
            opp_last = history.opponent_actions[-1, :]
            my_last = history.my_actions[-1]
            coop_last = int(np.sum(opp_last) + (1 if bool(my_last) else 0))
        if self.mode == 'PUNISH':
            action = Action.D
            if coop_last < self.n:
                num_def = self.n - coop_last
                extension = self.P0 * max(1, int(num_def))
                self.punish_countdown = min(self.punish_countdown + extension, remaining)
            self.punish_countdown = max(0, int(self.punish_countdown) - 1)
            if self.punish_countdown <= 0:
                self.mode = 'COOP'
                self.punish_countdown = 0
                self.consecutive_full_coop = 0
            return action
        if coop_last < self.n:
            num_def = self.n - coop_last
            self.punish_countdown = min(remaining, self.P0 * max(1, int(num_def)))
            self.mode = 'PUNISH'
            self.consecutive_full_coop = 0
            return Action.D
        self.consecutive_full_coop += 1
        if self.consecutive_full_coop >= self.G:
            pass
        return Action.C

class Strategy_AGGRESSIVE_15(BaseStrategy):
    """
    Adaptive Exploit & Punish (AEP)

    Aggressive, probe-driven exploiter:
    - Default: Defect every round.
    - Occasionally performs a single-round cooperation probe to test whether many opponents
      reciprocate in the following round.
    - If a probe is followed by at least alpha fraction of opponents cooperating, switch to
      exploit_mode and defect permanently (still defect in final round).
    - If a probe fails, back off for B = S rounds before re-probing. Never probe in final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, max(1, math.floor(self.r / 10)))
        self.S = max(3, math.ceil(self.r / 8))
        self.alpha = 0.65
        self.M = max(1, math.ceil(self.r / self.S)) if self.S > 0 else 1
        self.B = self.S
        self.probes_done = 0
        self.last_probe_round = 0
        self.last_probe_evaluated = 0
        self.exploit_mode = False
        if self.r <= 2:
            self.next_probe_round = self.r + 1
        else:
            self.next_probe_round = 2 if 2 < self.r else self.r + 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is not None and self.last_probe_round > 0:
            p = self.last_probe_round
            if self.last_probe_evaluated < p and history.round_number >= p + 1:
                try:
                    opp_actions_round_p1 = history.opponent_actions[p, :]
                except Exception:
                    opp_actions_round_p1 = None
                if opp_actions_round_p1 is not None:
                    coop_count_others = int(np.sum(opp_actions_round_p1))
                    if self.n - 1 > 0:
                        f = coop_count_others / float(self.n - 1)
                    else:
                        f = 0.0
                    if f >= self.alpha:
                        self.exploit_mode = True
                    else:
                        scheduled = p + self.B
                        if scheduled >= self.r:
                            self.next_probe_round = self.r + 1
                        else:
                            self.next_probe_round = scheduled
                self.last_probe_evaluated = p
        if t == self.r:
            return Action.D
        if self.exploit_mode:
            return Action.D
        if self.r <= 2:
            return Action.D
        if t == self.next_probe_round and self.probes_done < self.M and (t < self.r):
            self.last_probe_round = t
            self.probes_done += 1
            tentative_next = self.next_probe_round + self.S
            if tentative_next >= self.r:
                self.next_probe_round = self.r + 1
            else:
                self.next_probe_round = tentative_next
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_16(BaseStrategy):
    """
    Aggressive Grim-Exploiter (AGE)

    - Starts by signalling cooperation.
    - If ever cooperated in a round where at least one other player defected, flips to permanent defection (grim).
    - While not exploited, cooperates when recent group cooperation is acceptable.
    - If S_coop consecutive past rounds showed full cooperation, takes a one-shot defection to exploit, then returns to baseline (unless exploited).
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploited = False
        self.last_exploit_round = None
        r = max(1, int(self.game_description.n_rounds))
        self.S_coop = min(3, max(1, math.floor(r / 10)))
        self.W = min(5, max(0, r - 1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        if history is None or history.round_number == 0:
            return Action.C
        t = history.round_number + 1
        if t == r:
            return Action.D

        def coop_count_at(idx: int) -> int:
            my_coop = 1 if bool(history.my_actions[idx]) else 0
            others_coop = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size > 0 else 0
            return my_coop + others_coop
        if not self.exploited:
            for s_idx in range(0, history.round_number):
                if bool(history.my_actions[s_idx]) and coop_count_at(s_idx) < n:
                    self.exploited = True
                    break
        if self.exploited:
            return Action.D
        last_idx = history.round_number - 1
        last_coop = coop_count_at(last_idx)
        if last_coop <= math.floor(n / 2):
            return Action.D
        rounds_to_check = min(self.S_coop, history.round_number)
        start_idx = history.round_number - rounds_to_check
        full_streak = True
        for s_idx in range(start_idx, history.round_number):
            if coop_count_at(s_idx) < n:
                full_streak = False
                break
        if full_streak:
            if self.last_exploit_round is None or self.last_exploit_round < t - self.S_coop:
                self.last_exploit_round = t
                return Action.D
        return Action.C

class Strategy_AGGRESSIVE_17(BaseStrategy):
    """
    Aggressive Opportunistic Exploiter (AOE)

    Default: defect. Opportunistically contribute only to (a) restore conditional
    cooperators so their cooperation can be exploited, or (b) probe mixed groups
    with small probability. Uses a short lookback window to detect conditionality,
    punishers, and unconditional cooperators/defectors. Persistent state tracks
    short "restore" phases where the strategy contributes for a few rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w_param = min(5, max(0, self.r - 1))
        if self.w_param < 1:
            self.w_param = 1
        self.T_high = 0.6
        self.T_low = 0.2
        self.delta_cond = 0.12
        self.L_restore = min(3, max(1, math.floor(self.r / 10))) if self.r > 1 else 1
        self.p_probe = 0.1
        self.forgive_fraction = 0.5
        self.remaining_restore_rounds = 0
        self.last_restore_start_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        t = int(state.round_number)
        if t >= self.r:
            return Action.D
        num_past = history.round_number
        if num_past == 0:
            return Action.D
        w = min(self.w_param, num_past)
        start_idx = num_past - w
        recent_slice = slice(start_idx, num_past)
        opp_actions = history.opponent_actions[recent_slice, :]
        my_actions = history.my_actions[recent_slice]
        if opp_actions.size == 0:
            group_coop_rate_recent = 0.0
        else:
            total_opp_coops = float(np.sum(opp_actions))
            denom = float(w * (self.n - 1))
            group_coop_rate_recent = total_opp_coops / denom if denom > 0 else 0.0
        indices_coop = np.nonzero(my_actions)[0]
        indices_defect = np.nonzero(~my_actions)[0]

        def compute_rate_for_indices(idx_array):
            if idx_array.size == 0:
                return None
            sel = opp_actions[idx_array, :]
            if sel.size == 0:
                return None
            return float(np.sum(sel)) / float(sel.shape[0] * sel.shape[1])
        coop_rate_when_I_coop = compute_rate_for_indices(indices_coop)
        coop_rate_when_I_defect = compute_rate_for_indices(indices_defect)
        if coop_rate_when_I_coop is None:
            coop_rate_when_I_coop = group_coop_rate_recent
        if coop_rate_when_I_defect is None:
            coop_rate_when_I_defect = group_coop_rate_recent
        CI = coop_rate_when_I_coop - coop_rate_when_I_defect
        persistent_defectors = 0
        if opp_actions.size > 0:
            per_opponent_coops = np.sum(opp_actions, axis=0)
            persistent_defectors = int(np.sum(per_opponent_coops == 0))
        half_threshold = (self.n - 1) / 2.0
        many_persistent_defectors = persistent_defectors >= math.floor(half_threshold)
        if group_coop_rate_recent <= self.T_low:
            return Action.D
        if group_coop_rate_recent >= self.T_high:
            return Action.D
        if self.remaining_restore_rounds > 0:
            self.remaining_restore_rounds -= 1
            return Action.C
        active_punishment_detected = False
        if w >= 2:
            first_part_slice = slice(start_idx, num_past - 1)
            opp_first_part = history.opponent_actions[first_part_slice, :]
            denom_fp = float((w - 1) * (self.n - 1))
            if denom_fp > 0:
                group_coop_before = float(np.sum(opp_first_part)) / denom_fp
                drop = group_coop_before - group_coop_rate_recent
                if drop > 2.0 * self.delta_cond:
                    if history.my_actions[-1] == False:
                        active_punishment_detected = True
        if active_punishment_detected and (not many_persistent_defectors):
            self.remaining_restore_rounds = max(self.remaining_restore_rounds, self.L_restore - 1)
            self.last_restore_start_round = t
            return Action.C
        if CI >= self.delta_cond and (not many_persistent_defectors):
            if coop_rate_when_I_defect < coop_rate_when_I_coop:
                self.remaining_restore_rounds = max(self.remaining_restore_rounds, self.L_restore - 1)
                self.last_restore_start_round = t
                return Action.C
        if self.T_low < group_coop_rate_recent < self.T_high and CI < self.delta_cond:
            if random.random() < self.p_probe:
                return Action.C
            else:
                return Action.D
        if many_persistent_defectors:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_18(BaseStrategy):
    """
    Adaptive Exploitive Defector (AED)

    - Defaults to defection.
    - Occasionally probes by cooperating one round to test whether opponents raise cooperation.
    - If a probe shows a sufficient, sustained increase in others' cooperation, enters a short
      exploitation burst (defecting while they continue to cooperate) to harvest one-shot gains.
    - If opponents punish (cooperation falls sufficiently), abort exploitation, enter cooldown,
      and return to default defection.
    - Always defect in the final endgame rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, max(1, math.floor(self.r / 10)))
        self.probe_interval = max(1, math.floor(self.r / 10))
        self.p_threshold_resp = 0.6
        self.delta_resp = 0.2
        self.exploit_burst = max(1, math.floor(self.r / 6))
        self.endgame_length = 1
        self.punish_tolerance = 0.25
        self.protect_min_rounds = 1
        self.p_probe_random = 0.02
        self.cooldown_length = 2 * self.probe_interval
        self.last_probe_round = None
        self.baseline_coop_rate = None
        self.evaluated_probe = True
        self.exploit_mode = False
        self.exploit_rounds_remaining = 0
        self.responsive = False
        self.cooldown_until_round = 0
        self.always_defect_short_game = self.r <= 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if self.always_defect_short_game:
            return Action.D
        if t > self.r - self.endgame_length:
            return Action.D

        def others_coop_rate_over_last_m(m: int) -> float:
            if history is None or history.round_number == 0:
                return 0.0
            m = max(1, m)
            end = history.round_number
            start = max(0, end - m)
            segment = history.opponent_actions[start:end, :]
            if segment.size == 0:
                return 0.0
            per_round_frac = np.mean(segment.astype(float), axis=1)
            return float(np.mean(per_round_frac))

        def others_coop_rate_last_round() -> float:
            if history is None or history.round_number == 0:
                return 0.0
            last = history.opponent_actions[-1, :]
            return float(np.mean(last.astype(float)))
        if self.last_probe_round is not None and (not self.evaluated_probe):
            p = self.last_probe_round
            if history is not None and history.round_number >= p + self.protect_min_rounds:
                start_idx = p
                end_idx = p + self.protect_min_rounds
                end_idx = min(end_idx, history.round_number)
                if start_idx < end_idx:
                    segment = history.opponent_actions[start_idx:end_idx, :]
                    post_per_round = np.mean(segment.astype(float), axis=1)
                    post_probe_rate = float(np.mean(post_per_round))
                else:
                    post_probe_rate = 0.0
                base = 0.0 if self.baseline_coop_rate is None else self.baseline_coop_rate
                increase = post_probe_rate - base
                if increase >= self.delta_resp and post_probe_rate >= self.p_threshold_resp:
                    self.responsive = True
                    self.exploit_mode = True
                    self.exploit_rounds_remaining = self.exploit_burst
                else:
                    self.responsive = False
                    self.exploit_mode = False
                    self.exploit_rounds_remaining = 0
                self.evaluated_probe = True
        if self.exploit_mode and self.exploit_rounds_remaining > 0 and (history is not None) and (self.baseline_coop_rate is not None):
            recent_rate = others_coop_rate_over_last_m(max(1, self.w))
            if recent_rate < self.baseline_coop_rate * (1.0 - self.punish_tolerance):
                self.exploit_mode = False
                self.responsive = False
                self.exploit_rounds_remaining = 0
                self.cooldown_until_round = t + self.cooldown_length
        if self.exploit_mode and self.exploit_rounds_remaining > 0:
            self.exploit_rounds_remaining -= 1
            if self.exploit_rounds_remaining == 0:
                self.exploit_mode = False
                self.responsive = False
                self.cooldown_until_round = t + self.cooldown_length
            return Action.D
        can_probe = True
        if t <= (self.last_probe_round or 0):
            can_probe = False
        if t <= self.cooldown_until_round:
            can_probe = False
        if t > self.r - (self.exploit_burst + self.endgame_length + 1):
            can_probe = False
        want_probe = False
        if not self.exploit_mode and can_probe:
            if t % self.probe_interval == 0 or random.random() < self.p_probe_random:
                want_probe = True
        if want_probe:
            base = others_coop_rate_over_last_m(self.w) if history is not None else 0.0
            self.last_probe_round = t
            self.baseline_coop_rate = base
            self.evaluated_probe = False
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_19(BaseStrategy):
    """
    Exploitative Grim with Probing (EGrim-Probe)

    - Default behavior: defect every round.
    - Rarely perform a one-shot cooperate probe to identify tolerant/unconditional cooperators.
    - If a large/stable fraction of opponents cooperate during the probe and do not punish
      in the immediate follow-up rounds, switch to EXPLOITING: permanently defect to
      exploit their continued cooperation.
    - If opponents punish (cooperation fraction drops below threshold), revert to DEFAULT.
    - Always defect in the final endgame rounds and do not probe if the game is too short.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.mode = 'DEFAULT'
        self.last_probe_round = None
        self.theta = 0.8
        self.phi = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        W = min(5, max(1, math.floor(r / 10)))
        endgame_rounds = min(3, max(0, r - 1))
        probe_interval = max(3, math.ceil(r / 10))
        probing_enabled = r > 4
        if history is None:
            current_round = 1
        else:
            current_round = history.round_number + 1
        if history is None:
            self.mode = 'DEFAULT'
            self.last_probe_round = None
            return Action.D
        if current_round > r - endgame_rounds:
            self.mode = 'DEFAULT'
            self.last_probe_round = None
            return Action.D

        def coop_fraction_over_last_w(window: int) -> float:
            rounds_available = history.round_number
            if rounds_available == 0:
                return 0.0
            take = min(window, rounds_available)
            arr = history.opponent_actions[-take:, :]
            if arr.size == 0:
                return 0.0
            return float(np.mean(arr))

        def coop_fraction_in_round_one_indexed(round_one_indexed: int) -> float:
            idx = round_one_indexed - 1
            if idx < 0 or idx >= history.opponent_actions.shape[0]:
                return 0.0
            row = history.opponent_actions[idx, :]
            if row.size == 0:
                return 0.0
            return float(np.mean(row))

        def coop_fraction_per_round_list(start_one_indexed: int, end_one_indexed: int):
            res = []
            for r1 in range(start_one_indexed, end_one_indexed + 1):
                idx = r1 - 1
                if 0 <= idx < history.opponent_actions.shape[0]:
                    row = history.opponent_actions[idx, :]
                    if row.size == 0:
                        res.append(0.0)
                    else:
                        res.append(float(np.mean(row)))
                else:
                    break
            return res
        if self.mode == 'EXPLOITING':
            coop_recent = coop_fraction_over_last_w(W)
            if coop_recent < self.phi:
                self.mode = 'DEFAULT'
                self.last_probe_round = None
            return Action.D
        if self.mode == 'PROBING':
            if self.last_probe_round is None:
                self.mode = 'DEFAULT'
                return Action.D
            available_followups = history.round_number - self.last_probe_round
            if available_followups < 1:
                return Action.D
            probe_round = self.last_probe_round
            coop_at_probe = coop_fraction_in_round_one_indexed(probe_round)
            followups_to_consider = min(W, available_followups)
            followup_start = probe_round + 1
            followup_end = probe_round + followups_to_consider
            fracs = coop_fraction_per_round_list(followup_start, followup_end)
            if len(fracs) == 0:
                coop_in_followups_min = 0.0
            else:
                coop_in_followups_min = float(min(fracs))
            if coop_at_probe >= self.theta and coop_in_followups_min >= self.phi:
                self.mode = 'EXPLOITING'
                return Action.D
            else:
                self.mode = 'DEFAULT'
                self.last_probe_round = None
                return Action.D
        if self.mode == 'DEFAULT':
            if not probing_enabled:
                return Action.D
            if current_round <= 1:
                return Action.D
            is_scheduled_probe = (current_round - 1) % probe_interval == 0
            if is_scheduled_probe and current_round <= r - endgame_rounds:
                if self.last_probe_round is None or self.last_probe_round < current_round:
                    self.last_probe_round = current_round
                    self.mode = 'PROBING'
                    return Action.C
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_20(BaseStrategy):
    """
    Aggressive Exploiter with Probing and Permanent Exploitation (AEPP).

    - Baseline: always defect except for rare, instrumented probes to identify
      unconditional cooperators ("suckers") and then permanently exploit them.
    - Tracks opponent cooperation rates over a sliding window m to identify
      exploitable players. Permanently flags such players and defects to exploit.
    - Defects in an endgame window of E rounds (including the last round) to avoid
      being suckered near the end.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.m = min(20, max(1, math.floor(self.r / 5)))
        self.probe_rate = min(0.2, 2.0 / max(1, self.r))
        self.exploiter_threshold = 0.9
        self.low_coop_threshold = 0.2
        self.group_exploit_frac = 0.6
        self.endgame_frac = 0.15
        self.E = max(1, math.ceil(self.endgame_frac * self.r))
        self.flagged_exploitable_set = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if history is None:
            return Action.D
        if t == self.r:
            return Action.D
        if t > self.r - self.E:
            return Action.D
        completed = history.round_number
        if completed <= 0:
            return Action.D
        window = min(self.m, completed)
        recent = history.opponent_actions[-window:, :]
        coop_counts = np.sum(recent, axis=0).astype(float)
        coop_rates = coop_counts / float(window)
        for j in range(coop_rates.shape[0]):
            if coop_rates[j] >= self.exploiter_threshold:
                self.flagged_exploitable_set.add(int(j))
        if len(self.flagged_exploitable_set) > 0:
            return Action.D
        if random.random() < self.probe_rate:
            return Action.C
        last_round_cooperators = int(np.sum(history.opponent_actions[-1, :]))
        threshold_coopers = math.ceil(self.group_exploit_frac * float(self.n - 1))
        if last_round_cooperators >= threshold_coopers:
            return Action.D
        low_cooperators = int(np.sum(coop_rates <= self.low_coop_threshold))
        if low_cooperators > (self.n - 1) / 2.0:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_21(BaseStrategy):
    """
    Opportunistic Exploiter with Targeted Probing and Ruthless Retaliation (OETRR)

    Aggressive strategy that defaults to defection, probes rarely early to find
    exploitable cooperators, exploits a sufficiently cooperative subset
    persistently, and responds to collective punishment with permanent defection
    (while keeping extremely rare probes to discover new opportunities).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.m = self.k / self.n
        self.W = min(10, max(1, self.r - 1))
        self.R_test = max(3, math.ceil(0.25 * self.r))
        self.E = min(3, max(1, self.r - 1))
        self.T_high = 0.8
        self.T_low = 0.3
        self.Exploit_group_fraction = 0.5
        self.K = math.ceil((self.n - 1) * self.Exploit_group_fraction)
        self.p_probe_base = min(max(0.05, 0.25 * self.m), 0.2)
        self.persistent_exploit_flag = False
        self.punished_state = False
        self.last_group_level_when_exploiting = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t == 1:
            return Action.D
        if t >= self.r - self.E + 1:
            return Action.D
        if history is None:
            return Action.D
        rounds_so_far = max(1, history.round_number)
        denom = min(self.W, rounds_so_far)
        opp_actions_recent = history.opponent_actions[-denom:, :]
        coop_counts = np.sum(opp_actions_recent.astype(float), axis=0)
        coop_rate_j = coop_counts / float(denom)
        if coop_rate_j.size == 0:
            group_recent = 0.0
        else:
            group_recent = float(np.mean(coop_rate_j))
        count_high = int(np.sum(coop_rate_j >= self.T_high))
        if self.persistent_exploit_flag:
            if group_recent < self.T_low:
                self.persistent_exploit_flag = False
                self.punished_state = True
                return Action.D
            else:
                return Action.D
        if count_high >= self.K or group_recent >= self.T_high:
            self.persistent_exploit_flag = True
            self.last_group_level_when_exploiting = group_recent
            return Action.D
        if t <= self.R_test:
            p_probe_t = self.p_probe_base * (1.0 - float(t - 1) / float(self.R_test))
            if p_probe_t < 0.0:
                p_probe_t = 0.0
            if random.random() < p_probe_t:
                return Action.C
            else:
                return Action.D
        if self.punished_state:
            p_probe = self.p_probe_base / 5.0
            if random.random() < p_probe:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_22(BaseStrategy):
    """
    Aggressive Adaptive Exploiter (AAE)

    - Default aggressive: defect unless it's unsafe to do so.
    - Probe occasionally to find exploitable cooperators.
    - Track per-opponent cooperation frequency and retaliation tendency.
    - If many opponents are non-retaliatory, exploit (defect persistently).
    - If many opponents punish defection, adopt conditional cooperation (cooperate when the group last cooperated).
    - Defect in the final two rounds (conservative endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_explore = min(0.08, 5.0 / max(1, self.game_description.n_rounds))
        self.theta_retaliate = 0.4
        self.theta_safe_ratio = 0.6
        self.theta_group_exploit = 0.6
        self.min_history_rounds = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        r = self.game_description.n_rounds
        n = self.game_description.n_players
        t = state.round_number
        if history is None:
            if random.random() < self.p_explore:
                return Action.C
            return Action.D
        if t >= r - 1:
            return Action.D
        completed = history.round_number
        if completed < self.min_history_rounds:
            if random.random() < self.p_explore:
                return Action.C
            return Action.D
        if completed > 0:
            coop_counts = np.sum(history.opponent_actions[:completed, :], axis=0).astype(float)
        else:
            coop_counts = np.zeros(n - 1, dtype=float)
        coop_freqs = coop_counts / float(max(1, completed))
        numer = np.zeros(n - 1, dtype=float)
        denom = np.zeros(n - 1, dtype=float)
        for s_idx in range(0, max(0, completed - 1)):
            my_was_coop = bool(history.my_actions[s_idx])
            if not my_was_coop:
                denom += 1.0
                next_actions = history.opponent_actions[s_idx + 1, :].astype(bool)
                punished = np.logical_not(next_actions)
                numer += punished.astype(float)
        retaliation_rates = np.zeros(n - 1, dtype=float)
        for j in range(n - 1):
            if denom[j] > 0:
                retaliation_rates[j] = numer[j] / denom[j]
            else:
                retaliation_rates[j] = 1.0 - coop_freqs[j]
        safe_opponents = int(np.sum(retaliation_rates < self.theta_retaliate))
        safety_ratio = safe_opponents / float(max(1, n - 1))
        last_my = bool(history.my_actions[-1])
        last_opps = history.opponent_actions[-1, :].astype(bool)
        group_coop_last = int(last_my) + int(np.sum(last_opps))
        if group_coop_last >= math.ceil(self.theta_group_exploit * n):
            if safety_ratio >= 0.25:
                return Action.D
        if safety_ratio >= self.theta_safe_ratio:
            if random.random() < 1.0 - self.p_explore:
                return Action.D
            else:
                return Action.C
        if safety_ratio < self.theta_safe_ratio:
            if group_coop_last >= math.ceil(0.5 * n):
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_23(BaseStrategy):
    """
    Aggressive Exploiter (AE)

    - Instrumental cooperation: short initial probe to seed/test cooperation.
    - Exploits when recent opponent cooperation is above p_exploit by defecting.
    - If exploitation causes a collapse (cooperation < p_collapsed), attempt up to
      max_resets short cooperative resets to re-seed exploitable cooperation.
    - Always defects in the final endgame_margin rounds.
    - Deterministic and stateful across rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.probe_rounds = min(3, max(1, math.floor(self.r / 6)))
        self.window = min(5, max(1, self.r))
        self.p_exploit = 0.4
        self.p_collapsed = 0.2
        self.reset_length = 2
        self.max_resets = 2
        self.endgame_margin = min(3, self.r)
        self.resets_done = 0
        self.exploited_last_phase_flag = False
        self.reset_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t > self.r - self.endgame_margin:
            return Action.D
        if self.reset_counter > 0:
            self.reset_counter -= 1
            if self.reset_counter == 0:
                self.exploited_last_phase_flag = False
            return Action.C
        if history is None or history.round_number == 0:
            if t <= self.probe_rounds:
                return Action.C
        if t <= self.probe_rounds:
            return Action.C
        L = min(self.window, max(0, t - 1))
        last_L_coop_rate = 0.0
        if L > 0 and history is not None:
            start_idx = max(0, t - 1 - L)
            end_idx = t - 1
            opp_slice = history.opponent_actions[start_idx:end_idx, :]
            last_L_coops = float(np.sum(opp_slice))
            denom = float(L * max(1, self.n - 1))
            last_L_coop_rate = last_L_coops / denom if denom > 0 else 0.0
        if last_L_coop_rate >= self.p_exploit:
            self.exploited_last_phase_flag = True
            return Action.D
        if self.exploited_last_phase_flag and last_L_coop_rate < self.p_collapsed and (self.resets_done < self.max_resets):
            self.resets_done += 1
            self.reset_counter = max(0, self.reset_length - 1)
            self.exploited_last_phase_flag = False
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_24(BaseStrategy):
    """
    Aggressive Exploiter with Calibrated Punishment (AECP).

    Summary:
    - Default posture is hostile: mostly defect.
    - If a sizable, stable coalition of cooperators is detected, defect to exploit them.
    - Detects retaliation: if others consistently reduce cooperation following your defections,
      confirm after P such drops and then lock into permanent defection (grim).
    - Occasionally probe (tiny probability) with cooperation to discover new exploitable cooperators,
      but only when there are remaining rounds to exploit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.epsilon_probe = 0.03
        self.coalition_freq_threshold = 0.75
        self.retaliation_drop_threshold = 0.2
        self.H_final = 2
        self.punish_confirm_count = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        if history is None:
            rounds_completed = 0
        else:
            rounds_completed = int(history.round_number)
        t = rounds_completed + 1
        if t > r - self.H_final:
            return Action.D
        W = min(5, max(1, math.floor(r / 4)))
        if history is None or rounds_completed == 0:
            opponent_actions = np.zeros((0, max(0, n - 1)), dtype=bool)
        else:
            opponent_actions = history.opponent_actions
        rounds_available = opponent_actions.shape[0]
        window = min(W, rounds_available)
        if window > 0:
            recent = opponent_actions[-window:, :]
            f_j = np.mean(recent.astype(float), axis=0)
        else:
            f_j = np.zeros((max(0, n - 1),), dtype=float)
        coalition_indices = [idx for idx, freq in enumerate(f_j) if freq >= self.coalition_freq_threshold]
        coalition_size = len(coalition_indices)
        coalition_size_threshold = math.ceil(n / 2)
        retaliation_count = 0
        if history is not None and rounds_available >= 3:
            my_actions = history.my_actions
            for s in range(1, rounds_available - 1):
                if not bool(my_actions[s]):
                    coop_before = float(np.mean(opponent_actions[s - 1, :])) if opponent_actions.shape[1] > 0 else 0.0
                    coop_after = float(np.mean(opponent_actions[s + 1, :])) if opponent_actions.shape[1] > 0 else 0.0
                    if coop_after <= coop_before - self.retaliation_drop_threshold:
                        retaliation_count += 1
        retaliation_detected = retaliation_count >= self.punish_confirm_count
        if retaliation_detected:
            return Action.D
        if coalition_size >= coalition_size_threshold:
            return Action.D
        remaining_after_this_round = r - t
        if remaining_after_this_round >= 1:
            if random.random() < self.epsilon_probe:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_25(BaseStrategy):
    """
    Aggressive Trigger-and-Exploit (ATE)

    - Probes early (cooperates a little), exploits very cooperative groups with occasional defections,
      and imposes long, credible punishments when observed cooperation is low.
    - Always defects in the final round.
    - Stateful: maintains punished_counter and last_exploit_round across rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.punished_counter = 0
        self.last_exploit_round = -10 ** 9
        self.T_good = 0.8
        self.T_bad = 0.5
        self.exploit_cooldown = 1
        self.max_w = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        r = self.r
        n = self.n
        k = self.k
        rem = r - t + 1
        coop_incentive = float(k) / float(n) if n > 0 else 0.0
        if r <= 3:
            s = 1
        else:
            s = min(2, max(1, r - 1))
        if t == r:
            return Action.D
        if history is None or (hasattr(history, 'round_number') and history.round_number == 0):
            if t <= s and r > 2:
                return Action.C
            return Action.D
        if self.punished_counter > 0:
            self.punished_counter -= 1
            return Action.D
        available_rounds = history.round_number
        w = min(self.max_w, max(0, r - 1))
        window = min(w, available_rounds)
        p = 0.0
        if window > 0:
            recent = history.opponent_actions[-window:, :]
            counts = np.sum(recent.astype(float), axis=1)
            denom = float(max(1, n - 1))
            per_round_props = counts / denom
            p = float(np.mean(per_round_props))
        else:
            p = 0.0
        if t <= s and r > 2:
            return Action.C
        if p >= self.T_good and t - self.last_exploit_round > self.exploit_cooldown and (rem > 1):
            self.last_exploit_round = t
            return Action.D
        if p >= self.T_bad and p < self.T_good:
            if rem >= 3:
                return Action.C
            else:
                return Action.D
        if p < self.T_bad:
            base = 0.5 + (1.0 - coop_incentive)
            if base < 0.5:
                base = 0.5
            if base > 0.9:
                base = 0.9
            L_punish = max(1, int(math.ceil(rem * base)))
            self.punished_counter = L_punish - 1
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_26(BaseStrategy):
    """
    Aggressive Threshold-Trigger (ATT)

    - Starts cooperating.
    - Requires a high cooperation threshold M to continue cooperating.
    - If the group falls below M in the previous round, enter a Punishing state for P rounds (or until game end).
    - After punishment, perform a Probing round; only return to Cooperative if probe elicits >= M_probe cooperators.
    - Occasionally performs a single-round exploit defection when the group has been very near-unanimously cooperative for a recent window W;
      exploits are spaced by at least S rounds.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.n = int(n)
        self.r = int(r)
        self.k = float(game_description.k)
        self.M = max(2, math.ceil(0.75 * self.n))
        self.P = max(3, math.ceil(self.r / 4))
        self.W = min(max(3, math.ceil(math.sqrt(self.r))), max(0, self.r - 1))
        self.S = max(2, math.floor(self.W / 2))
        self.M_probe = max(2, math.ceil(0.9 * self.n))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C

        def total_cooperators_in_round(idx: int) -> int:
            my = int(history.my_actions[idx])
            opp = int(np.sum(history.opponent_actions[idx, :])) if history.opponent_actions.size else 0
            return my + opp
        m = history.round_number
        t_next = m + 1
        if t_next == self.r:
            return Action.D
        past_totals = [total_cooperators_in_round(i) for i in range(m)]

        def is_very_cooperative_for_past_count(past_count: int) -> bool:
            if past_count <= 0:
                return False
            window_len = min(self.W, past_count)
            start = past_count - window_len
            for idx in range(start, past_count):
                if past_totals[idx] < self.M_probe:
                    return False
            return True
        status = 'Cooperative'
        punishment_timer = 0
        last_exploit_round = -10 ** 9
        for t in range(1, m + 1):
            past_count = t - 1
            if past_count == 0:
                continue
            c_prev = past_totals[past_count - 1]
            if status == 'Cooperative':
                if c_prev >= self.M:
                    very_coop = is_very_cooperative_for_past_count(past_count)
                    if very_coop and t - last_exploit_round >= self.S:
                        last_exploit_round = t
                    else:
                        pass
                else:
                    punishment_timer = min(self.P, max(0, self.r - t))
                    status = 'Punishing'
            elif status == 'Punishing':
                if punishment_timer > 0:
                    punishment_timer -= 1
                else:
                    status = 'Probing'
            elif c_prev >= self.M_probe:
                status = 'Cooperative'
            else:
                punishment_timer = min(max(0, self.r - t), max(self.P, 2 * self.P))
                status = 'Punishing'
        if m == 0:
            return Action.C
        c_prev = past_totals[-1]
        past_count_for_next = m
        very_coop_next = is_very_cooperative_for_past_count(past_count_for_next)
        if status == 'Cooperative':
            if c_prev >= self.M:
                if very_coop_next and t_next - last_exploit_round >= self.S:
                    return Action.D
                else:
                    return Action.C
            else:
                return Action.D
        elif status == 'Punishing':
            if punishment_timer > 0:
                return Action.D
            else:
                return Action.C
        elif c_prev >= self.M_probe:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_27(BaseStrategy):
    """
    Aggressive Exploit-and-Punish (AEP)

    Default: defect. Detect stable cooperation among opponents and enter Exploit mode:
    free-ride with occasional deterministic probes to maintain exploitation. If probes fail
    or group cooperation falls sharply, inflict a short, harsh punishment (unconditional
    defection for P rounds) and then resume probing. Never cooperate on the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, max(1, self.r - 1))
        self.tau_exploit = 0.7
        self.R_min_for_exploitation = 3
        self.P = min(3, max(1, math.floor(self.r / 10)))
        self.M_probe = max(3, math.floor(self.r / 6))
        self.tau_coop_player = 0.8
        self.exploit_probe_success_frac = 0.6
        self.mode = 'Normal'
        self.punish_counter = 0
        self.exploit_run_length = 0
        self.last_probe_round = 0
        self.pending_probe_round = 0
        self.below_exploit_windows_count = 0
        self.scheduled_one_time_offer_round = 0
        self.one_time_offer_used = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        t = history.round_number + 1
        R_rem = self.r - t + 1
        rounds_played = history.round_number
        opp_actions = history.opponent_actions
        if self.pending_probe_round and rounds_played >= self.pending_probe_round + 1:
            idx = self.pending_probe_round
            if 0 <= idx < rounds_played:
                coop_others_frac = float(np.sum(opp_actions[idx, :])) / max(1, opp_actions.shape[1])
            else:
                coop_others_frac = 0.0
            if coop_others_frac >= self.exploit_probe_success_frac:
                self.mode = 'Exploit'
            else:
                self.mode = 'Punish'
                self.punish_counter = self.P
                self.exploit_run_length = 0
            self.pending_probe_round = 0
        if t == self.r:
            return Action.D
        if self.scheduled_one_time_offer_round == t and (not self.one_time_offer_used):
            self.one_time_offer_used = True
            self.last_probe_round = t
            self.pending_probe_round = t
            return Action.C
        if self.punish_counter > 0:
            self.punish_counter -= 1
            if self.punish_counter == 0:
                self.mode = 'Normal'
            return Action.D
        if rounds_played == 0:
            group_recent_avg = 0.0
        else:
            window = min(self.W, rounds_played)
            start_idx = rounds_played - window
            recent_sums = np.sum(opp_actions[start_idx:rounds_played, :], axis=1)
            frac_per_round = recent_sums / max(1, opp_actions.shape[1])
            group_recent_avg = float(np.mean(frac_per_round)) if frac_per_round.size > 0 else 0.0
        if group_recent_avg < self.tau_exploit - 0.1:
            self.below_exploit_windows_count += 1
        else:
            self.below_exploit_windows_count = 0
        if self.mode == 'Exploit' and self.below_exploit_windows_count >= 2:
            self.mode = 'Punish'
            self.punish_counter = self.P
            self.exploit_run_length = 0
            return Action.D
        if self.mode == 'Exploit':
            if R_rem < self.R_min_for_exploitation:
                self.mode = 'Normal'
                self.exploit_run_length = 0
            else:
                self.exploit_run_length += 1
                last_probe = self.last_probe_round or 0
                if t - last_probe >= self.M_probe and R_rem > 1:
                    self.last_probe_round = t
                    self.pending_probe_round = t
                    return Action.C
                else:
                    return Action.D
        coop_rates = None
        if rounds_played > 0:
            coop_rates = np.sum(opp_actions, axis=0) / float(rounds_played)
        if group_recent_avg >= self.tau_exploit and R_rem >= self.R_min_for_exploitation:
            self.mode = 'Exploit'
            self.exploit_run_length = 0
            self.last_probe_round = max(0, t - 1)
            return Action.D
        if not self.one_time_offer_used and R_rem >= 2 * self.M_probe and (group_recent_avg >= 0.5) and (rounds_played > 0):
            q = math.ceil((self.n - 1) * 0.5)
            if coop_rates is not None:
                reliable_count = int(np.sum(coop_rates >= self.tau_coop_player))
                if reliable_count >= q:
                    self.scheduled_one_time_offer_round = t + 1
                    return Action.D
        return Action.D

class Strategy_AGGRESSIVE_28(BaseStrategy):
    """
    Aggressive Opportunistic Punisher (AOP)

    - Defects by default and in the final T_end rounds.
    - Detects stable cooperators using a recent window W and threshold q.
    - If a majority M are stable cooperators, starts a short cooperation burst of length B.
    - If any of the triggering stable cooperators defect during the burst (observed next round),
      switch to a long P-round punishment (defect).
    - After punishment, perform a single-round TEST (probe). If the probe round shows >= M
      cooperators, re-enter a cooperation burst. Otherwise return to DEFAULT.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.mode = 'DEFAULT'
        self.burst_remaining = 0
        self.punish_remaining = 0
        self.S_trigger_opponents = set()
        self._test_in_progress = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.mode = 'DEFAULT'
            self.burst_remaining = 0
            self.punish_remaining = 0
            self.S_trigger_opponents = set()
            self._test_in_progress = False
            return Action.D
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        W = min(5, max(1, r - 1))
        q = 0.8
        M = math.ceil(n / 2)
        B = min(4, max(1, math.floor(r / 10 + 1)))
        P = min(r, max(6, math.ceil(r / 3)))
        T_end = min(r, max(1, math.ceil(r / 10)))
        current_round = int(state.round_number)
        completed_rounds = history.round_number
        if current_round > r - T_end:
            return Action.D

        def get_recent_window():
            denom = min(W, completed_rounds)
            return denom
        if self.mode == 'TEST' and self._test_in_progress and (completed_rounds >= 1):
            last_my = bool(history.my_actions[-1])
            last_opponents = history.opponent_actions[-1, :] if completed_rounds >= 1 else np.array([], dtype=bool)
            total_cooperators_last = int(np.sum(last_opponents)) + (1 if last_my else 0)
            if total_cooperators_last >= M:
                self.mode = 'COOP_BURST'
                self.burst_remaining = max(0, B - 1)
                denom = get_recent_window()
                if denom > 0:
                    opp_slice = history.opponent_actions[-denom:, :]
                    opp_counts = np.sum(opp_slice.astype(int), axis=0)
                    opp_freqs = opp_counts / denom
                    my_slice = history.my_actions[-denom:]
                    my_freq = int(np.sum(my_slice)) / denom
                else:
                    opp_freqs = np.zeros(n - 1)
                    my_freq = 0.0
                self.S_trigger_opponents = set((int(i) for i, f in enumerate(opp_freqs) if f >= q))
                self._test_in_progress = False
            else:
                self.mode = 'DEFAULT'
                self._test_in_progress = False
                self.S_trigger_opponents = set()
                self.burst_remaining = 0
        if self.mode == 'COOP_BURST' and completed_rounds >= 1 and (len(self.S_trigger_opponents) > 0):
            last_opponents = history.opponent_actions[-1, :]
            betrayed = False
            for opp_idx in self.S_trigger_opponents:
                if opp_idx < last_opponents.shape[0]:
                    if not bool(last_opponents[opp_idx]):
                        betrayed = True
                        break
            if betrayed:
                self.mode = 'PUNISH'
                self.punish_remaining = P
                self.burst_remaining = 0
                self.S_trigger_opponents = set()
        if self.mode == 'PUNISH':
            action = Action.D
            if self.punish_remaining > 0:
                self.punish_remaining -= 1
            if self.punish_remaining == 0:
                self.mode = 'TEST'
                self._test_in_progress = False
            return action
        if self.mode == 'COOP_BURST':
            if self.burst_remaining <= 0:
                self.mode = 'DEFAULT'
                self.S_trigger_opponents = set()
                return Action.D
            action = Action.C
            self.burst_remaining -= 1
            if self.burst_remaining == 0:
                self.mode = 'DEFAULT'
                self.S_trigger_opponents = set()
            return action
        if self.mode == 'TEST' and (not self._test_in_progress):
            self._test_in_progress = True
            return Action.C
        denom = get_recent_window()
        if denom <= 0:
            return Action.D
        opp_slice = history.opponent_actions[-denom:, :]
        opp_counts = np.sum(opp_slice.astype(int), axis=0)
        opp_freqs = opp_counts / denom
        my_slice = history.my_actions[-denom:]
        my_count = int(np.sum(my_slice.astype(int)))
        my_freq = my_count / denom if denom > 0 else 0.0
        S_opponents = set((int(i) for i, f in enumerate(opp_freqs) if f >= q))
        stable_count = len(S_opponents) + (1 if my_freq >= q else 0)
        if stable_count >= M:
            self.mode = 'COOP_BURST'
            self.burst_remaining = B
            self.S_trigger_opponents = S_opponents.copy()
            self.burst_remaining -= 1
            if self.burst_remaining == 0:
                self.mode = 'DEFAULT'
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_29(BaseStrategy):
    """
    Aggressive-exploitative public goods strategy with short repair behavior.

    - Default: defect to exploit cooperators.
    - Detects when others punish defections (sharp drop in others' cooperation).
    - If punished, enters a short repair/conditional mode: mirror the group majority
      for a small number of rounds to stop punishment, then return to exploitation.
    - Always defect in the final rounds (no endgame trust).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.M = min(8, max(1, self.r - 1))
        self.COOP_EXPLOIT_TH = 0.75
        self.PUNISH_DROP = 0.25
        self.REPAIR_ROUNDS = min(3, max(1, math.floor(self.r / 10)))
        self.MAJ_THRESHOLD = math.ceil((self.n - 1) / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        t = history.round_number + 1
        remaining = self.r - t + 1
        if remaining <= 2 or t >= self.r:
            return Action.D
        if t == 1:
            return Action.D
        lookback = min(self.M, t - 1)
        avg_other_coop = 0.0
        if lookback > 0:
            sum_other_coop = 0.0
            start_idx = t - 1 - lookback
            for idx in range(start_idx, t - 1):
                other_c = int(np.sum(history.opponent_actions[idx, :]))
                sum_other_coop += other_c / max(1, self.n - 1)
            avg_other_coop = sum_other_coop / lookback
        punished = False
        last_pun_round = None
        for s in range(2, t):
            my_prev_action_idx = s - 2
            if my_prev_action_idx < 0 or my_prev_action_idx >= len(history.my_actions):
                continue
            if not bool(history.my_actions[my_prev_action_idx]):
                if s - 3 >= 0:
                    prev_idx = s - 3
                    prev_other = int(np.sum(history.opponent_actions[prev_idx, :])) / max(1, self.n - 1)
                else:
                    prev_other = avg_other_coop
                after_idx = s - 2
                after_other = int(np.sum(history.opponent_actions[after_idx, :])) / max(1, self.n - 1)
                if prev_other - after_other >= self.PUNISH_DROP:
                    punished = True
                    last_pun_round = s
                    break
        if not punished:
            return Action.D
        repair_done = 0
        if last_pun_round is not None:
            for u in range(last_pun_round, t):
                idx = u - 1
                if 0 <= idx < len(history.my_actions) and bool(history.my_actions[idx]):
                    repair_done += 1
        R = min(self.REPAIR_ROUNDS, max(1, remaining - 1))
        last_other_count = 0
        if history.opponent_actions.shape[0] > 0:
            last_other_count = int(np.sum(history.opponent_actions[-1, :]))
        if repair_done < R:
            if last_other_count >= self.MAJ_THRESHOLD:
                return Action.C
            else:
                return Action.D
        if avg_other_coop >= self.COOP_EXPLOIT_TH:
            return Action.D
        if last_other_count >= self.MAJ_THRESHOLD:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_30(BaseStrategy):
    """
    Aggressive Opportunistic Exploiter (AOE)

    - Defaults to defection.
    - Looks for empirical responsiveness of others to its cooperation (using a local window).
    - If cooperating reliably increases others' cooperation (R_group >= threshold), AOE will
      "seed" (cooperate) and then "harvest" (defect) the next round to exploit the induced contributions.
    - If exploitation is punished (others' cooperation drops persistently beyond tolerance),
      AOE permanently reverts to defection.
    - Always defects in the final E rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.W_window_max = 10
        self.E = 1
        self.resp_threshold = 0.08
        self.group_resp_threshold = 0.06
        self.min_rem_for_exploit = 2
        self.exploitation_cycle = 2
        self.punishment_tolerance = 0.02
        self.exploitation_punished = False
        self.exploited_recently = None
        self.last_action = Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.last_action = Action.D
            return Action.D
        t = int(state.round_number)
        n_rounds = int(self.game_description.n_rounds)
        rem = n_rounds - t + 1
        if rem <= self.E:
            self.last_action = Action.D
            return Action.D
        if self.exploitation_punished:
            self.last_action = Action.D
            return Action.D
        completed = history.round_number
        if completed < 2:
            self.last_action = Action.D
            return Action.D
        W = min(self.W_window_max, max(1, t - 1))
        available_pairs = max(0, completed - 1)
        pair_window = min(W, available_pairs)
        if pair_window < 1:
            self.last_action = Action.D
            return Action.D
        prev_actions = history.my_actions[:-1]
        next_opponents = history.opponent_actions[1:, :]
        prev_tail = np.asarray(prev_actions[-pair_window:], dtype=np.bool_)
        next_tail = np.asarray(next_opponents[-pair_window:, :], dtype=np.bool_)
        n_opponents = next_tail.shape[1] if next_tail.ndim == 2 else 0
        pseudo_count_num = 1.0
        pseudo_count_den = 2.0
        p_C_list = []
        p_D_list = []
        R_list = []
        for j in range(n_opponents):
            mask_C = prev_tail
            mask_D = ~prev_tail
            coop_given_C = float(np.sum(next_tail[mask_C, j])) if mask_C.any() else 0.0
            coop_given_D = float(np.sum(next_tail[mask_D, j])) if mask_D.any() else 0.0
            denom_C = float(np.count_nonzero(mask_C))
            denom_D = float(np.count_nonzero(mask_D))
            p_C = (coop_given_C + pseudo_count_num) / (denom_C + pseudo_count_den)
            p_D = (coop_given_D + pseudo_count_num) / (denom_D + pseudo_count_den)
            R_j = p_C - p_D
            p_C_list.append(p_C)
            p_D_list.append(p_D)
            R_list.append(R_j)
        if n_opponents > 0:
            f_others_given_I_C = float(np.mean(p_C_list))
            f_others_given_I_D = float(np.mean(p_D_list))
            R_group = f_others_given_I_C - f_others_given_I_D
            responsive_count = int(np.sum(np.array(R_list) >= self.resp_threshold))
        else:
            f_others_given_I_C = 0.0
            f_others_given_I_D = 0.0
            R_group = 0.0
            responsive_count = 0
        baseline_window = min(W, completed)
        if baseline_window >= 1:
            baseline_slice = history.opponent_actions[-baseline_window:, :]
            baseline = float(np.mean(baseline_slice)) if baseline_slice.size > 0 else 0.0
        else:
            baseline = 0.0
        if self.exploited_recently is not None and self.exploited_recently.get('harvest_round') is not None:
            harvest_round = int(self.exploited_recently['harvest_round'])
            if history.round_number >= harvest_round + 2:
                start_idx = harvest_round
                end_idx = harvest_round + 2
                max_idx = history.opponent_actions.shape[0]
                if start_idx < max_idx:
                    end_idx = min(end_idx, max_idx)
                    post_slice = history.opponent_actions[start_idx:end_idx, :]
                    post = float(np.mean(post_slice)) if post_slice.size > 0 else 0.0
                    baseline_recorded = float(self.exploited_recently.get('baseline', baseline))
                    if baseline_recorded - post > self.punishment_tolerance:
                        self.exploitation_punished = True
                        self.exploited_recently = None
                        self.last_action = Action.D
                        return Action.D
                    else:
                        self.exploited_recently = None
        if R_group >= self.group_resp_threshold and rem >= self.min_rem_for_exploit:
            last_act_bool = bool(history.my_actions[-1])
            last_action_enum = Action.C if last_act_bool else Action.D
            if last_action_enum == Action.D:
                if rem >= self.exploitation_cycle:
                    self.exploited_recently = {'seed_round': t, 'baseline': baseline, 'harvest_round': None}
                    self.last_action = Action.C
                    return Action.C
                else:
                    self.last_action = Action.D
                    return Action.D
            else:
                if self.exploited_recently is None:
                    self.exploited_recently = {'seed_round': t - 1, 'baseline': baseline, 'harvest_round': t}
                else:
                    self.exploited_recently['harvest_round'] = t
                self.last_action = Action.D
                return Action.D
        self.last_action = Action.D
        return Action.D

class Strategy_AGGRESSIVE_31(BaseStrategy):
    """
    Aggressive Opportunistic Punisher (AOP)

    - Default: defect.
    - Detects cooperative populations and exploits them via mostly-defection with
      infrequent (periodic) cooperation to sustain conditionals.
    - Targets persistent defectors with short, harsh punishment windows.
    - Probes predominantly-defecting populations occasionally to find conditionals.
    - Always defects in the final few rounds (endgame horizon).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.endgame_horizon = min(3, max(0, self.r - 1))
        self.W_default = 5
        denom = max(1, self.n - 2)
        adjusted_factor = max(0.8, 1 - (self.k - 1) / denom) if denom > 0 else 1.0
        self.p_high = 0.6 * adjusted_factor
        if self.p_high <= 0:
            self.p_high = 0.6
        self.p_low = 0.15
        self.M = 4
        self.periodic_offset = 2
        self.L = 3
        self.detection_threshold = 0.1
        self.punishment_timer = 0
        self.punishment_targets = set()
        self.periodic_enabled = True
        self.enter_exploit_next = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        t = history.round_number + 1
        if t > self.r - self.endgame_horizon:
            return Action.D
        if self.punishment_timer > 0:
            self._update_punishment_targets_and_maybe_exit(history)
            if self.punishment_timer > 0:
                self.punishment_timer -= 1
                return Action.D
        W = min(self.W_default, max(1, t - 1))
        opp_actions = history.opponent_actions
        rounds_available = opp_actions.shape[0]
        if rounds_available >= 1:
            slice_start = max(0, rounds_available - W)
            recent_block = opp_actions[slice_start:rounds_available, :]
            coop_rates = np.mean(recent_block.astype(float), axis=0)
        else:
            coop_rates = np.zeros(self.n - 1, dtype=float)
        if coop_rates.size > 0:
            group_recent_rate = float(np.mean(coop_rates))
        else:
            group_recent_rate = 0.0
        persistent_mask = coop_rates < self.detection_threshold
        persistent_indices = set((int(i) for i, v in enumerate(persistent_mask) if v))
        if rounds_available >= 1:
            m_prev_others = int(np.sum(opp_actions[-1, :].astype(int)))
        else:
            m_prev_others = 0
        if persistent_indices and group_recent_rate >= self.p_high:
            self.punishment_targets = persistent_indices.copy()
            self.punishment_timer = self.L
            self.punishment_timer -= 1
            return Action.D
        if self.enter_exploit_next:
            self.enter_exploit_next = False
            chosen = self._exploit_decision(t, history)
            return chosen
        if group_recent_rate >= self.p_high:
            rounds_to_check = min(rounds_available, 3)
            if rounds_to_check >= 2:
                recent_rounds = opp_actions[-rounds_to_check:, :].astype(int)
                per_round_props = np.mean(recent_rounds, axis=1)
                if rounds_to_check >= 2:
                    if per_round_props[-1] < self.p_low and per_round_props[-2] < self.p_low:
                        if history.my_actions.size >= 1 and (not bool(history.my_actions[-1])):
                            self.periodic_enabled = False
            chosen = self._exploit_decision(t, history)
            return chosen
        if self.p_low <= group_recent_rate < self.p_high:
            majority_needed = math.ceil((self.n - 1) / 2)
            if m_prev_others >= majority_needed:
                return Action.C
            else:
                return Action.D
        T_test = 5
        if t % T_test == 0:
            return Action.C
        else:
            if rounds_available >= 2:
                coop_counts = np.sum(opp_actions.astype(int), axis=1)
                coop_jump_thresh = max(1, math.floor(0.25 * (self.n - 1)))
                lookback = min(rounds_available, self.W_default + 1)
                start_idx = rounds_available - lookback
                for idx in range(start_idx, rounds_available - 1):
                    prev_count = int(coop_counts[idx])
                    next_count = int(coop_counts[idx + 1])
                    prev_round_number = idx + 1
                    if (prev_round_number + 1) % T_test == 0:
                        if next_count - prev_count >= coop_jump_thresh:
                            self.enter_exploit_next = True
                            self.periodic_enabled = True
                            break
            return Action.D

    def _update_punishment_targets_and_maybe_exit(self, history: PlayerHistory):
        """
        During punishment, recompute coop_rates and exit early if all targets raise
        coop_rate_j >= detection threshold.
        """
        opp_actions = history.opponent_actions
        rounds_available = opp_actions.shape[0]
        if rounds_available <= 0 or len(self.punishment_targets) == 0:
            return
        W = min(self.W_default, rounds_available)
        recent_block = opp_actions[max(0, rounds_available - W):rounds_available, :].astype(float)
        coop_rates = np.mean(recent_block, axis=0)
        remaining = set()
        for idx in self.punishment_targets:
            if coop_rates[idx] < self.detection_threshold:
                remaining.add(idx)
        self.punishment_targets = remaining
        if len(self.punishment_targets) == 0:
            self.punishment_timer = 0

    def _exploit_decision(self, t: int, history: PlayerHistory) -> Action:
        """
        Decide in exploit mode. Mostly defect, but occasionally cooperate every M rounds
        at deterministic offset if periodic_enabled.
        """
        if not self.periodic_enabled:
            return Action.D
        if t % self.M == self.periodic_offset:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_32(BaseStrategy):
    """
    Aggressive Opportunistic Grudger (AOG)

    - Starts defecting.
    - Uses a short sliding window reputation to detect reliably cooperative players.
    - If enough players look cooperative, enters an exploitation cycle:
        cooperate S_coop rounds, then defect one round to harvest.
    - If an exploitation harvest appears to collapse cooperation (drop >= 0.20 or below threshold),
      punish with P_punish rounds of defection. After two punishments that fail, switch to Grim
      (defect forever).
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.gamma = self.k / float(self.n)
        self.W = int(min(5, max(1, math.floor(self.r / 10))))
        self.p_ind = 0.6
        self.frac_req = max(0.5, min(0.75, 1.0 - self.gamma / 2.0))
        self.S_coop = 3
        self.cycle_pos = None
        self.in_exploit_mode = False
        self.in_punish_mode = False
        self.punish_remaining = 0
        self.punish_failures = 0
        self.in_grim = False
        self.last_was_exploit_harvest = False
        self.coop_fraction_before_exploit = None
        self.last_coop_fraction = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        remaining_rounds = self.r - t + 1
        if history is None or history.round_number == 0:
            self.in_exploit_mode = False
            self.cycle_pos = None
            self.last_was_exploit_harvest = False
            self.coop_fraction_before_exploit = None
            self.last_coop_fraction = 0.0
            return Action.D
        if self.in_grim:
            return Action.D
        if t == self.r:
            return Action.D
        if self.in_punish_mode:
            action = Action.D
            self.punish_remaining -= 1
            if self.punish_remaining <= 0:
                self.in_punish_mode = False
            self.last_was_exploit_harvest = False
            self.coop_fraction_before_exploit = None
            return action
        past_rounds = history.round_number
        start_idx = max(0, past_rounds - self.W)
        my_actions = np.array(history.my_actions, dtype=np.float64)
        if past_rounds > 0:
            my_window = my_actions[start_idx:past_rounds]
            my_rate = float(np.mean(my_window)) if my_window.size > 0 else 0.0
        else:
            my_rate = 0.0
        opp_actions = np.array(history.opponent_actions, dtype=np.float64)
        if past_rounds > 0:
            opp_window = opp_actions[start_idx:past_rounds, :]
            if opp_window.size == 0:
                opp_rates = np.zeros(self.n - 1, dtype=np.float64)
            else:
                opp_rates = np.array(np.mean(opp_window, axis=0), dtype=np.float64)
        else:
            opp_rates = np.zeros(self.n - 1, dtype=np.float64)
        coop_rates = np.concatenate(([my_rate], opp_rates))
        cooperative_count = int(np.sum(coop_rates >= self.p_ind))
        coop_fraction = cooperative_count / float(self.n)
        if self.last_was_exploit_harvest and self.coop_fraction_before_exploit is not None:
            drop = self.coop_fraction_before_exploit - coop_fraction
            collapse = drop >= 0.2 or coop_fraction < self.frac_req
            if collapse:
                candidate = int(max(2, math.ceil(self.r / 10.0)))
                max_allowed = max(0, remaining_rounds - 1)
                P_punish = min(candidate, max_allowed)
                if P_punish > 0:
                    self.in_punish_mode = True
                    self.punish_remaining = P_punish
                self.punish_failures += 1
                self.in_exploit_mode = False
                self.cycle_pos = None
                if self.punish_failures >= 2:
                    self.in_grim = True
                    self.last_was_exploit_harvest = False
                    self.coop_fraction_before_exploit = None
                    return Action.D
                self.last_was_exploit_harvest = False
                self.coop_fraction_before_exploit = None
                if self.in_punish_mode:
                    return Action.D
        if self.in_exploit_mode:
            if coop_fraction < self.frac_req:
                self.in_exploit_mode = False
                self.cycle_pos = None
                self.last_was_exploit_harvest = False
                self.coop_fraction_before_exploit = None
                self.last_coop_fraction = coop_fraction
                return Action.D
            if self.cycle_pos is None:
                self.cycle_pos = 1
            if 1 <= self.cycle_pos <= self.S_coop:
                action = Action.C
                self.cycle_pos += 1
                self.last_was_exploit_harvest = False
                self.coop_fraction_before_exploit = None
                self.last_coop_fraction = coop_fraction
                return action
            else:
                action = Action.D
                self.cycle_pos = 1
                self.last_was_exploit_harvest = True
                self.coop_fraction_before_exploit = coop_fraction
                self.last_coop_fraction = coop_fraction
                return action
        if coop_fraction >= self.frac_req and remaining_rounds >= self.S_coop + 1:
            self.in_exploit_mode = True
            self.cycle_pos = 1
            action = Action.C
            self.cycle_pos += 1
            self.last_was_exploit_harvest = False
            self.coop_fraction_before_exploit = None
            self.last_coop_fraction = coop_fraction
            return action
        if coop_fraction >= self.frac_req and remaining_rounds >= 2:
            action = Action.C
            self.in_exploit_mode = False
            self.cycle_pos = None
            self.last_was_exploit_harvest = False
            self.coop_fraction_before_exploit = None
            self.last_coop_fraction = coop_fraction
            return action
        self.last_coop_fraction = coop_fraction
        return Action.D

class Strategy_AGGRESSIVE_33(BaseStrategy):
    """
    Aggressive Grim-Prober (AGP)

    - Default: defect.
    - Probe briefly (random small prob of cooperating) to find reciprocators.
    - Only switch to cooperation when there have been >=2 consecutive unanimous-cooperation rounds
      and there are enough rounds left (not within the guard window).
    - If AGP cooperates and any player defects that same round, enter a punishment phase of
      punish_len rounds (defect for that many rounds). Punishment is heavy and immediate.
    - Always defect in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_unanimous = 0
        self.punishment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        if n < 2:
            n = max(2, n)
        if r < 1:
            r = max(1, r)
        m = k / float(n) if n != 0 else 0.0
        probe_rounds = max(1, min(3, math.floor(r / 4)))
        epsilon = min(0.15, 1.0 / float(r))
        guard = max(1, math.floor(r / 5))
        denom = max(1e-12, 1.0 - m)
        punish_len = max(2, math.ceil(2.0 / denom))
        t = int(state.round_number)
        if r == 1:
            return Action.D
        if t == r:
            return Action.D
        if history is None:
            if t <= probe_rounds:
                if random.random() < epsilon:
                    return Action.C
                else:
                    return Action.D
            return Action.D
        if history.round_number >= 1:
            prev_my_action = bool(history.my_actions[-1])
            prev_opponent_actions = history.opponent_actions[-1, :]
            total_cooperators_prev = int(prev_my_action) + int(np.sum(prev_opponent_actions.astype(int)))
            if total_cooperators_prev == n:
                self.consecutive_unanimous += 1
            else:
                self.consecutive_unanimous = 0
            if prev_my_action and np.any(prev_opponent_actions == False):
                self.punishment_counter = int(punish_len)
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            return Action.D
        if t <= probe_rounds:
            if random.random() < epsilon:
                return Action.C
            else:
                return Action.D
        if self.consecutive_unanimous >= 2 and t <= r - guard:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_34(BaseStrategy):
    """
    Aggressive Opportunistic Exploiter (AOE)

    Mostly always defect; infrequently run a cheap one-round probe (test) to find
    pockets of persistent cooperators. If a test indicates enough exploitable
    opponents, harvest by defecting for a short H_harvest period. If opponents
    punish (cooperation drops substantially), lock into permanent defection.
    Small random exploration (eps) allows re-checking the environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(10, max(1, self.r - 1))
        self.L_end = 1
        self.T_test = max(2, math.floor(self.r / 10)) if self.r > 10 else max(2, math.floor(self.r / 10))
        if self.r <= 3:
            self.T_test = max(self.r, 1000)
        self.H_harvest = min(5, max(1, math.floor(self.r / 6)))
        if self.r <= 3:
            self.H_harvest = 1
        self.eps = 0.03
        self.alpha_coop = 0.6
        self.alpha_after_defect = 0.5
        self.punish_drop = 0.3
        self.q_min = max(1, math.ceil(0.25 * (self.n - 1)))
        self.state = 'IDLE'
        self.pending_test_round = None
        self.exploit_remaining = 0
        self.exploit_start_round = None
        self.pre_test_avg_coop = None
        self.pre_test_window = (None, None)
        self.tests_conducted = 0
        self.tests_successful = 0
        self._eps_small = 1e-08

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        cur = history.round_number
        if cur >= max(0, self.r - self.L_end):
            return Action.D
        if self.state == 'PERMANENT_DEFECT':
            return Action.D
        if self.pending_test_round is not None and cur > self.pending_test_round:
            test_round = self.pending_test_round
            if test_round < history.opponent_actions.shape[0]:
                coopers_test = int(np.sum(history.opponent_actions[test_round, :]))
                coopers_fraction_test = coopers_test / max(1, self.n - 1)
                window_end = min(history.round_number, test_round + 1)
                window_start = max(0, window_end - self.W)
                opp_actions_window = history.opponent_actions[window_start:window_end, :] if window_end > window_start else np.zeros((0, self.n - 1), dtype=bool)
                if opp_actions_window.size == 0:
                    coop_rate = np.zeros(self.n - 1)
                else:
                    coop_rate = np.mean(opp_actions_window.astype(float), axis=0)
                my_actions = history.my_actions
                count_my_D_follow_pairs = 0
                coop_after_my_D = np.zeros(self.n - 1)
                for t in range(window_start, window_end - 1):
                    if not bool(my_actions[t]):
                        count_my_D_follow_pairs += 1
                        coop_after_my_D += history.opponent_actions[t + 1, :].astype(float)
                if count_my_D_follow_pairs > 0:
                    coop_after_my_D = coop_after_my_D / float(count_my_D_follow_pairs)
                else:
                    coop_after_my_D = coop_rate.copy()
                good_targets = 0
                for j in range(self.n - 1):
                    if coop_rate[j] >= self.alpha_coop and coop_after_my_D[j] >= self.alpha_after_defect:
                        good_targets += 1
                self.tests_conducted += 1
                if coopers_test >= self.q_min and good_targets >= self.q_min:
                    self.state = 'EXPLOIT'
                    self.exploit_remaining = self.H_harvest
                    self.exploit_start_round = test_round + 1
                    pre_end = test_round
                    pre_start = max(0, pre_end - self.W)
                    if pre_end > pre_start:
                        pre_window = history.opponent_actions[pre_start:pre_end, :]
                        self.pre_test_avg_coop = float(np.mean(pre_window.astype(float)))
                    else:
                        self.pre_test_avg_coop = float(coopers_fraction_test)
                    self.pre_test_window = (pre_start, pre_end)
                    self.tests_successful += 1
                else:
                    self.state = 'IDLE'
            self.pending_test_round = None
        if self.state == 'EXPLOIT' and self.exploit_start_round is not None:
            if history.round_number > self.exploit_start_round:
                post_start = self.exploit_start_round
                post_end = min(history.round_number, self.exploit_start_round + self.H_harvest)
                if post_end > post_start:
                    post_window = history.opponent_actions[post_start:post_end, :]
                    post_avg = float(np.mean(post_window.astype(float)))
                    pre_avg = float(self.pre_test_avg_coop) if self.pre_test_avg_coop is not None else 0.0
                    denom = max(pre_avg, self._eps_small)
                    drop_frac = (pre_avg - post_avg) / denom
                    if drop_frac > self.punish_drop:
                        self.state = 'PERMANENT_DEFECT'
                        self.exploit_remaining = 0
                        self.exploit_start_round = None
                        self.pre_test_avg_coop = None
                        return Action.D
        if self.tests_conducted >= max(1, math.floor(self.r / max(1, self.T_test))) and history.round_number > self.r // 2 and (self.tests_successful == 0):
            self.state = 'PERMANENT_DEFECT'
            return Action.D
        if self.state == 'EXPLOIT' and self.exploit_remaining > 0:
            action = Action.D
            self.exploit_remaining -= 1
            if self.exploit_remaining <= 0:
                self.state = 'IDLE'
                self.exploit_start_round = None
            return action
        if self.state == 'IDLE' and self.T_test > 0:
            if cur > 0 and cur % self.T_test == 0 and (cur < self.r - self.L_end):
                self.pending_test_round = cur
                self.state = 'TESTING'
                return Action.C
        if self.state != 'PERMANENT_DEFECT' and cur < self.r - self.L_end:
            if random.random() < self.eps:
                self.pending_test_round = cur
                self.state = 'TESTING'
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_35(BaseStrategy):
    """
    Aggressive Exploit-and-Grim (AEG)

    - Default: defect (aggressive baseline).
    - Probe periodically by playing C once to test whether others reciprocate;
      if they do, exploit (defect) while cooperation persists.
    - If cooperation collapses (below T_low) or a probe fails, switch to permanent
      defection (grim).
    - Always defect in the final round.
    - Small epsilon randomization to avoid deterministic cycles.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.mode = 'EXPLOIT'
        self.rounds_since_last_probe = 0
        self.probe_scheduled = False
        self.in_probe = False
        self.probe_start_round = None
        self.M_probe = None
        r = max(1, getattr(game_description, 'n_rounds', 1))
        self.W = min(5, max(0, r - 1))
        self.P_probe = max(3, math.floor(r / 6)) if r > 6 else max(3, math.floor(r / 6))
        self.B_exploit = 1
        self.T_high = 0.6
        self.T_low = 0.35
        self.T_high_response = max(0.55, self.T_high - 0.05)
        self.epsilon = 0.02

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = getattr(state, 'round_number', None)
        completed = 0
        if history is not None:
            completed = int(history.round_number)
        if t is None:
            t = completed + 1
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)

        def maybe_flip(action: Action) -> Action:
            if random.random() < self.epsilon:
                return Action.C if action == Action.D else Action.D
            return action
        if history is None or completed == 0:
            self.rounds_since_last_probe += 1
            return maybe_flip(Action.D)
        if t >= r:
            self.mode = 'PUNISH'
            return maybe_flip(Action.D)
        if self.mode == 'PUNISH':
            return maybe_flip(Action.D)
        if self.in_probe:
            if t == self.probe_start_round:
                return maybe_flip(Action.C)
            if self.M_probe is None:
                self.M_probe = 1
            probe_eval_end = self.probe_start_round + self.M_probe - 1
            if completed >= probe_eval_end:
                start_idx = max(0, self.probe_start_round - 1)
                end_idx = min(completed - 1, probe_eval_end - 1)
                num_rounds_window = end_idx - start_idx + 1
                if num_rounds_window <= 0:
                    coop_rate = 0.0
                else:
                    window_actions = history.opponent_actions[start_idx:end_idx + 1, :]
                    coop_count = int(np.sum(window_actions))
                    coop_rate = coop_count / ((n - 1) * num_rounds_window)
                if coop_rate >= self.T_high_response:
                    self.mode = 'EXPLOIT'
                    self.in_probe = False
                    self.probe_start_round = None
                    self.M_probe = None
                    self.rounds_since_last_probe = 0
                else:
                    self.mode = 'PUNISH'
                    self.in_probe = False
                    self.probe_start_round = None
                    self.M_probe = None
                return maybe_flip(Action.D)
            else:
                return maybe_flip(Action.D)
        W_t = min(self.W, completed)
        if W_t <= 0:
            coop_rate = 0.0
        else:
            window = history.opponent_actions[-W_t:, :] if W_t > 0 else np.empty((0, n - 1), dtype=bool)
            coop_count = int(np.sum(window)) if window.size > 0 else 0
            coop_rate = coop_count / ((n - 1) * W_t)
        if coop_rate < self.T_low:
            self.mode = 'PUNISH'
            return maybe_flip(Action.D)
        if coop_rate >= self.T_high:
            self.rounds_since_last_probe += 1
            return maybe_flip(Action.D)
        rounds_remaining = r - t
        min_remaining_for_probe = 2
        if self.rounds_since_last_probe >= self.P_probe and rounds_remaining >= min_remaining_for_probe:
            self.probe_scheduled = True
            self.rounds_since_last_probe = 0
            return maybe_flip(Action.D)
        if self.probe_scheduled:
            self.in_probe = True
            self.probe_scheduled = False
            self.probe_start_round = t
            self.M_probe = min(3, max(1, r - t + 1))
            self.rounds_since_last_probe = 0
            return maybe_flip(Action.C)
        self.rounds_since_last_probe += 1
        return maybe_flip(Action.D)

class Strategy_AGGRESSIVE_36(BaseStrategy):
    """
    Aggressive Grim with Reset (AGR)

    - Default MODE = "grim": start with C, cooperate only while every past round was unanimous C,
      permanently switch to D after any observed defection, and always play D in the final round.
    - MODE = "finite-punish": instead of permanent punishment, punish for P rounds (pun_until_round)
      when a defection is observed and require R_reset consecutive unanimous-C rounds to reset.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.MODE = 'grim'
        self.P = max(2, math.ceil(self.game_description.n_rounds / 4))
        self.R_reset = 2
        self.ever_defected = False
        self.pun_until_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        r = self.game_description.n_rounds
        if history is None:
            return Action.C
        if t == r:
            return Action.D

        def last_round_all_C() -> bool:
            if history.round_number == 0:
                return False
            my_last = bool(history.my_actions[-1])
            opp_last = True
            if np.size(history.opponent_actions) > 0:
                opp_last = bool(np.all(history.opponent_actions[-1, :]))
            return my_last and opp_last

        def consecutive_allC(s: int) -> bool:
            if s <= 0:
                return True
            if history.round_number < s:
                return False
            my_segment = history.my_actions[-s:]
            opp_segment = history.opponent_actions[-s:, :] if np.size(history.opponent_actions) > 0 else np.ones((s, 0), dtype=bool)
            return bool(np.all(my_segment)) and bool(np.all(opp_segment))
        if not self.ever_defected:
            my_all_coop = True
            opp_all_coop = True
            if np.size(history.my_actions) > 0:
                my_all_coop = bool(np.all(history.my_actions))
            if np.size(history.opponent_actions) > 0:
                opp_all_coop = bool(np.all(history.opponent_actions))
            if not (my_all_coop and opp_all_coop):
                self.ever_defected = True
        if self.MODE == 'grim':
            if not self.ever_defected:
                if last_round_all_C():
                    return Action.C
                else:
                    self.ever_defected = True
                    return Action.D
            return Action.D
        if self.MODE == 'finite-punish':
            if t <= self.pun_until_round:
                return Action.D
            if history.round_number > 0:
                last_round_my = bool(history.my_actions[-1])
                last_round_opps = True
                if np.size(history.opponent_actions) > 0:
                    last_round_opps = bool(np.all(history.opponent_actions[-1, :]))
                if not (last_round_my and last_round_opps):
                    self.pun_until_round = t + max(1, int(self.P)) - 1
                    return Action.D
            if self.pun_until_round == 0:
                if last_round_all_C():
                    return Action.C
                else:
                    return Action.D
            if self.pun_until_round > 0:
                if consecutive_allC(self.R_reset):
                    self.pun_until_round = 0
                    return Action.C
                else:
                    return Action.D
            return Action.D
        if not self.ever_defected:
            if last_round_all_C():
                return Action.C
            else:
                self.ever_defected = True
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_37(BaseStrategy):
    """
    Opportunistic Exploiter (aggressive)

    Aggressive, adaptive public-goods strategy:
    - Default: defect to exploit cooperators.
    - Probe rarely in early rounds to find naive cooperators.
    - Detect if opponents punish our defections; if so, enter a temporary RETREAT
      state where we concede (cooperate with some probability) until cooperation
      by others recovers or punishment signals disappear.
    - Always defect in final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        if self.n <= 1:
            denom = 1.0
        else:
            denom = float(self.n - 1)
        raw_probe = 0.2 * (self.k - 1.0) / denom
        self.probe_prob = min(max(raw_probe, 0.05), 0.25)
        raw_retreat = 0.6 * (self.k - 1.0) / denom
        self.retreat_coop_prob = min(max(raw_retreat, 0.2), 0.8)
        self.probe_rounds = min(3, max(0, self.r - 1))
        self.punish_detect_threshold = 0.15
        self.punish_window_min = 3
        self.in_retreat = False
        self.retreat_baseline = 0.0
        self.recovery_consec = 0
        seed = int(self.n * 1000003 + round(self.k * 1009) + self.r * 101)
        self.rng = random.Random(seed)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = self.n
        r = self.r
        if t == r:
            self.in_retreat = False
            return Action.D
        if r <= 2:
            self.in_retreat = False
            return Action.D
        if history is None or history.round_number == 0:
            if t <= self.probe_rounds and self.probe_rounds > 0:
                if self.rng.random() < self.probe_prob:
                    return Action.C
            return Action.D
        completed = history.round_number
        w = min(5, max(0, t - 1))
        if w == 0:
            return Action.D
        if t <= self.probe_rounds and self.probe_rounds > 0:
            if self.rng.random() < self.probe_prob:
                return Action.C
            else:
                return Action.D
        start_idx = max(0, completed - w)
        recent_slice = history.opponent_actions[start_idx:completed, :]
        total_coops = int(np.sum(recent_slice))
        denom = float((n - 1) * w) if (n - 1) * w > 0 else 1.0
        p_recent = float(total_coops) / denom
        list_defected_followers = []
        list_cooperated_followers = []
        last_index_for_follow = completed - 2
        for s in range(0, last_index_for_follow + 1):
            my_act = bool(history.my_actions[s])
            others_next = history.opponent_actions[s + 1, :]
            others_next_frac = float(np.sum(others_next)) / float(max(1, n - 1))
            if my_act:
                list_cooperated_followers.append(others_next_frac)
            else:
                list_defected_followers.append(others_next_frac)
        punish_detected = False
        p_after_I_defected = None
        p_after_I_cooperated = None
        if len(list_defected_followers) >= self.punish_window_min and len(list_cooperated_followers) >= self.punish_window_min:
            p_after_I_defected = float(np.mean(list_defected_followers))
            p_after_I_cooperated = float(np.mean(list_cooperated_followers))
            if p_after_I_cooperated - p_after_I_defected >= self.punish_detect_threshold:
                punish_detected = True
        remaining = r - t + 1
        K_recover = min(3, max(-1000, remaining - 1))
        if K_recover <= 0:
            K_recover = 0
        if punish_detected and (not self.in_retreat):
            self.in_retreat = True
            self.retreat_baseline = p_recent
            self.recovery_consec = 0
        if self.in_retreat:
            punishment_still_present = False
            if len(list_defected_followers) >= self.punish_window_min and len(list_cooperated_followers) >= self.punish_window_min:
                p_after_I_defected = float(np.mean(list_defected_followers))
                p_after_I_cooperated = float(np.mean(list_cooperated_followers))
                if p_after_I_cooperated - p_after_I_defected >= self.punish_detect_threshold:
                    punishment_still_present = True
            if not punishment_still_present:
                self.in_retreat = False
                self.recovery_consec = 0
            else:
                if completed >= 1 and K_recover > 0:
                    last_others_frac = float(np.sum(history.opponent_actions[completed - 1, :])) / float(max(1, n - 1))
                    if last_others_frac >= self.retreat_baseline + 0.05:
                        self.recovery_consec += 1
                    else:
                        self.recovery_consec = 0
                    if self.recovery_consec >= K_recover:
                        self.in_retreat = False
                        self.recovery_consec = 0
                        return Action.D
                elif K_recover == 0:
                    self.in_retreat = False
                    self.recovery_consec = 0
                    return Action.D
                if self.rng.random() < self.retreat_coop_prob:
                    return Action.C
                else:
                    return Action.D
        if p_recent >= 0.9 and remaining >= 4:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_38(BaseStrategy):
    """
    Aggressive Exploit-and-Punish (AEP)

    Default is defect. Track short-window cooperation rates for each opponent and the group.
    Exploit persistent cooperators and largely-cooperative groups by defecting. Rarely probe
    (enter a short rebuild mode of S rounds) to see if cooperation can be re-established;
    if anyone defects during a rebuild attempt, switch to grim (permanent defection).
    Always defect in the final E rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game = game_description
        n = self.game.n_players
        r = self.game.n_rounds
        self.W = int(min(5, max(1, math.floor(r / 10))))
        self.group_exploit_threshold = 0.7
        self.individual_cooperator_threshold = 0.85
        self.S = int(min(2, max(1, math.floor(r / 20))))
        self.probe_prob = min(0.1, 3 / max(1, r))
        self.E = 1
        self.in_rebuild_mode = 0
        self.grim = False
        self.rebuild_flag_history = []
        self.last_processed_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        hist_rounds = history.round_number if history is not None else 0
        if history is not None and hist_rounds > self.last_processed_round:
            for r_idx in range(self.last_processed_round, hist_rounds):
                if self.rebuild_flag_history:
                    was_rebuild_round = self.rebuild_flag_history.pop(0)
                else:
                    was_rebuild_round = False
                if was_rebuild_round:
                    try:
                        my_cooperated = bool(history.my_actions[r_idx])
                        opps = history.opponent_actions[r_idx, :]
                    except Exception:
                        my_cooperated = False
                        opps = np.array([], dtype=bool)
                    if my_cooperated:
                        if opps.size > 0 and (not np.all(opps)):
                            self.grim = True
                            self.in_rebuild_mode = 0
                            self.rebuild_flag_history = []
            self.last_processed_round = hist_rounds
        t = state.round_number

        def compute_rates():
            n_players = self.game.n_players
            if history is None or history.round_number == 0:
                coop_counts = np.zeros(n_players - 1, dtype=float)
                group_coop_rate = 0.0
                coop_rates = coop_counts / max(1, self.W)
                return (coop_rates, group_coop_rate)
            total_rounds = history.round_number
            window_size = self.W
            start_idx = max(0, total_rounds - window_size)
            window = history.opponent_actions[start_idx:total_rounds, :]
            if window.size == 0:
                coop_counts = np.zeros(history.opponent_actions.shape[1], dtype=float)
                group_coop_rate = 0.0
            else:
                coop_counts = np.sum(window, axis=0).astype(float)
                group_coop_rate = float(np.sum(window)) / (window_size * history.opponent_actions.shape[1])
            coop_rates = coop_counts / max(1, window_size)
            return (coop_rates, group_coop_rate)
        if t > self.game.n_rounds - self.E:
            self.rebuild_flag_history.append(False)
            return Action.D
        if self.grim:
            self.rebuild_flag_history.append(False)
            return Action.D
        if t == 1:
            self.rebuild_flag_history.append(False)
            return Action.D
        coop_rates, group_coop_rate = compute_rates()
        if coop_rates.size > 0 and np.any(coop_rates >= self.individual_cooperator_threshold):
            self.rebuild_flag_history.append(False)
            return Action.D
        if group_coop_rate >= self.group_exploit_threshold:
            self.rebuild_flag_history.append(False)
            return Action.D
        if self.in_rebuild_mode > 0:
            self.in_rebuild_mode = max(0, self.in_rebuild_mode - 1)
            self.rebuild_flag_history.append(True)
            return Action.C
        if group_coop_rate <= 0.15 and random.random() < self.probe_prob:
            remaining_after_this = max(0, self.S - 1)
            self.in_rebuild_mode = remaining_after_this
            self.rebuild_flag_history.append(True)
            return Action.C
        self.rebuild_flag_history.append(False)
        return Action.D

class Strategy_AGGRESSIVE_39(BaseStrategy):
    """
    Aggressive Conditional Exploiter (ACE)

    - Default: defect.
    - If enough non-blacklisted opponents have recently cooperated frequently, start an S-round
      cooperation "offer" (cooperate for S consecutive rounds).
    - If the offer completes without betrayal, assume a cooperating cluster and switch to
      Exploiting mode: defect each round to free-ride on them.
    - If anyone defects during an offer, permanently blacklist them and enter a Punishing window.
    - Never cooperate in the final round.
    - Uses sliding-window statistics of opponent cooperation to detect frequent cooperators.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, max(1, math.floor(self.r / 10)))
        denom = 1.0 - self.k / float(self.n)
        if denom <= 0:
            S_calc = self.r - 1
        else:
            S_calc = math.ceil(2.0 / denom)
        self.S = min(max(2, S_calc), max(0, self.r - 1))
        self.P = min(max(0, self.r - 1), 2 * self.S)
        self.T_ind = 0.8
        self.M = int(math.floor(0.6 * (self.n - 1)))
        self.blacklisted = np.zeros(self.n - 1, dtype=bool)
        self.cluster_members = np.zeros(self.n - 1, dtype=bool)
        self.mode = 'Default'
        self.rounds_left_in_mode = 0
        self._last_processed_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
            completed_rounds = 0
        else:
            completed_rounds = history.round_number
            t = completed_rounds + 1
        lookback = min(self.W, max(0, t - 1))
        if history is None or lookback == 0:
            coop_rates = np.zeros(self.n - 1, dtype=float)
        else:
            recent = history.opponent_actions[-lookback:, :]
            coop_rates = np.sum(recent.astype(float), axis=0) / float(lookback)
        coopful = int(np.sum((coop_rates >= self.T_ind) & ~self.blacklisted))
        if completed_rounds >= 1 and completed_rounds > self._last_processed_rounds:
            my_last = bool(history.my_actions[-1])
            opp_last = history.opponent_actions[-1, :]
            if self.mode == 'Offering':
                if my_last:
                    betrayers_mask = ~opp_last
                    if np.any(betrayers_mask):
                        self.blacklisted = self.blacklisted | betrayers_mask
                        self.mode = 'Punishing'
                        self.rounds_left_in_mode = self.P
                    else:
                        self.rounds_left_in_mode = max(0, self.rounds_left_in_mode - 1)
                        if self.rounds_left_in_mode <= 0:
                            self.mode = 'Exploiting'
                            remaining = max(0, self.r - completed_rounds)
                            self.rounds_left_in_mode = remaining
                            offer_rows = min(self.S, completed_rounds)
                            if offer_rows > 0:
                                last_offers = history.opponent_actions[-offer_rows:, :]
                                all_coop = np.all(last_offers, axis=0)
                                self.cluster_members = all_coop & ~self.blacklisted
                            else:
                                self.cluster_members = np.zeros(self.n - 1, dtype=bool)
                else:
                    self.mode = 'Default'
                    self.rounds_left_in_mode = 0
            elif self.mode == 'Punishing':
                self.rounds_left_in_mode = max(0, self.rounds_left_in_mode - 1)
                if self.rounds_left_in_mode <= 0:
                    if t <= self.r - self.S and coopful >= self.M:
                        self.mode = 'Offering'
                        self.rounds_left_in_mode = self.S
                    else:
                        self.mode = 'Default'
                        self.rounds_left_in_mode = 0
            elif self.mode == 'Exploiting':
                if np.any(self.cluster_members):
                    betrayed_mask = self.cluster_members & ~opp_last
                    if np.any(betrayed_mask):
                        self.blacklisted = self.blacklisted | betrayed_mask
                        self.cluster_members = self.cluster_members & ~betrayed_mask
                collapse = False
                if completed_rounds >= 2:
                    denom_mask = ~self.blacklisted
                    denom_count = int(np.sum(denom_mask))
                    if denom_count > 0:
                        last_two = history.opponent_actions[-2:, :]
                        frac1 = float(np.sum(last_two[0, denom_mask])) / float(denom_count)
                        frac2 = float(np.sum(last_two[1, denom_mask])) / float(denom_count)
                        if frac1 < 0.5 and frac2 < 0.5:
                            collapse = True
                    else:
                        collapse = True
                remaining_rounds = max(0, self.r - completed_rounds)
                if collapse or remaining_rounds <= math.floor(self.S / 2):
                    self.mode = 'Default'
                    self.rounds_left_in_mode = 0
                    self.cluster_members = np.zeros(self.n - 1, dtype=bool)
            self._last_processed_rounds = completed_rounds
        if t >= self.r:
            return Action.D
        lookback = min(self.W, max(0, t - 1))
        if history is None or lookback == 0:
            coop_rates = np.zeros(self.n - 1, dtype=float)
        else:
            recent = history.opponent_actions[-lookback:, :]
            coop_rates = np.sum(recent.astype(float), axis=0) / float(lookback)
        coopful = int(np.sum((coop_rates >= self.T_ind) & ~self.blacklisted))
        if self.mode == 'Default':
            if t <= self.r - self.S and coopful >= self.M and (self.S >= 1):
                self.mode = 'Offering'
                self.rounds_left_in_mode = self.S
                return Action.C
            else:
                return Action.D
        elif self.mode == 'Offering':
            return Action.C
        elif self.mode == 'Exploiting':
            return Action.D
        elif self.mode == 'Punishing':
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_40(BaseStrategy):
    """
    Exploit-and-Punish (Aggressor)

    - Default: defect (exploit unconditional cooperators).
    - Probe briefly at the start (but still defect by default in the first rounds).
    - Identify reliable cooperators by coop_rate >= T_hi.
    - Cooperate only when there are enough reliable partners and enough remaining rounds
      to recoup the one-time cooperation cost (mathematically computed).
    - If a trusted partner "betrays" (defects when we expected cooperation), start a short,
      high-impact public punishment (play D for a calculated number of rounds). Forgive if
      the player re-establishes reliable cooperation after punishment; otherwise, reapply/escalate.
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.T_hi = 0.8
        self.T_lo = 0.3
        self.beta = 0.5
        self.P_max = math.ceil(0.5 * self.r)
        self.M_min = 1
        self.Probe_rounds = min(2, max(0, self.r - 1))
        self.punishments = {}
        self.betrayals_recorded = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = self.n
        r = self.r
        k = self.k
        k_over_n = k / n
        cost_of_cooperating_now = 1.0 - k_over_n
        if t == r:
            return Action.D
        if history is None:
            return Action.D
        remaining_rounds_after_current = r - t
        completed_rounds = history.round_number
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        num_opponents = n - 1
        if num_opponents <= 0:
            return Action.D
        if completed_rounds == 0:
            coop_counts = np.zeros((num_opponents,), dtype=int)
            coop_rates = np.zeros((num_opponents,), dtype=float)
        else:
            coop_counts = np.sum(opp_actions.astype(int), axis=0)
            denom = max(1, completed_rounds)
            coop_rates = coop_counts.astype(float) / float(denom)

        def detect_and_register_betrayals():
            for s_idx in range(1, completed_rounds + 1):
                my_cooperated = bool(my_actions[s_idx - 1])
                if not my_cooperated:
                    continue
                for j in range(num_opponents):
                    opp_played = bool(opp_actions[s_idx - 1, j])
                    if opp_played:
                        continue
                    key = (j, s_idx)
                    if key in self.betrayals_recorded:
                        continue
                    prior_rounds = s_idx - 1
                    if prior_rounds <= 0:
                        prior_coop_rate = 0.0
                    else:
                        prior_coop_count = int(np.sum(opp_actions[:prior_rounds, j].astype(int)))
                        prior_coop_rate = prior_coop_count / float(max(1, prior_rounds))
                    cooperated_previous_round = False
                    if prior_rounds >= 1:
                        cooperated_previous_round = bool(opp_actions[prior_rounds - 1, j])
                    if prior_coop_rate >= self.T_hi and cooperated_previous_round:
                        self.betrayals_recorded.add(key)
                        remaining_after_betrayal = r - s_idx
                        L = max(1, math.ceil(self.beta * remaining_after_betrayal))
                        L = min(self.P_max, L)
                        end_round = s_idx + L
                        if j not in self.punishments:
                            self.punishments[j] = []
                        self.punishments[j].append({'start': s_idx, 'end': end_round})
        detect_and_register_betrayals()
        any_active_punishment = False
        to_remove_entries = []
        reapplications = []
        for j, entries in list(self.punishments.items()):
            new_entries_for_j = []
            for idx, entry in enumerate(entries):
                s = int(entry['start'])
                e = int(entry['end'])
                if s < t <= e:
                    any_active_punishment = True
                    new_entries_for_j.append(entry)
                    continue
                if t > e:
                    start_check = e + 1
                    end_check = t - 1
                    if start_check <= end_check and completed_rounds >= start_check:
                        a = int(start_check - 1)
                        b = int(end_check - 1)
                        window_len = b - a + 1
                        coop_in_window = int(np.sum(opp_actions[a:b + 1, j].astype(int)))
                        coop_rate_since = coop_in_window / float(max(1, window_len))
                    else:
                        coop_rate_since = 0.0
                    if coop_rate_since >= self.T_hi:
                        continue
                    else:
                        remaining_rounds = r - t + 1
                        if remaining_rounds > 0:
                            L2 = max(1, math.ceil(self.beta * (r - t)))
                            L2 = min(self.P_max, L2)
                            new_start = t - 1
                            new_end = new_start + L2
                            reapplications.append((j, {'start': new_start, 'end': new_end}))
                        continue
                new_entries_for_j.append(entry)
            if new_entries_for_j:
                self.punishments[j] = new_entries_for_j
            else:
                self.punishments.pop(j, None)
        for j, ent in reapplications:
            if j not in self.punishments:
                self.punishments[j] = []
            if ent not in self.punishments[j]:
                self.punishments[j].append(ent)
                if ent['start'] < t <= ent['end']:
                    any_active_punishment = True
        if any_active_punishment:
            return Action.D
        if t <= self.Probe_rounds:
            return Action.D
        S = []
        if completed_rounds >= 1:
            last_round_actions = opp_actions[completed_rounds - 1, :]
            for j in range(num_opponents):
                if coop_rates[j] >= self.T_hi and bool(last_round_actions[j]):
                    S.append(j)
        s = len(S)
        if s == 0:
            return Action.D
        if remaining_rounds_after_current <= 0:
            return Action.D
        denom = remaining_rounds_after_current * k_over_n
        if denom <= 0:
            s_needed = float('inf')
        else:
            s_needed = math.ceil(cost_of_cooperating_now / denom)
        threshold = max(self.M_min, int(s_needed))
        if s >= threshold:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_41(BaseStrategy):
    """
    Adaptive Aggressor for N-Player Public Goods Game.

    Aggressive, history-aware strategy that:
    - Probes/cooperates briefly at start;
    - Punishes mass or persistent defection with multi-round defections;
    - Opportunistically exploits naïve cooperators and late-stage cooperation;
    - Uses a short forgiveness observation window after punishment ends before
      returning to normal cooperative default.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_timer = 0
        self.in_punishment = False
        self.awaiting_forgiveness = False
        self.forgiveness_rounds_remaining = 0
        self._prev_punishment_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory):
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        INITIAL_ROUNDS = min(2, max(1, r - 1))
        denom = max(1e-09, 1.0 - k / float(n))
        PUNISH_BASE = max(1, math.ceil(3.0 / denom))
        PUNISH_MAX = r
        P = min(PUNISH_MAX, PUNISH_BASE)
        MAJOR_DEFECT_FRACTION = 0.25
        PERSISTENT_DEFECTOR_THRESH = 0.4
        NAIVE_COOP_THRESH = 0.95
        LATE_EXPLOIT_REMAIN = 3
        LATE_EXPLOIT_AVG = 0.8
        FORGIVE_WINDOW = max(1, math.ceil(r / 10))
        EPSILON = 0.02
        if history is None:
            t = 1
            rem = r - t + 1
            opponent_count = n - 1
            p_j = np.full(opponent_count, 0.5)
            short_term_p_j = np.full(opponent_count, 0.5)
            avg_coop = 0.5
            m_last = 0
        else:
            completed = history.round_number
            t = completed + 1
            rem = r - t + 1
            opponent_actions = np.asarray(history.opponent_actions, dtype=np.bool_)
            opponent_count = opponent_actions.shape[1] if opponent_actions.ndim == 2 else n - 1
            if completed > 0:
                sums = np.sum(opponent_actions, axis=0)
                p_j = sums / float(completed)
            else:
                p_j = np.full(n - 1, 0.5)
            W = min(6, max(1, completed))
            if completed > 0:
                short_window = opponent_actions[-W:, :] if completed >= W else opponent_actions
                short_sums = np.sum(short_window, axis=0)
                short_term_p_j = short_sums / float(min(W, completed))
            else:
                short_term_p_j = np.full(n - 1, 0.5)
            avg_coop = float(np.mean(p_j)) if p_j.size > 0 else 0.5
            if completed >= 1:
                last_opponent_round = opponent_actions[-1, :]
                my_last = bool(history.my_actions[-1])
                m_last = int(np.sum(last_opponent_round)) + (1 if my_last else 0)
            else:
                m_last = 0
        chosen_action = Action.C
        if t == r:
            chosen_action = Action.D
        elif self.punishment_timer > 0:
            chosen_action = Action.D
            self.punishment_timer = max(0, self.punishment_timer - 1)
            if self.punishment_timer == 0 and self.in_punishment:
                self.in_punishment = False
                self.awaiting_forgiveness = True
                self.forgiveness_rounds_remaining = FORGIVE_WINDOW
        else:
            if self.awaiting_forgiveness:
                coop_recent = 0.0
                if history is not None and history.opponent_actions.size > 0:
                    opponent_actions = np.asarray(history.opponent_actions, dtype=np.bool_)
                    available = opponent_actions.shape[0]
                    window = min(FORGIVE_WINDOW, available)
                    if window > 0:
                        coop_recent = float(np.sum(opponent_actions[-window:, :]) / (window * (n - 1)))
                    else:
                        coop_recent = 0.0
                else:
                    coop_recent = 0.0
                if coop_recent >= 0.6:
                    self.awaiting_forgiveness = False
                    self.forgiveness_rounds_remaining = 0
                else:
                    self.forgiveness_rounds_remaining = max(0, self.forgiveness_rounds_remaining - 1)
                    if self.forgiveness_rounds_remaining == 0:
                        self.awaiting_forgiveness = False
                    chosen_action = Action.D
                    early_return = True
                    if random.random() < EPSILON:
                        chosen_action = Action.C if chosen_action == Action.D else Action.D
                    return chosen_action
            if t <= INITIAL_ROUNDS:
                chosen_action = Action.C
            elif rem <= LATE_EXPLOIT_REMAIN and avg_coop >= LATE_EXPLOIT_AVG:
                chosen_action = Action.D
            elif history is not None and m_last < n and (history.round_number >= 1):
                fraction_defected_last = float((n - m_last) / float(n))
                if fraction_defected_last >= MAJOR_DEFECT_FRACTION:
                    set_len = min(P, max(0, rem - 1))
                    self.punishment_timer = set_len
                    if set_len > 0:
                        self.in_punishment = True
                    chosen_action = Action.D
                else:
                    last_opponent_round = np.asarray(history.opponent_actions[-1, :], dtype=np.bool_)
                    defectors_indices = [idx for idx, val in enumerate(last_opponent_round) if not val]
                    persistent_found = False
                    for idx in defectors_indices:
                        if idx < len(short_term_p_j) and short_term_p_j[idx] <= PERSISTENT_DEFECTOR_THRESH:
                            persistent_found = True
                            break
                    if persistent_found:
                        set_len = min(P, max(0, rem - 1))
                        self.punishment_timer = set_len
                        if set_len > 0:
                            self.in_punishment = True
                        chosen_action = Action.D
                    else:
                        chosen_action = Action.C
            elif any((p_j >= NAIVE_COOP_THRESH for p_j in p_j)) and rem <= math.ceil(r / 4.0):
                if random.random() < 0.5:
                    chosen_action = Action.D
                else:
                    chosen_action = Action.C if avg_coop >= 0.5 else Action.D
            else:
                chosen_action = Action.C if avg_coop >= 0.5 else Action.D
        if random.random() < EPSILON:
            chosen_action = Action.C if chosen_action == Action.D else Action.D
        return chosen_action

class Strategy_AGGRESSIVE_42(BaseStrategy):
    """
    Aggressive Exploiter with Conditional Punishment (AECP).

    - Default: defect.
    - Probe occasionally early to reveal conditional cooperators.
    - If a sufficient fraction of opponents have been cooperating recently (f_t >= T_exploit),
      exploit by defecting. Record baseline cooperation level when exploitation starts.
    - If opponents reduce cooperation by more than delta_retaliation within 1-2 rounds after exploitation
      (interpreted as retaliation), enter a short punishment phase of pure defection for L_punish rounds.
    - Always defect in the final E rounds (endgame safety).
    - Small epsilon randomization flips actions occasionally to avoid full predictability.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, max(1, self.r - 1))
        self.T_exploit = 0.5
        self.delta_retaliation = 0.2
        self.L_punish = min(3, self.r)
        self.p_probe = 0.1
        self.probe_phase_len = min(3, max(1, self.r - 1))
        self.E = min(3, self.r)
        self.eps_random = 0.02
        self.remaining_punish = 0
        self.f_baseline = 0.0
        self.in_exploit = False
        self.exploit_start_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            completed = 0
            t = 1
        else:
            completed = history.round_number
            t = completed + 1
        if t > self.r - self.E:
            action = Action.D
            if random.random() < self.eps_random:
                action = Action.C if action == Action.D else Action.D
            return action
        window_len = min(self.w, max(0, t - 1))
        if window_len <= 0 or history is None:
            f_t = 0.0
        else:
            recent = history.opponent_actions[-window_len:, :]
            total_coop = float(np.sum(recent))
            denom = float((self.n - 1) * window_len)
            f_t = total_coop / denom if denom > 0.0 else 0.0
        if self.remaining_punish > 0:
            action = Action.D
            self.remaining_punish -= 1
            if self.remaining_punish == 0:
                self.in_exploit = False
                self.exploit_start_round = None
            if random.random() < self.eps_random:
                action = Action.C if action == Action.D else Action.D
            return action
        if t <= self.probe_phase_len:
            if random.random() < self.p_probe:
                action = Action.C
            else:
                action = Action.D
            if random.random() < self.eps_random:
                action = Action.C if action == Action.D else Action.D
            return action
        action = Action.D
        if f_t >= self.T_exploit:
            if not self.in_exploit:
                self.in_exploit = True
                self.f_baseline = f_t
                self.exploit_start_round = t
                action = Action.D
            else:
                action = Action.D
                if self.exploit_start_round is not None:
                    rounds_since = t - self.exploit_start_round
                else:
                    rounds_since = 999
                if rounds_since <= 2 and self.f_baseline - f_t > self.delta_retaliation:
                    self.remaining_punish = int(self.L_punish)
                    if self.remaining_punish > 0:
                        self.remaining_punish -= 1
                    self.in_exploit = False
                    self.exploit_start_round = None
                    action = Action.D
                    if self.remaining_punish == 0:
                        self.in_exploit = False
                    if random.random() < self.eps_random:
                        action = Action.C if action == Action.D else Action.D
                    return action
        else:
            self.in_exploit = False
            self.exploit_start_round = None
            action = Action.D
        if random.random() < self.eps_random:
            action = Action.C if action == Action.D else Action.D
        return action

class Strategy_AGGRESSIVE_43(BaseStrategy):
    """
    Aggressive Opportunistic Exploiter (AOE)

    Aggressive, adaptive public-goods player:
    - Defaults to defection, probes rarely early to learn opponents.
    - If many unconditional cooperators are detected -> permanently exploit (always D).
    - If a sufficient set of conditional cooperators are detected (they cooperate more after I cooperate),
      perform short bait-and-exploit cycles: a brief C to induce cooperation followed by D to free-ride.
    - If many opponents punish my defections (they reduce cooperation after I defect), back off and reduce baiting.
    - Always defect in the final round. For very short games (r <= 3) always defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.last_action = None
        self.high_coop_frac = 0.6
        self.cond_delta_thr = 0.15
        self.cond_count_thr_frac = 0.25
        self.punishment_drop = 0.2
        self.bait_length = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        if r <= 3:
            self.last_action = Action.D
            return Action.D
        t = int(state.round_number)
        if history is None or history.round_number == 0:
            self.last_action = Action.D
            return Action.D
        if t == r:
            self.last_action = Action.D
            return Action.D
        W = min(5, max(0, r - 1))
        probe_prob = min(0.15, 3.0 / r)
        rounds_completed = history.round_number
        H = min(W, rounds_completed)
        if H <= 0:
            if t <= 1 + math.ceil(math.sqrt(r)):
                action = Action.C if random.random() < probe_prob else Action.D
            else:
                action = Action.C if random.random() < probe_prob / 2.0 else Action.D
            self.last_action = action
            return action
        start_idx = rounds_completed - H
        end_idx = rounds_completed - 1
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        n_opponents = n - 1
        coop_given_myC = np.zeros(n_opponents, dtype=float)
        count_given_myC = np.zeros(n_opponents, dtype=float)
        coop_given_myD = np.zeros(n_opponents, dtype=float)
        count_given_myD = np.zeros(n_opponents, dtype=float)
        other_coop_vals = []
        coop_after_myC_vals = []
        coop_after_myD_vals = []
        for r_idx in range(start_idx, end_idx + 1):
            prev_idx = r_idx - 1
            if prev_idx < 0:
                continue
            prev_my_cooperated = bool(my_actions[prev_idx])
            opp_coop_fraction = float(np.mean(opp_actions[r_idx, :])) if n_opponents > 0 else 0.0
            other_coop_vals.append(opp_coop_fraction)
            if prev_my_cooperated:
                coop_after_myC_vals.append(opp_coop_fraction)
            else:
                coop_after_myD_vals.append(opp_coop_fraction)
            for j in range(n_opponents):
                did_coop = bool(opp_actions[r_idx, j])
                if prev_my_cooperated:
                    coop_given_myC[j] += 1.0 if did_coop else 0.0
                    count_given_myC[j] += 1.0
                else:
                    coop_given_myD[j] += 1.0 if did_coop else 0.0
                    count_given_myD[j] += 1.0
        if len(other_coop_vals) > 0:
            avg_other_coop = float(np.mean(other_coop_vals))
        elif rounds_completed > 0:
            avg_other_coop = float(np.mean(opp_actions[max(0, rounds_completed - H):rounds_completed, :]))
        else:
            avg_other_coop = 0.0
        if len(coop_after_myD_vals) > 0:
            avg_coop_after_myD = float(np.mean(coop_after_myD_vals))
        else:
            avg_coop_after_myD = avg_other_coop
        if len(coop_after_myC_vals) > 0:
            avg_coop_after_myC = float(np.mean(coop_after_myC_vals))
        else:
            avg_coop_after_myC = avg_other_coop
        P_if_myC = np.zeros(n_opponents, dtype=float)
        P_if_myD = np.zeros(n_opponents, dtype=float)
        delta = np.zeros(n_opponents, dtype=float)
        cond_flags = np.zeros(n_opponents, dtype=int)
        for j in range(n_opponents):
            PjC = (coop_given_myC[j] + 1.0) / (count_given_myC[j] + 2.0)
            PjD = (coop_given_myD[j] + 1.0) / (count_given_myD[j] + 2.0)
            P_if_myC[j] = float(PjC)
            P_if_myD[j] = float(PjD)
            delta[j] = P_if_myC[j] - P_if_myD[j]
            if delta[j] >= self.cond_delta_thr and P_if_myC[j] >= 0.5:
                cond_flags[j] = 1
            else:
                cond_flags[j] = 0
        cond_count = int(np.sum(cond_flags))
        punishers_present = False
        if avg_coop_after_myD <= avg_coop_after_myC - self.punishment_drop:
            punishers_present = True
        if avg_other_coop >= self.high_coop_frac:
            action = Action.D
            self.last_action = action
            return action
        cond_count_threshold = math.ceil(self.cond_count_thr_frac * max(1, n_opponents))
        if cond_count >= cond_count_threshold and (not punishers_present):
            if self.last_action == Action.C:
                action = Action.D
                self.last_action = action
                return action
            else:
                ac = max(0.0, min(1.0, avg_coop_after_myC))
                p_bait = 0.5 * min(1.0, ac + 0.2)
                if random.random() < p_bait:
                    action = Action.C
                else:
                    action = Action.D
                self.last_action = action
                return action
        exploration_thresh = 1 + math.ceil(math.sqrt(r))
        if t <= exploration_thresh:
            if random.random() < probe_prob:
                action = Action.C
            else:
                action = Action.D
            self.last_action = action
            return action
        else:
            if random.random() < probe_prob / 2.0:
                action = Action.C
            else:
                action = Action.D
            self.last_action = action
            return action

class Strategy_AGGRESSIVE_44(BaseStrategy):
    """
    Aggressive Conditional Exploiter (ACE)

    Defaults to defecting, exploits predictable cooperators, probes rarely,
    and cooperates only when estimated induced future cooperation justifies
    the immediate cost. Uses last w rounds to estimate responsiveness of
    opponents to this player's actions, with Laplace smoothing and simple
    punishment detection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.window = None
        self.min_samples = 3
        self.probe_epsilon = 0.08
        self.exploitable_individual_threshold = 0.9
        self.exploit_group_threshold = 0.6
        self.punishment_drop_threshold = 0.15

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        t = int(state.round_number)
        if history is None:
            return Action.D
        if t >= r:
            return Action.D
        w = min(5, max(0, r - 1))
        self.window = w
        H = history.round_number
        if H == 0:
            return Action.D
        m_rounds = min(w, H)
        start_rounds = max(0, H - m_rounds)
        round_indices = list(range(start_rounds, start_rounds + m_rounds))
        usable_leads = min(w, max(0, H - 1))
        lead_start = max(0, H - 1 - usable_leads)
        lead_indices = list(range(lead_start, lead_start + usable_leads))
        if len(round_indices) == 0:
            group_coop_rate = 0.0
        else:
            opp_actions_window = history.opponent_actions[round_indices, :]
            coop_counts = np.sum(opp_actions_window, axis=1).astype(float)
            group_coop_rate = float(np.mean(coop_counts / float(max(1, n - 1))))
        coop_rate_js = []
        if len(round_indices) == 0:
            coop_rate_js = [0.0] * (n - 1)
        else:
            sums_per_opponent = np.sum(history.opponent_actions[round_indices, :], axis=0).astype(float)
            coop_rate_js = [float(s / len(round_indices)) for s in sums_per_opponent]
        Lc = []
        Ld = []
        for u in lead_indices:
            my_action_u = bool(history.my_actions[u])
            others_next = history.opponent_actions[u + 1, :]
            others_frac = float(np.sum(others_next) / float(max(1, n - 1)))
            if my_action_u:
                Lc.append(others_frac)
            else:
                Ld.append(others_frac)
        p_after_C = (sum(Lc) + 1.0) / (len(Lc) + 2.0)
        p_after_D = (sum(Ld) + 1.0) / (len(Ld) + 2.0)
        delta_p = p_after_C - p_after_D
        delta_num = delta_p * float(max(1, n - 1))
        exploitable_individual = any((cr >= self.exploitable_individual_threshold for cr in coop_rate_js))
        if group_coop_rate >= self.exploit_group_threshold or exploitable_individual:
            return Action.D
        Rf = r - t
        if Rf <= 0:
            return Action.D
        delta_threshold = (float(n) - k) / (k * float(Rf))
        punishment_detected = p_after_D + self.punishment_drop_threshold < group_coop_rate
        if len(Lc) >= self.min_samples and len(Ld) >= self.min_samples:
            if punishment_detected:
                if delta_num > 2.0 * delta_threshold and Rf >= 3:
                    return Action.C
                else:
                    return Action.D
            elif delta_num > max(delta_threshold, 0.0) and delta_num > 0.0:
                return Action.C
            else:
                return Action.D
        elif random.random() < self.probe_epsilon:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_45(BaseStrategy):
    """
    Aggressive Conditional Exploiter (ACE)

    - Short probe on first move (if game long enough).
    - Exploit clear cooperative majorities by defecting repeatedly when safe.
    - Occasionally cooperate with small probability to probe/seed cooperation.
    - Detect coordinated retaliation (sharp drop in others' cooperation after one of our defections)
      and switch to permanent defection (grim).
    - Always defect in the final round and in very short games (r <= 3).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.grim = False
        r = self.game_description.n_rounds
        self.P = min(3, max(0, r - 1))
        self.W = min(5, max(0, r - 1))
        self.rho_high = 0.6
        self.rho_low = 0.2
        self.M = max(1, math.floor(r / 10))
        self.p_soft_coop = 0.2
        self.delta = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if r <= 3:
            return Action.D
        if history is None:
            T = 0
            t = 1
        else:
            T = history.round_number
            t = T + 1
        if history is not None and T >= 2:
            if history.my_actions[-2] is False:
                defection_index = T - 2
                reaction_index = T - 1
                Wcur = min(self.W, defection_index)
                if Wcur >= 1:
                    start = defection_index - Wcur
                    end = defection_index
                    baseline_window = history.opponent_actions[start:end, :]
                    baseline = float(np.sum(baseline_window)) / float(Wcur)
                    current_other_coop = int(np.sum(history.opponent_actions[reaction_index, :]))
                    if current_other_coop <= baseline * (1.0 - self.delta):
                        self.grim = True
        if self.grim:
            return Action.D
        if t == r:
            return Action.D
        if history is None:
            return Action.C
        P = self.P
        L = min(P, T) if P > 0 else 0
        if L <= 0:
            if random.random() < self.p_soft_coop:
                return Action.C
            return Action.D
        recent_opponents = history.opponent_actions[-L:, :]
        coop_count_others = float(np.sum(recent_opponents))
        coop_rate = coop_count_others / float(L * (n - 1))
        if coop_rate >= self.rho_high and t <= r - self.M:
            return Action.D
        if coop_rate <= self.rho_low:
            return Action.D
        if random.random() < 1.0 - self.p_soft_coop:
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_46(BaseStrategy):
    """
    Aggressive Exploit-and-Punish (AEP)

    - Baseline: defect.
    - Identify "reliable cooperators" using a short sliding window and threshold tau.
    - Exploit reliable cooperators with high probability (exploit_prob), occasionally signal
      cooperation with small probability (signal_prob) to sustain exploitable streams.
    - If a previously reliable cooperator clearly betrays (defects while other reliables cooperate),
      flip a permanent_punish flag and defect for the remainder of the game.
    - If no reliable cooperators exist but the group's recent cooperation rate is high,
      probe occasionally (p_probe) to try to seed cooperators to exploit later.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.permanent_punish = False
        self.tau = 0.8
        self.exploit_prob = 0.95
        self.signal_prob = 0.05
        self.minimal_group_coop = 0.7
        self.p_probe = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        n_players = int(self.game_description.n_players)
        r_initial = int(self.game_description.n_rounds)
        if t == 1 or history is None:
            return Action.D
        if t == r_initial:
            return Action.D
        if self.permanent_punish:
            return Action.D
        r_remaining = r_initial - t + 1
        W = max(1, min(5, r_remaining))
        rounds_available = max(0, history.round_number)
        denom = min(W, rounds_available)
        if denom <= 0:
            return Action.D
        recent_opponent_actions = history.opponent_actions[-denom:, :]
        recent_my_actions = history.my_actions[-denom:]
        n_opponents = recent_opponent_actions.shape[1] if recent_opponent_actions.ndim == 2 else 0
        if n_opponents > 0:
            recent_coop_counts = np.sum(recent_opponent_actions.astype(int), axis=0)
        else:
            recent_coop_counts = np.array([], dtype=int)
        reliable_flags = recent_coop_counts.astype(float) / float(denom) >= self.tau
        R_indices = [idx for idx, val in enumerate(reliable_flags) if val]
        r_count = len(R_indices)
        last_round_opponent_actions = history.opponent_actions[-1, :]
        if r_count >= 1:
            for ridx in R_indices:
                betrayed = not bool(last_round_opponent_actions[ridx])
                if betrayed:
                    if r_count - 1 > 0:
                        other_indices = [i for i in R_indices if i != ridx]
                        other_coops = sum((bool(last_round_opponent_actions[i]) for i in other_indices))
                        cond = other_coops >= max(0, r_count - 1)
                    else:
                        cond = True
                    if cond:
                        self.permanent_punish = True
                        return Action.D
        if r_count >= 1:
            if random.random() < self.exploit_prob:
                return Action.D
            else:
                return Action.C
        sum_opponent_coops = int(np.sum(recent_coop_counts)) if recent_coop_counts.size > 0 else 0
        sum_my_coops = int(np.sum(recent_my_actions.astype(int)))
        total_coops = sum_opponent_coops + sum_my_coops
        total_possible = n_players * denom
        total_recent_group_coop_rate = float(total_coops) / float(total_possible)
        if total_recent_group_coop_rate >= self.minimal_group_coop:
            if random.random() < self.p_probe:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_47(BaseStrategy):
    """
    Aggressive stateful trigger strategy for N-player Public Goods Game.
    Defects by default, only cooperates after unanimous clean signals (handshake S=2),
    punishes deviations proportional to deviation size, requires 2 clean rounds to
    re-enter cooperation, opportunistically exploits near-unanimous cooperation,
    always defects in the final round and when too close to the end.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.S = 2
        self.R_min = 3
        self.forgiveness_required = 2
        n_minus_1 = max(0, self.game_description.n_players - 1)
        self.opportunistic_exploit_threshold = math.ceil(0.8 * n_minus_1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        n = self.game_description.n_players
        r = self.game_description.n_rounds

        def P(deviation_count: int, rem_rounds: int) -> int:
            return min(rem_rounds, 1 + 2 * deviation_count)
        BASELINE = 'BASELINE'
        COOP_STREAK = 'COOP_STREAK'
        PUNISH = 'PUNISH'
        WAIT_FOR_REENTRY = 'WAIT_FOR_REENTRY'
        state_str = BASELINE
        coop_streak_remaining = 0
        punish_remaining = 0
        clean_rounds_count = 0
        completed_rounds = history.round_number
        for t in range(1, completed_rounds + 1):
            rem = r - t + 1
            if t == 1:
                m_prev = 0
            else:
                m_prev = int(np.sum(history.opponent_actions[t - 2, :]))
            deviation_count = n - 1 - m_prev
            if t == r or rem < self.R_min:
                continue
            if state_str == PUNISH:
                punish_remaining -= 1
                if punish_remaining <= 0:
                    state_str = WAIT_FOR_REENTRY
                    clean_rounds_count = 0
                continue
            if state_str == COOP_STREAK:
                if m_prev < n - 1:
                    punish_remaining = P(deviation_count, rem)
                    state_str = PUNISH
                    punish_remaining -= 1
                    if punish_remaining <= 0:
                        state_str = WAIT_FOR_REENTRY
                        clean_rounds_count = 0
                    continue
                coop_streak_remaining -= 1
                if coop_streak_remaining <= 0:
                    state_str = BASELINE
                continue
            if state_str == WAIT_FOR_REENTRY:
                if m_prev == n - 1:
                    clean_rounds_count += 1
                    if clean_rounds_count >= self.forgiveness_required and rem >= self.R_min:
                        state_str = COOP_STREAK
                        coop_streak_remaining = self.S
                    else:
                        pass
                else:
                    clean_rounds_count = 0
                continue
            if state_str == BASELINE:
                if m_prev == n - 1 and rem >= self.R_min:
                    state_str = COOP_STREAK
                    coop_streak_remaining = self.S
                    continue
                if m_prev >= self.opportunistic_exploit_threshold and m_prev < n - 1:
                    punish_remaining = P(deviation_count, rem)
                    state_str = PUNISH
                    punish_remaining -= 1
                    if punish_remaining <= 0:
                        state_str = WAIT_FOR_REENTRY
                        clean_rounds_count = 0
                    continue
                continue
        t_current = completed_rounds + 1
        rem_current = r - t_current + 1
        if t_current == r or rem_current < self.R_min:
            return Action.D
        if t_current == 1:
            return Action.D
        m_prev = int(np.sum(history.opponent_actions[t_current - 2, :])) if t_current > 1 else 0
        deviation_count = n - 1 - m_prev
        if state_str == PUNISH:
            return Action.D
        if state_str == COOP_STREAK:
            if m_prev < n - 1:
                return Action.D
            return Action.C
        if state_str == WAIT_FOR_REENTRY:
            if m_prev == n - 1:
                if clean_rounds_count + 1 >= self.forgiveness_required and rem_current >= self.R_min:
                    return Action.C
                else:
                    return Action.D
            else:
                return Action.D
        if m_prev == n - 1 and rem_current >= self.R_min:
            return Action.C
        if m_prev >= self.opportunistic_exploit_threshold and m_prev < n - 1:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_48(BaseStrategy):
    """
    Aggressive, profit-driven Public Goods strategy.

    - Default: defect.
    - Rare early probing to detect unconditional cooperators.
    - Cooperate only when an estimated future benefit (from others increasing cooperation
      after I cooperate) sufficiently outweighs the immediate one-shot loss.
    - If the group punishes me after I defect, retaliate with extended defection.
    - Exploit reliably cooperative opponents when safe.
    - Always defect in the last and penultimate rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.L = max(5, min(20, self.r))
        self.epsilon_probe = 0.03
        self.alpha_persist = 0.6
        self.punish_threshold = 0.25
        self.max_punish_len = math.ceil(0.25 * self.r)
        self.benefit_margin = 1.05
        self.opponent_count = max(0, self.n - 1)
        self.punish_score = np.zeros(self.opponent_count, dtype=float)
        self.current_coop_streak = 0
        self.punish_until_round = 0
        self._recently_punished = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if t >= self.r - 0:
            if t == self.r:
                return Action.D
        if t == self.r - 1:
            return Action.D
        if history is None or history.round_number == 0:
            if t <= math.ceil(0.5 * self.r) and random.random() < self.epsilon_probe:
                return Action.C
            return Action.D
        rounds_played = history.round_number
        if t != rounds_played + 1:
            t = rounds_played + 1
        if t <= self.punish_until_round:
            return Action.D
        if t <= math.ceil(0.5 * self.r):
            if random.random() < self.epsilon_probe:
                return Action.C
        w = min(self.L, rounds_played)
        start = max(0, rounds_played - w)
        if w > 0:
            recent_op_actions = history.opponent_actions[start:rounds_played, :]
        else:
            recent_op_actions = np.zeros((0, self.opponent_count), dtype=bool)
        CR = np.zeros(self.opponent_count, dtype=float)
        if w > 0 and self.opponent_count > 0:
            coop_counts = np.sum(recent_op_actions, axis=0)
            CR = (coop_counts + 1.0) / (w + 2.0)
        else:
            CR = np.ones(self.opponent_count, dtype=float) * 0.5
        if w > 0:
            my_recent = history.my_actions[start:rounds_played]
            coop_mask = my_recent.astype(bool)
            if np.any(coop_mask):
                coop_rows = recent_op_actions[coop_mask, :]
                defections_when_myC = 1.0 - np.mean(coop_rows.astype(float), axis=0)
                self.punish_score = 0.6 * self.punish_score + 0.4 * defections_when_myC
            else:
                self.punish_score = 0.9 * self.punish_score
        else:
            self.punish_score = 0.9 * self.punish_score
        if w > 0 and self.opponent_count > 0:
            per_round_fracs = np.sum(recent_op_actions, axis=1) / float(self.opponent_count)
            baseline_group_rate = float(np.mean(per_round_fracs))
        else:
            baseline_group_rate = 0.0
        P_next_after_myC_vals = []
        P_next_after_myD_vals = []
        my_actions_all = history.my_actions
        opp_actions_all = history.opponent_actions
        for s in range(0, rounds_played - 1):
            frac_next = float(np.sum(opp_actions_all[s + 1, :]) / max(1, self.opponent_count))
            if bool(my_actions_all[s]):
                P_next_after_myC_vals.append(frac_next)
            else:
                P_next_after_myD_vals.append(frac_next)
        if len(P_next_after_myC_vals) > 0:
            P_group_next_given_my_C = float(np.mean(P_next_after_myC_vals))
        else:
            P_group_next_given_my_C = None
        if len(P_next_after_myD_vals) > 0:
            P_group_next_given_my_D = float(np.mean(P_next_after_myD_vals))
        else:
            P_group_next_given_my_D = None
        if P_group_next_given_my_C is None or P_group_next_given_my_D is None:
            delta_p_raw = 0.0
        else:
            delta_p_raw = P_group_next_given_my_C - P_group_next_given_my_D
        delta_p = self.alpha_persist * delta_p_raw
        delta0 = 1.0 - self.k / float(self.n)
        Rf = max(0, self.r - t)
        deltaF = self.k / float(self.n) * Rf * delta_p
        observed_drop = 0.0
        detected_punish = False
        if w > 0:
            s_start = max(0, rounds_played - w)
            for s in range(s_start, rounds_played - 1):
                if not bool(history.my_actions[s]):
                    frac_next = float(np.sum(history.opponent_actions[s + 1, :]) / max(1, self.opponent_count))
                    drop = baseline_group_rate - frac_next
                    if drop > self.punish_threshold and drop > observed_drop:
                        observed_drop = drop
                        detected_punish = True
        if detected_punish and observed_drop > 0.0:
            punish_len = min(self.max_punish_len, math.ceil(2.0 * observed_drop * float(self.r)))
            self.punish_until_round = max(self.punish_until_round, t + punish_len - 1)
            self._recently_punished = True
            return Action.D
        if self._recently_punished and t > self.punish_until_round:
            if delta_p_raw > 0.05 and Rf > 0:
                self._recently_punished = False
                return Action.C
            else:
                self._recently_punished = False
                return Action.D
        if self.current_coop_streak > 0:
            self.current_coop_streak -= 1
            return Action.C
        effective_benefit_margin = self.benefit_margin
        if rounds_played < max(3, int(self.L / 2)):
            effective_benefit_margin = max(self.benefit_margin, 1.2)
        if deltaF > effective_benefit_margin * delta0 and delta_p > 0.0 and (Rf > 0):
            streak_len = min(max(1, math.ceil(Rf / 2.0)), self.L)
            self.current_coop_streak = max(0, streak_len - 1)
            return Action.C
        if self.opponent_count > 0:
            P_j_after_myC = np.zeros(self.opponent_count, dtype=float)
            P_j_after_myD = np.zeros(self.opponent_count, dtype=float)
            for j in range(self.opponent_count):
                vals_afterC = []
                vals_afterD = []
                for s in range(0, rounds_played - 1):
                    if bool(history.my_actions[s]):
                        vals_afterC.append(float(history.opponent_actions[s + 1, j]))
                    else:
                        vals_afterD.append(float(history.opponent_actions[s + 1, j]))
                if len(vals_afterC) > 0:
                    P_j_after_myC[j] = float(np.mean(vals_afterC))
                else:
                    P_j_after_myC[j] = CR[j]
                if len(vals_afterD) > 0:
                    P_j_after_myD[j] = float(np.mean(vals_afterD))
                else:
                    P_j_after_myD[j] = CR[j]
            exploitable_mask = (CR >= 0.9) & (np.abs(P_j_after_myD - P_j_after_myC) <= 0.05)
            exploitable_count = int(np.sum(exploitable_mask))
            threshold_count = max(1, math.ceil(0.3 * self.opponent_count))
            if exploitable_count >= threshold_count:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_49(BaseStrategy):
    """
    Aggressive Exploiter with Responsive Probing (AERP)

    - Default to defect; perform minimal early probes to measure responsiveness.
    - Estimate delta = change in opponents' cooperation rate after my cooperation vs defection.
    - Cooperate only if conservative expected future gain > immediate cost (1) + margin.
    - Always defect in final round. Periodic deterministic probes when data is insufficient,
      unless opponents appear to punish defections or are highly cooperative (in which case exploit).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.margin = 0.05
        self.coop_exploit_threshold = 0.75
        self.punish_threshold = 0.25

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        if history is None:
            return Action.D
        completed_rounds = history.round_number
        t = completed_rounds + 1
        if t > r:
            return Action.D
        if t == r:
            return Action.D
        T_probe = min(3, max(0, r - 1))
        P = max(6, math.ceil(r / 10))
        if t <= T_probe:
            if t == 1:
                return Action.D
            if t == 2 and r > 1:
                return Action.C
            return Action.D
        if completed_rounds == 0:
            others_coop_rates = np.array([], dtype=float)
        else:
            coop_counts = np.sum(history.opponent_actions.astype(float), axis=1)
            denom = max(1, n - 1)
            others_coop_rates = coop_counts / float(denom)
        recent_L = min(5, completed_rounds)
        if recent_L > 0:
            recent_avg = float(np.mean(others_coop_rates[-recent_L:]))
        else:
            recent_avg = 0.0
        if recent_avg >= self.coop_exploit_threshold:
            return Action.D
        A_defined = False
        B_defined = False
        A = None
        B = None
        if completed_rounds >= 2:
            my_actions = history.my_actions.astype(bool)
            s_indices = np.arange(0, completed_rounds - 1)
            if s_indices.size > 0:
                next_rates = others_coop_rates[s_indices + 1]
                acted_C_mask = my_actions[s_indices]
                acted_D_mask = ~acted_C_mask
                if np.any(acted_C_mask):
                    A = float(np.mean(next_rates[acted_C_mask]))
                    A_defined = True
                if np.any(acted_D_mask):
                    B = float(np.mean(next_rates[acted_D_mask]))
                    B_defined = True
        if completed_rounds > 0:
            baseline_overall = float(np.mean(others_coop_rates))
        else:
            baseline_overall = 0.0
        punish_detected = False
        if B_defined:
            punish_detected = B < baseline_overall - self.punish_threshold
        if A_defined and B_defined:
            delta = A - B
            if delta <= 0:
                if not punish_detected and (t - 2) % P == 0:
                    return Action.C
                return Action.D
            H = r - t
            expected_gain = k / float(n) * delta * H
            if expected_gain > 1.0 + self.margin and (not punish_detected):
                return Action.C
            else:
                if not punish_detected and (t - 2) % P == 0:
                    return Action.C
                return Action.D
        else:
            if not punish_detected and (t - 2) % P == 0:
                return Action.C
            return Action.D

class Strategy_AGGRESSIVE_50(BaseStrategy):
    """
    Aggressive Exploiter with Conditional Probing (AECP).

    - Default: defect.
    - Probe periodically by cooperating to detect exploitable opponents.
    - If a probe elicits sustained cooperation from enough opponents, exploit
      them for a short block of defections.
    - If many opponents punish our defections (sharp drop in cooperation),
      switch permanently to defection for the remainder of the game.
    - Always defect in the final EndDefect rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.P = max(1, math.floor(r / 10))
        self.H = max(1, math.floor(r / 10))
        self.B = max(1, math.floor(r / 5))
        self.EndDefect = min(3, max(1, r - 1))
        self.SuccessThreshold = math.ceil(0.6 * (n - 1))
        self.PunishDrop = 0.4
        self.PunishMajority = math.ceil(0.4 * (n - 1))
        self.next_probe = 1 if self.P <= 1 else self.P
        self.PunitiveEnvironment = False
        self.probed_rounds = set()
        self.evaluated_probes = set()
        self.processed_defections = set()
        self.exploit_start = None
        self.exploit_remaining = 0
        self.last_processed_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            try:
                t = int(state.round_number)
            except Exception:
                t = history.round_number + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if r <= 3:
            return Action.D
        if history is not None:
            completed_rounds = history.round_number
            for proc_round in range(self.last_processed_round + 1, completed_rounds + 1):
                if proc_round in self.probed_rounds and proc_round not in self.evaluated_probes:
                    reaction_round = proc_round + 1
                    if completed_rounds >= reaction_round:
                        coop_after_probe = int(np.sum(history.opponent_actions[reaction_round - 1, :]))
                        if coop_after_probe >= self.SuccessThreshold:
                            start = proc_round + 2
                            if start <= r - self.EndDefect:
                                self.exploit_start = start
                                self.exploit_remaining = self.H
                        else:
                            blackout_next = proc_round + self.B + 1
                            if blackout_next > self.next_probe:
                                self.next_probe = blackout_next
                        self.evaluated_probes.add(proc_round)
                if proc_round not in self.processed_defections:
                    my_action = bool(history.my_actions[proc_round - 1])
                    if not my_action:
                        reaction_round = proc_round + 1
                        if completed_rounds >= reaction_round:
                            baseline_start = max(1, proc_round - 5)
                            baseline_end = proc_round - 1
                            baseline_len = baseline_end - baseline_start + 1
                            if baseline_len >= 1:
                                opp_actions = history.opponent_actions
                                baseline_slice = opp_actions[baseline_start - 1:baseline_end]
                                baseline_rates = np.mean(baseline_slice, axis=0)
                                p_after = opp_actions[reaction_round - 1, :]
                                drops = baseline_rates - p_after
                                punishers = int(np.sum(drops >= self.PunishDrop))
                                if punishers >= self.PunishMajority:
                                    self.PunitiveEnvironment = True
                            self.processed_defections.add(proc_round)
                    else:
                        self.processed_defections.add(proc_round)
            self.last_processed_round = completed_rounds
        if t > r - self.EndDefect:
            return Action.D
        if self.PunitiveEnvironment:
            return Action.D
        if self.exploit_remaining > 0 and self.exploit_start is not None and (t >= self.exploit_start):
            self.exploit_remaining -= 1
            if self.exploit_remaining == 0:
                self.exploit_start = None
            return Action.D
        if t == self.next_probe:
            self.next_probe = t + self.P
            self.probed_rounds.add(t)
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_51(BaseStrategy):
    """
    Aggressive Conditional Exploiter (ACE)

    Opportunistic conditional cooperator: probe early to find reciprocators,
    exploit guaranteed cooperators occasionally, and punish defectors rapidly
    and for a sustained period. Always defects in the final R_safe rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.L = min(5, self.r)
        self.S_probe = min(2, max(1, self.r - 1))
        self.theta_start = 0.7
        self.theta_continue = 0.5
        self.tau_punish = max(3, math.floor(self.r / 6))
        self.s_redeem = 3
        self.R_safe = 1
        self.t_gap = max(3, math.floor(self.r / 6))
        self.mode = 'Probe'

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        remaining_rounds = max(0, self.r - t + 1)
        if history is None:
            self.mode = 'Probe'
            return Action.C if self.S_probe >= 1 else Action.D
        completed = history.round_number
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        num_opponents = self.n - 1
        if t > self.r - self.R_safe:
            self.mode = 'DefectDefault'
            return Action.D

        def frac_coop_over(rows_slice):
            if num_opponents == 0:
                return 0.0
            arr = opp_actions[rows_slice, :]
            if arr.size == 0:
                return 0.0
            return float(np.mean(arr))
        P_probe = 0.0
        if completed >= 1:
            probe_rows = min(self.S_probe, completed)
            if probe_rows > 0:
                P_probe = frac_coop_over(slice(0, probe_rows))
        if self.mode == 'Probe' and t <= self.S_probe:
            if completed >= 1:
                probe_rows_so_far = min(self.S_probe, completed)
                P_probe_so_far = frac_coop_over(slice(0, probe_rows_so_far))
                if P_probe_so_far < 0.5:
                    self.mode = 'DefectDefault'
                    return Action.D
            return Action.C
        if self.mode == 'Probe' and t > self.S_probe:
            if P_probe >= self.theta_start and remaining_rounds >= 2:
                self.mode = 'CoopConditional'
            else:
                self.mode = 'DefectDefault'
        if self.mode == 'DefectDefault':
            if completed >= self.t_gap:
                recent_frac = frac_coop_over(slice(max(0, completed - self.t_gap), completed))
                if recent_frac >= self.theta_start and remaining_rounds >= max(1, self.S_probe):
                    self.mode = 'Probe'
                    return Action.C
            return Action.D
        last_trigger = [None] * num_opponents
        for j in range(num_opponents):
            if completed == 0:
                continue
            my_coop_indices = np.nonzero(my_actions)[0]
            triggered = []
            for idx in my_coop_indices:
                if idx < completed and (not bool(opp_actions[idx, j])):
                    triggered.append(idx + 1)
            if len(triggered) > 0:
                last_trigger[j] = max(triggered)
        punished_active = [False] * num_opponents
        for j in range(num_opponents):
            tr = last_trigger[j]
            if tr is None:
                continue
            punish_start = tr + 1
            punish_end = tr + self.tau_punish
            if not punish_start <= t <= punish_end:
                continue
            redeemed = False
            start_idx = tr
            end_idx = t - 2
            if end_idx >= start_idx:
                consec = 0
                for idx in range(start_idx, end_idx + 1):
                    if bool(opp_actions[idx, j]):
                        consec += 1
                        if consec >= self.s_redeem:
                            redeemed = True
                            break
                    else:
                        consec = 0
            if completed == 0:
                P_L = P_probe
            else:
                w = min(self.L, completed)
                P_L = frac_coop_over(slice(max(0, completed - w), completed))
            if redeemed and P_L >= self.theta_continue:
                punished_active[j] = False
            else:
                punished_active[j] = True
        any_punished = any(punished_active)
        if any_punished:
            return Action.D
        if completed == 0:
            P_L = P_probe
        else:
            w = min(self.L, completed)
            if w <= 0:
                P_L = P_probe
            else:
                P_L = frac_coop_over(slice(max(0, completed - w), completed))
        if P_L >= self.theta_continue and remaining_rounds >= 2:
            if completed >= 1 and np.all(opp_actions[-1, :]):
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_52(BaseStrategy):
    """
    Aggressive Conditional Punisher (ACP)

    - Probes early by defecting to detect naive cooperators and exploit them.
    - If many others cooperate while I defect, enter exploit_mode and defect permanently.
    - After probing, conditionally cooperate when recent cooperation by others is high.
    - If I am betrayed while cooperating (or if cooperation drops sharply), trigger a short,
      scaled punishment (defect for P rounds). Forgive only after sustained high cooperation.
    - Always defect in the final few rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.alpha = self.k / float(self.n)
        self.L = min(3, max(1, math.floor(self.r / 5)))
        self.W = min(5, max(1, self.r - 1))
        qc = 0.6 - 0.3 * self.alpha
        self.q_coop = min(0.8, max(0.4, qc))
        self.exploit_threshold = 0.8
        self.punishment_base = 2
        self.R_thresh = 3
        self.q_heal = min(0.95, self.q_coop + 0.1)
        self.punishment_mode = False
        self.punishment_end_round = 0
        self.exploit_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:

        def clamp(x, lo, hi):
            return lo if x < lo else hi if x > hi else x
        if history is None or history.round_number == 0:
            return Action.D
        completed = int(history.round_number)
        next_round = completed + 1
        remaining = max(0, self.r - next_round + 1)
        if remaining <= self.R_thresh or next_round == self.r:
            return Action.D
        opp = history.opponent_actions
        per_round_opponent_frac = None
        if opp.size == 0:
            per_round_opponent_frac = np.array([], dtype=float)
        else:
            per_round_opponent_frac = np.mean(opp.astype(float), axis=1)
        if not self.exploit_mode:
            myacts = history.my_actions.astype(bool)
            defect_rounds = np.where(myacts == False)[0]
            for idx in defect_rounds:
                if idx < len(per_round_opponent_frac):
                    if per_round_opponent_frac[idx] >= self.exploit_threshold:
                        self.exploit_mode = True
                        break
        if self.punishment_mode and next_round > self.punishment_end_round:
            healed = False
            total_completed = completed
            W = self.W
            if total_completed >= 2 * W:
                last_slice = per_round_opponent_frac[-W:]
                prev_slice = per_round_opponent_frac[-2 * W:-W]
                if np.mean(last_slice) >= self.q_heal and np.mean(prev_slice) >= self.q_heal:
                    healed = True
            elif total_completed >= W:
                last_slice = per_round_opponent_frac[-W:]
                if np.mean(last_slice) >= self.q_heal:
                    healed = True
            elif total_completed > 0 and np.mean(per_round_opponent_frac) >= self.q_heal:
                healed = True
            if healed:
                self.punishment_mode = False
                self.punishment_end_round = 0
            else:
                self.punishment_mode = True
        if completed >= 1:
            last_idx = completed - 1
            my_last = bool(history.my_actions[last_idx])
            if my_last:
                if last_idx < opp.shape[0]:
                    opp_last = opp[last_idx, :].astype(bool)
                    num_defectors_last = int((~opp_last).sum())
                    if num_defectors_last > 0:
                        rem_at_trigger = max(0, self.r - next_round + 1)
                        P = min(max(0, rem_at_trigger - 1), self.punishment_base + num_defectors_last)
                        self.punishment_mode = True
                        self.punishment_end_round = min(self.r, next_round + P)
            if len(per_round_opponent_frac) >= 2:
                W = self.W
                recent_window = per_round_opponent_frac[-W:] if len(per_round_opponent_frac) >= W else per_round_opponent_frac
                recent_avg = float(np.mean(recent_window)) if recent_window.size > 0 else 0.0
                prev_window = None
                if len(per_round_opponent_frac) >= 2 * W:
                    prev_window = per_round_opponent_frac[-2 * W:-W]
                elif len(per_round_opponent_frac) > W:
                    prev_window = per_round_opponent_frac[:-W]
                if prev_window is not None and prev_window.size > 0:
                    prev_avg = float(np.mean(prev_window))
                    if prev_avg - recent_avg > 0.25:
                        last_idx = len(per_round_opponent_frac) - 1
                        num_defectors_last = int((~opp[last_idx, :].astype(bool)).sum())
                        rem_at_trigger = max(0, self.r - next_round + 1)
                        P = min(max(0, rem_at_trigger - 1), self.punishment_base + num_defectors_last)
                        self.punishment_mode = True
                        self.punishment_end_round = min(self.r, next_round + P)
        if self.exploit_mode:
            return Action.D
        if self.punishment_mode and next_round <= self.punishment_end_round:
            return Action.D
        if len(per_round_opponent_frac) == 0:
            f_recent = 0.0
        else:
            use_last = per_round_opponent_frac[-self.W:] if len(per_round_opponent_frac) >= self.W else per_round_opponent_frac
            f_recent = float(np.mean(use_last))
        if next_round <= self.L:
            return Action.D
        if f_recent >= self.q_coop and remaining > self.R_thresh:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_53(BaseStrategy):
    """
    Aggressive Gatekeeper (AG) strategy for the N-player repeated public goods game.

    Summary:
    - Probe for a small number of initial rounds by cooperating to identify cooperators.
    - Maintain a sliding-window cooperation estimate for each opponent.
    - Form a trusted/cooperator set G and only attempt coordinated cooperation when G is large enough.
    - Opportunistically exploit a cooperating coalition with low probability p_exploit.
    - If a trusted player betrays the coalition, escalate multi-round punishments that grow
      with repeat betrayals. Also apply a short community punishment on sudden cooperation collapses.
    - Default to defection when no credible coalition exists. Always defect in the final rounds.
    - Includes limited randomized forgiveness to escape persistent mutual-defection spirals.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.probe_rounds = min(2, max(1, math.floor(self.r / 10)))
        self.L = min(10, max(3, math.floor(self.r / 4)))
        self.p_good = 0.65
        self.majority_frac = 0.5
        self.majority_threshold = math.ceil((self.n - 1) * self.majority_frac)
        self.p_exploit = 0.25
        self.base_punish = 3
        self.punish_scale = 2
        self.forgive_window = self.L
        self.endgame_horizon = min(3, math.floor(self.r / 10))
        self.forgiveness_prob = 0.05
        self.betray_count = np.zeros(self.n - 1, dtype=int)
        self.punishment_counter = 0
        self.last_opponent_actions = None
        self.last_my_action = None
        self._last_processed_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.last_opponent_actions = None
            self.last_my_action = True
            return Action.C
        completed = history.round_number
        t = completed + 1
        opponents = history.opponent_actions
        my_actions = history.my_actions
        if completed > self._last_processed_round:
            last_idx = completed - 1
            rounds_available_before = max(0, completed - 1)
            window_before = min(self.L, rounds_available_before)
            G_old = set()
            if window_before > 0:
                start = rounds_available_before - window_before
                slice_before = opponents[start:start + window_before, :] if window_before > 0 else opponents[0:0, :]
                coop_counts_before = np.sum(slice_before, axis=0)
                coop_rates_before = coop_counts_before / float(window_before)
                G_old = set((int(j) for j, rate in enumerate(coop_rates_before) if rate >= self.p_good))
            if len(G_old) >= self.majority_threshold and completed >= 1:
                last_round_opponents = opponents[last_idx, :]
                if len(G_old) > 0:
                    coop_in_G = sum((1 for j in G_old if bool(last_round_opponents[j])))
                    most_threshold = max(1, math.ceil(0.6 * len(G_old)))
                    if coop_in_G >= most_threshold:
                        for j in G_old:
                            if not bool(last_round_opponents[j]):
                                self.betray_count[j] += 1
                                new_punish = int(self.base_punish * self.punish_scale ** (self.betray_count[j] - 1))
                                if new_punish > self.punishment_counter:
                                    self.punishment_counter = new_punish
            if rounds_available_before > 0:
                wb = min(self.L, rounds_available_before)
                start_prev = rounds_available_before - wb
                prev_opponents = opponents[start_prev:start_prev + wb, :] if wb > 0 else opponents[0:0, :]
                prev_my = my_actions[start_prev:start_prev + wb] if wb > 0 else my_actions[0:0]
                if wb > 0:
                    prev_total_coop = 0
                    for rr in range(wb):
                        prev_total_coop += int(prev_my[rr])
                        prev_total_coop += int(np.sum(prev_opponents[rr, :]))
                    prev_global_rate = prev_total_coop / float(wb * self.n)
                    last_my = bool(my_actions[last_idx]) if completed >= 1 else False
                    last_total_coop = int(last_my) + int(np.sum(opponents[last_idx, :]))
                    last_global_rate = last_total_coop / float(self.n)
                    if prev_global_rate >= 0.7 and last_global_rate < 0.5:
                        if self.base_punish > self.punishment_counter:
                            self.punishment_counter = self.base_punish
            self._last_processed_round = completed
        if t <= self.probe_rounds:
            self.last_my_action = True
            return Action.C
        if t == self.r or t > self.r - self.endgame_horizon:
            self.last_my_action = False
            return Action.D
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            self.last_my_action = False
            return Action.D
        window_size = min(self.L, max(0, completed))
        coop_rates = np.zeros(self.n - 1, dtype=float)
        if window_size > 0:
            start = completed - window_size
            recent = opponents[start:start + window_size, :]
            coop_counts = np.sum(recent, axis=0)
            coop_rates = coop_counts / float(window_size)
        else:
            coop_rates = np.zeros(self.n - 1, dtype=float)
        G = set((int(j) for j, rate in enumerate(coop_rates) if rate >= self.p_good))
        credible = len(G) >= self.majority_threshold
        last_universal_coop = False
        if completed >= 1:
            last_opponents = opponents[-1, :]
            last_my = bool(my_actions[-1])
            if last_my and bool(np.all(last_opponents)):
                last_universal_coop = True
        action_cooperate = False
        if credible:
            if last_universal_coop:
                if random.random() < self.p_exploit:
                    action_cooperate = False
                else:
                    action_cooperate = True
            else:
                action_cooperate = True
        else:
            action_cooperate = False
        if not credible:
            if window_size > 0:
                start = completed - window_size
                recent_opp = opponents[start:start + window_size, :]
                recent_my = my_actions[start:start + window_size]
                total_coop = int(np.sum(recent_opp)) + int(np.sum(recent_my))
                recent_global_rate = total_coop / float(window_size * self.n)
            else:
                recent_global_rate = 0.0
            if recent_global_rate < 0.2 and random.random() < self.forgiveness_prob:
                action_cooperate = True
        self.last_my_action = bool(action_cooperate)
        if completed >= 1:
            self.last_opponent_actions = opponents[-1, :].copy()
        else:
            self.last_opponent_actions = None
        return Action.C if action_cooperate else Action.D

class Strategy_AGGRESSIVE_54(BaseStrategy):
    """
    Aggressive Exploit-and-Punish (AEP)

    Defect by default (SEEK). When a majority of other players show
    persistent cooperation, play a single cooperative BAIT to elicit
    continued cooperation. If others respond (majority cooperates in the
    round after the bait) move to EXPLOIT and defect for a controlled
    number of rounds to harvest free-rides. If others punish (cooperation
    falls below threshold) or if cooperation collapses during EXPLOIT,
    switch to permanent PUNISH (grim). Always defect in the final F rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(getattr(game_description, 'n_players', 2))
        self.r = int(getattr(game_description, 'n_rounds', 1))
        self.k = float(getattr(game_description, 'k', 1.0))
        self.w = min(3, max(1, math.floor(self.r / 10))) if self.r > 0 else 1
        self.H = math.ceil((self.n - 1) / 2)
        self.C_seed = 2
        self.cooldown = max(1, math.floor(self.r / 10)) if self.r > 0 else 1
        self.F = 1
        self.mode = 'SEEK'
        self.last_bait_round = -10000
        self.exploit_remaining = 0
        self.collapse_counter = 0
        self.epsilon = 0.02

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.n = int(getattr(self.game_description, 'n_players', self.n))
            self.r = int(getattr(self.game_description, 'n_rounds', self.r))
            self.k = float(getattr(self.game_description, 'k', self.k))
            self.w = min(3, max(1, math.floor(self.r / 10))) if self.r > 0 else 1
            self.H = math.ceil((self.n - 1) / 2)
            self.cooldown = max(1, math.floor(self.r / 10)) if self.r > 0 else 1
            self.mode = 'SEEK'
            self.last_bait_round = -10000
            self.exploit_remaining = 0
            self.collapse_counter = 0
            return Action.D
        completed = int(history.round_number)
        t = completed + 1
        remaining = max(0, self.r - t + 1)
        if self.r <= 2:
            self.mode = 'PUNISH'
            return Action.D
        if remaining <= self.F:
            self.mode = 'PUNISH'
            return Action.D

        def others_coop_in_round(idx: int) -> int:
            if idx < 0 or idx >= history.opponent_actions.shape[0]:
                return 0
            return int(np.sum(history.opponent_actions[idx, :]))
        consecutive_high = 0
        last_round_index = completed - 1
        checks = 0
        idx = last_round_index
        while checks < self.w and idx >= 0:
            if others_coop_in_round(idx) >= self.H:
                consecutive_high += 1
            else:
                break
            checks += 1
            idx -= 1
        if self.mode == 'BAIT':
            if self.last_bait_round == t - 1:
                m_prev = others_coop_in_round(last_round_index)
                if m_prev >= self.H:
                    self.mode = 'EXPLOIT'
                    E_base = max(1, math.floor(self.r / 4))
                    E_allowed = max(0, remaining - 1)
                    self.exploit_remaining = min(E_base, E_allowed) if E_allowed >= 1 else 0
                    if self.exploit_remaining <= 0:
                        self.mode = 'SEEK'
                        self.exploit_remaining = 0
                        return Action.D
                    if m_prev < self.H:
                        self.collapse_counter += 1
                    else:
                        self.collapse_counter = 0
                    self.exploit_remaining = max(0, self.exploit_remaining - 1)
                    return Action.D
                else:
                    self.mode = 'PUNISH'
                    self.exploit_remaining = 0
                    self.collapse_counter = 0
                    return Action.D
            else:
                self.mode = 'SEEK'
        if self.mode == 'EXPLOIT':
            m_prev = others_coop_in_round(last_round_index) if last_round_index >= 0 else 0
            if m_prev < self.H:
                self.collapse_counter += 1
            else:
                self.collapse_counter = 0
            if self.collapse_counter >= 2:
                self.mode = 'PUNISH'
                self.exploit_remaining = 0
                return Action.D
            if self.exploit_remaining > 0:
                self.exploit_remaining = max(0, self.exploit_remaining - 1)
                if self.exploit_remaining == 0:
                    self.mode = 'SEEK'
                return Action.D
            else:
                self.mode = 'SEEK'
                return Action.D
        if self.mode == 'PUNISH':
            return Action.D
        if consecutive_high >= self.C_seed and t - self.last_bait_round >= self.cooldown and (remaining >= 2):
            self.mode = 'BAIT'
            self.last_bait_round = t
            return Action.C
        if random.random() < self.epsilon and remaining > self.F:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_55(BaseStrategy):
    """
    Grim-Triggered Selective Exploiter (GTSE)

    Aggressive public-goods strategy:
    - Default: defect.
    - Cooperates only if the immediately previous round showed a sufficiently large coalition (prev_coops >= m_min).
    - If it cooperates in a round and the observed cooperators that round < m_min (i.e., it was exploited),
      it enters permanent punish mode (punish_flag) and defects forever thereafter.
    - Always defects on the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, max(0, self.r - 1))
        raw_p = 0.6 + 0.4 * (1.0 - self.k / max(1.0, self.n))
        self.p = min(max(raw_p, 0.6), 0.99)
        self.m_min = max(1, math.ceil(self.p * self.n))
        self.punish_flag = False
        self._last_processed_round_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is not None and history.round_number > self._last_processed_round_count:
            if history.round_number > 0:
                last_idx = history.round_number - 1
                my_coop_last = bool(history.my_actions[last_idx])
                opp_coops_last = int(np.sum(history.opponent_actions[last_idx, :])) if history.opponent_actions.size else 0
                total_coops_last = (1 if my_coop_last else 0) + opp_coops_last
                if my_coop_last and total_coops_last < self.m_min:
                    self.punish_flag = True
            self._last_processed_round_count = history.round_number
        if t >= self.r:
            return Action.D
        if self.punish_flag:
            return Action.D
        if history is None or history.round_number == 0 or t == 1:
            return Action.D
        prev_idx = history.round_number - 1
        prev_my = bool(history.my_actions[prev_idx])
        prev_opp_coops = int(np.sum(history.opponent_actions[prev_idx, :])) if history.opponent_actions.size else 0
        prev_total_coops = (1 if prev_my else 0) + prev_opp_coops
        if prev_total_coops >= self.m_min:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_56(BaseStrategy):
    """
    Aggressive, history-based Public Goods strategy.
    - Default: defect.
    - Probe for cooperation when recent group cooperation is high (short trial).
    - If trial succeeds, enter COOP mode while group cooperation stays above threshold.
    - If cooperation drops, punish for P rounds (unconditional defection).
    - After punishment, offer a short probation (few cooperative rounds). If probation elicits cooperation, resume COOP; otherwise punish again.
    - Always defect in the final few rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.L = min(10, max(1, math.floor(math.sqrt(max(1, self.r)))))
        self.theta_start_coop = 0.75
        self.theta_maintain = 0.6
        self.theta_punish = 0.45
        self.P = min(max(1, math.ceil(self.r / 10)), 6)
        self.S_init = 2
        self.probation_length = 1
        self.endgame_cutoff = min(3, self.r)
        self.mode = 'DEFAULT'
        self.period_start_round = None
        self.period_end_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.mode = 'DEFAULT'
            self.period_start_round = None
            self.period_end_round = None
            return Action.D
        completed = history.round_number
        t = completed + 1
        remaining = max(0, self.r - t + 1)
        if remaining <= self.endgame_cutoff:
            self.mode = 'DEFAULT'
            self.period_start_round = None
            self.period_end_round = None
            return Action.D
        lookback_rounds = min(self.L, completed)
        if lookback_rounds <= 0:
            G = 0.0
            coop_rate_per_player = np.zeros(self.n - 1, dtype=float)
        else:
            recent_ops = history.opponent_actions[-lookback_rounds:, :]
            coop_counts_per_player = np.sum(recent_ops, axis=0).astype(float)
            coop_rate_per_player = coop_counts_per_player / float(lookback_rounds)
            total_coops = float(np.sum(coop_counts_per_player))
            G = total_coops / float((self.n - 1) * lookback_rounds)

        def group_coop_in_window(start_round_idx_inclusive: int, end_round_idx_exclusive: int) -> float:
            if end_round_idx_exclusive <= start_round_idx_inclusive:
                return 0.0
            start_idx = max(0, start_round_idx_inclusive - 1)
            end_idx = min(completed, end_round_idx_exclusive - 1) + 1
            if start_idx >= end_idx:
                return 0.0
            window = history.opponent_actions[start_idx:end_idx, :]
            if window.size == 0:
                return 0.0
            return float(np.sum(window)) / float(window.shape[0] * window.shape[1])
        if self.mode == 'PUNISH' and self.period_end_round is not None and (completed >= self.period_end_round):
            self.mode = 'PROBATION'
            self.period_start_round = self.period_end_round + 1
            self.period_start_round = completed + 1
            self.period_end_round = completed + self.probation_length
        if self.mode == 'COOP_TRIAL' and self.period_end_round is not None and (completed >= self.period_end_round):
            trial_start = self.period_start_round if self.period_start_round is not None else completed - self.S_init + 1
            trial_end = self.period_end_round
            trial_group_coop = group_coop_in_window(trial_start, trial_end + 1)
            if trial_group_coop >= self.theta_maintain:
                self.mode = 'COOP'
                self.period_start_round = None
                self.period_end_round = None
            else:
                self.mode = 'PUNISH'
                self.period_start_round = completed + 1
                self.period_end_round = completed + self.P
        if self.mode == 'PROBATION' and self.period_end_round is not None and (completed >= self.period_end_round):
            prob_start = self.period_start_round if self.period_start_round is not None else completed - self.probation_length + 1
            prob_end = self.period_end_round
            prob_group_coop = group_coop_in_window(prob_start, prob_end + 1)
            if prob_group_coop >= self.theta_maintain:
                self.mode = 'COOP'
                self.period_start_round = None
                self.period_end_round = None
            else:
                self.mode = 'PUNISH'
                self.period_start_round = completed + 1
                self.period_end_round = completed + self.P
        if self.mode == 'COOP':
            if G < self.theta_maintain:
                self.mode = 'PUNISH'
                self.period_start_round = completed + 1
                self.period_end_round = completed + self.P
                return Action.D
            else:
                return Action.C
        if self.mode == 'PUNISH':
            if self.period_end_round is None or completed < self.period_end_round:
                return Action.D
        if self.mode == 'PROBATION':
            if self.period_end_round is None or completed < self.period_end_round:
                return Action.C
        if self.mode == 'COOP_TRIAL':
            if self.period_end_round is None or completed < self.period_end_round:
                return Action.C
        min_remaining_needed = self.P + self.probation_length + 1
        if G >= self.theta_start_coop and remaining > min_remaining_needed:
            self.mode = 'COOP_TRIAL'
            self.period_start_round = t
            self.period_end_round = completed + self.S_init
            return Action.C
        if G < self.theta_punish:
            self.mode = 'PUNISH'
            self.period_start_round = completed + 1
            self.period_end_round = completed + self.P
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_57(BaseStrategy):
    """
    Aggressive Conditional Exploiter (ACE)

    Deterministic, history-based strategy for the N-player public goods game.
    - Almost always defects.
    - Occasionally sends short cooperation probes on a schedule to detect
      conditional cooperators.
    - If a probe is successful (many opponents cooperated on probe round and
      in the immediate following round), enters an exploitation run of repeated
      defects to harvest cooperating opponents' contributions.
    - If opponents retaliate (cooperation fraction drops sufficiently), abort
      exploitation and enter a punish/wait period before resuming probes.
    - Never cooperates in the final T_end rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = int(game_description.n_players)
        r = int(game_description.n_rounds)
        k = float(game_description.k)
        self.W = min(10, max(1, r - 1))
        self.S_probe = max(2, math.ceil(r / 8))
        self.Th_reciprocate = 0.6
        self.E_exploit = max(1, math.ceil(k / max(1, n - 1) * math.ceil(r / 8)))
        self.P_punish = max(2, math.ceil(r / 10))
        self.T_end = min(3, max(0, r - 1))
        self.retaliation_drop = 0.2
        self.mode = 'DEFAULT'
        self.exploit_rounds_left = 0
        self.punish_rounds_left = 0
        self.last_successful_probe_coop_rate = 0.0
        self.pending_probe_round = -1
        self.n = n
        self.r = r
        self.k = k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        completed_rounds = history.round_number
        t = completed_rounds + 1

        def opponents_coop_fraction_for_round(round_index_1based: int) -> float:
            if round_index_1based < 1 or round_index_1based > completed_rounds:
                return 0.0
            row_idx = round_index_1based - 1
            coop_count = int(np.sum(history.opponent_actions[row_idx, :]))
            return coop_count / max(1, self.n - 1)
        if self.pending_probe_round >= 1 and completed_rounds >= self.pending_probe_round + 1:
            probe_r = self.pending_probe_round
            coop_at_probe = opponents_coop_fraction_for_round(probe_r)
            coop_after_probe = opponents_coop_fraction_for_round(probe_r + 1)
            if coop_at_probe >= self.Th_reciprocate and coop_after_probe >= self.Th_reciprocate:
                self.mode = 'EXPLOIT'
                self.exploit_rounds_left = int(self.E_exploit)
                self.last_successful_probe_coop_rate = (coop_at_probe + coop_after_probe) / 2.0
            else:
                self.last_successful_probe_coop_rate = 0.0
                if self.mode == 'EXPLOIT':
                    pass
            self.pending_probe_round = -1
        if self.mode == 'EXPLOIT' and completed_rounds >= 1:
            recent_opp_coop = opponents_coop_fraction_for_round(completed_rounds)
            threshold = max(self.Th_reciprocate - self.retaliation_drop, 0.0)
            if recent_opp_coop < threshold:
                self.mode = 'PUNISH'
                self.punish_rounds_left = int(self.P_punish)
                self.exploit_rounds_left = 0
        if self.T_end >= 1 and t > self.r - self.T_end:
            action = Action.D
        elif self.mode == 'EXPLOIT':
            action = Action.D
        elif self.mode == 'PUNISH':
            action = Action.D
        elif t % self.S_probe == 0 and self.pending_probe_round == -1:
            action = Action.C
            self.pending_probe_round = t
        else:
            action = Action.D
        if self.mode == 'EXPLOIT':
            if self.exploit_rounds_left > 0:
                self.exploit_rounds_left -= 1
                if self.exploit_rounds_left <= 0:
                    self.mode = 'DEFAULT'
        elif self.mode == 'PUNISH':
            if self.punish_rounds_left > 0:
                self.punish_rounds_left -= 1
                if self.punish_rounds_left <= 0:
                    self.mode = 'DEFAULT'
        return action

class Strategy_AGGRESSIVE_58(BaseStrategy):
    """
    Aggressive Opportunistic Exploiter (AOE)

    - Default: defect.
    - Join cooperation only when many others have been cooperating stably.
    - After cooperating twice, do an occasional one-shot defection to exploit highly cooperative others.
    - If others sharply reduce cooperation after my defection (interpreted as punishment), enter a short bounded punishment phase (defect for punish_len rounds).
    - Never cooperate in the final end_horizon rounds.
    Deterministic and based only on n, k, r and observed full-history of actions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(8, max(0, self.r - 1))
        raw_t = 0.7 + 0.2 * (self.k - 1.0) / max(1, self.n - 1)
        self.T_coop = min(0.95, max(0.6, raw_t))
        self.T_exploit = max(self.T_coop, 0.9)
        self.stability_delta = 0.02
        self.punish_len = min(4, max(1, math.floor(self.r / 6)))
        self.end_horizon = 2
        self.punish_until_round = 0
        self.coop_streak = 0
        self.last_my_action = Action.D
        self._last_checked_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            self.coop_streak = 0
            self.last_my_action = Action.D
            return Action.D
        completed_rounds = int(history.round_number)
        if completed_rounds > 0:
            last_action_bool = bool(history.my_actions[-1])
            self.last_my_action = Action.C if last_action_bool else Action.D
            cs = 0
            for a in reversed(history.my_actions):
                if bool(a):
                    cs += 1
                else:
                    break
            self.coop_streak = cs
        else:
            self.coop_streak = 0
            self.last_my_action = Action.D
        max_s_to_check = max(0, completed_rounds - 1)
        if max_s_to_check >= self._last_checked_round + 1:
            for s in range(self._last_checked_round + 1, max_s_to_check + 1):
                idx_s = s - 1
                idx_s_plus = s
                if idx_s < len(history.my_actions) and (not bool(history.my_actions[idx_s])):
                    start = max(1, s - self.w)
                    p_list = []
                    for j in range(start, s + 1):
                        j_idx = j - 1
                        if j_idx < len(history.opponent_actions):
                            row = history.opponent_actions[j_idx]
                            if self.n - 1 > 0:
                                p_j = float(np.sum(row)) / float(self.n - 1)
                            else:
                                p_j = 0.0
                            p_list.append(p_j)
                    avg_before = float(np.mean(p_list)) if len(p_list) > 0 else 0.0
                    if idx_s_plus < len(history.opponent_actions):
                        row_next = history.opponent_actions[idx_s_plus]
                        p_next = float(np.sum(row_next)) / float(max(1, self.n - 1))
                        if p_next <= avg_before - 0.2:
                            new_until = min(self.r, s + self.punish_len)
                            if new_until > self.punish_until_round:
                                self.punish_until_round = new_until
            self._last_checked_round = max(self._last_checked_round, max_s_to_check)
        if t >= self.r - self.end_horizon + 1:
            self.coop_streak = 0
            self.last_my_action = Action.D
            return Action.D
        if t <= self.punish_until_round:
            self.coop_streak = 0
            self.last_my_action = Action.D
            return Action.D
        if t == 1:
            self.coop_streak = 0
            self.last_my_action = Action.D
            return Action.D
        S_start = max(1, t - self.w)
        S_end = t - 1
        p_s_list = []
        for s in range(S_start, S_end + 1):
            idx = s - 1
            if idx < len(history.opponent_actions):
                row = history.opponent_actions[idx]
                if self.n - 1 > 0:
                    p_s = float(np.sum(row)) / float(self.n - 1)
                else:
                    p_s = 0.0
                p_s_list.append(p_s)
        if len(p_s_list) == 0:
            avg_p = 0.0
            min_p = 0.0
            stability = 0.0
        else:
            avg_p = float(np.mean(p_s_list))
            min_p = float(np.min(p_s_list))
            if len(p_s_list) >= 2:
                mid = len(p_s_list) // 2
                older = p_s_list[:mid]
                newer = p_s_list[mid:]
                mean_older = float(np.mean(older)) if len(older) > 0 else 0.0
                mean_newer = float(np.mean(newer)) if len(newer) > 0 else 0.0
                stability = mean_newer - mean_older
            else:
                stability = 0.0
        p_last = 0.0
        if t - 1 - 1 >= 0 and t - 1 - 1 < len(history.opponent_actions):
            last_row = history.opponent_actions[t - 2]
            p_last = float(np.sum(last_row)) / float(max(1, self.n - 1))
        if self.coop_streak >= 2 and p_last >= self.T_exploit:
            self.coop_streak = 0
            self.last_my_action = Action.D
            return Action.D
        remaining_rounds = self.r - t + 1
        safe_coop = avg_p >= self.T_coop and min_p >= self.T_coop - 0.1 and (stability >= -self.stability_delta) and (remaining_rounds >= 3)
        if safe_coop:
            self.coop_streak = self.coop_streak + 1
            self.last_my_action = Action.C
            return Action.C
        else:
            self.coop_streak = 0
            self.last_my_action = Action.D
            return Action.D

class Strategy_AGGRESSIVE_59(BaseStrategy):
    """
    Aggressive Punishing Exploiter (APE)

    Default: defect. Periodically probe by cooperating. If many players cooperated
    in the previous round, exploit by defecting. If cooperation collapses sharply
    relative to a recent baseline and the collapse coincides with a probe or an
    exploit, respond with a short, deterministic punishment (defection) window.
    Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        self.probe_interval = max(1, math.floor(r / 6))
        self.punishment_length_base = max(1, math.floor(r / 10))
        self.punishment_length = min(self.punishment_length_base, max(0, r - 1))
        self.exploit_threshold = max(1, math.ceil(n * 0.7))
        self.drop_threshold = max(1, math.ceil(n * 0.25))
        self.lookback = min(3, max(0, r - 1))
        self.punish_until = 0
        self.last_probe_round = 0
        self.n_players = n
        self.n_rounds = r

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        S_list = []
        if history is not None and history.round_number > 0:
            rounds_completed = history.round_number
            for i in range(rounds_completed):
                coop_opponents = int(np.sum(history.opponent_actions[i, :])) if history.opponent_actions.size else 0
                my_coop = int(bool(history.my_actions[i]))
                S_list.append(my_coop + coop_opponents)
        if t >= self.n_rounds:
            return Action.D
        if history is not None and history.round_number >= 1:
            last_completed = history.round_number
            s_index = last_completed - 1
            S_last = S_list[s_index]
            baseline_count = min(self.lookback, max(0, last_completed - 1))
            if baseline_count >= 1:
                start_idx = max(0, last_completed - 1 - baseline_count)
                baseline_values = S_list[start_idx:start_idx + baseline_count]
                if len(baseline_values) > 0:
                    baseline = int(round(float(np.mean(baseline_values))))
                    if baseline - S_last >= self.drop_threshold:
                        my_last_action_coop = bool(history.my_actions[s_index])
                        triggered_by_probe = my_last_action_coop
                        triggered_by_exploit = False
                        if history.round_number >= 2:
                            prev_S = S_list[s_index - 1]
                            if not my_last_action_coop and prev_S >= self.exploit_threshold:
                                triggered_by_exploit = True
                        if triggered_by_probe or triggered_by_exploit:
                            candidate = min(max(0, self.n_rounds - 1), last_completed + self.punishment_length)
                            if candidate > self.punish_until:
                                self.punish_until = candidate
        if t <= self.punish_until:
            return Action.D
        if t > 1 and t - self.last_probe_round >= self.probe_interval:
            self.last_probe_round = t
            return Action.C
        if history is not None and history.round_number >= 1:
            S_prev = S_list[-1]
            if S_prev >= self.exploit_threshold:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_60(BaseStrategy):
    """
    Aggressive exploitative public-goods strategy.

    - Default: defect (including round 1 and final round).
    - Rare probes (cooperate) to test opponents' willingness to cooperate after you defect.
    - Track, per opponent, how often they cooperate in rounds following a round where you defected.
      Use Laplace smoothing to estimate conditional cooperation rates.
    - If a sufficient number of opponents appear reliably exploitable, enter a short bounded HARVEST
      period where we repeatedly defect to free-ride them.
    - If widespread retaliation is detected (sharp drop in opponents' cooperation after our defects),
      transition to LOCKDOWN and defect for the rest of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.mode = 'EXPLOIT'
        self.harvest_rounds = 0
        n = game_description.n_players
        r = game_description.n_rounds
        self.p_probe = min(0.08, 3.0 / max(1.0, r))
        if n == 2:
            self.s_threshold = 1
        else:
            self.s_threshold = max(2, math.ceil(n / 4))
        self.coop_high = 0.65
        self.coop_low = 0.4
        self.harvest_max = max(1, math.ceil(r / 6))
        self.min_samples = 3
        self.smoothing_alpha = 1.0
        self.sliding_L = min(10, max(1, r - 1))
        self.punish_fraction = 0.5
        self.n = n
        self.r = r

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = getattr(state, 'round_number', 1)
        if history is None or t == 1:
            return Action.D
        if t == self.r:
            self.mode = 'EXPLOIT'
            return Action.D
        m = history.round_number
        opp_actions = history.opponent_actions
        my_actions = history.my_actions
        n_opps = self.n - 1
        total_samples = np.zeros(n_opps, dtype=int)
        coop_after_myD = np.zeros(n_opps, dtype=int)
        for i in range(1, m):
            if not bool(my_actions[i - 1]):
                row = opp_actions[i, :]
                total_samples += 1
                coop_after_myD += row.astype(int)
        alpha = float(self.smoothing_alpha)
        conditional_coop = (coop_after_myD + alpha) / (total_samples + 2.0 * alpha)
        candidate_mask = np.logical_and(total_samples >= self.min_samples, conditional_coop >= self.coop_high)
        num_candidates = int(np.sum(candidate_mask))
        if np.any(total_samples >= 1):
            group_conditional_coop = float(np.mean(conditional_coop[total_samples >= 1]))
        else:
            group_conditional_coop = float(np.mean(conditional_coop)) if n_opps > 0 else 0.0

        def detect_heavy_retaliation() -> bool:
            L = self.sliding_L
            if m < 2:
                return False
            recent_start = max(0, m - L)
            recent_end = m
            recent = opp_actions[recent_start:recent_end, :]
            recent_len = recent.shape[0]
            prior_end = recent_start
            prior_start = max(0, prior_end - L)
            prior = opp_actions[prior_start:prior_end, :] if prior_end > prior_start else None
            prior_len = 0 if prior is None else prior.shape[0]
            if prior is None or prior_len == 0:
                return False
            prior_avg = float(np.mean(prior))
            recent_avg = float(np.mean(recent))
            if prior_avg > 0 and recent_avg < 0.5 * prior_avg:
                return True
            prior_rates = np.mean(prior, axis=0)
            recent_rates = np.mean(recent, axis=0)
            drops = prior_rates - recent_rates
            num_large_drops = int(np.sum(drops > 0.5))
            if num_large_drops / max(1, n_opps) > self.punish_fraction:
                return True
            return False
        if self.mode == 'LOCKDOWN':
            return Action.D
        if detect_heavy_retaliation():
            self.mode = 'LOCKDOWN'
            return Action.D
        if self.mode == 'HARVEST':
            if self.harvest_rounds >= self.harvest_max or num_candidates < self.s_threshold or group_conditional_coop < self.coop_low:
                self.mode = 'EXPLOIT'
                self.harvest_rounds = 0
            else:
                self.harvest_rounds += 1
                return Action.D
        if num_candidates >= self.s_threshold:
            self.mode = 'HARVEST'
            self.harvest_rounds = 1
            return Action.D
        if m >= 1 and bool(my_actions[-1]):
            last_round_coop_frac = float(np.mean(opp_actions[-1, :])) if n_opps > 0 else 0.0
            if last_round_coop_frac >= 0.5:
                num_cooperators_last = int(np.sum(opp_actions[-1, :]))
                if num_cooperators_last >= self.s_threshold:
                    self.mode = 'HARVEST'
                    self.harvest_rounds = 1
                    return Action.D
        if random.random() < self.p_probe:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_61(BaseStrategy):
    """
    Aggressive Conditional Defector (ACD)

    - Default: defect.
    - Probe rarely to find unconditional cooperators.
    - If many opponents are highly cooperative and do not punish your defections, exploit them (defect).
    - If opponents punish your defections (detect drop in their cooperation after you defect), attempt short, costly
      coaxing (cooperate for a small L_coax window) only when remaining rounds and observed cooperation justify it.
    - Maintain per-opponent flags for punishers and forgive them only after strong evidence of high cooperation.
    - Always defect in the final rounds (no end-game cooperation).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.W = 5
        self.coop_high = 0.8
        self.coop_low = 0.2
        self.probe_prob = 0.05
        self.L_coax = 3
        self.punish_grace = None
        self.S_end = 1
        self.delta_drop = 0.2
        self.mode = 'Exploit'
        self.coax_rounds_left = 0
        self.flagged_punishers = set()

    def __call__(self, state: GameState, history: None | PlayerHistory):
        n = self.game_description.n_players
        r_total = self.game_description.n_rounds
        if history is None:
            if r_total <= 3:
                return Action.D
            return Action.D
        rounds_completed = history.round_number
        current_round = rounds_completed + 1
        rounds_remaining = r_total - current_round + 1
        if r_total <= 3:
            return Action.D
        S_end = self.S_end
        if r_total <= 5:
            S_end = max(1, min(2, r_total // 2))
        if rounds_remaining <= S_end:
            self.mode = 'Exploit'
            self.coax_rounds_left = 0
            return Action.D
        opponent_actions = history.opponent_actions
        n_opponents = n - 1
        Wt = min(self.W, rounds_completed)
        if Wt > 0:
            recent_window = opponent_actions[-Wt:, :]
            f_j = np.mean(recent_window.astype(float), axis=0)
        else:
            f_j = np.zeros(n_opponents, dtype=float)
        if n_opponents > 0:
            F_avg = float(np.mean(f_j))
        else:
            F_avg = 0.0
        if rounds_completed >= 1:
            group_coop_prev = float(np.mean(opponent_actions[-1, :].astype(float)))
        else:
            group_coop_prev = 0.0
        W_pre = min(self.W, max(0, rounds_completed - 1))
        if W_pre > 0:
            start_idx = -(W_pre + 1)
            end_idx = -1
            before_window = opponent_actions[start_idx:end_idx, :]
            group_coop_before = float(np.mean(before_window.astype(float)))
        else:
            group_coop_before = group_coop_prev
        i_defected_last = False
        if rounds_completed >= 1:
            i_defected_last = history.my_actions[-1] == False
        punished_now = False
        if i_defected_last and group_coop_before - group_coop_prev >= self.delta_drop:
            punished_now = True
            if W_pre > 0:
                prior_rates = np.mean(opponent_actions[start_idx:end_idx, :].astype(float), axis=0)
                last_round_actions = opponent_actions[-1, :].astype(int)
                for j in range(n_opponents):
                    if prior_rates[j] >= self.coop_high and last_round_actions[j] == 0:
                        self.flagged_punishers.add(int(j))
            elif rounds_completed >= 1:
                last_round_actions = opponent_actions[-1, :].astype(int)
                for j in range(n_opponents):
                    if last_round_actions[j] == 0:
                        self.flagged_punishers.add(int(j))
        if len(self.flagged_punishers) > 0 and Wt > 0:
            for j in list(self.flagged_punishers):
                if f_j[j] >= self.coop_high:
                    self.flagged_punishers.discard(j)
        if self.mode == 'Coaxing' and self.coax_rounds_left > 0:
            self.coax_rounds_left -= 1
            action = Action.C
            if self.coax_rounds_left == 0:
                if F_avg >= self.coop_high:
                    self.mode = 'Exploit'
                else:
                    self.mode = 'Punish'
            return action
        if F_avg >= self.coop_high:
            if rounds_remaining > S_end + 1 and random.random() < self.probe_prob:
                return Action.C
            else:
                return Action.D
        if F_avg <= self.coop_low:
            if rounds_remaining > S_end + 1 and random.random() < self.probe_prob:
                return Action.C
            return Action.D
        punished_history = False
        lookback = min(5, rounds_completed)
        if lookback >= 2:
            drops = 0
            checks = 0
            for offset in range(lookback, 0, -1):
                idx = -offset
                next_idx = idx + 1
                if next_idx >= 0:
                    my_act_t = history.my_actions[idx]
                    if my_act_t == False:
                        checks += 1
                        group_t = float(np.mean(history.opponent_actions[idx, :].astype(float)))
                        group_t1 = float(np.mean(history.opponent_actions[next_idx, :].astype(float)))
                        if group_t - group_t1 >= self.delta_drop:
                            drops += 1
            if checks > 0 and drops / checks >= 0.5:
                punished_history = True
        punished = punished_now or punished_history or len(self.flagged_punishers) > 0
        if punished:
            baseline_ok = F_avg > 0.4
            enough_time = rounds_remaining >= self.L_coax + 2
            too_many_flagged = len(self.flagged_punishers) > max(1, n_opponents // 2)
            if baseline_ok and enough_time and (not too_many_flagged):
                self.mode = 'Coaxing'
                self.coax_rounds_left = self.L_coax - 1
                return Action.C
            else:
                self.mode = 'Punish'
                return Action.D
        if 0.25 < F_avg < self.coop_high:
            if rounds_remaining > S_end + 1 and random.random() < self.probe_prob:
                return Action.C
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_62(BaseStrategy):
    """
    Aggressive Conditional-Grim with Probes (ACG-Probe)

    - Aggressive opening: defect in round 1 and always defect in final round.
    - Cooperate only if a strong majority cooperated in the previous round (threshold tau).
    - If you cooperate and cooperation collapses (C_t < tau), enter punish_mode (permanent defection)
      but periodically execute probes (cooperate every P_probe rounds) to test whether the group
      has re-established strong cooperation. If a probe achieves C_t >= tau, exit punish_mode.
    - Uses only observable history (counts of cooperators per round) and game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.tau = math.ceil(0.8 * self.n)
        self.P_probe = max(2, math.floor(math.sqrt(self.r)))
        self.punish_mode = False
        self.last_probe_round = 0
        self.processed_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        available_completed = history.round_number
        for j in range(self.processed_rounds, available_completed):
            t_completed = j + 1
            my_action = bool(history.my_actions[j])
            opp_coops = int(np.sum(history.opponent_actions[j, :])) if history.opponent_actions.size else 0
            total_cooperators = int(my_action) + opp_coops
            if my_action and t_completed < self.r:
                if self.punish_mode and total_cooperators >= self.tau:
                    self.punish_mode = False
                if total_cooperators < self.tau:
                    self.punish_mode = True
                    self.last_probe_round = t_completed
        self.processed_rounds = available_completed
        t = available_completed + 1
        if t == self.r:
            return Action.D
        if t == 1:
            return Action.D
        if self.punish_mode:
            if t - self.last_probe_round >= self.P_probe:
                self.last_probe_round = t
                return Action.C
            else:
                return Action.D
        else:
            prev_index = available_completed - 1
            if prev_index < 0:
                return Action.D
            prev_my = bool(history.my_actions[prev_index])
            prev_opp_coops = int(np.sum(history.opponent_actions[prev_index, :])) if history.opponent_actions.size else 0
            prev_total_cooperators = int(prev_my) + prev_opp_coops
            if prev_total_cooperators >= self.tau:
                return Action.C
            else:
                return Action.D

class Strategy_AGGRESSIVE_63(BaseStrategy):
    """
    Aggressive Opportunistic Exploiter (AOE)

    Plays to maximize cumulative payoff by exploiting observable clusters of cooperators,
    cooperating only when there is credible evidence cooperation can be sustained and
    harvested, probing cautiously, and punishing/abandoning groups that repeatedly betray.
    Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.state = 'neutral'
        self.exploit_timer = 0
        self.just_exploited = False
        self.last_probe_index = None
        self.S_probe = 1
        self.p_low = 0.25
        self.betray_count_limit = 2
        self.epsilon = 0.05

    def _compute_L(self):
        return min(5, max(1, math.floor(self.r / 10)))

    def _adaptive_p_high(self):
        base = 0.6 - 0.2 * (self.k / self.n - 0.5)
        return max(0.5, min(0.8, base))

    def _compute_exploit_length(self, r_remaining):
        base = max(1, math.floor(0.15 * max(1, r_remaining)))
        adj = random.choice([-1, 0, 1])
        candidate = base + adj
        max_allowed = max(1, r_remaining - 1)
        return max(1, min(candidate, max_allowed))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            rounds_played = 0
        else:
            rounds_played = int(history.round_number)
        r_remaining = self.r - rounds_played
        if r_remaining == 1:
            if self.state != 'punish':
                self.state = self.state
            return Action.D
        L = self._compute_L()
        lookback = min(L, rounds_played)
        p_high = self._adaptive_p_high()
        if lookback > 0 and history is not None:
            opp_slice = history.opponent_actions[-lookback:, :]
            others_coop_frac_list = np.mean(opp_slice.astype(float), axis=1)
            p_recent = float(np.mean(others_coop_frac_list)) if lookback > 0 else 0.0
            my_slice = history.my_actions[-lookback:].astype(bool)
            betray_count = int(np.sum(my_slice & (others_coop_frac_list < self.p_low)))
        else:
            others_coop_frac_list = np.array([])
            p_recent = 0.0
            betray_count = 0
        if self.state == 'punish':
            return Action.D
        if self.state == 'exploiting':
            if self.exploit_timer > 0:
                action = Action.D
                self.exploit_timer -= 1
                if self.exploit_timer == 0:
                    self.state = 'neutral'
                    self.just_exploited = True
                return action
            else:
                self.state = 'neutral'
        if self.just_exploited:
            if p_recent <= self.p_low:
                self.state = 'punish'
                self.just_exploited = False
                return Action.D
            self.just_exploited = False
        if self.state == 'probing':
            self.state = 'neutral'
            if p_recent >= p_high and r_remaining > 1 + 0:
                S_exploit = self._compute_exploit_length(r_remaining)
                if r_remaining > 1 + S_exploit:
                    self.state = 'exploiting'
                    self.exploit_timer = S_exploit
                    return Action.D
            return Action.D
        potential_S_exploit = self._compute_exploit_length(r_remaining)
        R_min = 1 + potential_S_exploit
        if r_remaining <= R_min:
            return Action.D
        if p_recent >= p_high and r_remaining > R_min:
            S_exploit = self._compute_exploit_length(r_remaining)
            self.state = 'exploiting'
            self.exploit_timer = S_exploit
            return Action.D
        if p_recent <= self.p_low:
            if betray_count >= self.betray_count_limit:
                self.state = 'punish'
                return Action.D
            else:
                return Action.D
        if random.random() < self.epsilon:
            self.state = 'probing'
            self.last_probe_index = rounds_played
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_64(BaseStrategy):
    """
    Aggressive Conditional Exploiter (ACE)

    - Default: defect.
    - Exploits unconditional cooperators (high per-player cooperation rate).
    - Opportunistically defects after coop-rich rounds to harvest.
    - Will cooperate only if empirical evidence (delta_est) suggests cooperating
      causally raises others' cooperation enough to recoup the one-shot cost L
      over the remaining rounds.
    - If betrayed after cooperating, enters a multi-round punishment mode (defect).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.L = 1.0 - self.k / float(self.n)
        self.W = int(min(10, max(1, self.r - 1)))
        self.E = 1
        self.exploitation_threshold_high = 0.75
        self.unconditional_cooperator_rate = 0.9
        self.min_detect_delta = 0.1
        self.M = int(min(max(3, math.floor(self.r / 5)), max(1, self.r - 1)))
        self.punish_until = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = state.round_number if hasattr(state, 'round_number') else 1
        else:
            t = state.round_number if hasattr(state, 'round_number') else history.round_number + 1
            try:
                if history.round_number + 1 != t:
                    t = history.round_number + 1
            except Exception:
                pass
        if t > self.r - self.E and t <= self.r:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        completed = history.round_number
        window_len = min(self.W, completed)
        window_start = max(0, completed - window_len)
        coop_rates = np.zeros(self.n - 1, dtype=float)
        if window_len > 0:
            opp_slice = history.opponent_actions[window_start:completed, :]
            coop_counts = np.sum(opp_slice, axis=0)
            coop_rates = coop_counts.astype(float) / float(window_len)
        else:
            coop_rates = np.zeros(self.n - 1, dtype=float)
        if np.any(coop_rates >= self.unconditional_cooperator_rate):
            return Action.D
        if completed >= 1:
            prev_idx = completed - 1
            others_prev_coops = int(np.sum(history.opponent_actions[prev_idx, :])) if history.opponent_actions.shape[0] > prev_idx else 0
            my_prev = bool(history.my_actions[prev_idx])
            total_coops_prev = others_prev_coops + (1 if my_prev else 0)
            threshold_count = math.floor(self.exploitation_threshold_high * float(self.n))
            if total_coops_prev >= threshold_count:
                return Action.D
        others_after_C = []
        others_after_D = []
        max_idx_for_next = completed - 2
        if max_idx_for_next >= window_start:
            for idx in range(window_start, max_idx_for_next + 1):
                my_act = bool(history.my_actions[idx])
                next_others_coops = int(np.sum(history.opponent_actions[idx + 1, :]))
                frac = float(next_others_coops) / float(self.n - 1) if self.n - 1 > 0 else 0.0
                if my_act:
                    others_after_C.append(frac)
                else:
                    others_after_D.append(frac)
        count_C = len(others_after_C)
        count_D = len(others_after_D)
        if count_C < 2 or count_D < 2:
            delta_est = 0.0
        else:
            avg_after_C = float(np.mean(others_after_C)) if count_C > 0 else 0.0
            avg_after_D = float(np.mean(others_after_D)) if count_D > 0 else 0.0
            delta_est = avg_after_C - avg_after_D
        R_remain = self.r - t
        expected_gain_per_round = self.k / float(self.n) * delta_est
        total_expected_gain = R_remain * expected_gain_per_round
        if delta_est >= self.min_detect_delta and total_expected_gain > self.L:
            return Action.C
        if completed >= 1:
            prev_idx = completed - 1
            if bool(history.my_actions[prev_idx]):
                others_prev_coops = int(np.sum(history.opponent_actions[prev_idx, :]))
                betrayal_threshold = math.floor((self.exploitation_threshold_high - 0.25) * float(self.n))
                if others_prev_coops <= betrayal_threshold:
                    new_punish_until = t + self.M - 1
                    new_punish_until = min(new_punish_until, self.r)
                    if new_punish_until > self.punish_until:
                        self.punish_until = new_punish_until
        if t <= self.punish_until:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_65(BaseStrategy):
    """
    Aggressive Conditional Coercer (ACC)

    - Cooperates only when almost everyone is reliably cooperative (trust threshold).
    - If a near-unanimous cooperation round appears, exploits once (defects) then may punish.
    - Punishes opportunistic defectors for a prolonged interval.
    - Always defects on the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = max(1, math.floor(self.r / 10))
        self.q = 0.9
        self.phi = 0.8 if self.r <= 10 else 1.0
        self.P = max(1, math.floor(0.3 * self.r))
        self.X = max(1, math.floor(self.r / 8))
        self.last_exploit_round = -10 ** 9
        self.currently_punishing_until = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            if self.r >= 6:
                return Action.C
            else:
                return Action.D
        if t == self.r:
            return Action.D
        completed = int(history.round_number)
        if completed >= 1:
            my_last = bool(history.my_actions[-1])
            opponents_last = history.opponent_actions[-1, :]
            opp_coops_last = int(np.sum(opponents_last))
            last_round_cooperator_count = int(my_last) + opp_coops_last
            required_coops = math.ceil(self.phi * self.n)
            if last_round_cooperator_count >= required_coops and last_round_cooperator_count < self.n:
                remaining = max(0, self.r - t)
                punish_length = min(self.P, remaining)
                end_punish = t + max(1, punish_length)
                if end_punish > self.currently_punishing_until:
                    self.currently_punishing_until = end_punish
        else:
            last_round_cooperator_count = 0
            required_coops = math.ceil(self.phi * self.n)
        if t <= self.currently_punishing_until:
            return Action.D
        if self.n <= 1:
            return Action.D
        w_actual = min(self.w, completed) if completed > 0 else 0
        recent_rates = []
        if w_actual <= 0:
            recent_rates = [0.0] * (self.n - 1)
        else:
            recent_slice = history.opponent_actions[-w_actual:, :]
            coop_counts = np.sum(recent_slice, axis=0)
            recent_rates = [float(coop_counts[j]) / float(w_actual) for j in range(coop_counts.shape[0])]
        trusted_count = sum((1 for rate in recent_rates if rate >= self.q))
        trusted_fraction = trusted_count / float(self.n - 1)
        if trusted_fraction >= self.phi:
            if t - self.last_exploit_round >= self.X and last_round_cooperator_count >= required_coops:
                self.last_exploit_round = t
                return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_66(BaseStrategy):
    """
    Aggressive self-interested public-goods strategy.

    Summary: Default to defect every round, probe rarely to find exploitable cooperators,
    permanently exploit sizable cohorts of persistent cooperators, and never cooperate
    in the final round. Uses lightweight burn detection to stop any cooperation attempts
    if aggregate cooperation collapses.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploitation_mode = False
        self.burned = False
        self.persistent_cooperators = set()
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        self.p_probe = min(0.05, 5.0 / max(1.0, float(r)))
        self.exploitable_rate = 0.8
        self.exploitable_fraction = max(0.25, 2.0 / float(max(2, n)))
        self.burn_threshold = 0.5
        self.probe_window = int(min(5, max(1, r)))
        self.min_exploit_horizon = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if history is None:
            t = 1
            rounds_completed = 0
        else:
            rounds_completed = history.round_number
            t = rounds_completed + 1
        r_remain = r - t + 1
        if t >= r:
            return Action.D
        if self.burned:
            return Action.D
        if history is None or rounds_completed == 0:
            u = random.random()
            if u < self.p_probe:
                return Action.C
            else:
                return Action.D
        opp_actions = np.array(history.opponent_actions, copy=False)
        if opp_actions.ndim != 2 or opp_actions.shape[1] != n - 1:
            return Action.D
        denom = float(max(1, rounds_completed))
        coop_counts = np.sum(opp_actions.astype(float), axis=0)
        coop_rates = coop_counts / denom
        window = min(self.probe_window, rounds_completed)
        recent_slice = opp_actions[-window:, :] if window > 0 else np.zeros((0, n - 1), dtype=float)
        recent_counts = np.sum(recent_slice.astype(float), axis=0) if window > 0 else np.zeros(n - 1)
        recent_rates = recent_counts / float(max(1, window))
        prev_rounds = rounds_completed - window
        if prev_rounds > 0:
            prev_slice = opp_actions[:prev_rounds, :]
            prev_counts = np.sum(prev_slice.astype(float), axis=0)
            prev_rates = prev_counts / float(prev_rounds)
        else:
            prev_rates = np.zeros(n - 1)
        persistent = set()
        for j in range(n - 1):
            if coop_rates[j] >= self.exploitable_rate:
                persistent.add(j)
            elif recent_rates[j] - prev_rates[j] >= 0.3 and recent_rates[j] > 0.0:
                persistent.add(j)
        self.persistent_cooperators = persistent
        num_persistent = len(self.persistent_cooperators)
        required = self.exploitable_fraction * float(max(1, n - 1))
        if not self.exploitation_mode and num_persistent >= math.ceil(required) and (r_remain >= self.min_exploit_horizon):
            self.exploitation_mode = True
            return Action.D
        if self.exploitation_mode:
            check_window = min(self.probe_window, rounds_completed)
            if check_window > 0:
                recent_opp = opp_actions[-check_window:, :]
                my_actions = np.array(history.my_actions, copy=False)[-check_window:]
                totals = np.sum(recent_opp.astype(float), axis=1) + my_actions.astype(float)
                fracs = totals / float(n)
                if np.all(fracs < self.burn_threshold):
                    self.burned = True
                    self.exploitation_mode = False
                    return Action.D
            return Action.D
        probe_prob = self.p_probe
        if r_remain <= 1:
            probe_prob = 0.0
        elif r_remain == 2:
            probe_prob = max(0.0, self.p_probe * 0.2)
        if random.random() < probe_prob:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_67(BaseStrategy):
    """
    Aggressive Conditional Punisher (ACP)

    - Starts cooperative to capture gains from cooperators.
    - On observing defections while in COOP, immediately punishes targeted defectors for a long, pre-committed period.
    - After punishment, issues one-round reconciliation probes; requires F consecutive unanimous-cooperation probes to forgive.
    - Converts to permanent defection against the group if any opponent reaches S separate offenses.
    - Occasionally performs single-shot opportunistic defections during long unanimous-cooperation stretches to extract extra payoff.
    - Always defects on the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.P_default = math.ceil(0.5 * self.r)
        self.F = 2
        self.S = 2
        self.E = max(3, math.ceil(self.r / 10))
        self.mode = 'COOP'
        self.n_opponents = max(0, self.n_players - 1)
        self.punished_players = set()
        self.punishment_timer = 0
        self.offense_count = np.zeros(self.n_opponents, dtype=int)
        self.last_unanimous_coop_round = 0
        self.last_exploit_round = -10 ** 9
        self.forgiveness_streak = 0
        self.last_processed_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if history is not None and history.round_number > self.last_processed_round:
            for round_idx in range(self.last_processed_round, history.round_number):
                my_action = bool(history.my_actions[round_idx])
                opp_actions = np.array(history.opponent_actions[round_idx, :], dtype=bool) if self.n_opponents > 0 else np.array([], dtype=bool)
                everyone_cooperated = my_action and (opp_actions.size == 0 or opp_actions.all())
                if self.mode == 'PERM_DEFECT':
                    pass
                elif self.mode == 'PUNISH':
                    if self.punishment_timer > 0:
                        self.punishment_timer -= 1
                    if self.punishment_timer == 0:
                        self.mode = 'PROBE'
                        self.forgiveness_streak = 0
                elif self.mode == 'PROBE':
                    if everyone_cooperated:
                        self.forgiveness_streak += 1
                        if self.forgiveness_streak >= self.F:
                            self.punished_players.clear()
                            self.mode = 'COOP'
                            self.forgiveness_streak = 0
                    else:
                        if opp_actions.size > 0:
                            D_indices = set((int(i) for i, a in enumerate(opp_actions) if not a))
                        else:
                            D_indices = set()
                        for j in D_indices:
                            if 0 <= j < self.n_opponents:
                                self.offense_count[j] += 1
                        self.punished_players = set(D_indices)
                        self.punishment_timer = self.P_default
                        if any((self.offense_count[j] >= self.S for j in D_indices)):
                            self.mode = 'PERM_DEFECT'
                        else:
                            self.mode = 'PUNISH'
                elif self.mode == 'COOP':
                    if not my_action:
                        self.last_exploit_round = round_idx + 1
                    if opp_actions.size > 0 and (opp_actions == False).any():
                        D_indices = set((int(i) for i, a in enumerate(opp_actions) if not a))
                        for j in D_indices:
                            if 0 <= j < self.n_opponents:
                                self.offense_count[j] += 1
                        self.punished_players = set(D_indices)
                        self.punishment_timer = self.P_default
                        if any((self.offense_count[j] >= self.S for j in D_indices)):
                            self.mode = 'PERM_DEFECT'
                        else:
                            self.mode = 'PUNISH'
                if everyone_cooperated:
                    self.last_unanimous_coop_round = round_idx + 1
            self.last_processed_round = history.round_number
        if t == self.r:
            return Action.D
        if self.mode == 'PERM_DEFECT':
            return Action.D
        if self.mode == 'PUNISH':
            return Action.D
        if self.mode == 'PROBE':
            return Action.C
        if self.mode == 'COOP':
            W_candidate = max(3, math.ceil(self.r / 10))
            W = min(W_candidate, max(0, t - 1))
            can_exploit = False
            if W >= 1 and history is not None and (history.round_number >= W):
                start_idx = history.round_number - W
                unanimous = True
                for idx in range(start_idx, history.round_number):
                    my_a = bool(history.my_actions[idx])
                    opp_a = np.array(history.opponent_actions[idx, :], dtype=bool) if self.n_opponents > 0 else np.array([], dtype=bool)
                    if not (my_a and (opp_a.size == 0 or opp_a.all())):
                        unanimous = False
                        break
                if unanimous and t - self.last_exploit_round >= self.E:
                    can_exploit = True
            if can_exploit:
                self.last_exploit_round = t
                return Action.D
            else:
                return Action.C
        return Action.C

class Strategy_AGGRESSIVE_68(BaseStrategy):
    """
    Aggressive Bully with Probing and Targeted Punishment (ABPP).

    - Aggressively defects by default and exploits clearly cooperative rounds.
    - Probes occasionally to discover cooperators.
    - Maintains a per-player blacklist for targeted punishment and forgiveness.
    - Issues short group-level punishments if an exploitation produces a large collapse.
    - Never cooperates in the final round and defects in the first round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.H = min(10, max(1, math.floor(r / 3)))
        self.L = min(self.H, 5)
        self.theta_personal = 0.6
        self.theta_high = 0.6
        self.theta_low = 0.3
        self.P = max(1, math.ceil(r / 6))
        self.S = max(3, math.ceil(r / 6))
        self.delta_collapse = 0.3
        self.punish_timer = 0
        self.personal_blacklist = set()
        self.last_probe_round = 0
        self.last_exploit_round = None
        self.group_coop_recent_at_exploit = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        t = state.round_number
        if history is None:
            return Action.D
        rounds_completed = history.round_number
        if t == r:
            return Action.D
        if self.last_exploit_round is not None and self.last_exploit_round == t - 1 and (self.group_coop_recent_at_exploit is not None):
            coop_my_last = bool(history.my_actions[-1])
            coop_opponents_last = int(np.sum(history.opponent_actions[-1, :]))
            group_coop_now = (int(coop_my_last) + coop_opponents_last) / float(n)
            if self.group_coop_recent_at_exploit - group_coop_now >= self.delta_collapse:
                remaining_after_exploit = max(0, r - self.last_exploit_round)
                self.punish_timer = min(self.P, remaining_after_exploit)
            self.last_exploit_round = None
            self.group_coop_recent_at_exploit = None
        H_t = min(self.H, max(0, t - 1))
        if H_t == 0:
            H_t = 0
        if self.punish_timer > 0:
            self.punish_timer -= 1
            return Action.D
        last_L = min(self.L, max(0, t - 1))
        if rounds_completed == 0:
            return Action.D
        coop_rates = []
        reliable_flags = []
        if H_t > 0:
            opp_slice = history.opponent_actions[-H_t:, :]
            coop_rates_np = np.mean(opp_slice.astype(float), axis=0)
            coop_rates = [float(x) for x in coop_rates_np]
            reliable_flags = [rate >= self.theta_personal for rate in coop_rates]
        else:
            coop_rates = [0.0 for _ in range(n - 1)]
            reliable_flags = [False for _ in range(n - 1)]
        reliable_fraction = 0.0
        if n - 1 > 0:
            reliable_fraction = sum(reliable_flags) / float(n - 1)
        coop_my_last = bool(history.my_actions[-1])
        coop_opponents_last = int(np.sum(history.opponent_actions[-1, :]))
        group_coop_last = (int(coop_my_last) + coop_opponents_last) / float(n)
        if H_t > 0:
            my_segment = history.my_actions[-H_t:].astype(int)
            opp_segment = history.opponent_actions[-H_t:, :].astype(int)
            coop_counts_per_round = my_segment + np.sum(opp_segment, axis=1)
            group_coop_recent = float(np.mean(coop_counts_per_round / float(n)))
        else:
            group_coop_recent = float(group_coop_last)
        if last_L > 0:
            coop_threshold_count = int(math.ceil(n * self.theta_high))
            my_lastL = history.my_actions[-last_L:].astype(int)
            opp_lastL = history.opponent_actions[-last_L:, :].astype(int)
            coop_counts_lastL = my_lastL + np.sum(opp_lastL, axis=1)
            for j in range(n - 1):
                j_actions_lastL = opp_lastL[:, j]
                exploited_rounds_mask = (coop_counts_lastL >= coop_threshold_count) & (j_actions_lastL == 0)
                if np.any(exploited_rounds_mask):
                    self.personal_blacklist.add(j)
                if j in self.personal_blacklist:
                    if np.all(j_actions_lastL == 1):
                        self.personal_blacklist.remove(j)
        if group_coop_last >= self.theta_high:
            self.last_exploit_round = t
            self.group_coop_recent_at_exploit = group_coop_recent
            if last_L > 0:
                coop_threshold_count = int(math.ceil(n * self.theta_high))
                my_lastL = history.my_actions[-last_L:].astype(int)
                opp_lastL = history.opponent_actions[-last_L:, :].astype(int)
                coop_counts_lastL = my_lastL + np.sum(opp_lastL, axis=1)
                for j in range(n - 1):
                    j_actions_lastL = opp_lastL[:, j]
                    mask = (coop_counts_lastL >= coop_threshold_count) & (j_actions_lastL == 0)
                    if np.sum(mask) >= 2:
                        self.personal_blacklist.add(j)
            return Action.D
        if reliable_fraction >= self.theta_high:
            reliable_indices = [idx for idx, flag in enumerate(reliable_flags) if flag]
            conflict_with_blacklist = any((idx in self.personal_blacklist for idx in reliable_indices))
            if not conflict_with_blacklist:
                return Action.C
            else:
                return Action.D
        if t - self.last_probe_round >= self.S and t <= r - 2:
            self.last_probe_round = t
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_69(BaseStrategy):
    """
    Opportunistic Exploiter (OE)

    Aggressive default: defect. Uses short probes and brief pardons to detect and exploit
    forgiving opponents while avoiding long cycles of mutual punishment. Keeps internal
    bookkeeping of an active pardon window to ensure consecutive cooperative pardons
    when triggered.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self._pardoning_until_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        R_threshold = 0.25
        p_high = 0.65
        p_low = 0.2
        delta_punish = 0.15
        probe_interval = max(3, math.floor(r / 6))
        if history is None:
            return Action.D
        t = int(state.round_number)
        rounds_remaining = max(0, r - t + 1)
        if t == r:
            self._pardoning_until_round = 0
            return Action.D
        past_rounds = int(history.round_number)
        if past_rounds > 0:
            opp_actions = np.array(history.opponent_actions, dtype=float)
            if opp_actions.size == 0:
                coop_fracs = np.array([], dtype=float)
            else:
                coop_fracs = np.mean(opp_actions, axis=1)
        else:
            coop_fracs = np.array([], dtype=float)
        L = min(5, max(0, t - 1))
        L_actual = min(L, past_rounds)
        if L_actual > 0:
            p_recent = float(np.mean(coop_fracs[-L_actual:]))
        else:
            p_recent = 0.0
        punish_cases = 0
        defections_considered = 0
        if past_rounds >= 2:
            my_actions = np.array(history.my_actions, dtype=bool)
            for s in range(0, past_rounds - 1):
                if not bool(my_actions[s]):
                    delta_s = float(coop_fracs[s + 1] - coop_fracs[s])
                    defections_considered += 1
                    if delta_s <= -delta_punish:
                        punish_cases += 1
        retaliation_rate = punish_cases / defections_considered if defections_considered > 0 else 0.0
        pardon_length = min(3, max(1, math.floor(max(0, rounds_remaining) / 6)))
        if self._pardoning_until_round != 0 and t > self._pardoning_until_round:
            self._pardoning_until_round = 0
        if p_recent <= p_low:
            self._pardoning_until_round = 0
            return Action.D
        short_run_threshold = max(3, math.floor(r / 10))
        if retaliation_rate > R_threshold and rounds_remaining <= short_run_threshold:
            if self._pardoning_until_round >= t:
                abort_pardon = False
                if past_rounds >= 1:
                    last_coop = float(coop_fracs[-1])
                    if last_coop <= p_low:
                        abort_pardon = True
                if not abort_pardon and past_rounds >= 2:
                    if float(coop_fracs[-1] - coop_fracs[-2]) <= -delta_punish:
                        abort_pardon = True
                if abort_pardon:
                    self._pardoning_until_round = 0
                    return Action.D
                else:
                    return Action.C
            else:
                self._pardoning_until_round = t + pardon_length - 1
                return Action.C
        if retaliation_rate <= R_threshold:
            self._pardoning_until_round = 0
            return Action.D
        if probe_interval > 0 and t % probe_interval == 0 and (rounds_remaining > 1):
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_70(BaseStrategy):
    """
    Aggressive Grim-Exploit (AGE)

    - Start by cooperating.
    - If any opponent has ever defected in any past round, enter permanent punishment (always defect).
    - If not punished and the previous round all other players cooperated, exploit once by defecting.
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punished = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            self.punished = False
            return Action.C
        if history.opponent_actions.size > 0:
            any_opponent_defected = bool(np.any(~history.opponent_actions))
            self.punished = self.punished or any_opponent_defected
        else:
            any_opponent_defected = False
        t = int(state.round_number)
        if t == int(self.game_description.n_rounds):
            return Action.D
        if self.punished:
            return Action.D
        try:
            last_round_opponents = history.opponent_actions[-1, :]
            if last_round_opponents.size > 0 and bool(np.all(last_round_opponents)):
                return Action.D
            else:
                return Action.C
        except Exception:
            return Action.D

class Strategy_AGGRESSIVE_71(BaseStrategy):
    """
    Aggressive Conditional Exploiter (ACE)

    - Starts with a short cooperative probe.
    - Tracks recent cooperation rates for each opponent (sliding window).
    - Classifies opponents as reliable cooperators or reliable defectors.
    - Targets reliable defectors with timed punishment (defect for punish_length rounds).
    - Cooperates only when recent group cooperation is high and a majority of opponents are reliable cooperators.
    - Escalates to permanent defection on a surprising mass defection.
    - Always defects in the final `endgame` rounds.
    - Adds a tiny randomness epsilon when cooperating to reduce pure determinism.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.w = min(5, max(1, math.floor(self.r / 4)))
        self.init_grace = 1
        self.q_high = 0.75
        self.q_low = 0.2
        self.alpha = 0.7
        self.punish_length = min(max(2, math.floor(self.r / 6)), self.r)
        self.endgame = 1
        self.epsilon = 0.02
        self.punish_timers = {}
        self.perm_defect = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if history is None:
            return Action.C
        if self.perm_defect:
            return Action.D
        if t > self.r - self.endgame:
            return Action.D
        num_opponents = self.n - 1
        window = min(self.w, max(0, history.round_number))
        rates = np.zeros(num_opponents, dtype=float)
        if window > 0:
            recent = history.opponent_actions[-window:, :]
            counts = np.sum(recent, axis=0)
            rates = counts / float(window)
        reliable_cooperators = [j for j in range(num_opponents) if rates[j] >= self.q_high]
        reliable_defectors = [j for j in range(num_opponents) if rates[j] <= self.q_low]
        expired = []
        for j in list(self.punish_timers.keys()):
            self.punish_timers[j] -= 1
            if self.punish_timers[j] <= 0:
                expired.append(j)
        for j in expired:
            self.punish_timers.pop(j, None)
        for j in reliable_defectors:
            if j not in self.punish_timers:
                self.punish_timers[j] = self.punish_length
        if len(self.punish_timers) > 0:
            return Action.D
        if history.round_number >= 2:
            coop_prev = int(np.sum(history.opponent_actions[-1, :]))
            coop_prevprev = int(np.sum(history.opponent_actions[-2, :]))
            if coop_prevprev > 0:
                decline_fraction = (coop_prevprev - coop_prev) / float(num_opponents)
                if decline_fraction >= 0.4:
                    self.perm_defect = True
                    return Action.D
        coop_others_last = int(np.sum(history.opponent_actions[-1, :])) if history.round_number >= 1 else 0
        fraction_cooperators_last = coop_others_last / float(num_opponents) if num_opponents > 0 else 0.0
        majority_required = math.ceil(num_opponents / 2.0)
        if fraction_cooperators_last >= self.alpha and len(reliable_cooperators) >= majority_required:
            if random.random() < self.epsilon:
                return Action.D
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_72(BaseStrategy):
    """
    Aggressive Exploit-and-Grim (AEG)

    - Defaults to defection.
    - Only enters a cooperative run when every other player cooperated in the
      previous round (unanimous signal) AND there are at least T_min rounds
      remaining (including the current) so that sustained cooperation strictly
      outperforms a one-shot exploitation.
    - If cooperating and any other player defects afterwards, switch to permanent
      punishment (defect forever).
    - Always defect in the first and last rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        denom = self.k - 1.0
        if denom <= 0.0:
            self.T_min = 1
        else:
            raw = (1.0 - self.k / float(self.n)) / denom
            self.T_min = max(1, int(math.ceil(raw)))
        self.coop_mode = False
        self.punished = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        t = int(state.round_number)
        if self.punished:
            return Action.D
        if t == 1 or t == self.r:
            return Action.D
        if history.round_number == 0:
            others_prev = 0
        else:
            others_prev = int(np.sum(history.opponent_actions[-1, :]))
        n_minus_1 = self.n - 1
        R_remain = self.r - t + 1
        if self.coop_mode:
            if others_prev == n_minus_1:
                return Action.C
            else:
                self.punished = True
                return Action.D
        if others_prev == n_minus_1 and R_remain >= self.T_min:
            self.coop_mode = True
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_73(BaseStrategy):
    """
    Aggressive Opportunistic Exploiter (AOE)

    Defect-by-default, occasional randomized probes, opportunistic building of cooperation,
    exploit when recent other-cooperation rate is very high, and grim permanent punishment
    if ever 'suckered' (i.e., cooperated while a strong majority defected).
    Also always defects in the last two rounds (backward induction).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, max(1, self.r - 1))
        self.p_probe = min(0.25, 2.0 / max(2, self.n_players))
        self.p_build_base = 0.6
        k_over_n = self.k / float(self.n_players)
        self.tau_build = 0.55
        self.tau_exploit = 0.85 * (1.0 - 0.5 * max(0.0, k_over_n - 0.8))
        if k_over_n >= 0.8:
            self.p_build = min(0.95, self.p_build_base * (1.0 + (k_over_n - 0.8)))
        else:
            self.p_build = max(0.15, self.p_build_base * (1.0 - (0.8 - k_over_n) * 0.25))
        self.punish_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t >= max(1, self.r - 1):
            return Action.D
        if history is not None and history.round_number > 0:
            last_idx = history.round_number - 1
            try:
                my_last = bool(history.my_actions[last_idx])
            except Exception:
                my_last = False
            try:
                opp_last_counts = int(np.sum(history.opponent_actions[last_idx, :]))
            except Exception:
                opp_last_counts = 0
            total_last_cooperators = opp_last_counts + (1 if my_last else 0)
            thresh = math.floor((self.n_players - 1) / 2)
            if my_last and total_last_cooperators <= thresh:
                self.punish_mode = True
            if not self.punish_mode and my_last and (history.round_number >= 3):
                L_pre = min(self.W, history.round_number - 1)
                if L_pre >= 1:
                    try:
                        recent_others = np.sum(history.opponent_actions[-(L_pre + 1):-1, :], axis=0)
                        recent_rate_pre = float(np.sum(recent_others)) / (L_pre * max(1, self.n_players - 1))
                    except Exception:
                        recent_rate_pre = 0.0
                    try:
                        last_other_rate = float(opp_last_counts) / max(1, self.n_players - 1)
                    except Exception:
                        last_other_rate = 0.0
                    if recent_rate_pre - last_other_rate > 0.3:
                        self.punish_mode = True
        if self.punish_mode:
            return Action.D
        if history is None or history.round_number == 0:
            if random.random() < self.p_probe:
                return Action.C
            else:
                return Action.D
        L = min(self.W, history.round_number)
        if L <= 0:
            coop_rate = 0.0
        else:
            try:
                recent_opponent_actions = history.opponent_actions[-L:, :]
                total_other_coops = float(np.sum(recent_opponent_actions))
                coop_rate = total_other_coops / (L * max(1, self.n_players - 1))
            except Exception:
                coop_rate = 0.0
        if coop_rate >= self.tau_exploit:
            return Action.D
        if coop_rate >= self.tau_build:
            if random.random() < self.p_build:
                return Action.C
            else:
                return Action.D
        if random.random() < self.p_probe:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_74(BaseStrategy):
    """
    Aggressive Conditional Cooperator (AggroCC)

    - Probes by cooperating on first round for sufficiently long games.
    - Cooperates when recent aggregate cooperation is high (f_recent >= theta).
    - When there is a clear stable full-cooperation streak, exploits deterministically every E rounds.
    - If betrayed while cooperating, enters a calibrated punishment phase:
        * majority betrayal -> defect for remaining rounds
        * minority betrayal -> defect for punish_length = max(punish_min, ceil(alpha * r * (d/n)))
    - Punishments can be forgiven early only after observing two consecutive full-cooperation rounds.
    - Always defects in very short games (r <= R_short) and in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.m = 2
        self.theta = 0.66
        self.alpha = 0.25
        self.punish_min = 1
        self.E = 4
        self.R_short = 4
        self.punish_timer = 0
        self.consecutive_fullcoop = 0
        self.last_exploit_round = -10 ** 9

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if self.r <= self.R_short:
            return Action.D
        if t == self.r:
            return Action.D
        if history is None:
            return Action.C

        def cooperators_in_round(idx: int) -> int:
            my = int(bool(history.my_actions[idx]))
            opp_sum = int(np.sum(history.opponent_actions[idx, :]))
            return my + opp_sum
        rounds_completed = history.round_number
        if rounds_completed >= 1:
            S_prev = cooperators_in_round(rounds_completed - 1)
            f_prev = float(S_prev) / float(self.n)
        else:
            S_prev = None
            f_prev = 0.0
        recent_counts = []
        if rounds_completed >= 1:
            start_idx = max(0, rounds_completed - self.m)
            for idx in range(start_idx, rounds_completed):
                S = cooperators_in_round(idx)
                recent_counts.append(float(S) / float(self.n))
        if len(recent_counts) > 0:
            f_recent = float(np.mean(np.array(recent_counts)))
        else:
            f_recent = 0.0
        if S_prev is not None and S_prev == self.n:
            self.consecutive_fullcoop += 1
        else:
            self.consecutive_fullcoop = 0
        if rounds_completed >= 1:
            my_last = bool(history.my_actions[-1])
            if my_last and S_prev is not None and (S_prev < self.n):
                d = self.n - S_prev
                rem_including_current = max(0, self.r - t + 1)
                if d > self.n / 2.0:
                    new_punish = rem_including_current
                else:
                    punish_length = int(math.ceil(self.alpha * float(self.r) * (float(d) / float(self.n))))
                    punish_length = max(self.punish_min, punish_length)
                    new_punish = min(punish_length, rem_including_current)
                if new_punish > self.punish_timer:
                    self.punish_timer = new_punish
        if self.punish_timer > 0 and rounds_completed >= 2:
            S_last = cooperators_in_round(rounds_completed - 1)
            S_prev2 = cooperators_in_round(rounds_completed - 2)
            if S_last == self.n and S_prev2 == self.n:
                self.punish_timer = 0
        if self.punish_timer > 0:
            action = Action.D
            self.punish_timer = max(0, self.punish_timer - 1)
            return action
        if self.consecutive_fullcoop >= 2 and t - self.last_exploit_round >= self.E:
            self.last_exploit_round = t
            self.consecutive_fullcoop = 0
            return Action.D
        if f_recent >= self.theta:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_75(BaseStrategy):
    """
    Aggressive Exploit-and-Grim (AEG)

    - Default: defect.
    - Exploit if many others cooperated last round.
    - Rarely probe/build to try to establish near-universal cooperation, only on a probe schedule.
    - If you cooperate and any opponent defects in that same round, enter permanent GRIM (always defect).
    - Never cooperate in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.n = int(n)
        self.r = int(r)
        self.M = min(10, max(1, self.r - 1))
        self.P_probe = max(3, int(math.floor(self.r / 10)))
        self.T_exploit = int(math.ceil(0.6 * (self.n - 1)))
        self.T_build = int(math.ceil(0.9 * (self.n - 1)))
        self.T_build_frac = self.T_build / (self.n - 1) if self.n - 1 > 0 else 1.0
        self.GRIM = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is not None and history.round_number > 0:
            last_my_action = bool(history.my_actions[-1])
            if last_my_action:
                last_opp_actions = history.opponent_actions[-1, :]
                if np.any(last_opp_actions == False):
                    self.GRIM = True
        if t == self.r:
            return Action.D
        if self.GRIM:
            return Action.D
        coop_count_last = 0
        coop_recent_avg = 0.0
        if history is not None and history.round_number > 0:
            coop_count_last = int(np.sum(history.opponent_actions[-1, :]))
            observed_rounds = min(self.M, history.round_number)
            if observed_rounds > 0:
                recent_slice = history.opponent_actions[-observed_rounds:, :]
                coop_recent_total = int(np.sum(recent_slice))
                coop_recent_avg = coop_recent_total / (observed_rounds * (self.n - 1))
            else:
                coop_recent_avg = 0.0
        if history is not None and history.round_number > 0 and (coop_count_last >= self.T_exploit):
            return Action.D
        if coop_recent_avg >= self.T_build_frac and t % self.P_probe == 0:
            return Action.C
        if history is not None and history.round_number > 0 and (coop_recent_avg >= 0.5) and (t % self.P_probe == 0):
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_76(BaseStrategy):
    """
    Aggressive Conditional Blacklist (ACB)

    Default: defect. Probe briefly, open short cooperation windows only when a conservative
    non-blacklisted majority cooperated in the previous round. Inside cooperation windows,
    cooperate except for periodic exploitation slots. Permanently blacklist players who
    defect while we cooperated; allow forgiveness only after sustained cooperative behaviour
    in rounds where we did not cooperate. Always defect in the final round. Tunable
    parameters provided as sensible defaults.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.blacklist = set()
        self.coop_phase = False
        self.coop_phase_age = 0
        self.planned_length = 0
        self.last_abort_round = -999
        self.W = min(5, max(1, self.game_description.n_rounds - 1))
        self.R_forgive = 3
        self.E_exploit = 4
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        self.M = math.ceil((n - 1) / 2)
        self.L_max = max(1, math.floor(r / 4))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        r_total = self.game_description.n_rounds
        n_players = self.game_description.n_players
        if self.game_description.k <= 1.05:
            return Action.D
        if history is None:
            return Action.D
        completed = history.round_number
        t = completed + 1
        if t == r_total:
            return Action.D
        if r_total <= 2:
            return Action.D
        num_opponents = n_players - 1
        for j in range(num_opponents):
            for rr in range(completed):
                if history.my_actions[rr] and (not bool(history.opponent_actions[rr, j])):
                    self.blacklist.add(j)
                    break
        to_remove = []
        for j in list(self.blacklist):
            consec = 0
            for rr in range(completed - 1, -1, -1):
                if not history.my_actions[rr]:
                    if bool(history.opponent_actions[rr, j]):
                        consec += 1
                        if consec >= self.R_forgive:
                            to_remove.append(j)
                            break
                    else:
                        break
                else:
                    continue
        for j in to_remove:
            self.blacklist.discard(j)
        S = 0
        S_nonblack = 0
        if completed >= 1:
            prev_idx = completed - 1
            prev_row = history.opponent_actions[prev_idx, :]
            S = int(np.sum(prev_row))
            nonblack_mask = [j not in self.blacklist for j in range(num_opponents)]
            S_nonblack = int(np.sum([1 for j in range(num_opponents) if nonblack_mask[j] and bool(prev_row[j])]))
        else:
            S = 0
            S_nonblack = 0
        action = Action.D
        if self.coop_phase:
            if S_nonblack < self.M:
                self.coop_phase = False
                self.last_abort_round = t
                action = Action.D
            elif self.coop_phase_age >= self.planned_length:
                self.coop_phase = False
                self.last_abort_round = t
                action = Action.D
            else:
                if (self.coop_phase_age + 1) % self.E_exploit == 0:
                    action = Action.D
                else:
                    action = Action.C
                self.coop_phase_age += 1
                if self.coop_phase_age >= self.planned_length:
                    pass
        else:
            C_attempt = S_nonblack >= self.M
            has_future_round_after = r_total - t >= 1
            waited_after_abort = t > self.last_abort_round + 1
            if C_attempt and has_future_round_after and waited_after_abort:
                self.coop_phase = True
                self.coop_phase_age = 0
                self.planned_length = min(self.L_max, r_total - t)
                if 1 % self.E_exploit == 0:
                    action = Action.D
                else:
                    action = Action.C
                self.coop_phase_age += 1
                if self.coop_phase_age >= self.planned_length:
                    self.coop_phase = False
                    self.last_abort_round = t
            else:
                action = Action.D
        return action

class Strategy_AGGRESSIVE_77(BaseStrategy):
    """
    Probe-Exploit-Grim (PEG)

    Aggressive, adaptive public-goods strategy:
    - Probe for m rounds by cooperating.
    - Exploit when group cooperation is high (defect to extract payoff).
    - Harsh punishment when betrayed after cooperating: punish for P_base * betray_count rounds (bounded).
    - After punishment finishes, give one cooperating test round (unless in endgame).
    - Always defect in the last E rounds (endgame).
    - Uses a small forgiveness probability epsilon during punishment to occasionally probe.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = int(self.game_description.n_rounds)
        self.n = int(self.game_description.n_players)
        self.W = min(5, max(1, r - 1))
        self.m = min(2, max(1, r - 1))
        self.E = min(2, max(1, r - 1))
        self.tau_high = 0.75
        self.tau_low = 0.25
        self.P_base = max(1, math.ceil(r / 10))
        self.epsilon = 0.02
        self.punishment_remaining = 0
        self.just_finished_punishment = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        r = int(self.game_description.n_rounds)
        n = self.n
        if history is None:
            return Action.C
        if t <= self.m:
            return Action.C
        if t > r - self.E:
            self.just_finished_punishment = False
            return Action.D
        if self.just_finished_punishment:
            self.just_finished_punishment = False
            return Action.C
        if self.punishment_remaining > 0:
            if random.random() < self.epsilon:
                action = Action.C
            else:
                action = Action.D
            self.punishment_remaining -= 1
            if self.punishment_remaining <= 0:
                self.punishment_remaining = 0
                self.just_finished_punishment = True
            return action
        if history.round_number >= 1:
            you_cooperated_last = bool(history.my_actions[-1])
            opp_last = history.opponent_actions[-1, :]
            betray_count = int((~opp_last).sum())
            if you_cooperated_last and betray_count > 0:
                remaining_rounds = max(0, r - t + 1)
                punish_len = min(remaining_rounds, self.P_base * betray_count)
                self.punishment_remaining = int(punish_len)
                action = Action.D
                self.punishment_remaining -= 1
                if self.punishment_remaining <= 0:
                    self.punishment_remaining = 0
                    self.just_finished_punishment = True
                else:
                    self.just_finished_punishment = False
                return action
        available_rounds = int(history.round_number)
        window_len = min(self.W, max(1, available_rounds))
        opp_recent = history.opponent_actions[-window_len:, :]
        coop_counts = np.sum(opp_recent, axis=0)
        s_j = coop_counts / float(window_len)
        S = float(np.mean(s_j)) if s_j.size > 0 else 0.0
        last_round_opp_coops = int(history.opponent_actions[-1, :].sum())
        last_round_self_coop = int(bool(history.my_actions[-1]))
        last_round_cooperators = last_round_opp_coops + last_round_self_coop
        last_round_others_cooperators = last_round_cooperators - last_round_self_coop
        if S >= self.tau_high and last_round_others_cooperators >= math.ceil((n - 1) / 2):
            return Action.D
        if S <= self.tau_low:
            return Action.D
        if last_round_cooperators >= math.ceil(n / 2):
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_78(BaseStrategy):
    """
    Aggressive Adaptive Free-Rider (AAFR)

    Default is defection. Probe minimally, detect whether the group punishes defections,
    and if so only cooperate when nearly all others cooperated last round or when a
    forgiveness/reset probe is triggered after sustained payoff drops.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.probe_len = max(1, min(4, math.floor(self.r / 10)))
        self.lookback_w = min(5, max(1, self.r - 1))
        self.punish_drop = 0.2
        self.punish_fraction_threshold = 0.5
        self.join_fraction = 0.8
        self.forgiveness_drop_frac = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        t = int(state.round_number)
        n = int(self.n)
        r = int(self.r)
        k = float(self.k)
        if r <= 3:
            return Action.D
        if t == r:
            return Action.D
        if t <= self.probe_len:
            return Action.D
        completed = int(history.round_number)
        if completed < 2:
            return Action.D
        defect_rounds = [s for s in range(completed - 1) if not bool(history.my_actions[s])]
        recent = defect_rounds[-self.lookback_w:]
        if len(recent) == 0:
            return Action.D
        punish_count = 0
        for s in recent:
            avg_s = float(np.mean(history.opponent_actions[s, :]))
            avg_s1 = float(np.mean(history.opponent_actions[s + 1, :]))
            if avg_s1 <= avg_s - self.punish_drop:
                punish_count += 1
        p_punish = punish_count / len(recent)
        if p_punish <= self.punish_fraction_threshold:
            return Action.D
        window = min(self.lookback_w, completed)
        if window >= 1:
            recent_payoffs = np.array(history.my_payoffs[-window:], dtype=float)
            my_recent_avg = float(np.mean(recent_payoffs))
            others_frac_per_round = np.mean(history.opponent_actions[-window:, :], axis=1)
            avg_others_frac = float(np.mean(others_frac_per_round))
            expected_defect_payoff = 1.0 + k / n * (avg_others_frac * (n - 1))
            if my_recent_avg < (1.0 - self.forgiveness_drop_frac) * expected_defect_payoff:
                return Action.C
        last_round_idx = completed - 1
        if last_round_idx >= 0:
            last_round_others = int(np.sum(history.opponent_actions[last_round_idx, :]))
            required = math.ceil((n - 1) * self.join_fraction)
            if last_round_others >= required:
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_79(BaseStrategy):
    """
    Aggressive Adaptive Exploiter (AAE)

    - Starts by defecting (probe).
    - Estimates whether the population punishes defections by looking for drops
      in cooperation following high-cooperation rounds over a lookback window L.
    - If population appears non-retaliatory, stays in Exploit Mode (defect every round,
      with a tiny probe probability to test punishments).
    - If population appears retaliatory, uses Conditional-Cooperation Mode:
      cooperate when the majority cooperated in the previous round, otherwise defect,
      to avoid provoking group punishment.
    - If the strategy detects that the group specifically punished it after it defected
      following a high-cooperation round, it switches to Exploit-Forever (permanent defection).
    - Always defects in the first round and in the final two rounds (endgame logic).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = getattr(game_description, 'n_rounds', 1)
        self.L_default = min(6, max(0, r - 1))
        self.F_high = 0.65
        self.drop_delta = 0.25
        self.P_threshold = 0.5
        self.Majority_threshold = 0.5
        self.Forgiveness_period = 3
        denom = max(1, r / 10.0)
        self.Probe_prob = min(0.05, 1.0 / denom)
        self.exploit_forever = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        t = int(state.round_number)
        if history is None:
            return Action.D
        completed = int(history.round_number)
        H = r - t + 1
        if H <= 2:
            return Action.D
        if self.exploit_forever:
            return Action.D
        L = min(self.L_default, max(0, completed))
        s_start = max(2, completed - L + 1) if completed >= 2 else 2
        s_end = completed
        eligible = 0
        punitive = 0

        def coop_fraction_at(round_idx_zero_based: int) -> float:
            my_a = bool(history.my_actions[round_idx_zero_based])
            opp_row = history.opponent_actions[round_idx_zero_based]
            opp_coops = int(np.sum(opp_row))
            total_coops = opp_coops + (1 if my_a else 0)
            return float(total_coops) / float(n)
        for s in range(s_start, s_end + 1):
            prev_idx = s - 2
            curr_idx = s - 1
            if prev_idx < 0 or curr_idx < 0 or curr_idx >= completed:
                continue
            f_prev = coop_fraction_at(prev_idx)
            f_curr = coop_fraction_at(curr_idx)
            if f_prev >= self.F_high:
                eligible += 1
                if f_curr <= f_prev - self.drop_delta:
                    punitive += 1
                    my_defected_in_curr = not bool(history.my_actions[curr_idx])
                    if my_defected_in_curr:
                        self.exploit_forever = True
        if eligible == 0:
            eligible = 1
        PunishmentEvidence = float(punitive) / float(eligible)
        if self.exploit_forever:
            return Action.D
        if PunishmentEvidence < self.P_threshold:
            if random.random() < self.Probe_prob:
                return Action.C
            return Action.D
        elif completed >= 1:
            last_round_idx = completed - 1
            f_last = coop_fraction_at(last_round_idx)
            if f_last > self.Majority_threshold:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_AGGRESSIVE_80(BaseStrategy):
    """
    Predatory Conditional Exploiter (PCE)

    Aggressive default defecting strategy that:
    - probes occasionally to learn which opponents forgive defections,
    - exploits forgiving majorities by defecting,
    - punishes when exploitation triggers retaliation,
    - stabilizes (cooperates for a short block) when many opponents retaliate
      and long-run cooperation is plausibly beneficial.

    Implements the parameters and behavior described in the specification.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.L = max(1, min(6, math.floor(self.r / 5)))
        self.theta_forgive = 0.75
        self.theta_retaliate = 0.6
        self.tau_exploit = 0.6
        self.tau_punish = 0.5
        self.probe_interval = max(3, math.ceil(self.r / 10))
        self.punish_length = min(self.L, max(1, math.ceil(self.r / 10)))
        self.safety_margin_rounds = 1
        self.mode = 'DEFAULT'
        self.punish_countdown = 0
        self.stabilize_countdown = 0
        self.last_probe_round = 0
        self.exploit_entry_Pg = None
        self.stabilize_entry_Pg = None
        self.previous_Pg = 0.0
        self._small_improvement_threshold = 0.1
        self._drop_threshold = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        if self.r <= 2:
            return Action.D
        if t >= self.r:
            self.mode = 'DEFAULT'
            self.punish_countdown = 0
            self.stabilize_countdown = 0
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        completed = history.round_number
        window = min(self.L, completed)
        start = completed - window
        num_opponents = history.opponent_actions.shape[1] if history.opponent_actions.size else 0
        if num_opponents == 0:
            return Action.D
        if window > 0:
            window_actions = history.opponent_actions[start:completed, :]
            p_j = np.mean(window_actions.astype(float), axis=0)
        else:
            p_j = np.zeros(num_opponents, dtype=float)
        f_j = np.zeros(num_opponents, dtype=float)
        q_j = np.zeros(num_opponents, dtype=float)
        cond_idxs = [i for i in range(max(start, 1), completed)]
        if len(cond_idxs) > 0:
            prev_my = history.my_actions[np.array(cond_idxs) - 1]
            opp_at_i = history.opponent_actions[np.array(cond_idxs), :]
            prev_def_mask = ~prev_my
            denom = prev_def_mask.sum()
            if denom > 0:
                rows = np.where(prev_def_mask)[0]
                selected_opp = opp_at_i[rows, :].astype(float)
                forgive_counts = selected_opp.sum(axis=0)
                retaliate_counts = (1.0 - selected_opp).sum(axis=0)
                f_j = forgive_counts / float(denom)
                q_j = retaliate_counts / float(denom)
            else:
                f_j = np.zeros(num_opponents, dtype=float)
                q_j = np.zeros(num_opponents, dtype=float)
        else:
            f_j = np.zeros(num_opponents, dtype=float)
            q_j = np.zeros(num_opponents, dtype=float)
        Fg = float((f_j >= self.theta_forgive).sum()) / float(num_opponents)
        Rg = float((q_j >= self.theta_retaliate).sum()) / float(num_opponents)
        Pg = float(np.mean(p_j)) if p_j.size > 0 else 0.0
        m = self.r - t + 1
        if self.punish_countdown > 0:
            self.punish_countdown -= 1
            if Pg > self.previous_Pg + self._small_improvement_threshold:
                self.punish_countdown = 0
                self.mode = 'DEFAULT'
            return Action.D
        if self.mode == 'EXPLOIT':
            if Rg >= self.tau_punish:
                self.mode = 'PUNISH'
                self.punish_countdown = self.punish_length
                self.previous_Pg = Pg
                return Action.D
            if self.exploit_entry_Pg is not None and self.exploit_entry_Pg - Pg >= self._drop_threshold:
                self.mode = 'PUNISH'
                self.punish_countdown = self.punish_length
                self.previous_Pg = Pg
                return Action.D
            return Action.D
        if self.mode == 'STABILIZE':
            if self.stabilize_countdown > 0:
                self.stabilize_countdown -= 1
                if Pg > self.stabilize_entry_Pg + self._small_improvement_threshold:
                    self.mode = 'DEFAULT'
                return Action.C
            elif Pg >= 0.9 and m >= 3 * self.L:
                self.mode = 'EXPLOIT'
                self.exploit_entry_Pg = Pg
                return Action.D
            else:
                self.mode = 'DEFAULT'
                return Action.D
        if Rg >= self.tau_punish and m >= 3 * self.L:
            self.mode = 'STABILIZE'
            self.stabilize_countdown = min(self.L, math.ceil(m / 4.0))
            self.stabilize_entry_Pg = Pg
            return Action.C
        if Fg >= self.tau_exploit and m > self.safety_margin_rounds:
            self.mode = 'EXPLOIT'
            self.exploit_entry_Pg = Pg
            return Action.D
        if Pg >= 0.9 and m >= 3 * self.L:
            self.mode = 'STABILIZE'
            self.stabilize_countdown = min(self.L, math.ceil(m / 4.0))
            self.stabilize_entry_Pg = Pg
            return Action.C
        if t - self.last_probe_round >= self.probe_interval and t < self.r - self.safety_margin_rounds:
            self.last_probe_round = t
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_81(BaseStrategy):
    """
    Aggressive Opportunistic Defector with Triggered Cooperation (AOD-TC).

    - Default: defect.
    - Exploit when a large fraction of opponents have recently cooperated.
    - When an upward trend is detected, start a short cooperation trial (S rounds).
    - If a trial is exploited (majority of opponents do not reciprocate in the first trial round),
      enter Grim punishment (permanent defection).
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.Punish = False
        self.Trial_remaining = 0
        self.trial_total = 2
        self.trial_start = None
        self.trial_evaluated = True
        self.theta_exploit = 0.6
        self.theta_abandon = 0.2
        self.delta = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        t = int(state.round_number)
        if history is None or history.round_number == 0:
            past_rounds = 0
        else:
            past_rounds = history.round_number
        if not self.trial_evaluated and self.trial_start is not None:
            if history is not None and history.round_number >= self.trial_start:
                idx = self.trial_start - 1
                coop_others = int(np.sum(history.opponent_actions[idx, :]))
                if coop_others <= math.floor((n - 1) / 2):
                    self.Punish = True
                self.trial_evaluated = True
        if t == r:
            return Action.D
        if self.Punish:
            return Action.D
        if self.Trial_remaining > 0:
            self.Trial_remaining -= 1
            return Action.C
        if history is None or past_rounds == 0:
            return Action.D
        w = min(6, max(1, r - 1))
        recent_start = max(0, past_rounds - w)
        recent_end = past_rounds
        recent_opponent_actions = history.opponent_actions[recent_start:recent_end, :]
        if recent_opponent_actions.size == 0:
            avg_recent = 0.0
        else:
            per_round_fracs = np.sum(recent_opponent_actions, axis=1) / float(n - 1)
            avg_recent = float(np.mean(per_round_fracs))
        if avg_recent >= self.theta_exploit:
            return Action.D
        if avg_recent <= self.theta_abandon:
            return Action.D
        trend = 0.0
        if past_rounds >= 2 * w:
            earlier_start = past_rounds - 2 * w
            earlier_end = past_rounds - w
            earlier_actions = history.opponent_actions[earlier_start:earlier_end, :]
            if earlier_actions.size > 0:
                earlier_fracs = np.sum(earlier_actions, axis=1) / float(n - 1)
                avg_earlier = float(np.mean(earlier_fracs))
                trend = avg_recent - avg_earlier
            else:
                trend = 0.0
        else:
            trend = 0.0
        if trend >= self.delta and (not self.Punish):
            self.Trial_remaining = max(0, self.trial_total - 1)
            self.trial_start = t
            self.trial_evaluated = False
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_82(BaseStrategy):
    """
    Aggressive Grim + Targeted Punishment (AGTP)

    - Aggressive by default: start with defection, defect in final L rounds, and punish deviations harshly.
    - Attempt to form a cooperation coalition only when a sufficient fraction of opponents have
      cooperated recently (window w) and there is enough remaining horizon to make punishment credible.
    - Punishments are targeted per-opponent (punish_until array). Default punishment is grim (P = r).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, max(1, math.floor(self.r / 4)))
        self.alpha = max(0.6, 1.0 - 1.0 / max(1e-09, self.k))
        self.P = self.r
        self.Q = 2
        self.L = 1
        self.S_min = math.ceil(self.alpha * max(0, self.n - 1))
        self.m = max(0, self.n - 1)
        self.punish_until = [0] * self.m
        self.coalition_mode = False
        self.coalition_members = set()
        self._last_processed_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        rounds_completed = history.round_number
        t = rounds_completed + 1
        rem = max(0, self.r - t + 1)
        if t > self.r - self.L:
            return Action.D
        if self.m == 0:
            return Action.D
        window = min(self.w, rounds_completed)
        if window > 0:
            recent = history.opponent_actions[-window:, :]
            coop_counts = list(np.sum(recent, axis=0).astype(int))
            last_actions = list(history.opponent_actions[-1, :].astype(bool))
        else:
            coop_counts = [0] * self.m
            last_actions = [False] * self.m
        threshold_window_count = math.ceil(self.alpha * window)
        candidates = {j for j, cnt in enumerate(coop_counts) if cnt >= threshold_window_count}
        any_punished = any((self.punish_until[j] >= t for j in range(self.m)))
        if self.coalition_mode:
            if not self.coalition_members:
                self.coalition_mode = False
            else:
                if rounds_completed >= 1:
                    for j in list(self.coalition_members):
                        if j < len(last_actions) and (not last_actions[j]):
                            self.punish_until[j] = max(self.punish_until[j], t + self.P - 1)
                    any_punished = any((self.punish_until[j] >= t for j in range(self.m)))
                if self.P < self.r and self.Q > 0 and (rem > self.P) and (rounds_completed >= self.Q):
                    for j in range(self.m):
                        if self.punish_until[j] > 0:
                            last_q = history.opponent_actions[-self.Q:, j]
                            if np.all(last_q):
                                self.punish_until[j] = 0
                    any_punished = any((self.punish_until[j] >= t for j in range(self.m)))
                if any_punished:
                    return Action.D
                threshold_window_count = math.ceil(self.alpha * window)
                candidates = {j for j, cnt in enumerate(coop_counts) if cnt >= threshold_window_count}
                if len(candidates) >= self.S_min and rem > self.P:
                    self.coalition_members = set(candidates)
                    return Action.C
                else:
                    self.coalition_mode = False
                    self.coalition_members = set()
                    return Action.D
        if len(candidates) >= self.S_min and rem > self.P:
            self.coalition_mode = True
            self.coalition_members = set(candidates)
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_83(BaseStrategy):
    """
    Aggressive Free-Rider with Conditional Investment (AFRCI)

    - Defaults to defection.
    - Estimates recent cooperation levels and individual responsiveness.
    - Occasionally makes one-shot cooperative "investments" to seed cooperation
      when opponents look responsive and there are enough rounds left to recoup.
    - Exploits cooperative majorities by defecting while others cooperate.
    - Detects when opponents successfully punish (sharp drop in cooperation after
      you defect) and switches to permanent defection (punishment_mode).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = int(game_description.n_rounds)
        if r <= 6:
            self.w = max(1, min(3, r - 1))
            self.T_min = max(2, math.ceil(r / 4))
        else:
            self.w = max(1, min(5, r - 1))
            self.T_min = 4
        self.alpha = 0.6
        self.theta = 0.5
        self.resp_threshold = 0.25
        self.L = 3
        self.Delta_drop = 0.25
        self.punishment_mode = False
        self.last_coop_round = -10 ** 6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            return Action.D
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        t = history.round_number + 1
        t_rem = r - t + 1
        if not self.punishment_mode and history.round_number >= 2:
            my_last = bool(history.my_actions[-1])
            if not my_last:
                opp_rate_prev = float(np.mean(history.opponent_actions[-2, :]))
                opp_rate_after = float(np.mean(history.opponent_actions[-1, :]))
                if opp_rate_prev - opp_rate_after > self.Delta_drop:
                    self.punishment_mode = True
        if self.punishment_mode:
            return Action.D
        if t == r:
            return Action.D
        if t_rem < self.T_min:
            return Action.D
        if t - 1 < self.w:
            return Action.D
        w = int(self.w)
        opp_actions_window = np.array(history.opponent_actions[-w:, :], dtype=np.bool_)
        my_actions_window = np.array(history.my_actions[-w:], dtype=np.bool_)
        p_j = np.mean(opp_actions_window.astype(float), axis=0)
        p_group_recent = float(np.mean(p_j)) if p_j.size > 0 else 0.0
        num_opponents = n - 1
        r_j_list = np.zeros(num_opponents, dtype=float)
        for j in range(num_opponents):
            coop_when_major_true = 0
            total_true = 0
            coop_when_major_false = 0
            total_false = 0
            for s_local in range(1, w):
                opp_action_s = bool(opp_actions_window[s_local, j])
                prev_my_action = bool(my_actions_window[s_local - 1])
                prev_opp_row = opp_actions_window[s_local - 1, :]
                if num_opponents > 0:
                    if j == 0:
                        others_excl_j = np.concatenate(([prev_my_action], prev_opp_row[1:]))
                    elif j == num_opponents - 1:
                        others_excl_j = np.concatenate(([prev_my_action], prev_opp_row[:j]))
                    else:
                        others_excl_j = np.concatenate(([prev_my_action], prev_opp_row[:j], prev_opp_row[j + 1:]))
                else:
                    others_excl_j = np.array([prev_my_action], dtype=bool)
                coop_count = int(np.sum(others_excl_j.astype(int)))
                majority_threshold = (n - 1) / 2.0
                majority_prev = coop_count > majority_threshold
                if majority_prev:
                    total_true += 1
                    if opp_action_s:
                        coop_when_major_true += 1
                else:
                    total_false += 1
                    if opp_action_s:
                        coop_when_major_false += 1
            if total_true >= 1 and total_false >= 1:
                p_true = coop_when_major_true / total_true
                p_false = coop_when_major_false / total_false
                r_j_list[j] = p_true - p_false
            else:
                r_j_list[j] = 0.0
        if r_j_list.size > 0:
            R_frac = float(np.mean(r_j_list >= self.resp_threshold))
        else:
            R_frac = 0.0
        prev_opp_row = np.array(history.opponent_actions[-1, :], dtype=np.bool_)
        prev_my_action = bool(history.my_actions[-1])
        coop_count_others = int(np.sum(prev_opp_row.astype(int)))
        majority_prev = coop_count_others > (n - 1) / 2.0
        if p_group_recent >= self.alpha and R_frac >= self.theta and (t_rem >= self.T_min):
            if majority_prev:
                action = Action.D
            elif t - self.last_coop_round > self.L:
                action = Action.C
                self.last_coop_round = t
            else:
                action = Action.D
        else:
            action = Action.D
        return action

class Strategy_AGGRESSIVE_84(BaseStrategy):
    """
    Predatory Opportunistic Defector (POD)

    Aggressive adaptive strategy for the public goods game.
    Defaults to defection, may perform a small cooperative probe to recruit
    partners when the horizon justifies it, exploits persistent cooperators,
    and switches to permanent defection (grim) if a recruitment fails or a
    partnership breaks.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.W_static = None
        self.theta_exploit = 0.6
        self.theta_sustain = 0.5
        self.delta_min = 0.15
        self.S_build = 1
        self.L_max = 1
        self.S_hold = 1
        self.H_harvest = 1
        self.mode = 'DEFAULT'
        self.attempts_made = 0
        self.recruit_build_counter = 0
        self.recruit_phase = None
        self.recruit_pre_G = 0.0
        self.recruit_start_round = None
        self.coop_hold_counter = 0
        self.harvest_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        W = min(10, max(3, math.floor(self.r / 10)))
        self.W_static = W
        denom = self.k - 1.0
        if denom <= 0:
            H_recoup = self.r + 1
        else:
            H_recoup_real = (1.0 - self.k / float(self.n)) / denom
            H_recoup = int(math.ceil(H_recoup_real)) + 1
            if H_recoup < 0:
                H_recoup = 0
        if history is None:
            self.mode = 'DEFAULT'
            self.attempts_made = 0
            self.recruit_build_counter = 0
            self.recruit_phase = None
            self.recruit_pre_G = 0.0
            self.recruit_start_round = None
            self.coop_hold_counter = 0
            self.harvest_remaining = 0
            return Action.D
        completed = int(history.round_number)
        opp_actions = history.opponent_actions
        if completed == 0:
            G = 0.0
            G_total = 0.0
            G_prev = 0.0
            deltaG = 0.0
            per_player_freq = np.zeros((self.n - 1,)) if self.n - 1 > 0 else np.array([])
        else:
            win = min(W, completed)
            recent_slice = opp_actions[-win:, :]
            G = float(np.mean(recent_slice.astype(float))) if recent_slice.size > 0 else 0.0
            G_total = float(np.mean(opp_actions.astype(float))) if opp_actions.size > 0 else 0.0
            if completed >= 2 * win:
                prev_slice = opp_actions[-2 * win:-win, :]
                G_prev = float(np.mean(prev_slice.astype(float))) if prev_slice.size > 0 else G
            else:
                G_prev = G
            deltaG = G - G_prev
            if opp_actions.size > 0:
                per_player_freq = np.mean(opp_actions.astype(float), axis=0)
            else:
                per_player_freq = np.zeros((self.n - 1,))
        remaining = self.r - t + 1
        if t == self.r:
            return Action.D
        if remaining <= H_recoup:
            if self.mode == 'COOP_PARTNERSHIP':
                self.mode = 'GRIM'
            return Action.D
        if self.mode == 'GRIM':
            return Action.D
        if self.mode == 'RECRUIT_BUILD':
            if self.recruit_phase is None:
                self.recruit_phase = 'building'
                self.recruit_build_counter = 0
                self.recruit_pre_G = G
                self.recruit_start_round = t
            if self.recruit_phase == 'building':
                self.recruit_build_counter += 1
                if self.recruit_build_counter < self.S_build:
                    return Action.C
                else:
                    self.recruit_phase = 'waiting_eval'
                    return Action.C
            if self.recruit_phase == 'waiting_eval':
                rounds_since_build = completed - (self.recruit_start_round - 1)
                post_build_observed = max(0, rounds_since_build - self.S_build)
                if post_build_observed <= 0:
                    return Action.D
                else:
                    eval_win = min(W, post_build_observed)
                    eval_start_idx = completed - post_build_observed
                    eval_slice = opp_actions[eval_start_idx:eval_start_idx + eval_win, :]
                    R = float(np.mean(eval_slice.astype(float))) if eval_slice.size > 0 else 0.0
                    if R - self.recruit_pre_G >= self.delta_min and R >= self.theta_sustain:
                        self.mode = 'COOP_PARTNERSHIP'
                        self.recruit_phase = None
                        self.recruit_build_counter = 0
                        self.recruit_start_round = None
                        self.coop_hold_counter = 0
                        self.harvest_remaining = 0
                        return Action.C
                    else:
                        self.attempts_made += 1
                        self.mode = 'GRIM'
                        self.recruit_phase = None
                        self.recruit_build_counter = 0
                        self.recruit_start_round = None
                        return Action.D
        if self.mode == 'COOP_PARTNERSHIP':
            if G < self.theta_sustain or deltaG <= -self.delta_min:
                self.mode = 'GRIM'
                return Action.D
            if self.harvest_remaining > 0:
                self.harvest_remaining -= 1
                if self.harvest_remaining == 0:
                    if G < self.theta_sustain:
                        self.mode = 'GRIM'
                        return Action.D
                return Action.D
            self.coop_hold_counter += 1
            if self.coop_hold_counter >= self.S_hold:
                self.harvest_remaining = self.H_harvest
                self.coop_hold_counter = 0
                self.harvest_remaining -= 1
                return Action.D
            return Action.C
        if self.mode == 'EXPLOIT':
            if G < self.theta_exploit - 0.1 or G < 0.05:
                self.mode = 'DEFAULT'
                return Action.D
            else:
                return Action.D
        if G >= self.theta_exploit:
            self.mode = 'EXPLOIT'
            return Action.D
        if self.attempts_made < self.L_max and remaining > H_recoup:
            high_personal = False
            if per_player_freq.size > 0:
                if np.max(per_player_freq) >= 0.7:
                    high_personal = True
            pocket_condition = G_total >= 0.4 and deltaG < 0
            if high_personal or pocket_condition:
                self.mode = 'RECRUIT_BUILD'
                self.recruit_phase = None
                self.recruit_build_counter = 0
                self.recruit_start_round = t
                self.recruit_phase = 'building'
                self.recruit_build_counter = 1
                self.recruit_pre_G = G
                if self.S_build <= 1:
                    self.recruit_phase = 'waiting_eval'
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_85(BaseStrategy):
    """
    Aggressive Opportunistic Exploiter (AOE)

    - Default: defect.
    - Occasionally probe early by cooperating to detect if opponents increase cooperation afterwards.
    - If a probe produces a sufficiently large positive response (delta_detect), commit to a
      seed phase (cooperate for a block) followed by a short harvest phase (defect) near the end.
    - Always defect on the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploit_flag = False
        self.seed_target_end = None
        self.harvest_start = None
        self.probes_done = 0
        self.baseline_others_rate = 0.0
        self.pending_probe = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        t = int(state.round_number)
        W = min(5, max(1, r - 1))
        M = min(2, max(1, r - 1))
        p_probe = min(0.15, 1.0 / (2.0 * n)) if n > 0 else 0.0
        delta_detect = 0.2
        H = max(1, math.ceil(0.15 * r))
        seed_min = min(max(2, math.ceil(0.3 * r)), max(0, r - H - 1))
        if r == 2:
            if t == r:
                return Action.D
            return Action.D
        if t == r:
            return Action.D
        rounds_completed = 0 if history is None else history.round_number

        def compute_recent_others_rate(use_history: PlayerHistory | None, upto_rounds: int=None):
            if use_history is None:
                return 0.0
            total_rounds = use_history.round_number if upto_rounds is None else min(upto_rounds, use_history.round_number)
            if total_rounds <= 0:
                return 0.0
            start = max(0, total_rounds - W)
            opp_slice = use_history.opponent_actions[start:total_rounds, :]
            if opp_slice.size == 0:
                return 0.0
            per_opponent = np.mean(opp_slice.astype(float), axis=0)
            return float(np.mean(per_opponent))
        if history is not None:
            others_rate = compute_recent_others_rate(history)
            if t <= W + 1 and self.baseline_others_rate == 0.0:
                self.baseline_others_rate = others_rate
        if self.pending_probe is not None and history is not None:
            probe_round = self.pending_probe.get('probe_round', None)
            if probe_round is not None and history.round_number >= probe_round + M:
                baseline_start = max(0, probe_round - 1 - W)
                baseline_end = max(0, probe_round - 1)
                baseline_rate_others = 0.0
                if baseline_end > baseline_start:
                    opp_slice = history.opponent_actions[baseline_start:baseline_end, :]
                    if opp_slice.size > 0:
                        per_opponent = np.mean(opp_slice.astype(float), axis=0)
                        baseline_rate_others = float(np.mean(per_opponent))
                else:
                    baseline_rate_others = float(self.baseline_others_rate)
                post_start = probe_round
                post_end = probe_round + M
                post_rate_others = 0.0
                if post_end <= history.round_number:
                    opp_slice_post = history.opponent_actions[post_start:post_end, :]
                    if opp_slice_post.size > 0:
                        per_opponent_post = np.mean(opp_slice_post.astype(float), axis=0)
                        post_rate_others = float(np.mean(per_opponent_post))
                if post_rate_others - baseline_rate_others >= delta_detect:
                    planned_harvest_start = max(probe_round + seed_min, r - H)
                    planned_seed_target_end = planned_harvest_start - 1
                    if planned_seed_target_end < r and planned_harvest_start < r:
                        self.exploit_flag = True
                        self.harvest_start = int(planned_harvest_start)
                        self.seed_target_end = int(planned_seed_target_end)
                        self.pending_probe = None
                    else:
                        self.pending_probe = None
                else:
                    self.pending_probe = None
        if self.exploit_flag:
            if self.harvest_start is None:
                return Action.D
            if t < self.harvest_start:
                return Action.C
            else:
                return Action.D
        max_probes = max(1, math.floor(0.2 * r))
        if r == 2:
            max_probes = 0
        allow_probe_this_round = True
        if self.probes_done >= max_probes:
            allow_probe_this_round = False
        if t + M > r:
            allow_probe_this_round = False
        if t > r - 2:
            allow_probe_this_round = False
        if r == 3 and t != 1:
            allow_probe_this_round = False
        if self.pending_probe is not None:
            allow_probe_this_round = False
        if allow_probe_this_round and random.random() < p_probe:
            self.probes_done += 1
            probe_round = t
            self.pending_probe = {'probe_round': probe_round}
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_86(BaseStrategy):
    """
    Aggressive Conditional Cooperator with Escalating Punishment (ACCEP).

    - Starts defecting.
    - Cooperates only when recent cooperation among others exceeds a threshold
      (threshold lowered modestly when k is larger).
    - If exploited (I cooperated while at least one other defected) in any newly
      observed recent rounds, begins an escalating deterministic punishment of D
      for multiple rounds.
    - Opportunistically defects for exactly one round if recent cooperation among
      others is extremely high (>= 0.90) to harvest a free-ride.
    - Always defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.L = min(5, max(0, self.r - 1))
        threshold_base = 0.5
        threshold_k_adjust = (self.k - 1.0) / max(1.0, self.n - 1)
        raw_threshold = threshold_base - 0.15 * threshold_k_adjust
        self.threshold = max(0.25, min(0.75, raw_threshold))
        self.P_min = 2
        self.P_cap = int(max(3, math.floor(self.r / 4.0)))
        self.punish_remaining = 0
        self.punish_level = 0
        self.last_rnd_my_action = None
        self._processed_up_to = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            self.last_rnd_my_action = Action.D
            return Action.D
        completed = int(history.round_number)
        if self._processed_up_to < completed:
            start_idx = self._processed_up_to
            end_idx = completed
            for idx in range(start_idx, end_idx):
                my_coop = bool(history.my_actions[idx])
                opp_coops = history.opponent_actions[idx, :]
                if my_coop and (not np.all(opp_coops)):
                    self.punish_level += 1
                    new_length = max(self.P_min, 2 * self.punish_level)
                    self.punish_remaining = min(self.P_cap, new_length)
            self._processed_up_to = completed
        if t >= self.r:
            self.last_rnd_my_action = Action.D
            return Action.D
        if self.punish_remaining > 0:
            action = Action.D
            self.punish_remaining = max(0, self.punish_remaining - 1)
            self.last_rnd_my_action = action
            return action
        L_eff = min(self.L, completed)
        if L_eff == 0:
            self.last_rnd_my_action = Action.D
            return Action.D
        start_idx = completed - L_eff
        total_coop_count = int(np.sum(history.opponent_actions[start_idx:completed, :]))
        denom = float(L_eff * (self.n - 1))
        f_L = total_coop_count / denom if denom > 0.0 else 0.0
        coop_last = int(np.sum(history.opponent_actions[completed - 1, :])) if completed >= 1 else 0
        f_last = coop_last / float(self.n - 1) if self.n - 1 > 0 else 0.0
        if f_L >= 0.9:
            action = Action.D
            self.last_rnd_my_action = action
            return action
        if f_L >= self.threshold:
            action = Action.C
        else:
            action = Action.D
        self.last_rnd_my_action = action
        return action

class Strategy_AGGRESSIVE_87(BaseStrategy):
    """
    Aggressive Adaptive Defector (AAD)

    - Default bias to defect with a small aggression bias epsilon.
    - Estimates opponents' cooperation frequencies and responsiveness to my actions
      over a recent window to decide whether defecting now (plus limited future
      punishment) yields higher expected value than cooperating.
    - Detects punishers and, if just punished, issues a short harsh punishment
      streak (up to 3 rounds) before returning to normal adaptive behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_timer = 0
        self.epsilon = 0.1
        self.delta_punisher = -0.15
        self.p_punisher = 0.3
        self.delta_conditional = 0.15
        self.unconditional_cooperator_p = 0.9
        self.defector_p = 0.1
        self.max_punish_len = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        t = int(state.round_number)
        if history is None or (hasattr(history, 'round_number') and history.round_number == 0) or t == 1:
            return Action.D
        if t >= r:
            return Action.D
        if getattr(self, 'punish_timer', 0) > 0:
            self.punish_timer = max(0, self.punish_timer - 1)
            return Action.D
        completed_rounds = history.round_number
        L = min(8, max(0, completed_rounds))
        if L == 0:
            return Action.D
        opp_actions = np.asarray(history.opponent_actions)
        my_actions = np.asarray(history.my_actions)
        opp_window = opp_actions[-L:, :]
        my_window = my_actions[-L:]
        n_opponents = opp_window.shape[1]
        p_j = np.zeros(n_opponents, dtype=float)
        delta_j = np.zeros(n_opponents, dtype=float)
        s_j = np.zeros(n_opponents, dtype=float)
        counts = np.sum(opp_window.astype(int), axis=0)
        p_j = (counts + 1.0) / (L + 2.0)
        mask_myC = my_window == True
        mask_myD = my_window == False
        nC = int(np.sum(mask_myC))
        nD = int(np.sum(mask_myD))
        for j in range(n_opponents):
            p_given_myC = None
            p_given_myD = None
            if nC > 0:
                count_c_given_myC = int(np.sum(opp_window[mask_myC, j].astype(int)))
                p_given_myC = (count_c_given_myC + 1.0) / (nC + 2.0)
            if nD > 0:
                count_c_given_myD = int(np.sum(opp_window[mask_myD, j].astype(int)))
                p_given_myD = (count_c_given_myD + 1.0) / (nD + 2.0)
            if p_given_myC is None or p_given_myD is None:
                delta_j[j] = 0.0
            else:
                delta_j[j] = float(p_given_myC - p_given_myD)
            s_j[j] = max(0.0, -delta_j[j])
        M_est_excl = float(np.sum(p_j))
        last_my_action = bool(history.my_actions[-1])
        M_if_I_defect = 0.0
        for j in range(n_opponents):
            used = None
            if last_my_action and nC > 0:
                used = p_j[j] + 0.5 * delta_j[j]
            elif not last_my_action and nD > 0:
                used = p_j[j] - 0.5 * delta_j[j]
            if used is None:
                used = p_j[j]
            used = min(1.0, max(0.0, used))
            M_if_I_defect += used
        positive_delta_sum = float(np.sum(np.maximum(0.0, delta_j)))
        M_if_I_cooperate = M_est_excl + positive_delta_sum
        pi_D = 1.0 + k / n * M_if_I_defect
        pi_C = k / n * (M_if_I_cooperate + 1.0)
        Rem = max(0, r - t)
        L_future = k / n * Rem * float(np.sum(s_j))
        left = pi_D + L_future + self.epsilon
        right = pi_C
        play_defect = left >= right
        if play_defect:
            punisher_flags = (delta_j <= self.delta_punisher) & (p_j >= self.p_punisher)
            any_punisher = bool(np.any(punisher_flags))
            was_just_punished = False
            if completed_rounds >= 2:
                my_last = bool(history.my_actions[-1])
                if my_last == False:
                    opp_last = int(np.sum(history.opponent_actions[-1, :].astype(int)))
                    opp_prev = int(np.sum(history.opponent_actions[-2, :].astype(int)))
                    my_prev = bool(history.my_actions[-2])
                    coop_last_total = opp_last + (1 if not my_last and False else 1 if my_last else 0)
                    coop_drop = opp_prev - opp_last
                    thresh = max(1, int(math.ceil(0.2 * n)))
                    if coop_drop >= thresh:
                        was_just_punished = True
            if any_punisher and was_just_punished and (Rem > 0):
                self.punish_timer = min(self.max_punish_len, Rem)
                return Action.D
            return Action.D
        else:
            return Action.C

class Strategy_AGGRESSIVE_88(BaseStrategy):
    """
    Aggressive Opportunist (AggroOp)

    Default: defect. Occasionally spend one round cooperating ("lure") if recent
    history shows a high fraction of other players cooperating and there are
    enough rounds left to exploit. If the lure succeeds (many others cooperated
    in that lure round), run a short deterministic exploitation window of
    consecutive defections to extract payoff. Always defect in the final S
    rounds and give up on lures after two failed lures.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.W = min(5, max(1, r // 10 + 1))
        self.E = min(3, max(1, r // 6))
        self.S = 1
        self.p_high = 0.6
        self.Rmin = self.E + 1 + self.S
        self.last_lure_round = None
        self.exploit_timer = 0
        self.failed_lure_count = 0
        self.disable_future_lures = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if r <= 3:
            return Action.D
        if history is not None and self.last_lure_round is not None:
            L = self.last_lure_round
            if history.round_number >= L and self.exploit_timer == 0:
                idx = L - 1
                if 0 <= idx < history.opponent_actions.shape[0]:
                    coop_others_in_L = float(history.opponent_actions[idx, :].sum()) / max(1, n - 1)
                else:
                    coop_others_in_L = 0.0
                if coop_others_in_L >= self.p_high:
                    self.exploit_timer = self.E
                    self.last_lure_round = None
                else:
                    self.failed_lure_count += 1
                    self.last_lure_round = None
                    if self.failed_lure_count >= 2:
                        self.disable_future_lures = True
        if t > r - self.S:
            return Action.D
        if self.exploit_timer > 0:
            self.exploit_timer -= 1
            return Action.D
        if history is None:
            return Action.D
        if history.round_number == 0:
            coop_others_last = 0.0
        else:
            coop_others_last = float(history.opponent_actions[-1, :].sum()) / max(1, n - 1)
        rounds_available = history.round_number
        if rounds_available == 0:
            coop_others_window = 0.0
        else:
            window_size = min(self.W, rounds_available)
            recent = history.opponent_actions[-window_size:, :]
            per_round_fractions = (recent.sum(axis=1) / max(1, n - 1)).astype(float)
            coop_others_window = float(np.mean(per_round_fractions))
        rounds_remaining = r - t + 1
        can_lure_now = rounds_remaining >= self.Rmin and (not self.disable_future_lures) and (coop_others_last >= self.p_high or coop_others_window >= self.p_high)
        if can_lure_now:
            self.last_lure_round = t
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_89(BaseStrategy):
    """
    Aggressive Coercive Trigger (ACT)

    Starts by defecting (aggressive). If opponents show reliably high cooperation
    over a recent window, attempt to build cooperation for S rounds. After a build
    block, perform a single-round exploit (defect). If opponents retaliate (their
    cooperation fraction drops by >= delta between adjacent windows), impose a
    long deterministic punishment P (defect for P rounds). After punishment,
    forgive for F rounds (cooperate) and then probe whether opponents are again
    reliably cooperative. Always defect in the final round or when too few rounds
    remain for punishment to be effective.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = int(game_description.n_rounds)
        self.w = max(1, math.floor(r / 8))
        self.S = max(1, math.ceil(r / 10))
        self.P = max(2, math.floor(r / 4))
        self.F = max(1, math.floor(r / 10))
        self.T = 0.75
        self.delta = 0.2
        self.mode = 'DEFECT_MODE'
        self.coop_counter = 0
        self.punish_counter = 0
        self.forgive_counter = 0
        self.pending_forgive_eval = False

    def __call__(self, state: GameState, history: None | PlayerHistory):

        def opp_coop_fraction(start_round: int, end_round: int) -> float | None:
            if history is None:
                return None
            total_completed = history.round_number
            if start_round < 1 or end_round < start_round or end_round > total_completed:
                return None
            s_idx = start_round - 1
            e_idx = end_round
            coop_count = int(np.sum(history.opponent_actions[s_idx:e_idx, :]))
            denom = (self.game_description.n_players - 1) * (end_round - start_round + 1)
            if denom <= 0:
                return None
            return coop_count / float(denom)
        t = int(state.round_number)
        r = int(self.game_description.n_rounds)
        remaining = r - t + 1
        if t == r:
            return Action.D
        if remaining <= self.P:
            return Action.D
        if self.punish_counter > 0:
            action = Action.D
            self.punish_counter -= 1
            if self.punish_counter == 0:
                self.mode = 'FORGIVE_MODE'
                self.forgive_counter = self.F
                self.pending_forgive_eval = False
            return action
        if self.pending_forgive_eval and history is not None:
            start_w = max(1, t - self.w)
            end_w = t - 1
            p = opp_coop_fraction(start_w, end_w)
            if p is None:
                self.mode = 'DEFECT_MODE'
            elif p >= self.T:
                self.mode = 'COOP_MODE'
                self.coop_counter = self.S
            else:
                self.mode = 'DEFECT_MODE'
            self.pending_forgive_eval = False
        if self.mode in ('COOP_MODE', 'DEFECT_MODE') and history is not None and (history.round_number >= self.w + 1):
            pre_start = t - 1 - self.w
            pre_end = t - 2
            post_start = t - self.w
            post_end = t - 1
            pre_val = opp_coop_fraction(pre_start, pre_end)
            post_val = opp_coop_fraction(post_start, post_end)
            if pre_val is not None and post_val is not None and (pre_val - post_val >= self.delta):
                punish_len = min(self.P, max(0, remaining - 1))
                if punish_len > 0:
                    self.mode = 'PUNISH_MODE'
                    self.punish_counter = int(punish_len)
                    self.punish_counter -= 1
                    if self.punish_counter == 0:
                        self.mode = 'FORGIVE_MODE'
                        self.forgive_counter = self.F
                        self.pending_forgive_eval = False
                    return Action.D
        if self.mode == 'FORGIVE_MODE':
            action = Action.C
            self.forgive_counter -= 1
            if self.forgive_counter <= 0:
                self.pending_forgive_eval = True
            return action
        if self.mode == 'DEFECT_MODE':
            if history is not None and history.round_number >= self.w:
                start_w = t - self.w
                end_w = t - 1
                p = opp_coop_fraction(start_w, end_w)
                if p is not None and p >= self.T and (remaining > self.S + self.P):
                    self.mode = 'COOP_MODE'
                    self.coop_counter = self.S
                    self.coop_counter -= 1
                    return Action.C
            return Action.D
        if self.mode == 'COOP_MODE':
            if self.coop_counter > 0:
                self.coop_counter -= 1
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_90(BaseStrategy):
    """
    Calculated Aggressor (CalcAgg)

    Aggressive, payoff-maximizing repeated-player strategy for the public goods game.
    Defaults to defection, does a single cheap probe early in suitable games, estimates
    how others respond to my cooperation, and only seeds cooperation when a simple
    expected-value test (using k, n, and remaining rounds) indicates a net benefit.
    Exploits high-baseline cooperative groups by defecting, monitors for collapse, and
    retreats to permanent defection if cooperation falls sharply.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.H_min = 3
        self.probe_k_over_n_threshold = 0.5
        self.exploit_baseline_threshold = 0.6
        self.collapse_threshold = 0.2
        self.heuristic_k_over_n_threshold = 0.75
        self.heuristic_min_remaining_rounds = 3
        self.coop_probe_done = False
        self.mode = 'undecided'
        self.exploit_start_baseline = None
        self.last_baseline = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        if history is None:
            t = 1
        else:
            try:
                t = int(state.round_number)
            except Exception:
                t = history.round_number + 1
        if t >= r:
            return Action.D
        if r <= 3:
            self.coop_probe_done = True
            return Action.D
        if history is None or history.round_number == 0:
            if not self.coop_probe_done and t == 1 and (r >= 4) and (k / n >= self.probe_k_over_n_threshold):
                self.coop_probe_done = True
                return Action.C
            else:
                return Action.D
        n_others = n - 1
        rounds_completed = history.round_number
        if rounds_completed == 0 or history.opponent_actions.size == 0:
            return Action.D
        opp_actions = np.asarray(history.opponent_actions, dtype=np.bool_)
        m_others_per_round = np.sum(opp_actions, axis=1).astype(float)
        baseline_group_coop = float(np.mean(m_others_per_round / float(max(1, n_others))))
        my_actions = np.asarray(history.my_actions, dtype=np.bool_)
        coop_round_indices = list(np.nonzero(my_actions)[0])
        others_after_mycoop = []
        for s in coop_round_indices:
            next_idx = s + 1
            if next_idx < rounds_completed:
                others_after_mycoop.append(float(m_others_per_round[next_idx]) / float(max(1, n_others)))
        if len(others_after_mycoop) >= self.H_min:
            mean_others_after_mycoop = float(np.mean(others_after_mycoop))
            response_to_my_coop = mean_others_after_mycoop - baseline_group_coop
            response_known = True
        else:
            response_to_my_coop = None
            response_known = False
        remaining_rounds_excluding_current = r - t
        if self.mode == 'retreat':
            return Action.D
        if self.mode == 'exploit' and self.exploit_start_baseline is not None:
            if self.exploit_start_baseline - baseline_group_coop > self.collapse_threshold:
                self.mode = 'retreat'
                return Action.D
        immediate_cost = 1.0 - k / float(n)
        if baseline_group_coop >= self.exploit_baseline_threshold:
            if self.mode != 'exploit':
                self.mode = 'exploit'
                self.exploit_start_baseline = baseline_group_coop
            return Action.D
        if response_known:
            expected_extra_others_per_round = max(0.0, response_to_my_coop) * float(n_others)
            expected_future_benefit = expected_extra_others_per_round * (k / float(n)) * float(remaining_rounds_excluding_current)
            if expected_future_benefit > immediate_cost:
                self.mode = 'seed'
                return Action.C
            else:
                return Action.D
        if not response_known:
            if k / float(n) >= self.heuristic_k_over_n_threshold and remaining_rounds_excluding_current >= self.heuristic_min_remaining_rounds:
                self.coop_probe_done = True
                self.mode = 'seed'
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_91(BaseStrategy):
    """
    Aggressive Exploiter with Selective Punishment (AESP)

    - Probe first round with C.
    - Default to defect.
    - Detect high recent cooperation among others and exploit in short bursts.
    - Attempt targeted coalition probes when some opponents are reliably cooperative.
    - If cooperated and someone defected that round (I was exploited), escalate punishment
      (defect for an increasing number of rounds).
    - Always defect in final rounds where future punishment is infeasible.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.T_det = min(5, max(1, math.floor(self.r / 6)))
        self.p_high = 0.6
        self.p_low = 0.25
        self.exploit_burst_max = min(4, max(1, math.floor(self.r / 10)))
        self.base_punish_len = 2
        self.final_phase_start = max(self.r - 3, 1)
        self.exploit_mode_counter = 0
        self.punish_counter = 0
        self.times_exploited = 0
        self.last_probe_round = 0
        self.last_probe_targets = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t >= self.final_phase_start:
            self.last_probe_targets = None
            self.last_probe_round = 0
            return Action.D
        remaining_inclusive = max(0, self.r - t + 1)
        if history is not None and history.round_number >= 1:
            my_last = bool(history.my_actions[-1])
            opp_last = np.asarray(history.opponent_actions[-1, :], dtype=bool)
            if my_last and (not opp_last.all()):
                self.times_exploited += 1
                self.punish_counter = min(remaining_inclusive, self.base_punish_len + self.times_exploited)
            if self.last_probe_round == t - 1 and my_last:
                if self.last_probe_targets is not None and len(self.last_probe_targets) > 0:
                    all_target_cooperated = True
                    for idx in self.last_probe_targets:
                        if not bool(history.opponent_actions[-1, idx]):
                            all_target_cooperated = False
                            break
                    if all_target_cooperated:
                        self.exploit_mode_counter = min(self.exploit_burst_max, remaining_inclusive)
                        self.last_probe_targets = None
                        self.last_probe_round = 0
                else:
                    T_obs = min(self.T_det, history.round_number)
                    if T_obs > 0:
                        recent = np.sum(history.opponent_actions[-T_obs:, :], axis=0)
                        recent_rates = recent / float(T_obs)
                        if np.any(recent_rates == 1.0):
                            self.exploit_mode_counter = min(self.exploit_burst_max, remaining_inclusive)
                            self.last_probe_targets = None
                            self.last_probe_round = 0
        if self.punish_counter > 0:
            self.punish_counter -= 1
            return Action.D
        if self.exploit_mode_counter > 0:
            self.exploit_mode_counter -= 1
            if history is not None and history.round_number >= 1:
                T_obs_check = min(self.T_det, history.round_number)
                if T_obs_check > 0:
                    recent = np.sum(history.opponent_actions[-T_obs_check:, :], axis=0)
                    group_recent_rate = float(np.mean(recent / float(T_obs_check)))
                    if group_recent_rate < self.p_high:
                        self.exploit_mode_counter = 0
            return Action.D
        if history is None:
            self.last_probe_round = 1
            self.last_probe_targets = None
            return Action.C
        T_obs = min(self.T_det, history.round_number)
        recent_sums = np.sum(history.opponent_actions[-T_obs:, :], axis=0).astype(float)
        recent_rates = recent_sums / float(T_obs)
        group_recent_rate = float(np.mean(recent_rates))
        if group_recent_rate >= self.p_high:
            self.exploit_mode_counter = min(self.exploit_burst_max, remaining_inclusive)
            self.exploit_mode_counter = max(0, self.exploit_mode_counter - 1)
            self.last_probe_targets = None
            self.last_probe_round = 0
            return Action.D
        if group_recent_rate > self.p_low and group_recent_rate < self.p_high:
            reliable_idxs = [int(i) for i, rate in enumerate(recent_rates) if float(rate) == 1.0]
            if len(reliable_idxs) >= 1:
                self.last_probe_round = t
                self.last_probe_targets = reliable_idxs
                return Action.C
            else:
                return Action.D
        if group_recent_rate <= self.p_low:
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_92(BaseStrategy):
    """
    Exploitative Conditional Punisher (ECP)

    Aggressive, exploitative strategy for the N-player public goods game.
    - Defaults to defection.
    - Exploits cooperative opponents by mostly defecting, but occasionally
      cooperates (maintenance) to sustain exploitable cooperation.
    - Detects large cooperation drops ("shocks") and low-cooperation regimes,
      and responds with a long punishment phase of defecting rounds.
    - Adds small randomized probes and maintenance moves to avoid pure
      determinism (eps jitter).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = int(game_description.n_players)
        r = int(game_description.n_rounds)
        k = float(game_description.k)
        self.n = n
        self.r = r
        self.k = k
        self.W = min(10, max(1, r - 1))

        def clamp(x, a, b):
            return max(a, min(b, x))
        if n - 1 > 0:
            frac = (k - 1.0) / float(max(1, n - 1))
        else:
            frac = 0.0
        self.target_high = clamp(0.55 + 0.25 * frac, 0.55, 0.8)
        self.target_low = clamp(0.25 + 0.15 * frac, 0.2, 0.4)
        self.maintain_prob = clamp(0.15 + 0.65 * frac, 0.15, 0.8)
        self.probe_prob = clamp(0.03 + 0.07 * frac, 0.03, 0.1)
        self.punish_len = min(r, max(3, math.ceil(r / 4.0)))
        self.max_defect_gap = max(1, math.ceil(1.0 / max(1e-09, self.maintain_prob)))
        self.eps = 0.02
        self.punish_remaining = 0
        self.consecutive_defects = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            self.consecutive_defects = 1
            return Action.D
        if t == self.r:
            self.consecutive_defects += 1
            return Action.D
        if self.punish_remaining > 0:
            self.punish_remaining -= 1
            self.consecutive_defects += 1
            return Action.D
        rounds_available = int(history.round_number)
        use_W = min(self.W, rounds_available)
        p = 0.0
        avg_coop_last_W = 0.0
        if use_W > 0:
            opp_slice = history.opponent_actions[-use_W:, :]
            my_slice = history.my_actions[-use_W:]
            opp_counts = np.sum(opp_slice, axis=1).astype(float)
            opp_fracs = opp_counts / float(max(1, self.n - 1))
            p = float(np.mean(opp_fracs))
            my_counts = my_slice.astype(float)
            coop_counts = opp_counts + my_counts
            avg_coop_last_W = float(np.mean(coop_counts))
        else:
            p = 0.0
            avg_coop_last_W = 0.0
        shock_threshold = 0.25 * float(self.n)
        if rounds_available >= 1:
            last_opp_count = float(np.sum(history.opponent_actions[-1, :]))
            last_my = float(history.my_actions[-1])
            last_coop_count = last_opp_count + last_my
            if use_W > 0 and last_coop_count < avg_coop_last_W - shock_threshold:
                self.punish_remaining = self.punish_len
                self.punish_remaining -= 1
                self.consecutive_defects += 1
                return Action.D
        if p >= self.target_high:
            if self.consecutive_defects >= self.max_defect_gap:
                self.consecutive_defects = 0
                return Action.C
            else:
                prob = max(0.0, self.maintain_prob - self.eps)
                if random.random() < prob:
                    self.consecutive_defects = 0
                    return Action.C
                else:
                    self.consecutive_defects += 1
                    return Action.D
        elif p <= self.target_low:
            self.punish_remaining = self.punish_len
            self.punish_remaining -= 1
            self.consecutive_defects += 1
            return Action.D
        else:
            prob = max(0.0, self.probe_prob - self.eps)
            if random.random() < prob:
                self.consecutive_defects = 0
                return Action.C
            else:
                self.consecutive_defects += 1
                return Action.D

class Strategy_AGGRESSIVE_93(BaseStrategy):
    """
    Aggressive Exploitative Defector (AED)

    Summary:
    - Default: defect every round.
    - Never cooperate in the last round.
    - Use a sliding window of recent rounds to detect:
        * Exploitable opponents (frequent cooperators) and exploit them by defecting.
        * Retaliators (those who sharply reduce cooperation after defections) and avoid appeasing them.
    - Optional: very low probability probes to test for mutual cooperation; disabled by default (p_probe=0).
    - Minimal internal state to support multi-round probes when enabled.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.P_exploit = 0.75
        self.G_exploit = 0.6
        self.R_retaliate = 0.25
        self.retaliation_fraction_threshold = 0.4
        self.p_probe = 0.0
        self.F = 3
        self.W_default = min(10, max(1, self.game_description.n_rounds - 1))
        self.probe_active = False
        self.probe_remaining = 0
        self.probe_start_completed = None

    def __call__(self, state: GameState, history: None | PlayerHistory):
        if history is None:
            self.probe_active = False
            self.probe_remaining = 0
            self.probe_start_completed = None
            return Action.D
        completed = int(history.round_number)
        upcoming_round = completed + 1
        r = int(self.game_description.n_rounds)
        n = int(self.game_description.n_players)
        num_opponents = max(0, n - 1)
        if upcoming_round >= r:
            self.probe_active = False
            self.probe_remaining = 0
            self.probe_start_completed = None
            return Action.D
        if num_opponents == 0:
            return Action.D
        W = min(self.W_default, max(1, r - 1))
        recent_len = max(0, completed - max(0, completed - W))
        opp_actions = history.opponent_actions
        start_recent = max(0, completed - W)
        if completed > start_recent:
            recent_slice = opp_actions[start_recent:completed, :]
            coop_j_recent = np.array(np.mean(recent_slice.astype(float), axis=0))
        else:
            coop_j_recent = np.zeros((num_opponents,), dtype=float)
        if coop_j_recent.size > 0:
            group_coop = float(np.mean(coop_j_recent))
        else:
            group_coop = 0.0
        exploitable_mask = coop_j_recent >= self.P_exploit
        exploitable_count = int(np.sum(exploitable_mask))
        exploit_rich = False
        if group_coop >= self.G_exploit or exploitable_count >= math.ceil((n - 1) * 0.5):
            exploit_rich = True
        prev_start = max(0, completed - 2 * W)
        prev_end = max(0, completed - W)
        coop_prev = np.zeros((num_opponents,), dtype=float)
        prev_len = 0
        if prev_end > prev_start:
            prev_slice = opp_actions[prev_start:prev_end, :]
            prev_len = prev_slice.shape[0]
            if prev_len > 0:
                coop_prev = np.array(np.mean(prev_slice.astype(float), axis=0))
        retaliation_mask = np.zeros((num_opponents,), dtype=bool)
        if prev_len > 0 and coop_j_recent.size == num_opponents:
            drops = coop_prev - coop_j_recent
            retaliation_mask = drops >= self.R_retaliate
        retaliatory_count = int(np.sum(retaliation_mask))
        retaliatory_group = retaliatory_count >= self.retaliation_fraction_threshold * (n - 1)
        if self.probe_active:
            if upcoming_round >= r:
                self.probe_active = False
                self.probe_remaining = 0
                self.probe_start_completed = None
                return Action.D
            abort_probe = False
            if self.probe_start_completed is not None:
                baseline_end = self.probe_start_completed
                baseline_start = max(0, baseline_end - W)
                if baseline_end > baseline_start:
                    baseline_slice = opp_actions[baseline_start:baseline_end, :]
                    baseline_coop = np.mean(baseline_slice.astype(float), axis=0)
                    drops_from_baseline = baseline_coop - coop_j_recent
                    retaliation_from_baseline = drops_from_baseline >= self.R_retaliate
                    if int(np.sum(retaliation_from_baseline)) >= self.retaliation_fraction_threshold * (n - 1):
                        abort_probe = True
            if not abort_probe:
                if self.probe_start_completed is not None and completed > self.probe_start_completed:
                    my_probe_rounds_slice = slice(self.probe_start_completed, completed)
                    my_probe_payoffs = history.my_payoffs[my_probe_rounds_slice]
                    if my_probe_payoffs.size > 0:
                        if float(np.mean(my_probe_payoffs)) < 1.0:
                            abort_probe = True
            if abort_probe:
                self.probe_active = False
                self.probe_remaining = 0
                self.probe_start_completed = None
                return Action.D
            action = Action.C
            self.probe_remaining -= 1
            if self.probe_remaining <= 0:
                self.probe_active = False
                self.probe_remaining = 0
                self.probe_start_completed = None
            return action
        if exploit_rich:
            return Action.D
        if retaliatory_group:
            return Action.D
        if self.p_probe > 0.0:
            if random.random() < self.p_probe and r - upcoming_round >= 1:
                self.probe_active = True
                self.probe_remaining = max(1, int(self.F))
                self.probe_start_completed = completed
                self.probe_remaining -= 1
                if self.probe_remaining <= 0:
                    self.probe_active = False
                    self.probe_remaining = 0
                    self.probe_start_completed = None
                return Action.C
        return Action.D

class Strategy_AGGRESSIVE_94(BaseStrategy):
    """
    Aggressive Reciprocal Exploiter (ARE)

    - Probes once (round 1) by cooperating.
    - Attempts short recruitment windows when recent cooperation looks promising.
    - Exploits immediately after highly cooperative rounds by defecting.
    - Punishes persistent exploiters for a duration scaled to r.
    - Always defects in the last E rounds (endgame safety).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = game_description.n_rounds
        self.n = game_description.n_players
        self.E = min(2, max(1, r - 1))
        self.P = 1
        self.W_rec = min(5, max(1, math.floor(r / 6)))
        self.theta = 0.6
        self.phi = 0.4
        self.L_pun = min(max(2, math.floor(r / 8)), max(1, r - 1))
        self.alpha = 0.3
        self.W_pers = min(8, max(1, math.floor(r / 4)))
        self.punishment_until_round = 0
        self.last_recruit_start = 0

    def __call__(self, state: GameState, history: None | PlayerHistory):
        r = self.game_description.n_rounds
        n = self.n
        t = state.round_number
        if t > r - self.E:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.C
        R = history.round_number
        if t <= self.punishment_until_round and t <= r:
            return Action.D
        opp_actions = np.array(history.opponent_actions, dtype=np.int32)
        my_actions = np.array(history.my_actions, dtype=np.int32)
        if R > 0:
            opp_sums = np.sum(opp_actions, axis=1) if opp_actions.size else np.zeros(R, dtype=int)
            coop_fracs = (opp_sums + my_actions) / float(n)
        else:
            coop_fracs = np.array([], dtype=float)
        last_coop_frac = float(coop_fracs[-1]) if coop_fracs.size > 0 else 0.0
        if coop_fracs.size > 0:
            recent_window = min(self.W_rec, coop_fracs.size)
            recent_avg = float(np.mean(coop_fracs[-recent_window:]))
        else:
            recent_avg = 0.0
        Wp = min(self.W_pers, R) if R > 0 else 0
        persistent_defectors = set()
        if Wp > 0:
            recent_opp = opp_actions[-Wp:, :]
            coop_counts = np.sum(recent_opp, axis=0)
            coop_rates = coop_counts / float(Wp)
            for j_idx, rate in enumerate(coop_rates):
                if rate < self.alpha:
                    persistent_defectors.add(j_idx)
        else:
            persistent_defectors = set()
        if coop_fracs.size > 0 and len(persistent_defectors) > 0:
            lookback = min(self.W_pers, coop_fracs.size)
            if np.any(coop_fracs[-lookback:] >= self.theta):
                self.punishment_until_round = max(self.punishment_until_round, t + self.L_pun - 1)
        if last_coop_frac >= self.theta:
            if len(persistent_defectors) >= 1:
                self.punishment_until_round = max(self.punishment_until_round, t + self.L_pun - 1)
            return Action.D
        if recent_avg >= self.phi:
            if self.last_recruit_start == 0 or t > self.last_recruit_start + self.W_rec - 1:
                self.last_recruit_start = t
            start_round = self.last_recruit_start
            prev_start_idx = max(0, start_round - 1 - self.W_rec)
            prev_end_idx = max(0, start_round - 1)
            prev_slice = coop_fracs[prev_start_idx:prev_end_idx] if prev_end_idx > prev_start_idx else np.array([])
            cur_start_idx = start_round - 1
            cur_end_idx = R
            cur_slice = coop_fracs[cur_start_idx:cur_end_idx] if cur_end_idx > cur_start_idx else np.array([])
            prev_avg = float(np.mean(prev_slice)) if prev_slice.size > 0 else 0.0
            cur_avg = float(np.mean(cur_slice)) if cur_slice.size > 0 else 0.0
            if cur_slice.size > 0 and cur_avg <= prev_avg:
                self.punishment_until_round = max(self.punishment_until_round, t + self.L_pun - 1)
                return Action.D
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_95(BaseStrategy):
    """
    Aggressive Adaptive Exploiter (AAE)

    - Signals cooperation on the first move.
    - When recent others' cooperation is high (>= theta_high) it defects for one round
      (exploit) then cooperates for a short forgiveness window.
    - When recent cooperation is low (<= theta_low) it enters a punishment block of
      length scaling with the collapse severity (with occasional probabilistic probes).
    - In the middle (NORMAL) it follows last-round majority (reward cooperation), but
      biases to defect when the majority is not cooperating.
    - Always defects in the final endgame_horizon rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.L = min(10, max(1, math.floor(self.r / 5)))
        self.theta_high = 0.8
        self.theta_low = 0.4
        self.exploit_length = 1
        self.forgiveness_length = 2
        self.base_punish_min = 3
        self.probe_prob = 0.1
        self.endgame_horizon = 2
        self.last_exploit_round = -10 ** 9
        self.in_punishment_until = 0
        self.last_forgive_until = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        remaining = self.r - t + 1
        if remaining <= self.endgame_horizon:
            return Action.D
        if history is None:
            return Action.C
        m = min(self.L, t - 1)
        if m <= 0:
            avg_others_coop_rate = 1.0
        else:
            recent_opponents = history.opponent_actions[-m:, :]
            sum_others = float(np.sum(recent_opponents))
            denom = (self.n - 1) * m
            if denom > 0:
                avg_others_coop_rate = sum_others / denom
            else:
                avg_others_coop_rate = 1.0
        if t > 1:
            last_opponents = history.opponent_actions[-1, :]
            last_opponents_count = int(np.sum(last_opponents))
            last_my = 1 if bool(history.my_actions[-1]) else 0
            last_round_count = last_opponents_count + last_my
        else:
            last_round_count = None
        if self.in_punishment_until >= t and avg_others_coop_rate > self.theta_low:
            self.in_punishment_until = 0
        if t <= self.in_punishment_until:
            if random.random() < self.probe_prob:
                return Action.C
            else:
                return Action.D
        if t <= self.last_forgive_until:
            return Action.C
        if avg_others_coop_rate >= self.theta_high:
            self.last_exploit_round = t
            self.last_forgive_until = t + self.forgiveness_length
            return Action.D
        if avg_others_coop_rate <= self.theta_low:
            punish_T = max(self.base_punish_min, math.ceil((1.0 - avg_others_coop_rate) * self.r / 2.0))
            self.in_punishment_until = t + punish_T - 1
            if random.random() < self.probe_prob:
                return Action.C
            else:
                return Action.D
        if last_round_count is None:
            return Action.D
        if last_round_count >= math.ceil(self.n / 2):
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_96(BaseStrategy):
    """
    Aggressive Enforcer (AE)

    Starts by defecting, probes/exploits stable cooperative cores, applies targeted
    punishments to defectors until they rebuild trust, and escalates to group punishments
    (and eventual grim) if the population repeatedly breaks down. Always defects in the
    final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, max(1, math.floor(self.r / 10)))
        coop_frac_raw = 0.5 + 0.4 * (self.k - 1.0) / max(1.0, self.n - 1.0)
        self.coop_frac = max(0.5, min(0.9, coop_frac_raw))
        self.coop_threshold = math.ceil(self.coop_frac * self.n)
        self.individual_repair = max(1, math.ceil(self.r / 10))
        self.group_punish_len_base = max(1, math.ceil(self.n / max(1, round(self.k))))
        self.final_rounds_defect = 1
        self.punished = [False] * max(0, self.n - 1)
        self.exploit_rounds_left = 0
        self.group_punish_rounds_left = 0
        self.group_punish_episodes = 0
        self.grim = False
        self._last_coop_counts = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if history is None:
            self.punished = [False] * max(0, self.n - 1)
            self.exploit_rounds_left = 0
            self.group_punish_rounds_left = 0
            self.group_punish_episodes = 0
            self.grim = False
            self._last_coop_counts = []
            return Action.D
        remaining_rounds_including_current = max(0, self.r - t + 1)
        if remaining_rounds_including_current <= self.final_rounds_defect:
            return Action.D
        rounds_completed = history.round_number
        opp_actions = history.opponent_actions if rounds_completed > 0 else np.zeros((0, max(0, self.n - 1)), dtype=bool)
        my_actions = history.my_actions if rounds_completed > 0 else np.zeros((0,), dtype=bool)
        coop_counts = []
        for ridx in range(rounds_completed):
            opp_coops = int(np.sum(opp_actions[ridx, :])) if self.n - 1 > 0 else 0
            me_coop = int(my_actions[ridx])
            coop_counts.append(opp_coops + me_coop)
        self._last_coop_counts = coop_counts
        computed_punished = [False] * max(0, self.n - 1)
        for j in range(max(0, self.n - 1)):
            consec = 0
            trusted = False
            punished = False
            for ridx in range(rounds_completed):
                a = bool(opp_actions[ridx, j])
                if a:
                    consec += 1
                    if punished and consec >= self.individual_repair:
                        punished = False
                        trusted = True
                    elif not punished and consec >= self.individual_repair:
                        trusted = True
                else:
                    if trusted:
                        punished = True
                    trusted = False
                    consec = 0
            computed_punished[j] = punished
        self.punished = computed_punished
        window = self.w
        start_idx = max(0, rounds_completed - window)
        window_len = rounds_completed - start_idx
        recent_cooperators_count = 0
        if window_len > 0:
            for j in range(max(0, self.n - 1)):
                count_j = int(np.sum(opp_actions[start_idx:rounds_completed, j]))
                if count_j >= math.ceil(window_len / 2):
                    recent_cooperators_count += 1
            my_count = int(np.sum(my_actions[start_idx:rounds_completed])) if window_len > 0 else 0
            if my_count >= math.ceil(window_len / 2):
                recent_cooperators_count += 1
        else:
            recent_cooperators_count = 0
        sudden_mass_defection = False
        if rounds_completed >= 1:
            coop_last = coop_counts[-1]
            coop_prev = coop_counts[-2] if rounds_completed >= 2 else coop_last
            thresh = math.ceil((1.0 - self.k / float(self.n)) * self.n)
            if coop_last <= thresh:
                sudden_mass_defection = True
            if coop_prev - coop_last > self.n // 4:
                sudden_mass_defection = True
        if self.grim:
            return Action.D
        if self.group_punish_rounds_left > 0:
            self.group_punish_rounds_left -= 1
            if self.group_punish_rounds > 0 if False else False:
                pass
            if self.group_punish_rounds_left == 0 and self.group_punish_episodes > 2:
                self.grim = True
            return Action.D
        if any(self.punished):
            return Action.D
        if self.exploit_rounds_left > 0:
            retaliatory = False
            if rounds_completed >= 2:
                coop_last = coop_counts[-1]
                coop_prev = coop_counts[-2]
                if coop_prev - coop_last > self.n // 4:
                    retaliatory = True
            if retaliatory:
                self.exploit_rounds_left = 0
                remaining = max(0, self.r - t)
                P = min(remaining, self.group_punish_len_base * 2)
                if P <= 0:
                    P = 1
                self.group_punish_rounds_left = P
                self.group_punish_episodes += 1
                if self.group_punish_episodes > 2:
                    self.grim = True
                return Action.D
            self.exploit_rounds_left -= 1
            return Action.D
        if self.r <= 3:
            if recent_cooperators_count >= self.n and t < self.r:
                return Action.C
            else:
                return Action.D
        if recent_cooperators_count >= self.coop_threshold:
            E = min(self.group_punish_len_base, max(1, self.r - t))
            self.exploit_rounds_left = E
            self.exploit_rounds_left -= 1
            return Action.D
        if recent_cooperators_count >= self.coop_threshold and (not any(self.punished)):
            return Action.C
        if recent_cooperators_count < self.coop_threshold:
            if sudden_mass_defection:
                remaining = max(0, self.r - t)
                P = min(remaining, self.group_punish_len_base * 2)
                if P <= 0:
                    P = 1
                self.group_punish_rounds_left = P
                self.group_punish_episodes += 1
                if self.group_punish_episodes > 2:
                    self.grim = True
                return Action.D
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_97(BaseStrategy):
    """
    Aggressive Conditional Exploiter (ACE)

    Short description:
    - Default: defect.
    - Cooperate only when recent, widespread cooperation is observed.
    - Occasionally perform a single-round exploit when near-universal cooperation is detected.
    - If exploitation or a cooperation collapse appears (retaliation), punish for multiple rounds.
    - After punishment, perform a short monitoring (forgiveness) phase to see if cooperation returns.
    - Always defect in a short predefined endgame.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.last_exploit_round = -10 ** 9
        self.last_exploit_checked = -10 ** 9
        self.in_punish_until = 0
        self.need_monitor_when_punishment_ends = False
        self.monitor_remaining = 0
        self.group_defection_active = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        L = min(5, max(1, r - 1))
        E = min(3, max(1, math.floor(r / 10)))
        P_max = min(5, r)
        M = max(3, math.floor(r / 10))
        F = 2
        T_coop = 0.7
        T_exploit = 0.9
        T_retaliation = 0.6
        if history is None:
            return Action.D
        completed = int(history.round_number)
        t = completed + 1
        remaining = r - t + 1
        window_size = min(L, completed)
        if window_size <= 0:
            other_coop_rate = 0.0
        else:
            opp = history.opponent_actions[-window_size:, :]
            coop_count = float(np.sum(opp))
            other_coop_rate = coop_count / (window_size * (n - 1))
        group_defect_detected = False
        if window_size > 0:
            opp = history.opponent_actions[-window_size:, :]
            def_counts = window_size - np.sum(opp, axis=0)
            thresh_most = math.ceil(0.6 * window_size)
            players_most_defected = int(np.sum(def_counts >= thresh_most))
            if players_most_defected / max(1, n - 1) >= 0.3:
                group_defect_detected = True
        if self.group_defection_active:
            if other_coop_rate >= T_coop:
                self.group_defection_active = False
            else:
                return Action.D
        if group_defect_detected:
            self.group_defection_active = True
            return Action.D
        if self.last_exploit_round > -10 ** 8 and self.last_exploit_checked < self.last_exploit_round:
            if completed >= self.last_exploit_round:
                if other_coop_rate < T_retaliation:
                    remaining_after_end = remaining - E
                    if remaining_after_end >= 1:
                        P = min(P_max, remaining_after_end)
                        if P >= 1:
                            self.in_punish_until = self.last_exploit_round + P
                            self.need_monitor_when_punishment_ends = True
                self.last_exploit_checked = self.last_exploit_round
        if self.need_monitor_when_punishment_ends and t > self.in_punish_until:
            if remaining > E:
                self.monitor_remaining = F
            else:
                self.monitor_remaining = 0
            self.need_monitor_when_punishment_ends = False
        if remaining <= E:
            return Action.D
        if t <= self.in_punish_until:
            return Action.D
        if self.monitor_remaining > 0:
            if other_coop_rate >= T_coop:
                self.monitor_remaining -= 1
                return Action.C
            else:
                self.monitor_remaining = 0
                return Action.D
        can_exploit = other_coop_rate >= T_exploit and t - self.last_exploit_round >= M and (remaining > E + 1)
        if can_exploit:
            self.last_exploit_round = t
            self.last_exploit_checked = -10 ** 9
            return Action.D
        if other_coop_rate >= T_coop:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_98(BaseStrategy):
    """
    Aggressive opportunistic public-goods strategy with offer, cooperation-with-exploitation,
    and grim punishment. Starts in an OFFER phase (short probing cooperation). If a strong
    signal of reciprocation is observed (fraction of others cooperating ≥ ALPHA over S rounds),
    it enters COOP_EXPLOIT where it mostly cooperates but occasionally defects (stochastic
    exploitation). Any sustained drop in others' cooperation below BETA triggers an immediate
    permanent PUNISH (grim) of defection. Always defects in the final ENDGAME_H rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.S = int(min(3, max(1, math.floor(self.r / 10))))
        self.ENDGAME_H = int(min(2, max(0, self.r - 1)))
        share = self.k / max(1.0, float(self.n))
        if share >= 0.8:
            self.ALPHA = 0.5
        else:
            self.ALPHA = 0.75
        self.BETA = max(0.0, self.ALPHA - 0.1)
        self.exploit_mode = 'stochastic'
        self.p_exploit = 0.12
        self.E = 4
        self.mode = 'OFFER'
        self._rounds_since_exploit = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = getattr(state, 'round_number', 1) or 1
        else:
            t = getattr(state, 'round_number', history.round_number + 1)
        if self.r <= 2:
            return Action.D
        if self.ENDGAME_H > 0 and t > self.r - self.ENDGAME_H:
            self.mode = 'PUNISH'
            return Action.D
        if history is None:
            self.mode = 'OFFER'
            return Action.C
        opp_actions = history.opponent_actions
        completed = history.round_number
        num_opponents = max(1, self.n - 1)

        def fraction_over_rows(rows_slice):
            if rows_slice.size == 0:
                return 0.0
            coop_counts = np.sum(rows_slice, axis=1)
            frac = float(np.mean(coop_counts / float(num_opponents)))
            return frac
        if self.mode == 'OFFER':
            if t <= self.S:
                return Action.C
            else:
                rows_available = min(completed, self.S)
                if rows_available <= 0:
                    mean_fraction = 0.0
                else:
                    mean_fraction = fraction_over_rows(opp_actions[:rows_available, :])
                if mean_fraction >= self.ALPHA:
                    self.mode = 'COOP'
                    self._rounds_since_exploit = 0
                else:
                    self.mode = 'PUNISH'
                if self.mode == 'PUNISH':
                    return Action.D
        if self.mode == 'PUNISH':
            return Action.D
        if self.mode == 'COOP':
            if completed >= 1:
                last_frac = float(np.sum(opp_actions[-1, :]) / float(num_opponents))
                if last_frac < self.BETA:
                    self.mode = 'PUNISH'
                    return Action.D
            window = min(3, completed)
            if window > 0:
                moving_avg = fraction_over_rows(opp_actions[-window:, :])
                if moving_avg < self.BETA:
                    self.mode = 'PUNISH'
                    return Action.D
            action_cooperate = True
            if self.exploit_mode == 'stochastic':
                if random.random() < self.p_exploit:
                    action_cooperate = False
            else:
                self._rounds_since_exploit += 1
                if self._rounds_since_exploit >= self.E:
                    action_cooperate = False
                    self._rounds_since_exploit = 0
            return Action.C if action_cooperate else Action.D
        return Action.D

class Strategy_AGGRESSIVE_99(BaseStrategy):
    """
    Aggressive Opportunistic Exploiter (AOE)

    - Defects by default, probes early, and always defects in endgame.
    - Builds sliding-window stats per-opponent to detect reliably cooperative non-punishers (exploit them).
    - When the group is largely cooperative with few punishers, enters a cooperate-mostly / defect-periodically cycle to siphon extra payoff.
    - If many opponents punish defections and your payoffs fall, performs short appeasements (a few consecutive C) to restore cooperation.
    - Adds tiny randomness to scheduled actions (except endgame) to avoid perfect predictability.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.delta_defect = 1.0 - self.k / float(self.n)
        if self.delta_defect <= 0:
            self.delta_defect = max(1e-06, self.delta_defect)
        self.W = int(min(10, max(3, math.floor(self.r / 4))))
        self.T_probe = int(min(3, math.floor(self.r / 4)))
        self.T_end = int(min(2, self.r))
        self.C_high = 0.75
        self.P_high = 0.45
        self.P_low = 0.2
        self.G_high = 0.65
        self.p_probe = 0.15
        self.epsilon = 0.02
        self.R_frac = 0.4
        self.R_appease = int(min(2, math.floor(self.W / 2))) if self.W >= 2 else 1
        self.M_c = int(min(max(3, math.ceil(1.0 / self.delta_defect)), 6))
        self.mode = 'default'
        self.cycle_pos = 0
        self.appease_remaining = 0
        self.min_cond_obs = 3

    def __call__(self, state: GameState, history: None | PlayerHistory):
        if history is None:
            if random.random() < self.p_probe:
                return Action.C
            return Action.D
        t = history.round_number + 1
        if t > self.r - self.T_end:
            self.mode = 'default'
            self.cycle_pos = 0
            self.appease_remaining = 0
            return Action.D
        if self.appease_remaining > 0:
            self.appease_remaining -= 1
            if self.appease_remaining == 0:
                self.mode = 'default'
                self.cycle_pos = 0
            return Action.C
        if t <= self.T_probe:
            if random.random() < self.p_probe:
                return Action.C
            return Action.D
        total_rounds = history.round_number
        start_idx = max(0, total_rounds - self.W)
        my_window = np.array(history.my_actions[start_idx:total_rounds], dtype=bool)
        opp_window = np.array(history.opponent_actions[start_idx:total_rounds, :], dtype=bool)
        my_payoffs = np.array(history.my_payoffs[start_idx:total_rounds], dtype=float)
        opp_payoffs = np.array(history.opponent_payoffs[start_idx:total_rounds, :], dtype=float)
        window_len = my_window.shape[0]
        n_opponents = opp_window.shape[1] if opp_window.ndim == 2 else 0

        def safe_mean(arr):
            if arr.size == 0:
                return 0.0
            return float(np.mean(arr))
        coop_rate = np.zeros(n_opponents, dtype=float)
        baseline_def = np.zeros(n_opponents, dtype=float)
        punish_sensitivity = np.zeros(n_opponents, dtype=float)
        if window_len > 0 and n_opponents > 0:
            coop_rate = np.array([safe_mean(opp_window[:, j]) for j in range(n_opponents)])
            baseline_def = 1.0 - coop_rate
            if window_len >= 2:
                my_prev = my_window[:-1]
                opp_curr = opp_window[1:, :]
                for j in range(n_opponents):
                    indices = np.where(my_prev == False)[0]
                    denom = indices.size
                    if denom < self.min_cond_obs:
                        punish_sensitivity[j] = self.P_high
                    else:
                        opp_def_after_my_def = np.sum(opp_curr[indices, j] == False)
                        p_def_given_my_def = float(opp_def_after_my_def) / float(denom)
                        punish_sensitivity[j] = max(0.0, p_def_given_my_def - baseline_def[j])
            else:
                punish_sensitivity[:] = self.P_high
        else:
            coop_rate = np.zeros(n_opponents, dtype=float)
            baseline_def = np.ones(n_opponents, dtype=float)
            punish_sensitivity = np.ones(n_opponents, dtype=float) * self.P_high
        global_coop_rate = 0.0
        if window_len > 0 and n_opponents > 0:
            global_coop_rate = float(np.mean(opp_window))
        fraction_punishers = 0.0
        if n_opponents > 0:
            fraction_punishers = float(np.sum(punish_sensitivity >= self.P_high) / float(n_opponents))

        def detect_heavy_retaliation_and_payoff_drop():
            if n_opponents == 0:
                return False
            significant_retaliators = np.sum(punish_sensitivity >= self.P_low)
            retaliator_frac = float(significant_retaliators) / float(n_opponents)
            payout_drop = 0.0
            if total_rounds >= 2 * self.W and self.W >= 1:
                recent = np.array(history.my_payoffs[total_rounds - self.W:total_rounds], dtype=float)
                prev = np.array(history.my_payoffs[total_rounds - 2 * self.W:total_rounds - self.W], dtype=float)
                payout_drop = float(np.mean(prev) - np.mean(recent))
            elif total_rounds >= 2:
                recent_avg = float(np.mean(history.my_payoffs[start_idx:total_rounds]))
                last_payoff = float(history.my_payoffs[-1])
                payout_drop = float(recent_avg - last_payoff)
            else:
                payout_drop = 0.0
            return retaliator_frac >= self.R_frac and payout_drop > self.delta_defect / 2.0
        exploitable_exists = False
        if n_opponents > 0:
            exploitable_mask = (coop_rate >= self.C_high) & (punish_sensitivity <= self.P_low)
            exploitable_exists = bool(np.any(exploitable_mask))
        if exploitable_exists:
            action_bool = False
            if random.random() < self.epsilon:
                action_bool = not action_bool
            return Action.C if action_bool else Action.D
        if global_coop_rate >= self.G_high and fraction_punishers <= 0.25:
            if self.mode != 'cycle':
                self.mode = 'cycle'
                self.cycle_pos = 0
            if detect_heavy_retaliation_and_payoff_drop():
                self.mode = 'appease'
                self.appease_remaining = max(1, self.R_appease)
                return Action.C
            if self.cycle_pos == self.M_c - 1:
                action_bool = False
            else:
                action_bool = True
            self.cycle_pos = (self.cycle_pos + 1) % self.M_c
            if random.random() < self.epsilon:
                action_bool = not action_bool
            return Action.C if action_bool else Action.D
        if detect_heavy_retaliation_and_payoff_drop():
            self.mode = 'appease'
            self.appease_remaining = max(1, self.R_appease)
            return Action.C
        action_bool = False
        if random.random() < self.epsilon:
            action_bool = not action_bool
        return Action.C if action_bool else Action.D

class Strategy_AGGRESSIVE_100(BaseStrategy):
    """
    Adaptive Exploitative Aggressor (AEA)

    Aggressive default: defect. Empirically tests whether cooperating now will
    induce a persistent enough uplift in opponents' cooperation to make the
    immediate cost worth the long-run benefit. Exploits near-unconditional
    cooperators, defects on the last round, and occasionally explores to gather
    responsiveness data.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.window_max = 20
        self.min_samples = 4
        self.gamma_conservatism = 1.5
        self.exploit_threshold = 0.95
        self.epsilon_floor = 0.02
        self._eps = 1e-12

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        rounds_completed = history.round_number
        t = rounds_completed + 1
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        L = r - t + 1
        if L <= 1:
            return Action.D
        W = min(self.window_max, rounds_completed)
        if W <= 0:
            return Action.D
        start_idx = rounds_completed - W
        opp_actions = history.opponent_actions
        try:
            _, opp_count = opp_actions.shape
        except Exception:
            return Action.D
        recent_opp = opp_actions[start_idx:rounds_completed, :]
        recent_opp_float = recent_opp.astype(float)
        p_js = np.mean(recent_opp_float, axis=0) if recent_opp_float.size > 0 else np.zeros(opp_count)
        if np.any(p_js >= self.exploit_threshold):
            return Action.D
        my_actions = history.my_actions
        my_recent = my_actions[start_idx:rounds_completed]
        s_indices = list(range(start_idx, rounds_completed))
        coop_follow_after_C = []
        coop_follow_after_D = []
        for idx, s in enumerate(s_indices):
            available_future = rounds_completed - (s + 1)
            if available_future <= 0:
                continue
            H_s = min(3, available_future)
            future_slice = slice(s + 1, s + 1 + H_s)
            future_matrix = opp_actions[future_slice, :]
            if future_matrix.size == 0:
                continue
            mean_future = float(np.mean(future_matrix.astype(float)))
            if my_actions[s]:
                coop_follow_after_C.append(mean_future)
            else:
                coop_follow_after_D.append(mean_future)
        examples_C = len(coop_follow_after_C)
        examples_D = len(coop_follow_after_D)
        epsilon_explore = max(self.epsilon_floor, 0.1 / max(1, L))
        if examples_C == 0 or examples_D == 0:
            if random.random() < epsilon_explore:
                return Action.C
            else:
                return Action.D
        avg_after_C = float(np.mean(np.array(coop_follow_after_C, dtype=float)))
        avg_after_D = float(np.mean(np.array(coop_follow_after_D, dtype=float)))
        delta_per_round = avg_after_C - avg_after_D
        if delta_per_round <= 0.0 + self._eps:
            if random.random() < epsilon_explore:
                return Action.C
            return Action.D
        if L - 1 <= 0:
            required_delta = float('inf')
        else:
            required_delta = (float(n) / k - 1.0) / float(L - 1)
        if examples_C < self.min_samples or examples_D < self.min_samples:
            required_delta *= self.gamma_conservatism
        if delta_per_round >= required_delta:
            if random.random() < 1e-06:
                return Action.D
            return Action.C
        else:
            if random.random() < epsilon_explore:
                return Action.C
            return Action.D

class Strategy_AGGRESSIVE_101(BaseStrategy):
    """
    Aggressive Public Goods strategy.

    - Probe briefly in the opening (cooperate for T_test rounds).
    - If any other player cooperated during the probe, enter EXPLOIT mode:
        defect whenever others cooperated in the previous round to free-ride.
        occasionally probe to bait cooperators if nobody cooperated last round.
        monitor for punishment: if others' cooperation rate drops by >= delta
        compared to the level before exploitation, switch to SAFE permanently.
    - If no cooperators seen in probe, enter SAFE mode:
        mostly defect, but occasionally probe with small probability p_probe.
        if probes reveal cooperators (sustained cooperation over window W),
        switch to EXPLOIT.
    - Always defect in the last G rounds (endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        if self.r <= 3:
            self.T_test = 1
            self.G = max(0, self.r - 1)
        else:
            self.T_test = min(2, max(1, math.floor(self.r / 10)))
            self.G = min(2, self.r - 1)
        self.p_probe = min(0.15, 1.0 / max(1.0, self.r / 10.0))
        self.delta = 0.3
        self.W = min(5, max(1, self.r - 1))
        self.mode = 'PROBE'
        self.exploit_started = False
        self.coop_rate_before_exploit = None
        self.safe_locked = False

    def _recent_coop_rate(self, history: PlayerHistory, window: int) -> float:
        """Return average cooperation rate among all other players over the last `window` rounds.
        If fewer than `window` rounds exist, uses all available rounds."""
        if history is None or history.round_number == 0:
            return 0.0
        w = min(window, history.round_number)
        recent = history.opponent_actions[-w:, :]
        return float(np.mean(recent))

    def _any_other_cooperated_in_first_T(self, history: PlayerHistory) -> bool:
        if history is None or history.round_number < self.T_test:
            return False
        first_block = history.opponent_actions[:self.T_test, :]
        return bool(np.any(first_block))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t > self.r - self.G:
            return Action.D
        if t <= self.T_test:
            return Action.C
        if self.mode == 'PROBE':
            if self._any_other_cooperated_in_first_T(history):
                self.mode = 'EXPLOIT'
                self.coop_rate_before_exploit = self._recent_coop_rate(history, self.T_test)
                self.exploit_started = True
            else:
                self.mode = 'SAFE'
                self.exploit_started = False
        if self.safe_locked:
            self.mode = 'SAFE'
        recent_coop_rate = self._recent_coop_rate(history, self.W)
        recent_payoff = None
        if history is not None and history.round_number > 0:
            w = min(self.W, history.round_number)
            recent_payoff = float(np.mean(history.my_payoffs[-w:]))
        if recent_payoff is not None and recent_payoff < 1.0:
            self.mode = 'SAFE'
            self.safe_locked = True
            return Action.D
        if self.mode == 'SAFE':
            if history is not None:
                w = min(self.W, history.round_number)
                if w > 0 and bool(np.any(history.opponent_actions[-w:, :])):
                    self.mode = 'EXPLOIT'
                    self.coop_rate_before_exploit = self._recent_coop_rate(history, self.W)
                    self.exploit_started = True
                elif random.random() < self.p_probe:
                    return Action.C
                else:
                    return Action.D
        if self.mode == 'EXPLOIT':
            m_prev = 0
            if history is not None and history.round_number >= 1:
                m_prev = int(np.sum(history.opponent_actions[-1, :]))
            if not self.exploit_started or self.coop_rate_before_exploit is None:
                self.coop_rate_before_exploit = self._recent_coop_rate(history, self.W)
                self.exploit_started = True
            if m_prev >= 1:
                action = Action.D
            else:
                action = Action.C if random.random() < self.p_probe else Action.D
            if self.coop_rate_before_exploit is not None:
                drop = self.coop_rate_before_exploit - recent_coop_rate
                if drop >= self.delta:
                    self.mode = 'SAFE'
                    self.safe_locked = True
                    return Action.D
            if recent_payoff is not None and recent_payoff < 1.0:
                self.mode = 'SAFE'
                self.safe_locked = True
                return Action.D
            return action
        return Action.D

class Strategy_AGGRESSIVE_102(BaseStrategy):
    """
    Aggressive Conditional Defector (ACD)

    - Default: defect.
    - Exploit: defect whenever recent cooperation among others is high (recent_frac >= alpha).
    - Conditional cooperation: cooperate only if there are enough reliably cooperating opponents and overall cooperation is above beta.
    - Punish harshly and quickly when exploited or when reliable cooperators collapse.
    - Uses EWMA reliability scores S_j for each opponent (lambda=0.6).
    - Deterministic (no probing).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.n = n
        self.r = r
        self.k = game_description.k
        self.L = min(5, max(0, r - 1))
        self.E = min(3, max(0, r - 1))
        self.alpha = 0.6
        self.beta = max(0.35, self.alpha - 0.25)
        self.reliable_min = max(1, math.floor(0.15 * (n - 1)))
        self.P = min(5, max(0, r - 1))
        self.lmbda = 0.6
        self.delta_drop = 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory):
        t = getattr(state, 'round_number', None)
        if t is None:
            if history is None:
                t = 1
            else:
                t = history.round_number + 1
        if history is None or history.round_number == 0:
            return Action.D
        rounds_completed = history.round_number
        opp_actions = np.asarray(history.opponent_actions)
        my_actions = np.asarray(history.my_actions)
        n_minus_1 = self.n - 1
        if n_minus_1 <= 0:
            return Action.D
        S = np.zeros(n_minus_1, dtype=float)
        punish_until = 0
        last_round_coop_frac = 0.0
        for i in range(rounds_completed):
            opp_i = opp_actions[i]
            coop_frac_i = float(np.sum(opp_i) / n_minus_1)
            S_old = S.copy()
            S = self.lmbda * S + (1.0 - self.lmbda) * opp_i.astype(float)
            collapse_mask = (S_old >= 0.75) & (S < 0.3)
            if np.any(collapse_mask):
                punish_candidate = i + 1 + self.P
                if punish_candidate > punish_until:
                    punish_until = punish_candidate
            if my_actions[i]:
                coop_prev = last_round_coop_frac
                if i + 1 < rounds_completed:
                    coop_next = float(np.sum(opp_actions[i + 1]) / n_minus_1)
                    if coop_next - coop_prev < -self.delta_drop:
                        punish_candidate = i + 1 + self.P
                        if punish_candidate > punish_until:
                            punish_until = punish_candidate
            last_round_coop_frac = coop_frac_i
        if t > self.r - self.E:
            return Action.D
        if t <= punish_until:
            return Action.D
        if self.L <= 0 or rounds_completed == 0:
            recent_frac = 0.0
        else:
            window_start = max(0, rounds_completed - self.L)
            window = opp_actions[window_start:rounds_completed, :]
            if window.size == 0:
                recent_frac = 0.0
            else:
                per_round_fracs = np.sum(window, axis=1) / float(n_minus_1)
                recent_frac = float(np.mean(per_round_fracs))
        reliable_count = int(np.sum(S >= 0.75))
        if recent_frac >= self.alpha:
            return Action.D
        if reliable_count >= self.reliable_min and recent_frac >= self.beta:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_103(BaseStrategy):
    """
    Aggressive Exploiter (AEX)

    - Default defector that probes early, detects stable cooperation with a short memory,
      and exploits it with a repeating [C, D, D] cycle.
    - Harsh punishment: if exploitation sees cooperation collapse, switch to permanent defection
      (except for deterministic early probes while in the early probing window). Forgiveness
      only after observing strong cooperation (phi_high) for L consecutive rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.L = min(3, max(1, self.r - 1))
        self.t_seed = max(2, min(max(1, self.r - 1), math.ceil(0.15 * self.r)))
        self.phi_high = 0.8
        self.phi_med = 0.5
        self.exploit_cycle = [Action.C, Action.D, Action.D]
        self.cycle_len = len(self.exploit_cycle)
        self.last_round = self.r
        self.cycle_active = False
        self.cycle_pos = 0
        self.perm_defect = False
        self.phi_high_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        t = state.round_number
        if t is None:
            t = history.round_number + 1
        if t == self.last_round:
            return Action.D
        rounds_available = history.round_number
        others_fracs = []
        if rounds_available > 0:
            opp_actions = history.opponent_actions
            start_idx = max(0, rounds_available - self.L)
            recent = opp_actions[start_idx:rounds_available, :]
            if recent.size > 0:
                per_round_sums = np.sum(recent, axis=1)
                others_fracs = (per_round_sums / float(self.n - 1)).tolist()
        if len(others_fracs) == 0:
            f = 0.0
        else:
            f = float(np.mean(others_fracs))
        if self.perm_defect:
            if f >= self.phi_high:
                self.phi_high_streak += 1
            else:
                self.phi_high_streak = 0
            if self.phi_high_streak >= self.L:
                self.perm_defect = False
                self.phi_high_streak = 0
                self.cycle_active = False
                self.cycle_pos = 0
        if self.cycle_active and f < self.phi_med:
            self.cycle_active = False
            self.perm_defect = True
            self.phi_high_streak = 0
        if self.perm_defect:
            if t <= self.t_seed and t % 3 == 1 and (rounds_available > 0):
                recent_others_sum = int(np.sum(history.opponent_actions[-1, :]))
                if recent_others_sum >= 1:
                    return Action.C
            return Action.D
        if f >= self.phi_high:
            if not self.cycle_active:
                self.cycle_active = True
                self.cycle_pos = 0
            action = self.exploit_cycle[self.cycle_pos % self.cycle_len]
            self.cycle_pos += 1
            return action
        if self.phi_med <= f < self.phi_high:
            if t <= self.t_seed and rounds_available > 0:
                recent_others_frac = float(np.sum(history.opponent_actions[-1, :]) / float(self.n - 1))
                if recent_others_frac >= 0.5:
                    return Action.C
            return Action.D
        if f < self.phi_med:
            if t <= self.t_seed and t % 3 == 1 and (rounds_available > 0):
                recent_others_sum = int(np.sum(history.opponent_actions[-1, :]))
                if recent_others_sum >= 1:
                    return Action.C
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_104(BaseStrategy):
    """
    Aggressive Grim-Threshold (AGT) strategy for N-player Public Goods Game.

    - Probes for cooperation for P = min(2, r-1) rounds (never in final round).
    - Cooperates only if previous round had at least M = n-1 cooperators.
    - If exploited while cooperating, enters punishment mode (punish=True) and defects
      until S = 2 consecutive unanimous rounds are observed.
    - Always defects in the final round.
    - Maintains internal state across calls to track punishment and forgiveness progress.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.M = max(0, self.n - 1)
        self.P = min(2, max(0, self.r - 1))
        self.S = 2
        self.punish = False
        self.consecutive_unanimous = 0
        self._processed_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory):
        if state is not None and isinstance(state.round_number, int) and (state.round_number > 0):
            t = int(state.round_number)
        elif history is not None:
            t = history.round_number + 1
        else:
            t = 1
        if history is not None:
            total_completed = history.round_number
            for i in range(self._processed_rounds, total_completed):
                my_coop = bool(history.my_actions[i])
                opp_coops = int(np.sum(history.opponent_actions[i, :])) if history.opponent_actions.size else 0
                observed_cooperators = (1 if my_coop else 0) + int(opp_coops)
                if self.punish:
                    if observed_cooperators == self.n:
                        self.consecutive_unanimous += 1
                    else:
                        self.consecutive_unanimous = 0
                    if self.consecutive_unanimous >= self.S:
                        self.punish = False
                        self.consecutive_unanimous = 0
                elif my_coop and observed_cooperators < self.n:
                    self.punish = True
                    self.consecutive_unanimous = 0
            self._processed_rounds = total_completed
        if t >= self.r:
            return Action.D
        if t <= self.P:
            return Action.C
        if self.punish:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        prev_my_coop = bool(history.my_actions[-1])
        prev_opp_coops = int(np.sum(history.opponent_actions[-1, :])) if history.opponent_actions.size else 0
        prev_total_cooperators = (1 if prev_my_coop else 0) + int(prev_opp_coops)
        if prev_total_cooperators >= self.M:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_105(BaseStrategy):
    """
    Predatory Adaptive Defector (PAD)

    Aggressive default: defect. Probe rarely to learn whether opponents will tolerate
    defections. If opponents are forgiving, exploit (defect) normally. If opponents
    are retaliatory (cooperation drops after our defections), switch to minimal
    cooperative maintenance when preserving cooperation is plausibly valuable.
    Always defect in the final 'tail' rounds. Detect persistent all-cooperators and
    exploit them by defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.tail = max(1, math.floor(self.r / 10))
        self.window = min(5, max(1, self.r - 1))
        self.probe_init = 0.1
        self.probe_decay = 0.9
        self.probe_prob = float(self.probe_init)
        self.rho_threshold = 0.3
        self.coop_maintain_threshold = 0.6
        self.d_drop = 0.15
        self.perm_exploit = False
        self.punished = False

    def __call__(self, state: GameState, history: None | PlayerHistory):
        t = int(state.round_number)
        if t > self.r - self.tail:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        rounds_played = history.round_number
        my_actions = np.asarray(history.my_actions).astype(np.bool_)
        opp_actions = np.asarray(history.opponent_actions).astype(np.bool_)
        coop_counts = np.sum(opp_actions, axis=1) + my_actions.astype(int)
        f = coop_counts.astype(float) / float(self.n)
        if rounds_played >= 1:
            if opp_actions.shape[1] >= 1:
                always_coop = np.all(opp_actions, axis=0)
                if np.any(always_coop):
                    return Action.D
        lookback = min(self.window, rounds_played)
        start_idx = rounds_played - lookback
        indices = list(range(start_idx, rounds_played))
        our_defection_rounds = [idx for idx in indices if not bool(my_actions[idx])]
        if len(our_defection_rounds) == 0:
            retaliation_rate = 0.0
        else:
            count_drops = 0
            for d in our_defection_rounds:
                if d + 1 < rounds_played:
                    delta = float(f[d + 1]) - float(f[d])
                    if delta <= -self.d_drop:
                        count_drops += 1
            retaliation_rate = float(count_drops) / float(max(1, len(our_defection_rounds)))
        recently_punished = False
        if len(our_defection_rounds) > 0:
            for d in our_defection_rounds:
                if d + 1 < rounds_played:
                    if float(f[d + 1]) - float(f[d]) <= -self.d_drop:
                        recently_punished = True
                        break
        if recently_punished:
            self.punished = True
            self.perm_exploit = False
        forgiving = retaliation_rate < self.rho_threshold
        if self.punished and forgiving:
            self.punished = False
            self.perm_exploit = True
        rounds_left = self.r - t + 1
        can_probe = t <= self.r - self.tail - 1 and rounds_left > 1
        if forgiving and (not self.punished):
            if can_probe and random.random() < self.probe_prob:
                self.probe_prob *= self.probe_decay
                return Action.C
            if retaliation_rate < self.rho_threshold and len(our_defection_rounds) >= 1:
                self.perm_exploit = True
            return Action.D
        if can_probe and random.random() < self.probe_prob:
            self.probe_prob *= self.probe_decay
            return Action.C
        f_prev = float(f[-1]) if len(f) >= 1 else 0.0
        if f_prev >= self.coop_maintain_threshold and rounds_left > self.tail + 1:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_106(BaseStrategy):
    """
    Aggressive Opportunistic Exploiter (AOE)

    Probes early to identify cooperators, classifies opponents into categories
    (Unconditional cooperators, Conditional cooperators, Defectors, Mixed),
    then aggressively defects to exploit detected unconditional cooperators or
    large cooperative groups, sustains conditional cooperation with reciprocators,
    uses short harsh punishments after being exploited, and always defects in
    the final round(s) to harvest end-game gains. Periodically probes while in
    defect/exploit modes to discover new cooperators.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.UncondCoopThresh = 0.9
        self.CondCoopThresh = 0.6
        self.ProbeRounds = min(2, max(0, self.r - 1))
        self.punish_remaining = 0
        self.mode = 'Probe'
        self.last_probe_round = 0
        self.last_classification = {'U': set(), 'Q': set(), 'Z': set()}
        self.cond_coop_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        R = self.r - t + 1
        if R == 1:
            self.mode = 'Defect'
            return Action.D
        if t <= self.ProbeRounds:
            self.mode = 'Probe'
            self.last_probe_round = t
            return Action.C
        if self.punish_remaining > 0:
            self.punish_remaining -= 1
            if self.punish_remaining == 0:
                self.mode = 'Defect'
            else:
                self.mode = 'Punish'
            return Action.D
        if history is None:
            return Action.D
        full_rounds = history.round_number
        L = min(6, max(1, full_rounds))
        start_idx = max(0, full_rounds - L)
        opp_actions_window = history.opponent_actions[start_idx:full_rounds, :]
        my_actions_window = history.my_actions[start_idx:full_rounds]
        m = self.n - 1
        opp_coop_counts = np.sum(opp_actions_window.astype(int), axis=0)
        freq_j = opp_coop_counts / float(L)
        meC_mask = my_actions_window.astype(bool)
        meD_mask = ~my_actions_window.astype(bool)
        num_meC = int(np.sum(meC_mask))
        num_meD = int(np.sum(meD_mask))
        if num_meC > 0:
            opp_after_meC = opp_actions_window[meC_mask, :].astype(int)
            freq_j_meC = np.sum(opp_after_meC, axis=0) / float(num_meC)
        else:
            freq_j_meC = np.zeros(m, dtype=float)
        if num_meD > 0:
            opp_after_meD = opp_actions_window[meD_mask, :].astype(int)
            freq_j_meD = np.sum(opp_after_meD, axis=0) / float(num_meD)
        else:
            freq_j_meD = np.zeros(m, dtype=float)
        U = set()
        Q = set()
        Z = set()
        for j in range(m):
            if freq_j[j] >= self.UncondCoopThresh:
                U.add(j)
            if freq_j[j] <= 1.0 - self.UncondCoopThresh:
                Z.add(j)
            if freq_j_meC[j] >= self.CondCoopThresh and freq_j_meD[j] <= freq_j_meC[j] - 0.2:
                Q.add(j)
        self.last_classification = {'U': U, 'Q': Q, 'Z': Z}
        last_round_idx = full_rounds - 1
        opp_coop_last = int(np.sum(history.opponent_actions[last_round_idx, :].astype(int)))
        my_coop_last = int(history.my_actions[last_round_idx])
        group_recent_coop = (opp_coop_last + my_coop_last) / float(self.n)
        RecentGroupCoopThresh = max(0.5, 0.5 + 0.25 * (self.k / float(self.n)))
        S = max(1, math.ceil(R / 3))
        P = min(3, max(0, R - 1))
        if my_coop_last == 1:
            others_coop = opp_coop_last
            if others_coop <= m // 2:
                self.punish_remaining = P
                self.mode = 'Punish'
                if self.punish_remaining > 0:
                    self.punish_remaining -= 1
                    return Action.D
        exploit_condition = len(U) > 0 or group_recent_coop >= RecentGroupCoopThresh
        many_Q_thresh = max(1, math.ceil(0.4 * float(m)))
        many_Q = len(Q) >= many_Q_thresh
        probe_period = max(5, S)
        if exploit_condition:
            self.mode = 'Exploit'
            if t - self.last_probe_round >= probe_period:
                if R > S:
                    self.last_probe_round = t
                    return Action.C
            return Action.D
        if many_Q:
            self.mode = 'ConditionalCoop'
            if R <= S:
                return Action.D
            if len(Q) > 0:
                q_last_coops = 0
                for j in Q:
                    q_last_coops += int(history.opponent_actions[last_round_idx, j])
                if q_last_coops >= math.ceil(len(Q) * self.CondCoopThresh):
                    self.cond_coop_streak += 1
                    return Action.C
                else:
                    self.punish_remaining = P
                    self.mode = 'Punish'
                    if self.punish_remaining > 0:
                        self.punish_remaining -= 1
                    return Action.D
            else:
                self.mode = 'Defect'
                return Action.D
        self.mode = 'Defect'
        if t - self.last_probe_round >= probe_period and R > S:
            self.last_probe_round = t
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_107(BaseStrategy):
    """
    Exploit-and-Enforce (E&E) strategy for the N-player Public Goods Game.

    Summary:
    - Short test phase of cooperation to probe others (T_test rounds).
    - Never cooperate in the final T_end rounds.
    - Track per-player cooperation rates over a window W.
    - If a previously-trusted cooperator defects, trigger a long punishment window (P rounds) of unconditional defection.
    - If the group appears reliably cooperative (>= τ fraction of players with coop_rate >= θ),
      attempt short exploitation (defect for a short exploit window) to gain immediate advantage.
    - Otherwise, defect outside the test phase.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = int(self.game_description.n_rounds)
        self.T_test = max(1, min(3, math.floor(r / 4)))
        self.T_end = 1
        self.W = min(5, r)
        self.theta = 0.85
        self.tau = 0.6
        self.P = max(math.ceil(r / 3), 5)
        self.exploit_window = 1
        self.current_mode = 'TRY_COOP'
        self.punish_timer = 0
        self.exploit_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.current_mode = 'TRY_COOP'
            return Action.C
        R = history.round_number
        t = R + 1
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        if t > r - self.T_end:
            self.current_mode = 'PUNISH' if self.punish_timer > 0 else 'ENDGAME'
            return Action.D
        denom_prev = min(self.W, max(0, R - 1))
        coop_rate_prev = np.zeros(n, dtype=float)
        if denom_prev > 0:
            actions = np.zeros((R, n), dtype=float)
            if R > 0:
                actions[:R, 0] = history.my_actions.astype(float)
            if R > 0 and history.opponent_actions.size != 0:
                ops = history.opponent_actions
                if ops.ndim == 1:
                    ops = ops.reshape(R, -1)
                actions[:R, 1:] = ops.astype(float)
            start0 = max(0, R - self.W)
            end0 = R - 2
            if end0 >= start0:
                window = actions[start0:end0 + 1, :]
                coop_rate_prev = np.mean(window, axis=0)
            else:
                coop_rate_prev = np.zeros(n, dtype=float)
        else:
            coop_rate_prev = np.zeros(n, dtype=float)
        if R >= 1:
            last_actions = np.zeros(n, dtype=bool)
            last_actions[0] = bool(history.my_actions[-1])
            if history.opponent_actions.size != 0:
                ops_last = history.opponent_actions[-1, :]
                last_actions[1:] = ops_last.astype(bool)
            remaining_punishable = max(0, r - R - self.T_end)
            if remaining_punishable > 0:
                for j in range(n):
                    if coop_rate_prev[j] >= self.theta and (not last_actions[j]):
                        self.punish_timer = min(self.P, remaining_punishable)
                        self.current_mode = 'PUNISH'
                        break
        if self.punish_timer > 0:
            self.punish_timer = max(0, self.punish_timer - 1)
            self.current_mode = 'PUNISH'
            return Action.D
        if t <= self.T_test and self.current_mode != 'PUNISH':
            self.current_mode = 'TRY_COOP'
            return Action.C
        denom_recent = min(self.W, max(0, R))
        coop_rate_recent = np.zeros(n, dtype=float)
        if denom_recent > 0:
            actions_recent = np.zeros((R, n), dtype=float)
            if R > 0:
                actions_recent[:R, 0] = history.my_actions.astype(float)
            if R > 0 and history.opponent_actions.size != 0:
                ops = history.opponent_actions
                if ops.ndim == 1:
                    ops = ops.reshape(R, -1)
                actions_recent[:R, 1:] = ops.astype(float)
            start0 = max(0, R - self.W)
            end0 = R - 1
            if end0 >= start0:
                window_recent = actions_recent[start0:end0 + 1, :]
                coop_rate_recent = np.mean(window_recent, axis=0)
            else:
                coop_rate_recent = np.zeros(n, dtype=float)
        else:
            coop_rate_recent = np.zeros(n, dtype=float)
        coop_count = int(np.sum(coop_rate_recent >= self.theta))
        if coop_count / float(n) >= self.tau:
            self.current_mode = 'EXPLOIT'
            if self.exploit_timer > 0:
                self.exploit_timer = max(0, self.exploit_timer - 1)
                return Action.D
            else:
                self.exploit_timer = max(0, self.exploit_window - 1)
                return Action.D
        self.current_mode = 'DEFECT'
        return Action.D

class Strategy_AGGRESSIVE_108(BaseStrategy):
    """
    Aggressor — Probe–Exploit–Punish (APEP)

    - Deterministic single probe early (if r >= 3).
    - If a majority of other players cooperate in the probe, enter EXPLOITING.
    - While EXPLOITING, defect every round; if a majority of others stop cooperating,
      enter PUNISHING for a fixed punish_timer.
    - After punishment, check a small recovery window to decide whether to resume exploiting
      or fall back to default (defect).
    - Always defect in the last round. First round defects as well.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.PROBE_ROUND = min(2, max(1, r - 1))
        self.PROBE_SUCCESS_THRESHOLD = math.ceil((n - 1) / 2.0)
        self.EXPLOIT_SUSTAIN_THRESHOLD = math.ceil((n - 1) / 2.0)
        tentative = int(math.floor(min(r / 4.0, 6.0)))
        tentative = max(2, tentative)
        cap = max(0, r - 2)
        self.PUNISH_LENGTH = min(tentative, cap)
        self.RECOVERY_WINDOW = min(3, max(0, r - 2))
        self._mode = 'DEFAULT'
        self._probe_done = False
        self._punish_timer = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
            last_completed = 0
        else:
            last_completed = history.round_number
            t = last_completed + 1
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        prev_mode = self._mode

        def others_cooperators_in_round(round_number_1idx: int) -> int:
            if history is None:
                return 0
            idx = round_number_1idx - 1
            if idx < 0 or idx >= history.opponent_actions.shape[0]:
                return 0
            return int(np.sum(history.opponent_actions[idx, :]))
        if history is not None and prev_mode == 'DEFAULT' and self._probe_done:
            if last_completed >= self.PROBE_ROUND:
                others_coop = others_cooperators_in_round(self.PROBE_ROUND)
                if others_coop >= self.PROBE_SUCCESS_THRESHOLD and r >= 3:
                    self._mode = 'EXPLOITING'
        if history is not None and prev_mode == 'EXPLOITING' and (last_completed >= 1):
            others_coop_last = others_cooperators_in_round(last_completed)
            if others_coop_last < self.EXPLOIT_SUSTAIN_THRESHOLD:
                self._mode = 'PUNISHING'
                self._punish_timer = self.PUNISH_LENGTH
        if t == r:
            if self._mode == 'PUNISHING' and self._punish_timer > 0:
                self._punish_timer = max(0, self._punish_timer - 1)
            return Action.D
        if t == 1:
            return Action.D
        if self._mode == 'PUNISHING':
            if self._punish_timer > 0:
                self._punish_timer -= 1
            if self._punish_timer == 0:
                recovered = False
                W = self.RECOVERY_WINDOW
                if W > 0 and history is not None and (last_completed >= 1):
                    window_start = max(1, t - W)
                    if window_start <= last_completed:
                        recovered = True
                        for s in range(window_start, last_completed + 1):
                            others_coop = others_cooperators_in_round(s)
                            if others_coop < self.EXPLOIT_SUSTAIN_THRESHOLD:
                                recovered = False
                                break
                if recovered:
                    self._mode = 'EXPLOITING'
                else:
                    self._mode = 'DEFAULT'
            return Action.D
        if self._mode == 'EXPLOITING':
            return Action.D
        if not self._probe_done and t == self.PROBE_ROUND and (r >= 3):
            self._probe_done = True
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_109(BaseStrategy):
    """
    Aggressive Probe–Exploit–Grim (APEG)

    - Probe once in the first round.
    - Default posture: defect.
    - Rejoin only if recent group cooperation is high (stable window).
    - While in stable cooperation, exploit (defect) at most once every S_exploit rounds.
    - After an exploit, evaluate whether group cooperation collapsed; if it did, set a permanent grim trigger.
    - Last round always defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.T_probe = 1
        self.L_stable = min(3, max(1, math.floor(self.r / 10)))
        self.alpha_stable = 0.8
        self.S_exploit = 3
        self.collapse_delta = 0.3
        self.grim_trigger = False
        self.last_exploit_round = float('-inf')
        self.awaiting_exploit_eval = False
        self.exploit_round_to_eval = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:

        def _coop_counts(hist: PlayerHistory) -> np.ndarray:
            my = hist.my_actions.astype(np.int64)
            opp_sum = np.sum(hist.opponent_actions.astype(np.int64), axis=1)
            return my + opp_sum
        if history is not None and self.awaiting_exploit_eval:
            if history.round_number >= (self.exploit_round_to_eval or 0):
                coop_counts = _coop_counts(history)
                t = int(self.exploit_round_to_eval)
                start_before = max(1, t - self.L_stable)
                end_before = t - 1
                rounds_before = end_before - start_before + 1
                if rounds_before > 0:
                    idx_start_b = start_before - 1
                    idx_end_b = end_before - 1
                    sum_before = int(np.sum(coop_counts[idx_start_b:idx_end_b + 1]))
                    coop_rate_before = sum_before / (rounds_before * self.n)
                else:
                    coop_rate_before = 0.0
                start_after = max(1, t - self.L_stable + 1)
                end_after = t
                rounds_after = end_after - start_after + 1
                if rounds_after > 0:
                    idx_start_a = start_after - 1
                    idx_end_a = end_after - 1
                    idx_end_a = min(idx_end_a, history.round_number - 1)
                    if idx_start_a <= idx_end_a:
                        sum_after = int(np.sum(coop_counts[idx_start_a:idx_end_a + 1]))
                        coop_rate_after = sum_after / (rounds_after * self.n)
                    else:
                        coop_rate_after = 0.0
                else:
                    coop_rate_after = 0.0
                if coop_rate_after <= coop_rate_before - self.collapse_delta:
                    self.grim_trigger = True
                self.awaiting_exploit_eval = False
                self.exploit_round_to_eval = None
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t == self.r:
            return Action.D
        if self.grim_trigger:
            return Action.D
        if t <= self.T_probe:
            return Action.C
        if history is None:
            return Action.D
        window_start = max(1, t - self.L_stable)
        window_end = t - 1
        rounds_in_window = window_end - window_start + 1
        coop_counts = _coop_counts(history)
        if rounds_in_window > 0:
            idx_start = window_start - 1
            idx_end = window_end - 1
            coop_sum = int(np.sum(coop_counts[idx_start:idx_end + 1]))
            coop_rate_recent = coop_sum / (rounds_in_window * self.n)
        else:
            coop_rate_recent = 0.0
        if coop_rate_recent >= self.alpha_stable:
            if t - self.last_exploit_round >= self.S_exploit:
                self.last_exploit_round = t
                self.awaiting_exploit_eval = True
                self.exploit_round_to_eval = t
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_110(BaseStrategy):
    """
    Opportunistic Exploiter with Targeted Rebuild & Punish (OETRP).

    Aggressive default: defect. Signal cooperation briefly at start, exploit when
    recent group cooperation is high, rebuild when cooperation collapses with a
    short cooperative window, and punish small groups of persistent defectors
    briefly (followed by forgiveness). Always defect in the final S_end rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(10, max(3, math.floor(self.r / 10)))
        self.S_init = min(3, max(1, math.floor(self.r * 0.06)))
        self.S_end = min(3, max(1, self.r - 1)) if self.r > 1 else 1
        self.S_rebuild = 2
        self.S_punish = min(3, max(1, math.floor(0.06 * self.r)))
        public_return = self.k / float(self.n)
        raw_G_exploit = 0.5 + 0.3 * public_return
        self.G_exploit = min(max(raw_G_exploit, 0.5), 0.85)
        self.G_rebuild = max(0.15, self.G_exploit - 0.25)
        self.punish_until_round = 0
        self.rebuild_until_round = 0
        self.last_punish_end = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t <= self.S_init:
            return Action.C
        if t > self.r - self.S_end:
            return Action.D
        if t <= self.punish_until_round:
            return Action.D
        if t <= self.rebuild_until_round:
            return Action.C
        if history is None or history.opponent_actions.size == 0:
            return Action.D
        window = min(self.W, max(1, t - 1))
        opp_actions = history.opponent_actions
        if opp_actions.shape[0] < window:
            window = opp_actions.shape[0] if opp_actions.shape[0] > 0 else 0
        if window == 0:
            return Action.D
        recent = opp_actions[-window:, :]
        counts_per_player = np.sum(recent.astype(int), axis=0)
        total_coop_counts = int(np.sum(counts_per_player))
        denom = (self.n - 1) * window
        G = float(total_coop_counts) / float(denom) if denom > 0 else 0.0
        p_j = counts_per_player.astype(float) / float(window)
        if G >= self.G_exploit:
            return Action.D
        if G <= self.G_rebuild:
            self.rebuild_until_round = t + self.S_rebuild - 1
            return Action.C
        low_idx = np.where(p_j <= 0.2)[0]
        num_low = int(len(low_idx))
        max_allowed = math.floor((self.n - 1) / 2)
        if total_coop_counts > 0:
            low_contrib = int(np.sum(counts_per_player[low_idx])) if num_low > 0 else 0
            frac_low = float(low_contrib) / float(total_coop_counts)
        else:
            frac_low = 0.0
        if 1 <= num_low <= max_allowed and frac_low <= 0.5:
            self.punish_until_round = t + self.S_punish - 1
            self.last_punish_end = self.punish_until_round
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_111(BaseStrategy):
    """
    Aggressive Exploiter (AggEx)

    - Default: defect.
    - Probe occasionally with probability epsilon to find exploitable cooperators.
    - If ever cooperating and being clearly exploited (very few cooperators that round),
      trigger permanent grim (always defect thereafter).
    - Exploit reliably cooperative groups by defecting when their average cooperation >= θ_exploit.
    - Always defect in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.L = max(1, math.floor(self.r / 4))
        raw_eps = 0.0
        if self.n - 1 > 0:
            raw_eps = (self.k - 1.0) / (self.n - 1.0)
        self.epsilon = min(0.25, max(0.05, raw_eps))
        self.theta_exploit = 0.6
        self.theta_all_defect = 0.15
        self.exploited_threshold = math.ceil(self.n / 3)
        self.grim_triggered = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t == self.r:
            return Action.D
        if history is not None and (not self.grim_triggered) and (history.round_number > 0):
            my_actions = np.asarray(history.my_actions, dtype=bool)
            opp_actions = np.asarray(history.opponent_actions, dtype=bool)
            if my_actions.size > 0:
                opp_counts = np.sum(opp_actions, axis=1).astype(int)
                my_int = my_actions.astype(int)
                S_arr = my_int + opp_counts
                exploited_mask = (my_int == 1) & (S_arr <= self.exploited_threshold)
                if np.any(exploited_mask):
                    self.grim_triggered = True
        if self.grim_triggered:
            return Action.D
        if t == 1:
            return Action.D
        if history is None or history.round_number == 0:
            return Action.D
        window_len = min(self.L, t - 1)
        opp_hist = np.asarray(history.opponent_actions, dtype=bool)
        if window_len <= 0 or opp_hist.size == 0:
            return Action.D
        opp_window = opp_hist[-window_len:, :]
        p_j = np.mean(opp_window.astype(float), axis=0)
        P = float(np.mean(p_j))
        if P >= self.theta_exploit:
            return Action.D
        if P <= self.theta_all_defect:
            return Action.D
        if random.random() < self.epsilon:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_112(BaseStrategy):
    """
    Aggressive Trigger-and-Exploit (ATE)

    Defect-by-default aggressive strategy for N-player public goods.
    Probes once in long games, exploits when many others cooperated last round,
    imposes sustained punishments when recent cooperation is very low, and
    performs a single cooperative test after a punishment to check for recovery.
    Always defects in the final round and avoids cooperative gambits near the end.
    Deterministic, uses only observed history and game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        self.n = n
        self.r = r
        self.k = game_description.k
        self.exploit_threshold_excl = max(1, math.ceil(0.2 * n))
        self.punish_threshold_excl = math.floor(0.15 * n)
        self.initial_probe_rounds = min(1, math.floor(r / 10))
        self.base_punish_length = max(2, math.ceil(r / 10))
        self.endgame_safe_horizon = min(3, r)
        self.w = min(10, max(0, r - 1))
        self.punish_timer = 0
        self.exploit_timer = 0
        self._post_punish_test = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = state.round_number
        rounds_left = self.r - t + 1
        if history is None:
            if t == 1 and self.initial_probe_rounds >= 1 and (self.r >= 10):
                return Action.C
            return Action.D
        completed = history.round_number
        if completed >= 1:
            last_round_others = int(np.sum(history.opponent_actions[-1, :]))
        else:
            last_round_others = 0
        w_eff = min(self.w, completed) if self.w > 0 else 0
        if w_eff > 0:
            recent_others_sum = float(np.sum(history.opponent_actions[-w_eff:, :]))
            recent_group_rate = recent_others_sum / (w_eff * (self.n - 1))
        else:
            recent_group_rate = 0.0
        if w_eff > 0 and completed > 0:
            my_recent_payoff = float(np.mean(history.my_payoffs[-w_eff:]))
        else:
            my_recent_payoff = 0.0
        if t == self.r:
            return Action.D
        if rounds_left <= self.endgame_safe_horizon:
            return Action.D
        if self.punish_timer > 0:
            self.punish_timer = max(0, self.punish_timer - 1)
            if self.punish_timer == 0:
                self._post_punish_test = True
            return Action.D
        if self.exploit_timer > 0:
            self.exploit_timer = max(0, self.exploit_timer - 1)
            return Action.D
        if t == 1:
            if self.initial_probe_rounds >= 1 and self.r >= 10:
                return Action.C
            return Action.D
        if self._post_punish_test:
            self._post_punish_test = False
            return Action.C
        if last_round_others >= self.exploit_threshold_excl and rounds_left > self.endgame_safe_horizon:
            self.exploit_timer = 1
            return Action.D
        avg_other_cooperators = recent_group_rate * (self.n - 1)
        if avg_other_cooperators <= float(self.punish_threshold_excl) and rounds_left > self.base_punish_length + 1:
            self.punish_timer = self.base_punish_length
            self.punish_timer = max(0, self.punish_timer - 1)
            if self.punish_timer == 0:
                self._post_punish_test = True
            return Action.D
        shared_from_others = self.k / self.n * (recent_group_rate * (self.n - 1))
        threshold_payoff = 1.05 * (1.0 + shared_from_others)
        if recent_group_rate >= 0.6 and my_recent_payoff < threshold_payoff and (rounds_left > self.endgame_safe_horizon):
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_113(BaseStrategy):
    """
    Aggressive Opportunist (AO) for N-player Public Goods Game.

    - Default: defect (D). Rare probe in first round (configurable).
    - Cooperates only when a sufficiently large set of reliable players is detected
      and expected remaining cooperative rounds justify risking the one-shot temptation.
    - In Cooperative Mode: mostly cooperate but occasionally defect to exploit, and
      punish sharp drops with a short but severe punishment (finite grim-like).
    - Maintains per-opponent persistent-bad exclusion for very low long-run reliability.
    - Random choices are pseudorandom and seeded from game parameters for repeatability.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_counter = 0
        self.coop_run_counter = 0
        self.persistently_bad = set()
        n = game_description.n_players
        k = game_description.k
        r = game_description.n_rounds
        seed = int((n * 10007 + int(round(k * 1009)) + r * 97) % (2 ** 31 - 1))
        self.rng = random.Random(seed)
        self.reliability_thresh = 0.8
        self.p_probe = 0.03
        self.drop_thresh = 0.4
        self.persistently_bad_thresh = 0.2
        self.long_run_window_cap = 20
        self.recent_window_cap = 5
        self.tie_tolerance = 0.5
        self.one_shot_gain = 1.0
        self.coop_per_round_gain = max(0.0, self.game_description.k - 1.0)
        denom = max(1e-09, self.coop_per_round_gain)
        self.required_future_rounds = math.ceil(self.one_shot_gain / denom)
        self.final_round_defect_window = min(self.game_description.n_rounds, max(1, self.required_future_rounds))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        num_opponents = n - 1
        t = int(state.round_number)
        if history is None or history.round_number == 0:
            if self.rng.random() < self.p_probe:
                return Action.C
            return Action.D
        if t > r - self.final_round_defect_window:
            self.coop_run_counter = 0
            return Action.D
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            return Action.D
        rounds_completed = history.round_number
        W = min(self.recent_window_cap, rounds_completed)
        if W <= 0:
            return Action.D
        opp_actions = np.asarray(history.opponent_actions)
        long_W = min(self.long_run_window_cap, rounds_completed)
        if long_W > 0:
            long_slice = opp_actions[-long_W:, :] if long_W > 0 else opp_actions[:0, :]
            long_reliabilities = np.zeros(num_opponents, dtype=float)
            if long_slice.size > 0:
                long_reliabilities = np.sum(long_slice, axis=0) / float(long_W)
            for j in range(num_opponents):
                if long_reliabilities[j] < self.persistently_bad_thresh:
                    self.persistently_bad.add(int(j))
        recent_slice = opp_actions[-W:, :]
        if recent_slice.size == 0:
            recent_reliabilities = np.zeros(num_opponents, dtype=float)
        else:
            recent_reliabilities = np.sum(recent_slice, axis=0) / float(W)
        reliable_mask = np.array([recent_reliabilities[j] >= self.reliability_thresh and j not in self.persistently_bad for j in range(num_opponents)], dtype=bool)
        reliable_indices = [int(j) for j in range(num_opponents) if reliable_mask[j]]
        num_reliable = len(reliable_indices)
        if num_reliable == 0:
            self.coop_run_counter = 0
            return Action.D
        if num_reliable > 0:
            reliable_slice = recent_slice[:, reliable_indices]
            if reliable_slice.size == 0:
                per_round_fracs = np.zeros(W, dtype=float)
            else:
                per_round_fracs = np.sum(reliable_slice, axis=1) / float(num_reliable)
            avg_reliable_coop_fraction = float(np.mean(per_round_fracs))
            last_round_fraction = float(per_round_fracs[-1]) if per_round_fracs.size > 0 else 0.0
        else:
            avg_reliable_coop_fraction = 0.0
            last_round_fraction = 0.0
        expected_remaining_coop_rounds_if_join = avg_reliable_coop_fraction * float(r - t + 1)
        if t <= 3 and num_reliable < math.ceil(0.5 * num_opponents):
            self.coop_run_counter = 0
            return Action.D
        required_half = math.ceil(0.5 * float(num_opponents))
        if num_reliable < required_half or expected_remaining_coop_rounds_if_join + self.tie_tolerance < self.required_future_rounds:
            self.coop_run_counter = 0
            return Action.D
        recent_avg = avg_reliable_coop_fraction
        drop_triggered = False
        if recent_avg > 0:
            drop_fraction = (recent_avg - last_round_fraction) / recent_avg
            if drop_fraction >= self.drop_thresh:
                drop_triggered = True
        if drop_triggered:
            punish_len = max(1, math.ceil(2 * float(self.required_future_rounds)))
            self.punishment_counter = punish_len
            self.punishment_counter -= 1
            self.coop_run_counter = 0
            return Action.D
        exploitation_interval = max(3, self.required_future_rounds)
        opportunistic_defect_probability = min(0.2, 1.0 / (1.0 + avg_reliable_coop_fraction * float(num_opponents)))
        if not isinstance(self.coop_run_counter, int):
            try:
                self.coop_run_counter = int(self.coop_run_counter)
            except Exception:
                self.coop_run_counter = 0
        if exploitation_interval > 0 and self.coop_run_counter % exploitation_interval == 0:
            self.coop_run_counter += 1
            return Action.D
        else:
            randv = self.rng.random()
            self.coop_run_counter += 1
            if randv < opportunistic_defect_probability:
                return Action.D
            else:
                return Action.C

class Strategy_AGGRESSIVE_114(BaseStrategy):
    """
    Aggressive Conditional Exploiter (ACE)

    - Probe for a few rounds by cooperating.
    - If community cooperation P is above a threshold, exploit: defect most rounds,
      but give deterministic cooperation "pulses" at a low frequency to keep cooperators engaged.
    - If community cooperation falls below threshold, enter Punish mode: defect for a (possibly long) punishment.
    - Always defect in final T_endgame rounds.
    - Tracks simple persistent mode flags (in_punish, in_exploit, perm_punish) to ensure deterministic behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.in_punish = False
        self.remaining_punish_rounds = 0
        self.in_exploit = False
        self.exploit_start_round = None
        self.exploit_start_P = None
        self.perm_punish = False
        self.p_high = 0.85
        self.p_low = 0.15
        self.L_punish_min = 3
        self.q_keepalive_max = 0.25
        self.delta_drop = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        t = int(state.round_number)
        T_probe = min(3, max(1, math.floor(r / 10)))
        T_endgame = min(2, max(1, r - 1))
        W = min(5, max(1, math.floor(r / 5)))
        if k >= 0.75 * n:
            p_threshold = 0.5
        else:
            p_threshold = 0.75
        if t > r - T_endgame:
            return Action.D
        if history is None or history.round_number == 0:
            self.in_punish = False
            self.remaining_punish_rounds = 0
            self.in_exploit = False
            self.exploit_start_round = None
            self.exploit_start_P = None
            self.perm_punish = False
            if t <= T_probe:
                return Action.C
            else:
                return Action.D
        completed = int(history.round_number)
        n_others = n - 1
        opp_actions = history.opponent_actions
        if t <= T_probe:
            return Action.C
        if completed <= 0:
            p_js = np.zeros(n_others, dtype=float)
        else:
            totals = np.sum(opp_actions, axis=0).astype(float)
            p_j_long = totals / float(completed)
            last_k = min(W, completed)
            if last_k <= 0:
                p_j_recent = np.zeros_like(p_j_long)
            else:
                recent_slice = opp_actions[-last_k:, :]
                recent_totals = np.sum(recent_slice, axis=0).astype(float)
                p_j_recent = recent_totals / float(last_k)
            p_js = 0.6 * p_j_recent + 0.4 * p_j_long
        if n_others > 0:
            P = float(np.mean(p_js))
        else:
            P = 0.0
        strong_cooperators = int(np.sum(p_js >= self.p_high))
        strong_defectors = int(np.sum(p_js <= self.p_low))
        if strong_defectors >= math.ceil(n_others * 0.6):
            self.perm_punish = True
            self.in_punish = True
            self.remaining_punish_rounds = max(self.remaining_punish_rounds, r)
        if self.in_punish and self.remaining_punish_rounds > 0:
            self.remaining_punish_rounds = max(0, self.remaining_punish_rounds - 1)
            if self.remaining_punish_rounds == 0 and (not self.perm_punish):
                self.in_punish = False
            return Action.D
        if self.perm_punish:
            self.in_punish = True
            return Action.D
        if self.in_exploit:
            if self.exploit_start_round is None:
                self.in_exploit = False
                self.exploit_start_P = None
                self.exploit_start_round = None
            else:
                rounds_since_exploit = t - self.exploit_start_round
                H = min(5, max(0, r - self.exploit_start_round))
                if rounds_since_exploit <= H and self.exploit_start_P - P > self.delta_drop:
                    extra = math.ceil((p_threshold - P) * 10) if p_threshold - P > 0 else 0
                    L_punish = min(max(self.L_punish_min, extra), max(0, r - t))
                    L_punish = max(1, L_punish)
                    self.in_punish = True
                    self.remaining_punish_rounds = L_punish
                    self.in_exploit = False
                    self.exploit_start_P = None
                    self.exploit_start_round = None
                    self.remaining_punish_rounds = max(0, self.remaining_punish_rounds - 1)
                    return Action.D
        if P >= p_threshold:
            self.in_exploit = True
            if self.exploit_start_round is None:
                self.exploit_start_round = t
                self.exploit_start_P = P
            raw_q = 0.0
            if 1.0 - p_threshold > 0:
                raw_q = (P - p_threshold) / (1.0 - p_threshold)
            q_keepalive = min(self.q_keepalive_max, max(0.0, raw_q))
            if strong_cooperators >= math.ceil(n_others / 2.0):
                q_keepalive = min(self.q_keepalive_max, max(q_keepalive, self.q_keepalive_max))
            if q_keepalive <= 0.0:
                return Action.D
            else:
                M = max(1, math.ceil(1.0 / float(q_keepalive)))
                if t % M == 0:
                    return Action.C
                else:
                    return Action.D
        else:
            extra = math.ceil((p_threshold - P) * 10) if p_threshold - P > 0 else 0
            L_punish = min(max(self.L_punish_min, extra), max(0, r - t))
            L_punish = max(1, L_punish)
            self.in_punish = True
            self.remaining_punish_rounds = L_punish - 1
            self.in_exploit = False
            self.exploit_start_round = None
            self.exploit_start_P = None
            return Action.D

class Strategy_AGGRESSIVE_115(BaseStrategy):
    """
    Predatory Opportunist (PO)

    Aggressive, defection-first public-goods strategy that:
    - Defaults to defecting.
    - Detects reliably cooperative opponents (short-window frequency).
    - "Milks" reliable cooperators by defecting almost always and giving rare cooperative
      maintenance gifts on a deterministic schedule.
    - Punishes sharp drops in group cooperation by continuing to defect for a
      bounded punishment interval.
    - Performs occasional deterministic probes to discover exploitable cooperators.
    - Always defects in the final S rounds (endgame shrink).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.f_high = 0.8
        self.rc_threshold = 0.5
        self.drop_threshold = 0.3
        self.M_advantage = 0.15

    def __call__(self, state: GameState, history: None | PlayerHistory):
        n = int(self.game_description.n_players)
        r = int(self.game_description.n_rounds)
        k = float(self.game_description.k)
        t = int(state.round_number)
        if history is None or (hasattr(history, 'round_number') and history.round_number == 0):
            return Action.D
        remaining = max(0, r - t + 1)
        S = min(3, r)
        if t >= r - S + 1:
            return Action.D
        H = min(8, max(1, t - 1))
        P_seed = max(3, math.ceil(6 * n / max(1.0, k)))
        P_probe = max(6, 2 * P_seed)
        P_punish = min(max(3, math.ceil(H)), remaining)
        opp_actions = np.asarray(history.opponent_actions)
        opp_payoffs = np.asarray(history.opponent_payoffs)
        my_payoffs = np.asarray(history.my_payoffs)
        last_H_actions = opp_actions[-H:, :]
        f_j = np.mean(last_H_actions.astype(float), axis=0)
        RC = float(np.mean(f_j >= self.f_high)) if n - 1 > 0 else 0.0
        if opp_actions.shape[0] >= 1:
            last_round_cooperators = float(np.sum(opp_actions[-1, :]))
            last_round_coop_fraction = last_round_cooperators / max(1, n - 1)
        else:
            last_round_coop_fraction = 0.0
        per_round_frac = np.mean(last_H_actions.astype(float), axis=1)
        avg_window_coop_fraction = float(np.mean(per_round_frac)) if per_round_frac.size > 0 else 0.0
        trend_drop = avg_window_coop_fraction - last_round_coop_fraction >= self.drop_threshold
        my_avg = float(np.mean(my_payoffs[-H:])) if my_payoffs.size >= 1 else 0.0
        others_avg = float(np.mean(opp_payoffs[-H:, :])) if opp_payoffs.size >= 1 else 0.0
        escalate = False
        if my_avg > 0:
            if others_avg > my_avg * (1.0 + self.M_advantage):
                escalate = True
        elif others_avg > 1e-12:
            escalate = True
        if escalate:
            P_seed = max(3, math.ceil(P_seed / 2))
            P_probe = max(6, math.ceil(P_probe / 2))
            P_punish = min(remaining, max(P_punish, math.ceil(P_punish * 1.5)))
        if RC >= self.rc_threshold:
            if P_seed > 0 and t % P_seed == 0:
                return Action.C
            return Action.D
        if last_round_coop_fraction >= 0.6:
            return Action.D
        if last_round_coop_fraction <= 0.2:
            return Action.D
        if trend_drop:
            return Action.D
        if P_probe > 0 and t % P_probe == 0:
            return Action.C
        return Action.D

class Strategy_AGGRESSIVE_116(BaseStrategy):
    """
    Aggressive Probe-and-Exploit with Grim Consequences (APEGC).

    - Brief: Default defect, run brief probes to identify persistent cooperators,
      repeatedly exploit them (defect while they cooperate). If a trusted cooperator
      defects when I cooperated, start a long punishment (defect for punishment_len rounds).
      Periodically re-probe to find new cooperators. Always defect in endgame rounds.
      If k/n >= 1, switch to cooperative-biased fallback (cooperate except during active punishment or endgame).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.always_defect_if_short = self.r <= 3
        self.t_probe = min(3, max(1, math.floor(self.r / 4)))
        self.endgame_len = max(1, math.ceil(self.r / 10))
        self.reprob_interval = max(3, math.ceil(self.r / 5))
        self.window = min(5, max(1, self.r - 1))
        self.coop_freq_threshold = 0.7
        self.punishment_len = max(self.endgame_len, math.ceil(self.r / 10))
        self.trusted_set = set()
        self.last_punish_round = -10 ** 9
        self.last_reprobe_round = -10 ** 9
        self.initial_trust_built = False
        self.last_processed_reprobe_round = -10 ** 9
        self.cooperative_fallback = self.k / self.n >= 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        if self.always_defect_if_short:
            return Action.D
        completed_rounds = 0 if history is None else history.round_number
        if not self.initial_trust_built and history is not None and (completed_rounds >= self.t_probe):
            required = math.ceil(self.coop_freq_threshold * self.t_probe)
            opps = history.opponent_actions
            probe_slice = opps[0:self.t_probe, :]
            if probe_slice.size > 0:
                coop_counts = np.sum(probe_slice, axis=0)
                for j in range(coop_counts.shape[0]):
                    if int(coop_counts[j]) >= required:
                        self.trusted_set.add(int(j))
            self.initial_trust_built = True
        if history is not None and self.last_reprobe_round > 0 and (completed_rounds >= self.last_reprobe_round) and (self.last_processed_reprobe_round < self.last_reprobe_round):
            reprobe_idx = self.last_reprobe_round - 1
            opps = history.opponent_actions
            if reprobe_idx < opps.shape[0]:
                start = max(0, completed_rounds - self.window)
                window_slice = opps[start:completed_rounds, :]
                for j in range(opps.shape[1]):
                    cooperated_in_reprobe = bool(opps[reprobe_idx, j])
                    if not cooperated_in_reprobe:
                        continue
                    if window_slice.size == 0:
                        continue
                    coop_count = int(np.sum(window_slice[:, j]))
                    window_len = window_slice.shape[0]
                    if coop_count >= math.ceil(0.5 * window_len):
                        self.trusted_set.add(int(j))
            self.last_processed_reprobe_round = self.last_reprobe_round
        if t > self.r - self.endgame_len:
            return Action.D
        if t <= self.last_punish_round + self.punishment_len:
            return Action.D
        if self.cooperative_fallback:
            return Action.C
        if t <= self.t_probe:
            return Action.C
        if t > self.t_probe and (t - self.t_probe) % self.reprob_interval == 0 and (t <= self.r - self.endgame_len):
            self.last_reprobe_round = t
            return Action.C
        if len(self.trusted_set) > 0:
            if completed_rounds == 0:
                return Action.D
            prev_opps = history.opponent_actions[-1, :]
            all_trusted_cooperated = True
            for j in self.trusted_set:
                if j < 0 or j >= prev_opps.shape[0]:
                    all_trusted_cooperated = False
                    break
                if not bool(prev_opps[j]):
                    all_trusted_cooperated = False
                    break
            if all_trusted_cooperated:
                return Action.D
            i_cooperated_prev = bool(history.my_actions[-1])
            if i_cooperated_prev:
                betrayers = set()
                for j in self.trusted_set:
                    if j < 0 or j >= prev_opps.shape[0]:
                        betrayers.add(j)
                    elif not bool(prev_opps[j]):
                        betrayers.add(j)
                if len(betrayers) > 0:
                    for b in betrayers:
                        if b in self.trusted_set:
                            self.trusted_set.discard(b)
                    self.last_punish_round = t
                    return Action.D
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_117(BaseStrategy):
    """
    Aggressive Seed-and-Harvest (ASH)

    Defect by default. Probe rarely to estimate whether my cooperation
    increases opponents' cooperation (responsiveness). If a small,
    economically-justified seed (1-round cooperation) is likely to pay off
    over the remaining rounds given observed responsiveness, play that seed.
    If a seed produces an observable increase in others' cooperation, enter
    HARVEST mode: defect persistently to exploit the induced cooperation,
    but monitor group cooperation and exit harvest if it collapses.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.mode = 'DEFAULT'
        self.last_seed_round = None
        self.baseline_group_rate = None
        self.disable_seeding = False
        self.harvest_decay_count = 0
        self.alpha = 1.0
        self.p_probe = 0.05
        self.B_thresh = 0.02
        self.delta_obs = 0.02
        self.tol = 0.02
        self.seed_size = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        k_over_n = float(k) / float(n)
        if history is None:
            t = 1
            completed_rounds = 0
        else:
            completed_rounds = history.round_number
            t = completed_rounds + 1
        r_rem = r - t + 1
        if t == r:
            return Action.D
        T_probe = min(5, max(1, math.floor(r / 10)))
        if history is None or completed_rounds == 0:
            if t <= T_probe and random.random() < self.p_probe:
                return Action.C
            return Action.D
        suggested_w = max(1, r - t + 1)
        w = min(10, suggested_w)
        available_rounds = completed_rounds
        window_size = min(w, available_rounds)
        start_idx = max(0, available_rounds - window_size)
        opp_actions_window = history.opponent_actions[start_idx:available_rounds, :]
        trials = opp_actions_window.shape[0]
        m = history.opponent_actions.shape[1] if history.opponent_actions.ndim == 2 else 0
        if trials <= 0 or m == 0:
            p_j = np.zeros((m,), dtype=float)
        else:
            coops_j = np.sum(opp_actions_window.astype(float), axis=0)
            p_j = (coops_j + self.alpha) / (trials + 2.0 * self.alpha)
        P_group = float(np.mean(p_j)) if p_j.size > 0 else 0.0
        if completed_rounds < 2:
            beta = np.zeros((m,), dtype=float)
        else:
            my_actions = history.my_actions.astype(bool)
            idx_from = np.arange(0, completed_rounds - 1)
            my_act_from = my_actions[idx_from]
            opp_at_next = history.opponent_actions[idx_from + 1, :]
            n_myC = int(np.sum(my_act_from))
            n_myD = int(len(my_act_from) - n_myC)
            if n_myC > 0:
                coops_after_myC = np.sum(opp_at_next[my_act_from, :].astype(float), axis=0)
            else:
                coops_after_myC = np.zeros((m,), dtype=float)
            if n_myD > 0:
                coops_after_myD = np.sum(opp_at_next[~my_act_from, :].astype(float), axis=0)
            else:
                coops_after_myD = np.zeros((m,), dtype=float)
            denom_C = float(n_myC) + 2.0 * self.alpha
            denom_D = float(n_myD) + 2.0 * self.alpha
            beta = (coops_after_myC + self.alpha) / denom_C - (coops_after_myD + self.alpha) / denom_D
        B_est = float(np.sum(np.maximum(0.0, beta))) if beta.size > 0 else 0.0
        C0 = 1.0 - k_over_n
        if t <= T_probe:
            if random.random() < self.p_probe:
                return Action.C
            return Action.D
        if self.disable_seeding or B_est <= self.B_thresh:
            return Action.D
        denom = k_over_n * B_est
        if denom <= 0:
            return Action.D
        required_rounds = 1 + int(math.ceil(C0 / denom))
        if r_rem < required_rounds:
            return Action.D
        if self.mode == 'HARVEST':
            if r_rem < required_rounds:
                self.mode = 'DEFAULT'
                self.harvest_decay_count = 0
                return Action.D
            if self.baseline_group_rate is not None:
                if P_group < self.baseline_group_rate - self.tol:
                    self.harvest_decay_count += 1
                else:
                    self.harvest_decay_count = 0
                if self.harvest_decay_count >= 2:
                    self.mode = 'DEFAULT'
                    self.harvest_decay_count = 0
                    return Action.D
            return Action.D
        if self.last_seed_round is not None and self.last_seed_round == t - 1:
            if self.baseline_group_rate is None:
                self.disable_seeding = True
                return Action.D
            if P_group >= self.baseline_group_rate + self.delta_obs:
                self.mode = 'HARVEST'
                self.harvest_decay_count = 0
                return Action.D
            else:
                self.disable_seeding = True
                return Action.D
        self.baseline_group_rate = P_group
        self.last_seed_round = t
        return Action.C

class Strategy_AGGRESSIVE_118(BaseStrategy):
    """
    Aggressive Probe–Exploit–Punish (APEP)

    Default: defect. Probe briefly (rounds 2-3) to find majority cooperators.
    If a majority of others cooperates in a probe, enter EXPLOIT: keep defecting
    while occasionally re-probing to ensure exploitation remains profitable.
    If a re-probe shows cooperation dropped by more than reprobe_drop_delta,
    enter a short PUNISH phase (collective defection) and then return to DEFAULT.
    Permanently blacklist any opponent who defected in a probe round where the
    majority cooperated. Always defect in the final round. For very short games
    (r <= 3) always defect every round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = int(game_description.n_players)
        self.n_rounds = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.small_game_all_defect_cutoff = 3
        self.initial_probes = min(2, max(0, self.n_rounds - 1))
        self.maj_threshold = math.ceil((self.n_players - 1) / 2)
        self.cooperaterate_threshold = 0.6
        self.exploit_reprobe_interval = max(3, math.floor(self.n_rounds / 4))
        self.punish_length = min(5, max(2, math.floor(self.n_rounds / 6)))
        self.reprobe_drop_delta = 0.33
        self.small_eps = 0.05
        self.mode = 'DEFAULT'
        self.blacklist = set()
        self.C_counts = None
        self.T_processed = 0
        self.positive_probe_count = 0
        self.baseline_coop_fraction = 0.0
        self.next_reprobe_round = None
        self.punish_timer = 0
        self.last_probe_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        completed = 0 if history is None else int(history.round_number)
        current_round = completed + 1
        r = self.n_rounds
        n = self.n_players
        if r <= self.small_game_all_defect_cutoff:
            return Action.D
        if history is not None and completed > self.T_processed:
            for t_completed in range(self.T_processed + 1, completed + 1):
                idx = t_completed - 1
                try:
                    my_act = bool(history.my_actions[idx])
                    opp_acts = history.opponent_actions[idx, :]
                except Exception:
                    my_act = False
                    opp_acts = np.zeros((n - 1,), dtype=np.bool_)
                other_coops = int(np.sum(opp_acts))
                fraction = other_coops / (n - 1) if n - 1 > 0 else 0.0
                remaining_after = max(0, r - t_completed)
                if my_act:
                    if other_coops >= self.maj_threshold:
                        for opp_idx in range(n - 1):
                            try:
                                if not bool(opp_acts[opp_idx]):
                                    self.blacklist.add(int(opp_idx))
                            except Exception:
                                continue
                        if t_completed in (2, 3):
                            self.positive_probe_count += 1
                            prev = self.baseline_coop_fraction
                            if self.positive_probe_count == 1:
                                self.baseline_coop_fraction = fraction
                            else:
                                self.baseline_coop_fraction = (prev * (self.positive_probe_count - 1) + fraction) / self.positive_probe_count
                            self.mode = 'EXPLOIT'
                            self.next_reprobe_round = t_completed + self.exploit_reprobe_interval
                            self.last_probe_round = t_completed
                        elif self.mode == 'EXPLOIT':
                            if self.next_reprobe_round is not None and t_completed == self.next_reprobe_round:
                                if fraction + 1e-09 >= self.baseline_coop_fraction - self.small_eps:
                                    self.baseline_coop_fraction = (self.baseline_coop_fraction + fraction) / 2.0
                                    self.next_reprobe_round = t_completed + self.exploit_reprobe_interval
                                    self.last_probe_round = t_completed
                                elif self.baseline_coop_fraction - fraction > self.reprobe_drop_delta:
                                    allowed_punish = max(0, r - t_completed - 1)
                                    self.punish_timer = min(self.punish_length, allowed_punish)
                                    if self.punish_timer > 0:
                                        self.mode = 'PUNISH'
                                    else:
                                        self.mode = 'DEFAULT'
                                    self.next_reprobe_round = None
                                    self.last_probe_round = t_completed
                                else:
                                    self.next_reprobe_round = t_completed + self.exploit_reprobe_interval
                                    self.last_probe_round = t_completed
                            else:
                                self.baseline_coop_fraction = fraction
                                self.mode = 'EXPLOIT'
                                self.next_reprobe_round = t_completed + self.exploit_reprobe_interval
                                self.last_probe_round = t_completed
                        else:
                            self.positive_probe_count += 1
                            if self.positive_probe_count == 1:
                                self.baseline_coop_fraction = fraction
                            else:
                                prev = self.baseline_coop_fraction
                                self.baseline_coop_fraction = (prev * (self.positive_probe_count - 1) + fraction) / self.positive_probe_count
                            self.mode = 'EXPLOIT'
                            self.next_reprobe_round = t_completed + self.exploit_reprobe_interval
                            self.last_probe_round = t_completed
                    elif self.mode == 'EXPLOIT':
                        if self.baseline_coop_fraction - fraction > self.reprobe_drop_delta:
                            allowed_punish = max(0, r - t_completed - 1)
                            self.punish_timer = min(self.punish_length, allowed_punish)
                            if self.punish_timer > 0:
                                self.mode = 'PUNISH'
                            else:
                                self.mode = 'DEFAULT'
                            self.next_reprobe_round = None
                        else:
                            self.next_reprobe_round = t_completed + self.exploit_reprobe_interval
                            self.last_probe_round = t_completed
                    else:
                        self.last_probe_round = t_completed
                else:
                    pass
            self.T_processed = completed
        if current_round == r:
            return Action.D
        remaining_rounds = r - current_round + 1
        if self.mode == 'PUNISH':
            if self.punish_timer > 0:
                self.punish_timer -= 1
                if self.punish_timer == 0:
                    self.mode = 'DEFAULT'
                return Action.D
            else:
                self.mode = 'DEFAULT'
        if current_round == 1:
            return Action.D
        if current_round in (2, 3) and r > 3:
            return Action.C
        if self.mode == 'EXPLOIT':
            if self.next_reprobe_round is not None and current_round == self.next_reprobe_round and (current_round < r):
                return Action.C
            else:
                return Action.D
        if self.mode == 'DEFAULT':
            cooldown_ok = True
            if self.last_probe_round > 0:
                cooldown_ok = current_round - self.last_probe_round >= self.exploit_reprobe_interval
            min_remaining_for_probe = math.floor(r / 5)
            if remaining_rounds > max(self.exploit_reprobe_interval + 1, min_remaining_for_probe) and cooldown_ok:
                if self.last_probe_round == 0:
                    if current_round >= 4 and (current_round - 4) % self.exploit_reprobe_interval == 0:
                        return Action.C
                elif current_round - self.last_probe_round >= self.exploit_reprobe_interval:
                    return Action.C
            return Action.D
        return Action.D

class Strategy_AGGRESSIVE_119(BaseStrategy):
    """
    Adaptive Exploiter with Probes and Punishment (AEP²)

    Aggressive, defect-by-default strategy that:
    - defects by default and in endgame,
    - exploits clearly cooperative populations,
    - performs occasional probes to test whether others respond to my cooperation,
    - if probes show positive responsiveness, briefly attempts coordinated cooperation,
    - punishes groups after sharp coordinated drops in cooperation.

    Internal FSM states: 'NORMAL', 'COOP', 'PUNISH'.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(10, max(1, self.r - 1))
        self.S = max(3, round(math.sqrt(max(1, self.r))))
        self.p_exploit_high = 0.7
        self.p_probe_low = 0.25
        self.p_probe_high = 0.7
        self.p_coop_switch = 0.8
        self.delta_resp = 0.2
        self.L_coop_default = 3
        self.P_punish_default = 5
        self.epsilon_probe = 0.05
        self.endgame_margin = 1
        if self.r < 10:
            self.L_coop = max(1, round(0.1 * self.r))
            self.P_punish = max(1, round(0.15 * self.r))
        else:
            self.L_coop = self.L_coop_default
            self.P_punish = self.P_punish_default
        self.mode = 'NORMAL'
        self.coop_run_remaining = 0
        self.punish_remaining = 0
        self.probes = []
        self.probe_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = self.n
        r = self.r

        def others_coop_rate_at_round(idx_1_indexed: int, hist: PlayerHistory) -> float:
            if hist is None:
                return 0.0
            idx = idx_1_indexed - 1
            if idx < 0 or idx >= hist.round_number:
                return 0.0
            if hist.opponent_actions.size == 0:
                return 0.0
            return float(np.mean(hist.opponent_actions[idx, :]))

        def recent_others_rate_from_history(hist: PlayerHistory, upto_round_exclusive_1idx: int) -> float:
            if hist is None or hist.round_number == 0:
                return 0.0
            end_idx = upto_round_exclusive_1idx - 1
            if end_idx <= 0:
                return 0.0
            start_idx = max(1, end_idx - self.W + 1)
            rates = []
            for u in range(start_idx, end_idx + 1):
                rates.append(others_coop_rate_at_round(u, hist))
            if not rates:
                return 0.0
            return float(np.mean(rates))
        if history is None or history.round_number == 0:
            return Action.D
        for p in self.probes:
            if p.get('response', None) is None:
                probe_round = p['round']
                if history.round_number >= probe_round + 1:
                    after_rate = others_coop_rate_at_round(probe_round + 1, history)
                    response = after_rate - p.get('baseline', 0.0)
                    p['response'] = float(response)
        if t >= r - self.endgame_margin + 1:
            self.mode = 'NORMAL'
            self.coop_run_remaining = 0
            self.punish_remaining = 0
            return Action.D
        last_round = history.round_number
        if last_round >= 1:
            baseline_before_last = recent_others_rate_from_history(history, upto_round_exclusive_1idx=last_round)
            last_rate = others_coop_rate_at_round(last_round, history)
            if self.mode != 'PUNISH':
                if last_rate <= baseline_before_last - 0.3:
                    remaining_rounds_in_game = max(0, r - t + 1)
                    self.mode = 'PUNISH'
                    self.punish_remaining = min(self.P_punish, remaining_rounds_in_game)
                    if self.punish_remaining > 0:
                        self.punish_remaining -= 1
                        if self.punish_remaining == 0:
                            self.mode = 'NORMAL'
                        return Action.D
        if self.mode == 'COOP' and self.coop_run_remaining > 0:
            self.coop_run_remaining -= 1
            if self.coop_run_remaining == 0:
                self.mode = 'NORMAL'
            return Action.C
        if self.mode == 'PUNISH' and self.punish_remaining > 0:
            self.punish_remaining -= 1
            if self.punish_remaining == 0:
                self.mode = 'NORMAL'
            return Action.D
        recent_others_rate = recent_others_rate_from_history(history, upto_round_exclusive_1idx=t)
        if recent_others_rate >= self.p_exploit_high:
            return Action.D
        if self.p_probe_low <= recent_others_rate < self.p_probe_high:
            if t % self.S == 0:
                baseline = recent_others_rate
                self.probes.append({'round': t, 'baseline': float(baseline), 'response': None})
                self.probe_count += 1
                return Action.C
            else:
                return Action.D
        if recent_others_rate >= self.p_coop_switch:
            responses = [p['response'] for p in self.probes if p.get('response') is not None]
            avg_resp = float(np.mean(responses)) if responses else 0.0
            if len(responses) >= 1 and avg_resp >= self.delta_resp:
                self.mode = 'COOP'
                self.coop_run_remaining = max(0, self.L_coop - 1)
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_120(BaseStrategy):
    """
    Aggressive Adaptive Exploiter with Grim Shield (AAE-GS).

    - Defaults to defecting and only rarely probes.
    - Attempts short BUILD phases when recent opponent cooperation and remaining rounds make it profitable.
    - Enters EXPLOIT when group cooperation is high: defects mostly, probing occasionally.
    - Detects opponents who retaliate after my defections; if credible collective punishment is detected,
      sets a global grim-shield and defects forever.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(10, max(1, self.r - 1))
        self.coop_threshold_high = 0.6
        self.coop_threshold_low = 0.4
        self.B_build = 2
        self.punishment_drop_alpha = 0.25
        self.probe_probability_base = 0.08
        self.global_permanent_defect_flag = False
        self.mode = 'DEFAULT'
        self.build_rounds_left = 0
        self.pre_build_group_coop = None
        self.punisher_flags = [False] * max(0, self.n - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        T_rem = self.r - t + 1
        if t == 1:
            return Action.D
        if t == self.r:
            return Action.D
        if self.global_permanent_defect_flag:
            return Action.D
        rounds_played = history.round_number
        window_len = min(self.W, rounds_played)
        start_idx = max(0, rounds_played - window_len)
        opp_actions_window = history.opponent_actions[start_idx:start_idx + window_len, :]
        if opp_actions_window.size == 0:
            coop_rate_j = np.zeros((max(0, self.n - 1),), dtype=float)
        else:
            coop_rate_j = np.array(np.mean(opp_actions_window.astype(float), axis=0), dtype=float)
        coop_rate_group = float(np.mean(coop_rate_j)) if coop_rate_j.size > 0 else 0.0
        if rounds_played >= 1:
            last_round_cooperators = int(np.sum(history.opponent_actions[-1, :]))
        else:
            last_round_cooperators = 0
        if rounds_played >= 2:
            my_action_defection_round = bool(history.my_actions[-2])
            if not my_action_defection_round:
                prior_window_len = min(self.W, rounds_played - 1)
                if prior_window_len > 0:
                    prior_start = max(0, rounds_played - 1 - prior_window_len)
                    prior_slice = history.opponent_actions[prior_start:prior_start + prior_window_len, :]
                    if prior_slice.size > 0:
                        coop_rate_prior = np.array(np.mean(prior_slice.astype(float), axis=0), dtype=float)
                        response = history.opponent_actions[-1, :].astype(float)
                        drops = coop_rate_prior - response
                        punishers_this_round = drops >= self.punishment_drop_alpha
                        for j in range(len(self.punisher_flags)):
                            if punishers_this_round[j]:
                                self.punisher_flags[j] = True
                        num_punishers = int(np.sum(punishers_this_round))
                        threshold_count = math.ceil((self.n - 1) * 0.25)
                        if threshold_count < 1:
                            threshold_count = 1
                        if num_punishers >= threshold_count:
                            self.global_permanent_defect_flag = True
        if self.mode == 'BUILD':
            if any(self.punisher_flags):
                self.mode = 'DEFAULT'
                self.build_rounds_left = 0
                self.pre_build_group_coop = None
                if self.global_permanent_defect_flag:
                    return Action.D
            elif self.build_rounds_left > 0:
                self.build_rounds_left -= 1
                return Action.C
            else:
                success = False
                if self.pre_build_group_coop is not None:
                    if coop_rate_group - self.pre_build_group_coop >= 0.15:
                        success = True
                self.mode = 'DEFAULT'
                self.pre_build_group_coop = None
                if success and (not any(self.punisher_flags)):
                    return Action.C
                else:
                    return Action.D
        majority_threshold = math.ceil((self.n - 1) * 0.5)
        if coop_rate_group >= self.coop_threshold_high or last_round_cooperators >= majority_threshold:
            self.mode = 'EXPLOIT'
            if any(self.punisher_flags):
                if self.global_permanent_defect_flag:
                    return Action.D
                return Action.D
            p_probe = min(self.probe_probability_base, 3.0 / max(1, T_rem))
            if random.random() < p_probe:
                return Action.C
            else:
                return Action.D
        if coop_rate_group >= self.coop_threshold_low and (self.k - 1.0) * float(T_rem) > float(self.B_build) and (not any(self.punisher_flags)):
            self.mode = 'BUILD'
            self.build_rounds_left = max(0, self.B_build - 1)
            self.pre_build_group_coop = coop_rate_group
            return Action.C
        p_probe_default = min(0.02, 2.0 / max(1, T_rem))
        if not any(self.punisher_flags) and random.random() < p_probe_default:
            return Action.C
        else:
            return Action.D

class Strategy_AGGRESSIVE_121(BaseStrategy):
    """
    Aggressive-adaptive public-goods strategy: default defect, probe occasionally,
    exploit sustained cooperation with repeated defection and periodic probes,
    detect and avoid costly punishers via a cautious conditional rule, and always
    defect in the final ENDGAME_ROUNDS.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.PROBE_ROUNDS = min(3, max(0, self.r - 1))
        self.TEST_INTERVAL = 5
        self.EXPLOIT_MIN_BURST = 1
        self.RECIPROCITY_THRESHOLD_HIGH = 0.6
        self.RECIPROCITY_THRESHOLD_LOW = 0.2
        self.PUNISH_DELTA = 0.25
        self.PUNISH_WINDOW = 3
        self.ENDGAME_ROUNDS = 1
        self.prev_mode = None
        self.last_probe_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if self.r <= 3:
            self.prev_mode = 'DEFAULT'
            return Action.D
        if t > self.r - self.ENDGAME_ROUNDS:
            self.prev_mode = 'DEFAULT'
            return Action.D
        if history is None:
            self.prev_mode = 'DEFAULT'
            return Action.D
        my_actions = np.asarray(history.my_actions, dtype=bool)
        opp_actions = np.asarray(history.opponent_actions, dtype=bool)
        T = my_actions.shape[0]
        if T > 0:
            coop_rate_others = opp_actions.mean(axis=1)
            overall_coop_rate = float(coop_rate_others.mean())
        else:
            coop_rate_others = np.array([], dtype=float)
            overall_coop_rate = 0.0
        after_defect = []
        after_coop = []
        for i in range(T - 1):
            if not my_actions[i]:
                after_defect.append(float(coop_rate_others[i + 1]))
            else:
                after_coop.append(float(coop_rate_others[i + 1]))
        if len(after_defect) == 0:
            coop_rate_after_my_defect = overall_coop_rate
        else:
            coop_rate_after_my_defect = float(np.mean(after_defect))
        if len(after_coop) == 0:
            coop_rate_after_my_coop = overall_coop_rate
        else:
            coop_rate_after_my_coop = float(np.mean(after_coop))
        immediate_response = coop_rate_after_my_coop - coop_rate_after_my_defect
        punishers_detected = immediate_response >= self.PUNISH_DELTA
        if punishers_detected:
            mode = 'CAUTIOUS'
        elif overall_coop_rate >= self.RECIPROCITY_THRESHOLD_HIGH:
            mode = 'EXPLOIT'
        else:
            mode = 'DEFAULT'
        action = Action.D
        just_entered_exploit = self.prev_mode != 'EXPLOIT' and mode == 'EXPLOIT'
        if mode == 'EXPLOIT':
            if just_entered_exploit:
                action = Action.C
                self.last_probe_round = t
            elif self.TEST_INTERVAL > 0 and (t - self.last_probe_round) % self.TEST_INTERVAL == 0:
                action = Action.C
                self.last_probe_round = t
            else:
                action = Action.D
        elif mode == 'DEFAULT':
            if T <= self.PROBE_ROUNDS:
                if self.TEST_INTERVAL > 0 and (t - self.last_probe_round) % self.TEST_INTERVAL == 0:
                    action = Action.C
                    self.last_probe_round = t
                else:
                    action = Action.D
            else:
                action = Action.D
        elif mode == 'CAUTIOUS':
            if T >= 1:
                last_round_coop_rate = float(coop_rate_others[-1])
                if last_round_coop_rate >= 0.5:
                    action = Action.C
                else:
                    action = Action.D
            else:
                action = Action.D
        self.prev_mode = mode
        return action

class Strategy_AGGRESSIVE_122(BaseStrategy):
    """
    Aggressive Opportunistic Exploiter (AOE)

    Summary:
    - Default: defect.
    - Identify "exploitable" opponents (high recent cooperation, low retaliation) using a sliding window.
    - If enough exploitable opponents exist, exploit them by defecting.
    - If group recent cooperation looks strong and no recent punishment, run occasional cooperation tests and, if successful, sustain conditional cooperation for a limited time.
    - If opponents punish my defections (many show retaliation), enter a finite punishment phase (defect for P_pun rounds), increase punishment severity for repeat punishers, then resume with a cooling-off before testing again.
    - Never cooperate in the final Endgame_horizon rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(10, max(3, math.floor(self.r / 5)))
        self.P_exploit = 0.7
        self.R_tol = 0.3
        self.M_exploit = max(1, math.ceil((self.n - 1) / 4))
        self.base_P_pun = min(5, max(1, math.floor(self.r / 10)))
        self.P_pun_current = self.base_P_pun
        self.P_pun_cap = max(self.base_P_pun, 10)
        self.Coop_test_interval = max(3, math.floor(self.r / 10))
        self.Endgame_horizon = 2
        self.punishment_timer = 0
        self.last_test_round = -10000
        self.last_test_evaluated_round = -10000
        self.cooperative_mode = False
        self.sustain_timer = 0
        self.per_opponent_reset_round = dict()
        self.last_punish_round = -10000
        self.P_detect_count = max(1, math.ceil((self.n - 1) / 6))
        self.adaptive_counter = 0
        self.min_coop_test_interval = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        rounds_played = history.round_number
        current_round = rounds_played + 1
        rounds_left = self.r - rounds_played
        if rounds_left <= self.Endgame_horizon:
            self.cooperative_mode = False
            self.sustain_timer = 0
            return Action.D
        if self.punishment_timer > 0:
            self.punishment_timer -= 1
            if self.punishment_timer == 0:
                self.last_punish_round = current_round
                self.last_test_round = current_round
            return Action.D
        if self.last_test_round > self.last_test_evaluated_round and rounds_played >= self.last_test_round:
            test_idx = self.last_test_round - 1
            if 0 <= test_idx < rounds_played:
                coop_count = int(np.sum(history.opponent_actions[test_idx, :]))
                coop_threshold = math.ceil(0.6 * (self.n - 1))
                retaliate_counts = []
                for j in range(self.n - 1):
                    retaliate_j = self._compute_retaliation_rate(j, history, rounds_played)
                    retaliate_counts.append(retaliate_j)
                punishers_recent = sum((1 for r in retaliate_counts if r > self.R_tol))
                if coop_count >= coop_threshold and punishers_recent <= self.P_detect_count:
                    self.cooperative_mode = True
                    max_possible = max(0, rounds_left - self.Endgame_horizon)
                    self.sustain_timer = max_possible
                elif coop_count < coop_threshold:
                    self.P_pun_current = min(self.P_pun_cap, self.P_pun_current + 1)
                    self.punishment_timer = self.P_pun_current
                    self.last_punish_round = current_round
                    self.last_test_round = current_round
            self.last_test_evaluated_round = self.last_test_round
        if self.cooperative_mode:
            prev_coop_last = int(np.sum(history.opponent_actions[-1, :])) if rounds_played >= 1 else 0
            coop_threshold = math.ceil(0.6 * (self.n - 1))
            if prev_coop_last >= coop_threshold and self.sustain_timer > 0 and (rounds_left > self.Endgame_horizon):
                self.sustain_timer -= 1
                if self.sustain_timer <= 0:
                    self.cooperative_mode = False
                return Action.C
            else:
                self.cooperative_mode = False
                self.sustain_timer = 0
        exploitable_flags = []
        retaliate_list = []
        p_list = []
        for j in range(self.n - 1):
            p_j = self._compute_coop_rate(j, history, rounds_played)
            retaliate_j = self._compute_retaliation_rate(j, history, rounds_played)
            p_list.append(p_j)
            retaliate_list.append(retaliate_j)
            exploitable_flags.append(p_j >= self.P_exploit and retaliate_j <= self.R_tol)
        E = sum((1 for f in exploitable_flags if f))
        if E >= self.M_exploit:
            self._maybe_adjust_test_interval(history, rounds_played)
            return Action.D
        prev_coop_last = int(np.sum(history.opponent_actions[-1, :])) if rounds_played >= 1 else 0
        coop_threshold = math.ceil(0.6 * (self.n - 1))
        since_last_test = current_round - self.last_test_round
        since_last_punish = current_round - self.last_punish_round
        can_test = since_last_test >= self.Coop_test_interval and since_last_punish >= self.Coop_test_interval
        punishers_recent = sum((1 for r in retaliate_list if r > self.R_tol))
        no_recent_punishment = punishers_recent <= self.P_detect_count
        if prev_coop_last >= coop_threshold and can_test and no_recent_punishment and (rounds_left > self.Endgame_horizon):
            self.last_test_round = current_round
            return Action.C
        punishers_recent = sum((1 for r in retaliate_list if r > self.R_tol))
        if punishers_recent > self.P_detect_count:
            self.punishment_timer = self.P_pun_current
            self.P_pun_current = min(self.P_pun_cap, self.P_pun_current + 1)
            self.last_punish_round = current_round
            for j, rrate in enumerate(retaliate_list):
                if rrate > self.R_tol:
                    self.per_opponent_reset_round[j] = rounds_played
            self.last_test_round = current_round
            return Action.D
        return Action.D

    def _compute_coop_rate(self, j: int, history: PlayerHistory, rounds_played: int) -> float:
        window_start = max(0, rounds_played - self.W)
        reset_round = self.per_opponent_reset_round.get(j, 0)
        if reset_round > window_start:
            window_start = reset_round
        window_end = rounds_played
        if window_end <= window_start:
            return 0.0
        seg = history.opponent_actions[window_start:window_end, j]
        if seg.size == 0:
            return 0.0
        return float(np.mean(seg))

    def _compute_retaliation_rate(self, j: int, history: PlayerHistory, rounds_played: int) -> float:
        start_t = max(0, rounds_played - self.W - 1)
        end_t = rounds_played - 1
        events = 0
        retaliations = 0
        for t in range(start_t, end_t):
            reset_round = self.per_opponent_reset_round.get(j, 0)
            if t + 1 < reset_round:
                continue
            my_action_t = bool(history.my_actions[t])
            if my_action_t is False:
                events += 1
                opp_next = bool(history.opponent_actions[t + 1, j])
                if opp_next is False:
                    retaliations += 1
        if events == 0:
            return 0.0
        return float(retaliations) / float(events)

    def _maybe_adjust_test_interval(self, history: PlayerHistory, rounds_played: int):
        W = max(2, min(self.W, rounds_played))
        if rounds_played < 2:
            return
        last_start = max(0, rounds_played - W)
        last_pay = float(np.mean(history.my_payoffs[last_start:rounds_played])) if rounds_played - last_start > 0 else float(history.my_payoffs[-1])
        init_end = min(W, rounds_played)
        init_pay = float(np.mean(history.my_payoffs[0:init_end])) if init_end > 0 else last_pay
        if init_pay > 0 and (init_pay - last_pay) / init_pay > 0.1:
            self.Coop_test_interval = max(self.min_coop_test_interval, self.Coop_test_interval - 1)
        else:
            default = max(3, math.floor(self.r / 10))
            if self.Coop_test_interval < default:
                self.Coop_test_interval = min(default, self.Coop_test_interval + 1)

class Strategy_AGGRESSIVE_123(BaseStrategy):
    """
    Aggressive Conditional Exploiter (ACE)

    - Defaults to defect.
    - Detects strong coalitions of cooperators among opponents and will join them
      but performs a single one-shot exploit (defect once) at the start of each
      detected coalition episode.
    - Maintains a blacklist of opponents who often defected when this agent cooperated;
      while anyone is blacklisted the agent defects until they show sustained cooperation.
    - Applies short-group punishments when group cooperation collapses.
    - Always defects in the final F rounds and in very short games (r <= 3).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        self.n = int(n)
        self.r = int(r)
        self.k = float(k)
        self.L = min(5, self.r)
        self.Theta_high = max(0.7, 1.0 - 0.5 * (self.k / max(1.0, self.n)))
        self.Theta_low = 0.5
        self.S = min(3, self.r)
        self.P = min(3, self.r)
        self.M = 2
        self.F = min(2, self.r)
        self.G = 2
        self.blacklist = set()
        self.exploited_this_coalition = False
        self.last_coalition_start_round = None
        self.punishment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if t > self.r - self.F:
            return Action.D
        if self.r <= 3:
            return Action.D
        if t == 1 or history is None:
            return Action.D
        completed = history.round_number
        window_size = min(self.L, completed)
        start_idx = completed - window_size
        end_idx = completed
        my_actions = history.my_actions
        opp_actions = history.opponent_actions
        if window_size == 0:
            return Action.D
        window_my = my_actions[start_idx:end_idx]
        window_opp = opp_actions[start_idx:end_idx, :]
        num_opponents = max(1, self.n - 1)
        coop_counts_per_round = np.sum(window_opp, axis=1)
        coop_fraction_others = coop_counts_per_round / float(num_opponents)
        avg_coop_others = float(np.mean(coop_fraction_others)) if coop_fraction_others.size > 0 else 0.0
        consec_high = 0
        for val in coop_fraction_others[::-1]:
            if val >= self.Theta_high:
                consec_high += 1
            else:
                break
        consec_low = 0
        for val in coop_fraction_others[::-1]:
            if val < self.Theta_low:
                consec_low += 1
            else:
                break
        coop_round_indices = np.where(window_my)[0]
        if coop_round_indices.size > 0:
            opp_when_I_coop = window_opp[coop_round_indices, :]
            opp_coop_counts = np.sum(opp_when_I_coop, axis=0)
            num_coop_rounds = coop_round_indices.size
            opp_defect_share = 1.0 - opp_coop_counts / float(num_coop_rounds)
            for j in range(opp_defect_share.size):
                if opp_defect_share[j] > 0.5:
                    self.blacklist.add(int(j))
        if completed >= self.G and len(self.blacklist) > 0:
            recent_window_for_rehab = opp_actions[-self.G:, :]
            for j in list(self.blacklist):
                if np.all(recent_window_for_rehab[:, j]):
                    self.blacklist.discard(j)
        if len(self.blacklist) > 0:
            return Action.D
        coalition_present = consec_high >= self.S and avg_coop_others >= self.Theta_high
        if coalition_present:
            if self.last_coalition_start_round is None or self.last_coalition_start_round < t - self.L:
                self.last_coalition_start_round = t
                self.exploited_this_coalition = False
            if not self.exploited_this_coalition:
                self.exploited_this_coalition = True
                return Action.D
            else:
                return Action.C
        if consec_low >= 1:
            if self.punishment_counter < self.P:
                self.punishment_counter += 1
                return Action.D
            elif consec_high >= self.M and avg_coop_others >= self.Theta_high:
                self.punishment_counter = 0
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_AGGRESSIVE_124(BaseStrategy):
    """
    Aggressive Grim with Endgame Exploit (AG-EE)

    - Start by cooperating to signal willingness.
    - If any player has ever defected, switch to permanent defection (grim trigger).
    - If everyone has cooperated so far and the current round is inside the final F = max(1, floor(r/3)) rounds, defect to exploit.
    - Always defect on the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        t = state.round_number
        r = self.game_description.n_rounds
        F = max(1, math.floor(r / 3))
        if t == r:
            return Action.D
        ever_defected = False
        if history.round_number > 0:
            if not np.all(history.my_actions):
                ever_defected = True
            elif history.opponent_actions.size > 0 and (not np.all(history.opponent_actions)):
                ever_defected = True
        if ever_defected:
            return Action.D
        if t == 1:
            return Action.C
        if t > r - F:
            return Action.D
        return Action.C

class Strategy_AGGRESSIVE_125(BaseStrategy):
    """
    Aggressive Predator (AP)

    - Defaults to defect.
    - Detects a sufficiently large set of "trusted" cooperators (based on recent W rounds).
    - If a trusted cluster of size >= M exists and enough rounds remain, enters coop_mode to
      cooperate with them for short streaks.
    - After cooperating for 'harvest_streak' rounds, performs a one-shot defection (harvest),
      then sets a punishment_timer to retaliate / deter exploitation for P rounds.
    - If any trusted cooperator defects while we cooperated, immediately enter punishment.
    - Always defects in the final round. First round defects.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.W = min(5, max(1, self.r - 1))
        self.trust_threshold = 0.8
        self.trust_count_fraction = 0.7
        self.M = math.ceil(self.trust_count_fraction * (self.n - 1))
        G = 1.0 - self.k / float(self.n)
        L = self.k - 1.0
        if L <= 0:
            self.P = self.r
        else:
            self.P = min(self.r, max(1, math.ceil(G / L) + 1))
        self.harvest_streak = 2
        self.endgame_safe_margin = self.P
        self.punishment_timer = 0
        self.coop_mode = False
        self.coop_streak = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.D
        rounds_completed = history.round_number
        t = rounds_completed + 1
        rem = self.r - t + 1

        def compute_trust_set():
            if rounds_completed <= 0:
                return set()
            last_k = min(self.W, rounds_completed)
            opp_slice = history.opponent_actions[-last_k:, :]
            p_js = np.mean(opp_slice.astype(float), axis=0)
            S = {j for j, pj in enumerate(p_js) if pj >= self.trust_threshold}
            return S
        S_trust = compute_trust_set()
        if rounds_completed >= 1:
            we_cooperated_last = bool(history.my_actions[-1])
            if we_cooperated_last and len(S_trust) > 0:
                opp_last = history.opponent_actions[-1, :]
                betrayed = any((not bool(opp_last[j]) for j in S_trust))
                if betrayed:
                    self.punishment_timer = min(self.P, max(0, rem - 1))
                    self.coop_mode = False
                    self.coop_streak = 0
        if self.punishment_timer > 0:
            action = Action.D
            self.punishment_timer = max(0, self.punishment_timer - 1)
            self.coop_mode = False
            self.coop_streak = 0
            return action
        if t == self.r:
            self.coop_mode = False
            self.coop_streak = 0
            return Action.D
        S_trust = compute_trust_set()
        trusted_count = len(S_trust)
        if trusted_count < self.M or rem <= self.endgame_safe_margin:
            self.coop_mode = False
            self.coop_streak = 0
            return Action.D
        if not self.coop_mode:
            if rounds_completed >= 1 and bool(history.my_actions[-1]):
                if trusted_count > 0:
                    opp_last = history.opponent_actions[-1, :]
                    coop_among_trusted = sum((bool(opp_last[j]) for j in S_trust))
                    if coop_among_trusted >= math.ceil(0.5 * trusted_count):
                        self.coop_mode = True
                        self.coop_streak = 1
                    else:
                        self.coop_mode = False
                        self.coop_streak = 0
                else:
                    self.coop_mode = False
                    self.coop_streak = 0
            else:
                self.coop_mode = False
                self.coop_streak = 0
        if self.coop_mode:
            if self.coop_streak >= self.harvest_streak:
                action = Action.D
                self.punishment_timer = min(self.P, max(0, rem - 1))
                self.coop_mode = False
                self.coop_streak = 0
                return action
            else:
                action = Action.C
                self.coop_streak += 1
                return action
        self.coop_mode = False
        self.coop_streak = 0
        return Action.D

class Strategy_AGGRESSIVE_126(BaseStrategy):
    """
    Aggressive Opportunist (AggOp)

    Start guarded, probe rarely to find naive cooperators, cooperate only when the
    group has reliably demonstrated willingness to cooperate, and whenever exploited
    while cooperating switch to permanent, harsh punishment (grim trigger). Always
    defect in the last L_end rounds. Small randomized probes (p_probe) are used to
    discover potential cooperators.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = int(min(5, max(1, math.floor(self.r / 5))))
        self.theta_high = 0.8
        self.p_probe = 0.05
        self.L_end = 1
        self.punished = False
        self.last_action_self = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or history.round_number == 0:
            self.punished = False
            self.last_action_self = False
            return Action.D
        recent_my_action = bool(history.my_actions[-1])
        recent_opponent_actions = history.opponent_actions[-1, :]
        recent_others_coop_count = int(np.sum(recent_opponent_actions))
        self.last_action_self = recent_my_action
        if recent_my_action is True and recent_others_coop_count < self.n - 1:
            self.punished = True
        t = history.round_number + 1
        if t > self.r - self.L_end:
            self.last_action_self = False
            return Action.D
        if self.punished:
            self.last_action_self = False
            return Action.D
        if t == 1:
            self.last_action_self = False
            return Action.D
        s0 = max(1, t - self.w)
        start_idx = s0 - 1
        end_idx = t - 2
        rounds_counted = end_idx - start_idx + 1
        if rounds_counted <= 0:
            F = 0.0
        else:
            window_opponent_slice = history.opponent_actions[start_idx:end_idx + 1, :]
            total_other_coop = int(np.sum(window_opponent_slice))
            denom = rounds_counted * (self.n - 1)
            F = float(total_other_coop) / float(denom) if denom > 0 else 0.0
        universal_last = bool(recent_my_action and np.all(recent_opponent_actions))
        choose_cooperate = False
        if universal_last or F >= self.theta_high:
            if self.last_action_self is True and recent_others_coop_count < self.n - 1:
                self.punished = True
                choose_cooperate = False
            else:
                choose_cooperate = True
        elif random.random() < self.p_probe:
            choose_cooperate = True
        else:
            choose_cooperate = False
        self.last_action_self = bool(choose_cooperate)
        return Action.C if choose_cooperate else Action.D

class Strategy_AGGRESSIVE_127(BaseStrategy):
    """
    Aggressive Opportunistic Exploiter (AOE)

    Adaptive mostly-defect strategy for the N-player public goods game.
    Exploits high group cooperation, probes occasionally, and punishes
    coordinated retaliation with escalating short punishment blocks.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        r = max(2, int(self.game_description.n_rounds))
        self.r = r
        self.n = int(self.game_description.n_players)
        self.W = min(5, max(1, r - 1))
        self.gamma_high = 0.75
        self.gamma_mid = 0.4
        self.gamma_low = 0.15
        self.delta_retaliation = 0.3
        self.P_init = 2
        pmax_candidate = int(max(2, min(10, r / 4)))
        pmax_cap = max(2, r // 3)
        self.P_max = min(pmax_candidate, pmax_cap)
        self.probe_prob_mid = 0.2
        self.periodic_probe_period = 4
        self.forgiveness_probe_prob = 0.25
        self.punish_remaining = 0
        self.escalation_count = 0
        self.last_round_I_defected = None
        self.baseline_coop_rate_before_exploit = None
        self.just_ended_punishment = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        t = int(state.round_number)
        n = self.n
        r = self.r
        if t == r:
            self.punish_remaining = 0
            self.just_ended_punishment = False
            return Action.D
        if history is None:
            self.last_round_I_defected = 1
            return Action.D
        completed_rounds = history.round_number
        if self.last_round_I_defected is not None and completed_rounds >= self.last_round_I_defected + 1:
            idx_after = self.last_round_I_defected
            if 0 <= idx_after < completed_rounds:
                others_actions_after = history.opponent_actions[idx_after, :]
                new_coop_rate = float(np.sum(others_actions_after) / max(1, n - 1))
                base = self.baseline_coop_rate_before_exploit if self.baseline_coop_rate_before_exploit is not None else None
                if base is not None:
                    if new_coop_rate <= base * (1.0 - self.delta_retaliation):
                        self.escalation_count = max(1, self.escalation_count)
                        self.escalation_count += 1 if self.escalation_count >= 1 else 0
                        esc = self.escalation_count
                        punish_length = int(min(self.P_max, self.P_init * 2 ** (esc - 1)))
                        max_allowed = max(0, r - t)
                        punish_length = min(punish_length, max_allowed)
                        if punish_length > 0:
                            self.punish_remaining = punish_length
                            self.just_ended_punishment = False
                        self.baseline_coop_rate_before_exploit = None
                else:
                    pass
        if self.punish_remaining > 0:
            action = Action.D
            self.punish_remaining -= 1
            if self.punish_remaining == 0:
                self.just_ended_punishment = True
            return action
        if self.just_ended_punishment:
            self.just_ended_punishment = False
            if random.random() < self.forgiveness_probe_prob:
                return Action.C
        W = min(self.W, max(1, completed_rounds))
        start_idx = max(0, completed_rounds - W)
        recent_window = history.opponent_actions[start_idx:completed_rounds, :]
        if recent_window.size == 0:
            recent_coop_rate = 0.0
        else:
            per_round_frac = np.sum(recent_window, axis=1) / max(1, n - 1)
            recent_coop_rate = float(np.mean(per_round_frac))
        individual_coop_freq = None
        if completed_rounds > 0:
            coop_counts = np.sum(history.opponent_actions[:completed_rounds, :], axis=0)
            individual_coop_freq = coop_counts / float(completed_rounds)
            extremely_cooperative = np.any(individual_coop_freq >= 0.9)
        else:
            extremely_cooperative = False
        action = Action.D
        if recent_coop_rate >= self.gamma_high:
            action = Action.D
            self.last_round_I_defected = t
            self.baseline_coop_rate_before_exploit = recent_coop_rate
        elif recent_coop_rate >= self.gamma_mid:
            if random.random() < self.probe_prob_mid:
                action = Action.C
            else:
                action = Action.D
                self.last_round_I_defected = t
                self.baseline_coop_rate_before_exploit = recent_coop_rate
        elif recent_coop_rate >= self.gamma_low:
            if self.periodic_probe_period > 0 and t % self.periodic_probe_period == 0:
                action = Action.C
            else:
                action = Action.D
                self.last_round_I_defected = t
                self.baseline_coop_rate_before_exploit = recent_coop_rate
        elif extremely_cooperative:
            action = Action.D
            self.last_round_I_defected = t
            self.baseline_coop_rate_before_exploit = recent_coop_rate
        else:
            action = Action.D
        return action

class Strategy_AGGRESSIVE_128(BaseStrategy):
    """
    Aggressive Grim-Prober for the N-player Public Goods Game.

    - Default: defect.
    - Periodically probe by cooperating for one round (T_probe interval).
    - If a probe is reciprocated by at least required_m other cooperators, enter COOP_MODE.
    - In COOP_MODE, cooperate only when recent average other cooperators >= required_m.
      If ever exploited (you cooperated in a recent round while other_cooperators < required_m),
      switch to PERM_DEFECT and never cooperate again.
    - Always defect in the final L_end rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = int(game_description.n_players)
        self.r = int(game_description.n_rounds)
        self.k = float(game_description.k)
        self.w = min(5, self.r)
        ratio = self.k / float(self.n)
        if ratio >= 0.6:
            self.theta = 0.6
        elif ratio <= 0.4:
            self.theta = 0.85
        else:
            self.theta = 0.85 + (ratio - 0.4) * (0.6 - 0.85) / 0.2
        self.required_m = math.ceil(self.theta * (self.n - 1))
        self.T_probe = max(3, math.ceil(self.r / 10))
        self.L_end = min(3, max(1, math.ceil(0.05 * self.r)))
        self.mode = 'DEFECT_MODE'
        self.rounds_since_last_probe = self.T_probe
        self.last_round_was_probe = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            t = 1
        else:
            t = history.round_number + 1
        if history is None:
            self.rounds_since_last_probe += 1
            self.last_round_was_probe = False
            return Action.D
        if self.last_round_was_probe and history.round_number >= 1:
            try:
                other_cooperators_last = int(np.sum(history.opponent_actions[-1, :]))
            except Exception:
                other_cooperators_last = 0
            if other_cooperators_last >= self.required_m:
                self.mode = 'COOP_MODE'
            self.last_round_was_probe = False
        if t > self.r - self.L_end:
            self.rounds_since_last_probe += 1
            return Action.D
        if self.mode == 'PERM_DEFECT':
            self.rounds_since_last_probe += 1
            return Action.D
        if self.mode == 'COOP_MODE':
            recent_count = min(self.w, t - 1)
            if recent_count <= 0:
                self.rounds_since_last_probe += 1
                return Action.D
            opp_recent = np.sum(history.opponent_actions[-recent_count:, :], axis=1).astype(int)
            my_recent = history.my_actions[-recent_count:]
            exploited = False
            for idx in range(recent_count):
                if bool(my_recent[idx]) and int(opp_recent[idx]) < self.required_m:
                    exploited = True
                    break
            if exploited:
                self.mode = 'PERM_DEFECT'
                self.rounds_since_last_probe += 1
                return Action.D
            recent_others_coop_avg = float(np.mean(opp_recent))
            if recent_others_coop_avg >= float(self.required_m):
                return Action.C
            else:
                self.rounds_since_last_probe += 1
                return Action.D
        if self.mode == 'DEFECT_MODE':
            if self.rounds_since_last_probe >= self.T_probe:
                self.last_round_was_probe = True
                self.rounds_since_last_probe = 0
                return Action.C
            else:
                self.rounds_since_last_probe += 1
                return Action.D
        self.rounds_since_last_probe += 1
        return Action.D
