"""
Generated LLM strategies for social dilemma games.

This file contains strategy classes generated by LLMs for game theory experiments.
Each strategy is a callable class that implements a specific approach to the game.

Generated with:
- Provider: Client
- Model: llama3.1:70b
- Game: public_goods
"""

from dataclasses import dataclass
from enum import Enum
import math
import numpy as np
from numpy.typing import NDArray
import random

from emergent_llm.players import BaseStrategy
from emergent_llm.games import PublicGoodsDescription, CollectiveRiskDescription, CommonPoolDescription, CommonPoolState
from emergent_llm.common import Action, C, D, PlayerHistory, GameState




class Strategy_COLLECTIVE_1(BaseStrategy):
    """
    Adaptive Cooperative Optimism Strategy for Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.previous_payoffs = []
        self.average_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        k = self.game_description.k
        theta = n_players / 3
        if state.round_number == 0 or len(history.my_actions) < 3:
            return Action.C
        average_cooperators = np.mean(history.opponent_actions[-1, :])
        self.average_cooperators = (self.average_cooperators * state.round_number + average_cooperators) / (state.round_number + 1)
        if len(self.previous_payoffs) > 0:
            previous_payoff = self.previous_payoffs[-1]
            if previous_payoff < 0.5:
                return Action.D
        p = k / n_players * average_cooperators / n_players
        if average_cooperators > theta:
            return random.choice([Action.C, Action.D], p=[p, 1 - p])
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if self.average_cooperators > n_players / 2:
                return Action.C
            else:
                return Action.D
        my_action = random.choice([Action.C, Action.D])
        payoff = k / n_players * average_cooperators + int(my_action == Action.D)
        self.previous_payoffs.append(payoff)
        return my_action

class Strategy_COLLECTIVE_2(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Public Goods Focus (ACPG)
    
    Prioritizes public goods aspect of the game, encouraging cooperation when collective contribution is sufficient.
    Adapts to changes in cooperation levels and responds to decreases in cooperative behavior, promoting a stable and mutually beneficial environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_pub_goods_prev = 0
        self.coop_count_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_contributions = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        self.avg_pub_goods_prev = self.game_description.k / self.game_description.n_players * total_contributions
        if self.avg_pub_goods_prev > self.game_description.k / self.game_description.n_players * 0.5:
            action = Action.C
        else:
            action = Action.D
        coop_count_current = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if coop_count_current >= self.coop_count_prev and action == Action.C:
            action = Action.C
        elif coop_count_current < self.coop_count_prev and action == Action.C:
            action = Action.D
        self.coop_count_prev = coop_count_current
        if state.round_number + 1 == self.game_description.n_rounds:
            return Action.D
        return action

class Strategy_COLLECTIVE_3(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) Strategy.
    
    This strategy adapts to various opponent behaviors and depends solely on game parameters and history.
    It balances individual and collective interests, promoting cooperation throughout the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.acr = 0.8
        self.punish_prob = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.acr else Action.D
        prev_round_actions = history.opponent_actions[-1, :] | np.array([history.my_actions[-1]])
        prev_coop_payoff = self.game_description.k / self.game_description.n_players * sum(prev_round_actions)
        if state.round_number < self.game_description.n_rounds - 1:
            acr = sum(prev_round_actions) / self.game_description.n_players
            cooperate = acr >= 0.5
            for j, action in enumerate(prev_round_actions):
                if not action and random.random() < self.punish_prob:
                    cooperate *= 0.9
            return Action.C if cooperate else Action.D
        else:
            total_coop_payoff = sum(history.my_payoffs) + prev_coop_payoff
            total_defect_payoff = sum(history.my_payoffs) + (1 - history.my_actions[-1])
            return Action.C if total_coop_payoff >= total_defect_payoff else Action.D

class Strategy_COLLECTIVE_4(BaseStrategy):
    """
    Adaptive Cooperation with Punishment (ACP) strategy.
    Balances cooperation and punishment to promote collective welfare,
    while adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold_value = 0.55
        self.punishment_intensity = 0.05
        self.cooperation_rate = None
        self.punish_intensity = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_coop_rate = np.mean(history.opponent_actions[-1, :])
        self.punish_intensity += self.punishment_intensity * (self.threshold_value - avg_coop_rate)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif avg_coop_rate >= self.threshold_value:
            return Action.C
        else:
            defect_probability = min(1.0, max(0.0, self.punish_intensity))
            return Action.D if random.random() < defect_probability else Action.C

class Strategy_COLLECTIVE_5(BaseStrategy):
    """
    Adaptive Reciprocity with Public Goods (ARPG) strategy.
    
    Cooperates when the majority of players cooperated in the previous round, 
    otherwise defects. Encourages cooperation while being robust against various 
    opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        my_prev_action = history.my_actions[-1]
        prev_coops = sum((1 for action in np.append(prev_round_actions, my_prev_action) if action))
        threshold = math.floor(self.game_description.n_players / 2)
        if prev_coops >= threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_6(BaseStrategy):
    """
    A collective strategy designed for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        ACR = self.calculate_average_contribution_rate(history)
        RS = self.calculate_reputation_scores(history)
        avg_RS = np.mean(RS)
        if ACR >= 0.5 and state.round_number < self.game_description.n_rounds or (avg_RS > 0.5 and state.round_number < self.game_description.n_rounds):
            return Action.C
        elif self.punishment_mechanism(history, RS):
            return Action.D
        else:
            if ACR >= 0.5:
                return Action.C
            return Action.D

    def calculate_average_contribution_rate(self, history: PlayerHistory) -> float:
        total_contributions = sum((sum((c_j for c_j in round_history)) for round_history in history.opponent_actions))
        return total_contributions / ((history.my_payoffs.shape[0] - 1) * self.game_description.n_players)

    def calculate_reputation_scores(self, history: PlayerHistory) -> NDArray[np.float64]:
        RS = np.mean(history.opponent_actions[:-1], axis=0)
        return RS

    def punishment_mechanism(self, history: PlayerHistory, RS: NDArray[np.float64]) -> bool:
        for i in range(self.game_description.n_players - 1):
            if not history.opponent_actions[-1][i] and RS[i] < 0.5:
                return True
        return False

class Strategy_COLLECTIVE_7(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the collective goal of maximizing overall payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.n_players / 2
        self.average_payoff_threshold = 1.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_cooperators = sum(history.opponent_actions[-1, :]) + 1
        average_payoff = np.mean(np.concatenate((history.my_payoffs, history.opponent_payoffs[-1, :])))
        if state.round_number < self.game_description.n_rounds - 1:
            if total_cooperators >= self.cooperation_threshold:
                action = Action.C
            else:
                action = Action.D
            if average_payoff > self.average_payoff_threshold:
                self.cooperation_threshold -= 0.1
            elif average_payoff < self.average_payoff_threshold:
                self.cooperation_threshold += 0.1
        else:
            action = Action.D
        return action

class Strategy_COLLECTIVE_8(BaseStrategy):
    """
    A collective strategy that balances individual payoff maximization with collective welfare,
    while being robust to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.k_over_n = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number > 0 and history is None):
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if previous_cooperators >= self.k_over_n * self.game_description.n_players:
            return Action.C
        elif all((not opponent_action for opponent_action in history.opponent_actions[-1, :])) and history.my_actions[-1] == 0 and (self.game_description.k > 2):
            return Action.C
        else:
            if previous_cooperators == math.ceil(self.k_over_n * self.game_description.n_players) - 1:
                return random.choice([Action.C, Action.D])
            return Action.D

class Strategy_COLLECTIVE_9(BaseStrategy):
    """
    An adaptive strategy based on monitoring the group's cooperation level and adjusting individual behavior accordingly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.previous_actions = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None and len(history.my_actions) > 1:
            acr = np.mean(history.my_actions[:-1] + history.opponent_actions[:-1, :].mean(axis=1))
        else:
            acr = 0.5
        if state.round_number == self.game_description.n_rounds - 1:
            total_coop_payoff = self.game_description.k * self.game_description.n_players
            expected_defect_payoff = self.game_description.k / self.game_description.n_players * np.sum(history.opponent_actions[-1, :])
            if total_coop_payoff > expected_defect_payoff:
                return Action.C
        if acr >= self.game_description.k / self.game_description.n_players:
            return Action.C
        else:
            p_d = 1 - acr
            if random.random() < p_d:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_10(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective well-being.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_action = None
        self.avg_cooperation = 0.5
        self.exploration_prob = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.avg_cooperation = np.mean(history.my_actions)
            prev_round_cooperators = sum(history.opponent_actions[-1, :])
            avg_prev_cooperation = prev_round_cooperators / self.game_description.n_players
            if avg_prev_cooperation > 0.5:
                action = Action.C
            elif self.prev_action == Action.D and avg_prev_cooperation < 0.5:
                action = Action.C
            else:
                action = Action.D
            if random.random() < self.exploration_prob:
                action = random.choice([Action.C, Action.D])
            self.prev_action = action
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_11(BaseStrategy):
    """
    Adaptive Cooperation strategy for the N-Player Public Goods Game.
    
    This strategy balances individual payoffs with collective well-being while adapting to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        avg_coop_prev_round = np.mean(opponent_actions)
        if avg_coop_prev_round >= self.game_description.k / self.game_description.n_players:
            return Action.C
        elif avg_coop_prev_round < self.game_description.k / self.game_description.n_players and history.my_actions[-1]:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_12(BaseStrategy):
    """
    The Adaptive Reciprocal Cooperation (ARC) strategy balances individual self-interest with collective well-being by adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_avg = 0
        self.defect_prob = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :] == True)
        self.coop_avg = (self.coop_avg * (state.round_number - 1) + num_cooperators) / state.round_number
        if self.coop_avg >= self.game_description.n_players / 2:
            action = Action.C
        else:
            num_defectors = sum(history.opponent_actions[-1, :] == False)
            self.defect_prob = num_defectors / self.game_description.n_players
            if random.random() < self.defect_prob:
                action = Action.D
            else:
                action = Action.C
        if history.my_payoffs[-1] <= 1:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if self.coop_avg >= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        return action

class Strategy_COLLECTIVE_13(BaseStrategy):
    """
    Collective strategy that balances individual self-interest with cooperation,
    adapting to the game's history and opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.grudge_players = set()
        self.grudge_rounds = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponents_defected = any((not opponent_action for opponent_action in history.opponent_actions[-1, :]))
        my_last_action = bool(history.my_actions[-1])
        if my_last_action and opponents_defected:
            self.grudge_players.add(0)
        if len(self.grudge_players) > 0 and state.round_number - max(self.grudge_players, default=0) > self.grudge_rounds:
            self.grudge_players.clear()
        prev_coop_payoff_avg = np.mean(history.my_payoffs[history.my_actions])
        prev_defect_payoff_avg = np.mean(history.my_payoffs[~history.my_actions])
        if len(self.grudge_players) > 0:
            return Action.D
        elif prev_coop_payoff_avg >= prev_defect_payoff_avg:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_14(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Punishment (ACGP) Strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.num_cooperators_prev = 0
        self.last_round_punished = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.num_cooperators_prev = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if self.game_description.n_rounds - state.round_number <= 1:
            return Action.D
        if self.num_cooperators_prev == self.game_description.n_players:
            return Action.C
        elif self.num_cooperators_prev >= self.game_description.n_players // 2:
            coop_prob = self.num_cooperators_prev / self.game_description.n_players
            if random.random() < coop_prob:
                return Action.C
            else:
                return Action.D
        elif not self.last_round_punished:
            self.last_round_punished = True
            if random.random() < 0.8:
                return Action.D
            else:
                return Action.C
        else:
            grad_punish_coop_prob = (self.num_cooperators_prev / self.game_description.n_players) ** 2
            if random.random() < grad_punish_coop_prob:
                return Action.C
            else:
                return Action.D
        self.last_round_punished = False

class Strategy_COLLECTIVE_15(BaseStrategy):
    """
    The Adaptive Cooperation (AC) strategy balances individual self-interest with collective well-being,
    adapting to various opponent behaviors while promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish = False
        self.previous_TC = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        TC = sum(history.opponent_actions[-1, :]) + 1
        avg_payoff = np.mean(np.concatenate((history.my_payoffs[-1:], history.opponent_payoffs[-1, :])))
        if TC >= self.game_description.n_players / 2 and (not self.punish):
            action = Action.C
        else:
            action = Action.D
        if avg_payoff < self.game_description.k and (not self.punish):
            self.punish = True
        elif TC > self.previous_TC and self.punish:
            self.punish = False
        self.previous_TC = TC
        return action

class Strategy_COLLECTIVE_16(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_coop = game_description.k / game_description.n_players
        self.tau = game_description.k / game_description.n_players * ((game_description.n_players - 1) / (game_description.n_players - game_description.k))
        self.decreased_p_coop = self.p_coop * 0.8

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_coop else Action.D
        opponent_actions_last_round = history.opponent_actions[-1]
        total_cooperators_last_round = sum(opponent_actions_last_round)
        avg_payoff_c_last_round = np.mean([history.opponent_payoffs[-1, i] for i in range(self.game_description.n_players - 1) if opponent_actions_last_round[i]])
        avg_payoff_d_last_round = np.mean([history.opponent_payoffs[-1, i] for i in range(self.game_description.n_players - 1) if not opponent_actions_last_round[i]])
        if avg_payoff_c_last_round > avg_payoff_d_last_round and total_cooperators_last_round >= self.tau:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        else:
            decreased_p_coop = True
            for i in range(self.game_description.n_players - 1):
                if not opponent_actions_last_round[i] and total_cooperators_last_round >= self.tau:
                    decreased_p_coop = False
            return Action.C if random.random() < (self.decreased_p_coop if decreased_p_coop else self.p_coop) else Action.D

class Strategy_COLLECTIVE_17(BaseStrategy):
    """
    The Adaptive Cooperate-to-Threaten (ACT) strategy balances individual self-interest with collective well-being.
    It initially cooperates and reciprocates cooperation from others, while punishing free-riding behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        n_cooperators = sum(history.opponent_actions[-1, :] | np.array([True]))
        if n_cooperators >= self.game_description.n_players / 2:
            return Action.C
        p_defect = (self.game_description.n_players - n_cooperators) / self.game_description.n_players
        return Action.D if random.random() < p_defect else Action.C

class Strategy_COLLECTIVE_18(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that encourages cooperation and mutual benefit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.acr = 0.5
        self.consecutive_defections = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choices([Action.C, Action.D], weights=[0.8, 0.2])[0]
        total_contributors = sum(history.opponent_actions[-1, :] & history.my_actions[-1])
        self.acr = total_contributors / self.game_description.n_players
        if self.acr >= 0.5:
            cooperate_probability = self.game_description.k / self.game_description.n_players * self.acr
            return random.choices([Action.C, Action.D], weights=[cooperate_probability, 1 - cooperate_probability])[0]
        else:
            defect_probability = 1 - self.game_description.k / self.game_description.n_players * self.acr
            return random.choices([Action.D, Action.C], weights=[defect_probability, 1 - defect_probability])[0]
        for player in range(self.game_description.n_players):
            if not history.opponent_actions[-1, player] and self.acr > 0.5:
                self.consecutive_defections[player] = self.consecutive_defections.get(player, 0) + 1
                if self.consecutive_defections[player] >= 3:
                    return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            average_cooperation_payoff = sum(history.my_payoffs[history.my_actions]) / (state.round_number + 1)
            average_defection_payoff = sum(history.my_payoffs[~history.my_actions]) / (state.round_number + 1)
            if average_cooperation_payoff > average_defection_payoff:
                return Action.C
            else:
                return Action.D
        if not any(history.opponent_actions[-1, :]):
            cooperate_probability = 0.5
            return random.choices([Action.C, Action.D], weights=[cooperate_probability, 1 - cooperate_probability])[0]

class Strategy_COLLECTIVE_19(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.0
        self.previous_round_actions = []
        self.free_riders = set()
        self.punishment_counter = {}

    def calculate_cooperation_threshold(self):
        previous_rounds_with_majority_cooperators = sum((1 for round_actions in self.previous_round_actions if sum(round_actions) >= len(round_actions) / 2))
        return self.game_description.k / self.game_description.n_players * previous_rounds_with_majority_cooperators

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 3:
            return Action.C
        self.cooperation_threshold = self.calculate_cooperation_threshold()
        if history is not None:
            last_round_actions = history.opponent_actions[-1, :]
            for i in range(len(last_round_actions)):
                if last_round_actions[i] == 0 and history.my_payoffs[-1] < history.opponent_payoffs[-1, i] and (state.round_number - 1 not in self.punishment_counter or i not in self.punishment_counter[state.round_number - 1]):
                    if state.round_number not in self.punishment_counter:
                        self.punishment_counter[state.round_number] = set()
                    self.punishment_counter[state.round_number].add(i)
                    self.free_riders.add(i)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if self.cooperation_threshold > 0.5 else Action.D
        if state.round_number % random.randint(5, 10) == 0 and random.random() < 0.2:
            return Action.D
        self.previous_round_actions.append(history.opponent_actions[-1, :])
        if self.cooperation_threshold > 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_20(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to the game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        total_contributions = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        my_payoff = self.game_description.k / self.game_description.n_players * total_contributions
        if history.my_actions[-1]:
            my_payoff -= 1
        opponent_payoffs = np.mean(history.opponent_payoffs[-1, :], axis=0)
        if total_contributions > self.cooperation_threshold:
            return Action.C
        elif total_contributions < self.cooperation_threshold and my_payoff < opponent_payoffs:
            return Action.D
        elif history.my_actions[-1] == 0:
            return Action.D
        else:
            return random.choice([Action.C, Action.D])

class Strategy_COLLECTIVE_21(BaseStrategy):
    """
    Adaptive Reciprocity with Group Benefit (ARG) strategy.

    This strategy balances individual self-interest with the need for cooperation.
    It uses a combination of reciprocity and group benefit considerations to decide
    whether to cooperate or defect in each round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.C_count_history = []
        self.D_count_history = []
        self.APC_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        C_count = sum(opponent_actions)
        D_count = len(opponent_actions) - C_count
        self.C_count_history.append(C_count)
        self.D_count_history.append(D_count)
        if len(self.APC_history) == 0:
            APC = 0.0
        else:
            APC = np.mean(self.APC_history)
        CR = C_count / self.game_description.n_players
        threshold = self.game_description.k / self.game_description.n_players * (self.game_description.n_players / 2)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif CR > 0.5 and APC >= threshold:
            return Action.C
        elif APC < threshold:
            return Action.D
        else:
            return Action.C
        self.APC_history.append(APC)

class Strategy_COLLECTIVE_22(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation.
    
    This strategy aims to balance individual self-interest with the benefits of cooperation,
    adapting to the dynamics of the game and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_init = game_description.k / game_description.n_players
        self.previous_action = None
        self.payoff = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        c_total = sum(history.opponent_actions[-1, :] + [self.previous_action]) if self.previous_action is not None else 0
        avg_payoff_c = np.mean([payoff for payoff, action in zip(history.my_payoffs, history.my_actions) if action])
        avg_payoff_d = np.mean([payoff for payoff, action in zip(history.my_payoffs, history.my_actions) if not action])
        if c_total == 0:
            avg_payoff_c = 1.0
        if c_total == self.game_description.n_players:
            avg_payoff_d = 1.0
        cooperate_prob = c_total / self.game_description.n_players if avg_payoff_c > avg_payoff_d else 1 - c_total / self.game_description.n_players
        if self.previous_action is not None:
            if self.previous_action and self.payoff >= avg_payoff_d:
                cooperate_prob = min(cooperate_prob + 0.05, 1)
            elif not self.previous_action and self.payoff <= avg_payoff_c:
                cooperate_prob = max(cooperate_prob - 0.05, 0)
        if state.round_number == self.game_description.n_rounds - 1:
            cooperate_prob = 1 if c_total >= self.game_description.n_players / 2 else 0
        elif avg_payoff_c == avg_payoff_d:
            cooperate_prob = 0.5
        action = Action.C if random.random() < cooperate_prob else Action.D
        self.previous_action = action.value
        self.payoff = 1 - action.value + self.game_description.k / self.game_description.n_players * c_total
        return action

class Strategy_COLLECTIVE_23(BaseStrategy):
    """
    Collective Strategy: Adaptive Tit-for-Tat with Gradual Cooperation.
    
    This strategy aims to balance individual self-interest with collective well-being. 
    By initially cooperating and gradually adjusting to the group's behavior, we encourage cooperation while being robust against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_rate = 1 / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if history.opponent_actions[-1].sum() > self.game_description.n_players / 2 and history.my_actions[-1] == False:
            return Action.D
        if prev_cooperators > self.game_description.n_players / 2:
            return Action.C
        elif prev_cooperators == self.game_description.n_players / 2:
            self.cooperate_rate = min(1, self.cooperate_rate + 1 / self.game_description.n_players)
            return Action.C if random.random() < self.cooperate_rate else Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_24(BaseStrategy):
    """
    Adaptive Cooperative Foresight Strategy.
    
    This strategy starts by cooperating unconditionally in the first round. 
    In subsequent rounds, it uses a probabilistic approach based on the history of cooperation rates observed in previous rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5
        self.sensitivity = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C if state.round_number == 0 else Action.D
        opponent_actions = history.opponent_actions[-1, :]
        cooperation_rate = np.mean(opponent_actions) + history.my_actions[-1] / self.game_description.n_players
        cooperate_probability = max(0, min(1, cooperation_rate - self.threshold + self.sensitivity))
        return Action.C if random.random() < cooperate_probability else Action.D

class Strategy_COLLECTIVE_25(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective well-being by adapting to the evolving behavior of other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        avgcoop = np.mean(history.opponent_actions[-n:, :])
        my_payoff = history.my_payoffs[-1]
        avg_payoff = np.mean(np.concatenate((history.my_payoffs[-1:], history.opponent_payoffs[-1, :])))
        if state.round_number <= r / 3:
            action = Action.C
        elif state.round_number < 2 * r / 3:
            if avgcoop > 0.5:
                action = Action.C
            else:
                action = random.choice([Action.C, Action.D])
        elif state.round_number == r - 1:
            if my_payoff > avg_payoff:
                action = Action.D
            else:
                action = Action.C
        elif avgcoop > 0.7:
            action = Action.C
        elif my_payoff < avg_payoff:
            action = Action.D
        else:
            action = Action.C
        return action

class Strategy_COLLECTIVE_26(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Learning (ACGL) strategy.
    
    This strategy prioritizes collective well-being while being robust to varying opponent behaviors.
    It initially cooperates, gradually adapts based on the group's performance, and learns from past outcomes.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.n == 2:
            return Action.C
        if state.round_number == 0:
            return Action.C
        elif state.round_number <= self.r // 3:
            avg_coop_prev = np.mean(history.opponent_actions[-1, :] | history.my_actions[-1])
            if avg_coop_prev >= 0.5:
                return Action.C
            else:
                p = 1 - avg_coop_prev
                return Action.D if random.random() < p else Action.C
        elif state.round_number < self.r - 1:
            pi_prev = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1, :])
            if pi_prev >= self.k:
                return Action.C
            else:
                p = 1 - pi_prev / self.k if self.k != 0 else 0
                if self.k >= self.n:
                    p *= self.n / self.k
                return Action.D if random.random() < p else Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_27(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Concession strategy for the N-player public goods game.
    Balances cooperation and defection to maximize collective payoff while adapting to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperators = []
        self.payoffs = []
        self.streak = 0

    def calculate_payoff(self, n, k, cooperators):
        total_cooperators = sum(cooperators)
        payoff = 1 - cooperators[-1] + k / n * total_cooperators
        return payoff

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            action = Action.C
            self.cooperators.append(True)
            self.payoffs.append(1.0)
        else:
            total_cooperators = sum(history.opponent_actions[-1, :] == True)
            recent_payoff = history.my_payoffs[-1]
            average_payoff = np.mean(self.payoffs)
            if state.round_number < math.floor(self.game_description.n_rounds / 4):
                if total_cooperators >= self.game_description.n_players / 2 and recent_payoff <= average_payoff:
                    action = Action.C
                else:
                    action = Action.D
            elif state.round_number < math.floor(3 * self.game_description.n_rounds / 4):
                past_r_4th_payoffs = np.mean(self.payoffs[-math.floor(self.game_description.n_rounds / 4):])
                if total_cooperators > self.game_description.n_players / 2 and recent_payoff <= past_r_4th_payoffs:
                    action = Action.C
                else:
                    action = Action.D
            elif state.round_number < self.game_description.n_rounds - 1:
                past_r_2nd_payoffs = np.mean(self.payoffs[-math.floor(2 * self.game_description.n_rounds / 4):])
                overall_average_payoff = np.mean(self.payoffs)
                if total_cooperators > self.game_description.n_players / 2 and past_r_2nd_payoffs <= overall_average_payoff:
                    action = Action.C
                else:
                    action = Action.D
            elif total_cooperators > self.game_description.n_players / 2:
                action = Action.C
            else:
                action = Action.D
            if recent_payoff <= average_payoff:
                self.streak += 1
                if self.streak >= math.floor(self.game_description.n_rounds / 4):
                    action = Action.D
                    self.streak = 0
            self.cooperators.append(action == Action.C)
            new_payoff = self.calculate_payoff(self.game_description.n_players, self.game_description.k, self.cooperators)
            self.payoffs.append(new_payoff)
        return action

class Strategy_COLLECTIVE_28(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperative Balance (ACB)

    The ACB strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate_history = []
        self.payoff_history = []
        self.cooperate_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or not history:
            return random.choice([Action.C, Action.D]) if random.random() < self.cooperate_probability else Action.D
        prev_round_cooperations = sum(history.opponent_actions[-1, :])
        cooperation_rate = prev_round_cooperations / self.game_description.n_players
        self.cooperation_rate_history.append(cooperation_rate)
        if len(self.payoff_history) > 0:
            payoff = self.payoff_history[-1]
        else:
            payoff = 0
        if cooperation_rate >= self.game_description.k / self.game_description.n_players:
            action = Action.C if random.random() < self.cooperate_probability else Action.D
            if payoff > 1 + self.game_description.k / self.game_description.n_players * self.game_description.n_players:
                self.cooperate_probability = min(self.cooperate_probability + 0.1, 1)
            elif payoff < 1:
                self.cooperate_probability = max(self.cooperate_probability - 0.1, 0)
        else:
            action = Action.D
        self.payoff_history.append(payoff)
        return action

class Strategy_COLLECTIVE_29(BaseStrategy):
    """
    Adaptive Cooperative Tit-for-Tat (ACT4T) strategy for N-Player Public Goods Game.
    Balances individual self-interest with cooperation to achieve better collective outcomes.
    Adapts to the game's history, responding to varying levels of cooperation and defection among opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_payoff_prev_round = np.mean(history.my_payoffs[-1] + np.sum(history.opponent_actions[-1, :] / self.game_description.n_players * (self.game_description.k - 1), axis=0))
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif num_cooperators_prev_round < self.game_description.n_players / 2 and (state.round_number > 1 and sum(history.opponent_actions[-2, :]) + history.my_actions[-2] < self.game_description.n_players / 2):
            return Action.D
        elif avg_payoff_prev_round >= self.game_description.k / self.game_description.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_30(BaseStrategy):
    """
    A strategy that balances cooperation and self-interest while promoting collective welfare.
    It adapts to changes in opponents' behaviors and adjusts cooperation levels accordingly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.5
        self.CT = 0.5
        self.never_cooperated_before = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choices([Action.C, Action.D], weights=[self.cooperate_prob, 1 - self.cooperate_prob])[0]
        avg_payoff_prev_round = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1].sum() / (self.game_description.n_players - 1))
        individual_payoff_prev_round = history.my_payoffs[-1]
        if avg_payoff_prev_round >= individual_payoff_prev_round:
            action = Action.C
        else:
            action = Action.D
        num_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if num_cooperators >= self.game_description.n_players * self.CT:
            self.CT += 0.05
        else:
            self.CT -= 0.05
        if history.my_actions.all() == False and random.random() < self.cooperate_prob:
            action = Action.C
            self.never_cooperated_before = False
        if state.round_number == self.game_description.n_rounds - 1:
            total_payoff = history.my_payoffs.sum()
            avg_total_payoff = (total_payoff + history.opponent_payoffs.sum()) / self.game_description.n_players
            if total_payoff < avg_total_payoff:
                action = Action.D
        return action

class Strategy_COLLECTIVE_31(BaseStrategy):
    """
    A strategy that aims to balance individual self-interest with collective welfare by adaptively responding to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.c = 0
        self.d = 0
        self.ρ = 0
        self.round_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.round_count += 1
        if history is not None:
            opponents_cooperated = sum(history.opponent_actions[-1, :])
            self.c = opponents_cooperated + (history.my_actions[-1] == Action.C)
            self.d = self.game_description.n_players - self.c
        try:
            self.ρ = self.c / (self.c + self.d)
        except ZeroDivisionError:
            self.ρ = 0.5
        if state.round_number <= 3:
            return Action.C if self.round_count % 2 == 1 else Action.D
        if self.ρ >= self.game_description.k / self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        if history is not None and len(history.opponent_actions) > 3:
            opponents_cooperated_last_3_rounds = sum([sum(round) for round in history.opponent_actions[-3:]])
            opponent_defected_last_round = not all(history.opponent_actions[-1, :])
            if opponents_cooperated_last_3_rounds >= self.game_description.n_players * 2 and opponent_defected_last_round:
                return Action.D
        if state.round_number >= self.game_description.n_rounds - 2:
            if self.ρ < self.game_description.k / self.game_description.n_players:
                action = Action.D
        return action

class Strategy_COLLECTIVE_32(BaseStrategy):
    """
    This strategy starts by cooperating and then adapts based on the proportion of cooperators in previous rounds.
    It also monitors average payoff over a window of rounds to adjust its behavior temporarily if necessary.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_t = 0
        self.avg_payoff_window = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.p_t = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if self.p_t >= self.game_description.k / self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        if len(self.avg_payoff_window) < 5:
            self.avg_payoff_window.append(history.my_payoffs[-1])
        else:
            self.avg_payoff_window.pop(0)
            self.avg_payoff_window.append(history.my_payoffs[-1])
        avg_payoff = sum(self.avg_payoff_window) / len(self.avg_payoff_window)
        if avg_payoff < 0.8 * self.game_description.k and state.round_number > 5:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            long_term_avg_payoff = sum(history.my_payoffs) / len(history.my_payoffs)
            if long_term_avg_payoff < 0.8 * self.game_description.k:
                action = Action.D
        return action

class Strategy_COLLECTIVE_33(BaseStrategy):
    """
    An adaptive cooperation strategy for the Public Goods game.
    
    This strategy starts by cooperating in the first round to encourage others and gather information. 
    In subsequent rounds, it adapts its cooperation based on the number of cooperators in the previous round.
    The adaptation is further refined by updating a threshold for cooperation based on the average payoff per player over the last 5 rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.n_players // 2
        self.average_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :])
        self.update_average_payoffs(history.my_payoffs[-1])
        self.adjust_threshold()
        if previous_round_cooperators >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

    def update_average_payoffs(self, payoff: float):
        self.average_payoffs.append(payoff)
        if len(self.average_payoffs) > 5:
            self.average_payoffs.pop(0)

    def adjust_threshold(self):
        average_payoff = sum(self.average_payoffs) / len(self.average_payoffs) if self.average_payoffs else 0
        threshold_update = average_payoff - self.game_description.k / 2
        if threshold_update > 0:
            self.cooperation_threshold -= 1
        elif threshold_update < 0 and self.cooperation_threshold < self.game_description.n_players:
            self.cooperation_threshold += 1

class Strategy_COLLECTIVE_34(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p else Action.D
        total_contributions = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        theta = self.game_description.k / self.game_description.n_players * (self.game_description.n_players / 2)
        if total_contributions > theta:
            action = Action.C
        else:
            action = Action.D
        avg_payoff = np.mean(history.opponent_payoffs[-1, :])
        if history.my_payoffs[-1] > avg_payoff:
            self.p = min(self.p + 0.1, self.game_description.k / self.game_description.n_players)
        else:
            self.p = max(self.p - 0.05, 0)
        return action

class Strategy_COLLECTIVE_35(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Concession (ACGC) strategy for Public Goods games.
    
    This strategy promotes cooperation by initially cooperating and gradually conceding when others defect.
    It adapts to aggressive opponents using a Tit-for-Tat component and reciprocates with cooperative opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.average_cooperators = 0
        self.concession_rate = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        p = max(0, 1 - self.game_description.k / self.game_description.n_players * (self.average_cooperators / self.game_description.n_players))
        if history is not None:
            recent_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
            self.average_cooperators = ((state.round_number - 2) * self.average_cooperators + recent_cooperators) / (state.round_number - 1)
            self.concession_rate += (self.game_description.n_players // 2 - self.average_cooperators) / (self.game_description.n_rounds // 2)
        if history is not None and state.round_number > 1:
            opponent_defected = not history.opponent_actions[-2, random.randint(0, self.game_description.n_players - 2)]
            if opponent_defected and random.random() < 0.5:
                return Action.D
        if history is not None and state.round_number > self.game_description.n_rounds // 3:
            cooperative_opponents = sum(np.all(history.opponent_actions[-self.game_description.n_rounds // 3:, :], axis=0))
            if cooperative_opponents >= self.game_description.n_players // 2 and random.random() < 0.8:
                return Action.C
        concede_prob = min(1, max(0, (self.game_description.n_players - self.average_cooperators) / self.concession_rate))
        if random.random() < p * (1 - concede_prob):
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_36(BaseStrategy):
    """
    Collective strategy that balances individual self-interest with collective well-being by adapting to the game's history and punishing free-riders.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        average_cooperation_rate = total_cooperators / self.game_description.n_players
        if average_cooperation_rate >= 0.5:
            self.consecutive_defections = 0
            return Action.C
        probability_of_defection = 1 - average_cooperation_rate
        if random.random() < probability_of_defection:
            self.consecutive_defections += 1
            if self.consecutive_defections > self.game_description.k / 2:
                self.consecutive_defections = 0
                return Action.C
            return Action.D
        else:
            self.consecutive_defections = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_37(BaseStrategy):
    """
    The Adaptive Cooperative Follower (ACF) strategy is designed to balance individual 
    self-interest with collective well-being, adapting to the evolving game dynamics and 
    opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :] == True)
        n_players = self.game_description.n_players
        if prev_round_coops > n_players / 2 or prev_round_coops == n_players / 2:
            return Action.C
        avg_defect_payoff = np.mean([payoff for action, payoff in zip(history.opponent_actions[-1], history.opponent_payoffs[-1]) if not action])
        avg_coop_payoff = np.mean([payoff for action, payoff in zip(history.opponent_actions[-1], history.opponent_payoffs[-1]) if action])
        if avg_defect_payoff > 0 and avg_coop_payoff > 0:
            if avg_defect_payoff > avg_coop_payoff:
                return Action.D
            else:
                return Action.C
        elif avg_defect_payoff > 0:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_38(BaseStrategy):
    """
    Adaptive Cooperation with Social Learning strategy for N-Player Public Goods Game.
    This strategy balances individual self-interest with collective well-being by adapting to various opponent behaviors and promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def estimate_expected_payoff(self, history: PlayerHistory, action: Action) -> float:
        if history is None or len(history.my_actions) < 2:
            return 1 - int(action == Action.C) + self.k / self.n_players
        total_cooperators = sum(history.opponent_actions[-1, :])
        if action == Action.D and len(history.my_payoffs) > 0:
            opponent_payoff_avg = np.mean(history.opponent_payoffs[-1, :])
            return max(1 + self.k / self.n_players * total_cooperators, opponent_payoff_avg)
        expected_cooperators = total_cooperators
        if action == Action.C and len(history.my_actions) > 0:
            my_last_action = history.my_actions[-1]
            my_last_payoff = history.my_payoffs[-1]
            if my_last_action and my_last_payoff < self.k / self.n_players:
                expected_cooperators -= 1
        return 1 - int(action == Action.C) + self.k / self.n_players * expected_cooperators

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2 or history is None:
            return Action.C
        total_cooperators = sum(history.opponent_actions[-1, :])
        if total_cooperators >= self.n_players // 2 and len(history.my_payoffs) > 0:
            my_last_action = history.my_actions[-1]
            my_last_payoff = history.my_payoffs[-1]
            return Action.C if my_last_action and my_last_payoff < self.k / self.n_players or (not my_last_action and my_last_payoff > 0) else Action.D
        explore_probability = 0.1
        if random.random() < explore_probability:
            return random.choice([Action.C, Action.D])
        expected_payoff_C = self.estimate_expected_payoff(history, Action.C)
        expected_payoff_D = self.estimate_expected_payoff(history, Action.D)
        if state.round_number == self.n_rounds - 1:
            my_total_coop_payoffs = np.sum([payoff for action, payoff in zip(history.my_actions, history.my_payoffs) if action])
            my_total_defect_payoffs = np.sum([payoff for action, payoff in zip(history.my_actions, history.my_payoffs) if not action])
            return Action.C if my_total_coop_payoffs > my_total_defect_payoffs else Action.D
        return Action.C if expected_payoff_C >= expected_payoff_D else Action.D

class Strategy_COLLECTIVE_39(BaseStrategy):
    """
    This strategy adapts to various opponent behaviors while promoting cooperation and robustness.
    It escalates cooperation when opponents cooperate, punishes defection, and switches to a more cautious approach in case of early exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_prob = 0.5
        self.opponent_defection_count = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.coop_prob else Action.D
        avg_opponent_coop = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        coop_threshold = self.game_description.k / self.game_description.n_players
        if avg_opponent_coop >= coop_threshold:
            self.coop_prob = 0.8
        else:
            self.coop_prob -= 0.1
        for i, action in enumerate(history.opponent_actions[-1, :]):
            if not action and history.my_actions[-1]:
                self.opponent_defection_count[i] += 1
                self.coop_prob -= 0.2
        if state.round_number <= 3 and any((count > 0 for count in self.opponent_defection_count)):
            self.coop_prob -= 0.2
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        self.coop_prob = max(0, min(self.coop_prob, 1))
        return Action.C if random.random() < self.coop_prob else Action.D

class Strategy_COLLECTIVE_40(BaseStrategy):
    """
    A robust and adaptive collective strategy for the N-Player Public Goods Game.
    Balances individual self-interest with collective well-being by dynamically adjusting cooperation levels based on observed opponent behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_c = 0.0
        self.thres = 0.6
        self.p_defect = 0.0
        self.opponent_last_actions = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.opponent_last_actions = history.opponent_actions[-1, :] if history else None
        recent_cooperation_rates = []
        for i in range(max(0, state.round_number - 5), state.round_number):
            round_cooperators = sum(history.opponent_actions[i, :]) + (history.my_actions[i] == Action.C)
            recent_cooperation_rates.append(round_cooperators / self.game_description.n_players)
        if recent_cooperation_rates:
            self.avg_c = np.mean(recent_cooperation_rates)
        if state.round_number < self.game_description.n_rounds - 1:
            self.p_defect = max(0, 1 - (self.avg_c - self.thres) / (1 - self.thres))
            if random.random() < self.p_defect:
                return Action.D
        for opponent_last_action in self.opponent_last_actions:
            if not opponent_last_action:
                p_punish = 0.8
                if random.random() < p_punish:
                    return Action.D
        if self.avg_c >= self.thres or (self.opponent_last_actions is not None and sum(self.opponent_last_actions) / len(self.opponent_last_actions) > 0.5):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_41(BaseStrategy):
    """
    Collective Strategy: Adaptive Tit-for-Tat with Threshold (ATTT).
    
    This strategy balances individual self-interest with collective cooperation by adapting to the game's history.
    It aims to promote a cooperative environment while discouraging exploitation by defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.threshold = 0.5 * game_description.n_players
        self.smoothing_factor = 0.1
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :])
        self.threshold = self.smoothing_factor * num_cooperators + (1 - self.smoothing_factor) * self.threshold
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if num_cooperators >= self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_42(BaseStrategy):
    """
    Adaptive Cooperative Escalation strategy for Public Goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (history is None and state.round_number > 0):
            return Action.C
        if len(history.my_actions) == self.game_description.n_rounds - 1:
            return Action.D
        num_cooperators_last_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_last_round >= self.k:
            return Action.C
        p = (self.k - num_cooperators_last_round) / (self.n_players - self.k)
        if random.random() < p:
            return Action.D
        all_defected_last_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1] == 0
        q = 1 / (self.n_players - 1)
        if random.random() < q and all_defected_last_round:
            return Action.C
        if self.game_description.n_players == 1:
            if self.k > 1:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_43(BaseStrategy):
    """
    Adaptive Cooperative Escalation (ACE) strategy.

    This strategy balances individual self-interest with collective cooperation.
    It adapts to the evolving game dynamics and opponent behaviors, promoting mutual benefits while minimizing exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1]
        my_action = history.my_actions[-1]
        my_payoff = history.my_payoffs[-1]
        CI = sum(opponent_actions) / self.game_description.n_players
        if self.game_description.k == 0:
            PPR = 0
        else:
            PPR = my_payoff / self.game_description.k
        p = 1 - CI
        if CI >= 0.5 and PPR >= 0.8:
            return Action.C
        elif random.random() < p:
            return Action.D
        if my_action == False and CI > sum(history.opponent_actions[-2, :]) / self.game_description.n_players:
            p = max(0.5, p + 0.1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.C

class Strategy_COLLECTIVE_44(BaseStrategy):
    """
    A collective strategy that combines elements of Tit-for-Tat and the public goods game's unique payoff structure.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_total_contributions = sum(history.opponent_actions[-1, :])
        pgt = self.game_description.k / self.game_description.n_players * previous_total_contributions
        opponent_actions = history.opponent_actions[-1, :]
        tft_decision = Action.C if all(opponent_actions) else Action.D
        if previous_total_contributions > pgt:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return tft_decision

class Strategy_COLLECTIVE_45(BaseStrategy):
    """
    A collective strategy for the N-player public goods game that balances individual payoff maximization with cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff = 0.0
        self.avg_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0:
            cooperate_prob = 0.5
        elif state.round_number < 5:
            opponent_cooperations = np.sum(history.opponent_actions, axis=1)
            avg_payoff = np.mean(opponent_cooperations) * k / n_players + (n_players - np.mean(opponent_cooperations))
            if avg_payoff > k / n_players:
                cooperate_prob = 1.0
            else:
                cooperate_prob = 1.0 - avg_payoff / (k / n_players)
        else:
            opponent_actions_last_round = history.opponent_actions[-1, :]
            cooperators_last_round = np.sum(opponent_actions_last_round)
            self.avg_cooperators = cooperators_last_round
            if cooperators_last_round > n_players / 2:
                cooperate_prob = 1.0
            else:
                cooperate_prob = 1.0 - cooperators_last_round / n_players
        if state.round_number == r - 1:
            opponent_cooperations = np.sum(history.opponent_actions, axis=1)
            avg_payoff_last_round = np.mean(opponent_cooperations) * k / n_players + (n_players - np.mean(opponent_cooperations))
            if avg_payoff_last_round > k / n_players:
                return Action.C
            else:
                return Action.D
        action = random.choices([Action.C, Action.D], weights=[cooperate_prob, 1 - cooperate_prob])[0]
        return action

class Strategy_COLLECTIVE_46(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with cooperative behavior,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number == 1 and (not np.all(history.opponent_actions[0, :])):
            return Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if prev_cooperators >= self.game_description.n_players // 2:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_47(BaseStrategy):
    """
    Collective strategy balancing individual self-interest with group cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or len(self.cooperation_history) < state.round_number:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :]) + self.cooperation_history[-1]
        if previous_cooperators >= self.game_description.k - 1:
            action = Action.C
        elif previous_cooperators > self.game_description.n_players - self.game_description.k + 1:
            action = Action.D
        else:
            average_payoff = np.mean(history.my_payoffs) if history.my_payoffs.size > 0 else 0
            opponent_payoffs = history.opponent_payoffs[-1, :]
            total_cooperators = previous_cooperators - (not self.cooperation_history[-1])
            public_good_share = self.game_description.k / self.game_description.n_players * total_cooperators
            if state.round_number < self.game_description.n_rounds - 1:
                expected_self_payoff = public_good_share
            else:
                expected_cooperators = sum(history.opponent_actions[-1, :])
                expected_self_payoff = self.game_description.k / self.game_description.n_players * (expected_cooperators + 1)
            if average_payoff > expected_self_payoff:
                action = Action.D
            else:
                action = Action.C
        self.cooperation_history.append(action == Action.C)
        return action

class Strategy_COLLECTIVE_48(BaseStrategy):
    """
    Collective Strategy: Adaptive Tit-for-Tat with Public Goods Incentives.

    This strategy aims to balance individual self-interest with collective cooperation,
    making it robust against various opponent strategies in an N-player public goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if self.game_description.n_players == 2:
            return Action.C if history.opponent_actions[-1, 0] else Action.D
        threshold = self.game_description.n_players * self.game_description.k / (self.game_description.k + 1)
        if num_cooperators_prev >= threshold:
            return Action.C
        p = 1 - num_cooperators_prev / self.game_description.n_players
        return Action.D if random.random() < p else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_49(BaseStrategy):
    """
    Summary: The Adaptive Cooperative Fingerprint (ACF) strategy aims to balance individual self-interest 
             with collective prosperity by initially cooperating and adapting to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_cooperate = 0.0
        self.cooperators = 0
        self.payoffs = []
        self.opponent_defected = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif 2 <= state.round_number < self.game_description.n_rounds // 4:
            self.cooperators = sum(history.opponent_actions[-1, :])
            self.p_cooperate = self.game_description.k / self.game_description.n_players * (self.cooperators / self.game_description.n_players)
        elif history.my_payoffs.size > 0 and self.cooperators > 0:
            avg_payoff_per_cooperator = np.mean(history.my_payoffs[-1] + np.sum(history.opponent_payoffs[-1, :]) / (self.game_description.n_players - 1))
            if avg_payoff_per_cooperator >= 1 + self.game_description.k / self.game_description.n_players:
                self.p_cooperate = avg_payoff_per_cooperator / self.game_description.n_players
            else:
                self.p_cooperate = 1 - avg_payoff_per_cooperator / self.game_description.n_players
        if state.round_number > 1 and history.my_actions[-2] == True and (sum(history.opponent_actions[-2, :]) < self.cooperators):
            self.opponent_defected = True
            return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            self.p_cooperate = 1.0
        if random.random() < self.p_cooperate:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_50(BaseStrategy):
    """
    Adaptive Cooperate-Defect strategy for N-Player Public Goods Game.
    Balances individual self-interest with cooperative behavior, adapting to the game's history and opponent actions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.tau = 0.5
        self.c_prev = None
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        else:
            rho = np.sum(history.opponent_actions[-1, :]) / self.game_description.n_players
            total_coop_payoff = np.sum([payoff for action, payoff in zip(history.my_actions[:-1], history.my_payoffs) if action])
            total_defect_payoff = np.sum([payoff for action, payoff in zip(history.my_actions[:-1], history.my_payoffs) if not action])
            if total_coop_payoff > total_defect_payoff:
                self.tau = min(1, self.tau + 0.1)
            elif total_coop_payoff < total_defect_payoff:
                self.tau = max(0, self.tau - 0.1)
            if rho == 0:
                self.tau = 0.5
            if rho >= self.tau:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_51(BaseStrategy):
    """
    The Adaptive Cooperativity strategy aims to balance individual payoff maximization with collective welfare,
    adapting to the evolving game dynamics and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_prev = 0
        self.coop_opponents_prev = 0
        self.punishment_phase = False
        self.theta = (game_description.n_players - game_description.k) / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_payoff = np.mean(history.my_payoffs)
        max_payoff = 1 + self.game_description.k / self.game_description.n_players * self.game_description.n_players
        if avg_payoff > self.theta * max_payoff:
            action = Action.C
        else:
            action = Action.D
        coop_opponents = sum(history.opponent_actions[-1, :])
        if coop_opponents > (self.game_description.n_players - 1) / 2:
            action = Action.C
        if self.punishment_phase and history.my_actions[-1] == False and (coop_opponents < (self.game_description.n_players - 1) / 2):
            action = Action.C
            self.punishment_phase = False
        elif history.my_actions[-1] == False and (state.round_number > 1 and history.my_actions[-2] == False):
            self.punishment_phase = True
        self.avg_payoff_prev = avg_payoff
        self.coop_opponents_prev = coop_opponents
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_COLLECTIVE_52(BaseStrategy):
    """
    Adaptive Cooperation with Retaliation (ACR) strategy for N-Player Public Goods Game.
    
    This strategy balances individual self-interest with collective well-being by adapting to various opponent behaviors while promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.k / game_description.n_players
        self.retaliation_flag = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_payoffs = np.sum(history.opponent_payoffs[-1, :], axis=0) + history.my_payoffs[-1]
        average_payoff = np.mean(previous_payoffs)
        if average_payoff >= self.threshold:
            action = Action.C
        elif any(~history.opponent_actions[-1, :]):
            action = Action.D
        else:
            action = Action.C
        if any(~history.opponent_actions[-1, :]) and (not self.retaliation_flag):
            self.retaliation_flag = True
        elif all(history.opponent_actions[-1, :]):
            self.retaliation_flag = False
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_54(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the desire for mutual benefit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.acr = 0.0
        self.theta = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        if 2 <= state.round_number <= self.game_description.n_rounds // 3:
            if state.round_number > 2:
                recent_opponent_actions = history.opponent_actions[state.round_number - 1:state.round_number + 1]
                self.acr = np.mean(recent_opponent_actions)
            return Action.C
        if self.acr > self.theta:
            cooperated_last_round = history.my_actions[-1]
            defected_last_round_opponents = np.any(np.logical_not(history.opponent_actions[-1, :]))
            if cooperated_last_round and defected_last_round_opponents:
                return Action.D
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_55(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation (AC)
    
    This strategy adapts its cooperation level based on the overall cooperation rate of all players.
    It balances individual self-interest with collective well-being, making it a robust and adaptive approach for a wide range of opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_prob = 0.5
        self.m = game_description.n_rounds // 10

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.coop_prob else Action.D
        avg_CR = self.calculate_average_cooperation_rate(history)
        c_prev = sum(history.opponent_actions[-1, :])
        if c_prev >= self.game_description.n_players / 2:
            self.coop_prob = min(self.coop_prob + 0.1, 1)
        elif c_prev < self.game_description.n_players / 4:
            self.coop_prob = max(self.coop_prob - 0.1, 0)
        if avg_CR > 0.5:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D if random.random() < self.coop_prob else Action.C

    def calculate_average_cooperation_rate(self, history: PlayerHistory) -> float:
        recent_rounds = min(self.m, len(history.my_actions))
        coop_counts = sum(np.sum(history.opponent_actions[-recent_rounds:, :], axis=0)) + np.count_nonzero(history.my_actions[-recent_rounds:])
        total_players = self.game_description.n_players
        return coop_counts / (total_players * recent_rounds)

class Strategy_COLLECTIVE_56(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Social Learning.
    
    This strategy aims to balance individual self-interest with collective well-being 
    by adaptively adjusting cooperation levels based on observed opponent behavior and game history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if self.game_description.n_players == 2:
            return Action.C
        avg_opponent_coop = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if state.round_number > 3:
            avg_payoff_last_3_rounds = np.mean(history.my_payoffs[max(0, state.round_number - 3):state.round_number])
            if history.my_payoffs[-1] > avg_payoff_last_3_rounds:
                self.theta += 0.05
            elif history.my_payoffs[-1] < avg_payoff_last_3_rounds:
                self.theta -= 0.05
        if avg_opponent_coop > self.theta:
            action = Action.C
        else:
            action = Action.D
        if not np.any(history.opponent_actions[-1, :]):
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_COLLECTIVE_57(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors 
    and aligns with the collective mindset, balancing individual self-interest with group well-being.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= self.game_description.n_players:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        threshold = self.game_description.k / self.game_description.n_players * (self.game_description.n_players / 2)
        prev_contributions = sum(history.opponent_actions[-1, :])
        if prev_contributions >= threshold:
            return Action.C
        elif prev_contributions < threshold and (not all((not action for action in history.opponent_actions[-2, :]))):
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_58(BaseStrategy):
    """
    This strategy adapts to various opponent behaviors and focuses on achieving a high total payoff.
    It initially cooperates, encourages others to cooperate by punishing defectors, and reverts 
    to cooperation after punishing. In later rounds, it adjusts its strategy based on the average 
    payoffs of all players in previous rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishing = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.punishing:
            self.punishing = False
            return Action.D
        prev_round_cooperators = sum(history.opponent_actions[-1, :])
        if state.round_number < 5:
            if prev_round_cooperators > self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        elif state.round_number < self.game_description.n_rounds // 2:
            avg_payoff = np.mean(history.my_payoffs[-3:])
            if avg_payoff >= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        elif state.round_number < self.game_description.n_rounds - 1:
            if prev_round_cooperators > self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        if prev_round_cooperators > self.game_description.n_players / 2 and history.my_actions[-1] == Action.D:
            self.punishing = True
        return Action.D

class Strategy_COLLECTIVE_59(BaseStrategy):
    """
    The Adaptive Cooperation strategy balances cooperation with robustness against exploitation.
    It encourages cooperation while adapting to the actions of other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :])
        previous_defectors = self.game_description.n_players - 1 - previous_cooperators
        if previous_cooperators > self.game_description.n_players / 2:
            return Action.C
        if previous_defectors >= previous_cooperators:
            self.consecutive_defections += 1
            return Action.D
        if state.round_number % 5 == 0 and state.round_number != self.game_description.n_rounds - 1:
            self.consecutive_defections = 0
            return Action.C
        if previous_cooperators == previous_defectors:
            return Action.C
        elif self.consecutive_defections > 3:
            self.consecutive_defections = 0
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_60(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_c = 0.0
        self.punishment_phase = False
        self.concession_phase = False
        self.cooperation_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < self.cooperation_probability else Action.D
        cooperators = np.where(history.opponent_actions[-1, :] | history.my_actions[-1], True, False)
        avg_payoff_c = np.sum(np.where(cooperators, history.opponent_payoffs[-1, :] + history.my_payoffs[-1], 0)) / np.sum(cooperators)
        if avg_payoff_c < 0.5 and (not self.punishment_phase):
            self.punishment_phase = True
            return Action.D
        opponent_avg_payoff_diff = (history.opponent_payoffs[-1, :] - history.my_payoffs[-1]).mean()
        if opponent_avg_payoff_diff > 0.5 and (not self.concession_phase):
            self.concession_phase = True
            self.cooperation_probability += 0.05
        return Action.C if avg_payoff_c >= self.game_description.k / self.game_description.n_players else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            cooperators = np.sum(history.opponent_actions[-1, :] | history.my_actions[-1])
            return Action.C if cooperators >= self.game_description.n_players / 2 else Action.D

class Strategy_COLLECTIVE_61(BaseStrategy):
    """
    A robust and adaptive collective strategy that balances cooperation and defection based on game history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        else:
            cooperators = sum(history.opponent_actions[-1, :])
            cr = cooperators / self.game_description.n_players
            T = self.game_description.k / self.game_description.n_players
            if cr >= T:
                return Action.C
            else:
                if math.isclose(cr, T):
                    return Action(random.choice([Action.C, Action.D]))
                return Action.D
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.D

class Strategy_COLLECTIVE_62(BaseStrategy):
    """
    The Adaptive Collective Optimism (ACO) strategy for the N-Player Public Goods Game.
    
    Encourages cooperation through initial optimism, reciprocates cooperation to sustain collective benefits,
    punishes non-cooperation to maintain fairness, and forgives past transgressions to promote continued cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_cooperators = 0.0
        self.prev_round_cooperators = 0
        self.punish_defect = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.8 else Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        self.prev_round_cooperators = opponent_cooperators
        if state.round_number == 1:
            self.avg_cooperators = float(opponent_cooperators)
        else:
            self.avg_cooperators = (self.avg_cooperators * (state.round_number - 1) + opponent_cooperators) / state.round_number
        if self.avg_cooperators > opponent_cooperators:
            action = Action.C
        elif self.punish_defect:
            action = Action.D
            self.punish_defect = False
        else:
            action = Action.D
        if history.my_actions[-1] and opponent_cooperators < self.avg_cooperators:
            self.punish_defect = True
        if not self.punish_defect and action == Action.D:
            action = Action.C
        return action

class Strategy_COLLECTIVE_63(BaseStrategy):
    """
    A strategy that adapts to various opponent behaviors and aligns with the collective mindset.
    It balances individual self-interest with collective well-being, prioritizing cooperation when it's likely to succeed.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n = self.game_description.n_players
        k = self.game_description.k
        avg_coop_rate = n / 2
        appc_prev = None
        if history is not None:
            opponent_actions = history.opponent_actions[-1, :]
            my_previous_payoff = history.my_payoffs[-1]
            previous_round_payoffs = np.append(history.my_payoffs[-1], history.opponent_payoffs[-1, :])
            num_cooperators = sum(opponent_actions) + (history.my_actions[-1] == 1)
            if state.round_number < min(5, self.game_description.n_rounds // 2):
                return Action.C if num_cooperators >= n / 4 else Action.D
            avg_payoff_prev = np.mean(previous_round_payoffs)
            appc_prev = avg_payoff_prev / (num_cooperators + 1e-09)
            expected_defect_payoff = 1 + k / n * avg_coop_rate
            if appc_prev >= expected_defect_payoff:
                return Action.C
            if num_cooperators < avg_coop_rate and appc_prev < expected_defect_payoff:
                return Action.D
        return Action.C if sum(history.opponent_actions[-1, :]) > n / 2 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_64(BaseStrategy):
    """
    A collective strategy that adapts to game history and social learning from others.
    Balances individual self-interest with collective well-being by cooperating 
    when previous round's total payoff is greater than or equal to average payoff so far,
    or if it's the last round, and defecting otherwise.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_payoff_avg = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_last_round_payoff = np.sum(history.opponent_payoffs[-1, :]) + history.my_payoffs[-1]
        self.total_payoff_avg = (self.total_payoff_avg * state.round_number + total_last_round_payoff) / (state.round_number + 1)
        previous_cooperators = np.sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif total_last_round_payoff >= self.total_payoff_avg and previous_cooperators > 0:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_65(BaseStrategy):
    """
    Adaptive Collective Convergence (ACC) strategy.
    Balances individual self-interest with collective cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def calculate_avg_payoff(self, n, k, previous_round):
        opponent_cooperators = sum(previous_round.opponent_actions[-1, :])
        return k / n * opponent_cooperators + 1 - np.mean(previous_round.my_actions)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number <= self.game_description.n_rounds // 4:
            if state.round_number % 2 == 1:
                return Action.D
            else:
                return Action.C
        else:
            avg_payoff = self.calculate_avg_payoff(self.game_description.n_players, self.game_description.k, history)
            if avg_payoff > self.game_description.k / self.game_description.n_players:
                return Action.C
            elif state.round_number == self.game_description.n_rounds - 1:
                return Action.C
            else:
                if history.my_actions[-1] and (not np.all(history.opponent_actions[-1, :])):
                    self.defection_reaction = 2
                    return Action.D
                elif hasattr(self, 'defection_reaction'):
                    self.defection_reaction -= 1
                    if self.defection_reaction > 0:
                        return Action.D
                return Action.D

class Strategy_COLLECTIVE_66(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy balances individual self-interest with collective well-being.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tolerance_threshold = 0.1 * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators >= self.game_description.n_players / 2 - self.tolerance_threshold:
            action = Action.C
        else:
            action = Action.D
        low_cooperation_rate = sum(history.my_actions) < 0.2 * state.round_number
        if low_cooperation_rate and random.random() < 0.2:
            action = Action.C
        elif low_cooperation_rate:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_COLLECTIVE_67(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Reciprocity (ACGR) in N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_coop_rate = np.mean(history.opponent_actions[-1, :])
        if prev_coop_rate > self.theta:
            return Action.C
        else:
            p_defect = min(1, state.round_number / self.game_description.n_rounds)
            return Action.D if random.random() < p_defect else Action.C
        if prev_coop_rate > 0.5:
            self.theta -= 0.05
        else:
            self.theta += 0.1

class Strategy_COLLECTIVE_68(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperative Tit-for-Tat (ACTT)

    Decision Rules:
    1. Initial Cooperation: In the first round, cooperate with a probability of 50%.
    2. Tit-for-Tat Adaptation: For rounds t > 1, if the number of cooperators in the previous round is greater than or equal to the number of defectors, cooperate.
    3. Punishment Mechanism: If a player defects when the majority cooperated in the previous round, punish by defecting for one round.
    4. Forgiveness: After punishing, revert to Tit-for-Tat adaptation.

    Edge Cases:
    * Last Round: In the final round (r), always cooperate regardless of previous rounds' outcomes.
    * Ties: If the number of cooperators equals the number of defectors in a round, consider it a "tie." In this case, maintain the previous action.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_probability = 0.5
        self.previous_round_cooperated = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_probability else Action.D
        opponent_actions = history.opponent_actions[-1, :]
        num_cooperators = sum(opponent_actions)
        if self.previous_round_cooperated is not None and (not self.previous_round_cooperated) and (num_cooperators > len(opponent_actions) / 2):
            return Action.D
        if num_cooperators >= len(opponent_actions) // 2:
            action = Action.C
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.C
        self.previous_round_cooperated = action == Action.C
        return action

class Strategy_COLLECTIVE_69(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Punishment (ACP)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.threshold = 0.5 + self.k / self.n_players * (self.n_players / 2)
        self.punished_players = []
        self.my_cooperate = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or len(history.opponent_actions) == 1:
            return Action.C
        avg_payoff_prev_round = np.mean(np.concatenate((history.my_payoffs[-1:], history.opponent_payoffs[-1, :])))
        if state.round_number > 1:
            for i in range(self.n_players - 1):
                opponent_defected = not history.opponent_actions[state.round_number - 2, i]
                more_than_half_cooperated = sum(history.opponent_actions[state.round_number - 2, :]) + (history.my_actions[-2] == Action.C) > self.n_players / 2
                if opponent_defected and more_than_half_cooperated:
                    self.punished_players.append(i)
        for i in range(self.n_players - 1):
            opponent_didnt_defect = history.opponent_actions[state.round_number - 1, i]
            if i in self.punished_players and opponent_didnt_defect:
                self.punished_players.remove(i)
        this_player_punished_by_others = sum((not action for action in history.opponent_actions[-2, :])) > 0
        if this_player_punished_by_others or state.round_number == self.n_rounds:
            self.my_cooperate = False
        elif avg_payoff_prev_round > self.threshold and len(self.punished_players) == 0:
            self.my_cooperate = True
        return Action.C if self.my_cooperate else Action.D

class Strategy_COLLECTIVE_70(BaseStrategy):
    """
    A collective strategy that balances individual payoff maximization with 
    collective cooperation, adapting to the game's history and opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.c_history = []
        self.θ = 0
        self.ε = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < self.game_description.n_players:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        self.c_history.append(prev_cooperators)
        if len(self.c_history) > 1 and self.c_history[-2] > self.game_description.n_players / 2:
            self.θ = 1 - self.ε
        else:
            self.θ = self.game_description.k / self.game_description.n_players * (prev_cooperators / self.game_description.n_players)
        if state.round_number == self.game_description.n_rounds or history.my_payoffs[-1] >= self.θ:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_71(BaseStrategy):
    """
    Collective Strategy: Adaptive Public Goods (APG)

    Decision Rules:
    - Initial Round: Cooperate to encourage cooperation and create a positive environment.
    - Subsequent Rounds: Use a hybrid approach combining reciprocity and public goods performance.

    Threshold Calculation: `threshold = (n * k) / (2 * n)`
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.n_players * game_description.k / (2 * game_description.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prevRoundCoops = sum(history.opponent_actions[-1, :])
        avgPayoffPrevRound = np.mean(np.concatenate((history.my_payoffs[-1:], history.opponent_payoffs[-1, :])))
        reciprocityThreshold = self.game_description.n_players / 2
        if prevRoundCoops >= reciprocityThreshold or avgPayoffPrevRound > self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_72(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Punishment (ACP)

    The ACP strategy balances individual self-interest with collective cooperation,
    adapting to the game's history while robustly responding to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.5
        self.previous_total_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        my_action = history.my_actions[-1]
        total_cooperators = sum(opponent_actions) + (1 if my_action else 0)
        majority_cooperated = total_cooperators > self.game_description.n_players / 2
        avg_payoff = np.mean(history.opponent_payoffs[-1, :]) + history.my_payoffs[-1]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if majority_cooperated:
            self.cooperation_probability = self.game_description.k / self.game_description.n_players * (total_cooperators / self.game_description.n_players)
            return Action.C if random.random() < self.cooperation_probability else Action.D
        elif avg_payoff < np.mean(history.my_payoffs):
            return Action.D
        else:
            self.adjust_cooperation_probability(total_cooperators)
            return Action.C if random.random() < self.cooperation_probability else Action.D

    def adjust_cooperation_probability(self, total_cooperators: int):
        """
        Adjust the cooperation probability based on changes in total cooperators.

        Increase p by 10% if total_cooperators increases, decrease p by 10% otherwise.
        Ensure p remains within bounds [0.1, k/n].
        """
        delta_p = 0.1 * (self.game_description.k / self.game_description.n_players)
        if total_cooperators > self.previous_total_cooperators:
            self.cooperation_probability = min(1, self.cooperation_probability + delta_p)
        elif total_cooperators <= self.previous_total_cooperators:
            self.cooperation_probability = max(0.1, self.cooperation_probability - delta_p)
        self.previous_total_cooperators = total_cooperators

class Strategy_COLLECTIVE_73(BaseStrategy):
    """
    Adaptive Collective Optimizer strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p = game_description.k / game_description.n_players
        self.theta = 1.5
        self.phi = 1
        self.w = 5
        self.x = 3
        self.gamma = game_description.n_rounds * self.theta

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p else Action.D
        avg_payoff_prev_round = np.mean(history.my_payoffs[-1:] + history.opponent_payoffs[-1, :])
        cooperators_prev_round = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        q = self.game_description.k / self.game_description.n_players * (cooperators_prev_round / self.game_description.n_players)
        if avg_payoff_prev_round > self.theta:
            return Action.C
        else:
            return Action.D if random.random() < q else Action.C
        avg_payoff_last_w_rounds = np.mean(history.my_payoffs[max(-self.w, -len(history.my_payoffs)):])
        if avg_payoff_last_w_rounds < self.phi and state.round_number + self.x <= self.game_description.n_rounds:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            total_payoff_so_far = np.sum(history.my_payoffs)
            return Action.C if total_payoff_so_far > self.gamma else Action.D

class Strategy_COLLECTIVE_74(BaseStrategy):
    """
    The Adaptive Cooperation strategy aims to balance individual self-interest with collective welfare by adapting to the evolving dynamics of cooperation and defection within the group.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_phase = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if total_cooperators < self.game_description.k and (not self.punishment_phase):
            self.punishment_phase = True
            return Action.D
        elif self.punishment_phase:
            self.punishment_phase = False
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif total_cooperators >= self.game_description.k:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_75(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Learning (ACL)
    
    ACL is a collective strategy designed for the N-Player Public Goods Game 
    that adapts to the behavior of other players while promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5
        self.prev_coops = 0
        self.avg_contrib = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.game_description.n_rounds // 4:
            return Action.C
        opponent_actions = history.opponent_actions[-self.game_description.n_rounds // 2:, :]
        avg_contrib = np.mean(np.concatenate((opponent_actions, history.my_actions[-self.game_description.n_rounds // 2:, None]), axis=1))
        if state.round_number > self.game_description.n_rounds // 2:
            prev_avg_payoff = np.mean(history.my_payoffs[-self.game_description.n_rounds // 2:-self.game_description.n_rounds // 4])
            curr_avg_payoff = np.mean(history.my_payoffs[-self.game_description.n_rounds // 4:])
            if curr_avg_payoff > prev_avg_payoff:
                self.theta -= 0.1
            else:
                self.theta += 0.1
        prev_coops = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if prev_coops < self.game_description.n_players // 2 and history.my_actions[-1] == Action.C:
            return Action.D
        if prev_coops < self.game_description.n_players // 4 and history.my_actions[-1] == Action.D:
            return Action.C
        if avg_contrib > self.theta:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_76(BaseStrategy):
    """
    Adaptive Cooperation with History-Based Adjustment (ACHA) strategy.

    This strategy adapts to the history of cooperation and defection, 
    balancing individual self-interest with collective well-being.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = np.array(history.opponent_actions)
        my_actions = np.array(history.my_actions)
        num_cooperators = sum(opponent_actions[-1, :]) + int(my_actions[-1])
        if state.round_number > 1:
            prev_num_cooperators = sum(opponent_actions[-2, :]) + int(my_actions[-2])
            if num_cooperators > prev_num_cooperators:
                self.p += 0.05
            elif num_cooperators < prev_num_cooperators:
                self.p -= 0.05
            self.p = max(0, min(self.p, 1))
        if num_cooperators >= self.game_description.n_players / 2:
            return Action.C
        else:
            if random.random() > self.p:
                return Action.D
            return Action.C

class Strategy_COLLECTIVE_77(BaseStrategy):
    """
    The Adaptive Cooperation (AC) strategy balances individual self-interest with collective well-being,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_payoff_prev_round = np.mean(history.my_payoffs + history.opponent_payoffs.sum(axis=1))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if avg_payoff_prev_round >= self.game_description.k / self.game_description.n_players:
            return Action.C
        elif np.sum(history.opponent_actions[-1, :]) > self.game_description.n_players // 2:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_79(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperative Escalation (ACE)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.first_round_cooperation_prob = 0.8

    def cooperate_with_prob(self, prob: float) -> Action:
        """
        Cooperate with a given probability.
        
        :param prob: Probability of cooperation (between 0 and 1)
        :return: Action.C if random.random() < prob else Action.D
        """
        return Action.C if random.random() < prob else Action.D

    def increase_cooperation_by(self, increase_prob: float) -> None:
        """
        Increase the probability of cooperation by a given amount.
        
        This method does not affect the class state but is used in the logic to decide whether
        to cooperate or defect based on the total payoff for all players in the previous round.

        :param increase_prob: Amount to increase the cooperation probability (between 0 and 1)
        """
        pass

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return self.cooperate_with_prob(self.first_round_cooperation_prob)
        prev_cooperators = sum([1 for action in history.opponent_actions[-1, :] if action])
        if prev_cooperators >= self.game_description.n_players / 2:
            return Action.C
        elif prev_cooperators < self.game_description.n_players / 4:
            return Action.D
        else:
            cooperation_prob = prev_cooperators / self.game_description.n_players
            opponent_payoffs = history.opponent_payoffs[-1, :]
            my_last_payoff = history.my_payoffs[-1]
            total_payoff = sum(opponent_payoffs) + my_last_payoff
            if total_payoff < self.game_description.k / 2 * self.game_description.n_players:
                cooperation_prob += 0.1
                cooperation_prob = min(cooperation_prob, 1)
                cooperation_prob = max(cooperation_prob, 0)
            return self.cooperate_with_prob(cooperation_prob)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_80(BaseStrategy):
    """
    A strategy that adapts to the collective behavior of opponents in a public goods game.
    It aims to balance cooperation with strategic self-interest by monitoring recent rounds' outcomes and adjusting its actions accordingly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperations = 0
        self.previous_round_payoff = 0
        self.my_previous_action = Action.D
        self.cooperation_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_payoff_prev_round = self.calculate_average_payoff(history)
        recent_cooperation_levels = self.evaluate_recent_cooperation_levels(self.cooperation_history, state.round_number)
        if avg_payoff_prev_round > 1.5:
            return Action.C
        elif recent_cooperation_levels > self.game_description.n_players / 2:
            return Action.D
        else:
            self.cooperation_history.append(sum(history.opponent_actions[-1, :]))
            if len(self.cooperation_history) > 3:
                self.cooperation_history.pop(0)
            return Action.C

    def calculate_average_payoff(self, history: PlayerHistory):
        total_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        avg_payoff_prev_round = (total_cooperators * self.game_description.k / self.game_description.n_players + (self.game_description.n_players - total_cooperators)) / self.game_description.n_players
        return avg_payoff_prev_round

    def evaluate_recent_cooperation_levels(self, cooperation_history: list[int], round_number: int):
        if len(cooperation_history) < 3:
            return sum(cooperation_history)
        recent_cooperation_levels = sum(cooperation_history[-3:])
        return recent_cooperation_levels

class Strategy_COLLECTIVE_81(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.threshold = self.k / self.n_players * (0.5 * self.n_players) / self.r
        self.p_init = 0.5
        self.p_punish = 0.2
        self.p_revive = 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        total_contributions = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        new_threshold = self.threshold + 1 / self.n_players * (self.k / self.n_players) * (total_contributions - self.threshold)
        self.threshold = new_threshold
        if total_contributions <= 0.5 * self.n_players and random.random() < self.p_punish:
            return Action.D
        if total_contributions == 0 and random.random() < self.p_revive:
            return Action.C
        if state.round_number == self.r - 1:
            return Action.C if total_contributions > self.threshold else Action.D
        return Action.C if total_contributions > self.threshold else Action.D

class Strategy_COLLECTIVE_82(BaseStrategy):
    """
    A collective strategy that balances individual and collective interests,
    while being adaptive and robust to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.round_number = 0
        self.my_actions = np.zeros(game_description.n_rounds, dtype=bool)
        self.opponent_actions = np.zeros((game_description.n_rounds, game_description.n_players - 1), dtype=bool)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.round_number = 1
            return Action.C
        self.round_number = state.round_number + 1
        previous_round_opponent_actions = history.opponent_actions[-1, :]
        num_cooperators = np.sum(previous_round_opponent_actions)
        if self.round_number <= 6:
            avg_payoff = np.mean(history.my_payoffs[-1])
            if avg_payoff > 1 or history.my_payoffs[-1] > 1:
                return Action.C
            else:
                return Action.D
        elif num_cooperators >= self.game_description.n_players / 2:
            return Action.C
        elif num_cooperators < self.game_description.n_players / 4:
            return Action.D
        else:
            p = self.game_description.k / self.game_description.n_players * num_cooperators
            return Action.C if random.random() < p else Action.D
        if self.round_number == self.game_description.n_rounds + 1:
            return Action.C
        return Action.C if random.choice([True, False]) else Action.D

class Strategy_COLLECTIVE_83(BaseStrategy):
    """
    Adaptive Cooperation with Grudges (ACG) Strategy.

    This strategy aims to promote cooperation while being robust against various opponent behaviors.
    It cooperates in the first round, then assesses the strength of cooperation and adapts its decisions accordingly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        ct = sum(history.opponent_actions[-1, :]) / (self.game_description.n_players - 1)
        gi = 0
        if not history.my_actions[-1]:
            avg_cooperator_payoff = np.mean(history.opponent_payoffs[-1, history.opponent_actions[-1, :]])
            if avg_cooperator_payoff > history.my_payoffs[-1]:
                gi = 1
        if ct >= self.game_description.k / self.game_description.n_players:
            return Action.C
        elif gi == 1:
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_84(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Retribution (ACR)

    ACR prioritizes collective well-being by encouraging cooperation through reciprocity,
    punishing free-riding to maintain fairness, and adapting to changing circumstances.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :] | np.array([True])) - 1
        prev_round_defectors = self.game_description.n_players - prev_round_cooperators - 1
        cooperate = prev_round_cooperators > prev_round_defectors or (prev_round_cooperators == prev_round_defectors and random.random() < 0.5)
        punish = False
        max_cooperator_payoff = self.game_description.k * min(1, sum(history.opponent_actions[-1, :] | np.array([True])) / self.game_description.n_players)
        for i in range(self.game_description.n_players - 1):
            if not history.opponent_actions[-1, i] and history.opponent_payoffs[-1, i] > 0.5 * max_cooperator_payoff:
                punish = True
                break
        return Action.D if punish else Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_85(BaseStrategy):
    """
    An adaptive collective strategy for the N-Player Public Goods Game,
    promoting cooperation and retaliating against defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_threshold = 0.75 * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_contributors = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        prev_defectors = self.game_description.n_players - prev_contributors
        if prev_contributors > self.game_description.n_players // 2:
            return Action.C
        if prev_contributors <= self.game_description.n_players // 2:
            return Action.D
        opponent_defection_rates = np.mean(history.opponent_actions == False, axis=0)
        if any((opponent_defection_rate > self.punish_threshold for opponent_defection_rate in opponent_defection_rates)):
            return Action.D
        opponent_contribution_rates = np.mean(history.opponent_actions, axis=0)
        for i, (opponent_contribution_rate, opponent_defection_rate) in enumerate(zip(opponent_contribution_rates, opponent_defection_rates)):
            if opponent_contribution_rate >= 0.5 and opponent_defection_rate <= 0.25:
                self.punish_threshold = 0.5 * self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            if prev_contributors > self.game_description.n_players // 2:
                return Action.C
            else:
                return Action.D
        return random.choice([Action.C, Action.D])

class Strategy_COLLECTIVE_86(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy.

    This strategy aims to balance individual self-interest with collective welfare by adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        prev_opponent_actions = history.opponent_actions[-1]
        prev_my_payoff = history.my_payoffs[-1]
        prev_avg_payoff = np.mean(history.opponent_payoffs[-1])
        cooperators = sum(prev_opponent_actions)
        if cooperators < self.game_description.n_players / 2 or prev_my_payoff < prev_avg_payoff:
            return Action.D
        if cooperators >= self.game_description.n_players / 2 and prev_my_payoff >= prev_avg_payoff:
            return Action.C
        if history.my_actions[-1] == Action.C and prev_my_payoff < prev_avg_payoff:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if cooperators >= self.game_description.n_players / 2:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_87(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_when_cooperating = 0.0
        self.opponent_cooperation_rate = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            p_init = 0.5
            return Action.C if random.random() < p_init else Action.D
        my_prev_action = history.my_actions[-1]
        if my_prev_action:
            self.avg_payoff_when_cooperating = (self.avg_payoff_when_cooperating * (state.round_number - 1) + history.my_payoffs[-1]) / state.round_number
        opponent_prev_actions = history.opponent_actions[-1, :]
        self.opponent_cooperation_rate = np.mean(opponent_prev_actions)
        CT = self.avg_payoff_when_cooperating
        k_over_n = self.game_description.k / self.game_description.n_players
        if CT >= k_over_n * self.game_description.n_players:
            return Action.C
        elif CT < 1 - k_over_n:
            return Action.D
        if self.opponent_cooperation_rate > 0.5:
            return Action.C
        elif self.opponent_cooperation_rate < 0.5 and CT > 1 - k_over_n:
            p_acc = 0.7
            return Action.C if random.random() < p_acc else Action.D
        if np.all(~history.opponent_actions):
            p_acc = 0.3
            return Action.C if random.random() < p_acc else Action.D
        return Action.D

class Strategy_COLLECTIVE_88(BaseStrategy):
    """
    Adaptive Reciprocity with Endgame Effect strategy.
    
    This strategy balances cooperation and self-interest while adapting to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self.payoffs.append(history.my_payoffs[-1])
        if history.opponent_actions.shape[0] == 1:
            return Action.C
        cooperators = sum(history.opponent_actions[-2, :])
        if cooperators >= self.game_description.n_players // 2:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            total_payoff_C = sum((payoff for payoff, action in zip(self.payoffs, history.my_actions) if action))
            total_payoff_D = sum((payoff for payoff, action in zip(self.payoffs, history.my_actions) if not action))
            return Action.C if total_payoff_C >= total_payoff_D else Action.D

class Strategy_COLLECTIVE_89(BaseStrategy):
    """
    Adaptive Conditional Cooperation (ACC) strategy for N-Player Public Goods Game.
    Balances individual self-interest with collective welfare, adapting to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_phase_active = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if len(history.opponent_actions) > 1 and sum(history.opponent_actions[-2, :]) < self.game_description.n_players // 2:
            self.punishment_phase_active = True
        elif len(history.opponent_actions) >= 3 and all((sum(round) >= self.game_description.n_players // 2 for round in history.opponent_actions[-2:])):
            self.punishment_phase_active = False
        if total_cooperators_prev_round >= self.game_description.n_players // 2:
            return Action.C
        elif self.punishment_phase_active:
            if any(history.opponent_actions[-1, :]) or history.my_actions[-1]:
                return Action.C
            else:
                return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif total_cooperators_prev_round <= self.game_description.n_players // 2 and len(history.opponent_actions) >= 2:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_90(BaseStrategy):
    """
    Adaptive Collective Optimism Strategy for N-Player Public Goods Game.

    This strategy adapts to various opponent behaviors and only relies on game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.round_number = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        opponent_actions = history.opponent_actions[-1, :]
        total_cooperators = np.sum(opponent_actions) + int(history.my_actions[-1])
        if total_cooperators >= self.n_players / 2:
            action = Action.C
        else:
            private_payoff = 1 - int(history.my_actions[-1])
            public_good_payoff = self.k / self.n_players * total_cooperators
            if public_good_payoff < 1 and random.random() < 0.5:
                action = Action.D
            else:
                action = Action.C
        if random.random() < 0.05:
            return random.choice([Action.C, Action.D])
        if self.round_number <= self.game_description.n_rounds * 0.2 and (not history.my_actions[-1]):
            if random.random() < 0.7:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            public_good_payoff = self.k / self.n_players * total_cooperators
            if public_good_payoff >= 1:
                return Action.C
        return action

class Strategy_COLLECTIVE_91(BaseStrategy):
    """
    A collective strategy for N-Player Public Goods Game that adapts behavior based on observed cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperated_last_round = False
        self.last_total_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_payoff_from_cooperation = self.calculate_total_payoff_from_cooperation(history)
        private_defect_payoff = 1.0
        if total_payoff_from_cooperation >= private_defect_payoff and self.last_total_cooperators > 0 and self.cooperated_last_round:
            return Action.C
        elif self.all_players_defected_last_round(history):
            return Action.C
        else:
            return Action.D
        self.update_internal_state(history)

    def calculate_total_payoff_from_cooperation(self, history: PlayerHistory) -> float:
        last_round_actions = history.opponent_actions[-1, :]
        total_cooperators = np.sum(last_round_actions) + (history.my_actions[-1] == Action.C)
        return self.game_description.k / self.game_description.n_players * total_cooperators

    def all_players_defected_last_round(self, history: PlayerHistory) -> bool:
        last_round_actions = history.opponent_actions[-1, :]
        return np.all(last_round_actions == False) and history.my_actions[-1] == Action.D

    def update_internal_state(self, history: PlayerHistory):
        self.cooperated_last_round = history.my_actions[-1] == Action.C
        self.last_total_cooperators = np.sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)

class Strategy_COLLECTIVE_92(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and aligns with the collective mindset.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.n_players * game_description.k / (2 * game_description.n_players + 1)
        self.previous_contributions = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None and history.opponent_actions.shape[0] > 0:
            previous_round_contributions = np.sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if len(history.my_actions) else 0)
        else:
            previous_round_contributions = self.previous_contributions
        self.previous_contributions = previous_round_contributions
        if previous_round_contributions >= self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_93(BaseStrategy):
    """
    A collective strategy called "Adaptive Collective Optimism" (ACO) that balances individual self-interest with collective well-being by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.n_players >= 4 and self.game_description.k > 1.5:
                return Action.C
            else:
                return Action.D
        c_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if c_prev >= self.game_description.n_players / 2:
            action = Action.C
        else:
            action = Action.D
        prev_payoff = 1 - history.my_actions[-1] + self.game_description.k / self.game_description.n_players * c_prev
        avg_payoff = np.mean(history.my_payoffs)
        if prev_payoff < avg_payoff:
            action = Action.C if action == Action.D else Action.D
        c_trend = [sum(history.opponent_actions[i, :]) + history.my_actions[i] for i in range(max(0, state.round_number - 3), state.round_number)]
        if len(c_trend) >= 3 and c_trend[-1] > c_trend[-2] > c_trend[-3]:
            action = Action.C
        elif len(c_trend) >= 3 and c_trend[-1] < c_trend[-2] < c_trend[-3]:
            action = Action.D
        total_payoff = np.sum(history.my_payoffs)
        avg_total_payoff = np.mean(np.sum(history.opponent_payoffs, axis=0))
        if state.round_number == self.game_description.n_rounds - 1 and total_payoff < avg_total_payoff:
            action = Action.C
        return action

class Strategy_COLLECTIVE_94(BaseStrategy):
    """
    Adaptive Cooperation with Retribution strategy for N-Player Public Goods Game.
    
    This strategy balances cooperation and retribution to adapt to various opponent behaviors.
    It encourages others to cooperate by initially contributing, then adjusts its behavior based on the previous round's outcome,
    introducing a degree of retribution for low cooperation levels while still allowing for potential future cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.is_last_round = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.is_last_round = state.round_number == self.game_description.n_rounds - 1
        prev_round_idx = -2 if self.is_last_round else -1
        prev_cooperators = sum(history.opponent_actions[prev_round_idx, :]) + history.my_actions[prev_round_idx]
        if prev_cooperators >= self.game_description.n_players / 2:
            return Action.C
        elif 0 < prev_cooperators < self.game_description.n_players / 2:
            retribution_prob = self.game_description.k / self.game_description.n_players * (prev_cooperators / self.game_description.n_players)
            return Action.D if random.random() < retribution_prob else Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_95(BaseStrategy):
    """
    Adaptive Cooperative Escalation (ACE) strategy for the N-Player Public Goods Game.
    
    This collective strategy balances individual self-interest with collective well-being 
    while adapting to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.alpha = self.n_players / 2 + math.sqrt(self.n_players) / 4
        self.beta = self.k * self.n_players / 2
        self.gamma = min(5, self.n_rounds // 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coops = sum(history.opponent_actions[-1, :] | [history.my_actions[-1]])
        avg_payoff_window = np.mean([history.my_payoffs[t] for t in range(max(0, state.round_number - self.gamma), state.round_number)])
        if prev_coops >= self.alpha:
            return Action.C
        elif history.my_payoffs[-1] < self.beta:
            return Action.C
        elif avg_payoff_window > history.my_payoffs[-1]:
            return Action(history.my_actions[-1])
        else:
            return Action.D
        if state.round_number == self.n_rounds - 1:
            return Action.D
        if self.n_players <= 3:
            if self.n_players == 2:
                self.alpha = 1
                self.beta = self.k * self.n_players / 4

class Strategy_COLLECTIVE_96(BaseStrategy):
    """
    This strategy implements the Collective Adaptive Cooperation with Gradual Reciprocity (ACGR) algorithm.
    
    The ACGR strategy balances individual self-interest with collective well-being by:
    - Cooperating initially to establish a cooperative tone and gather information.
    - Gradually adapting to the level of cooperation in the group, reciprocating cooperation when it is sufficient.
    - Increasing the likelihood of defection when cooperation is not reciprocated, while still maintaining some probability of cooperation.
    - Maximizing total game payoff by cooperating in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_level = 0.0
        self.payoff_difference = 0.0
        self.reciprocity_failure_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        self.cooperation_level = opponent_cooperators / (self.game_description.n_players - 1)
        my_last_payoff = history.my_payoffs[-1]
        opponents_avg_payoff = np.mean(history.opponent_payoffs[-1, :])
        self.payoff_difference = my_last_payoff - opponents_avg_payoff
        if self.cooperation_level >= self.game_description.k / self.game_description.n_players and self.payoff_difference <= 1:
            return Action.C
        else:
            self.reciprocity_failure_count += 1
            defect_probability = 1 - (self.game_description.k / self.game_description.n_players) ** self.reciprocity_failure_count
            return Action.D if random.random() < defect_probability else Action.C
        if state.round_number == self.game_description.n_rounds:
            return Action.C

class Strategy_COLLECTIVE_97(BaseStrategy):
    """
    A collective strategy that aims to balance individual self-interest with the benefits of cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate_history = []
        self.defect_count = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.cooperation_rate_history.append(np.mean(history.my_actions))
        for opponent, actions in enumerate(history.opponent_actions.T):
            defect_count = np.sum(~actions[-3:])
            self.defect_count[opponent] = defect_count
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        else:
            majority_cooperated = np.mean(history.opponent_actions[-1, :]) > 0.5
            opponent_defected = any((defect / state.round_number > 0.6 for defect in self.defect_count))
            if majority_cooperated or opponent_defected:
                action = Action.C
            else:
                action = Action.D
        avg_cooperation_rate = np.mean(self.cooperation_rate_history[-3:]) if len(self.cooperation_rate_history) >= 3 else 0.5
        if avg_cooperation_rate > 0.6:
            return Action.C if random.random() < (avg_cooperation_rate + 1) / 2 else Action.D
        elif avg_cooperation_rate < 0.4:
            return Action.D if random.random() < (1 - avg_cooperation_rate) / 2 else Action.C
        return action

class Strategy_COLLECTIVE_98(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Concession (ACGC) strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tau = 0.5
        self.p_concede = 0.2
        self.p_max = 0.8

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        p_prev = np.mean(history.opponent_actions[-1, :]) + history.my_actions[-1] / self.game_description.n_players
        pi_prev = history.my_payoffs[-1]
        opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        expected_defect_payoff = 1 - history.my_actions[-1] + self.game_description.k / self.game_description.n_players * opponent_cooperators
        if p_prev >= self.tau and pi_prev >= expected_defect_payoff:
            action = Action.C
        else:
            action = Action.D
            if history.my_actions[-1] == 0 and pi_prev < expected_defect_payoff:
                self.p_concede += 0.1
                self.p_concede = min(self.p_concede, self.p_max)
                if random.random() < self.p_concede:
                    action = Action.C
        return action

class Strategy_COLLECTIVE_99(BaseStrategy):
    """
    Adaptive Cooperation Strategy for Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop = [0] * game_description.n_rounds
        self.θ = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_coop_prev = sum(history.my_actions[:state.round_number - 1] + history.opponent_actions[:state.round_number - 1, :].sum(axis=1)) / (self.game_description.n_players * state.round_number)
        self.avg_coop[state.round_number - 1] = avg_coop_prev
        if state.round_number == self.game_description.n_rounds:
            return Action.C
        if self.avg_coop[state.round_number - 1] >= self.θ:
            return Action.C
        else:
            p = 1 - self.avg_coop[state.round_number - 1]
            return Action.D if random.random() < p else Action.C
        prev_round_opponent_actions = history.opponent_actions[-2, :] if state.round_number > 1 else [False] * self.game_description.n_players
        num_prev_round_defectors = self.game_description.n_players - sum(prev_round_opponent_actions) - (not history.my_actions[state.round_number - 2])
        if num_prev_round_defectors == 1:
            return Action.C

class Strategy_COLLECTIVE_100(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) strategy for the N-Player Public Goods Game.
    
    ACO balances individual self-interest with collective well-being by encouraging cooperation 
    while adapting to changing opponent behaviors. It fosters trust and promotes mutual benefits, 
    punishes non-cooperative behavior, and allows for exploration of the game environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_cooperation_prob = 0.8
        self.reciprocation_prob = 0.8
        self.conditional_cooperation_prob = 0.5
        self.punishment_prob = 0.8
        self.exploration_prob = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D]) if random.random() < self.initial_cooperation_prob else Action.D
        prev_round_actions = history.opponent_actions[-1, :]
        prev_round_coops = sum(prev_round_actions)
        maj_coop = prev_round_coops >= self.game_description.n_players // 2
        total_coops_exceed_threshold = prev_round_coops > self.game_description.n_players // 2
        if maj_coop:
            return Action.C if random.random() < self.reciprocation_prob else Action.D
        elif total_coops_exceed_threshold:
            return Action.C if random.random() < self.conditional_cooperation_prob else Action.D
        else:
            return Action.D if random.random() < self.punishment_prob else Action.C
        if random.random() < self.exploration_prob:
            return random.choice([Action.C, Action.D])

class Strategy_COLLECTIVE_101(BaseStrategy):
    """
    Collective strategy that balances individual self-interest with collective well-being 
    in the N-Player Public Goods Game by adapting to observed cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if np.all(history.opponent_actions == False):
            return Action.C
        n = self.game_description.n_players
        k = self.game_description.k
        CT = k / (n * (k - 1))
        cooperation_level = np.sum(history.opponent_actions[-1, :]) / n
        if cooperation_level >= CT:
            return Action.C
        else:
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.D
            return Action.D

class Strategy_COLLECTIVE_102(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective welfare by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_init = 0.8
        self.p_recip = 0.9
        self.p_punish = 0.8
        self.theta = 1.5
        self.epsilon = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < self.p_init else Action.D
        c_prev = sum(history.opponent_actions[-1, :] == True)
        if c_prev > self.game_description.n_players / 2:
            return Action.C if random.random() < self.p_recip else Action.D
        total_payoff_prev = np.sum(history.my_payoffs[-1:] + history.opponent_payoffs[-1, :])
        if total_payoff_prev < self.theta:
            return Action.D if random.random() < self.p_punish else Action.C
        return Action.C if random.random() < self.epsilon else Action.D

class Strategy_COLLECTIVE_103(BaseStrategy):
    """
    Adaptive Cooperative Escalation (ACE) strategy.
    
    This strategy aims to balance individual self-interest with collective well-being 
    by adaptively escalating cooperation based on observed behavior and game history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_level = 0.5
        self.defect_flag = False
        self.escape_cooperation = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        coop_percentage = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if coop_percentage > self.coop_level:
            self.coop_level = coop_percentage
            self.escape_cooperation = True
        elif coop_percentage < 0.5:
            self.defect_flag = True
        if coop_percentage >= 0.5 or self.escape_cooperation:
            return Action.C
        elif self.defect_flag or (state.round_number > 1 and history.opponent_actions[-2, :] == 0 and (history.opponent_actions[-1, :] == 0)).any():
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if coop_percentage >= 0.5:
                return Action.C
            else:
                return Action.D
        return Action.C

class Strategy_COLLECTIVE_104(BaseStrategy):
    """
    A collective strategy that balances individual payoff maximization with cooperation,
    adapting to the game's history and opponent behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if cooperators <= self.game_description.n_players // 2:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_105(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to the game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_prev_round = 0.0
        self.n_cooperators_prev_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        n_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        self.n_cooperators_prev_round = n_cooperators_prev_round + int(history.my_actions[-1])
        if self.game_description.k / self.game_description.n_players == 0:
            p_defect = 0
        else:
            p_defect = math.pow(self.n_cooperators_prev_round / self.game_description.n_players, 1 / self.game_description.k)
        avg_payoff_prev_round = np.mean(np.concatenate(([history.my_payoffs[-1]], history.opponent_payoffs[-1])))
        self.avg_payoff_prev_round = avg_payoff_prev_round
        if self.avg_payoff_prev_round >= 1 + self.game_description.k / self.game_description.n_players * (self.game_description.n_players / 2):
            return Action.C
        else:
            return random.choices([Action.C, Action.D], weights=[1 - p_defect, p_defect])[0]

class Strategy_COLLECTIVE_106(BaseStrategy):
    """
    Collective strategy balancing individual self-interest with collective well-being,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold_tau = game_description.n_players * game_description.k / (2 * game_description.n_players + game_description.k)
        self.adjustment_delta = 0.01
        self.community_focus_kappa = game_description.n_players * game_description.k / 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        prev_round_defects = self.game_description.n_players - prev_round_coops
        total_contributions = prev_round_coops * self.game_description.k
        if prev_round_coops >= self.threshold_tau:
            return Action.C
        elif prev_round_defects > self.game_description.n_players - self.threshold_tau:
            return Action.D
        elif total_contributions < self.community_focus_kappa:
            return Action.C
        else:
            return Action.D
        if prev_round_coops > self.game_description.n_players * self.threshold_tau / 100:
            self.threshold_tau -= self.adjustment_delta
        elif prev_round_defects > self.game_description.n_players - self.game_description.n_players * self.threshold_tau / 100:
            self.threshold_tau += self.adjustment_delta

class Strategy_COLLECTIVE_107(BaseStrategy):
    """
    The Adaptive Cooperation with Memory (ACM) strategy aims to balance individual payoffs 
    with collective well-being by adaptively adjusting cooperation levels based on game history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.memory_length = min(game_description.n_rounds // 2, 10)
        self.coop_payoffs = np.zeros(self.memory_length)
        self.total_payoffs = np.zeros(self.memory_length)
        self.my_cooperation_rates = np.zeros(self.memory_length)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choices([Action.C, Action.D], weights=[0.7, 0.3])[0]
        last_round_opponents_coop = sum(history.opponent_actions[-1, :])
        avg_payoff_last_round = np.mean(history.my_payoffs[-self.memory_length:])
        if history.my_actions[-1]:
            self.coop_payoffs = np.roll(self.coop_payoffs, -1)
            self.coop_payoffs[-1] = avg_payoff_last_round
            self.total_payoffs = np.roll(self.total_payoffs, -1)
            self.total_payoffs[-1] = avg_payoff_last_round * self.game_description.n_players + last_round_opponents_coop * self.game_description.k
            self.my_cooperation_rates = np.roll(self.my_cooperation_rates, -1)
            self.my_cooperation_rates[-1] = 1
        else:
            self.total_payoffs = np.roll(self.total_payoffs, -1)
            self.total_payoffs[-1] = avg_payoff_last_round * self.game_description.n_players + last_round_opponents_coop * self.game_description.k
            self.my_cooperation_rates = np.roll(self.my_cooperation_rates, -1)
            self.my_cooperation_rates[-1] = 0
        if state.round_number > 1:
            opponents_avg_coop_rate = np.mean(history.opponent_actions[-self.memory_length:, :])
        else:
            opponents_avg_coop_rate = 0.5
        avg_payoff_cooperators = np.mean(self.coop_payoffs)
        avg_total_payoff = np.mean(self.total_payoffs)
        if state.round_number == self.game_description.n_rounds - 1 or opponents_avg_coop_rate < 0.3:
            return Action.C
        elif len(history.my_actions) > 0 and last_round_opponents_coop >= 2:
            p_coop = max(0.5, self.game_description.k / self.game_description.n_players * avg_payoff_cooperators / avg_total_payoff)
        else:
            p_coop = 0.7
        return random.choices([Action.C, Action.D], weights=[p_coop, 1 - p_coop])[0]

class Strategy_COLLECTIVE_108(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperativity (AC)

    This strategy adapts to the game's dynamics by reciprocating altruism, punishing exploitation,
    and promoting a cooperative environment through forgiveness and an adaptive threshold.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_prob = 0.5
        self.avg_payoffs_coop = []
        self.avg_payoffs_defect = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.coop_prob else Action.D
        prev_round_actions = history.opponent_actions[-1, :]
        prev_round_payoffs = history.my_payoffs[-1]
        avg_payoff_coop_prev = np.mean(self.avg_payoffs_coop) if self.avg_payoffs_coop else 0.0
        avg_payoff_defect_prev = np.mean(self.avg_payoffs_defect) if self.avg_payoffs_defect else 0.0
        if avg_payoff_coop_prev > avg_payoff_defect_prev and any(prev_round_actions):
            action = Action.C
        else:
            action = Action.D
        if state.round_number > 1:
            if avg_payoff_coop_prev - avg_payoff_defect_prev > 0.2:
                self.coop_prob += 0.05
            elif avg_payoff_defect_prev - avg_payoff_coop_prev > 0.2:
                self.coop_prob -= 0.05
        if action == Action.C and history.my_actions[-1] == Action.D:
            self.coop_prob = 0.5
        if action == Action.C:
            self.avg_payoffs_coop.append(prev_round_payoffs)
        else:
            self.avg_payoffs_defect.append(prev_round_payoffs)
        return action

class Strategy_COLLECTIVE_109(BaseStrategy):
    """
    The Adaptive Cooperative Fingerprint (ACF) strategy aims to balance individual payoffs with collective cooperation, 
    adapting to the game's history and opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_defect = (1 - game_description.k / game_description.n_players) / (game_description.k / game_description.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_payoffs = history.opponent_payoffs[-1]
        prev_round_actions = history.opponent_actions[-1]
        coop_payoff = sum([payoff for payoff, action in zip(prev_round_payoffs, prev_round_actions) if action])
        defect_payoff = sum([payoff for payoff, action in zip(prev_round_payoffs, prev_round_actions) if not action])
        if coop_payoff > defect_payoff:
            action = Action.C
        else:
            action = Action.D if random.random() < self.p_defect else Action.C
        opponents_coop_rate = sum(prev_round_actions) / self.game_description.n_players
        if opponents_coop_rate > 0.5:
            action = Action.C
        if state.round_number > 1 and history.opponent_actions.shape[0] > 1:
            prev_prev_round_actions = history.opponent_actions[-2]
            defect_rate_increase = (sum(prev_round_actions) - sum(prev_prev_round_actions)) / self.game_description.n_players
            if defect_rate_increase > 0.2:
                action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.C
        return action

class Strategy_COLLECTIVE_110(BaseStrategy):
    """
    A collective strategy balancing individual self-interest with cooperation and adaptability.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.n_players <= 5:
                return Action.C
            else:
                return Action.D
        prev_coops = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_coops >= self.game_description.n_players / 2:
            p = self.game_description.k / self.game_description.n_players
            return Action.C if random.random() < p else Action.D
        if not history.my_actions[-1] and prev_coops > self.game_description.n_players / 2:
            p = 1 - self.game_description.k / self.game_description.n_players
            return Action.D if random.random() < p else Action.C
        if state.round_number % 5 == 0:
            return Action.C
        if not history.my_actions[-1]:
            self.consecutive_defections += 1
            if self.consecutive_defections >= 3 and sum(history.opponent_actions[-2:, :].sum(axis=1)) == 0:
                self.consecutive_defections = 0
                return Action.C
        else:
            self.consecutive_defections = 0
        if state.round_number == self.game_description.n_rounds - 1:
            prev_coops_second_last = sum(history.opponent_actions[-2, :]) + history.my_actions[-2]
            if prev_coops_second_last >= self.game_description.n_players / 2:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_111(BaseStrategy):
    """
    Adaptive Cooperation with Punishment (ACP) strategy for the N-Player Public Goods Game.
    
    This strategy aims to balance individual self-interest with collective well-being by adapting 
    to the evolving behavior of other players. It includes a punishment mechanism to deter 
    exploitation and maintain cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round = history.opponent_actions[-1, :]
        c = sum(previous_round)
        T = self.game_description.n_players * self.game_description.k / (2 * self.game_description.n_players)
        if history.my_payoffs.size > 0 and c < T:
            return Action.D
        elif c >= T:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_112(BaseStrategy):
    """
    Adaptive Cooperative Escalation (ACE) Strategy.
    
    This strategy prioritizes cooperation while being responsive to opponents' actions.
    It escalates cooperation when necessary, punishes defectors, and maintains cooperation even in the face of decreased participation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.escalate_cooperation = False

    def count_cooperators(self, actions):
        return sum((1 for action in actions if action))

    def calculate_payoff(self, my_action, total_contributions):
        k = self.game_description.k
        n = self.game_description.n_players
        c_i = int(my_action)
        return 1 - c_i + k / n * total_contributions

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.escalate_cooperation = False
            return Action.C
        n = self.game_description.n_players
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        prev_round_my_action = history.my_actions[-1]
        c_prev = self.count_cooperators(prev_round_opponent_actions)
        if c_prev >= n / 2:
            self.escalate_cooperation = False
            return Action.C
        for opponent_idx, opponent_action in enumerate(prev_round_opponent_actions):
            if not opponent_action and prev_round_my_action:
                payoff_diff = history.opponent_payoffs[-1, opponent_idx] - history.my_payoffs[-1]
                if payoff_diff > 0:
                    return Action.D
        if state.round_number >= 2:
            c_prev_prev = self.count_cooperators(history.opponent_actions[-2, :])
            total_contributions_prev = sum(prev_round_opponent_actions) + prev_round_my_action
            total_contributions_prev_prev = sum(history.opponent_actions[-2, :]) + history.my_actions[-2]
            if c_prev < 0.8 * c_prev_prev and self.calculate_payoff(True, total_contributions_prev) < self.calculate_payoff(True, total_contributions_prev_prev):
                self.escalate_cooperation = True
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if c_prev >= n / 2 else Action.D
        if all((not action for action in history.my_actions[-3:])):
            return Action.C
        if all((action for action in prev_round_opponent_actions)) and prev_round_my_action:
            return Action.C
        if self.escalate_cooperation:
            self.escalate_cooperation = not self.escalate_cooperation
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_113(BaseStrategy):
    """
    A collective strategy that promotes cooperation by initially cooperating, 
    punishing defectors, and adapting to changing opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.k / game_description.n_players * game_description.n_players / 2
        self.punishment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if history is not None:
            opponent_cooperators = sum(history.opponent_actions[-1, :])
            if opponent_cooperators >= self.cooperation_threshold:
                self.punishment_counter = 0
                return Action.C
            opponent_defectors = self.game_description.n_players - 1 - opponent_cooperators
            if opponent_defectors > self.game_description.n_players - self.cooperation_threshold:
                self.punishment_counter = min(self.game_description.n_rounds - state.round_number, 2)
        if self.punishment_counter > 0:
            self.punishment_counter -= 1
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_114(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and focuses on maximizing overall payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5
        self.target_payoff = 1.5
        self.lenient_threshold_rounds = 3
        self.my_payoffs: NDArray[np.float64] = np.zeros(game_description.n_rounds)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if prev_cooperators >= self.threshold * self.game_description.n_players or state.round_number < self.lenient_threshold_rounds:
            cooperate = True
        else:
            cooperate = False
        if state.round_number > self.lenient_threshold_rounds and history is not None:
            avg_payoff = np.mean(self.my_payoffs[:state.round_number])
            if avg_payoff > self.target_payoff:
                self.threshold += 0.05
            elif avg_payoff < self.target_payoff:
                self.threshold -= 0.05
        if state.round_number == self.game_description.n_rounds - 1:
            cooperate = True
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_115(BaseStrategy):
    """
    Collective strategy that balances individual self-interest with a willingness to cooperate when others do.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        previous_round_cooperators = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        if previous_round_cooperators > self.game_description.n_players / 2:
            return Action.C
        elif previous_round_cooperators < self.game_description.n_players / 2:
            return Action.D
        average_payoff = np.mean(history.opponent_payoffs[-1, :] + history.my_payoffs[-1])
        if history.my_payoffs[-1] > average_payoff:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            previous_previous_round_cooperators = sum(history.opponent_actions[-2, :] | history.my_actions[-2])
            if previous_previous_round_cooperators > self.game_description.n_players / 2:
                return Action.C
        total_contributions = np.sum(history.opponent_payoffs + history.my_payoffs)
        if total_contributions < self.game_description.k / self.game_description.n_players * self.game_description.n_players / 2:
            return Action.D
        return random.choice([Action.C, Action.D])

class Strategy_COLLECTIVE_116(BaseStrategy):
    """
    Adaptive Collective Optimism strategy.
    
    This strategy adapts to the group's cooperation level and adjusts its own behavior accordingly.
    It initially cooperates to establish a cooperative tone, then gradually updates the threshold 
    to balance individual and collective interests.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5
        self.avg_coop_prev = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_coop_curr = np.mean(history.opponent_actions[-1, :])
        self.threshold = max(0, min(1, self.threshold + self.game_description.k / self.game_description.n_players * (avg_coop_curr - self.avg_coop_prev)))
        self.avg_coop_prev = avg_coop_curr
        if avg_coop_curr >= self.threshold:
            action = Action.C
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_COLLECTIVE_117(BaseStrategy):
    """
    Adaptive Community Oriented (ACO) Strategy.

    Balances individual payoff with collective well-being while adapting to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploiters = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_players = self.game_description.n_players
        opponent_actions = history.opponent_actions[-1]
        cooperators = sum(opponent_actions)
        new_exploiters = {i for i, opp_action in enumerate(opponent_actions) if opp_action == 0 and np.sum(history.opponent_actions[:, i]) > len(history.opponent_actions) / 2}
        self.exploiters.update(new_exploiters)
        if cooperators >= n_players / 2 and (not self.exploiters):
            return Action.C
        elif new_exploiters or random.random() < 1 / (n_players + 1):
            return random.choice([Action.C, Action.D])
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_118(BaseStrategy):
    """
    Adaptive Reciprocity strategy for N-Player Public Goods Game.
    
    Combines elements of reciprocity and adaptability to promote cooperation 
    while being robust against various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tau = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1]
        n_cooperators = sum(opponent_actions)
        n_defectors = len(opponent_actions) - n_cooperators
        if n_cooperators >= n_defectors:
            action = Action.C
        else:
            action = Action.D
        avg_c_payoff = np.mean([payoff for payoff, act in zip(history.opponent_payoffs[-1], opponent_actions) if act])
        avg_d_payoff = np.mean([payoff for payoff, act in zip(history.opponent_payoffs[-1], opponent_actions) if not act])
        if avg_c_payoff > avg_d_payoff:
            self.tau = min(1, self.tau + 0.1)
        else:
            self.tau = max(0, self.tau - 0.1)
        exp_cooperators = n_cooperators / len(opponent_actions)
        if exp_cooperators >= self.tau * self.game_description.n_players:
            action = Action.C
        return action

class Strategy_COLLECTIVE_119(BaseStrategy):
    """
    This strategy adapts to the behavior of other players in a Public Goods game.
    It promotes cooperation by reciprocating cooperative actions and punishing defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_contribution_rate = np.mean(history.my_actions[:state.round_number])
        if self.game_description.k >= self.game_description.n_players / 2:
            return Action.C
        if history is not None and state.round_number > 1:
            previous_actions = history.opponent_actions[state.round_number - 1, :]
            for i, action in enumerate(previous_actions):
                if action == False and history.opponent_actions[state.round_number - 2, i] == True:
                    return Action.D
        if avg_contribution_rate > self.threshold:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_120(BaseStrategy):
    """
    Adaptive Collective Convergence (ACC) strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5
        self.punishment_threshold = 0.3
        self.convergence_criterion = 0.1
        self.p = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_contribution_rate = np.mean(history.opponent_actions[:, :])
        num_defectors = sum((1 for action in history.opponent_actions[-1, :] if not action))
        if avg_contribution_rate > self.threshold:
            return Action.C
        elif num_defectors / self.game_description.n_players > 0.25:
            return Action.D
        elif avg_contribution_rate < self.punishment_threshold:
            return Action.D
        else:
            avg_payoff = np.mean(history.opponent_payoffs)
            max_possible_payoff = self.game_description.k * self.game_description.n_players / self.game_description.n_players
            if abs(avg_payoff - max_possible_payoff) < self.convergence_criterion * max_possible_payoff:
                self.p = self.update_p(avg_contribution_rate, self.p)
                return Action.C if random.random() < self.p else Action.D
            elif state.round_number == self.game_description.n_rounds - 1:
                return Action.C
        return Action.D

    def update_p(self, avg_contribution_rate: float, p: float) -> float:
        if avg_contribution_rate > 0.5:
            p += 0.05
        else:
            p -= 0.05
        return max(0.1, min(p, 0.9))

class Strategy_COLLECTIVE_121(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Punishment (ACGP) Strategy.

    Balances cooperation and punishment to maximize payoffs while adapting to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_prev_round = 0
        self.p_punish = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        self.avg_coop_prev_round = (history.my_actions[-1] + opponent_cooperations) / self.game_description.n_players
        punish_threshold = self.game_description.k / (2 * self.game_description.n_players)
        if self.avg_coop_prev_round < punish_threshold:
            self.p_punish += 0.1 * (state.round_number - 1) / self.game_description.n_rounds
            self.p_punish = min(self.p_punish, 0.9)
        if random.random() < self.p_punish:
            return Action.D
        elif self.avg_coop_prev_round >= self.game_description.k / self.game_description.n_players or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_122(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to the cooperation level of other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tau = game_description.n_players // 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        total_payoff_prev_round = np.sum(history.opponent_payoffs[-1, :]) + history.my_payoffs[-1]
        if state.round_number > 1:
            avg_payoff_prev_round = np.mean(np.concatenate([history.opponent_payoffs[-2, :], [history.my_payoffs[-2]]]))
            if total_payoff_prev_round > avg_payoff_prev_round and self.tau > 0:
                self.tau -= 1
            elif total_payoff_prev_round < avg_payoff_prev_round:
                self.tau += 1
        if self.game_description.n_rounds - state.round_number <= 3:
            expected_payoff_from_C = self.game_description.k / self.game_description.n_players * prev_cooperators
            payoff_from_D = 1 + self.game_description.k / self.game_description.n_players * prev_cooperators
            if expected_payoff_from_C < payoff_from_D:
                return Action.D
        if prev_cooperators >= self.tau:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_123(BaseStrategy):
    """
    This strategy promotes cooperation while being robust to various opponent behaviors.
    It uses a collective mindset, encouraging mutual cooperation and punishing exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.cooperation_probability = 0.5
        self.punishment_defection_probability = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperation_probability else Action.D
        opponent_defected_last_two_rounds = all((not history.opponent_actions[-i, 0] for i in range(1, min(3, state.round_number + 1))))
        we_cooperated_last_round = history.my_actions[-1]
        punished_opponent = not history.opponent_actions[-1, 0]
        if opponent_defected_last_two_rounds:
            self.cooperation_probability += 0.1
        if punished_opponent and we_cooperated_last_round:
            self.punishment_defection_probability = 0.7
        average_cooperators = sum(history.my_actions[:-1] + history.opponent_actions[:-1, :].sum(axis=1)) / state.round_number
        cooperation_threshold = average_cooperators / self.n_players * self.k
        total_contributions = sum(history.my_actions[-1:] + history.opponent_actions[-1:, :].sum(axis=1))
        if total_contributions >= cooperation_threshold:
            action = Action.C
        else:
            action = Action.D
        return Action.D if random.random() < self.punishment_defection_probability else action

class Strategy_COLLECTIVE_124(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation.

    Decision Rules:
    1. First Round: Cooperate (C) to establish a cooperative tone and encourage others to reciprocate.
    2. Subsequent Rounds: Use a Tit-for-Tat-like approach, but with a twist:
        * If the majority of players cooperated in the previous round (i.e., ≥ n/2), cooperate (C).
        * Otherwise, defect (D) with a probability p = 1 - (k/n) \\* (previous_round_cooperators / n).
        * If the previous round was the last round (t = r), always defect (D).

    Edge Cases:
    1. Last Round: Always defect (D) to maximize individual payoff.
    2. Ties in Cooperation Rate: In cases where the number of cooperators is exactly equal to n/2, 
        cooperate (C) if the previous round's cooperation rate was higher than or equal to the current round's; otherwise, defect (D).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        majority_cooperated = previous_round_cooperators >= self.game_description.n_players / 2
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif majority_cooperated:
            return Action.C
        else:
            p_defect = 1 - self.game_description.k / self.game_description.n_players * (previous_round_cooperators / self.game_description.n_players)
            if random.random() < p_defect:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_125(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the pursuit of collective welfare.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        opponent_defectors = self.game_description.n_players - 1 - opponent_cooperators
        my_previous_action = history.my_actions[-1]
        avg_coop_payoff_prev_round = np.mean([payoff if action else 0 for payoff, action in zip(history.opponent_payoffs[-1, :], history.opponent_actions[-1, :])])
        if opponent_defectors == 0:
            avg_defect_payoff_prev_round = np.mean(history.opponent_payoffs[-1, :])
        elif opponent_cooperators == 0:
            avg_defect_payoff_prev_round = 0
        else:
            avg_defect_payoff_prev_round = np.mean([payoff if not action else 0 for payoff, action in zip(history.opponent_payoffs[-1, :], history.opponent_actions[-1, :])])
        if avg_coop_payoff_prev_round >= avg_defect_payoff_prev_round:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_126(BaseStrategy):
    """
    A collective strategy that balances individual payoffs with collective well-being by 
    adaptively responding to the group's cooperation level and the game's history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.m = min(game_description.n_rounds // 2, 5)
        self.tau = 0.6
        self.lambda_ = 0.4
        self.punishing = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_coop_rate = self.calculate_average_cooperation_rate(state, history)
        if avg_coop_rate > self.tau:
            self.punishing = False
            return Action.C
        elif avg_coop_rate < self.lambda_ and (not self.punishing):
            self.punishing = True
            return Action.D
        elif self.punishing and avg_coop_rate > self.tau:
            self.punishing = False
            return Action.C
        else:
            return Action.D

    def calculate_average_cooperation_rate(self, state: GameState, history: PlayerHistory) -> float:
        start_round = max(1, state.round_number - self.m + 1)
        sum_cooperations = np.sum(history.my_actions[start_round - 1:state.round_number] + np.sum(history.opponent_actions[start_round - 1:state.round_number, :], axis=1))
        return sum_cooperations / (self.game_description.n_players * self.m)

class Strategy_COLLECTIVE_127(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_prev = 0
        self.p_defect = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_payoff_prev = np.mean(history.my_payoffs[-1] + np.sum(history.opponent_payoffs[-1, :], axis=0))
        self.avg_payoff_prev = avg_payoff_prev
        if avg_payoff_prev >= self.game_description.k:
            return Action.C
        else:
            self.p_defect = (self.game_description.k - avg_payoff_prev) / (self.game_description.k - 1)
            if random.random() < self.p_defect:
                return Action.D
            else:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_128(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the benefits of cooperation.
    It adapts to various opponent behaviors while promoting a collective mindset.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.previous_majority_action = None
        self.previous_payoff = 0
        self.cooperative_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        majority_action = self.get_majority_action(history)
        if majority_action == Action.C:
            action = Action.C
        elif majority_action == Action.D:
            action = Action.D
        if self.previous_payoff < np.mean(history.opponent_payoffs[-1, :]):
            action = Action.D
        if self.cooperative_rounds >= self.game_description.n_players / 2:
            self.adjust_cooperation_likelihood(True)
        else:
            self.adjust_cooperation_likelihood(False)
        self.previous_majority_action = majority_action
        self.previous_payoff = np.mean(history.my_payoffs[-1:])
        self.cooperative_rounds += int(action == Action.C)
        return action

    def get_majority_action(self, history: PlayerHistory) -> Action:
        """Get the majority action from the previous round."""
        actions = np.concatenate((history.opponent_actions[-1, :], [True]))
        counts = np.bincount(actions.astype(int))
        if counts[0] > counts[1]:
            return Action.D
        elif counts[0] < counts[1]:
            return Action.C
        else:
            return Action.C

    def adjust_cooperation_likelihood(self, increase: bool):
        """Adjust the likelihood of cooperating based on the game history."""
        if increase:
            self.cooperative_rounds += 1
        else:
            self.cooperative_rounds -= 1

class Strategy_COLLECTIVE_129(BaseStrategy):
    """
    A strategy that encourages collective cooperation by adapting to the average contribution rate of others,
    punishing non-cooperators, and forgiving when others cooperate.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_contrib_rate = 0
        self.punish_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n = self.game_description.n_players
        k = self.game_description.k
        if history is not None and len(history.my_actions) > 1:
            avg_contrib_rate_t_minus_1 = np.mean(history.my_actions[:-1])
            coop_count = sum((1 for i in range(n - 1) if history.opponent_actions[-2, i]))
            self.avg_contrib_rate = (self.avg_contrib_rate * (len(history.my_actions) - 2) + coop_count) / (len(history.my_actions) - 1)
        exp_coop_payoff = k / n * (self.avg_contrib_rate + 1)
        exp_defect_payoff = 1 + k / n * self.avg_contrib_rate
        if exp_coop_payoff > exp_defect_payoff:
            action = Action.C
        elif self.avg_contrib_rate < self.punish_threshold:
            action = Action.D
        else:
            action = Action.D
        if action == Action.C and self.avg_contrib_rate >= self.punish_threshold:
            action = Action.C
        return action

class Strategy_COLLECTIVE_130(BaseStrategy):
    """
    This strategy initially sets an optimistic tone by cooperating randomly in the first round.
    It then adapts to the game's history, responding positively to cooperation from other players
    and encouraging reciprocity. The goal is to create a cooperative environment while being robust against various opponent strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.acr = None
        self.r_halfcoop = 0
        self.epsilon = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        else:
            opponent_actions = history.opponent_actions[-1, :]
            self.acr = np.mean(np.concatenate(([False], opponent_actions)))
            if self.acr >= 0.5:
                self.r_halfcoop += 1
            if self.acr is not None and self.acr >= 0.5 or self.r_halfcoop >= state.round_number / 2:
                return Action.C
            elif random.random() < self.epsilon:
                return Action.C if random.choice([True, False]) else Action.D
            else:
                return Action.D

class Strategy_COLLECTIVE_131(BaseStrategy):
    """
    Collective Strategy: "Adaptive Cooperator with Punishment" (ACP)
    
    This strategy balances cooperation, reciprocity, and punishment to create a robust collective approach.
    By adapting to the game's dynamics and responding to changes in cooperation levels, ACP aims to maximize overall payoffs while promoting a cooperative environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold_payoff = 1.5
        self.forgiveness_rate = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_actions = history.opponent_actions[-1]
        prev_round_payoffs = history.opponent_payoffs[-1]
        num_cooperators_prev_round = sum(prev_round_actions)
        p = num_cooperators_prev_round / self.game_description.n_players
        avg_payoff_prev_round = np.mean(prev_round_payoffs)
        if avg_payoff_prev_round < self.threshold_payoff:
            return Action.D
        if sum(history.my_actions) > 0 and avg_payoff_prev_round >= self.threshold_payoff:
            p += self.forgiveness_rate
        if random.random() < p:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_132(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for the N-Player Public Goods Game.

    This strategy adapts to various opponent behaviors and aligns with a collective mindset.
    It prioritizes cooperation, reciprocates cooperation when most players contribute,
    punishes widespread defection, and matches contributions when the group is divided.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialize strategy state variables.

        :param game_description: Game description object containing n_players, n_rounds, and k.
        """
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide the action to take based on the current state and history.

        :param state: Current game state object containing round_number.
        :param history: History of previous rounds' actions and payoffs (None for initial round).
        :return: Chosen action as an Action enum value (C or D).
        """
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        my_action = history.my_actions[-1]
        cooperators = sum(opponent_actions)
        if cooperators > self.game_description.n_players // 2:
            return Action.C
        elif cooperators < self.game_description.n_players // 2:
            return Action.D
        avg_contribution = self.game_description.k / self.game_description.n_players * sum(opponent_actions)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif cooperators == self.game_description.n_players // 2:
            if my_action and avg_contribution >= 1:
                return Action.C
            else:
                return Action.D
        return Action.C

class Strategy_COLLECTIVE_133(BaseStrategy):
    """
    Collective strategy that adapts to the game's dynamics and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0
        self.punish_next_round = False

    def update_theta(self, prev_cooperators):
        if prev_cooperators == 0:
            self.theta = max(1, min(self.game_description.n_players - 1, int(self.game_description.k / self.game_description.n_players * (prev_cooperators + 1))))
        else:
            self.theta = max(1, min(self.game_description.n_players - 1, int(self.game_description.k / self.game_description.n_players * prev_cooperators)))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or self.game_description.n_rounds - 1 == state.round_number:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if isinstance(history.my_actions, np.ndarray) else history.my_actions)
        self.update_theta(prev_cooperators)
        if self.punish_next_round:
            self.punish_next_round = False
            return Action.D
        if prev_cooperators >= self.theta:
            last_my_action = history.my_actions[-1] if isinstance(history.my_actions, np.ndarray) else history.my_actions
            if not last_my_action and any((not action for action in history.opponent_actions[-1])):
                self.punish_next_round = True
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_134(BaseStrategy):
    """
    A decision-making framework that balances individual self-interest with collective well-being by adapting to the evolving cooperative momentum of the group.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5
        self.alpha = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        cm_prev = np.mean(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        self.theta = (1 - self.alpha) * self.theta + self.alpha * cm_prev
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.C if cm_prev > self.theta else Action.D

class Strategy_COLLECTIVE_135(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the pursuit of collective well-being.
    It adapts to the environment by cooperating when enough others cooperate and signals a willingness to reciprocate.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_cooperators = self.calculate_average_cooperators(history)
        if avg_cooperators >= self.game_description.n_players / 2:
            return Action.C
        opponent_behavior = self.get_recent_opponent_actions(history, state.round_number)
        if self.recent_defector(opponent_behavior):
            return Action.D
        return Action.D

    def calculate_average_cooperators(self, history: PlayerHistory) -> float:
        cooperators_sum = sum([1 if action else 0 for actions in history.opponent_actions for action in actions])
        total_rounds = len(history.my_actions)
        return cooperators_sum / (total_rounds * self.game_description.n_players)

    def get_recent_opponent_actions(self, history: PlayerHistory, current_round: int) -> NDArray[np.bool_]:
        recent_rounds = max(0, current_round - 3)
        return history.opponent_actions[recent_rounds:]

    def recent_defector(self, opponent_behavior: NDArray[np.bool_]) -> bool:
        defectors = [not all(actions) for actions in opponent_behavior.T]
        return any(defectors)

class Strategy_COLLECTIVE_136(BaseStrategy):
    """
    This strategy encourages mutual benefit through initial cooperation and history-dependent adaptation.
    It incorporates triggered defection and punishment mechanisms to discourage exploitation while maintaining adaptability to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_prob = game_description.n_players / (game_description.n_players + game_description.k)
        self.payoffs_history = []
        self.avg_coop_payoff = 0.0
        self.avg_def_payoff = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.coop_prob else Action.D
        if len(self.payoffs_history) < state.round_number:
            self.payoffs_history.append(np.array([player_payoff for player_payoff in history.opponent_payoffs[-1, :]]))
        avg_coop_payoff = np.mean([payoff for i, payoff in enumerate(self.payoffs_history[-1]) if history.opponent_actions[-1, i]])
        avg_def_payoff = np.mean([payoff for i, payoff in enumerate(self.payoffs_history[-1]) if not history.opponent_actions[-1, i]])
        self.avg_coop_payoff = avg_coop_payoff
        self.avg_def_payoff = avg_def_payoff
        action = Action.C if avg_coop_payoff >= avg_def_payoff else Action.D
        max_diff = np.max(self.payoffs_history[-1]) - np.min(self.payoffs_history[-1])
        if history.my_payoffs[-1] < np.mean(history.my_payoffs[:-1]) and max_diff > self.game_description.k / self.game_description.n_players:
            action = Action.D
        for i, opponent_action in enumerate(history.opponent_actions[-1]):
            if not opponent_action:
                self.coop_prob *= (state.round_number - 1) / state.round_number
        return action

class Strategy_COLLECTIVE_138(BaseStrategy):
    """
    A strategy that initially cooperates and then reciprocates cooperation,
    punishing defectors with temporary defection before reverting to reciprocal cooperation.
    In the final round, it always cooperates for maximum collective payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if self.punishment_rounds > 0:
            self.punishment_rounds -= 1
            return Action.D
        opponent_cooperations = np.sum(history.opponent_actions[-1, :])
        if opponent_cooperations >= self.game_description.n_players / 2:
            my_last_action = history.my_actions[-1]
            if not my_last_action:
                self.punishment_rounds = 2
                return Action.D
            else:
                return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_139(BaseStrategy):
    """
    Adaptive Cooperative Foresight (ACF) strategy.
    
    This is a robust and adaptive collective strategy that balances individual self-interest 
    with collective well-being, leveraging game history to inform cooperative decisions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.k / game_description.n_players
        self.previous_round_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        total_payoff = np.sum(history.my_payoffs[-1]) + np.sum(history.opponent_payoffs[-1, :])
        avg_payoff_per_cooperator = total_payoff / (opponent_cooperators + 1) if opponent_cooperators > 0 else self.game_description.k / self.game_description.n_players
        if avg_payoff_per_cooperator >= self.cooperation_threshold and opponent_cooperators > 0:
            action = Action.C
        elif state.round_number == self.game_description.n_rounds - 1 and opponent_cooperators > 0 and (avg_payoff_per_cooperator >= self.game_description.k / self.game_description.n_players):
            action = Action.C
        else:
            action = Action.D
        if avg_payoff_per_cooperator > self.cooperation_threshold:
            self.cooperation_threshold += 0.05
        elif avg_payoff_per_cooperator <= self.cooperation_threshold and len(self.previous_round_payoffs) > 1 and (np.mean(self.previous_round_payoffs[-2:]) < self.game_description.k / self.game_description.n_players):
            self.cooperation_threshold -= 0.05
        return action

class Strategy_COLLECTIVE_140(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Public Goods Enhancement strategy.
    
    This strategy promotes collective cooperation by initially cooperating, 
    responding to the majority action in subsequent rounds (Tit-for-Tat), 
    enhancing public goods when sufficient players cooperate, and adapting 
    to the game's history to maintain a balance between individual and collective payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(game_description.k / 2)
        self.payoff_threshold = (1 + game_description.k / game_description.n_players) / 2
        self.cooperation_rate = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C if state.round_number == 0 else Action.D
        previous_opponent_actions = history.opponent_actions[-1, :]
        majority_action = np.mean(previous_opponent_actions) >= 0.5
        public_goods_enhancement_condition = np.sum(previous_opponent_actions) >= self.cooperation_threshold and history.my_actions[-1] == False
        if majority_action and self.cooperation_rate < 0.7:
            return Action.C
        elif public_goods_enhancement_condition:
            return Action.C
        else:
            return Action.D
        if state.round_number % 5 == 0 and history.my_payoffs.shape[0] >= 5:
            average_payoff = np.mean(history.my_payoffs[-5:])
            if average_payoff < self.payoff_threshold:
                self.cooperation_rate += 0.1

class Strategy_COLLECTIVE_141(BaseStrategy):
    """
    A hybrid strategy that combines elements of tit-for-tat (TFT) with a cooperative bias and adaptations based on the game's history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defect_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_outcomes = history.opponent_actions[-1, :] == 1
        total_cooperators = sum(previous_round_outcomes)
        if total_cooperators > n / 2:
            return Action.C
        elif total_cooperators < n / 2:
            self.defect_counter += 1
            if self.defect_counter >= 3:
                self.defect_counter = 0
                return Action.C
            return Action.D
        else:
            average_payoff_last_round = k / n * total_cooperators + 1 - total_cooperators / n
            if average_payoff_last_round > 1.5:
                return Action.C
            return Action.D

class Strategy_COLLECTIVE_142(BaseStrategy):
    """
    Adaptive Cooperative Foresight Strategy.
    
    This collective strategy balances cooperation and defection based on the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n = game_description.n_players
        r = game_description.n_rounds
        k = game_description.k
        self.theta = k / n * (1 - 1 / r)
        self.acr_window = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        w = 3
        acr_window_length = len(self.acr_window)
        if acr_window_length < w:
            self.acr_window.append(history.my_actions[-1])
        else:
            self.acr_window.pop(0)
            self.acr_window.append(history.my_actions[-1])
        avg_payoff = np.mean(history.my_payoffs[-w:])
        if avg_payoff > 0.8 * self.game_description.k:
            self.theta += 0.05
        elif avg_payoff < 0.5 * self.game_description.k and self.theta > self.game_description.k / self.game_description.n_players:
            self.theta -= 0.05
        acr = sum(self.acr_window) / len(self.acr_window)
        if acr >= self.theta:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_143(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.n_players * game_description.k / (game_description.n_players + game_description.k)
        self.adaptation_step = 0.01
        self.successive_cooperation = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        self.successive_cooperation = all([action for action in history.my_actions]) and all([all(row) for row in history.opponent_actions])
        if self.successive_cooperation:
            self.cooperation_threshold += self.adaptation_step
        elif total_cooperators == 0:
            self.cooperation_threshold -= self.adaptation_step
        if total_cooperators >= self.cooperation_threshold or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_144(BaseStrategy):
    """
    A collective strategy that combines elements of tit-for-tat, public goods awareness, and adaptive behavior to promote cooperation while being robust to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.internal_state = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_cooperators_previous_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        p = self.game_description.k / self.game_description.n_players * (total_cooperators_previous_round / self.game_description.n_players)
        if total_cooperators_previous_round >= self.game_description.n_players / 2:
            return Action.C
        elif random.random() < p:
            return Action.D
        q = 1 - self.game_description.k / self.game_description.n_players * (total_cooperators_previous_round / self.game_description.n_players)
        if q > 0 and history.my_actions[-1] == Action.D:
            if random.random() < q:
                return Action.D
        if history.my_actions[-1] == Action.C and q > 0:
            self.internal_state = True
            if random.random() < 1 - p:
                self.internal_state = False
        if self.internal_state is None or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif self.internal_state:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_145(BaseStrategy):
    """
    Collective Strategy: Adaptive Reciprocal Altruism (ARA)
    
    ARA aims to balance individual self-interest with collective well-being by adapting to the game's history and encouraging cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5 * (game_description.n_players + 1) / 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            avg_coop_prev_round = sum(history.opponent_actions[-2, :] + [history.my_actions[-2]]) / self.game_description.n_players
            self.cooperation_threshold = (self.cooperation_threshold * state.round_number + avg_coop_prev_round) / (state.round_number + 1)
            if avg_coop_prev_round >= self.cooperation_threshold:
                return Action.C
            else:
                prev_payoff = self._calculate_individual_payoff(history)
                avg_defector_payoff = self._calculate_average_payoff_of_defectors(history)
                if prev_payoff <= avg_defector_payoff:
                    return Action.D
                else:
                    return Action.C

    def _calculate_individual_payoff(self, history: PlayerHistory) -> float:
        total_coop_prev_round = sum(history.opponent_actions[-2, :] + [history.my_actions[-2]])
        if history.my_actions[-2]:
            return self.game_description.k / self.game_description.n_players * total_coop_prev_round
        else:
            return 1 + self.game_description.k / self.game_description.n_players * total_coop_prev_round

    def _calculate_average_payoff_of_defectors(self, history: PlayerHistory) -> float:
        num_defectors = sum((not action for action in history.opponent_actions[-2, :]))
        if not history.my_actions[-2]:
            total_coop_prev_round = sum(history.opponent_actions[-2, :] + [history.my_actions[-2]])
            return 1 + self.game_description.k / self.game_description.n_players * total_coop_prev_round + sum((1 + self.game_description.k / self.game_description.n_players * total_coop_prev_round for _ in range(num_defectors - 1)))
        else:
            total_coop_prev_round = sum(history.opponent_actions[-2, :] + [history.my_actions[-2]])
            return sum((1 + self.game_description.k / self.game_description.n_players * total_coop_prev_round for _ in range(num_defectors)))
        if num_defectors > 0:
            return total_payoff_of_defectors / num_defectors
        else:
            return float('inf')

class Strategy_COLLECTIVE_146(BaseStrategy):
    """
    Adaptive Cooperative Escalation (ACE) strategy.

    This strategy balances individual self-interest with collective welfare by adaptively responding to the level of cooperation in the group.
    It uses a simple decision rule that depends only on the game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.ACR = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        n_cooperators = sum(opponent_actions)
        self.ACR = (n_cooperators + 1) / self.game_description.n_players
        CT = 0.5
        p = self.ACR
        q = self.game_description.k / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.ACR >= CT:
            return Action.C if random.random() < p else Action.D
        else:
            return Action.C if random.random() < q else Action.D

class Strategy_COLLECTIVE_147(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and aligns with the collective mindset.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = max(0, game_description.k / game_description.n_players * (game_description.n_players - 1))
        self.delta = 0.01 * game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_contributions_prev_round = sum(history.opponent_actions[-1, :] + [True])
        avg_payoff_prev_round = np.mean(np.concatenate((history.my_payoffs, history.opponent_payoffs[-1, :])))
        if state.round_number < self.game_description.n_rounds - 1:
            if total_contributions_prev_round >= self.threshold:
                action = Action.C
            else:
                action = Action.D
            if avg_payoff_prev_round > (self.game_description.k - 1) / self.game_description.n_players:
                self.threshold -= self.delta
            else:
                self.threshold += self.delta
        elif total_contributions_prev_round >= self.threshold:
            action = Action.C
        else:
            action = Action.D
        return action

class Strategy_COLLECTIVE_148(BaseStrategy):
    """
    Collective strategy: Adaptive Tit-for-Tat with Public goods.
    
    Balances individual self-interest with the benefits of cooperation 
    in the public goods game, adapting to the behavior of other players 
    while being robust to various opponent strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.k / game_description.n_players
        self.total_payoff_threshold_high = 1.5 * game_description.n_players
        self.total_payoff_threshold_low = 0.5 * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        total_payoff = sum(history.my_payoffs) + sum((sum(opponent_payoffs) for opponent_payoffs in history.opponent_payoffs))
        if total_payoff > self.total_payoff_threshold_high:
            self.cooperation_threshold += 0.1 / self.game_description.n_players
        elif total_payoff < self.total_payoff_threshold_low:
            self.cooperation_threshold -= 0.1 / self.game_description.n_players
        num_cooperators_last_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if num_cooperators_last_round >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_149(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Withdrawal strategy.
    
    This strategy promotes cooperation while being robust to a wide range of opponent behaviors.
    It adapts to the game's history by adjusting the cooperation threshold based on the average 
    cooperation rate and individual payoffs. By gradually withdrawing from cooperation when 
    opponents exploit, ACGW aims to maintain a balance between individual payoff maximization 
    and collective welfare.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5 * (game_description.k / game_description.n_players) / (game_description.n_players - 1)
        self.avg_coop_prev = None
        self.my_payoff_prev = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_coop_prev = np.mean(history.opponent_actions[-1, :] + (history.my_actions[-1] == Action.C))
        self.avg_coop_prev = avg_coop_prev
        if avg_coop_prev >= self.theta:
            action = Action.C
        else:
            action = Action.D
            if history.my_payoffs[-1] > np.mean(history.opponent_payoffs[-1, :]):
                self.theta *= 0.9
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if np.sum(history.opponent_actions[-1, :] + (history.my_actions[-1] == Action.C)) == self.game_description.n_players - 1:
            if history.my_payoffs[-1] > np.mean(history.opponent_payoffs[-1, :]):
                self.theta *= 0.8
        return action

class Strategy_COLLECTIVE_150(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Withdrawal (ACGW) Strategy.

    This strategy aims to promote collective cooperation by adapting to the average cooperation rate of opponents.
    It encourages initial cooperation, assesses the collective contribution, and gradually withdraws cooperation if necessary.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initializes the ACGW strategy with the given game description.

        :param game_description: The game's parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.coop_prob = 1.0
        self.min_coop_prob = 0.2
        self.avg_coop_rate = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determines the action to take based on the current game state and history.

        :param state: The current game state (round_number).
        :param history: The player's history (my_actions, my_payoffs, opponent_actions, opponent_payoffs).
        :return: The chosen action (Action.C or Action.D).
        """
        if state.round_number == 0:
            return Action.C
        avg_coop_rate_window = min(5, state.round_number)
        opponent_actions_last_few_rounds = history.opponent_actions[-avg_coop_rate_window:, :]
        self.avg_coop_rate = np.mean(opponent_actions_last_few_rounds)
        if self.avg_coop_rate > self.game_description.k / self.game_description.n_players * (self.game_description.n_players / 2):
            self.coop_prob = 1.0
        elif self.avg_coop_rate < self.game_description.k / self.game_description.n_players * (self.game_description.n_players / 2) and self.coop_prob > self.min_coop_prob:
            self.coop_prob -= 0.05
        if state.round_number == self.game_description.n_rounds - 1:
            expected_coop_payoff = self.game_description.k / self.game_description.n_players * (self.avg_coop_rate + 1)
            expected_defect_payoff = 1 + self.game_description.k / self.game_description.n_players * self.avg_coop_rate
            if expected_coop_payoff > expected_defect_payoff:
                return Action.C
        if random.random() < self.coop_prob:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_151(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that balances cooperation and defection based on the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def _cooperate_with_probability(self, p: float) -> Action:
        return Action.C if random.random() < p else Action.D

    def _defect_with_probability(self, p: float) -> Action:
        return Action.D if random.random() < p else Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return self._cooperate_with_probability(0.5)
        n_cooperators = sum(history.opponent_actions[-1, :])
        my_payoff = history.my_payoffs[-1]
        opponent_payoffs = history.opponent_payoffs[-1, :]
        avg_opponent_payoff = np.mean(opponent_payoffs)
        if avg_opponent_payoff > self.game_description.k / 2:
            return self._defect_with_probability(0.7)
        elif n_cooperators >= self.game_description.n_players // 2 and my_payoff <= self.game_description.k / 2:
            return self._cooperate_with_probability(0.8)
        elif n_cooperators < self.game_description.n_players // 2 and avg_opponent_payoff < self.game_description.k / 2:
            return self._defect_with_probability(0.6)
        if state.round_number % 3 == 0:
            return self._cooperate_with_probability(0.9)
        cooperators_prev_rounds = sum(history.opponent_actions[-2:, :].sum(axis=1))
        if cooperators_prev_rounds < self.game_description.n_players // 4 * 2:
            return self._defect_with_probability(0.8)
        return self._cooperate_with_probability(0.5)

class Strategy_COLLECTIVE_152(BaseStrategy):
    """
    Adaptive Cooperation with Social Learning (ACSL) Strategy.

    This strategy balances individual self-interest with collective welfare by adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.quorum_threshold = 2 / 3
        self.defect_threshold = 1 / 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        try:
            avg_coop_payoff_prev_round = np.mean([payoff for payoff, is_cooperator in zip(history.opponent_payoffs[-1], history.opponent_actions[-1]) if is_cooperator] + [history.my_payoffs[-1]] if history.my_actions[-1] else [])
            avg_defect_payoff_prev_round = np.mean([payoff for payoff, is_cooperator in zip(history.opponent_payoffs[-1], history.opponent_actions[-1]) if not is_cooperator])
        except ZeroDivisionError:
            avg_coop_payoff_prev_round = 0
            avg_defect_payoff_prev_round = 0
        if previous_round_cooperators >= self.quorum_threshold * self.game_description.n_players:
            return Action.C
        elif len([payoff for payoff, is_cooperator in zip(history.opponent_payoffs[-1], history.opponent_actions[-1]) if not is_cooperator] + [history.my_payoffs[-1]] if not history.my_actions[-1] else []) >= self.defect_threshold * self.game_description.n_players:
            return Action.D
        if avg_coop_payoff_prev_round <= avg_defect_payoff_prev_round:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_153(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and game histories.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p = game_description.k / game_description.n_players
        self.avg_contributors = 0
        self.consecutive_free_ride = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p else Action.D
        recent_rounds = min(state.round_number, 5)
        avg_contributors = np.mean(np.sum(history.opponent_actions[-recent_rounds:], axis=1) + history.my_actions[-recent_rounds:])
        if avg_contributors >= self.game_description.n_players / 2:
            self.p = min(self.p + 0.1, self.game_description.k / self.game_description.n_players)
        elif avg_contributors < self.game_description.n_players / 4:
            self.p = max(self.p - 0.1, 0.5 * self.game_description.k / self.game_description.n_players)
        if np.sum(history.opponent_actions[-3:], axis=0).mean() < self.game_description.n_players / 2 and state.round_number > 2:
            self.consecutive_free_ride += 1
            if self.consecutive_free_ride > 2:
                return Action.D
        if random.random() < 0.05:
            return Action.C if random.random() < 0.5 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if avg_contributors >= self.game_description.n_players / 2 else Action.D
        return Action.C if random.random() < self.p else Action.D

class Strategy_COLLECTIVE_154(BaseStrategy):
    """
    Adaptive Cooperative Foresight (ACF) strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k
        self.p_i_m = np.zeros((self.n,), dtype=np.float64)
        self.C_t_1 = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = 0.5
            return Action.C if random.random() < cooperate_prob else Action.D
        opponent_actions_last_round = history.opponent_actions[-1, :]
        self.C_t_1 = np.mean(opponent_actions_last_round)
        if self.C_t_1 >= self.k / self.n:
            action = Action.C
        else:
            opponent_coop_rate = np.mean(self.p_i_m)
            if opponent_coop_rate >= 0.5:
                action = Action.C
            else:
                action = Action.D
        if state.round_number == self.r - 1:
            if self.C_t_1 >= self.k / self.n:
                action = Action.C
            else:
                action = Action.D
        self.p_i_m[state.round_number % self.n] = 1.0 if action == Action.C else 0.0
        return action

class Strategy_COLLECTIVE_155(BaseStrategy):
    """
    Adaptive Cooperation with Punishment strategy for the N-Player Public Goods Game.

    This strategy initially cooperates to encourage cooperation, then adapts based on opponents' behavior.
    If opponents are cooperative, it continues to cooperate; otherwise, it punishes by defecting for two rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_prev = 0
        self.punish_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_payoffs = np.sum(history.opponent_payoffs[-1, :])
        self.avg_payoff_prev = (opponent_payoffs + history.my_payoffs[-1]) / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.avg_payoff_prev >= 1 and self.punish_rounds == 0:
            return Action.C
        elif self.punish_rounds < 2:
            self.punish_rounds += 1
            return Action.D
        else:
            self.punish_rounds = 0
            return Action.C

class Strategy_COLLECTIVE_156(BaseStrategy):
    """
    The Adaptive Cooperation strategy balances individual payoff maximization with collective welfare,
    adapting to the behavior of other players over time.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (history and len(history.my_actions) >= self.game_description.n_rounds - 1):
            return Action.C
        opponent_cooperations = sum((1 for action in history.opponent_actions[-1, :] if action))
        threshold = self.game_description.n_players / 2
        p = opponent_cooperations / self.game_description.n_players * 2
        p = max(0.0, min(p, 1.0))
        return Action.C if random.random() < p else Action.D

class Strategy_COLLECTIVE_157(BaseStrategy):
    """
    A collective strategy that adapts to the game's history and parameters,
    balancing individual self-interest with collective well-being.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate_history = []
        self.total_payoffs = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.game_description.n_players:
            return Action.C
        avg_cooperation_rate = sum(self.cooperation_rate_history[-self.game_description.n_players:]) / self.game_description.n_players
        if avg_cooperation_rate > (self.game_description.k - 1) / self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        self.cooperation_rate_history.append(action == Action.C)
        if state.round_number == self.game_description.n_rounds - 1:
            expected_contributions = sum(self.cooperation_rate_history[-self.game_description.n_players:]) / self.game_description.n_players
            if self.game_description.k / self.game_description.n_players * expected_contributions > 1 - (action == Action.C):
                action = Action.C
        return action

class Strategy_COLLECTIVE_158(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective well-being.
    It adapts to various opponent behaviors and promotes cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_contributions = 0
        self.previous_payoffs = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n = self.game_description.n_players
        k = self.game_description.k
        prev_actions = history.opponent_actions[-1, :]
        prev_payoffs = history.opponent_payoffs[-1, :]
        total_cooperators = sum(prev_actions)
        self.total_contributions += total_cooperators
        if self.total_contributions >= n * k / 2:
            return Action.C
        avg_coop_payoff = np.mean(prev_payoffs[prev_actions])
        if prev_payoffs.max() > avg_coop_payoff:
            return Action.D
        if history.my_actions[-1] == 0 and np.all(prev_actions):
            return Action.C
        if state.round_number >= self.game_description.n_rounds / 2 and np.all(history.opponent_actions):
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1 and self.total_contributions >= n * k / 2:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_159(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Forgiveness (ACF)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] is True)
        if self.threshold is None or state.round_number == self.game_description.n_rounds - 1:
            if state.round_number < self.game_description.n_rounds - 1:
                self.threshold = self.game_description.k / self.game_description.n_players * (1 + (self.game_description.n_rounds - state.round_number - 1) / self.game_description.n_rounds)
        if math.isclose(previous_round_cooperators, self.threshold):
            return Action.D
        if previous_round_cooperators >= self.threshold:
            return Action.C
        else:
            return Action.D
        if state.round_number < self.game_description.n_rounds - 1:
            self.threshold = self.game_description.k / self.game_description.n_players * (1 + (self.game_description.n_rounds - state.round_number - 2) / self.game_description.n_rounds)

class Strategy_COLLECTIVE_160(BaseStrategy):
    """
    A collective strategy that balances individual payoff with collective well-being by adapting to the game's history and encouraging cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_thr = game_description.k / game_description.n_players * (1 + (game_description.n_players - 1) / game_description.n_players)
        self.avg_coop = 0
        self.prev_round_coop = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        total_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        self.avg_coop = (self.prev_round_coop + total_cooperators) / (2 * self.game_description.n_players)
        if self.avg_coop >= self.coop_thr:
            action = Action.C
        else:
            action = Action.D
        if total_cooperators < self.game_description.n_players / 2:
            self.coop_thr *= 1.1
        if history.my_actions[-1] == Action.C and total_cooperators == 0:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if self.avg_coop >= self.coop_thr:
                action = Action.C
            else:
                action = Action.D
        if total_cooperators == 0 and history.my_actions[-1] != Action.C:
            self.coop_thr = self.game_description.k / self.game_description.n_players * (1 + (self.game_description.n_players - 1) / self.game_description.n_players)
        self.prev_round_coop = total_cooperators
        return action

class Strategy_COLLECTIVE_161(BaseStrategy):
    """
    Adaptive Reciprocal Cooperation strategy for the N-Player Public Goods Game.
    
    This strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and encouraging cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5
        self.total_payoff = 0.0
        self.average_payoff = 0.0
        self.defection_phase = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.total_payoff += history.my_payoffs[-1]
        self.average_payoff = self.total_payoff / (state.round_number + 1)
        if history.my_payoffs[-1] > self.average_payoff:
            self.threshold -= 0.1
        else:
            self.threshold += 0.1
        self.threshold = max(0, min(self.threshold, 1))
        if self.defection_phase:
            self.defection_phase = False
            return Action.D
        cooperators = sum(history.opponent_actions[-1, :])
        cr = cooperators / self.game_description.n_players
        if cr < 0.2 and all((history.my_payoffs[max(0, state.round_number - i - 1)] < self.average_payoff for i in range(3))):
            self.defection_phase = True
            return Action.D
        if cr >= self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_162(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Punishment (ACGP) strategy.
    
    This strategy prioritizes cooperation while being adaptive to the behavior of other players.
    By gradually punishing persistent defectors, it encourages cooperation and promotes a collective benefit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.consecutive_rounds_below_threshold = 0
        self.max_consecutive_rounds_below_threshold = 3
        self.fixed_number_of_rounds_to_defect = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        cooperation_rate = (opponent_cooperators + 1) / self.n_players
        threshold = 0.5 + (self.k - 1) / (2 * self.n_players)
        if cooperation_rate >= threshold:
            self.consecutive_rounds_below_threshold = 0
            return Action.C
        self.consecutive_rounds_below_threshold += 1
        if self.consecutive_rounds_below_threshold > self.max_consecutive_rounds_below_threshold:
            for _ in range(self.fixed_number_of_rounds_to_defect):
                return Action.D
            self.consecutive_rounds_below_threshold = 0
        defect_probability = (threshold - cooperation_rate) / (1 - threshold)
        if random.random() < defect_probability:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_163(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Threshold strategy.
    
    This strategy is designed to balance individual self-interest with collective well-being,
    adapting to various opponent behaviors while promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.threshold = self.n_players * self.k / (self.n_players + self.k)
        self.epsilon = 0.01

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        cooperation_rate = (opponent_cooperators + history.my_actions[-1]) / self.n_players
        avg_payoff = np.mean(history.my_payoffs) if len(history.my_payoffs) > 1 else 0
        previous_avg_payoff = np.mean(history.my_payoffs[:-1]) if len(history.my_payoffs[:-1]) > 1 else 0
        if cooperation_rate > 0.5 and avg_payoff > previous_avg_payoff:
            self.threshold -= self.epsilon
        elif cooperation_rate < 0.5 or avg_payoff <= previous_avg_payoff:
            self.threshold += self.epsilon
        if opponent_cooperators >= self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_164(BaseStrategy):
    """
    A collective strategy called "Adaptive Cooperative Optimism" (ACO) for the N-Player Public Goods Game.
    It balances individual self-interest with collective cooperation, adapting to changing circumstances and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.tau = 0.5
        self.avg_payoff_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        tau_adjustment = 0.05
        if len(self.avg_payoff_history) > 0 and self.avg_payoff_history[-1] > self.k * (self.n_players / 2):
            self.tau += tau_adjustment
        elif len(self.avg_payoff_history) > 0 and self.avg_payoff_history[-1] < self.k * (self.n_players / 4):
            self.tau -= tau_adjustment
        cooperate = num_cooperators_prev_round >= self.tau * self.n_players or (len(self.avg_payoff_history) > 0 and random.random() < 1 - num_cooperators_prev_round / self.n_players)
        if state.round_number == self.game_description.n_rounds - 1:
            cooperate = True
        return Action.C if cooperate else Action.D

    def update_payoff_history(self, payoff: float):
        self.avg_payoff_history.append(payoff)

class Strategy_COLLECTIVE_165(BaseStrategy):
    """
    Adaptive Cooperation with Reputation-based Tit-for-Tat (ACTRT) strategy.
    Combines elements of cooperative game theory, such as Tit-for-Tat,
    with a reputation system to adapt to changing opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.reputation_scores = np.ones(game_description.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_actions = history.opponent_actions[-1]
        my_prev_action = history.my_actions[-1]
        avg_coop_prev = np.mean(prev_actions)
        for i, c_i in enumerate(prev_actions):
            if c_i and avg_coop_prev >= 0.5:
                self.reputation_scores[i] = min(self.reputation_scores[i] + 0.1, 2)
            elif not c_i and avg_coop_prev < 0.5:
                self.reputation_scores[i] = max(self.reputation_scores[i] - 0.1, 0)
        if avg_coop_prev >= 0.5 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        maj_coop_prev = np.sum(prev_actions) > self.game_description.n_players / 2
        return Action.C if maj_coop_prev else Action.D

class Strategy_COLLECTIVE_166(BaseStrategy):
    """
    A collective strategy that balances individual payoff maximization with group welfare promotion.
    It adapts to the game's history and parameters, making it robust against various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_opponent_actions = history.opponent_actions[-1]
        num_cooperators_prev_round = sum(prev_round_opponent_actions)
        average_payoff_prev_round = np.mean(history.my_payoffs[-1] + np.sum(history.opponent_payoffs[-1], axis=0))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if num_cooperators_prev_round == 0:
            return Action.D
        if average_payoff_prev_round >= 1:
            return Action.C if num_cooperators_prev_round >= self.game_description.n_players / 2 else Action.D
        else:
            return Action.C if num_cooperators_prev_round > self.game_description.n_players / 4 else Action.D

class Strategy_COLLECTIVE_167(BaseStrategy):
    """
    Collective strategy: Adaptive Reciprocal Altruism (ARA)

    ARA aims to balance individual self-interest with collective welfare by adapting to the group's cooperation level and reciprocating kindness.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators = 0
        self.avg_total_cooperators = 0
        self.initial_cooperation_prob = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperation_probability = self.initial_cooperation_prob
        else:
            total_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history is not None else False)
            cooperation_probability = 1 if total_cooperators_prev_round >= (self.game_description.k - 1) / self.game_description.n_players else 0
        action = Action.C if random.random() < cooperation_probability else Action.D
        if history is not None:
            self.total_cooperators += 1 if history.my_actions[-1] else 0
            self.avg_total_cooperators = (self.avg_total_cooperators * (state.round_number - 1) + self.total_cooperators) / state.round_number
            if state.round_number % 5 == 0:
                self.initial_cooperation_prob *= 1.1 if self.avg_total_cooperators >= self.game_description.k / self.game_description.n_players else 0.9
        return action

class Strategy_COLLECTIVE_168(BaseStrategy):
    """
    Adaptive Cooperate-Defect strategy for Public Goods game.
    
    This strategy aims to balance individual payoffs with collective welfare by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold_t = game_description.n_players * game_description.k / (2 * game_description.n_players + game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :] != False)
        punish_opponents = any((opponent_action and (not my_action) for opponent_action, my_action in zip(history.opponent_actions[-1, :], history.my_actions)))
        if num_cooperators_prev_round >= self.threshold_t:
            return Action.C
        elif num_cooperators_prev_round < self.threshold_t or punish_opponents:
            self.threshold_t -= 0.1
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_169(BaseStrategy):
    """
    A strategy that adapts to various opponent behaviors while promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tau = 0.5
        self.delta = 0.05
        self.punishment_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_cooperators = np.sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        average_payoff_cooperators = np.mean(np.concatenate((history.my_payoffs[-1:], history.opponent_payoffs[-1, :][history.opponent_actions[-1, :]])))
        average_payoff_defectors = np.mean(np.array([payoff for payoff, action in zip(history.opponent_payoffs[-1, :], history.opponent_actions[-1, :]) if not action]))
        if average_payoff_cooperators >= average_payoff_defectors:
            self.tau = max(0, self.tau - self.delta)
        else:
            self.tau = min(1, self.tau + self.delta)
        opponent_defected_while_i_cooperated = history.opponent_actions[-1, 0] == Action.D and history.my_actions[-1]
        if opponent_defected_while_i_cooperated:
            self.punishment_probability *= np.random.uniform(0, 1) < self.delta
        proportion_of_cooperators = previous_round_cooperators / self.game_description.n_players
        if proportion_of_cooperators >= self.tau and (not (opponent_defected_while_i_cooperated and random.random() < self.punishment_probability)):
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_170(BaseStrategy):
    """
    A collective strategy for N-Player Public Goods Game that adapts to various opponent behaviors 
    and maximizes overall payoffs while promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.C_avg_threshold = 0.4
        self.escalation_limit = 0.8
        self.punishment_duration = 2
        self.round_cooperation_probabilities = []
        self.cooperate_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < self.cooperate_probability else Action.D
        C_avg = self.calculate_C_avg(history)
        self.escalate_cooperation(C_avg)
        self.punish_defectors(history, state.round_number)
        return random.choice([Action.C, Action.D]) if random.random() < self.cooperate_probability else Action.D

    def calculate_C_avg(self, history: PlayerHistory) -> float:
        n_rounds = min(5, len(history.my_actions))
        C_avg = sum(history.my_actions[-n_rounds:]) / n_rounds + np.sum(history.opponent_actions[-n_rounds:, :]) / (self.game_description.n_players - 1)
        return C_avg / self.game_description.n_players

    def escalate_cooperation(self, C_avg: float):
        if C_avg < self.C_avg_threshold and self.cooperate_probability < self.escalation_limit:
            self.cooperate_probability = min(self.cooperate_probability + 0.1, self.escalation_limit)

    def punish_defectors(self, history: PlayerHistory, round_number: int):
        n_rounds = min(3, len(history.my_actions))
        opponent_cooperation = np.sum(history.opponent_actions[-n_rounds:, :]) / (self.game_description.n_players - 1)
        for i in range(self.game_description.n_players - 1):
            if history.opponent_actions[round_number - 1, i] == False and opponent_cooperation > self.C_avg_threshold:
                self.cooperate_probability = max(0.2, self.cooperate_probability * 0.8)

class Strategy_COLLECTIVE_171(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperativity with Social Learning (ACS)

    This strategy balances individual self-interest with collective well-being by adapting to the game's history and social environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop = 0.5
        self.coop_threshold = game_description.n_players / 2
        self.prev_round_num_coop = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_coop = np.sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        self.prev_round_num_coop = prev_round_coop
        avg_payoff_prev = np.mean(history.opponent_payoffs[-1, :] + history.my_payoffs[-1])
        if history.my_actions[-1]:
            if history.my_payoffs[-1] > (history.my_payoffs[:-1].mean() if len(history.my_payoffs) > 1 else 0):
                self.avg_coop = min(self.avg_coop + 0.1, 1)
        else:
            self.avg_coop = max(self.avg_coop - 0.1, 0)
        if len(history.my_actions) > 3:
            avg_coop_trend = (self.avg_coop - history.my_payoffs[:-4].mean()) / (history.my_payoffs[-4:].mean() - self.avg_coop)
            if avg_coop_trend > 0:
                self.coop_threshold -= 0.05
            elif avg_coop_trend < 0:
                self.coop_threshold += 0.05
        if prev_round_coop >= self.coop_threshold or avg_payoff_prev > self.game_description.k / self.game_description.n_players * (self.game_description.n_players / 2):
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_172(BaseStrategy):
    """
    Adaptive Cooperative Escalation (ACE) strategy for Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators = 0
        self.total_defectors = 0
        self.previous_round_payoff_c = 0
        self.previous_round_payoff_d = 0
        self.defection_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.total_cooperators = sum(history.opponent_actions[-1, :] == True)
        self.total_defectors = self.game_description.n_players - self.total_cooperators
        opponent_payoffs_c = history.opponent_payoffs[-1, :][history.opponent_actions[-1, :] == True]
        if len(opponent_payoffs_c) > 0:
            self.previous_round_payoff_c = np.mean(opponent_payoffs_c)
        else:
            self.previous_round_payoff_c = 0
        opponent_payoffs_d = history.opponent_payoffs[-1, :][history.opponent_actions[-1, :] == False]
        if len(opponent_payoffs_d) > 0:
            self.previous_round_payoff_d = np.mean(opponent_payoffs_d)
        else:
            self.previous_round_payoff_d = 0
        if self.previous_round_payoff_c >= self.previous_round_payoff_d:
            action = Action.C
        else:
            action = Action.D
        if self.total_cooperators / self.game_description.n_players < 1 / 3:
            if action == Action.D and self.defection_count > 0:
                action = Action.D
                self.defection_count += 1
            elif action == Action.C and self.defection_count > 1:
                action = Action.C
                self.defection_count = 0
            else:
                self.defection_count = 1
        return action

class Strategy_COLLECTIVE_173(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_init = 0.8
        self.epsilon = 0.1
        self.avg_payoff_coop = None

    def calculate_avg_payoff_coop(self, history: PlayerHistory) -> float:
        total_cooperators = sum((1 for a in history.opponent_actions[-1, :] if a))
        total_payoff_coop = sum((history.opponent_payoffs[-1, j] for j, a in enumerate(history.opponent_actions[-1, :]) if a))
        return total_payoff_coop / (total_cooperators or 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        if state.round_number + 1 >= self.game_description.n_rounds:
            return Action.C if random.random() < 0.5 else Action.D
        self.avg_payoff_coop = self.calculate_avg_payoff_coop(history)
        cooperate_prob = 0.9 if self.avg_payoff_coop > 1 else 0.4
        if random.random() < self.epsilon:
            return Action.C if random.random() < 0.5 else Action.D
        else:
            return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_COLLECTIVE_174(BaseStrategy):
    """
    A collective strategy called "Adaptive Cooperation" (AC) that balances individual payoff maximization with collective welfare.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tau = 0.5
        self.C_count = None
        self.D_count = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponents_defected = np.all(~history.opponent_actions[-1, :])
        if opponents_defected:
            return Action.D
        at_least_one_cooperated = np.any(history.opponent_actions[-1, :])
        if at_least_one_cooperated:
            return Action.C
        self.C_count = sum(np.logical_and(history.my_actions, history.opponent_actions).sum(axis=1))
        self.D_count = state.round_number - self.C_count
        new_tau = self.C_count / self.game_description.n_players + self.tau * (1 - 1 / self.game_description.n_rounds)
        if new_tau > 0.5 or (new_tau == 0.5 and state.round_number < self.game_description.n_rounds - 1):
            action = Action.C
        else:
            action = Action.D
        self.tau = new_tau
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_COLLECTIVE_175(BaseStrategy):
    """
    A collective strategy that balances individual payoff maximization with collective cooperation,
    adapting to opponents' behaviors while maintaining a robust and forgiving approach.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_debt = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_cooperators = sum(history.opponent_actions[-1, :])
        if history is not None and np.any(history.opponent_actions[-1, :] == True) and (self.cooperation_debt > 0):
            self.cooperation_debt -= 1
        if previous_cooperators >= self.game_description.n_players / 2:
            return Action.C
        else:
            self.cooperation_debt += 1
            return Action.D

class Strategy_COLLECTIVE_177(BaseStrategy):
    """
    Adaptive Cooperative Escalation (ACE) strategy.
    
    This is a dynamic strategy that balances individual self-interest with collective well-being 
    by adaptively adjusting cooperation levels based on game history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_payoff_prev_round = np.mean(np.concatenate(([history.my_payoffs[-1]], history.opponent_payoffs[-1, :])))
        if avg_payoff_prev_round < self.game_description.k / self.game_description.n_players:
            self.cooperation_threshold = max(0, min(1, avg_payoff_prev_round / (self.game_description.k / self.game_description.n_players) * ((state.round_number + 1) / self.game_description.n_rounds)))
        else:
            self.cooperation_threshold = max(0, min(1, self.cooperation_threshold))
        if random.random() < self.cooperation_threshold:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_178(BaseStrategy):
    """
    A collective strategy that prioritizes cooperation and adapts to the behavior of other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.n_players / 2 * game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        num_cooperators_prev = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if num_cooperators_prev > self.game_description.n_players / 2:
            self.threshold += 1
        else:
            self.threshold -= 1
        punish_prob = 0.5 if history.my_actions[-1] else 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if num_cooperators_prev >= self.threshold:
            return Action.C
        elif random.random() < punish_prob:
            return Action.D
        else:
            return Action.C if random.random() < 0.25 else Action.D

class Strategy_COLLECTIVE_179(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for N-Player Public Goods Game.
    
    This strategy adapts to the dynamics of the group by starting with cooperation 
    and adjusting based on observed behaviors, promoting an environment conducive 
    to mutual benefit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :])
        contribution_ratio = (previous_cooperators + 1) / self.n_players
        p_defect = max(0.5, 1 - contribution_ratio)
        if random.random() < p_defect:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            coop_threshold = self.k / self.n_players * (self.game_description.n_rounds - 1) * 0.6
            my_cooperative_payoff = np.sum(np.where(history.my_actions[:-1], self.k / self.n_players * np.sum(history.opponent_actions[:-1, :], axis=1), 0))
            potential_gain_from_defecting = 1 + self.k / self.n_players * previous_cooperators
            if my_cooperative_payoff - potential_gain_from_defecting > coop_threshold:
                return Action.C
        return Action.C

class Strategy_COLLECTIVE_180(BaseStrategy):
    """
    A strategy that adapts to various opponent behaviors and aligns with the collective mindset.
    It promotes collective cooperation by initially cooperating, then adapting to the group's behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        t_prev = state.round_number - 1
        if t_prev < self.n_players:
            T_prev = sum(history.opponent_actions[t_prev, :]) + (history.my_actions[t_prev] if history is not None else False)
            if T_prev >= self.n_players / 2:
                return Action.C
            return Action.D
        elif state.round_number == self.game_description.n_rounds:
            P_avg_prev = np.mean(history.opponent_payoffs[t_prev - 1, :] + [history.my_payoffs[t_prev - 1]] if history is not None else [])
            T_prev = sum(history.opponent_actions[t_prev - 1, :]) + (history.my_actions[t_prev - 1] if history is not None else False)
            if P_avg_prev >= self.k / 2 and T_prev >= self.n_players / 2:
                return Action.C
            return Action.D
        else:
            P_avg_prev = np.mean(history.opponent_payoffs[t_prev, :] + [history.my_payoffs[t_prev]] if history is not None else [])
            T_prev = sum(history.opponent_actions[t_prev, :]) + (history.my_actions[t_prev] if history is not None else False)
            if P_avg_prev >= self.k / 2 and T_prev >= self.n_players / 2:
                return Action.C
            return Action.D
        all_defected = np.all(~(history.opponent_actions[t_prev, :] + [history.my_actions[t_prev]]))
        perfect_cooperation = np.all(history.opponent_actions[t_prev, :] + [history.my_actions[t_prev]])
        if all_defected:
            return Action.C
        elif perfect_cooperation:
            return Action.C

class Strategy_COLLECTIVE_181(BaseStrategy):
    """
    The Adaptive Collective Cooperation (ACC) strategy balances individual self-interest with collective well-being.
    It adapts to the game's history and parameters, promoting cooperation when it is mutually beneficial.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_payoff = np.sum(history.opponent_payoffs[-1, :] + history.my_payoffs[-1:])
        avg_expected_payoff = (self.game_description.k - 1) / self.game_description.n_players + 1
        if prev_round_payoff >= avg_expected_payoff:
            return Action.C
        elif np.all(~history.opponent_actions[-1, :]):
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_182(BaseStrategy):
    """
    A collective strategy that prioritizes cooperation and adapts to varying opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_coop_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.8 else Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        total_cooperators = int(opponent_cooperators) + history.my_actions[-1]
        avg_payoff_coop = np.mean([payoff if cooperated else 0 for payoff, cooperated in zip(history.opponent_payoffs[-1, :], history.opponent_actions[-1, :])]) * (total_cooperators / self.game_description.n_players)
        self.avg_payoff_coop_history.append(avg_payoff_coop)
        if avg_payoff_coop > 1.5:
            return Action.C
        elif total_cooperators < self.game_description.n_players // 2:
            return Action.D if random.random() < 0.7 else Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif sum(1 - history.opponent_actions[-1, :]) > self.game_description.n_players // 2:
            return Action.D if random.random() < 0.9 else Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_183(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Gradual Retaliation (ACGR)
    
    This strategy aims to balance cooperation and defection while adapting to the opponent's behavior.
    It prioritizes cooperation in early rounds, gradually retaliating against defectors while maintaining
    a willingness to forgive and cooperate again.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_coop_rate = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if state.round_number < self.game_description.n_rounds // 2:
            return Action.C if opponent_coop_rate > 0.5 else Action.D
        retaliate = False
        if state.round_number >= self.game_description.n_rounds // 2 and state.round_number < 3 * self.game_description.n_rounds // 4:
            opponent_coop_rate_prev = sum(history.opponent_actions[-2, :]) / self.game_description.n_players
            if opponent_coop_rate_prev <= 0.5:
                retaliate = True
            return Action.D if retaliate else Action.C if opponent_coop_rate > 0.5 else Action.D
        overall_coop_rate = sum(sum(history.opponent_actions)) / (self.game_description.n_players * state.round_number)
        defect_count = sum((1 for round in history.opponent_actions if sum(round) < self.game_description.n_players // 2))
        return Action.C if overall_coop_rate > 2 / 3 and defect_count < 0.2 * self.game_description.n_players else Action.D
        return Action.D

class Strategy_COLLECTIVE_184(BaseStrategy):
    """
    Adaptive Cooperation with Self-Adjusting Threshold.
    
    This strategy starts by cooperating in the first round. 
    Then, it adjusts its cooperation threshold based on the number of cooperators in each round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5
        self.delta = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :])
        if prev_round_cooperators / self.game_description.n_players > self.theta + self.delta:
            self.theta -= self.delta
        elif prev_round_cooperators / self.game_description.n_players < self.theta - self.delta:
            self.theta += self.delta
        self.theta = max(0, min(self.theta, 1))
        if prev_round_cooperators / self.game_description.n_players >= self.theta:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_185(BaseStrategy):
    """
    The Adaptive Cooperative Optimizer (ACO) strategy aims to balance individual payoffs 
    with collective well-being by adapting to the evolving dynamics of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_init = 0.7
        self.p_coop = max(0.5, 0.8 * (game_description.k / game_description.n_players))
        self.p_defect = min(0.5, 0.2 * (game_description.n_players / game_description.k))
        self.avg_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        prev_round_payoff = history.my_payoffs[-1]
        opponent_actions = history.opponent_actions[-1, :]
        num_cooperators = sum(opponent_actions) + int(history.my_actions[-1])
        if state.round_number > 0:
            self.avg_payoff = (self.avg_payoff * (state.round_number - 1) + prev_round_payoff) / state.round_number
        if num_cooperators > self.game_description.n_players / 2:
            self.p_coop += 0.05
            self.p_defect -= 0.05
        elif num_cooperators < self.game_description.n_players / 4:
            self.p_coop -= 0.05
            self.p_defect += 0.05
        if prev_round_payoff > self.avg_payoff:
            return Action.C if random.random() < self.p_coop else Action.D
        else:
            return Action.D if random.random() < self.p_defect else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_186(BaseStrategy):
    """
    Adaptive Cooperation (AC) strategy aims to balance individual payoff maximization with collective welfare by adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.recent_history = []
        self.contribution_scores = []
        self.trend_scores = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        last_round_cooperators = sum(self.recent_history)
        trend_score = self.calculate_trend_score(history)
        contribution_score = self.get_contribution_score(history)
        if random.random() < contribution_score:
            cooperate = True
        else:
            cooperate = last_round_cooperators >= self.game_description.n_players / 2 or trend_score > 0
        if state.round_number == self.game_description.n_rounds - 1:
            if contribution_score < max(self.contribution_scores):
                cooperate = False
            else:
                cooperate = True
        if last_round_cooperators == 0:
            cooperate = random.random() < 0.5
        self.recent_history.append(cooperate)
        return Action.C if cooperate else Action.D

    def calculate_trend_score(self, history: PlayerHistory) -> int:
        scores = []
        for round in range(max(0, len(history.my_actions) - 3), len(history.my_actions)):
            cooperators = sum(history.opponent_actions[round, :]) + (history.my_actions[round] == Action.C)
            if cooperators > self.game_description.n_players / 2:
                scores.append(1)
            else:
                scores.append(-1)
        return sum(scores)

    def get_contribution_score(self, history: PlayerHistory) -> float:
        my_payoffs = history.my_payoffs
        opponent_payoffs = np.sum(history.opponent_payoffs, axis=1)
        total_payoffs = my_payoffs + opponent_payoffs
        self.contribution_scores.append(np.mean(total_payoffs))
        return np.random.uniform(0, 1) * (np.mean(self.contribution_scores) / len(self.contribution_scores)) if self.contribution_scores else 0.5

class Strategy_COLLECTIVE_187(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective well-being by adaptively responding to the group's cooperation level and adjusting its own behavior accordingly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        if opponent_cooperators >= self.cooperation_threshold:
            return Action.C
        defected_after_coop = any((history.my_actions[-1] and (not history.opponent_actions[-1, i]) for i in range(self.game_description.n_players - 1)))
        if defected_after_coop:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return random.choice([Action.C, Action.D])

class Strategy_COLLECTIVE_188(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and promotes robust cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponents_cooperation_rate = 0.0
        self.observation_phase_rounds = math.ceil(game_description.n_rounds / 3)
        self.adaptive_phase_started = False
        self.punishment_active = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        last_round_actions = history.opponent_actions[-1]
        self.opponents_cooperation_rate = np.mean(last_round_actions)
        if not self.adaptive_phase_started:
            if self.game_description.n_players > 1 and self.observation_phase_rounds <= state.round_number < self.game_description.n_rounds - 1:
                if self.opponents_cooperation_rate >= 0.5:
                    return Action.C
                else:
                    return Action.D
            self.adaptive_phase_started = True
        expected_payoff_from_cooperating = self.game_description.k / self.game_description.n_players * np.sum(last_round_actions)
        my_last_payoff = history.my_payoffs[-1]
        if self.opponents_cooperation_rate > 0.5 and expected_payoff_from_cooperating >= my_last_payoff:
            action = Action.C
        else:
            action = Action.D
        if not self.punishment_active and state.round_number > self.observation_phase_rounds + 1:
            avg_cooperation_rate_last_two_rounds = np.mean(history.opponent_actions[-2:, :])
            if avg_cooperation_rate_last_two_rounds < 0.3:
                action = Action.D
                self.punishment_active = True
        elif self.punishment_active and state.round_number > self.observation_phase_rounds + 1:
            self.punishment_active = False
        if state.round_number == self.game_description.n_rounds - 1:
            if expected_payoff_from_cooperating >= my_last_payoff:
                return Action.C
        return action

class Strategy_COLLECTIVE_189(BaseStrategy):
    """
    This strategy aims to balance individual payoff maximization with collective welfare by adapting to the group's behavior and promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if isinstance(history.my_actions, np.ndarray) else history.my_actions)
        if state.round_number < self.game_description.n_rounds - 1:
            if prev_cooperators >= self.game_description.k:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_190(BaseStrategy):
    """
    Collective strategy: "Adaptive Cooperation" 
    for N-Player Public Goods Game.

    Aims to balance individual payoffs with the benefits of cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_prev_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        payoffs_prev_round = np.sum(history.opponent_payoffs[-1, :]) + history.my_payoffs[-1]
        avg_payoff_t_minus_1 = payoffs_prev_round / self.game_description.n_players
        cooperators_prev_round = sum(history.opponent_actions[-1, :])
        if history.my_actions[-1]:
            cooperators_prev_round += 1
        if avg_payoff_t_minus_1 >= self.game_description.k / 2:
            return Action.C if history.my_actions[-1] else Action.D
        else:
            return Action.C if not history.my_actions[-1] else Action.D
        if cooperators_prev_round >= self.game_description.n_players // 2:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_191(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_defection_rate = None
        self.opponent_payoff_difference = None

    def _update_opponent_rates(self, history: PlayerHistory) -> None:
        if len(history.my_actions) < 3:
            return
        n_rounds = len(history.my_actions)
        self.opponent_defection_rate = np.mean((history.opponent_actions[-3:, :] == Action.D).astype(np.float64), axis=0)
        opponent_payoffs = history.opponent_payoffs[-3:, :]
        self.opponent_payoff_difference = np.mean((opponent_payoffs - 1) ** 2, axis=0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is None or len(history.my_actions) < 1:
            return Action.D
        self._update_opponent_rates(history)
        n_cooperators_last_round = sum((history.opponent_actions[-1, :] == Action.C).astype(np.int64))
        threshold_cooperators = self.game_description.n_players // 2
        exploration_probability = 0.1
        if n_cooperators_last_round >= threshold_cooperators:
            return Action.C
        elif random.random() < exploration_probability:
            return random.choice([Action.C, Action.D])
        elif self.opponent_defection_rate is not None and np.any((self.opponent_defection_rate > 0.6) & (self.opponent_payoff_difference > 0.2)):
            return Action.D
        elif state.round_number >= self.game_description.n_rounds - 5:
            return Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_192(BaseStrategy):
    """
    Adaptive Cooperation with History-Based Threshold (ACHT) strategy.
    
    This collective strategy balances individual self-interest with cooperative behavior to achieve mutually beneficial outcomes.
    It adapts to the game's history and opponent behaviors, ensuring robustness in a wide range of scenarios.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cr_threshold = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        my_prev_action = bool(history.my_actions[-1])
        cooperators = sum(prev_round_actions) + my_prev_action
        cr = cooperators / self.game_description.n_players
        opponent_cooperated = any((bool(action) for action in prev_round_actions))
        if cr >= self.cr_threshold:
            return Action.C
        elif cr < self.cr_threshold and (not my_prev_action) and opponent_cooperated:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_193(BaseStrategy):
    """
    Adaptive Cooperation with Retribution (ACR) strategy.
    
    This collective strategy balances individual self-interest with the well-being of the group by adapting to the group's cooperation level and responding to exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.historical_cooperation_rate = []
        self.retribution_flag = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        current_round_cooperations = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        avg_cooperation_rate = np.mean(self.historical_cooperation_rate + [current_round_cooperations / self.game_description.n_players])
        if len(self.historical_cooperation_rate) > 0 and abs(current_round_cooperations - avg_cooperation_rate * self.game_description.n_players) > 0.2 * self.game_description.n_players:
            return Action.D
        if avg_cooperation_rate >= 0.5 or random.random() < 0.01:
            action = Action.C
            self.retribution_flag = False
        else:
            action = Action.D
        self.historical_cooperation_rate.append(current_round_cooperations / self.game_description.n_players)
        return action

class Strategy_COLLECTIVE_194(BaseStrategy):
    """
    Adaptive Cooperative Escalation (ACE) strategy.

    This strategy balances individual payoff maximization with promoting cooperation and adapting to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.escalated_last_round = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        my_actions = history.my_actions
        my_payoffs = history.my_payoffs
        opponent_actions = history.opponent_actions
        opponent_payoffs = history.opponent_payoffs
        majority_cooperated = sum(opponent_actions[-1, :]) > self.game_description.n_players / 2
        my_payoff_last_round = my_payoffs[-1]
        avg_payoffs_last_round = np.mean(opponent_payoffs[-1, :]) + my_payoff_last_round / self.game_description.n_players
        my_payoff_geq_avg = my_payoff_last_round >= avg_payoffs_last_round
        if majority_cooperated and my_payoff_geq_avg:
            return Action.C
        elif not self.escalated_last_round:
            self.escalated_last_round = True
            return Action.D
        else:
            self.escalated_last_round = False
            return Action.C if majority_cooperated else Action.D

class Strategy_COLLECTIVE_195(BaseStrategy):
    """
    A strategy that aims to balance individual payoff maximization with collective welfare by adapting to the game's history and responding to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.retaliation_trigger = False
        self.retaliation_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        T = math.ceil(self.game_description.n_players * self.game_description.k / self.game_description.n_players)
        if history is not None:
            total_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + 1
        else:
            total_cooperators_prev_round = 0
        if self.retaliation_trigger:
            self.retaliation_count += 1
            if self.retaliation_count <= 2:
                return Action.D
            else:
                self.retaliation_trigger = False
                self.retaliation_count = 0
        if total_cooperators_prev_round >= T:
            my_action = Action.C
        else:
            my_action = Action.D
        if history is not None and len(history.my_actions) > 0 and (history.opponent_actions[-1, :].sum() < self.game_description.n_players - 1):
            if history.my_payoffs[-1] <= 1:
                self.retaliation_trigger = True
        return my_action

class Strategy_COLLECTIVE_196(BaseStrategy):
    """
    A collective strategy that adapts to the average cooperation rate and punishes those who don't cooperate.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_turns_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if self.punishment_turns_remaining > 0:
            self.punishment_turns_remaining -= 1
            return Action.D
        opponent_actions = history.opponent_actions[-1, :]
        cooperators = sum(opponent_actions) + history.my_actions[-1]
        ACR = cooperators / self.game_description.n_players
        if ACR >= 0.5:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_197(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective cooperation,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coops = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if prev_coops >= self.game_description.n_players / 2:
            cooperate = True
        elif prev_coops < self.game_description.n_players / 2 and any((not action for action in history.opponent_actions[-1, :])) or not history.my_actions[-1]:
            cooperate = False
        else:
            cooperate = False
        if state.round_number == self.game_description.n_rounds - 1:
            cooperate = True
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_198(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the need for cooperation to achieve mutual benefits.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.payoffs = []
        self.previous_round_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        self.previous_round_cooperators = sum(history.opponent_actions[-1, :])
        if history.my_payoffs.size > 0:
            self.payoffs.append(history.my_payoffs[-1])
        else:
            self.payoffs.append(0)
        average_payoff = np.mean(self.payoffs) if self.payoffs else 0
        if self.previous_round_cooperators > self.game_description.n_players / 2:
            return Action.C
        elif history.my_payoffs[-1] >= average_payoff:
            return Action.D
        else:
            p = self.game_description.k / self.game_description.n_players * (self.previous_round_cooperators / self.game_description.n_players)
            return Action.C if random.random() < p else Action.D
        for opponent in range(self.game_description.n_players):
            if history.opponent_actions[-2, opponent] == False and history.opponent_actions[-1, opponent] == False:
                if history.opponent_payoffs[-1, opponent] > average_payoff:
                    return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if history.my_payoffs[-1] >= average_payoff:
                return Action.D
            else:
                p = self.game_description.k / self.game_description.n_players * (self.previous_round_cooperators / self.game_description.n_players)
                return Action.C if random.random() < p else Action.D
        if all([not action for action in history.opponent_actions[:, opponent]]):
            return Action.D

class Strategy_COLLECTIVE_199(BaseStrategy):
    """
    A strategy that adapts to the game's history and parameters by balancing individual payoffs with collective well-being.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.ma_3 = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == 1:
            c_prev = sum(history.opponent_actions[0, :]) + 1
            return Action.C if c_prev >= self.game_description.n_players / 2 else Action.D
        cooperators = sum(history.opponent_actions[-1, :]) + 1
        if self.ma_3 is None:
            self.ma_3 = cooperators / self.game_description.n_players
        else:
            self.ma_3 = (self.ma_3 * 2 + cooperators / self.game_description.n_players) / 3
        if state.round_number < self.game_description.n_rounds - 1:
            return Action.C if self.ma_3 >= self.game_description.k / self.game_description.n_players else Action.D
        else:
            private_payoff_from_cooperation = self.game_description.k / self.game_description.n_players * cooperators
            private_payoff_from_defection = 1 + self.game_description.k / self.game_description.n_players * (cooperators - 1)
            return Action.C if private_payoff_from_cooperation > private_payoff_from_defection else Action.D

class Strategy_COLLECTIVE_200(BaseStrategy):
    """
    Adaptive Cooperative Threshold strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.alpha = 0.1
        self.theta = 0
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        contributions_prev_round = sum(history.opponent_actions[-1, :] + [True])
        self.theta = (1 - self.alpha) * self.theta + self.alpha * (self.k / self.n_players) * contributions_prev_round
        if state.round_number == self.n_rounds - 1:
            return Action.D
        cooperators_in_current_round = sum(history.opponent_actions[-1, :]) + 1 if history.my_actions[-1] else sum(history.opponent_actions[-1, :])
        if self.consecutive_defections >= 3:
            self.theta = 0
            self.consecutive_defections = 0
        if cooperators_in_current_round >= self.theta:
            return Action.C
        else:
            self.consecutive_defections += 1
            return Action.D

class Strategy_COLLECTIVE_201(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for N-Player Public Goods Game.
    This strategy initially sets an optimistic tone by cooperating, then adapts based on observed cooperation levels.
    It rewards sustained cooperation from others while signaling for increased cooperation through targeted defections.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_action = Action.C
        self.r_threshold = game_description.n_rounds // 2

    def calculate_average_cooperators(self, history: PlayerHistory) -> float:
        return np.mean(np.sum(history.opponent_actions[:history.my_payoffs.size - 1], axis=1))

    def calculate_moving_average_cooperators(self, history: PlayerHistory, window_size: int) -> float:
        start_idx = max(0, history.my_payoffs.size - 1 - window_size)
        return np.mean(np.sum(history.opponent_actions[start_idx:history.my_payoffs.size - 1], axis=1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number <= 3:
            action = Action.C
        elif state.round_number <= self.r_threshold:
            avg_coop = self.calculate_average_cooperators(history)
            action = Action.D if avg_coop < self.game_description.n_players / 2 else Action.C
        else:
            moving_avg_coop = self.calculate_moving_average_cooperators(history, self.r_threshold // 2)
            if moving_avg_coop >= self.game_description.n_players / 2 and self.prev_action == Action.D:
                action = Action.C
            elif moving_avg_coop < self.game_description.n_players / 2 and self.prev_action == Action.C:
                action = Action.D
            else:
                action = self.prev_action
        if state.round_number == self.game_description.n_rounds - 1:
            moving_avg_coop = self.calculate_moving_average_cooperators(history, self.r_threshold // 2)
            if moving_avg_coop >= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        self.prev_action = action
        return action

class Strategy_COLLECTIVE_202(BaseStrategy):
    """
    Adaptive Cooperation with History-Based Threshold Strategy.

    This strategy adapts to the observed cooperation rate and punishes opponents for defecting when the overall cooperation rate is high.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate_threshold = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :] + [True])
        cooperation_rate = previous_cooperators / self.game_description.n_players
        if cooperation_rate >= self.cooperation_rate_threshold:
            opponent_defected = not history.opponent_actions[-1, 0]
            if opponent_defected and state.round_number < self.game_description.n_rounds - 2:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_203(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Concession (ACGC) strategy for the N-Player Public Goods Game.
    
    This strategy balances individual interests with collective welfare by adapting to the behavior of other players.
    By maintaining a cooperative mindset and gradually conceding when cooperation is not reciprocated, this strategy aims
    to promote mutual cooperation and maximize overall payoff in the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        n_players, k_factor = (game_description.n_players, game_description.k)
        if n_players >= 3:
            self.tau = 0.5 * (k_factor / n_players)
        else:
            self.tau = 1 / 3
        self.delta = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] | [history.my_actions[-1]])
        cooperators_ratio = prev_cooperators / self.game_description.n_players
        if prev_cooperators == self.game_description.n_players - 1:
            return Action.C
        if cooperators_ratio >= self.tau:
            return Action.C
        else:
            self.tau -= self.delta
            self.tau = max(self.tau, 0)
            return Action.D

class Strategy_COLLECTIVE_204(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Punishment.
    
    Balances individual self-interest with collective well-being, 
    adapting to various opponent behaviors and game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        k_div_n = self.game_description.k / self.game_description.n_players
        if prev_round_coops > self.game_description.n_players / 2:
            return Action.C
        elif prev_round_coops < self.game_description.n_players / 2 and k_div_n <= 0.5:
            return Action.D
        else:
            return Action.C

    def last_round_override(self, state: GameState, history: PlayerHistory) -> bool:
        """
        Override for the last round (no future reciprocity).
        """
        return state.round_number == self.game_description.n_rounds - 1

class Strategy_COLLECTIVE_205(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.window_size = 3
        self.exploration_probability = 0.05

    def calculate_CCI(self, history: PlayerHistory) -> float:
        """
        Calculate the Collective Cooperation Index (CCI).
        
        Args:
        history (PlayerHistory): The current player's history.
        
        Returns:
        float: CCI value between 0 and 1.
        """
        last_round_cooperators = sum(history.opponent_actions[-1, :] + [True])
        return last_round_cooperators / self.game_description.n_players

    def monitor_opponent(self, history: PlayerHistory) -> str:
        """
        Monitor an opponent's behavior over a window of rounds.
        
        Args:
        history (PlayerHistory): The current player's history.
        
        Returns:
        str: "defect_twice" if the opponent defects twice within the window,
             "cooperate_consistently" if they cooperate consistently, or "" otherwise.
        """
        last_window = history.opponent_actions[-self.window_size:, 0]
        if sum(last_window) == self.window_size:
            return 'cooperate_consistently'
        elif sum((1 - x for x in last_window)) >= 2:
            return 'defect_twice'
        else:
            return ''

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Choose an action based on the Adaptive Collective Optimism strategy.
        
        Args:
        state (GameState): The current game state.
        history (None | PlayerHistory): The player's history or None in the first round.
        
        Returns:
        Action: The chosen action, either Action.C or Action.D.
        """
        if state.round_number == 0:
            return Action.C
        CCI = self.calculate_CCI(history)
        action = Action.C if CCI >= 0.5 else Action.D
        opponent_behavior = self.monitor_opponent(history)
        if opponent_behavior == 'defect_twice':
            action = Action.D
        elif opponent_behavior == 'cooperate_consistently':
            action = Action.C
        if random.random() < self.exploration_probability:
            action = random.choice([Action.C, Action.D])
        return action

class Strategy_COLLECTIVE_206(BaseStrategy):
    """
    A collective strategy that adapts to the evolving game dynamics and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.previous_actions = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        self.previous_actions = [int(action == 'C') for action in history.my_actions]
        t = state.round_number + 1
        if t <= math.floor(self.game_description.n_rounds / 2):
            cooperate_prob = 0.5
        elif t <= math.ceil(3 * self.game_description.n_rounds / 4):
            avg_coop_prev = np.mean(history.opponent_actions[-1, :])
            cooperate_prob = 0.7 if avg_coop_prev >= self.game_description.k / self.game_description.n_players else 0.3
        else:
            total_coop_prev = sum(history.opponent_actions[-1, :])
            cooperate_prob = 1 if total_coop_prev >= self.game_description.k else 0
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_COLLECTIVE_207(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game, 
    aiming to balance individual payoff maximization with collective well-being.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_forgiveness = 0.5
        self.detect_early_defection_rounds = math.ceil(0.2 * game_description.n_rounds)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        last_round_cooperators = sum(history.opponent_actions[-1, :] != False)
        my_last_payoff_coop = self.game_description.k / self.game_description.n_players * last_round_cooperators
        my_last_payoff_defect = 1 + self.game_description.k / self.game_description.n_players * last_round_cooperators
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if last_round_cooperators > 0 and my_last_payoff_coop >= my_last_payoff_defect:
            return Action.C
        early_defection_detection = sum(history.opponent_actions[:self.detect_early_defection_rounds, :].flatten() == False)
        if early_defection_detection > 0 and all(history.my_actions[:self.detect_early_defection_rounds] != True):
            return Action.D
        if random.random() < self.p_forgiveness:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_208(BaseStrategy):
    """
    Adaptive Cooperation with Punishment (ACP) strategy.
    
    This strategy aims to promote collective well-being by adapting to the group's cooperation level.
    It cooperates when others do and defects when cooperation falters, using a punishment mechanism
    to restore balance and motivate opponents to cooperate.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialize the ACP strategy.

        :param game_description: The description of the public goods game.
        """
        self.game_description = game_description
        self.threshold = game_description.n_players * game_description.k / (game_description.k + 1)
        self.coop_window_size = math.ceil(game_description.n_rounds / 4)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide on an action based on the current state and history.

        :param state: The current game state.
        :param history: The player's history in the game.
        :return: The chosen action (C or D).
        """
        if state.round_number == 0:
            return Action.C
        prev_coop_count = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if prev_coop_count >= self.threshold:
            return Action.C
        opponent_coop_rates = np.mean(history.opponent_actions[-self.coop_window_size:, :], axis=0)
        opponent_avg_coop_rate = np.mean(opponent_coop_rates)
        if opponent_avg_coop_rate < 1 / game_description.n_players:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_209(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Gradual Concession (ACGC)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_contributors = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        concession_threshold = max(1, prev_contributors + 1)
        if prev_contributors >= self.game_description.k:
            return Action.C
        curr_contributors = sum(history.opponent_actions[-1, :])
        if curr_contributors >= concession_threshold:
            return Action.C
        elif curr_contributors < self.game_description.n_players / 3:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_210(BaseStrategy):
    """
    Adaptive Collective Optimizer (ACO) for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_cooperators = 0
        self.defecting_opponents = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < 5:
            return Action.D if state.round_number % 2 == 0 else Action.C
        self.avg_cooperators = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        for opponent in history.opponent_actions[-1, :]:
            if not opponent:
                self.defecting_opponents.append(opponent)
        if self.avg_cooperators > self.game_description.n_players / 2:
            return Action.C
        elif self.game_description.k > 1.5 and self.avg_cooperators <= self.game_description.n_players / 2:
            return Action.D if random.random() < 0.5 else Action.C
        else:
            return Action.D if len(self.defecting_opponents) > self.game_description.n_players / 2 else Action.C

class Strategy_COLLECTIVE_211(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Gradual Withdrawal (ACGW)

    ACGW balances cooperation and self-interest by adapting to the group's behavior,
    gradually withdrawing from cooperation in response to widespread defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        theta = (self.game_description.n_players - self.game_description.k) / self.game_description.n_players
        previous_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        previous_defectors = self.game_description.n_players - previous_cooperators
        p = 1 / (1 + math.exp(-(previous_defectors - theta * self.game_description.n_players)))
        avg_payoff = np.mean(history.my_payoffs[-1])
        epsilon = 0.05
        if previous_cooperators >= theta * self.game_description.n_players and history.my_payoffs[-1] >= avg_payoff - epsilon:
            return Action.C
        elif random.random() < p:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_212(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Concession (ACGC) strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history.my_actions.size > 0 else 0)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if prev_cooperators < self.game_description.n_players // 4:
            return Action.D if random.random() < 0.5 else Action.C
        if prev_cooperators >= self.game_description.n_players / 2:
            return Action.C
        concession_prob = (self.game_description.n_players / 2 - prev_cooperators) / (self.game_description.n_players / 2)
        return Action.D if random.random() < concession_prob else Action.C

class Strategy_COLLECTIVE_213(BaseStrategy):
    """
    A collective strategy that adapts to various opponent behaviors while prioritizing cooperation and robustness.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_threshold = 0.5
        self.exploration_rounds = 5
        self.punish_coop_countdown = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.7 else Action.D
        if self.punish_coop_countdown > 0:
            self.punish_coop_countdown -= 1
            return Action.C
        prev_round_opponents = history.opponent_actions[-1, :]
        c_total = sum(prev_round_opponents) + (history.my_actions[-1] == Action.C)
        cooperate_prob = 1 if c_total >= self.game_description.n_players / 2 else 0
        avg_payoff = np.mean(history.my_payoffs)
        if avg_payoff < self.avg_payoff_threshold and history.my_actions[-1] == Action.D:
            cooperate_prob = 1
            self.punish_coop_countdown = 2
        if state.round_number % self.exploration_rounds == 0:
            cooperate_prob = 1
        if state.round_number + 1 == self.game_description.n_rounds:
            total_payoff = np.sum(history.my_payoffs)
            avg_opponent_payoff = np.mean(np.sum(history.opponent_payoffs, axis=1))
            if total_payoff < avg_opponent_payoff:
                return Action.D
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_COLLECTIVE_214(BaseStrategy):
    """
    Adaptive Cooperation strategy for the Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.previous_payoff = None
        self.total_cooperators = 0
        self.cooperation_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None and self.previous_payoff is not None:
            total_cooperators = sum(history.opponent_actions[-1, :] | True)
            avg_payoff_per_cooperator = self.previous_payoff / total_cooperators if total_cooperators > 0 else 0
            self.cooperation_threshold = 1 - self.game_description.k / self.game_description.n_players * avg_payoff_per_cooperator
        if history is not None:
            current_payoff_per_cooperator = history.my_payoffs[-1] / sum(history.opponent_actions[-1, :] | True)
            return Action.C if current_payoff_per_cooperator > self.cooperation_threshold else Action.D
        return Action.C if random.random() < self.cooperation_threshold else Action.D

class Strategy_COLLECTIVE_216(BaseStrategy):
    """
    Adaptive Cooperation with Retaliation (ACR) strategy.
    
    This strategy promotes cooperation while being robust against various opponent behaviors.
    It adapts to the game's history and adjusts the cooperation threshold accordingly.
    A retaliation mechanism is also implemented to deter opponents from exploiting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tau = game_description.n_players // 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < 5:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if num_cooperators >= self.tau:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_217(BaseStrategy):
    """
    Adaptive Cooperation with History-Driven Threshold strategy for the N-Player Public Goods Game.
    
    This strategy promotes a culture of reciprocity and mutual support by adapting its threshold 
    based on observed cooperation rates, encouraging cooperation when possible while avoiding 
    excessive exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialize ACHT strategy with the given game description.

        Args:
        - game_description (PublicGoodsDescription): Description of the N-Player Public Goods Game.
        """
        self.game_description = game_description
        self.threshold = 0.5
        self.history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide the action for the given game state and player history.

        Args:
        - state (GameState): Current game state.
        - history (None | PlayerHistory): Player's history, or None if this is the first round.

        Returns:
        - Action: Chosen action, either COOPERATE (Action.C) or DEFECT (Action.D).
        """
        if state.round_number == 0:
            return Action.C
        CR = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if CR > 0.5:
            self.threshold -= 0.05
        elif CR < 0.5:
            self.threshold += 0.05
        self.history.append(history.opponent_actions[-1, :])
        if CR >= self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_218(BaseStrategy):
    """
    This strategy balances individual self-interest with a willingness to cooperate for the greater good.
    It initially cooperates, then reciprocates cooperation when the majority of players do so,
    and adapts its cooperation level based on the overall cooperation rate.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_history = []
        self.threshold = 0.6
        self.adjustment = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        majority_cooperated = np.sum(opponent_actions) > self.game_description.n_players / 2
        avg_cooperation_rate = np.mean(self.cooperation_history[-min(5, state.round_number):])
        cooperate_likelihood = 0.5 + (avg_cooperation_rate - self.threshold) * self.adjustment
        if majority_cooperated and random.random() < cooperate_likelihood:
            return Action.C
        else:
            return Action.D
        self.cooperation_history.append(self == Action.C)

class Strategy_COLLECTIVE_219(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for N-Player Public Goods Game.
    
    This strategy adapts to various opponent behaviors while aligning with the collective mindset.
    It balances individual gain with collective welfare by promoting mutual benefits and positive behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.0
        self.total_cooperators = 0
        self.my_payoff = 0.0
        self.avg_payoff_of_all_players = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.my_payoff = history.my_payoffs[-1]
        self.avg_payoff_of_all_players = np.mean(history.opponent_payoffs[-1, :])
        opponent_actions_last_round = history.opponent_actions[-1, :]
        self.total_cooperators = sum(opponent_actions_last_round) + int(history.my_actions[-1])
        self.avg_coop_rate = self.total_cooperators / self.game_description.n_players
        if state.round_number < self.game_description.n_rounds // 2:
            if self.avg_coop_rate > 0.5:
                return Action.C
            else:
                return Action.D
        elif state.round_number == self.game_description.n_rounds // 2:
            if self.total_cooperators >= self.total_cooperators - int(history.my_actions[-1]):
                return Action.C
            elif self.avg_coop_rate > 0.5:
                return Action.C
            else:
                return Action.D
        elif self.my_payoff < self.avg_payoff_of_all_players and state.round_number != self.game_description.n_rounds - 1:
            return Action.D
        elif self.total_cooperators == self.game_description.n_players and self.avg_payoff_of_all_players > 0:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_220(BaseStrategy):
    """
    This strategy prioritizes cooperation when it benefits the group and adapts to changes in the game's dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.k / game_description.n_players
        self.last_round_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if num_cooperators > self.threshold:
            action = Action.C
        else:
            action = Action.D
        if state.round_number >= self.game_description.n_rounds // 2:
            avg_payoff_last_half = np.mean(history.my_payoffs[state.round_number - self.game_description.n_rounds // 2:state.round_number])
            if history.my_payoffs[-1] > avg_payoff_last_half:
                self.threshold += 0.01
            elif history.my_payoffs[-1] < avg_payoff_last_half and self.threshold > 0:
                self.threshold -= 0.01
        return action

class Strategy_COLLECTIVE_221(BaseStrategy):
    """
    Adaptive Cooperation with History-Dependent Adjustment (ACHDA) strategy.

    This strategy prioritizes cooperation when there is evidence of sufficient collective cooperation,
    while adapting to opponents' behaviors. It balances individual interests with the need for 
    collective action to achieve higher payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        else:
            total_cooperators_prev_round = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
            if total_cooperators_prev_round >= self.game_description.n_players / 2:
                return Action.C
            elif state.round_number == self.game_description.n_rounds - 1:
                total_cooperators_second_last_round = sum(history.opponent_actions[-2, :] + [history.my_actions[-2]])
                if total_cooperators_second_last_round < self.game_description.n_players / 2:
                    return Action.C
            return Action.D

class Strategy_COLLECTIVE_222(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective well-being.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.memory_window_size = 2 if game_description.n_rounds <= 3 else min(5, game_description.n_rounds - 1)
        self.memory = 0.0
        self.threshold = game_description.n_players / 2.0
        self.adaptation_rate_delta = 0.05
        self.adaptation_threshold_delta = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1] * 1
        self.memory = (self.memory_window_size - 1) / self.memory_window_size * self.memory + total_cooperators_prev_round / self.memory_window_size
        if self.memory > self.game_description.n_players / 2.0 + self.adaptation_threshold_delta:
            self.threshold += self.adaptation_rate_delta
        elif self.memory < self.game_description.n_players / 2.0 - self.adaptation_threshold_delta:
            self.threshold -= self.adaptation_rate_delta
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if total_cooperators_prev_round >= self.threshold:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_223(BaseStrategy):
    """
    The Adaptive Collective Conscience strategy for the N-Player Public Goods Game.
    
    This strategy balances individual self-interest with collective well-being by adapting to 
    the evolving environment, promoting early contributions, and enforcing social norms.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_contrib_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.game_description.k / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        opponent_actions = history.opponent_actions[-1, :]
        avg_contrib_t_1 = np.sum(opponent_actions) / self.game_description.n_players
        theta = self.game_description.k / self.game_description.n_players
        if avg_contrib_t_1 >= theta:
            cooperate_prob = 1 - (avg_contrib_t_1 - theta)
        else:
            n_cooperators = np.sum(opponent_actions)
            p_punish = 0.5 if n_cooperators <= self.game_description.n_players // 2 and avg_contrib_t_1 < theta else 0
            cooperate_prob = p_punish
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_COLLECTIVE_224(BaseStrategy):
    """
    Adaptive Cooperative Foresight strategy balances individual self-interest with collective well-being.
    It adapts to various opponent behaviors and anticipates potential cooperation breakdowns.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.theta = 0.5 + game_description.k / self.n_players / 2
        self.delta = 0.2
        self.avg_coop_rate = 0
        self.max_coop_rate = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_avg_coop_rate = self.avg_coop_rate
        opponent_actions = history.opponent_actions[-1, :]
        avg_coop_rate = np.mean(opponent_actions) + history.my_actions[-1] / self.n_players
        self.max_coop_rate = max(self.max_coop_rate, avg_coop_rate)
        diff = abs(avg_coop_rate - self.max_coop_rate)
        if avg_coop_rate > self.theta and diff < self.delta:
            action = Action.C
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        if np.all(~opponent_actions):
            action = Action.C
        return action

class Strategy_COLLECTIVE_226(BaseStrategy):
    """
    A strategy that adapts to various opponent behaviors in the N-Player Public Goods Game.
    It encourages cooperation when beneficial and protects against exploitation when cooperation is low.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.acr_history = []
        self.opponent_acr_history = [[] for _ in range(game_description.n_players)]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return random.choice([Action.C, Action.D])
        previous_round_opponent_actions = history.opponent_actions[-1, :]
        acr = np.mean(previous_round_opponent_actions)
        self.acr_history.append(acr)
        for i in range(self.game_description.n_players):
            if i == 0:
                continue
            opponent_acr = np.mean(history.opponent_actions[:, i - 1])
            self.opponent_acr_history[i].append(opponent_acr)
        if acr >= self.game_description.k / self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        for i in range(self.game_description.n_players):
            if i == 0:
                continue
            opponent_acr_deviation = abs(acr - self.opponent_acr_history[i][-1])
            if opponent_acr_deviation > 0.2:
                pass
        return action

class Strategy_COLLECTIVE_228(BaseStrategy):
    """
    Adaptive Cooperate-Defect Strategy.
    
    This strategy adapts to various opponent behaviors by considering the average cooperation rate and adjusting the cooperation threshold based on the multiplication factor k.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.acr = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        opponent_cooperations = np.sum(history.opponent_actions[-1, :])
        self.acr = opponent_cooperations / self.game_description.n_players
        ct = self.game_description.k / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif math.isclose(self.acr, ct):
            return Action.C if random.random() < 0.5 else Action.D
        elif self.acr >= ct:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_229(BaseStrategy):
    """
    Adaptive Cooperation Strategy: 
    This collective strategy aims to balance individual self-interest with the benefits of cooperation, 
    while being robust to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions_prev_round = history.opponent_actions[-1, :]
        total_cooperators_prev_round = sum(opponent_actions_prev_round)
        if total_cooperators_prev_round >= k:
            return Action.C
        opponent_cooperation_rates = np.mean(history.opponent_actions, axis=0)
        if any((opponent_cooperation_rate > 0.7 for opponent_cooperation_rate in opponent_cooperation_rates)):
            return Action.C
        collective_payoff = np.mean(history.my_payoffs)
        if collective_payoff > (1 + k / n_players) * 0.5 and any((not action for action in opponent_actions_prev_round)):
            return Action.D
        if state.round_number == r - 1:
            return Action.D
        if history.my_payoffs.size < n_players and all((history.opponent_actions[:, i].size == 0 for i in range(n_players))):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_230(BaseStrategy):
    """
    The ACC strategy aims to balance individual self-interest with collective well-being by adapting to the evolving game dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.n_players / 2
        self.reputation_system = {i: 0 for i in range(1, game_description.n_players + 1)}
        self.previous_average_cooperation_rate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < self.game_description.n_rounds // 3:
            cooperators = sum(history.opponent_actions[-1, :] == True)
            if cooperators >= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        if state.round_number < 2 * self.game_description.n_rounds // 3:
            average_cooperation_rate = np.mean([sum(history.opponent_actions[t, :] == True) for t in range(state.round_number - self.game_description.n_rounds // 2, state.round_number)]) / self.game_description.n_players
            if average_cooperation_rate > 0.5:
                return Action.C
            else:
                return Action.D
        cooperators = sum(history.opponent_actions[-1, :] == True)
        if cooperators >= 2 * self.game_description.n_players / 3:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_231(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with History-Based Tit-for-Tat (ACT)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_count = 0
        self.defect_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        my_previous_action = history.my_actions[-1]
        if my_previous_action:
            self.coop_count += 1
        else:
            self.defect_count += 1
        if state.round_number % (self.game_description.n_rounds // 2) == 0 and state.round_number > 0:
            self.coop_count = 0
            self.defect_count = 0
        prev_round_coops = sum(history.opponent_actions[-1, :])
        threshold = self.game_description.n_players / 2
        if self.coop_count > self.defect_count:
            threshold += 1
        elif self.coop_count < self.defect_count:
            threshold -= 1
        if prev_round_coops == threshold and state.round_number != self.game_description.n_rounds - 1:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if prev_round_coops >= threshold:
                return Action.C
            else:
                return Action.D
        if prev_round_coops >= threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_232(BaseStrategy):
    """
    Adaptive Cooperative Escalation strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_probability = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        avg_payoff_prev_round = np.mean(np.concatenate((history.my_payoffs, history.opponent_payoffs[-1, :])))
        if prev_round_cooperators >= 0.5 and avg_payoff_prev_round >= 1.5:
            return Action.C
        elif avg_payoff_prev_round < 1.5:
            self.cooperation_probability = min(1.0, self.cooperation_probability + 0.1)
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.C if random.random() < self.cooperation_probability else Action.D

class Strategy_COLLECTIVE_233(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and aligns with a collective mindset.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_contributions = np.sum(history.opponent_actions[-1, :], axis=0) + (history.my_actions[-1] == True)
        avg_contribution_rate = np.mean(prev_contributions)
        if avg_contribution_rate >= self.theta:
            return Action.C
        elif prev_contributions < self.game_description.n_players * self.game_description.k / self.game_description.n and history.my_actions[-1] == True:
            return Action.D
        elif np.any(history.opponent_actions[-1, :]):
            return Action.C
        else:
            return Action.D
        if state.round_number >= self.game_description.n_rounds - 2:
            return Action.C

class Strategy_COLLECTIVE_234(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperativity.
    
    This strategy promotes cooperation while being robust against various opponent behaviors. 
    By adapting to changing cooperation rates and adjusting the threshold, it aims to balance individual payoffs with collective welfare.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = game_description.k / game_description.n_players * (game_description.n_rounds / 2)
        self.cooperate_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        prev_round_coop_rate = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if prev_round_coop_rate > self.theta or (state.round_number != self.game_description.n_rounds - 1 and random.random() < self.cooperate_prob):
            action = Action.C
        else:
            action = Action.D
        if state.round_number % 5 == 0 and state.round_number > 10:
            avg_10round_coop_rate = sum(history.opponent_actions[-10:, :].mean(axis=1)) / self.game_description.n_players
            if avg_10round_coop_rate < self.theta:
                self.theta *= 0.9
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_COLLECTIVE_235(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective benefits by adapting cooperation levels based on past outcomes.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < self.p else Action.D
        avg_payoff_t_1 = np.mean(history.opponent_payoffs[-1, :])
        if avg_payoff_t_1 > self.game_description.k / 2:
            self.p += 0.05
        elif avg_payoff_t_1 < self.game_description.k / 4:
            self.p -= 0.05
        cooperators = sum(history.opponent_actions[-1, :])
        if cooperators >= self.game_description.n_players // 2 or (cooperators == self.game_description.n_players // 2 and random.random() < 0.5):
            return Action.C if not all((not opponent_action for opponent_action in history.opponent_actions[-1, :])) else Action.C if random.random() < self.p else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < self.p else Action.D

class Strategy_COLLECTIVE_236(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        alpha = np.mean(history.opponent_actions[-1, :])
        theta = self.game_description.k / self.game_description.n_players * (self.game_description.n_players / 2)
        if alpha >= theta:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_237(BaseStrategy):
    """
    Adaptive Cooperative Momentum (ACM) strategy for the N-Player Public Goods Game.
    
    This collective strategy balances individual payoffs with cooperative momentum to maximize overall group welfare.
    It adapts its cooperation threshold based on past cooperation rates and responds to deviations from this behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.n_players / 2
        self.adaptive_threshold_adjustment = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1] * 1
        if previous_cooperators >= self.cooperation_threshold:
            action = Action.C
        elif previous_cooperators < self.game_description.n_players / 4:
            action = Action.D
        else:
            action = Action.C
        if state.round_number % (self.game_description.n_rounds // 2) == 0 and state.round_number != 0:
            average_cooperation_rate = sum(history.my_actions[:state.round_number]) / state.round_number
            for opponent in history.opponent_actions.T:
                average_cooperation_rate += sum(opponent[:state.round_number]) / (state.round_number * self.game_description.n_players)
            average_cooperation_rate /= self.game_description.n_players
            if average_cooperation_rate > 0.5:
                self.cooperation_threshold -= self.adaptive_threshold_adjustment
            else:
                self.cooperation_threshold += self.adaptive_threshold_adjustment
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_238(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Punishment (ACP)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_round_outcomes = []
        self.punishment_list = [False] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_opponent_actions = history.opponent_actions[-1]
        prev_round_my_action = history.my_actions[-1]
        for i, opponent_defected in enumerate(prev_round_opponent_actions):
            if opponent_defected == 0 and prev_round_my_action:
                self.punishment_list[i] = True
        total_coop_payoff = sum([prev_round_outcome * self.game_description.k / self.game_description.n_players for prev_round_outcome in prev_round_opponent_actions])
        if total_coop_payoff >= 1:
            action = Action.C
        else:
            action = Action.D
        for i, punished in enumerate(self.punishment_list):
            if punished:
                action = Action.D
        self.prev_round_outcomes.append(action)
        return action

class Strategy_COLLECTIVE_239(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p = 0.5
        self.tau = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p else Action.D
        previous_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if previous_cooperators >= self.game_description.n_players / 2:
            cooperation_probability = min(self.game_description.k / self.game_description.n_players, 1)
        elif self.tau > self.game_description.n_rounds // 4:
            return Action.D
        else:
            cooperation_probability = self.p
        action = Action.C if random.random() < cooperation_probability else Action.D
        if history is not None and len(history.my_payoffs) > state.round_number - 1:
            payoff = history.my_payoffs[state.round_number - 1]
            if payoff > self.game_description.k / self.game_description.n_players:
                self.p = min(self.p + 0.1, 1)
            else:
                self.p = max(self.p - 0.05, 0)
        if previous_cooperators < self.game_description.n_players / 2:
            self.tau += 1
        else:
            self.tau = 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_COLLECTIVE_240(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for N-Player Public Goods Game.
    
    This strategy adapts to various opponent behaviors by adjusting cooperation probabilities based on the number of cooperators and average payoffs in previous rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.avg_payoff_threshold = self.n_players / 2
        self.p_C = 0.8
        self.p_D = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = self.p_C
        else:
            total_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
            if total_cooperators_prev_round > self.n_players / 2:
                cooperate_prob = 0.7 + 0.3 * (self.k / self.n_players)
            else:
                cooperate_prob = 1 - (0.4 + 0.6 * (1 - self.k / self.n_players))
        if state.round_number >= self.game_description.n_rounds // 2:
            avg_payoff = np.mean(history.my_payoffs[-self.game_description.n_rounds // 2:])
            if avg_payoff < self.avg_payoff_threshold:
                self.p_C *= 0.9
                self.p_D *= 1.1
            elif avg_payoff > self.avg_payoff_threshold:
                self.p_C *= 1.1
                self.p_D *= 0.9
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if random.random() < cooperate_prob:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_241(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and relies solely on game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.m = min(5, game_description.n_rounds)
        self.θ = 0.5
        self.δ = 0.3
        self.ε = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.n_rounds - 1:
            return Action.D if state.round_number == self.n_rounds - 1 else Action.C
        avg_contribution = sum(history.my_actions[max(0, state.round_number - self.m):state.round_number]) / min(self.m, state.round_number)
        num_defections = sum((not action for action in history.opponent_actions[state.round_number - 1, :]))
        if random.random() < self.ε:
            return Action(random.choice([True, False]))
        elif avg_contribution > self.θ:
            return Action.C
        elif num_defections > self.δ * (self.n_players - 1):
            return Action.D
        elif not history.my_actions[state.round_number - 1] and avg_contribution < self.θ:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_242(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation (AC)
    
    Balances individual self-interest with collective well-being by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_actions = history.my_actions
        opponent_actions = history.opponent_actions
        my_payoffs = history.my_payoffs
        opponent_payoffs = history.opponent_payoffs
        avg_c_payoff = np.mean([payoff for action, payoff in zip(my_actions, my_payoffs) if action])
        avg_d_payoff = np.mean([payoff for action, payoff in zip(my_actions, my_payoffs) if not action])
        cooperate = False
        if avg_c_payoff > avg_d_payoff:
            cooperate = True
        coop_trend = [sum(opponent_actions[round_idx]) / self.game_description.n_players for round_idx in range(max(0, state.round_number - 3), state.round_number)]
        if all((trend >= 0.5 for trend in coop_trend)):
            cooperate = True
        my_total_payoff = np.sum(my_payoffs)
        avg_total_payoff = np.mean([np.sum(opponent_payoffs[round_idx]) / self.game_description.n_players for round_idx in range(state.round_number)])
        if my_total_payoff > avg_total_payoff * state.round_number and (not all(my_actions[-3:])):
            cooperate = False
        if state.round_number >= self.game_description.n_rounds - 2:
            cooperate = False
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_243(BaseStrategy):
    """
    Collective strategy that adapts to the game's history and parameters.
    It balances individual payoffs with collective welfare by cooperating 
    when it is likely to be reciprocated or beneficial for the group.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_pending = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.punishment_pending and state.round_number < self.game_description.n_rounds - 1:
            self.punishment_pending = False
            return Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        total_contribution = opponent_cooperators + (history.my_actions[-1] == Action.C)
        if opponent_cooperators > self.game_description.n_players / 2:
            return Action.C
        elif total_contribution > self.game_description.k / 4 * self.game_description.n_players:
            return Action.C
        if history.my_actions[-1] == Action.D and opponent_cooperators >= self.game_description.n_players / 2:
            self.punishment_pending = True
        return Action.D

class Strategy_COLLECTIVE_244(BaseStrategy):
    """
    This strategy adaptively responds to the game's history and opponent behaviors 
    by balancing individual self-interest with collective welfare considerations.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.personal_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number > 1 and self.personal_payoffs:
            RF = np.mean(history.opponent_actions[-1, :])
            PEF = np.mean(self.personal_payoffs) > 1 + game_description.k / game_description.n_players
            if RF > 0.5 and PEF:
                return Action.C
            elif RF <= 0.5 or not PEF:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_245(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Memory (ACM).
    
    This strategy prioritizes cooperation while adapting to the actions of others.
    By cooperating initially and when cooperation is working, we encourage mutual cooperation.
    When cooperation isn't paying off, we adjust our strategy to defect, but still attempt to restart cooperation if all players have defected.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.memory_window = []
        self.m = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if prev_round_coops >= self.game_description.n_players / 2:
            action = Action.C
        else:
            my_prev_payoff = history.my_payoffs[-1]
            opponent_defectors = np.sum(history.opponent_actions[-1, :] == False)
            if opponent_defectors > 0:
                avg_defector_payoff = np.mean([history.opponent_payoffs[-1, i] for i in range(self.game_description.n_players - 1) if history.opponent_actions[-1, i] == False])
            else:
                avg_defector_payoff = my_prev_payoff
            if my_prev_payoff < avg_defector_payoff:
                action = Action.D
            else:
                self.memory_window.append(history.my_actions[-1])
                if len(self.memory_window) > self.m:
                    p_c = np.mean([a == True for a in self.memory_window])
                    p_d = 1 - p_c
                    if p_c > p_d:
                        action = Action.C
                    else:
                        action = Action.D
                else:
                    action = Action.D
        if np.all(history.opponent_actions[-1, :] == False) and (not history.my_actions[-1]):
            return Action.C
        return action

class Strategy_COLLECTIVE_246(BaseStrategy):
    """
    The ACF strategy aims to balance individual payoffs with collective well-being by adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        AC = np.mean(opponent_actions) + (history.my_actions[-1] if state.round_number > 0 else True)
        cooperate = AC >= self.game_description.k / self.game_description.n_players
        for opponent_action in opponent_actions:
            j = np.where(opponent_actions == opponent_action)[0][0]
            if not opponent_action and AC > self.game_description.k / self.game_description.n_players:
                cooperate = False
                break
        if not cooperate and state.round_number > 1 and (np.mean(history.opponent_actions[-2, :]) + (history.my_actions[-2] if state.round_number > 1 else True) >= self.game_description.k / self.game_description.n_players):
            cooperate = True
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_247(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective well-being by adapting to the game's history and opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defection_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        last_round_payoff = np.sum(history.my_payoffs[-1:] + history.opponent_payoffs[-1:])
        avg_individual_payoff = last_round_payoff / self.game_description.n_players
        if last_round_payoff >= avg_individual_payoff:
            return Action.C
        defectors_last_round = sum((1 - action for action in history.opponent_actions[-1, :]))
        p_defect = defectors_last_round / self.game_description.n_players
        random_defect = random.random() < p_defect
        if random_defect:
            self.defection_counter += 1
            return Action.D
        collective_payoff = last_round_payoff
        threshold = 0.75 * self.game_description.k
        if collective_payoff > threshold and defectors_last_round > 0:
            self.defection_counter += 1
        cooperators_last_round = sum(history.opponent_actions[-1, :])
        if collective_payoff < 0.25 * self.game_description.k and cooperators_last_round < self.game_description.n_players / 2:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_248(BaseStrategy):
    """
    A hybrid approach combining elements of tit-for-tat (TFT), win-stay lose-shift (WSLS),
    and a contribution-dependent mechanism to balance cooperation incentives with adaptability.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_payoffs = []
        self.last_round_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_total_payoff = sum(self.total_payoffs[-1]) if self.total_payoffs else 0
        my_prev_action = history.my_actions[-1]
        opponent_actions_last_round = history.opponent_actions[-1, :]
        self.last_round_cooperators = sum(opponent_actions_last_round) + int(my_prev_action)
        if prev_total_payoff >= self.game_description.n_players and self.last_round_cooperators > self.game_description.n_players / 2:
            return Action.C
        my_prev_payoff = history.my_payoffs[-1]
        if my_prev_payoff > 1:
            return Action.C
        return Action.D

    def update_history(self, state: GameState, history: PlayerHistory):
        self.total_payoffs.append(history.my_payoffs)

class Strategy_COLLECTIVE_249(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Social Learning.
    
    This strategy aims to balance individual payoffs with collective well-being by adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploration_phase = True
        self.punishment_mechanism = False

    def calculate_avg_coop_payoff(self, history: PlayerHistory) -> float:
        """Calculate the average payoff of cooperators in a given round."""
        previous_round_actions = history.opponent_actions[-1]
        previous_round_payoffs = history.opponent_payoffs[-1]
        num_cooperators = sum(previous_round_actions)
        if num_cooperators == 0:
            return 0
        avg_coop_payoff = np.mean(previous_round_payoffs[previous_round_actions])
        return avg_coop_payoff

    def calculate_avg_defect_payoff(self, history: PlayerHistory) -> float:
        """Calculate the average payoff of defectors in a given round."""
        previous_round_actions = history.opponent_actions[-1]
        previous_round_payoffs = history.opponent_payoffs[-1]
        num_defectors = self.game_description.n_players - 1 - sum(previous_round_actions)
        if num_defectors == 0:
            return 0
        avg_defect_payoff = np.mean(previous_round_payoffs[~previous_round_actions])
        return avg_defect_payoff

    def count_cooperators(self, history: PlayerHistory) -> int:
        """Count the number of cooperators in a given round."""
        previous_round_actions = history.opponent_actions[-1]
        num_cooperators = sum(previous_round_actions)
        return num_cooperators

    def was_exploited(self, history: PlayerHistory) -> bool:
        """Check if this player was exploited by a defector in the previous round."""
        previous_round_actions = history.opponent_actions[-1]
        previous_my_action = history.my_actions[-1]
        if not previous_my_action and any(~previous_round_actions):
            return True
        return False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.exploration_phase = True
            self.punishment_mechanism = False
            return Action.C
        if self.exploration_phase:
            action = Action.C if state.round_number % 2 == 0 else Action.D
            if state.round_number >= 4:
                self.exploration_phase = False
            return action
        avg_coop_payoff = self.calculate_avg_coop_payoff(history)
        avg_defect_payoff = self.calculate_avg_defect_payoff(history)
        num_cooperators = self.count_cooperators(history)
        if avg_coop_payoff > avg_defect_payoff and num_cooperators >= self.game_description.n_players / 2:
            action = Action.C
        else:
            action = Action.D
        if self.was_exploited(history):
            self.punishment_mechanism = True
            action = Action.D
        elif self.punishment_mechanism:
            if self.count_cooperators(history) >= self.count_cooperators(history) + 1:
                self.punishment_mechanism = False
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_250(BaseStrategy):
    """
    Collective strategy to balance individual self-interest with collective well-being.
    Adapts to various opponent behaviors and game histories, encouraging cooperation when others cooperate,
    gradually increasing cooperation as the payoff difference between cooperators and defectors grows,
    and avoiding exploitation by detecting consecutive defections.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.5
        self.avg_payoff_diff = 0
        self.consecutive_defections = 0
        self.prev_avg_payoff_diff = None

    def increasing(self) -> bool:
        return self.avg_payoff_diff > 0 and (self.prev_avg_payoff_diff is None or self.avg_payoff_diff > self.prev_avg_payoff_diff)

    def count_cooperators(self, history: PlayerHistory) -> int:
        return sum(history.opponent_actions[-1, :] | np.array([history.my_actions[-1]]))

    def update_avg_payoff_diff(self, my_payoffs: NDArray[np.float64], opponent_payoffs: NDArray[np.float64]):
        if len(my_payoffs) < 2:
            self.avg_payoff_diff = 0
            return
        payoff_differences = np.mean(opponent_payoffs[:-1] - opponent_payoffs[-1]) + my_payoffs[-1] - my_payoffs[-2]
        self.avg_payoff_diff = (payoff_differences + self.avg_payoff_diff * (len(my_payoffs) - 2)) / (len(my_payoffs) - 1)
        self.prev_avg_payoff_diff = payoff_differences

    def update_consecutive_defections(self, history: PlayerHistory):
        if len(history.my_actions) < 2:
            self.consecutive_defections = 0
            return
        recent_cooperations = np.sum(history.opponent_actions[-3:, :]) + sum(history.my_actions[-3:])
        if recent_cooperations == 0 and all(~history.my_actions[-3:]):
            self.consecutive_defections += 1
        else:
            self.consecutive_defections = max(0, self.consecutive_defections - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        num_cooperators_prev_round = self.count_cooperators(history)
        my_payoffs = np.array(history.my_payoffs)
        opponent_payoffs = history.opponent_payoffs
        self.update_avg_payoff_diff(my_payoffs, opponent_payoffs)
        self.update_consecutive_defections(history)
        if num_cooperators_prev_round > self.game_description.n_players // 2:
            return Action.C
        elif self.increasing():
            self.cooperate_prob = min(1, self.cooperate_prob + 0.1)
        elif self.consecutive_defections >= 2:
            return random.choice([Action.D] * 4 + [Action.C])
        if state.round_number in (self.game_description.n_rounds - 1, self.game_description.n_rounds):
            return Action.C if num_cooperators_prev_round >= self.game_description.n_players // 2 else Action.D
        elif all(~history.my_actions[-3:]):
            return random.choice([Action.D] * 7 + [Action.C])
        return random.choice([Action.C] if random.random() < self.cooperate_prob else [Action.D])

class Strategy_COLLECTIVE_251(BaseStrategy):
    """
    The Adaptive Cooperation strategy balances individual self-interest with collective welfare.
    It adapts to changing opponent behaviors and promotes cooperation while safeguarding against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_payoff = np.mean(history.my_payoffs[-1] + self.game_description.k / self.game_description.n_players * np.sum(history.opponent_actions[-1, :], axis=0))
        opponent_defections = np.sum(np.logical_not(history.opponent_actions), axis=0)
        if previous_payoff > self.game_description.k / 2:
            return Action.C
        elif previous_payoff < 1 / self.game_description.n_players:
            return Action.D
        else:
            p = state.round_number / self.game_description.n_rounds
            if random.random() < p:
                return Action.C
            else:
                return Action.D
        for opponent in range(self.game_description.n_players - 1):
            if not history.opponent_actions[-1, opponent] and previous_payoff > self.game_description.k / 2:
                return Action.D
            elif history.opponent_actions[-1, opponent] and state.round_number > 1:
                return Action.C
        return Action.C

class Strategy_COLLECTIVE_252(BaseStrategy):
    """
    A collective strategy that adapts to the game's history and opponent behaviors,
    balancing individual payoff with the collective good in a repeated public goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.C_prev = 0
        self.p_defect = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self.C_prev = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if state.round_number < self.game_description.n_rounds - 1:
            if self.C_prev >= self.game_description.k / 2:
                return Action.C
            else:
                self.p_defect = min(1, max(0, 1 - self.C_prev / self.game_description.k))
                return Action.D if random.random() < self.p_defect else Action.C
        else:
            return Action.D
        my_payoff = history.my_payoffs[-1]
        expected_payoff = 1 + self.game_description.k / self.game_description.n_players * self.C_prev
        if my_payoff < expected_payoff:
            self.p_defect = min(1, self.p_defect + 0.05)
        elif my_payoff > expected_payoff:
            self.p_defect = max(0, self.p_defect - 0.05)

class Strategy_COLLECTIVE_253(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) strategy for N-Player Public Goods Game.
    
    This strategy adapts to various opponent behaviors, balancing individual gain with collective optimality.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5
        self.alpha = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choices([Action.C, Action.D], weights=[0.8, 0.2])[0]
        prev_cooperation_rate = np.mean(history.opponent_actions[-1, :])
        if prev_cooperation_rate > self.theta:
            return random.choices([Action.C, Action.D], weights=[0.9, 0.1])[0]
        elif history.my_actions[-1] and prev_cooperation_rate < self.theta:
            return random.choices([Action.C, Action.D], weights=[0.2, 0.8])[0]
        opponent_cooperations = np.sum(history.opponent_actions[-1, :])
        self.alpha = (self.alpha * (state.round_number - 1) + opponent_cooperations / self.game_description.n_players) / state.round_number
        if state.round_number == self.game_description.n_rounds - 1:
            return random.choices([Action.C, Action.D], weights=[0.5, 0.5])[0]
        return Action.C

class Strategy_COLLECTIVE_254(BaseStrategy):
    """
    A collective strategy that balances individual payoffs with collective well-being by adapting to the game's history and promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.n_players * 0.5
        self.epsilon = 0.05
        self.delta = 0.1
        self.punishment_multiplier = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if previous_cooperators > self.threshold:
            p = self.game_description.k / self.game_description.n_players + self.epsilon
            action = random.choices([Action.C, Action.D], weights=[p, 1 - p])[0]
        else:
            q = self.game_description.n_players / self.game_description.k - self.epsilon
            action = random.choices([Action.D, Action.C], weights=[q * self.punishment_multiplier, 1 - q])[0]
            if previous_cooperators > self.threshold and history.my_actions[-1] == Action.D:
                self.punishment_multiplier += self.delta
            elif history.my_actions[-1] == Action.C and history.my_actions[-2] == Action.D:
                self.punishment_multiplier -= self.delta / (state.round_number + 1)
        return action

class Strategy_COLLECTIVE_255(BaseStrategy):
    """
    Collective strategy that balances cooperation and defection based on the game's history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_defections = 0

    def calculate_average_payoff(self, history: PlayerHistory) -> float:
        if len(history.my_payoffs) == 1:
            return 1.0
        else:
            payoffs = np.append(history.my_payoffs[:-1], history.opponent_payoffs[-2])
            return np.mean(payoffs)

    def count_cooperators(self, history: PlayerHistory) -> int:
        if len(history.my_actions) == 1:
            return int(history.my_actions[0])
        else:
            cooperators = sum(np.append([history.my_actions[-2]], history.opponent_actions[-2]))
            return int(cooperators)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        avg_payoff_prev = self.calculate_average_payoff(history)
        num_cooperators_prev = self.count_cooperators(history)
        if avg_payoff_prev > 1.5:
            self.consecutive_defections = 0
            return Action.C
        elif 1 <= avg_payoff_prev < 1.5:
            self.consecutive_defections += 1
            if random.random() < 0.5:
                return Action.D
            else:
                return Action.C
        else:
            self.consecutive_defections += 1
            return Action.D
        if num_cooperators_prev < self.game_description.n_players / 2 and history.my_payoffs[-1] < avg_payoff_prev:
            self.consecutive_defections = 0
            if random.random() < 0.8:
                return Action.D
        if self.consecutive_defections >= 3:
            self.consecutive_defections = 0
            return Action.C

class Strategy_COLLECTIVE_256(BaseStrategy):
    """
    Adaptive Collective Cooperation strategy.
    
    This strategy balances individual self-interest with collective well-being by adapting to opponents' behavior.
    It uses an initial exploration phase, a contribution threshold, and punishment/reward mechanisms to maintain cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialize the ACC strategy.

        Args:
            game_description (PublicGoodsDescription): The game description containing n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.threshold = game_description.k / game_description.n_players * 0.5
        self.punish_counter = 0
        self.reward_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide the action based on the current state and history.

        Args:
            state (GameState): The current game state.
            history (None | PlayerHistory): The player's history, or None if this is the first round.

        Returns:
            Action: The chosen action (C or D).
        """
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        avg_contributors = np.mean(history.my_actions[:state.round_number]) + np.mean(np.sum(history.opponent_actions[:state.round_number, :], axis=1))
        self.threshold = self.game_description.k / self.game_description.n_players * avg_contributors
        if self.punish_counter > 0:
            self.punish_counter -= 1
            return Action.D
        if self.reward_counter > 0:
            self.reward_counter -= 1
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        cooperators = np.sum(history.opponent_actions[-1, :]) + history.my_actions[state.round_number - 1]
        if cooperators < self.threshold:
            self.punish_counter = 2
            return Action.D
        if cooperators > self.threshold:
            self.reward_counter = 2
            return Action.C
        if avg_contributors > 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_257(BaseStrategy):
    """
    A collective strategy that adapts to the number of cooperators in the previous round.
    Cooperates initially and when the number of cooperators is greater than or equal to the multiplication factor,
    otherwise defects. Defects in the last round regardless of the previous round's outcome.
    In case of a tie, cooperates to maintain a cooperative atmosphere.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D
        prev_round_cooperators = sum((1 for action in history.opponent_actions[-1, :] if action))
        if prev_round_cooperators >= self.k:
            return Action.C
        elif prev_round_cooperators == self.n_players / 2:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_258(BaseStrategy):
    """
    The ACPG strategy aims to balance individual payoff maximization with collective welfare by adapting to the dynamics of cooperation and defection in the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.previous_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self.previous_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if self.game_description.n_rounds - state.round_number <= 1:
            return Action.C
        if self.previous_cooperators >= self.game_description.n_players / 2:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_259(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective welfare by adapting to the behavior of other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T = 1.0
        self.alpha = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.k > 2.5 and self.game_description.n_players > 3 else Action.D
        last_round_payoffs = np.mean(history.my_payoffs[-1])
        for i in range(self.game_description.n_players - 1):
            last_round_payoffs += history.opponent_payoffs[-1, i]
        last_round_payoffs /= self.game_description.n_players
        T_new = self.T + self.alpha * (last_round_payoffs - self.T)
        if np.sum(history.opponent_actions[-1]) < self.game_description.n_players / 2:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            expected_defection_payoff = last_round_payoffs + 1
            cooperation_payoff = self.game_description.k / self.game_description.n_players * np.sum(history.opponent_actions[-1])
            return Action.C if cooperation_payoff >= expected_defection_payoff else Action.D
        return Action.C if T_new < last_round_payoffs else Action.D

class Strategy_COLLECTIVE_260(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) strategy for N-Player Public Goods Game.
    
    This strategy balances individual self-interest with collective benefits, 
    adapting to the dynamics of the game while promoting cooperation when beneficial to all players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialize ACO strategy with game parameters.

        :param game_description: Game description containing n_players, n_rounds, and k
        """
        self.game_description = game_description
        self.p = 0.5
        self.CT = game_description.k / game_description.n_players * (game_description.n_players / 2)
        self.last_round_payoffs = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determine the action for a given game state and history.

        :param state: Current game state containing round_number
        :param history: History of previous rounds, or None if first round
        :return: Chosen action (Action.C or Action.D)
        """
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history is not None and self.all_defected(history, state.round_number - 1) and self.all_defected(history, state.round_number - 2):
            return Action.C if random.random() < 0.5 else Action.D
        if state.round_number == 0:
            return Action.C if random.random() < self.p else Action.D
        avg_payoff_prev_round = np.mean(history.my_payoffs[state.round_number - 1])
        if avg_payoff_prev_round > self.CT:
            self.p = min(self.p + 0.1, 1)
        else:
            self.p = max(self.p - 0.1, 0)
        return Action.C if random.random() < self.p else Action.D

    def all_defected(self, history: PlayerHistory, round_number: int) -> bool:
        """
        Check if all players defected in a given round.

        :param history: History of previous rounds
        :param round_number: Round number to check
        :return: True if all players defected, False otherwise
        """
        return not np.any(history.opponent_actions[round_number, :] | history.my_actions[round_number])

class Strategy_COLLECTIVE_261(BaseStrategy):
    """
    This strategy balances individual payoffs with collective well-being by adapting to diverse opponent behaviors.
    It uses a combination of exploration, exploitation, and social learning to optimize cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_coop = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        avg_payoff_per_coop = self.calculate_avg_payoff_per_coop(state, history)
        self.update_p_coop(avg_payoff_per_coop, history)
        opponent_prev_action = self.get_opponent_prev_action(history)
        if opponent_prev_action == Action.D:
            return Action.D if random.random() < 0.7 else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if self.game_description.n_rounds % 2 == 0:
                return Action.C if self.collective_payoff_was_higher_than_average(history) else Action.D
            else:
                return Action.D
        return Action.C if random.random() < self.p_coop else Action.D

    def calculate_avg_payoff_per_coop(self, state: GameState, history: PlayerHistory):
        prev_round_opponent_actions = history.opponent_actions[state.round_number - 1]
        cooperators = np.sum(prev_round_opponent_actions)
        if cooperators == 0:
            return 0
        avg_payoff_per_coop = np.mean(history.my_payoffs[:state.round_number]) / cooperators
        return avg_payoff_per_coop

    def update_p_coop(self, avg_payoff_per_coop: float, history: PlayerHistory):
        if avg_payoff_per_coop >= np.mean(history.my_payoffs[:len(history.my_payoffs)]):
            self.p_coop += 0.1
        else:
            self.p_coop -= 0.1
        self.p_coop = max(0, min(self.p_coop, 1))

    def get_opponent_prev_action(self, history: PlayerHistory):
        prev_round_opponent_actions = history.opponent_actions[len(history.my_payoffs) - 2]
        return Action.C if np.any(prev_round_opponent_actions) else Action.D

    def collective_payoff_was_higher_than_average(self, history: PlayerHistory):
        prev_round_opponent_payoffs = history.opponent_payoffs[len(history.my_payoffs) - 2]
        return np.mean(prev_round_opponent_payoffs) > np.mean(history.my_payoffs[:len(history.my_payoffs)])

class Strategy_COLLECTIVE_262(BaseStrategy):
    """
    The ACE strategy aims to balance individual self-interest with collective welfare by adapting to the game's history and encouraging cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if cooperators >= self.game_description.n_players // 2:
            return Action.C
        p_C = 0.5 + (state.round_number - 1) / (self.game_description.n_rounds - 1)
        if random.random() < min(p_C, 1.0):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_263(BaseStrategy):
    """
    Adaptive Cooperative Optimism strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_cooperation_rate = 0.5
        self.total_cooperators_prev = 0
        self.avg_payoff_per_cooperator_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        total_cooperators_curr = sum(history.opponent_actions[-1, :])
        opponent_payoffs = history.opponent_payoffs[-1, :]
        avg_payoff_per_cooperator_curr = np.mean(opponent_payoffs[history.opponent_actions[-1, :] == 1])
        self.total_cooperators_prev = total_cooperators_curr
        self.avg_payoff_per_cooperator_prev = avg_payoff_per_cooperator_curr
        if state.round_number > 1:
            total_cooperators_two_rounds_ago = sum(history.opponent_actions[-2, :])
            self.opponent_cooperation_rate = (self.opponent_cooperation_rate * (state.round_number - 1) + total_cooperators_curr / self.game_description.n_players) / state.round_number
        if avg_payoff_per_cooperator_curr >= 1:
            return Action.C
        elif total_cooperators_curr < self.total_cooperators_prev and avg_payoff_per_cooperator_curr < 1:
            return random.choice([Action.C, Action.D], p=[0.25, 0.75])
        else:
            return Action.D

class Strategy_COLLECTIVE_264(BaseStrategy):
    """
    Adaptive Cooperation with Punishment (ACP) Strategy.

    This strategy prioritizes cooperation while adapting to the behavior of others.
    It encourages cooperation through initial cooperation and punishment mechanisms,
    making it robust against various opponent strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_coop_count = sum(history.opponent_actions[-1, :]) + int(bool(history.my_actions[-1]))
        avg_payoff = np.mean(np.concatenate(([history.my_payoffs[-1]], history.opponent_payoffs[-1])))
        if not bool(history.my_actions[-1]) and history.my_payoffs[-1] > avg_payoff:
            return Action.C
        if prev_coop_count >= self.game_description.n_players / 2:
            return Action.C
        elif history.my_payoffs[-1] < avg_payoff:
            return Action.D
        if prev_coop_count == self.game_description.n_players / 2 or state.round_number == self.game_description.n_rounds - 1:
            if bool(history.my_actions[-1]):
                return Action.C
            else:
                return Action.D
        return Action.C

class Strategy_COLLECTIVE_265(BaseStrategy):
    """
    Adaptive Collective Contribution (ACC) strategy for N-player Public Goods game.

    This strategy balances individual self-interest with collective welfare,
    adapting to opponent behaviors and punishing non-cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperators = []
        self.payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :] + [True])
        tau = self.game_description.k / self.game_description.n_players * previous_cooperators
        total_payoff_public_good = np.sum(self.payoffs)
        avg_private_payoff = np.mean([1 - c_i for c_i in history.opponent_actions[-1, :]])
        if len(self.cooperators) >= tau:
            return Action.C
        elif total_payoff_public_good < avg_private_payoff:
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_266(BaseStrategy):
    """
    Adaptive Cooperative Fingerprint (ACF) strategy.
    
    ACF is a collective strategy that adapts cooperation based on the majority behavior of all players, 
    incorporating punishment and forgiveness mechanisms to promote cooperation while being robust against free-riding opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_rounds_left = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        prev_actions = history.opponent_actions[-1]
        prev_payoffs = history.opponent_payoffs[-1]
        num_cooperators = sum(prev_actions)
        avg_payoff = np.mean(prev_payoffs)
        cooperate_probability = 1.0 if num_cooperators > self.game_description.n_players / 2 else 0.4
        punish_defect_probability = 0.8 if history.my_payoffs[-1] < avg_payoff and num_cooperators > self.game_description.n_players / 2 else 0.0
        if punish_defect_probability == 0.8:
            self.punishment_rounds_left = 2
        forgive_cooperate_probability = 0.7 if punish_defect_probability == 0.8 and history.my_payoffs[-1] >= avg_payoff else 0.0
        cooperate_probability *= 1 - punish_defect_probability
        cooperate_probability += forgive_cooperate_probability * (1 - cooperate_probability)
        self.punishment_rounds_left -= 1 if self.punishment_rounds_left > 0 else 0
        return Action.C if random.random() < cooperate_probability else Action.D

class Strategy_COLLECTIVE_267(BaseStrategy):
    """
    The Adaptive Cooperator strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        k_factor = self.game_description.k
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions_last_round = history.opponent_actions[-1]
        majority_cooperated = np.sum(opponent_actions_last_round) > n_players / 2
        if majority_cooperated:
            return Action.C
        elif k_factor > 1.5 * n_players:
            return Action.C

        def exploitation_detected():
            count_defectors = np.sum(np.logical_not(opponent_actions_last_round))
            return count_defectors >= math.ceil(n_players / 2) - 1 and np.any(opponent_actions_last_round)
        if exploitation_detected():
            return Action.D

        def consecutive_defection():
            return all((not action for action in history.my_actions[-min(len(history.my_actions), state.round_number):])) and state.round_number > 2
        if consecutive_defection():
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_268(BaseStrategy):
    """
    Collective strategy that balances individual self-interest with collective well-being.
    Adapts to the evolving game dynamics and responds reciprocally to opponents' actions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 1.5
        self.punishment_threshold = game_description.n_players / 2
        self.exploration_probability = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        avg_payoff_prev = np.mean(history.my_payoffs[-1])
        num_cooperators_prev = sum(history.opponent_actions[-1, :])
        if avg_payoff_prev >= self.threshold and num_cooperators_prev >= self.punishment_threshold:
            action = Action.C
        elif random.random() < self.exploration_probability:
            action = random.choice([Action.C, Action.D])
        else:
            action = Action.D
        if num_cooperators_prev < self.punishment_threshold and state.round_number > 1:
            action = Action.D
        return action

class Strategy_COLLECTIVE_269(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation.
    
    This strategy aims to balance individual self-interest with the benefits of cooperation, 
    while being robust to various opponent behaviors. It uses a cooperation index to adapt 
    its decision based on the overall level of cooperation in the group.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number <= self.game_description.n_players and len(history.opponent_actions) < self.game_description.n_players - 1:
            prev_round_coops = sum((1 for action in history.my_actions))
            return Action.C if prev_round_coops >= self.game_description.n_players / 2 else Action.D
        if state.round_number <= self.game_description.n_players:
            prev_round_coops = sum(history.opponent_actions[-1, :])
            return Action.C if prev_round_coops >= self.game_description.n_players / 2 else Action.D
        cooperation_index = np.mean(np.sum(history.opponent_actions[-self.game_description.n_players:, :], axis=0))
        return Action.C if cooperation_index >= self.game_description.k / self.game_description.n_players else Action.D

class Strategy_COLLECTIVE_270(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Punishment (ACP)

    ACP promotes cooperation while being robust to exploitation.
    It adapts to the level of cooperation in the group, switching to defection when cooperation falls below a threshold.
    The punishment mechanism deters free-riding and incentivizes cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate = True
        self.punish = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number == 1 and history is None):
            self.cooperate = True
            self.punish = False
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        total_payoff = np.sum(history.my_payoffs)
        if state.round_number > 1:
            previous_round_payoff = np.sum(history.opponent_payoffs[-2, :]) + history.my_payoffs[-2]
            if previous_round_payoff < self.game_description.n_players:
                self.punish = True
            else:
                self.punish = False
        if self.cooperate and opponent_cooperators >= self.game_description.n_players / 2 or self.punish:
            return Action.D
        elif not self.cooperate and opponent_cooperators < self.game_description.n_players / 2:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_271(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Grudges (ACG)

    ACG prioritizes collective success by initially cooperating and continuing 
    to cooperate if the group's overall performance meets or exceeds expectations. 
    This approach encourages others to reciprocate cooperation, leading to a mutually beneficial outcome.
    
    However, when faced with suboptimal group performance, ACG adapts by defecting, 
    signaling that individual self-interest will be prioritized unless collective cooperation improves.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            total_payoff = np.sum(history.my_payoffs[-1]) + np.sum(history.opponent_payoffs[-1, :])
            expected_coop_payoff = self.game_description.k / self.game_description.n_players * self.game_description.n_players
            if total_payoff >= expected_coop_payoff:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_272(BaseStrategy):
    """
    Adaptive Collective Optimism Strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.n_players / 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        T = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if len(history.my_payoffs) > 1 and history.my_payoffs[-1] > np.mean(history.opponent_payoffs[-2, :]):
            self.threshold -= 1 / self.game_description.n_players
        else:
            self.threshold += 1 / self.game_description.n_players
        if T >= self.threshold and state.round_number < self.game_description.n_rounds or (state.round_number > 1 and history.my_actions[-2] == False and (T <= sum(history.opponent_actions[-2, :]))):
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if len(history.my_payoffs) > 1 and history.my_payoffs[-2] > np.mean(history.opponent_payoffs[-3, :]):
                return Action.C
        if state.round_number > 2 and sum([not a for a in history.my_actions[-3:]]) == 3:
            return Action.C
        if T >= self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_273(BaseStrategy):
    """
    Adaptive Reciprocity with Noise Tolerance (ARN) Strategy.

    This strategy balances cooperation with robustness against various opponent behaviors.
    It encourages mutual cooperation, adapts to payoff comparisons, and handles noise tolerance.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n = self.game_description.n_players
        k = self.game_description.k
        if history is None or len(history.my_actions) == 0:
            return Action.C
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        prev_round_payoffs = history.opponent_payoffs[-1, :]
        num_prev_round_cooperators = sum(prev_round_opponent_actions)
        coop_payoffs = [payoff for action, payoff in zip(prev_round_opponent_actions, prev_round_payoffs) if action]
        defect_payoffs = [payoff for action, payoff in zip(prev_round_opponent_actions, prev_round_payoffs) if not action]
        avg_coop_payoff = np.mean(coop_payoffs) if coop_payoffs else 0.0
        avg_defect_payoff = np.mean(defect_payoffs) if defect_payoffs else 0.0
        if avg_coop_payoff >= avg_defect_payoff:
            return Action.C
        elif num_prev_round_cooperators >= n / 2:
            return Action.C
        defect_prob = 1 - k / n
        return Action.D if random.random() < defect_prob else Action.C

class Strategy_COLLECTIVE_274(BaseStrategy):
    """
    Collective Strategy: Adaptive Tit-for-Tat with Public Goods Consideration.
    
    This strategy promotes collective cooperation while being robust against exploitation by non-cooperative players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.rounds_with_decreasing_cooperation = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :] | [history.my_actions[-1]]) - 1
        if previous_cooperators >= self.game_description.n_players // 2:
            action = Action.C
        else:
            previous_payoffs = history.opponent_payoffs[-1, :]
            average_cooperator_payoff = np.mean(previous_payoffs[history.opponent_actions[-1, :]])
            average_defector_payoff = np.mean(previous_payoffs[np.logical_not(history.opponent_actions[-1, :])])
            if average_cooperator_payoff >= average_defector_payoff:
                p_defect = 0.5
            else:
                p_defect = self.game_description.k / self.game_description.n_players * (average_cooperator_payoff / average_defector_payoff)
            if random.random() < p_defect:
                action = Action.D
            else:
                action = Action.C
        if history.my_actions[-1] == True and previous_cooperators < self.game_description.n_players // 2:
            self.rounds_with_decreasing_cooperation += 1
        else:
            self.rounds_with_decreasing_cooperation = max(self.rounds_with_decreasing_cooperation - 1, 0)
        if self.rounds_with_decreasing_cooperation > 2:
            action = Action.D
            self.rounds_with_decreasing_cooperation = 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_275(BaseStrategy):
    """
    This strategy balances individual self-interest with collective well-being by adapting to the game's history and encouraging cooperation while punishing defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defecting_opponents = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < 3 and history.my_actions[-1] == Action.D:
            self.defecting_opponents.add(1)
        elif state.round_number >= 2 and history.opponent_actions[-2, 0]:
            if history.my_actions[-1] == Action.C:
                self.defecting_opponents.add(1)
        retaliate = state.round_number <= state.round_number - len(self.defecting_opponents) + 2
        if retaliate:
            return Action.D
        elif history.my_actions[-1] == Action.C and sum(history.opponent_actions[-1, :]) < self.game_description.n_players // 2:
            return Action.D
        elif state.round_number >= self.game_description.n_rounds - 1:
            if sum(history.opponent_actions[-2, :]) > self.game_description.n_players // 2:
                return Action.C
        else:
            cooperators = sum(history.opponent_actions[-1, :])
            enough_public_good = cooperators >= self.game_description.k - 1
            most_cooperated_last_round = cooperators > self.game_description.n_players // 2
            if most_cooperated_last_round or enough_public_good:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_276(BaseStrategy):
    """
    Adaptive Cooperation with Conditional Defection (ACCD) strategy.
    
    Balances individual self-interest with collective well-being by adapting cooperation levels based on the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.k / game_description.n_players * (game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coops = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if prev_coops >= self.threshold:
            return Action.C
        p_defect = ((game_description.n_players - prev_coops) / game_description.n_players) ** 2
        return Action.D if random.random() < p_defect else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_277(BaseStrategy):
    """
    A strategy that adapts to the cooperation level of opponents and retaliates against defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round = history.opponent_actions[-1, :]
        cooperators = sum(previous_round)
        retaliating = False
        for i in range(len(history.my_actions) - 1):
            round = history.opponent_actions[i, :]
            if np.sum(round) > self.game_description.n_players / 2 and (not all(round)):
                retaliating = True
        if retaliating:
            return Action.D
        if cooperators > self.game_description.n_players / 2:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_278(BaseStrategy):
    """
    A collective strategy called "Adaptive Cooperative Escalation" (ACE) 
    that balances individual payoff maximization with promoting group cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_round_payoff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        coop_threshold = self.game_description.k / self.game_description.n_players
        avg_coop_rate = self.calculate_avg_coop_rate(history)
        if avg_coop_rate >= coop_threshold:
            action = Action.C
        elif self.prev_round_payoff is not None and self.prev_round_payoff < np.mean(history.my_payoffs) * 0.8:
            action = Action.D
        elif self.opponent_defected(history, state.round_number - 1):
            action = Action.D
        else:
            action = Action.C
        if history is not None and len(history.my_payoffs) > 0:
            self.prev_round_payoff = history.my_payoffs[-1]
        return action

    def calculate_avg_coop_rate(self, history: PlayerHistory):
        window_size = min(self.game_description.n_rounds, 5)
        coop_count = sum((1 for i in range(max(0, len(history.my_actions) - window_size), len(history.my_actions)) if history.my_actions[i]))
        return coop_count / (window_size * self.game_description.n_players)

    def opponent_defected(self, history: PlayerHistory, round_num: int):
        for i in range(self.game_description.n_players - 1):
            if not history.opponent_actions[round_num, i] and history.my_actions[round_num]:
                return True
        return False

    def calculate_payoff(self, history: PlayerHistory, action):
        total_coop = sum((1 for a in [action] + history.opponent_actions[-1, :].tolist() if a))
        return 1 - int(action == Action.D) + self.game_description.k / self.game_description.n_players * total_coop

class Strategy_COLLECTIVE_279(BaseStrategy):
    """
    The Adaptive Cooperativity (AC) strategy balances individual self-interest with collective well-being by adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.epsilon = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if cooperators >= self.game_description.k / self.game_description.n_players:
            return Action.C
        if random.random() < self.epsilon:
            return random.choice([Action.C, Action.D])
        return Action.D

class Strategy_COLLECTIVE_280(BaseStrategy):
    """
    A collective strategy that promotes cooperation while being robust against various opponent behaviors.
    It starts with a cooperative tone and gradually adapts to changes in the group's behavior, using a probabilistic approach for forgiveness.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def calculate_cooperation_rate(self, history: PlayerHistory) -> float:
        """
        Calculate the cooperation rate (CR) as the ratio of cooperators to total players in the previous round.
        """
        if len(history.my_actions) == 0 or history.opponent_actions.shape[1] == 0:
            return 0.5
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        my_action = int(history.my_actions[-1])
        total_cooperators = opponent_cooperators + my_action
        return total_cooperators / self.game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.game_description.n_rounds == 2:
            return Action.C
        if state.round_number == 0:
            return Action.C
        if state.round_number <= 4:
            cooperation_rate = self.calculate_cooperation_rate(history)
            if cooperation_rate >= 0.5:
                return Action.C
            else:
                return Action.D
        cooperation_rate = self.calculate_cooperation_rate(history)
        if cooperation_rate >= 0.7:
            return Action.C
        elif 0.5 <= cooperation_rate < 0.7:
            probability = (cooperation_rate - 0.5) / 0.2
            if random.random() < probability:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_281(BaseStrategy):
    """
    The Collective Strategy: Adaptive Cooperation with Retribution (ACR).
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.total_cooperators_prev = 0
        self.avg_payoff_all_players_prev = 0
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self.total_cooperators_prev = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        opponent_payoffs_last_round = history.opponent_payoffs[-1, :]
        my_payoff_last_round = history.my_payoffs[-1]
        total_payoffs_last_round = sum(opponent_payoffs_last_round) + my_payoff_last_round
        self.avg_payoff_all_players_prev = total_payoffs_last_round / self.game_description.n_players
        if state.round_number < self.game_description.n_rounds - 1:
            if self.total_cooperators_prev >= math.ceil((self.game_description.n_players + 1) / 2):
                action = Action.C
            else:
                action = Action.D
                if self.avg_payoff_all_players_prev < self.game_description.k * self.game_description.n_players / self.game_description.n_players and self.total_cooperators_prev < math.ceil((self.game_description.n_players + 1) / 2):
                    action = Action.D
        elif self.total_cooperators_prev >= math.ceil((self.game_description.n_players + 1) / 2):
            action = Action.C
        else:
            action = Action.D
        return action

class Strategy_COLLECTIVE_282(BaseStrategy):
    """
    Adaptive Reciprocity with Public Goods Orientation strategy.

    This strategy balances individual self-interest with collective well-being by adapting to the level of cooperation in previous rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.last_round_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history.my_actions[-1] else 0)
        cooperation_rate = cooperators / self.game_description.n_players
        probability_defect = (self.game_description.n_players - cooperators) / self.game_description.n_players
        if self.game_description.k > self.game_description.n_players / 2 and cooperation_rate > 0:
            return Action.C
        if random.random() < probability_defect:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_283(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        k = self.game_description.k
        round_number = state.round_number
        if round_number == 0:
            return Action.C
        avg_coop = sum(history.my_actions) / (round_number + 1)
        total_payoff_prev_round = history.my_payoffs[-1] + k / n_players * sum([int(action) for action in history.opponent_actions[-1, :]])
        prev_cooperators = sum(history.opponent_actions[-2, :] if round_number > 1 else [0])
        curr_cooperators = sum((int(action) for action in history.opponent_actions[-1, :]))
        increased_coop = curr_cooperators > prev_cooperators + (round_number == 1)
        if avg_coop >= n_players / 2:
            return Action.C
        elif total_payoff_prev_round < k / n_players * n_players / 2:
            return Action.D
        elif increased_coop:
            return Action.C
        if round_number == self.game_description.n_rounds - 1:
            avg_payoff = sum(history.my_payoffs) / (round_number + 1)
            if avg_payoff >= k / n_players * n_players / 2:
                return Action.C
            else:
                return Action.D
        opponent_defections = sum([not action for action in history.opponent_actions[-1, :]])
        if opponent_defections > n_players / 2 and history.my_actions[-1]:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_284(BaseStrategy):
    """
    A collective strategy that adapts to various opponent behaviors while focusing on maximizing overall payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.8 else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        cooperate_prob = 0.9 if prev_cooperators >= self.game_description.n_players / 2 else 0.1
        if state.round_number > 1 and sum(history.opponent_payoffs[-2, :]) + history.my_payoffs[-2] < self.game_description.k * self.game_description.n_players / 2:
            cooperate_prob = 0.8
        cooperate_prob += 0.1 * random.random()
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if cooperate_prob > 0.5 and sum(history.opponent_payoffs[-1, :]) + history.my_payoffs[-1] + self.game_description.k < self.game_description.n_players:
            return Action.C
        else:
            return Action.D if random.random() < cooperate_prob else Action.C

class Strategy_COLLECTIVE_285(BaseStrategy):
    """
    Collective Strategy: Adaptive Reciprocity with Public Goods Sensitivity.
    
    This strategy aims to balance cooperation and reciprocity in a public goods game,
    adapting to the collective behavior of other players while prioritizing mutual benefit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (history and len(history.opponent_actions) < self.n_players - 1):
            return Action.C
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        if state.round_number >= self.game_description.n_rounds - 1:
            avg_payoff = np.mean(history.my_payoffs)
            return Action.D if history.my_payoffs[-1] < avg_payoff else Action.C
        total_public_goods_contribution = sum([int(action) for action in history.opponent_actions[-1, :]]) + 1
        if opponent_cooperations > self.n_players / 2 or total_public_goods_contribution > self.k / 2 * self.n_players:
            return Action.C
        prev_payoffs = np.array([payoff for payoff in history.opponent_payoffs[-1, :]])
        defecting_opponents = [i for i, (action, payoff) in enumerate(zip(history.opponent_actions[-1, :], prev_payoffs)) if not action and payoff > np.mean(prev_payoffs[action])]
        if any(defecting_opponents):
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_286(BaseStrategy):
    """
    The Adaptive Collective Optimizer (ACO) strategy adapts to various opponent behaviors 
    and game parameters in the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if state.round_number == self.game_description.n_rounds - 1:
            if self.game_description.k > self.game_description.n_players - 1:
                return Action.C
            else:
                return Action.D
        prev_round_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_round_cooperators >= self.game_description.n_players / 2:
            return Action.C
        else:
            p = self.game_description.k / self.game_description.n_players * (prev_round_cooperators / self.game_description.n_players)
            return Action.D if random.random() < p else Action.C

class Strategy_COLLECTIVE_287(BaseStrategy):
    """
    Collective strategy that promotes cooperation while adapting to the game's dynamics and opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_thresh = 0.5
        self.expected_coops = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        prev_round_opponent_coops = sum(history.opponent_actions[-1, :])
        if prev_round_opponent_coops / self.game_description.n_players >= self.coop_thresh:
            self.coop_thresh -= 0.1
        else:
            self.coop_thresh += 0.1
        self.expected_coops = prev_round_opponent_coops / self.game_description.n_players * self.game_description.k
        if state.round_number < self.game_description.n_rounds - 1:
            if self.expected_coops >= self.coop_thresh:
                return Action.C
            defect_prob = (state.round_number + 1) / self.game_description.n_rounds
            if random.random() < defect_prob:
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_288(BaseStrategy):
    """
    Adaptive Collective Optimism strategy.
    
    This strategy balances individual self-interest with collective welfare, 
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n = self.game_description.n_players
        k = self.game_description.k
        r = self.game_description.n_rounds
        prev_cooperators = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        majority_cooperated = prev_cooperators > n / 2
        if majority_cooperated:
            return Action.C
        else:
            defectors_to_punish = []
            for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
                opponent_payoff = history.opponent_payoffs[-1, i]
                if not opponent_action and opponent_payoff > k / n * prev_cooperators:
                    defectors_to_punish.append(i)
            if len(defectors_to_punish) > 0:
                return Action.D
            else:
                return Action.C
        if state.round_number == r - 1:
            return Action.C

class Strategy_COLLECTIVE_289(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that balances individual self-interest with collective welfare.
    It adapts to the game history and opponent behaviors, aiming to achieve a stable equilibrium where cooperation is sustained.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :])
        my_prev_action = history.my_actions[-1]
        opponent_defected_and_others_cooperated = prev_cooperators > self.game_description.n_players / 2 and (not my_prev_action)
        opponent_cooperated_after_defecting = my_prev_action == False and prev_cooperators >= self.game_description.n_players / 2
        if opponent_defected_and_others_cooperated:
            return Action.D
        elif opponent_cooperated_after_defecting:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :])
        if prev_round_cooperators >= self.game_description.n_players / 2:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_291(BaseStrategy):
    """
    The Adaptive Public Goods strategy balances individual self-interest with collective well-being by adapting to the evolving game dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        elif state.round_number <= 3:
            return random.choice([Action.C, Action.D])
        else:
            cooperators = sum(history.opponent_actions[-1, :])
            avg_cooperator_payoff = self.game_description.k / self.game_description.n_players * cooperators
            avg_defector_payoff = 1 + self.game_description.k / self.game_description.n_players * cooperators
            if avg_cooperator_payoff > avg_defector_payoff:
                return Action.C
            elif state.round_number == self.game_description.n_rounds - 1:
                return Action.D
            else:
                return Action.D

class Strategy_COLLECTIVE_292(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_prob = 1.0
        self.acr_history = []
        self.t = game_description.k / game_description.n_players / (1 + game_description.k / game_description.n_players)
        self.num_uncoop_players = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        acr = np.mean(opponent_actions)
        self.acr_history.append(acr)
        if acr > self.t or np.sum(~opponent_actions) < self.game_description.n_players / 2:
            return Action.C
        self.num_uncoop_players = np.sum(~history.opponent_actions[-1, :])
        self.coop_prob -= 0.2 * (self.num_uncoop_players > 0)
        learning_rate = 0.05
        avg_acr = np.mean(self.acr_history)
        self.coop_prob = max(0, min(self.coop_prob + learning_rate * avg_acr, 1))
        return Action.C if random.random() < self.coop_prob else Action.D

class Strategy_COLLECTIVE_293(BaseStrategy):
    """
    Collective Adaptive Cooperation with Social Learning.
    
    This strategy aims to balance individual self-interest with collective welfare by adapting to the game's history and encouraging cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        prev_defectors = self.game_description.n_players - prev_cooperators
        if prev_cooperators > 0:
            avg_payoff_c = np.mean(np.concatenate((history.opponent_payoffs[-1, :][history.opponent_actions[-1, :] == True], [history.my_payoffs[-1]] if history.my_actions[-1] else [])))
        else:
            avg_payoff_c = float('nan')
        if prev_defectors > 0:
            avg_payoff_d = np.mean(np.concatenate((history.opponent_payoffs[-1, :][history.opponent_actions[-1, :] == False], [history.my_payoffs[-1]] if not history.my_actions[-1] else [])))
        else:
            avg_payoff_d = float('nan')
        if math.isnan(avg_payoff_c) or math.isnan(avg_payoff_d):
            return Action.D
        if avg_payoff_c > avg_payoff_d or prev_cooperators >= self.game_description.n_players / 2:
            action = Action.C
        else:
            action = Action.D
        if state.round_number > 1 and prev_defectors - sum(history.opponent_actions[-2, :] == False) - int(not history.my_actions[-2]) > 0.2 * (sum(history.opponent_actions[-2, :] == False) + int(not history.my_actions[-2])):
            action = Action.D
        if state.round_number == self.game_description.n_rounds:
            if prev_cooperators >= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        return action

class Strategy_COLLECTIVE_294(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Retaliation (ACR).
    
    This strategy prioritizes collective well-being over individual gain, 
    promoting cooperation when it benefits the group. By adapting to the 
    group's behavior, ACR fosters a sense of shared responsibility and 
    encourages others to cooperate as well.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_previous_action = None
        self.previous_round_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.previous_round_cooperators = sum(history.opponent_actions[-1, :])
        if self.my_previous_action == Action.C and history.my_payoffs[-1] < 1 + self.game_description.k / self.game_description.n_players:
            return Action.D
        elif self.my_previous_action == Action.D and self.previous_round_cooperators > self.game_description.n_players // 2:
            return Action.C
        elif self.previous_round_cooperators > self.game_description.n_players // 2:
            return Action.C
        else:
            return Action.D
        self.my_previous_action = history.my_actions[-1] if history is not None else Action.C

class Strategy_COLLECTIVE_295(BaseStrategy):
    """
    Adaptive Cooperative Optimism Strategy.
    
    This strategy balances individual self-interest with the desire for mutual cooperation,
    adapting to the game's history and parameters. It prioritizes cooperation while being cautious of exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.ocr_threshold = 0.5
        self.apc_threshold = 1.2
        self.exploitation_count = 0

    def calculate_payoffs(self, history: PlayerHistory) -> tuple[float, float]:
        recent_rounds = history.my_actions[-3:]
        recent_payoffs = history.my_payoffs[-3:]
        apc = np.mean(recent_payoffs[recent_rounds])
        apd = np.mean(recent_payoffs[~recent_rounds])
        return (apc, apd)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        elif 2 < state.round_number <= self.game_description.n_players:
            return Action.C if state.round_number % 2 == 0 else Action.D
        else:
            opponent_cooperations = np.sum(history.opponent_actions, axis=0) / state.round_number
            ocr = np.mean(opponent_cooperations)
            apc, apd = self.calculate_payoffs(history)
            if ocr >= self.ocr_threshold or apc > apd * self.apc_threshold:
                action = Action.C
            else:
                action = Action.D
            opponent_action = history.opponent_actions[-1, 0]
            if action == Action.C and (not opponent_action):
                self.exploitation_count += 1
                if self.exploitation_count >= 2:
                    return Action.D
        return action

class Strategy_COLLECTIVE_296(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Concession (ACGC) strategy.
    
    This strategy prioritizes collective welfare by initially exploring cooperative behavior, 
    then adapting based on observed opponent actions. The strategy balances individual self-interest 
    with group benefits, encouraging cooperation while minimizing exploitation risks.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions_last_round = history.opponent_actions[-1, :]
        own_action_last_round = history.my_actions[-1]
        total_cooperators = np.sum(opponent_actions_last_round) + (own_action_last_round == 1)
        own_payoff_last_round = history.my_payoffs[-1]
        min_beneficial_cooperators = math.ceil(self.game_description.n_players / 2)
        if total_cooperators >= min_beneficial_cooperators and own_payoff_last_round >= 1 - int(own_action_last_round):
            return Action.C
        elif total_cooperators > math.ceil(self.game_description.n_players / 4):
            cooperation_prob = total_cooperators / self.game_description.n_players * 0.5
            return Action.C if random.random() < cooperation_prob else Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.C if total_cooperators >= min_beneficial_cooperators else Action.D
        return Action.D

class Strategy_COLLECTIVE_297(BaseStrategy):
    """
    Adaptive Cooperative Momentum Strategy.
    
    This strategy aims to balance individual payoffs with collective well-being 
    by adapting to the evolving game dynamics. It leverages historical information 
    to adjust cooperation levels while ensuring robustness against various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_coop = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        opponent_payoffs_coop = [payoff for coop, payoff in zip(history.opponent_actions[-1, :], history.opponent_payoffs[-1, :]) if coop]
        opponent_payoffs_defect = [payoff for coop, payoff in zip(history.opponent_actions[-1, :], history.opponent_payoffs[-1, :]) if not coop]
        avg_payoff_coop = sum(opponent_payoffs_coop) / len(opponent_payoffs_coop) if opponent_payoffs_coop else 0
        avg_payoff_defect = sum(opponent_payoffs_defect) / len(opponent_payoffs_defect) if opponent_payoffs_defect else 0
        payoff_diff = avg_payoff_coop - avg_payoff_defect
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if avg_coop >= 0.5 and payoff_diff > 0:
            return Action.C
        elif self.game_description.k / self.game_description.n_players < payoff_diff:
            return Action.D
        else:
            prob_coop = avg_coop
            return Action.C if random.random() < prob_coop else Action.D

class Strategy_COLLECTIVE_298(BaseStrategy):
    """
    A collective strategy that adapts to the evolving game dynamics and opponent behaviors.
    It balances individual self-interest with the benefits of cooperation, using gradual reciprocity,
    punishment mechanisms, and exploration phases to maximize collective payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_ratio = 0.0
        self.punishment_active = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        if history is not None:
            opponent_cooperations = sum(history.opponent_actions[-1, :])
            self.cooperation_ratio = opponent_cooperations / self.game_description.n_players
        if self.punishment_active:
            self.punishment_active = False
            return Action.D
        if self.cooperation_ratio > self.game_description.k / self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        if state.round_number % 5 == 0 and random.random() < 0.2:
            action = random.choice([Action.C, Action.D])
        if history is not None and self.cooperation_ratio > self.game_description.k / self.game_description.n_players:
            opponent_defections = sum(1 - history.opponent_actions[-1, :])
            if opponent_defections > 0:
                self.punishment_active = True
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if self.game_description.n_players == 2:
            action = Action.C if state.round_number % 2 == 0 else Action.D
        return action

class Strategy_COLLECTIVE_299(BaseStrategy):
    """
    Collective Strategy: Adaptive Tit-for-Tat with Public Goods Foresight.

    This strategy balances individual self-interest with collective cooperation,
    adapting to the evolving game dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        opponent_defected = np.any(np.logical_not(prev_round_opponent_actions))
        if not opponent_defected:
            return Action.C
        p_defect = 1 - self.game_description.k / self.game_description.n_players
        defect_prob = random.random()
        if defect_prob < p_defect:
            return Action.D
        prev_round_cooperation = np.sum(prev_round_opponent_actions) + 1
        estimated_public_good_payoff = self.game_description.k / self.game_description.n_players * prev_round_cooperation
        if estimated_public_good_payoff > 1:
            return Action.C
        else:
            return Action.D

    def last_round_action(self):
        return Action.D

class Strategy_COLLECTIVE_300(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperative Threshold (ACT)

    The ACT strategy aims to balance individual payoff maximization with collective welfare by adapting to the game's history and parameters.
    This approach ensures robustness against various opponent behaviors while promoting cooperation when beneficial.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def calculate_average_payoff_per_cooperator(self, history: PlayerHistory) -> float:
        """
        Calculate the average payoff per cooperator in the previous round.
        
        :param history: The player's history
        :return: The average payoff per cooperator
        """
        cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        APC = cooperators * self.game_description.k / self.game_description.n_players / cooperators if cooperators > 0 else 0
        return APC

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.n_players % 2 == 0 or abs(self.game_description.k - self.game_description.n_players) < 1e-06 else Action.D
        APC = self.calculate_average_payoff_per_cooperator(history)
        CT = self.game_description.k / self.game_description.n_players * (self.game_description.n_players / 2)
        if APC > CT:
            return Action.C
        elif sum([c for c in history.opponent_actions[-1, :]]) + history.my_actions[-1] < self.game_description.n_players / 2:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_301(BaseStrategy):
    """
    Adaptive Cooperative Tit-for-Tat (ACT) Strategy.
    
    This strategy balances individual self-interest with cooperation, adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_opponents = history.opponent_actions[-1, :]
        cooperators = sum(prev_round_opponents)
        cr = cooperators / self.game_description.n_players
        if state.round_number < self.game_description.n_rounds - 2:
            return Action.C if cr > 0.5 else Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_303(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Forgetting (ACGF) Strategy.

    This strategy initially cooperates to encourage cooperation and then adapts based on the weighted average of past cooperation rates.
    It gradually forgets past behaviors, allowing it to adjust to changing opponent strategies while balancing cooperation and self-interest.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialize ACGF strategy with game description.

        :param game_description: Public goods game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.round_weights = []
        self.cooperation_rates = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide action based on current state and history.

        :param state: Current game state (round_number).
        :param history: Past actions and payoffs of all players (my_actions, my_payoffs, opponent_actions, opponent_payoffs).
        :return: Chosen action (Action.C or Action.D).
        """
        if state.round_number == 0:
            return Action.C
        self._update_round_weights(state)
        self._update_cooperation_rates(history)
        avg_coop_rate = self._calculate_avg_coop_rate()
        if avg_coop_rate > 0.6:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :])
        if cooperators >= self.game_description.n_players // 2:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return random.choice([Action.C, Action.D])

    def _update_round_weights(self, state):
        """
        Update round weights to reflect gradual forgetting.

        :param state: Current game state (round_number).
        """
        self.round_weights.append(1 / (state.round_number + 1))

    def _update_cooperation_rates(self, history):
        """
        Update cooperation rates for previous rounds.

        :param history: Past actions and payoffs of all players.
        """
        cooperators = sum(history.opponent_actions[-1, :])
        self.cooperation_rates.append(cooperators / self.game_description.n_players)

    def _calculate_avg_coop_rate(self):
        """
        Calculate weighted average cooperation rate.

        :return: Weighted average cooperation rate.
        """
        return np.average(self.cooperation_rates, weights=self.round_weights)

class Strategy_COLLECTIVE_304(BaseStrategy):
    """
    A strategy that adapts to the behavior of other players, balancing individual self-interest with collective well-being.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.actions_history = []
        self.total_payoffs = []
        self.num_cooperators = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        num_cooperators_round = sum(history.opponent_actions[-1, :])
        proportion_cooperating = num_cooperators_round / self.game_description.n_players
        prev_total_payoff = np.sum(history.my_payoffs)
        avg_individual_payoff = prev_total_payoff / state.round_number
        p = 1 - self.game_description.k / self.game_description.n_players
        if proportion_cooperating > 2 / 3:
            p_adjusted = max(p, 0.5)
            return Action.D if random.random() < 1 - p_adjusted else Action.C
        if prev_total_payoff >= avg_individual_payoff:
            action = Action.C
        else:
            action = Action.D if random.random() < p else Action.C
        self.actions_history.append(action)
        self.total_payoffs.append(np.sum(history.my_payoffs))
        self.num_cooperators.append(num_cooperators_round)
        return action

class Strategy_COLLECTIVE_305(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and aligns with the collective mindset.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5
        self.total_payoff_prev = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_cooperators >= self.theta * self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        if self.total_payoff_prev is not None and state.round_number > 1:
            total_payoff_current = sum(history.my_payoffs)
            if total_payoff_current > self.total_payoff_prev:
                self.theta -= 0.05
            elif total_payoff_current < self.total_payoff_prev:
                self.theta += 0.05
        self.total_payoff_prev = sum(history.my_payoffs)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_306(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Punishment (ACP)
    
    This strategy promotes cooperation by initially cooperating and then adapting to the previous round's outcome.
    It introduces a punishment mechanism for non-cooperative behavior, deterring opponents from defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_payoff = sum(history.my_payoffs[-1] + history.opponent_payoffs[-1])
        if prev_payoff >= self.k:
            return Action.C
        p_defect = (self.k - prev_payoff) / self.k
        if random.random() < p_defect:
            return Action.D
        else:
            return Action.C

    def _calculate_payoff(self, action: bool | Action, n_players: int, k: float) -> float:
        """
        Helper function to calculate payoff for a single round.

        :param action: This player's action (True/1 for COOPERATE, False/0 for DEFECT)
        :param n_players: Number of players in the game
        :param k: Multiplication factor
        :return: Payoff for this player
        """
        if isinstance(action, Action):
            c_i = int(action == Action.C)
        else:
            c_i = int(action)
        total_cooperators = sum(history.opponent_actions[-1, :] + [c_i])
        payoff = 1 - c_i + k / n_players * total_cooperators
        return payoff

class Strategy_COLLECTIVE_307(BaseStrategy):
    """
    Adaptive Cooperative Threshold (ACT) strategy for the N-Player Public Goods Game.
    
    This strategy balances individual self-interest with collective well-being, 
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_defections = 0
        self.total_rounds = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.total_defections += sum([1 for player in history.opponent_actions[-1, :] if not player])
            self.total_rounds += 1
            threshold = self.game_description.k / self.game_description.n_players * (1 - self.total_defections / self.total_rounds)
            total_contributions = sum(history.opponent_actions[-1, :]) + 1 if history.my_actions[-1] else sum(history.opponent_actions[-1, :])
            if total_contributions > threshold:
                return Action.C
            elif total_contributions == threshold:
                majority_cooperated = sum([1 for player in history.opponent_actions[-2, :] if player]) >= self.game_description.n_players // 2
                return Action.C if majority_cooperated else Action.D
            else:
                return Action.D
        return Action.C

class Strategy_COLLECTIVE_308(BaseStrategy):
    """
    A collective strategy that adapts to various opponent behaviors while promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.8 else Action.D
        avg_payoff_prev = np.mean(np.concatenate(([history.my_payoffs[-1]], history.opponent_payoffs[-1, :])))
        avg_contributors_prev = sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        self.coop_history.append(sum(history.opponent_actions[-1, :]) / len(history.opponent_actions[-1, :]))
        if avg_payoff_prev >= 1:
            action = Action.C if random.random() < 0.8 else Action.D
        elif avg_contributors_prev >= self.game_description.n_players // 2:
            action = Action.C if random.random() < 0.8 else Action.D
        elif sum(self.coop_history) / len(self.coop_history) >= 0.5:
            action = Action.C if random.random() < 0.8 else Action.D
        else:
            action = Action.D if random.random() < 0.6 else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            total_payoff_C = np.sum(history.my_payoffs) + self.game_description.k / self.game_description.n_players * sum(self.coop_history)
            total_payoff_D = np.sum(history.my_payoffs) + self.game_description.k / self.game_description.n_players * (sum(self.coop_history) - 1)
            if total_payoff_C > total_payoff_D:
                action = Action.C
        return action

class Strategy_COLLECTIVE_309(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective well-being by adapting to the game's history and opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        if history is not None:
            self.total_cooperators_prev = sum(history.opponent_actions[-1, :]) + 1 if history.my_actions[-1] else sum(history.opponent_actions[-1, :])
        previous_payoff = self.game_description.k / self.game_description.n_players * self.total_cooperators_prev
        if state.round_number > 1:
            previous_payoff += 1 - history.my_actions[-1]
        CT = self.game_description.k / self.game_description.n_players * (self.total_cooperators_prev / self.game_description.n_players)
        if previous_payoff <= 1:
            return Action.D
        elif CT >= 0.5 and previous_payoff > 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_310(BaseStrategy):
    """
    Adaptive Cooperation Strategy for N-Player Public Goods Game.
    
    This strategy balances individual self-interest with collective benefits, 
    while adapting to various opponent behaviors. It prioritizes cooperation 
    when it is likely to be reciprocated and adapts to exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif prev_cooperators == self.game_description.n_players // 2:
            return Action.C
        opponent_defects_consistently = np.all(history.opponent_actions[:, 0] == False)
        if opponent_defects_consistently:
            p = max(self.game_description.k / self.game_description.n_players, 0.5)
        else:
            p = self.game_description.k / self.game_description.n_players
        if prev_cooperators >= self.game_description.n_players // 2:
            return Action.C
        elif random.random() < p:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_311(BaseStrategy):
    """
    An adaptive cooperation strategy that balances individual payoff maximization with collective well-being.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        total_cooperators = sum(history.my_actions[-1:]) + np.sum(history.opponent_actions[-1, :])
        theta = total_cooperators / self.game_description.n_players
        if theta >= self.game_description.k / self.game_description.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_312(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Gradual Retribution.

    This strategy balances individual self-interest with collective well-being by adapting to the game's history and gradually punishing defectors while encouraging cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defect_count = 0
        self.prev_avg_payoff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_payoffs = history.opponent_payoffs[-1, :]
        avg_payoff_last_round = np.mean(opponent_payoffs)
        if self.prev_avg_payoff is not None:
            avg_payoff_last_round = (self.prev_avg_payoff * (self.game_description.n_players - 1) + avg_payoff_last_round) / self.game_description.n_players
        self.prev_avg_payoff = avg_payoff_last_round
        if avg_payoff_last_round > self.game_description.k / self.game_description.n_players:
            self.defect_count = 0
            return Action.C
        elif avg_payoff_last_round <= 1 - self.game_description.k / self.game_description.n_players:
            p_defect = min(1, self.defect_count / (self.game_description.n_rounds - state.round_number))
            if random.random() < p_defect:
                self.defect_count += 1
                return Action.D
            else:
                return Action.C
        else:
            return Action.C
        if avg_payoff_last_round <= 1 - self.game_description.k / self.game_description.n_players:
            self.defect_count += 1
        elif avg_payoff_last_round > self.game_description.k / self.game_description.n_players:
            self.defect_count = 0

class Strategy_COLLECTIVE_313(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_c = 0.5
        self.total_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_c else Action.D
        last_round_payoff = history.my_payoffs[-1]
        opponent_actions_last_round = history.opponent_actions[-1, :]
        total_contributions_last_round = sum(opponent_actions_last_round) + 1
        avg_historical_payoff = np.mean(history.my_payoffs[:-1]) if len(history.my_payoffs) > 1 else 0
        if last_round_payoff >= avg_historical_payoff:
            return Action.C
        else:
            p_d = 0.5 + self.game_description.k / self.game_description.n_players * (total_contributions_last_round / self.game_description.n_players)
            return Action.D if random.random() < p_d else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            expected_total_payoff = last_round_payoff + self.game_description.k / self.game_description.n_players * total_contributions_last_round
            return Action.C if expected_total_payoff > self.total_payoff else Action.D
        self.total_payoff += last_round_payoff

class Strategy_COLLECTIVE_314(BaseStrategy):
    """
    The ACT strategy promotes collective well-being while adapting to diverse opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if len(history.my_actions) > 0 and history.my_payoffs[-1] < 1 - self.k / self.n:
            return Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        if opponent_cooperators >= self.n // 2:
            return Action.C
        elif opponent_cooperators < self.n / self.k:
            return Action.D
        defector_count = np.count_nonzero(np.all(~history.opponent_actions[:, :], axis=0))
        for i in range(len(history.my_payoffs)):
            if history.my_actions[i] == 0 and len([j for j in range(i, len(history.my_payoffs)) if history.my_actions[j] == 0]) >= self.k:
                return Action.C
        return random.choice([Action.C, Action.D])

class Strategy_COLLECTIVE_315(BaseStrategy):
    """
    This strategy promotes collective well-being in the N-Player Public Goods Game.
    
    It adapts to opponents' behavior by reciprocating cooperation, defecting when necessary,
    and occasionally exploring cooperation to escape suboptimal equilibria.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5
        self.exploration_probability = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        if history is not None and len(history.my_actions) > 0:
            p_t_1 = np.sum(history.opponent_actions[-1, :] + [history.my_actions[-1]]) / self.game_description.n_players
            if p_t_1 > self.threshold:
                return Action.C
            if p_t_1 == 0:
                return Action.C if random.random() < 0.5 else Action.D
            if random.random() < self.exploration_probability:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_316(BaseStrategy):
    """
    An adaptive collective optimizer strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = (game_description.k + 1) / 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :])
        previous_defectors = self.game_description.n_players - 1 - previous_cooperators
        diff = previous_cooperators - previous_defectors
        if diff > self.theta:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1 and previous_cooperators > 0:
            return Action.C

class Strategy_COLLECTIVE_317(BaseStrategy):
    """
    Adaptive Collective Strategy with Self-Regulation (ACSR) in the N-Player Public Goods Game.
    
    ACSR promotes collective cooperation while adapting to diverse opponent behaviors. 
    It introduces self-regulation and responds to opponents' actions, balancing individual interests with group benefits.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5
        self.P_max = 3
        self.P = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_coop_rate = np.mean(history.opponent_actions[-1, :])
        if self.P >= self.P_max:
            action = Action.D
            self.P -= 1
        elif avg_coop_rate > self.theta:
            action = Action.C
            self.P += 1
        else:
            action = Action.D
        opponent_defection_rates = np.mean(history.opponent_actions, axis=0)
        if np.any(opponent_defection_rates > 0.5):
            self.theta -= 0.1
        return action

class Strategy_COLLECTIVE_318(BaseStrategy):
    """
    An adaptive strategy for N-Player Public Goods Game that encourages cooperation 
    by adapting to the game's history and promoting collective payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5
        self.avg_cooperation_rate = None
        self.avg_payoff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C] * 8 + [Action.D])
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        self.avg_cooperation_rate = (opponent_cooperations + int(history.my_actions[-1])) / self.game_description.n_players
        if self.avg_cooperation_rate > self.threshold:
            action = Action.C
        else:
            action_probability = (self.threshold - self.avg_cooperation_rate) / (1 - self.threshold)
            action = random.choices([Action.C, Action.D], weights=[action_probability, 1 - action_probability])[0]
        if random.random() < 0.1:
            action = Action.C if action == Action.D else Action.D
        my_payoff = self.game_description.k / self.game_description.n_players * sum(history.opponent_actions[-1, :]) + int(not history.my_actions[-1])
        opponent_payoffs = np.sum(self.game_description.k / self.game_description.n_players * sum(history.opponent_actions[-1, :]) + (1 - history.opponent_actions[-1, :]), axis=0)
        avg_payoff_this_round = np.mean(np.append(my_payoff, opponent_payoffs))
        if self.avg_payoff is None:
            self.avg_payoff = avg_payoff_this_round
        else:
            self.avg_payoff = (self.avg_payoff * (state.round_number - 1) + avg_payoff_this_round) / state.round_number
        if avg_payoff_this_round > self.avg_payoff:
            self.threshold -= 0.05
        else:
            self.threshold += 0.05
        return action

class Strategy_COLLECTIVE_319(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Retaliation strategy.
    
    This collective strategy aims to balance individual self-interest 
    with cooperative behavior, adapting to the game's history and opponent actions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.retaliation_factor = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        self.cooperation_threshold = (self.cooperation_threshold + previous_round_cooperators / self.game_description.n_players) / 2
        self.retaliation_factor = (self.cooperation_threshold - previous_round_cooperators / self.game_description.n_players) / self.cooperation_threshold
        if self.retaliation_factor > 0:
            p_D = self.retaliation_factor
        else:
            p_D = 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.C if random.random() > p_D else Action.D

class Strategy_COLLECTIVE_320(BaseStrategy):
    """
    Adaptive Cooperation with Social Learning strategy.
    Balances individual self-interest with collective welfare in N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_payoff = None
        self.avg_defect_payoff = None
        self.coop_threshold = None
        self.most_successful_player = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.k >= 2 else Action.D
        if self.coop_threshold is None:
            if self.game_description.k > 2.5:
                self.coop_threshold = 0.6
            elif self.game_description.k <= 2.5 and self.game_description.k > 1:
                self.coop_threshold = 0.4
            else:
                self.coop_threshold = 0.2
        if history is not None:
            num_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
            num_defectors = self.game_description.n_players - num_cooperators
            self.avg_coop_payoff = np.mean([p for i, p in enumerate(np.concatenate(([history.my_payoffs[-1]], history.opponent_payoffs[-1, :]))) if i < len(history.opponent_actions[-1]) and history.opponent_actions[-1][i] == Action.C or (i >= len(history.opponent_actions[-1]) and history.my_actions[-1] == Action.C)])
            self.avg_defect_payoff = np.mean([p for i, p in enumerate(np.concatenate(([history.my_payoffs[-1]], history.opponent_payoffs[-1, :]))) if i < len(history.opponent_actions[-1]) and history.opponent_actions[-1][i] == Action.D or (i >= len(history.opponent_actions[-1]) and history.my_actions[-1] == Action.D)])
        else:
            self.avg_coop_payoff = 0
            self.avg_defect_payoff = 0
        if self.avg_coop_payoff is not None and self.coop_threshold is not None:
            return Action.C if self.avg_coop_payoff >= self.coop_threshold else Action.D
        num_cooperators_last_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif num_cooperators_last_round / self.game_description.n_players < 0.2:
            return Action.D
        elif num_cooperators_last_round / self.game_description.n_players > 0.8 and history.my_actions[-1] == Action.C:
            return Action.C
        if self.most_successful_player is not None:
            return history.opponent_actions[-1, self.most_successful_player]
        else:
            payoffs = np.concatenate(([history.my_payoffs[-1]], history.opponent_payoffs[-1, :]))
            actions = np.concatenate(([history.my_actions[-1]], history.opponent_actions[-1, :]))
            max_payoff_index = np.argmax(payoffs)
            self.most_successful_player = max_payoff_index
            return actions[max_payoff_index]

class Strategy_COLLECTIVE_321(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Escalation (ACE) strategy.

    This strategy encourages cooperation while being robust against various opponent behaviors.
    It cooperates initially, then adapts based on the number of cooperators in previous rounds,
    introducing a gradual escalation mechanism to respond to repeated defections.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.c_prev = 0
        self.defect_count = 0
        self.escalation_threshold = math.ceil(game_description.n_players / 4)
        self.escalation_period = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.c_prev = sum(history.opponent_actions[-1, :])
        if self.defect_count >= self.escalation_threshold:
            self.defect_count -= 1
            return Action.D
        if self.c_prev >= math.ceil(self.game_description.n_players / 2):
            self.defect_count = 0
            return Action.C
        else:
            self.defect_count += 1
            return Action.D

class Strategy_COLLECTIVE_322(BaseStrategy):
    """
    Adaptive Cooperativity strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :]) + 1
        coop_threshold = self.game_description.n_players / 2
        if prev_round_coops > coop_threshold:
            return Action.C
        elif prev_round_coops == coop_threshold:
            return Action.D if random.random() < 0.5 else Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_324(BaseStrategy):
    """
    Collective Strategy: Adaptive Reciprocal Altruism (ARA)
    
    Balances individual self-interest with group cooperation, adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.self_protection_triggered = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        previous_round_actions = history.opponent_actions[-1, :] if history else []
        my_previous_action = history.my_actions[-1] if history and len(history.my_actions) > 0 else False
        if state.round_number == self.game_description.n_rounds - 1:
            last_round_cooperation = sum(history.opponent_actions[-2, :]) >= self.game_description.n_players / 2
            return Action.C if last_round_cooperation and self.game_description.k / self.game_description.n_players > 0.5 else Action.D
        majority_cooperated = sum(previous_round_actions) + my_previous_action >= self.game_description.n_players / 2
        if majority_cooperated:
            return Action.C
        opponent_defectors = np.logical_and(np.logical_not(previous_round_actions), np.sum(history.opponent_actions, axis=0) > 1)
        altruistic_punishment_trigger = sum(opponent_defectors) > 0 and majority_cooperated
        if altruistic_punishment_trigger:
            return Action.D
        self_protection_threshold = max(2, math.ceil(self.game_description.n_rounds / 4))
        consistent_exploitation = np.sum(history.my_payoffs[-self_protection_threshold:]) < self_protection_threshold * 0.5
        if consistent_exploitation and (not self.self_protection_triggered):
            self.self_protection_triggered = True
            return Action.D
        if self.self_protection_triggered and state.round_number % 2 == 0:
            self.self_protection_triggered = False
        return Action.C

class Strategy_COLLECTIVE_325(BaseStrategy):
    """
    A collective strategy that promotes cooperation by adapting to the group's behavior and punishing defections.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punished = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        average_contribution_rate = self.calculate_average_contribution_rate(history)
        theta = (self.game_description.n_players + self.game_description.k) / (2 * self.game_description.n_players)
        if average_contribution_rate > theta:
            self.punished = False
            return Action.C
        elif average_contribution_rate < theta and self.punished_player(history):
            theta_p = theta - self.game_description.k / self.game_description.n_players
            self.punished = True
            return Action.D if random.random() < 0.5 else Action.C
        else:
            self.punished = False
            return Action.D

    def calculate_average_contribution_rate(self, history: PlayerHistory) -> float:
        total_contributions = sum(history.my_actions[-1:]) + sum(history.opponent_actions[-1, :])
        average_contribution_rate = total_contributions / self.game_description.n_players
        return average_contribution_rate

    def punished_player(self, history: PlayerHistory) -> bool:
        if not history.my_actions[-1] and any(history.opponent_actions[-1, :]):
            return True
        else:
            return False

class Strategy_COLLECTIVE_326(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Conditional Defection.

    This strategy balances individual self-interest with the collective benefit of cooperation.
    It adapts to the game's history and opponent behaviors, incentivizing continued cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_total_c = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        opponent_coop_rate = np.mean(history.opponent_actions[-1, :])
        if state.round_number == self.n_rounds - 1:
            return Action.C
        elif prev_total_c >= self.k:
            return Action.C
        elif prev_total_c < self.k and opponent_coop_rate > 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_327(BaseStrategy):
    """
    The Adaptive Cooperation strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        opponent_actions = history.opponent_actions[-1]
        num_coop = sum(opponent_actions)
        coop_rate = num_coop / self.game_description.n_players
        if coop_rate >= self.game_description.k / self.game_description.n_players:
            return Action.C
        elif coop_rate == self.game_description.k / self.game_description.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_328(BaseStrategy):
    """
    The Adaptive Cooperative Threshold strategy adapts to various opponent behaviors 
    and maximizes overall payoff by maintaining a dynamic cooperation threshold.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        c_total = sum([sum(history.opponent_actions[i, :]) + int(history.my_actions[i]) for i in range(state.round_number)])
        d_total = state.round_number * self.game_description.n_players - c_total
        theta = self.game_description.k / self.game_description.n_players * (1 - d_total / state.round_number)
        if sum(history.opponent_actions[-1, :] + [int(history.my_actions[-1])]) >= theta:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_329(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective well-being by adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.grudge_threshold = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1]
        my_payoff = history.my_payoffs[-1]
        avg_payoff_C, avg_payoff_D = self.calculate_average_payoffs(history)
        defect_streaks = self.update_defect_streaks(history.opponent_actions)
        grudged_opponents = [i for i, streak in enumerate(defect_streaks) if streak > self.grudge_threshold]
        if avg_payoff_C >= avg_payoff_D:
            action = Action.C
        else:
            action = Action.D
        if any((opponent_actions[i] for i in grudged_opponents)):
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

    def calculate_average_payoffs(self, history: PlayerHistory) -> tuple[float, float]:
        """Calculate average payoffs of cooperators and defectors in the previous round."""
        opponent_actions = history.opponent_actions[-1]
        opponent_payoffs = history.opponent_payoffs[-1]
        avg_payoff_C = np.mean(opponent_payoffs[opponent_actions == True])
        avg_payoff_D = np.mean(opponent_payoffs[opponent_actions == False])
        return (avg_payoff_C, avg_payoff_D)

    def update_defect_streaks(self, opponent_actions: NDArray[np.bool_]) -> list[int]:
        """Update the defect streaks for each opponent."""
        defect_streaks = [0] * self.game_description.n_players
        for i in range(opponent_actions.shape[1]):
            if not opponent_actions[-1, i]:
                defect_streaks[i] += 1
            else:
                defect_streaks[i] = 0
        return defect_streaks

class Strategy_COLLECTIVE_330(BaseStrategy):
    """
    Collective Strategy: "Adaptive Cooperation with Retaliation" (ACR).
    
    This strategy aims to balance individual interests with collective well-being.
    It cooperates initially, adapts to the group's behavior, and penalizes free-riding.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history.my_actions.size > 0 else 0)
        coop_rate_prev_round = prev_round_coops / self.game_description.n_players
        if history.opponent_actions.shape[0] > 1 and (not all(history.my_actions[-2:])):
            return Action.D
        if coop_rate_prev_round >= self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_331(BaseStrategy):
    """
    Adaptive Cooperation with Retaliation strategy.
    
    This strategy adapts to the game's history by cooperating initially, retaliating against non-cooperation,
    and switching between cooperation and defection based on payoffs. It also tests for potential cooperation
    after two rounds of defection and maximizes collective payoffs in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if previous_cooperators <= self.game_description.n_players / 2:
            return Action.D
        my_payoff_C = history.my_payoffs[-1] if history.my_actions[-1] else 0
        opponent_payoffs_C = [payoff for payoff, action in zip(history.opponent_payoffs[-1], history.opponent_actions[-1]) if action]
        average_payoff_C = sum(opponent_payoffs_C) / len(opponent_payoffs_C) if opponent_payoffs_C else my_payoff_C
        my_payoff_D = 0 if history.my_actions[-1] else history.my_payoffs[-1]
        opponent_payoffs_D = [payoff for payoff, action in zip(history.opponent_payoffs[-1], history.opponent_actions[-1]) if not action]
        average_payoff_D = sum(opponent_payoffs_D) / len(opponent_payoffs_D) if opponent_payoffs_D else my_payoff_D
        if average_payoff_C > average_payoff_D:
            return Action.C
        elif history.my_actions[-1] == False and (state.round_number >= 2 and history.my_actions[-2] == False):
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_332(BaseStrategy):
    """
    Collective Optimism with Adaptive Forgiveness Strategy.
    
    This strategy initially cooperates to encourage others, then reciprocates cooperation 
    based on a proportion of players who cooperated in the previous round. It also 
    includes an adaptive forgiveness component to avoid getting stuck in a cycle of 
    defection and allows for some exploration and potential re-cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        theta = (self.k - 1) / self.n_players
        phi = theta * 0.5
        epsilon = 0.05
        if history.my_actions is not None and len(history.my_actions) > state.round_number:
            my_prev_action = history.my_actions[state.round_number - 1]
        else:
            my_prev_action = False
        if history.opponent_actions is not None and len(history.opponent_actions) > state.round_number:
            prev_cooperators = sum(history.opponent_actions[state.round_number - 1, :])
        else:
            prev_cooperators = 0
        if prev_cooperators >= theta * self.n_players or (prev_cooperators >= phi * self.n_players and random.random() < epsilon):
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.n_players == 2:
            if my_prev_action:
                return history.opponent_actions[state.round_number - 1, 0] and Action.C or Action.D
            else:
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_333(BaseStrategy):
    """
    A collective strategy designed to balance individual self-interest with collective well-being,
    adapting to various opponent behaviors while maintaining a robust and cooperative approach.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators_prev_round = 0
        self.prev_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions_prev_round = history.opponent_actions[-1]
        total_cooperators_prev_round = np.sum(opponent_actions_prev_round)
        self.total_cooperators_prev_round = (self.total_cooperators_prev_round * ((state.round_number - 1) // 2) + total_cooperators_prev_round) / (state.round_number // 2)
        if self.total_cooperators_prev_round > self.game_description.n_players / 2:
            action = Action.C
        else:
            p = 1 - self.game_description.k / self.game_description.n_players * (self.total_cooperators_prev_round / self.game_description.n_players)
            action = Action.D if random.random() < p else Action.C
        current_payoff = history.my_payoffs[-1]
        delta = 0.05
        if current_payoff < self.game_description.k and state.round_number > 1:
            p_prime = min(1, max(0, p + delta))
            action = Action.D if random.random() < p_prime else action
        self.prev_payoff = current_payoff
        return action

class Strategy_COLLECTIVE_334(BaseStrategy):
    """
    The Adaptive Collective Cooperation strategy promotes collective cooperation by encouraging initial cooperation, 
    reciprocating high cooperation rates from previous rounds, punishing low cooperation rates, and adapting to changing conditions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.0
        self.average_payoff_over_past_rounds = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.cooperation_probability = self.game_description.k / self.game_description.n_players
            return Action.C if random.random() < self.cooperation_probability else Action.D
        c_prev = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if c_prev >= self.game_description.k / self.game_description.n_players:
            self.cooperation_probability = 1 - (1 - c_prev) ** 2
        else:
            self.cooperation_probability = (1 - c_prev) ** 2
        action = Action.C if random.random() < self.cooperation_probability else Action.D
        if history is not None:
            payoff = history.my_payoffs[-1]
            self.average_payoff_over_past_rounds = (self.average_payoff_over_past_rounds * state.round_number + payoff) / (state.round_number + 1)
            if payoff > self.average_payoff_over_past_rounds:
                self.cooperation_probability += 0.05
            else:
                self.cooperation_probability -= 0.05
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_COLLECTIVE_335(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with group cooperation,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_contribution_threshold = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :] != False)
        if prev_round_cooperators > self.avg_contribution_threshold:
            return Action.C
        opponent_payoff = history.opponent_payoffs[-1, 0]
        if opponent_payoff > 1 + self.game_description.k / 2:
            return Action.D
        prev_round_defectors = sum(history.opponent_actions[-1, :] == False)
        if prev_round_defectors > 0 and opponent_payoff > self.game_description.k / self.game_description.n_players * prev_round_cooperators:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_336(BaseStrategy):
    """
    Collective Adaptive Cooperation strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.CR = 0.5
        self.TR = math.floor(game_description.n_rounds / 2)
        self.TFT_F_History = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            opponent_cooperators = sum(history.opponent_actions[-1, :])
            self.CR = opponent_cooperators / self.game_description.n_players
        if self.CR > 0.7 and self.TR > 1:
            self.TR -= 1
        elif self.CR < 0.3 and self.TR < self.game_description.n_rounds - 1:
            self.TR += 1
        if self.CR >= 0.5:
            return Action.C
        elif state.round_number >= self.TR:
            return Action.D
        else:
            if history is not None:
                self.TFT_F_History.append(history.opponent_actions[-1, :])
            action = self.tit_for_tat_with_forgiveness(self.TFT_F_History)
            return Action.C if action else Action.D

    def tit_for_tat_with_forgiveness(self, opponent_history):
        if len(opponent_history) < 1:
            return True
        last_round_cooperation = sum(opponent_history[-1])
        cooperation_rate = np.mean(np.sum(opponent_history, axis=0))
        return last_round_cooperation > 0 or cooperation_rate > 0.5

class Strategy_COLLECTIVE_337(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperative Escalation (ACE)

    This strategy balances individual self-interest with collective well-being.
    It initially cooperates and adapts to the group's behavior, encouraging mutual cooperation while minimizing exploitation by defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.n_players / (self.game_description.n_players + 1) else Action.D
        opponents_cooperated = sum(history.opponent_actions[-1, :])
        majority_cooperated = opponents_cooperated >= self.game_description.n_players / 2
        if majority_cooperated:
            action = Action.C
        else:
            action = Action.C if random.random() < self.game_description.k / self.game_description.n_players else Action.D
        any_defected = not all(history.opponent_actions[-1, :])
        if any_defected:
            self.cooperation_threshold += 1
        elif opponents_cooperated == self.game_description.n_players - 1:
            self.cooperation_threshold -= 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_COLLECTIVE_338(BaseStrategy):
    """
    A collective strategy called "Adaptive Collective Optimism" (ACO) for the N-Player Public Goods Game.
    It balances individual self-interest with collective well-being by adapting to the game's history and opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        min_rounds_for_acr = min(state.round_number // 2, 5)
        acr = sum(history.my_actions[-min_rounds_for_acr:]) / min_rounds_for_acr
        avg_payoff = np.mean(history.my_payoffs[-min_rounds_for_acr:])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if acr >= 0.5 else Action.D
        if avg_payoff < self.game_description.k / self.game_description.n_players * (self.game_description.n_players - 1):
            return Action.C if acr >= 0.7 else Action.D
        else:
            return Action.C if acr >= 0.6 else Action.D

class Strategy_COLLECTIVE_339(BaseStrategy):
    """
    Adaptive Collective Optimizer (ACO) strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        elif 2 < state.round_number <= 5:
            total_cooperators = sum(history.opponent_actions[-1, :] | [True]) - 1
            if total_cooperators > self.n_players / 2:
                return Action.C
            else:
                p_defect = (self.k - 1) / (self.n_players - 1)
                return Action.D if random.random() < p_defect else Action.C
        elif state.round_number == self.game_description.n_rounds:
            return Action.C
        else:
            opponent_actions = history.opponent_actions[-1, :]
            my_action = history.my_actions[-1]
            if not my_action:
                avg_payoff_C = 0
            elif sum(opponent_actions | [True]) == self.n_players:
                avg_payoff_C = self.k / self.n_players * self.n_players
            else:
                opponent_cooperators = sum(opponent_actions)
                avg_payoff_C = 1 - my_action + self.k / self.n_players * (opponent_cooperators + 1) if not my_action else 0
            if avg_payoff_C >= self.k / self.n_players:
                return Action.C
            else:
                p_defect = 1 - avg_payoff_C / (self.k / self.n_players)
                return Action.D if random.random() < p_defect else Action.C
        total_cooperators = sum(history.opponent_actions[-1, :] | [True]) - 1
        if total_cooperators < self.n_players / 4:
            p_defect = 0.5
            return Action.D if random.random() < p_defect else Action.C

class Strategy_COLLECTIVE_340(BaseStrategy):
    """
    The Adaptive Cooperation (AC) strategy adapts its cooperation level based on the previous round's outcomes.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_round_actions = None

    def calculate_cooperation_threshold(self, avg_coop_rate, k, n):
        if avg_coop_rate > 0.5:
            return min(k / n * (1 + avg_coop_rate), 1)
        else:
            return max(k / n * (1 - avg_coop_rate), 0)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = np.concatenate([history.opponent_actions[-1], [True]])
        avg_coop_rate = np.mean(prev_round_actions)
        coop_threshold = self.calculate_cooperation_threshold(avg_coop_rate, self.game_description.k, self.game_description.n_players)
        return Action.C if coop_threshold >= random.random() else Action.D

class Strategy_COLLECTIVE_341(BaseStrategy):
    """
    This strategy aims to balance individual payoff maximization with collective welfare,
    adapting to changing opponent behaviors and game history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        n_cooperators = sum(opponent_actions)
        if state.round_number > 1:
            prev_round_action = history.my_actions[state.round_number - 2]
            majority_cooperated = n_cooperators >= self.game_description.n_players / 2
            if not prev_round_action and majority_cooperated:
                return Action.D
        if state.round_number > 2:
            prev_round_opponent_actions = history.opponent_actions[-2, :]
            prev_n_cooperators = sum(prev_round_opponent_actions)
            prev_majority_cooperated = prev_n_cooperators >= self.game_description.n_players / 2
            if not history.my_actions[state.round_number - 3] and prev_majority_cooperated:
                return Action.C
        majority_cooperated = n_cooperators > self.game_description.n_players / 2
        if majority_cooperated:
            return Action.C
        elif n_cooperators == self.game_description.n_players // 2:
            return Action.D
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_342(BaseStrategy):
    """
    This strategy aims to balance individual payoffs with collective well-being by adapting to the game's history and promoting cooperation through social norms.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.c_rate = 0.0
        self.prev_payoff = 0.0
        self.avg_payoff = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number <= self.game_description.n_rounds // 2:
            c_t = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            d_t = self.game_description.n_players - c_t
            self.c_rate = (self.c_rate * (state.round_number - 1) + c_t) / state.round_number
        elif self.c_rate >= self.game_description.k / self.game_description.n_players:
            return Action.C
        elif self.prev_payoff > self.avg_payoff and state.round_number > self.game_description.n_rounds // 2:
            return Action.C
        else:
            return Action.D
        opponent_defected = history.opponent_actions[-1, :].any() and (not history.my_actions[-1])
        if opponent_defected:
            return Action.D
        self.prev_payoff = (history.my_payoffs[-1] - self.prev_payoff * (state.round_number - 2)) / (state.round_number - 1) if state.round_number > 1 else history.my_payoffs[-1]
        self.avg_payoff = np.mean(history.my_payoffs[max(-self.game_description.n_rounds // 4, -len(history.my_payoffs)):])
        return Action.C

class Strategy_COLLECTIVE_343(BaseStrategy):
    """
    This strategy adapts to various opponent behaviors and aligns with the collective mindset.
    It encourages initial cooperation, tracks contribution threshold, responds to exploitation,
    and prioritizes collective well-being.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.C_payoff = []
        self.D_payoff = []
        self.exploit_count = 0
        self.last_round_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        if state.round_number == 2 and len(self.C_payoff) == 0:
            self.C_payoff = [0] * (state.round_number + 1)
            self.D_payoff = [0] * (state.round_number + 1)
        if history is not None and state.round_number > 2:
            opponent_cooperation = sum(history.opponent_actions[-1, :])
            my_action = Action.C if history.my_actions[-1] else Action.D
            if my_action == Action.C and self.last_round_action == Action.D or (self.exploit_count > 0 and my_action == Action.C):
                opponent_payoff_diff = np.mean(history.opponent_payoffs[-2:, :]) - history.my_payoffs[-2]
                if opponent_payoff_diff > 0:
                    self.exploit_count += 1
            self.last_round_action = my_action
        if len(self.C_payoff) > 0 and sum(self.C_payoff) >= self.game_description.n_players / 2:
            return Action.C
        if len(self.D_payoff) > 1 and sum(self.C_payoff[-3:]) < sum(self.D_payoff[-3:]):
            return Action.D
        if self.exploit_count >= 3:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_344(BaseStrategy):
    """
    Adaptive Cooperation strategy for the N-Player Public Goods Game.
    
    This strategy prioritizes collective well-being by encouraging cooperation through reciprocal behavior,
    punishing exploitation and free-riding, and adapting to changing circumstances using an adaptive threshold.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_round_actions = []
        self.avg_coop_rate = 1.0
        self.t = 3
        self.theta = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        coop_count = sum((1 for j in self.prev_round_actions[-self.t:] if j))
        self.avg_coop_rate = coop_count / self.t
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        majority_cooperated = opponent_cooperators >= self.game_description.n_players / 2
        if self.avg_coop_rate >= self.theta and majority_cooperated:
            action = Action.C
        elif not history.my_payoffs[-1] and majority_cooperated:
            action = Action.D
        else:
            action = Action.D
        self.prev_round_actions.append(action == Action.C)
        return action

class Strategy_COLLECTIVE_345(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for N-Player Public Goods Game.
    
    This strategy prioritizes collective cooperation by initially cooperating and then adapting based on the game's history.
    It calculates average payoffs for cooperators and defectors, adjusts an adaptive threshold, and uses a punishment mechanism to address non-cooperative behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tau = 0.5

    def calculate_average_payoff(self, history: PlayerHistory, action: Action) -> float:
        """Calculate average payoff for a given action (cooperate or defect) in the previous round."""
        payoffs = history.my_payoffs if action == Action.C else history.opponent_payoffs[:, 0]
        return np.mean(payoffs[-1]) if len(payoffs) > 0 else 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        AC = self.calculate_average_payoff(history, Action.C)
        AD = self.calculate_average_payoff(history, Action.D)
        cooperators = sum(history.opponent_actions[-1, :])
        if cooperators > self.tau * self.game_description.n_players:
            self.tau += 0.1
        elif cooperators < self.tau * self.game_description.n_players:
            self.tau -= 0.1
        if AC > AD and AC > self.tau:
            return Action.C
        for opponent_action in history.opponent_actions[-1, :]:
            if not opponent_action and history.my_actions[-1]:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_346(BaseStrategy):
    """
    The Adaptive Public Goods strategy adapts to the game's history by balancing cooperation and defection based on the observed collective behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate = True
        self.observation_phase = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        self.observation_phase = True
        C, D = (sum(history.opponent_actions[-1, :]), self.game_description.n_players - 1 - sum(history.opponent_actions[-1, :]))
        if C / (C + D) >= self.game_description.k / self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            my_coop_payoffs = sum(history.my_payoffs[history.my_actions])
            my_defect_payoffs = sum(history.my_payoffs[~history.my_actions])
            if my_coop_payoffs > my_defect_payoffs:
                action = Action.C
        return action

class Strategy_COLLECTIVE_347(BaseStrategy):
    """
    A collective strategy that adapts to the behavior of other players, 
    balancing cooperation and self-interest through reciprocity, exploitation prevention, 
    and payoff-based adaptation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperators = []
        self.payoffs = []
        self.defect_count = 0

    def calculate_payoff(self, actions: NDArray[np.bool_]):
        """
        Calculate the payoff for a single round based on player actions.
        
        :param actions: A boolean array of player actions (True/1 means COOPERATE, False/0 means DEFECT)
        :return: The calculated payoff
        """
        n_players = self.game_description.n_players
        k = self.game_description.k
        total_cooperators = sum(actions)
        private_payoff = 1 - np.mean(actions) * n_players
        public_good_share = k / n_players * total_cooperators
        return private_payoff + public_good_share

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_opponent_actions = history.opponent_actions[-1, :]
        prev_total_cooperators = sum(prev_opponent_actions)
        reciprocity_condition = prev_total_cooperators >= self.game_description.n_players // 2
        if self.defect_count > self.game_description.n_rounds // 2:
            exploitation_condition = True
        else:
            exploitation_condition = False
        average_payoff = np.mean(self.payoffs) if self.payoffs else 0
        threshold = math.ceil(self.game_description.k / self.game_description.n_players * self.game_description.n_players)
        if average_payoff < (self.game_description.k + 1) / self.game_description.n_players:
            threshold += 1
        elif average_payoff > self.game_description.k / self.game_description.n_players:
            threshold -= 1
        if state.round_number == self.game_description.n_rounds - 1:
            last_reciprocity_condition = self.cooperators[-2] >= self.game_description.n_players // 2
            last_exploitation_condition = self.defect_count > self.game_description.n_rounds // 2
            return Action.C if last_reciprocity_condition and (not last_exploitation_condition) else Action.D
        if reciprocity_condition or not exploitation_condition:
            action = Action.C
        elif prev_total_cooperators == threshold - 1:
            action = Action.C
        else:
            action = Action.D
            self.defect_count += 1
        self.cooperators.append(action == Action.C)
        self.payoffs.append(self.calculate_payoff(np.concatenate((self.cooperators, prev_opponent_actions))))
        return action

class Strategy_COLLECTIVE_348(BaseStrategy):
    """
    Adaptive Cooperation with History-Driven Threshold (ACHT) strategy.
    
    This collective strategy adapts to the game's history and balances individual self-interest with collective benefits.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        avg_coop_t_minus_1 = np.mean(np.concatenate(([history.my_actions[-1]], opponent_actions)))
        tau = self.game_description.k / self.game_description.n_players if avg_coop_t_minus_1 < 0.5 else self.game_description.k / self.game_description.n_players * (1 + avg_coop_t_minus_1)
        num_coop_t_minus_1 = np.sum(np.concatenate(([history.my_actions[-1]], opponent_actions)))
        return Action.C if num_coop_t_minus_1 >= tau else Action.D

class Strategy_COLLECTIVE_349(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the benefits of cooperation,
    while being robust to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.punishment_flag = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number <= 3:
            if state.round_number == 3:
                opponent_actions = history.opponent_actions[-1, :]
                average_cooperation_rate = np.mean(opponent_actions)
                if average_cooperation_rate < self.cooperation_threshold:
                    return Action.D
            return Action.C
        previous_payoff = history.my_payoffs[-1]
        game_average_payoff = np.mean(history.my_payoffs)
        opponent_actions = history.opponent_actions[-1, :]
        num_cooperators = sum(opponent_actions)
        if previous_payoff >= game_average_payoff:
            return Action.C
        if self.punishment_flag:
            self.punishment_flag = False
            return Action.D
        opponent_defected_after_cooperation = history.opponent_actions[-1, 0] == 0 and history.opponent_actions[-2, 0] == 1 and (history.my_actions[-2] == 1)
        if opponent_defected_after_cooperation:
            self.punishment_flag = True
        if num_cooperators < self.game_description.n_players // 4:
            return Action.D
        if num_cooperators < self.game_description.n_players // 2:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_350(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective well-being.
    Adapts to the game's history and opponent behaviors, promoting cooperation while being robust against defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions = history.opponent_actions[-1]
        my_previous_action = history.my_actions[-1]
        cooperators_prev_round = np.sum(opponent_actions)
        defectors_prev_round = self.game_description.n_players - 1 - cooperators_prev_round
        avg_coop_payoff_prev_round = np.mean(history.opponent_payoffs[-1, opponent_actions])
        avg_defect_payoff_prev_round = np.mean(history.opponent_payoffs[-1, ~opponent_actions])
        if math.isnan(avg_coop_payoff_prev_round) or math.isclose(avg_coop_payoff_prev_round, avg_defect_payoff_prev_round):
            return Action.C
        if avg_coop_payoff_prev_round > avg_defect_payoff_prev_round:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_351(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_prob = 0.5
        self.prev_coop_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.coop_prob else Action.D
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        total_payoff = sum(history.my_payoffs) + sum((sum(row) for row in history.opponent_payoffs))
        if self.prev_coop_count >= self.game_description.n_players / 2:
            self.coop_prob = min(self.coop_prob + 0.1, 1)
        else:
            self.coop_prob = max(self.coop_prob - 0.1, 0)
        if total_payoff >= self.game_description.n_players * (self.game_description.k / self.game_description.n_players):
            self.coop_prob += 0.1
        else:
            self.coop_prob -= 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < self.coop_prob else Action.D

class Strategy_COLLECTIVE_352(BaseStrategy):
    """
    This is an adaptive cooperation strategy in public goods game.
    It balances individual payoffs with collective welfare by adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_payoff_rounds = math.floor(self.n_rounds / 4)
        start_idx = max(0, state.round_number - avg_payoff_rounds - 1)
        if history is not None:
            opponent_actions = history.opponent_actions[start_idx:state.round_number]
            my_actions = history.my_actions[start_idx:state.round_number]
            avg_payoff_per_player = np.mean(self.calculate_payoffs(opponent_actions, my_actions))
        if state.round_number <= math.floor(self.n_rounds / 2):
            majority_action = self.get_majority_action(history.opponent_actions[state.round_number - 1])
            return Action.C if majority_action else Action.D
        elif state.round_number > math.floor(self.n_rounds / 2) and state.round_number < self.n_rounds - 2:
            threshold = 1.5
            return Action.C if avg_payoff_per_player < threshold else Action.D
        else:
            if state.round_number == self.n_rounds - 1:
                return Action.C
            majority_action = self.get_majority_action(history.opponent_actions[state.round_number - 1])
            return Action.C if majority_action else Action.D

    def calculate_payoffs(self, opponent_actions: NDArray[np.bool_], my_actions: NDArray[np.bool_]) -> NDArray[np.float64]:
        n_players = self.n_players
        k = self.k
        payoffs = np.full(my_actions.shape[0], 1.0, dtype=np.float64)
        for i in range(opponent_actions.shape[0]):
            total_contributions = sum((1 if action else 0 for action in opponent_actions[i])) + (1 if my_actions[i] else 0)
            payoffs[i] += k * total_contributions / n_players
        return payoffs

    def get_majority_action(self, actions: NDArray[np.bool_]) -> bool:
        cooperators = sum((1 for action in actions if action))
        defectors = len(actions) - cooperators
        if cooperators > defectors:
            return True
        elif cooperators == defectors:
            return True
        else:
            return False

class Strategy_COLLECTIVE_353(BaseStrategy):
    """
    A collective strategy that balances individual payoffs with collective welfare by adapting to the group's cooperation level and gradually withdrawing cooperation in response to widespread defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.rounds = game_description.n_rounds
        self.alpha = min(3, self.n_players / 2)
        self.beta = self.n_players / 2
        self.tau = self.k / self.n_players
        self.gamma = self.n_players / 3
        self.punish_threshold = 0.5
        self.cooperation_scores = [1.0] * self.n_players
        self.avg_score = 1.0
        self.punished_opponents = [False] * self.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if not self.punished_opponents[i]:
                if opponent_action:
                    self.cooperation_scores[i] += 0.1
                else:
                    self.cooperation_scores[i] -= 0.2
        for i in range(self.n_players):
            if self.cooperation_scores[i] < self.punish_threshold and (not self.punished_opponents[i]):
                self.punished_opponents[i] = True
                self.cooperation_scores[i] = self.punish_threshold
        self.avg_score = sum(self.cooperation_scores) / self.n_players
        if state.round_number > self.alpha:
            if self.avg_score < self.tau and state.round_number % int(self.beta) == 0:
                self.avg_score -= 0.1
        if self.avg_score >= self.tau or state.round_number == self.rounds - 1:
            return Action.C
        else:
            for i in range(self.n_players):
                if self.punished_opponents[i]:
                    self.cooperation_scores[i] = max(0, self.cooperation_scores[i] + 0.2)
                    self.punished_opponents[i] = False
            return Action.D

class Strategy_COLLECTIVE_354(BaseStrategy):
    """
    Collective Strategy: "Adaptive Cooperation with Tit-for-Tat".

    This strategy aims to balance individual self-interest with collective well-being by adapting to the group's behavior and using a tit-for-tat approach.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tit_for_tat = True
        self.cooperation_rate = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        prev_round_payoff_avg = np.mean(np.append(history.my_payoffs[-1], history.opponent_payoffs[-1, :]))
        if self.tit_for_tat and prev_round_cooperators > self.game_description.n_players / 2:
            self.cooperation_rate = 1.0
        else:
            self.cooperation_rate = 0.0
        if prev_round_payoff_avg >= 1.5 and self.cooperation_rate < 1.0:
            self.cooperation_rate += 0.1
        if history.my_payoffs[-1] < 0.5 and (history.my_payoffs[-2] < 0.5 if state.round_number > 1 else False):
            self.cooperation_rate = 0.0
        if state.round_number == self.game_description.n_rounds - 1 and prev_round_cooperators >= self.game_description.n_players / 2:
            return Action.C
        return Action.C if random.random() < self.cooperation_rate else Action.D

class Strategy_COLLECTIVE_355(BaseStrategy):
    """
    This strategy balances individual self-interest with collective benefits,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_round_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :])
        self.prev_round_cooperators = prev_round_cooperators
        if prev_round_cooperators > self.game_description.n_players / 2:
            return Action.C
        elif prev_round_cooperators < self.game_description.n_players / 4:
            return Action.D
        else:
            avg_payoff = np.mean(history.my_payoffs[-1:] + history.opponent_payoffs[-1, :])
            if avg_payoff > 1.5:
                return Action.C
            elif avg_payoff == 1.5:
                return random.choice([Action.C, Action.D])
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1 and prev_round_cooperators < self.game_description.n_players / 4:
            return Action.D

class Strategy_COLLECTIVE_356(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective well-being.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        my_payoffs = history.my_payoffs[-1]
        all_cooperated = np.all(opponent_actions)
        if not all_cooperated:
            self.cooperate = False
        else:
            avg_payoff_from_cooperation = np.mean(history.my_payoffs)
            if avg_payoff_from_cooperation >= 1:
                self.cooperate = True
            else:
                num_defectors = np.sum(~opponent_actions)
                if num_defectors < self.game_description.n_players / 2:
                    self.cooperate = False
        if state.round_number == self.game_description.n_rounds - 1:
            if all_cooperated:
                return Action.C
            else:
                return Action.D
        elif history.my_actions[-3:].all():
            return Action.D
        return Action.C if self.cooperate else Action.D

class Strategy_COLLECTIVE_357(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective well-being by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history else 0)
        if state.round_number == self.game_description.n_rounds - 1 or total_cooperators >= self.game_description.k:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_358(BaseStrategy):
    """
    A strategy that adapts to the cooperation rate of other players.
    It prioritizes cooperation while being responsive to the actions of others.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.learning_rate = 0.1
        self.punishment_rate = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_cooperations = history.opponent_actions[-1, :]
        total_cooperators = sum(opponent_cooperations) + (history.my_actions[-1] == Action.C)
        cooperation_rate = total_cooperators / self.game_description.n_players
        if state.round_number < self.game_description.n_rounds - 1:
            self.cooperation_threshold += self.learning_rate * (cooperation_rate - self.cooperation_threshold)
            if history.my_payoffs[-1] > np.mean(history.opponent_payoffs[-1, :]) and history.my_actions[-1] == Action.D:
                self.cooperation_threshold -= self.punishment_rate
            if cooperation_rate >= self.cooperation_threshold:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_359(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective cooperation,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def analyze_history(self, history: PlayerHistory) -> dict:
        """
        Analyze the game history to determine opponent behavior.

        Args:
            history (PlayerHistory): The player's and opponents' actions and payoffs.

        Returns:
            dict: A dictionary containing the cooperation rate and history.
        """
        cooperation_rate = sum((1 for actions in history.opponent_actions if np.count_nonzero(actions) > len(actions) / 2)) / len(history.opponent_actions)
        cooperation_history = [np.count_nonzero(actions) for actions in history.opponent_actions]
        return {'cooperation_rate': cooperation_rate, 'cooperation_history': cooperation_history}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_behavior = self.analyze_history(history)
        prev_cooperators = np.count_nonzero(history.opponent_actions[-1, :])
        if prev_cooperators < self.game_description.n_players / 2:
            return Action.D
        elif prev_cooperators >= self.game_description.n_players / 2:
            return Action.C
        if opponent_behavior['cooperation_rate'] > 0.75:
            return Action.C
        elif opponent_behavior['cooperation_rate'] < 0.25:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if any((cooperation > 0 for cooperation in opponent_behavior['cooperation_history'])):
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_360(BaseStrategy):
    """
    Collective Strategy: "Adaptive Cooperativity with Robustness" (ACR)
    
    ACR prioritizes cooperation when the collective good is likely to be achieved, 
    while incorporating a punishment mechanism for defectors and forgiveness for reformed opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_round_cooperators = 0
        self.opponent_defections = [False] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        prev_round_payoffs = history.opponent_payoffs[-1, :]
        self.update_opponent_defections(prev_round_actions, prev_round_payoffs)
        self.prev_round_cooperators = sum(prev_round_actions)
        if state.round_number < self.game_description.n_rounds - 1:
            return self.standard_decision_rule()
        else:
            return self.endgame_decision_rule(history)

    def standard_decision_rule(self) -> Action:
        """
        Decide action based on ACR's standard decision rule.
        
        Cooperate if at least half of the players cooperated in the previous round, 
        otherwise defect. Also consider punishment mechanism for previously detected defectors.
        """
        if self.prev_round_cooperators >= self.game_description.n_players / 2:
            action = Action.C
        else:
            action = Action.D
        for opponent, defects in enumerate(self.opponent_defections):
            if defects:
                action = Action.D
        return action

    def endgame_decision_rule(self, history: PlayerHistory) -> Action:
        """
        Decide action based on ACR's endgame decision rule.
        
        Cooperate if total payoff is below the average total payoff of all players, 
        otherwise defect.
        """
        my_total_payoff = sum(history.my_payoffs)
        avg_opponent_payoff = np.mean(sum((opponent for opponent in history.opponent_payoffs)))
        if my_total_payoff < avg_opponent_payoff:
            return Action.C
        else:
            return Action.D

    def update_opponent_defections(self, prev_round_actions: NDArray[np.bool_], prev_round_payoffs: NDArray[np.float64]):
        """
        Update opponent defections based on previous round's observations.
        
        Mark an opponent as a defector if they defected while this player cooperated 
        and their payoff is less than the average payoff of all players in that round.
        """
        my_action = bool(prev_round_actions[0])
        avg_payoff = np.mean(prev_round_payoffs)
        for opponent, (action, payoff) in enumerate(zip(prev_round_actions, prev_round_payoffs)):
            if not my_action and action and (payoff < avg_payoff):
                self.opponent_defections[opponent] = True

    def count_cooperators(self, actions: NDArray[np.bool_]) -> int:
        """
        Count the number of cooperators in a given set of actions.
        
        Returns the sum of boolean values (True=1, False=0) representing cooperation.
        """
        return sum(actions)

class Strategy_COLLECTIVE_361(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Gradual Punishment (ACGP)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5
        self.p = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_coop_rate = np.mean(history.opponent_actions[-1, :])
        if avg_coop_rate >= self.theta:
            action = Action.C
        else:
            p_adjustment = self.adjust_p(avg_coop_rate, history)
            self.p += p_adjustment
            self.p = max(0.0, min(self.p, 1.0))
            action = Action.D if random.random() < self.p else Action.C
        return action

    def adjust_p(self, avg_coop_rate: float, history: PlayerHistory) -> float:
        round_number = len(history.my_actions)
        opponent_avg_coop_rate_prev_round = np.mean(history.opponent_actions[-2, :]) if round_number > 1 else 0.5
        rounds_remaining = self.game_description.n_rounds - round_number
        if round_number < self.game_description.n_rounds // 2:
            return 0.0
        p_adjustment = 0.1 * (-1 if avg_coop_rate >= opponent_avg_coop_rate_prev_round else 1)
        p_adjustment *= max(1 - rounds_remaining / self.game_description.n_rounds, 0)
        return p_adjustment

class Strategy_COLLECTIVE_362(BaseStrategy):
    """
    A collective strategy that encourages cooperation by initially cooperating,
    gradually increasing punishment (defection) in response to low cooperation rates,
    and providing opportunities for cooperation to recover.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = game_description.k / game_description.n_players
        self.defect_prob = 0.2
        self.consecutive_defects = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_coop_rate_prev_round = np.mean(history.opponent_actions[-1, :])
        if avg_coop_rate_prev_round >= self.coop_threshold:
            self.consecutive_defects = 0
            return Action.C
        self.consecutive_defects += 1
        self.defect_prob = min(1, self.defect_prob + 0.1 * self.consecutive_defects)
        if self.consecutive_defects > 5:
            return Action.D
        return Action.D if random.random() < self.defect_prob else Action.C

class Strategy_COLLECTIVE_363(BaseStrategy):
    """
    Adaptive Cooperate-Defect Strategy for Public Goods Game.
    
    This strategy adapts its cooperation probability based on opponent behavior and average payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 1.0
        self.cooperate_prob = 0.5
        self.avg_payoff_last_5_rounds = None
        self.avg_payoff_prev_5_rounds = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        if state.round_number % 5 == 0 and state.round_number > 0:
            avg_payoff_last_5_rounds = np.mean(history.my_payoffs[-5:])
            if self.avg_payoff_last_5_rounds is not None:
                self.threshold += 0.1 if avg_payoff_last_5_rounds > self.avg_payoff_last_5_rounds else -0.1
            self.avg_payoff_prev_5_rounds = self.avg_payoff_last_5_rounds
            self.avg_payoff_last_5_rounds = avg_payoff_last_5_rounds
        avg_coop_payoff = np.mean(history.my_payoffs[history.my_actions == True])
        avg_defect_payoff = np.mean(history.my_payoffs[history.my_actions == False])
        opponent_defection_rate = 1 - np.mean(history.opponent_actions[-1, :])
        if opponent_defection_rate > 0.7:
            self.cooperate_prob = 0.2
        else:
            self.cooperate_prob = avg_coop_payoff >= self.threshold
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < self.cooperate_prob else Action.D

class Strategy_COLLECTIVE_364(BaseStrategy):
    """
    A strategy that balances individual payoffs with collective gains while adapting to the dynamics of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        n_players = self.game_description.n_players
        k = self.game_description.k
        T = k / n_players * (1 - 1 / self.game_description.n_rounds)
        avg_coop_rate = np.mean(history.my_actions[:state.round_number])
        if avg_coop_rate >= T:
            action = Action.C
        else:
            action = Action.D
        if state.round_number > 1:
            expected_payoff = k / n_players * np.sum(history.opponent_actions[state.round_number - 2, :]) + int(action == Action.D)
            actual_payoff = history.my_payoffs[state.round_number - 1]
            if action == Action.C and actual_payoff < expected_payoff:
                self.cooperate_prob -= 0.1
            elif action == Action.D and actual_payoff > expected_payoff:
                self.cooperate_prob += 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if self.game_description.n_rounds % 2 == 0 else Action.D
        return action

class Strategy_COLLECTIVE_365(BaseStrategy):
    """
    Adaptive Collective Optimism Strategy for N-Player Public Goods Game.
    Balances individual self-interest with willingness to contribute to public good,
    adapting to behavior of others.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = game_description.k / game_description.n_players
        self.delta = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number < self.game_description.n_rounds - 1:
            num_cooperators_prev_round = np.sum(history.opponent_actions[-1, :], axis=0) + (history.my_actions[-1] == True)
            avg_cooperators_prev_rounds = np.mean(np.sum(history.opponent_actions[:state.round_number, :], axis=1) + (history.my_actions[:state.round_number] == True), axis=0)
            if avg_cooperators_prev_rounds >= self.theta:
                action = Action.C
            else:
                action = Action.D
            if num_cooperators_prev_round > self.theta * self.game_description.n_players:
                self.theta -= self.delta
            elif num_cooperators_prev_round < self.theta * self.game_description.n_players:
                self.theta += self.delta
        else:
            action = Action.D
        return action

class Strategy_COLLECTIVE_366(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) strategy for the N-Player Public Goods Game.
    Balances cooperation with cautious adaptation based on game history to maximize payoffs while encouraging group cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.history = []
        self.payoff_thresholds = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if len(self.history) else 0)
        p_defect = 1 - self.game_description.k / self.game_description.n_players * prev_round_coops / self.game_description.n_players
        if len(self.payoff_thresholds) >= 2 and history.my_payoffs[-1] < 1 + self.game_description.k / self.game_description.n_players * prev_round_coops and (self.payoff_thresholds[-2] < 1 + self.game_description.k / self.game_description.n_players * sum(history.opponent_actions[-3, :])):
            p_defect += 0.05
        if state.round_number >= self.game_description.n_rounds - 2:
            avg_payoff = np.mean(self.payoff_thresholds[:-1])
            if sum(history.my_payoffs) > avg_payoff * (self.game_description.k / self.game_description.n_players):
                return Action.C
        action = Action.D if random.random() < p_defect else Action.C
        self.history.append(action)
        self.payoff_thresholds.append(history.my_payoffs[-1])
        return action

class Strategy_COLLECTIVE_367(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Community Awareness strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.CR = 0
        self.previous_coop_ratio = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        c_total = sum(history.opponent_actions[-1, :]) + 1
        self.CR = c_total / self.game_description.n_players
        if self.CR >= self.game_description.k / self.game_description.n_players:
            return Action.C
        else:
            for opponent in history.opponent_actions[-1, :]:
                if not opponent and self.previous_coop_ratio >= self.game_description.k / self.game_description.n_players:
                    return Action.D
            return Action.D
        self.previous_coop_ratio = self.CR

class Strategy_COLLECTIVE_368(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Public Goods Sensitivity (ATTPS) strategy.
    
    This strategy prioritizes cooperation when the public goods share is significant, 
    indicating that enough opponents are contributing. It encourages collective cooperation 
    and maximizes overall payoffs by adapting to the observed behavior of opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_cooperators_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        public_goods_share = self.game_description.k / self.game_description.n_players * total_cooperators_prev
        if public_goods_share > 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_369(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation.
    
    Balances cooperation and self-interest by adapting to opponents' actions,
    while maintaining some level of cooperation. Prioritizes cooperation when 
    it's likely to be reciprocated, while protecting against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        else:
            num_cooperators = sum(history.opponent_actions[-1, :])
            if num_cooperators >= math.ceil((self.game_description.n_players + 1) / 2):
                return Action.C
            else:
                return Action.D if random.random() < 0.7 else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_370(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Retaliation (ACR)
    
    This strategy aims to balance cooperation and defection to maximize collective payoff 
    while being robust to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold_coop = game_description.n_players * game_description.k / (2 * game_description.n_players)
        self.retaliation_window = 3
        self.retaliation_threshold = 2
        self.retaliation_rounds = 2
        self.retaliate_until = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number >= self.game_description.n_rounds - 1:
            return Action.C
        total_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if self.retaliate_until > state.round_number:
            return Action.D
        if total_cooperators >= self.threshold_coop * self.game_description.n_players:
            return Action.C
        elif self.defections_count(history) > self.retaliation_threshold and self.retaliate_until <= state.round_number:
            self.retaliate_until = state.round_number + self.retaliation_rounds
            return Action.D
        else:
            return Action.D

    def defections_count(self, history: PlayerHistory):
        count = 0
        for i in range(max(0, len(history.my_actions) - self.retaliation_window), len(history.my_actions)):
            if not history.my_actions[i]:
                count += 1
        return count

class Strategy_COLLECTIVE_371(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Concession strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(game_description.n_players / 2)
        self.concession_probability = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_coop = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if num_coop >= self.cooperation_threshold:
            return Action.C
        elif num_coop > 0:
            self.concession_probability = min(1, max(0, self.game_description.k / self.game_description.n_players * num_coop))
            return Action.C if random.random() < self.concession_probability else Action.D
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            expected_cooperation_payoff = self.game_description.k / self.game_description.n_players * num_coop
            expected_defection_payoff = 1 + self.game_description.k / self.game_description.n_players * num_coop
            return Action.C if expected_cooperation_payoff > expected_defection_payoff else Action.D

class Strategy_COLLECTIVE_373(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Forgiveness (ACGF) strategy.
    
    This strategy adapts to the group's behavior, gradually forgiving past defections,
    and balances individual self-interest with collective benefits.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.n_players * 0.6
        self.consecutive_defection_limit = 3
        self.forgiveness_period = 2
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_payoff = history.my_payoffs[-1]
        total_cooperators_prev = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        cooperation_prob = prev_payoff / (self.game_description.k * self.game_description.n_players)
        if total_cooperators_prev >= self.cooperation_threshold:
            action = Action.C
        elif prev_payoff >= 1:
            action = Action.C if random.random() < cooperation_prob else Action.D
        else:
            action = Action.D
        if total_cooperators_prev < self.cooperation_threshold:
            self.consecutive_defections += 1
        else:
            self.consecutive_defections = 0
        if self.consecutive_defections >= self.consecutive_defection_limit:
            action = Action.C
            self.consecutive_defections -= self.forgiveness_period
        if state.round_number == self.game_description.n_rounds - 1 and prev_payoff >= 0:
            return Action.D
        return action

class Strategy_COLLECTIVE_374(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.first_round = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.first_round:
            self.first_round = False
            return random.choice([Action.C, Action.D])
        total_cooperators_prev = sum((1 for action in history.opponent_actions[-1, :] if action))
        if state.round_number == self.game_description.n_rounds - 1:
            expected_payoff_C = self.game_description.k * sum((1 for action in history.opponent_actions[-2, :] if action))
            expected_payoff_D = 1 + self.game_description.k / self.game_description.n_players * total_cooperators_prev
            return Action.C if expected_payoff_C > expected_payoff_D else Action.D
        if total_cooperators_prev >= self.game_description.n_players / 2:
            return Action.C
        else:
            defect_prob = (self.game_description.k - total_cooperators_prev) / self.game_description.k
            return Action.D if random.random() < defect_prob else Action.C

class Strategy_COLLECTIVE_375(BaseStrategy):
    """
    Adaptive Cooperate-and-Punish strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        cooperators_last_round = sum(history.opponent_actions[-1, :] != False)
        total_payoff_last_round = np.sum(history.opponent_payoffs[-1, :]) + (history.my_payoffs[-1] if len(history.my_payoffs) > 0 else 0)
        if cooperators_last_round >= self.game_description.n_players / 2:
            return Action.C
        if total_payoff_last_round < self.game_description.k * (self.game_description.n_players - 1) / self.game_description.n_players and state.round_number > 2:
            self.punishment_counter += 1
            if self.punishment_counter == 1:
                return Action.D
            else:
                self.punishment_counter = 0
        if cooperators_last_round >= self.game_description.n_players / 2 and state.round_number > 2:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_376(BaseStrategy):
    """
    This strategy adapts to the number of cooperators in previous rounds.
    It starts by cooperating, then adjusts its contribution threshold based on whether the total payoff is greater than or equal to (k/n) * n.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tau = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_cooperators = np.sum(history.opponent_actions[-1, :]) + 1
        total_payoff = self.game_description.k * previous_cooperators / self.game_description.n_players
        if total_payoff >= self.game_description.k / self.game_description.n_players * self.game_description.n_players:
            self.tau += 0.05
        else:
            self.tau -= 0.05
        self.tau = max(0, min(self.tau, 1))
        if previous_cooperators >= self.tau * self.game_description.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_377(BaseStrategy):
    """
    A collective strategy that adapts to group behavior in the N-Player Public Goods Game.
    It balances individual self-interest with collective well-being by establishing a cooperative fingerprint,
    promoting cooperation, and punishing defection based on a simple Cooperation Quotient metric.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        CQ = total_cooperators / self.game_description.n_players
        if CQ >= 0.5:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_378(BaseStrategy):
    """
    Adaptive Cooperation (AC) strategy.
    
    The AC strategy initially cooperates with a probability of 0.5 in the first round, 
    then adapts its cooperation probability based on the number of cooperators and payoffs 
    received in subsequent rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperation_probability else Action.D
        total_cooperators_prev = sum(history.opponent_actions[-1, :] != False)
        pi_i_prev = history.my_payoffs[-1]
        pi_c_prev = pi_i_prev if history.my_actions[-1] == True else 0
        pi_d_prev = pi_i_prev if history.my_actions[-1] == False else 0
        if len(history.my_payoffs) > 1:
            pi_c_prev_last_round, pi_d_prev_last_round = (history.my_payoffs[-2], history.my_payoffs[-2])
        else:
            pi_c_prev_last_round, pi_d_prev_last_round = (0, 0)
        if pi_i_prev > pi_d_prev_last_round and self.cooperation_probability < 1:
            self.cooperation_probability = min(1, self.cooperation_probability + 0.1)
        elif pi_i_prev < pi_c_prev_last_round and self.cooperation_probability > 0:
            self.cooperation_probability = max(0, self.cooperation_probability - 0.1)
        if total_cooperators_prev >= self.game_description.n_players / 2 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C if random.random() < self.cooperation_probability else Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_379(BaseStrategy):
    """
    Adaptive Cooperative Balance Strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop = 0.5
        self.cooperation_probability = 0.5
        self.total_payoff = 0
        self.opponent_defection_response = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        self.avg_coop = sum(history.my_actions[:-1]) / state.round_number
        if self.avg_coop > self.game_description.n_players / (self.game_description.k + 1):
            self.cooperation_probability = self.avg_coop
        else:
            self.cooperation_probability = 1 - self.avg_coop / self.game_description.n_players
        if history.my_actions[-1] and (not all(history.opponent_actions[-1, :])):
            self.opponent_defection_response = True
        else:
            self.opponent_defection_response = False
        if self.opponent_defection_response:
            self.cooperation_probability -= 0.1
        opponent_payoffs = history.opponent_payoffs[-1, :]
        average_opponent_payoff = np.mean(opponent_payoffs)
        my_last_round_payoff = self.game_description.k / self.game_description.n_players * sum([int(x) for x in history.my_actions[:-1]]) + 1 - int(history.my_actions[-1])
        self.total_payoff += my_last_round_payoff
        if self.total_payoff > average_opponent_payoff:
            self.cooperation_probability -= 0.05
        elif self.total_payoff < average_opponent_payoff:
            self.cooperation_probability += 0.1
        return random.choices([Action.C, Action.D], weights=[self.cooperation_probability, 1 - self.cooperation_probability])[0]

class Strategy_COLLECTIVE_380(BaseStrategy):
    """
    Adaptive Cooperative Escalation (ACE) strategy.
    
    This collective strategy balances individual self-interest with collective well-being by adaptively responding to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < 5:
            opponent_cooperations = sum(history.opponent_actions[-1, :])
            majority_cooperated = opponent_cooperations > self.game_description.n_players / 2
            action = Action.C if majority_cooperated else Action.D
        else:
            cooperation_sum = sum([sum(round) for round in history.opponent_actions[-3:, :]])
            average_cooperation_rate = cooperation_sum / (self.game_description.n_players * 3)
            if average_cooperation_rate > 0.6:
                action = Action.C
            elif average_cooperation_rate < 0.4:
                action = Action.D
            else:
                opponent_action = random.choice(history.opponent_actions[-1, :])
                action = Action.C if opponent_action else Action.D
        if state.round_number > self.game_description.n_rounds - 2:
            private_defect_payoff = 1 + self.game_description.k / self.game_description.n_players * sum([not c for c in history.opponent_actions[-1, :]])
            if history.my_payoffs[-1] <= private_defect_payoff:
                action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_COLLECTIVE_381(BaseStrategy):
    """
    Collective strategy "Adaptive Cooperative Escalation" (ACE) for the N-Player Public Goods Game.
    
    This strategy prioritizes collective cooperation while adapting to the actions of other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        theta = 1 / self.n_players
        if prev_cooperators > self.n_players / 2:
            return Action.C
        elif prev_cooperators <= theta:
            return Action.D
        p_cooperate = prev_cooperators / self.n_players * (self.k - 1)
        if random.random() < p_cooperate:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_382(BaseStrategy):
    """
    Adaptive Collective Optimism Strategy for N-Player Public Goods Game.
    
    This strategy balances cooperation with self-interest, adapting to observed behavior and outcomes.
    It prioritizes collective action while safeguarding against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_threshold = 0.75 * game_description.k
        self.avg_cooperation_rate = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_payoffs = np.mean(history.my_payoffs[:-1])
        avg_cooperation_rate = np.mean(np.concatenate((history.opponent_actions, history.my_actions[:, None]), axis=1))
        if prev_round_payoffs > self.avg_payoff_threshold:
            return Action.C
        min_payoff_prev_round = np.min(np.concatenate((history.my_payoffs[:-1], history.opponent_payoffs[:, :-1].flatten())))
        if min_payoff_prev_round < self.avg_payoff_threshold * 0.9:
            self.avg_payoff_threshold *= 0.9
        p_defect = (1 - avg_cooperation_rate) / 2
        return Action.D if random.random() < p_defect else Action.C

class Strategy_COLLECTIVE_383(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective well-being.
    It adapts to the game's history, encouraging cooperation through reciprocity and punishment mechanisms.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_rounds = math.ceil(0.1 * game_description.n_rounds)
        self.threshold = 0.5
        self.cooperation_probability = 0.8
        self.defection_probability = 0.4
        self.punishment_rounds = 2
        self.adaptation_period = 10
        self.opponent_defection_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.initial_rounds:
            return Action.C if random.random() < self.cooperation_probability else Action.D
        opponent_cooperation_rate = np.mean(history.opponent_actions[-1, :])
        if opponent_cooperation_rate > self.threshold:
            return Action.C if random.random() < self.cooperation_probability else Action.D
        elif opponent_cooperation_rate == self.threshold:
            return Action.C if random.random() < 0.1 else Action.D
        else:
            return Action.D if random.random() < self.defection_probability else Action.C
        if history.opponent_actions[-2, :].sum(axis=0) >= 2:
            self.opponent_defection_count += 1
            if self.opponent_defection_count <= self.punishment_rounds:
                return Action.D
            else:
                self.opponent_defection_count = 0
        if state.round_number % self.adaptation_period == 0:
            collective_payoff = np.mean(history.my_payoffs)
            average_payoff = np.mean(history.opponent_payoffs, axis=1).mean()
            if collective_payoff > average_payoff:
                self.threshold += 0.05
            elif collective_payoff < average_payoff:
                self.threshold -= 0.05
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_384(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperative Governance (ACG)
    
    This strategy prioritizes collective cooperation while maintaining individual adaptability.
    It promotes a balanced approach that benefits both the individual and the collective by responding 
    to the group's overall cooperation level and accounting for personal payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_payoff = None
        self.avg_coop_rate_threshold = 0.5
        self.min_payoff_percentage = 0.25
        self.max_payoff = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        avg_coop_rate = sum(opponent_actions) / len(opponent_actions)
        if self.prev_payoff is None or self.prev_payoff >= self.min_payoff_percentage * self.max_payoff:
            if avg_coop_rate > self.avg_coop_rate_threshold:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

    def update(self, payoff: float):
        self.prev_payoff = payoff

class Strategy_COLLECTIVE_385(BaseStrategy):
    """
    The Adaptive Collective Cooperation strategy balances cooperation with cautious defection based on observed behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        avg_coop_prev = np.mean(history.opponent_actions[-2, :] | history.my_actions[-2])
        coop_threshold = self.game_description.k / self.game_description.n_players * 0.5
        if avg_coop_prev >= coop_threshold:
            return Action.C
        else:
            defect_prob = 1 - avg_coop_prev
            if random.random() < defect_prob:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_386(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective welfare,
    adapting to various opponent behaviors while promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round = history.opponent_actions[-1, :]
        num_cooperators_prev_round = sum(prev_round)
        if num_cooperators_prev_round >= self.game_description.k / self.game_description.n_players:
            cooperate = True
        elif num_cooperators_prev_round < self.game_description.k / self.game_description.n_players:
            cooperate = False
        elif num_cooperators_prev_round == math.floor(self.game_description.k):
            individual_payoff = history.my_payoffs[-1]
            average_payoff = np.mean(history.opponent_payoffs[-1, :])
            if individual_payoff > average_payoff:
                cooperate = True
            else:
                cooperate = False
        elif state.round_number == self.game_description.n_rounds - 1:
            if num_cooperators_prev_round < self.game_description.k / self.game_description.n_players:
                cooperate = False
        action = Action.C if cooperate else Action.D
        return action

class Strategy_COLLECTIVE_387(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) strategy for N-Player Public Goods Game.
    
    ACO adapts to various opponent behaviors and focuses on maximizing overall payoffs by initially cooperating 
    with a probability of 0.5, then updating its cooperation probability based on past performance.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.adaptive_threshold = 0.5
        self.previous_payoff_avg = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        if self.previous_payoff_avg is None:
            self.previous_payoff_avg = np.mean(history.my_payoffs)
        current_payoff_avg = np.mean(history.my_payoffs[-1])
        if current_payoff_avg > 1:
            self.adaptive_threshold = min(self.adaptive_threshold + 0.05, 1)
        elif current_payoff_avg <= 1:
            self.adaptive_threshold = max(self.adaptive_threshold - 0.05, 0)
        if sum(history.opponent_actions[-1]) < self.game_description.n_players // 2:
            return Action.D
        self.previous_payoff_avg = current_payoff_avg
        return Action.C if random.random() < self.adaptive_threshold else Action.D

class Strategy_COLLECTIVE_388(BaseStrategy):
    """
    A strategy that adapts to the game's history and responds to changes in contribution rates.
    It encourages cooperation when contributions are high, punishes defection when contributions drop,
    and forgives and adapts to changes in opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_contrib_rate = 0
        self.punish_defect = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        opponent_actions = history.opponent_actions[-1, :]
        avg_contrib_rate = np.mean(opponent_actions) + history.my_actions[-1] / self.game_description.n_players
        if avg_contrib_rate < self.game_description.k / self.game_description.n_players and (not self.punish_defect):
            self.punish_defect = True
        if self.punish_defect and random.random() < 0.8:
            return Action.D
        if avg_contrib_rate >= self.game_description.k / self.game_description.n_players:
            coop_prob = max(0.5, avg_contrib_rate)
        else:
            coop_prob = 1 - min(0.5, self.game_description.k / self.game_description.n_players / avg_contrib_rate)
        if random.random() < coop_prob:
            return Action.C
        else:
            return Action.D
        if state.round_number > 1 and avg_contrib_rate >= self.game_description.k / self.game_description.n_players:
            self.punish_defect = False

class Strategy_COLLECTIVE_389(BaseStrategy):
    """
    The Adaptive Collective Contribute strategy prioritizes cooperation when there are sufficient contributors.
    It adapts to the current round's contributions and previous payoffs, aiming to maintain a cooperative atmosphere.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = math.ceil(game_description.n_players / game_description.k * 0.5)
        self.last_payoff_difference = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_contributions = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if self.last_payoff_difference is not None:
            cooperate_condition = total_contributions >= self.threshold or self.last_payoff_difference > 0
        else:
            cooperate_condition = total_contributions >= self.threshold
        if history.my_actions[-1]:
            my_last_payoff = history.my_payoffs[-1]
        else:
            my_last_payoff = history.my_payoffs[-2]
        opponent_last_payoff = np.mean(history.opponent_payoffs[-1, :])
        self.last_payoff_difference = my_last_payoff - opponent_last_payoff
        return Action.C if cooperate_condition else Action.D

class Strategy_COLLECTIVE_390(BaseStrategy):
    """
    A collective strategy that adapts its cooperation threshold based on game history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number < self.n_players:
            prev_coops = sum(history.opponent_actions[-1, :])
            if prev_coops < self.n_players / 2:
                return Action.D
            else:
                return Action.C
        elif self.n_players <= state.round_number <= self.r - self.n_players:
            avg_payoff = np.mean([np.sum(round) for round in history.my_payoffs[-self.n_players:]])
            if avg_payoff < self.k / self.n_players:
                return Action.D
            else:
                coop_prob = sum(history.opponent_actions[-1, :]) / self.n_players
                return Action.C if random.random() < coop_prob else Action.D
        else:
            total_payoff = np.sum([np.sum(round) for round in history.my_payoffs])
            if total_payoff > self.n_players * self.k:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_391(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Gradual Concession (ACGC)

    This strategy adapts to the level of cooperation in the group while maintaining a degree of flexibility through gradual concession.
    It prioritizes collective well-being by initially cooperating and adapting to the level of cooperation in the group.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators = 0
        self.cumulative_defections = 0
        self.concession_threshold = 5 * self.game_description.n_players / 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.total_cooperators = sum(history.opponent_actions[-1, :])
        if state.round_number < self.game_description.n_rounds - 1:
            if self.total_cooperators >= self.game_description.n_players / 2:
                action = Action.C
            else:
                action = Action.D
                self.cumulative_defections += self.game_description.n_players - self.total_cooperators
            if self.cumulative_defections > self.concession_threshold:
                action = Action.D
                self.cumulative_defections = 0
        else:
            action = Action.D
        return action

class Strategy_COLLECTIVE_392(BaseStrategy):
    """
    Adaptive Cooperation with History-Dependent Tit-for-Tat Strategy.
    
    This strategy initiates cooperation and adapts to changes in opponent behavior 
    based on the history of collective cooperation. It aims to balance individual self-interest 
    with collective well-being by encouraging reciprocity and mutual benefit.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_cooperators = np.sum(history.opponent_actions[-1, :])
        avg_cooperation = total_cooperators / self.game_description.n_players
        threshold = (self.game_description.k - 1) / self.game_description.n_players
        if avg_cooperation >= threshold:
            return Action.C
        elif avg_cooperation < threshold and history.my_actions[-1]:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_393(BaseStrategy):
    """
    Adaptive Cooperation with History-Dependent Adjustment (ACHDA) Strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_init = 0.5
        self.p_reset = 0.25

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        alpha = np.mean(history.my_actions)
        beta = np.max(history.opponent_payoffs, axis=1).max() - np.min(history.opponent_payoffs, axis=1).min()
        theta = max(0, min(1, (alpha + 0.5 * beta) / self.game_description.n_players))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if alpha > 0.5 else Action.D
        consecutive_defections = np.all(~history.opponent_actions[-2:, :])
        if consecutive_defections and random.random() < self.p_reset:
            return Action.C
        return Action.C if random.random() < theta else Action.D

class Strategy_COLLECTIVE_394(BaseStrategy):
    """
    A strategy that aims to balance individual self-interest with collective well-being by adapting to the game's history and punishing free-riders while encouraging cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_round_defection = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_prev_round = history.opponent_actions[-2] if len(history.opponent_actions) > 1 else np.array([False])
        self.prev_round_defection = sum((1 for action in prev_prev_round if not action)) > 0
        cooperators = sum(history.opponent_actions[-1, :])
        if cooperators >= math.ceil(self.game_description.n_players / 2):
            return Action.C
        if self.prev_round_defection:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_396(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the greater good,
    adapting to the community's behavior over time.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.observation_rounds = min(5, game_description.n_rounds // 2)
        self.recent_cooperation_window_size = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number < self.observation_rounds:
            return Action.D
        else:
            total_cooperations = sum((1 for t, actions in enumerate(history.opponent_actions) if np.any(actions)))
            community_cooperation_rate = total_cooperations / (self.game_description.n_players * self.observation_rounds)
            recent_defectors = [i for i, opponent_history in enumerate(np.transpose(history.opponent_actions)) if sum(opponent_history[-self.recent_cooperation_window_size:]) < self.recent_cooperation_window_size // 2]
            if community_cooperation_rate > 0.5:
                return Action.C
            elif state.round_number == self.game_description.n_rounds - 1:
                return Action.C
            elif community_cooperation_rate < 0.3:
                return Action.D
            elif community_cooperation_rate > 0.8 and state.round_number > self.observation_rounds + self.recent_cooperation_window_size:
                return Action.C
            elif any((player == recent_defectors for player in range(self.game_description.n_players - 1))):
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_397(BaseStrategy):
    """
    A collective strategy that adapts cooperation levels based on the game's history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_prev = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_coop = np.mean(np.concatenate(([history.my_actions[-1]], history.opponent_actions[-1, :])))
        self.avg_coop_prev = prev_round_coop
        theta = self.game_description.k / self.game_description.n_players * (self.game_description.n_players / 2)
        if state.round_number < self.game_description.n_rounds - 1:
            if self.avg_coop_prev >= theta or math.isclose(self.avg_coop_prev, theta):
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_398(BaseStrategy):
    """
    A strategy that aims to balance individual payoff maximization with collective cooperation,
    while being robust against various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_insufficient_cooperation = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperation_threshold = self.game_description.k / self.game_description.n_players
        opponent_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if opponent_cooperators >= cooperation_threshold * self.game_description.n_players:
            return Action.C
        else:
            self.consecutive_insufficient_cooperation += 1
            if self.consecutive_insufficient_cooperation >= 2:
                self.consecutive_insufficient_cooperation = 0
                return Action.C
            return Action.D

class Strategy_COLLECTIVE_399(BaseStrategy):
    """
    Adaptive Cooperative Tit-for-Tat (ACTT) strategy.

    This strategy promotes cooperation by adapting to the collective behavior of the group.
    It initially cooperates and then responds to the number of cooperators and defectors,
    encouraging mutual cooperation while punishing excessive defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        prev_coops = sum((1 for action in history.opponent_actions[-1] if action))
        prev_defs = self.game_description.n_players - 1 - prev_coops
        if prev_coops > prev_defs:
            self.cooperate_prob = min(self.cooperate_prob + 0.1, 0.9)
        elif prev_defs > prev_coops:
            self.cooperate_prob = max(self.cooperate_prob - 0.1, 0.1)
        if prev_defs - prev_coops > 0.2 * (self.game_description.n_players - 1):
            return Action.D
        elif prev_coops >= prev_defs:
            self.cooperate_prob = 0.8
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < self.cooperate_prob else Action.D

class Strategy_COLLECTIVE_400(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective well-being by adapting to the game's history and responding to the level of cooperation exhibited by other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_payoff_c = np.mean(history.my_payoffs[history.my_actions])
        avg_payoff_d = np.mean(history.my_payoffs[~history.my_actions])
        if avg_payoff_c >= avg_payoff_d:
            action = Action.C
        else:
            action = Action.D
        hcr = np.sum(history.my_actions) / len(history.my_actions)
        p = (1 + self.game_description.k / self.game_description.n_players * hcr) / (1 + self.game_description.k / self.game_description.n_players)
        if random.random() < p:
            action = Action.C
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1 and np.sum(history.my_payoffs[history.my_actions]) >= np.sum(history.my_payoffs[~history.my_actions]):
            action = Action.C
        return action

class Strategy_COLLECTIVE_401(BaseStrategy):
    """
    A strategy that balances individual payoff maximization with collective welfare,
    adapting to the evolving game dynamics and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.n_players * game_description.k / (game_description.n_players + game_description.k)
        self.round_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_cooperators_prev = sum((1 for action in history.opponent_actions[-1, :] if action))
        if total_cooperators_prev >= self.threshold:
            return Action.C
        else:
            current_total_payoff = sum(self.round_payoffs) if self.round_payoffs else 0
            prev_total_payoff = sum(self.round_payoffs[:-self.game_description.n_rounds // 2]) if len(self.round_payoffs) > self.game_description.n_rounds // 2 else 0
            if current_total_payoff > prev_total_payoff:
                return Action.C
            elif total_cooperators_prev >= self.threshold:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_402(BaseStrategy):
    """
    Adaptive Cooperation with Retaliation Strategy.
    
    This strategy aims to balance individual self-interest with collective well-being 
    by adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.retaliation_probabilities = [0.5] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1]
        num_cooperators = sum(opponent_actions)
        my_payoff = history.my_payoffs[-1]
        for i, (action, payoff) in enumerate(zip(opponent_actions, history.opponent_payoffs[-1])):
            if action == 0 and payoff > my_payoff:
                self.retaliation_probabilities[i] = min(self.retaliation_probabilities[i] + 0.1, 0.9)
        p = 1 - num_cooperators / self.game_description.n_players
        if num_cooperators <= self.game_description.n_players / self.game_description.k:
            retaliate = random.random() < max(p, self.retaliation_probabilities[0])
        else:
            retaliate = False
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if not retaliate else Action.D

class Strategy_COLLECTIVE_403(BaseStrategy):
    """
    Collective strategy that promotes cooperation by reciprocating cooperation and punishing defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.alpha = game_description.k / 2
        self.cooperation_prob = 0.5
        self.prev_round_payoffs = []
        self.total_cooperators_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperation_prob else Action.D
        elif state.round_number == 1 and history is not None:
            total_cooperators_prev = sum(history.opponent_actions[0, :] == Action.C)
            if total_cooperators_prev >= self.alpha:
                return Action.C
            elif self.game_description.n_players - total_cooperators_prev > self.alpha:
                return Action.D
        else:
            total_cooperators_prev = sum(history.opponent_actions[-1, :] == Action.C)
            avg_payoff_C = np.mean([payoff for payoff, action in zip(history.my_payoffs[1:], history.my_actions[1:]) if action])
            avg_payoff_D = np.mean([payoff for payoff, action in zip(history.my_payoffs[1:], history.my_actions[1:]) if not action])
            self.cooperation_prob = (avg_payoff_C - avg_payoff_D) / (self.game_description.k / self.game_description.n_players)
            return Action.C if random.random() < self.cooperation_prob else Action.D

class Strategy_COLLECTIVE_404(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Social Learning (ACSL)
    
    This strategy balances individual self-interest with cooperation to achieve mutually beneficial outcomes.
    It leverages social learning, adaptability, and robustness to counter various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialize ACSL strategy.

        Args:
            game_description (PublicGoodsDescription): Game description containing n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.tau = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide action based on the current state and history.

        Args:
            state (GameState): Current game state containing round_number.
            history (None | PlayerHistory): History of actions and payoffs, or None for initial rounds.

        Returns:
            Action: Decided action (C or D).
        """
        if state.round_number < 4:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_actions = history.opponent_actions[-2, :]
        prev_round_payoffs = history.opponent_payoffs[-2, :]
        avg_coop_payoff = np.mean(prev_round_payoffs[prev_round_actions == 1])
        avg_defect_payoff = np.mean(prev_round_payoffs[prev_round_actions == 0])
        if avg_coop_payoff >= avg_defect_payoff:
            self.tau -= 0.05
        else:
            self.tau += 0.05
        exp_num_cooperators = np.sum(prev_round_actions == 1)
        if exp_num_cooperators > self.tau * self.game_description.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_405(BaseStrategy):
    """
    Collective strategy that balances individual payoffs with collective well-being by adapting to the game's history and punishing defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_target = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players, n_rounds, k = (self.game_description.n_players, self.game_description.n_rounds, self.game_description.k)
        if state.round_number == 0 or state.round_number < math.ceil(n_players / 2):
            return Action.C
        opponent_actions = history.opponent_actions[-math.floor(n_players / 2):, :]
        average_cooperation_rate = np.mean(np.sum(opponent_actions, axis=1))
        if average_cooperation_rate >= k / n_players:
            action = Action.C
            if self.punishment_target is not None and self.punishment_target == state.round_number % n_players + 1:
                action = Action.D
        else:
            action = Action.D
        if history.my_actions[-1] == False and average_cooperation_rate >= k / n_players:
            self.punishment_target = (state.round_number + 1) % n_players + 1
        return action

class Strategy_COLLECTIVE_406(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors while promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        if sum(prev_round_actions) > 0:
            action = Action.C
        elif all((p == False for p in prev_round_actions)):
            action = Action.D
        else:
            avg_cooperation_rate = self.calculate_avg_cooperation_rate(history, state.round_number)
            if avg_cooperation_rate >= 0.5:
                self.cooperation_prob += 0.1
            elif avg_cooperation_rate < 0.5:
                self.cooperation_prob -= 0.1
            action = Action.C if random.random() < self.cooperation_prob else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        for opponent_actions in history.opponent_actions:
            if sum(opponent_actions) == 0 and all((p == False for p in opponent_actions)):
                return Action.D
        return action

    def calculate_avg_cooperation_rate(self, history: PlayerHistory, round_number: int) -> float:
        avg_cooperation_rate = (sum(history.my_actions[:round_number]) + sum((sum(opponent_actions) for opponent_actions in history.opponent_actions[:round_number]))) / (2 * self.game_description.n_players)
        return avg_cooperation_rate

class Strategy_COLLECTIVE_407(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Reciprocity strategy.
    This strategy adapts to changing opponent behaviors by gradually adjusting reciprocity levels based on cooperation scores.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_scores = [0.0] * game_description.n_players
        self.prev_C_count = 0
        self.prev_D_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        for opponent in range(self.game_description.n_players):
            if opponent != self.game_description.n_players - 1:
                action = history.opponent_actions[-1, opponent]
                self.cooperation_scores[opponent] += 2 * int(action) / self.game_description.n_players - 1
        theta = self.game_description.k / self.game_description.n_players
        C_count = sum((int(action) for action in history.opponent_actions[-1, :]))
        D_count = self.game_description.n_players - 1 - C_count
        avg_CS = np.mean(self.cooperation_scores)
        weighted_C_count = sum((1 if self.cooperation_scores[opponent] > avg_CS else 0 for opponent in range(self.game_description.n_players)))
        if C_count >= theta * self.game_description.n_players:
            return Action.C
        elif weighted_C_count < theta * (self.game_description.n_players - 1):
            return Action.D
        if C_count == self.prev_C_count and D_count == self.prev_D_count:
            return history.opponent_actions[-1, self.game_description.n_players - 2]
        else:
            return Action.C
        self.prev_C_count = C_count
        self.prev_D_count = D_count

class Strategy_COLLECTIVE_408(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation (AC)
    
    This strategy promotes collective cooperation by adapting to the group's behavior.
    It encourages mutual cooperation and reinforces cooperative behavior while discouraging defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate_opponent = 0
        self.total_payoff_cooperation = 0
        self.total_payoff_defection = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = 0.5
            return Action.C if random.random() < cooperate_prob else Action.D
        if history is not None:
            self.cooperation_rate_opponent = np.mean(history.opponent_actions)
            self.total_payoff_cooperation += np.sum(history.my_payoffs[history.my_actions])
            self.total_payoff_defection += np.sum(history.my_payoffs[~history.my_actions])
        if state.round_number < self.game_description.n_players // 2:
            if self.cooperation_rate_opponent > self.game_description.k / self.game_description.n_players:
                cooperate_prob = 0.7
            else:
                cooperate_prob = 0
            return Action.C if random.random() < cooperate_prob else Action.D
        elif self.game_description.n_players // 2 <= state.round_number < self.game_description.n_rounds - self.game_description.n_players // 2:
            if self.cooperation_rate_opponent > self.game_description.k / self.game_description.n_players:
                return Action.C
            elif self.cooperation_rate_opponent < self.game_description.k / self.game_description.n_players and np.mean(history.opponent_actions[-1]) > np.mean(history.opponent_actions[-2]):
                cooperate_prob = 0.6
                return Action.C if random.random() < cooperate_prob else Action.D
            else:
                return Action.D
        elif self.total_payoff_cooperation > self.total_payoff_defection:
            return Action.C
        elif self.cooperation_rate_opponent > self.game_description.k / self.game_description.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_409(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) Strategy for N-Player Public Goods Game.
    
    This strategy adapts to the game parameters and history by encouraging cooperation 
    when enough players cooperate, while balancing individual self-interest with collective benefits.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.window_size = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :])
        coop_prob = self.game_description.k / self.game_description.n_players * (1 - prev_round_coops / self.game_description.n_players)
        recent_coop_count = sum((sum((action for action in round_history if action)) for round_history in history.opponent_actions[-self.window_size:, :]))
        if prev_round_coops >= self.game_description.n_players / 2 or recent_coop_count > self.game_description.n_players / 2 * self.window_size:
            return Action.C
        return Action.C if random.random() < coop_prob else Action.D

class Strategy_COLLECTIVE_410(BaseStrategy):
    """
    A strategy that balances individual payoffs with collective well-being by adapting to the game's history and encouraging cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        ocr = np.mean(history.opponent_actions[-1, :])
        tau = 0.5 + (self.game_description.k - self.game_description.n_players) / self.game_description.n_players
        if self.punishment_rounds > 0:
            self.punishment_rounds -= 1
            return Action.D
        if ocr > tau or state.round_number == self.game_description.n_rounds - 1:
            defectors = np.where(~history.opponent_actions[-1, :])[0]
            if len(defectors) > 0:
                self.punishment_rounds = min(1, self.game_description.n_rounds - state.round_number)
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_411(BaseStrategy):
    """
    A collective strategy that adapts to the game's history and parameters,
    while being robust to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = game_description.n_players * game_description.k / (game_description.n_players + game_description.k)
        self.defect_fraction = 0.5
        self.delta = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if cooperators < self.game_description.n_players * 0.2:
            return Action.D
        coop_rate = cooperators / self.game_description.n_players
        if coop_rate > 0.8 and self.coop_threshold < 1 - self.delta:
            self.coop_threshold += self.delta
        elif coop_rate < 0.2 and self.coop_threshold > self.delta:
            self.coop_threshold -= self.delta
        if cooperators > self.game_description.n_players * self.coop_threshold:
            return Action.C
        else:
            total_contribution = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            payoff_from_cooperation = self.game_description.k / self.game_description.n_players * total_contribution
            max_payoff = self.game_description.k
            if payoff_from_cooperation < self.defect_fraction * max_payoff:
                return Action.D
            else:
                return Action.C

class Strategy_COLLECTIVE_412(BaseStrategy):
    """
    Adaptive Collective Optimism Strategy for N-Player Public Goods Game.
    
    This strategy initially cooperates, then adapts based on opponent behavior,
    promoting group welfare while protecting individual payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploration_probability = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :])
        if previous_round_cooperators >= self.game_description.n_players / 2:
            return Action.C
        elif previous_round_cooperators < self.game_description.n_players / 4:
            return Action.D
        if random.random() < self.exploration_probability:
            return Action.C if random.random() >= 0.5 else Action.D
        return Action.D

    def __call_last_round(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return super().__call__(state, history)

class Strategy_COLLECTIVE_413(BaseStrategy):
    """
    The Adaptive Cooperative Tracker (ACT) strategy balances individual self-interest 
    with collective cooperation by leveraging game history and parameters to adapt 
    to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.0
        self.coop_threshold = game_description.k / game_description.n_players
        self.opponent_defections = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        self.avg_coop_rate = num_cooperators / self.game_description.n_players * (state.round_number / self.game_description.n_rounds)
        if self.avg_coop_rate >= self.coop_threshold or num_cooperators >= self.game_description.n_players // 2:
            action = Action.C
        elif len(self.opponent_defections) > 1 and sum([1 for defection in self.opponent_defections if state.round_number - defection <= min(self.game_description.n_rounds / 4, 10)]) > 0:
            action = Action.D
        else:
            action = Action.C
        if history.my_actions[-1] == False:
            self.opponent_defections.append(state.round_number)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_414(BaseStrategy):
    """
    The AC strategy aims to balance individual payoff maximization with collective welfare by adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = (game_description.k - 1) / game_description.n_players
        self.opponent_defection_streaks = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions_last_round = history.opponent_actions[-1, :]
        avg_cooperation_rate = np.mean(opponent_actions_last_round)
        if avg_cooperation_rate >= self.cooperation_threshold:
            action = Action.C
        else:
            action = Action.D
        my_action_last_round = bool(history.my_actions[-1])
        for i, opponent_defected in enumerate(opponent_actions_last_round == 0):
            if opponent_defected and my_action_last_round:
                self.opponent_defection_streaks[i] += 1
                if self.opponent_defection_streaks[i] > 2:
                    action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_415(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with contributions to the public good while adapting to opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defection_threshold = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        average_payoff_per_cooperator = np.mean(history.my_payoffs[-1] if history.my_actions[-1] else 0 + self.game_description.k / self.game_description.n_players * opponent_cooperators)
        if opponent_cooperators >= self.game_description.n_players // 2:
            return Action.C
        elif average_payoff_per_cooperator > 1:
            return random.choice([Action.C, Action.D], p=[self.game_description.k / self.game_description.n_players, 1 - self.game_description.k / self.game_description.n_players])
        else:
            self.defection_threshold += 1
            if self.defection_threshold > 3:
                return Action.D
            elif history.my_actions[-1] and (not all(history.opponent_actions[-1, :])):
                self.defection_threshold += 1
            return random.choice([Action.C, Action.D])

class Strategy_COLLECTIVE_416(BaseStrategy):
    """
    Adaptive collective strategy for N-player public goods game.
    Cooperates in early rounds and responds to changes in contribution rates.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def calculate_avg_contributions(self, history: PlayerHistory) -> float:
        contributions = sum(history.opponent_actions[-1, :] + (history.my_actions[-1],))
        return contributions / self.n

    def find_prev_defect(self, history: PlayerHistory) -> int | None:
        for i in range(len(history.my_actions) - 2, -1, -1):
            if not history.my_actions[i]:
                return i
        return None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_contributions_last_round = self.calculate_avg_contributions(history)
        if state.round_number < self.r / 2:
            if avg_contributions_last_round > 0.5:
                return Action.C
            else:
                p = 0.5 - self.k / self.n * avg_contributions_last_round
                return Action.D if random.random() < p else Action.C
        elif state.round_number >= self.r / 2 and state.round_number < self.r - 1:
            prev_defect_t = self.find_prev_defect(history)
            if prev_defect_t is None or avg_contributions_last_round >= self.calculate_avg_contributions(PlayerHistory(my_actions=history.my_actions[:prev_defect_t + 1], my_payoffs=history.my_payoffs[:prev_defect_t + 1], opponent_actions=history.opponent_actions[:prev_defect_t + 1, :], opponent_payoffs=history.opponent_payoffs[:prev_defect_t + 1, :])):
                return Action.C
            else:
                p = 0.5 - self.k / self.n * avg_contributions_last_round
                return Action.D if random.random() < p else Action.C
        elif state.round_number == self.r - 1:
            return Action.C
        if all((not action for action in history.opponent_actions[-1, :])):
            return Action.C if random.random() < 0.5 + self.k / self.n else Action.D
        elif sum(history.opponent_actions[-1, :] + (history.my_actions[-1],)) == 1:
            return Action.D if random.random() < 0.5 + self.k / self.n else Action.C
        return Action.C

class Strategy_COLLECTIVE_417(BaseStrategy):
    """
    Adaptive Cooperative Escalation (ACE) strategy for the N-Player Public Goods Game.
    Balances individual self-interest with collective well-being by escalating cooperation based on observed behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T = 0
        self.previous_round_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :])
        if previous_round_cooperators > 0:
            return Action.C
        self.T = (self.previous_round_cooperators + 1) / self.game_description.n_players
        if history.my_payoffs[-1] > np.mean(history.opponent_payoffs[-1, :]):
            return Action.D
        else:
            return Action.C
        if sum(1 - history.opponent_actions[-1, :]) > self.game_description.n_players // 2:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if any(history.opponent_actions[-2, :]):
                return Action.C
            else:
                return Action.D
        self.previous_round_cooperators = previous_round_cooperators
        return Action.C

class Strategy_COLLECTIVE_418(BaseStrategy):
    """
    Adaptive Reciprocity with Forgiveness strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.forgiveness_parameter = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        prev_round_payoffs = history.opponent_payoffs[-1, :]
        prev_coop_count = sum(prev_round_actions)
        avg_payoff = np.mean(prev_round_payoffs)
        if history.my_payoffs[-1] >= avg_payoff:
            self.forgiveness_parameter += 0.05
        else:
            self.forgiveness_parameter -= 0.05
        self.forgiveness_parameter = max(0, min(self.forgiveness_parameter, 1))
        if prev_coop_count >= self.game_description.n_players / 2:
            return Action.C
        else:
            probability_of_cooperation = self.forgiveness_parameter * self.game_description.k / self.game_description.n_players / (1 + self.game_description.k / self.game_description.n_players)
            if random.random() < probability_of_cooperation:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_419(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Social Learning (ACSL)

    This strategy balances cooperation with adaptability, making it robust to various opponent behaviors while promoting a collective mindset of cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_defections = 0
        self.cooperation_probability = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        total_payoff_coop = self.game_description.k / self.game_description.n_players * num_cooperators
        if num_cooperators >= self.game_description.n_players // 2:
            return Action.C
        elif total_payoff_coop >= 1 - history.my_actions[-1]:
            return Action.C
        else:
            self.consecutive_defections += 1
            if random.random() < self.cooperation_probability:
                return Action.C
            else:
                if num_cooperators < self.game_description.n_players // 2 and self.cooperation_probability < 1.0:
                    self.cooperation_probability = min(1.0, self.cooperation_probability + 0.1)
                if self.consecutive_defections >= self.game_description.n_rounds // 2:
                    return Action.C
                else:
                    return Action.D

class Strategy_COLLECTIVE_420(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperate-Defect (ACD)

    The ACD strategy aims to balance individual payoff maximization with collective welfare,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_threshold = game_description.n_players // 2
        self.punishment_probability = 0.5
        self.exploration_probability = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        total_cooperators = sum(opponent_actions)
        total_defectors = self.game_description.n_players - 1 - total_cooperators
        pi_C = self.game_description.k / self.game_description.n_players * total_cooperators + (total_defectors > 0)
        pi_D = (total_cooperators > 0) + (1 - self.game_description.k / self.game_description.n_players * total_cooperators)
        if pi_C > pi_D:
            action = Action.C
        else:
            action = Action.D
        if total_cooperators < self.punishment_threshold and random.random() < self.punishment_probability:
            action = Action.C
        if random.random() < self.exploration_probability:
            action = random.choice([Action.C, Action.D])
        return action

class Strategy_COLLECTIVE_421(BaseStrategy):
    """
    Collective Strategy: "Adaptive Reciprocity with Endgame Cooperation"
    
    Our strategy aims to balance individual payoff maximization with collective cooperation, 
    adapting to opponent behaviors while ensuring robustness against a wide range of actions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.game_description.n_players:
            return Action.C
        opponent_prev_action = history.opponent_actions[-1, 0] if history else True
        if state.round_number < self.game_description.n_rounds - 1:
            probability = self.game_description.k / self.game_description.n_players
            if opponent_prev_action:
                return Action.C if random.random() < probability else Action.D
            else:
                return Action.D if random.random() < 1 - probability else Action.C
        return Action.C

class Strategy_COLLECTIVE_422(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective success.
    It adapts to the group's behavior, initially cooperating and then deciding based on a dynamic threshold.
    The forgiveness mechanism prevents retaliatory cycles and encourages players to return to cooperation after temporary setbacks.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_thresholds = []
        self.forgiveness_counts = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_actions = history.opponent_actions[-1, :] if history else []
        cooperation_ratio = sum(previous_round_actions) / self.game_description.n_players
        dynamic_threshold = self.game_description.k / self.game_description.n_players + (1 - self.game_description.k / self.game_description.n_players) * cooperation_ratio
        for player, action in enumerate(history.opponent_actions[-1, :]):
            if not action and history.my_payoffs.shape[0] > 2:
                self.forgiveness_counts[player] += 1
                if self.forgiveness_counts[player] >= 2:
                    return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            previous_cooperators = sum(history.opponent_actions[-2, :])
            if previous_cooperators >= self.game_description.n_players // 2:
                return Action.C
        if cooperation_ratio > dynamic_threshold or sum(previous_round_actions) == self.game_description.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_423(BaseStrategy):
    """
    Adaptive Cooperation with Punishment (ACP) strategy.
    
    Initially cooperates to establish a cooperative tone, then adapts its behavior based on the number of cooperators and defectors in previous rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_flag = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1]
        c_count = sum(prev_round_actions)
        d_count = self.game_description.n_players - 1 - c_count
        if self.punishment_flag and (not c_count > d_count + 1):
            return Action.D
        p_defect = (1 - self.game_description.k / self.game_description.n_players) ** c_count
        random_defection = random.random() < p_defect
        if c_count > d_count:
            return Action.C
        elif c_count <= d_count and (not random_defection):
            return Action.C
        else:
            self.punishment_flag = True
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_424(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Gradual Concession.
    
    This strategy balances individual self-interest with collective well-being by adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.game_description.n_players:
            return Action.C
        if history is not None:
            acr = np.mean(history.opponent_actions[max(0, state.round_number - self.game_description.n_players):state.round_number])
        if state.round_number < self.game_description.n_rounds - 1:
            if acr >= (self.game_description.k - 1) / self.game_description.k:
                return Action.C
            else:
                p_defect = (1 - acr) / (2 - acr)
                return Action.D if random.random() < p_defect else Action.C
        if history is not None and state.round_number == self.game_description.n_rounds - 1:
            total_payoff = np.sum(history.my_payoffs)
            expected_final_payoff = self.game_description.k / self.game_description.n_players * (self.game_description.n_players - 1)
            return Action.C if total_payoff > expected_final_payoff else Action.D
        return Action.D

class Strategy_COLLECTIVE_425(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Self-Interest (ACSI)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate = True
        self.total_cooperators_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.total_cooperators_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if self.total_cooperators_prev >= self.game_description.n_players / 2:
            self.cooperate = True
        else:
            self.cooperate = False
        if history.my_actions[-1] and self.total_cooperators_prev > self.game_description.n_players / 2:
            self.cooperate = False
        if state.round_number >= int(0.9 * self.game_description.n_rounds):
            expected_payoff_from_cooperation = self.game_description.k / self.game_description.n_players * self.total_cooperators_prev
            private_payoff_from_defection = 1
            if expected_payoff_from_cooperation > private_payoff_from_defection:
                self.cooperate = True
        return Action.C if self.cooperate else Action.D

class Strategy_COLLECTIVE_426(BaseStrategy):
    """
    A collective strategy that promotes cooperation by adapting to the game's history.
    
    In the initial round, cooperate to encourage cooperation and create a positive initial condition.
    In subsequent rounds, calculate a cooperation threshold based on the previous round's total contributions.
    If this threshold exceeds 0.5, cooperate again; otherwise, defect.
    In the final round, defect to maximize individual payoff.
    
    This strategy adapts to potential exploitation by increasing the cooperation threshold if a significant deviation from expected cooperation level is observed.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold_adjustment = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_contributions = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history.my_actions.size > 0 else 0)
        cooperation_threshold = self.game_description.k / self.game_description.n_players * (previous_round_contributions / self.game_description.n_players)
        if previous_round_contributions < self.game_description.n_players * 0.8:
            cooperation_threshold += self.threshold_adjustment
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if cooperation_threshold > 0.5 else Action.D

class Strategy_COLLECTIVE_427(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Public Goods Awareness (ATTPGA) strategy.
    
    Encourages cooperation in early rounds, adapts to changes in cooperation rates 
    and total payoffs, and fosters cooperation through the tit-for-tat mechanism.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_c = 0.0
        self.total_payoff = 0.0
        self.adj_factor = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_players = self.game_description.n_players
        c_prev = np.sum(history.opponent_actions[-1, :]) / n_players
        self.avg_c += (c_prev - self.avg_c) / state.round_number
        self.total_payoff += history.my_payoffs[-1]
        if c_prev > self.avg_c:
            action = Action.C
        else:
            action = Action.D
        expected_payoff = self.game_description.k * n_players
        payoff_diff = expected_payoff - self.total_payoff
        self.adj_factor = 1 + payoff_diff / (self.game_description.k * n_players)
        if random.random() < self.adj_factor:
            action = Action.C
        return action

class Strategy_COLLECTIVE_428(BaseStrategy):
    """
    The Adaptive Cooperative Threshold (ACT) strategy is designed to balance individual 
    self-interest with collective cooperation, adapting to the evolving game dynamics and 
    opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.CR = 0.0
        self.theta = min(0.5, game_description.k / game_description.n_players)
        self.epsilon = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if history.my_actions is not None and len(history.my_actions) > 0:
            self.CR = sum(history.my_actions) / state.round_number
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if random.random() < self.epsilon:
            return random.choice([Action.C, Action.D])
        if self.CR > self.theta or (self.CR == self.theta and random.random() < 0.5):
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_429(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with History-Dependent Adjustment.
    
    This strategy promotes cooperation by initially cooperating, 
    then adapting to the game's history through a dynamic cooperation threshold. 
    It also includes a punishment mechanism to discourage defection when others cooperate.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0
        self.punishment_rounds = 0
        self.opponents_to_punish = set()
        self.previous_cooperation_rate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        if history is not None and history.opponent_actions.size > 0:
            previous_round_cooperation_rate = np.mean(history.opponent_actions[-1, :])
            self.previous_cooperation_rate = previous_round_cooperation_rate
            self.cooperation_threshold = self.game_description.k / self.game_description.n_players * (np.mean(history.my_payoffs[:state.round_number]) + np.mean(history.opponent_payoffs[:, :state.round_number]))
        if history is not None and history.opponent_actions.size > 0:
            for opponent_index in range(self.game_description.n_players - 1):
                if not history.opponent_actions[-1, opponent_index] and previous_round_cooperation_rate >= self.cooperation_threshold / 2:
                    self.opponents_to_punish.add(opponent_index)
                    self.punishment_rounds = min(state.round_number // 4, 3)
        if history is not None and history.opponent_actions.size > 0:
            if previous_round_cooperation_rate >= self.cooperation_threshold and state.round_number < self.game_description.n_rounds - 1:
                return Action.C
            for opponent_index in list(self.opponents_to_punish):
                if self.punishment_rounds > 0:
                    self.punishment_rounds -= 1
                    break
                else:
                    self.opponents_to_punish.remove(opponent_index)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_430(BaseStrategy):
    """
    A collective strategy called "Adaptive Tit-for-Tat with Public Goods Sensitivity" (ATTPGS).
    This strategy aims to balance individual self-interest with the desire to contribute to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        cooperation_threshold = self.game_description.n_players // 2
        if self.game_description.k > (self.game_description.n_players - 1) / 4:
            cooperation_threshold = self.game_description.n_players // 3
        if opponent_cooperators >= cooperation_threshold and self.game_description.k > (self.game_description.n_players - 1) / 2:
            return Action.C
        elif opponent_cooperators == self.game_description.n_players // 2:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_431(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with cooperation to maximize overall payoff.
    It adapts to changing cooperation rates and prioritizes cooperation when it is likely to benefit from collective action.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = game_description.k / game_description.n_players * (game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_coop_prev = np.mean(history.opponent_actions[-1, :])
        if avg_coop_prev >= self.theta and state.round_number < self.game_description.n_rounds - 1:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_432(BaseStrategy):
    """
    A collective strategy prioritizing cooperation and mutual benefit while incorporating 
    a punishment mechanism to deter non-cooperative behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.previous_round_defectors = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        if self.previous_round_defectors is not None and len(self.previous_round_defectors) > 0:
            return Action.D
        if opponent_cooperators >= self.game_description.n_players / 2:
            action = Action.C
        else:
            action = Action.D
            self.previous_round_defectors = [i for i, x in enumerate(history.opponent_actions[-1, :]) if not x]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_433(BaseStrategy):
    """
    Balances individual self-interest with collective welfare by adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.payoff_threshold = 0.75 * (game_description.k - 1 / game_description.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_coop_prev = self.calculate_avg_coop(history)
        if avg_coop_prev >= 0.5:
            action = Action.C
        else:
            action = Action.D
        my_payoff = history.my_payoffs[-1]
        if my_payoff < self.payoff_threshold:
            avg_opponent_payoff = np.mean(history.opponent_payoffs[-1, :])
            if my_payoff < avg_opponent_payoff:
                action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            total_my_payoff = np.sum(history.my_payoffs)
            if total_my_payoff < self.payoff_threshold * self.game_description.n_rounds:
                return Action.D
        return action

    def calculate_avg_coop(self, history: PlayerHistory) -> float:
        opponent_actions = history.opponent_actions[-1, :]
        coop_actions = np.sum(opponent_actions)
        return coop_actions / self.game_description.n_players - 1

class Strategy_COLLECTIVE_434(BaseStrategy):
    """
    Collective strategy "Adaptive Tit-for-Tat with Gradual Cooperation" for N-Player Public Goods Game.
    Balances individual self-interest with collective well-being by adapting to opponent behaviors and promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_cooperators = 0
        self.cooperation_threshold = math.ceil(game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 and history is None:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if previous_cooperators >= self.cooperation_threshold:
            action = Action.C
            self.consecutive_cooperators += 1
        else:
            action = Action.D
            self.consecutive_cooperators = 0
        if self.consecutive_cooperators > math.ceil(self.game_description.n_rounds / 2):
            self.cooperation_threshold += 1
        elif self.consecutive_cooperators < math.floor(self.game_description.n_rounds / 4):
            self.cooperation_threshold = max(math.ceil(self.game_description.k), self.cooperation_threshold - 1)
        return action

class Strategy_COLLECTIVE_435(BaseStrategy):
    """
    Adaptive Cooperative Threshold (ACT) strategy for N-Player Public Goods Game.
    
    This strategy adapts to various opponent behaviors and aligns with the game's parameters.
    It prioritizes cooperation while adapting to the group's behavior, encouraging cooperation 
    without being overly exploitable. The punishment mechanism discourages free-riding, promoting 
    a more equitable distribution of payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T = 0.5

    def update_threshold(self, prev_cooperators):
        return prev_cooperators / self.game_description.n_players * 0.9 + self.T * 0.1

    def calculate_expectations(self, prev_cooperators):
        E_coop = self.game_description.k / self.game_description.n_players * prev_cooperators + 0.5
        E_defect = 1 + self.game_description.k / self.game_description.n_players * prev_cooperators - 0.5
        return (E_coop, E_defect)

    def should_cooperate(self, E_coop, E_defect):
        return E_coop > E_defect and random.random() < self.T

    def should_punish(self, prev_cooperators):
        punishment_prob = max(0, (self.T - prev_cooperators / self.game_description.n_players) * 2)
        return random.random() < punishment_prob

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.T = 0.5
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :])
        self.T = self.update_threshold(prev_cooperators)
        E_coop, E_defect = self.calculate_expectations(prev_cooperators)
        if self.should_punish(prev_cooperators):
            return Action.D
        elif self.game_description.n_players == 2:
            if history.opponent_actions[-1, 0]:
                return Action.C
            else:
                return Action.D
        elif self.should_cooperate(E_coop, E_defect):
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_436(BaseStrategy):
    """
    Adaptive Collective Conscience (ACC) strategy for the N-Player Public Goods Game.
    
    This strategy balances individual self-interest with collective well-being by adaptively adjusting cooperation levels based on game history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number <= 3:
            return Action.C if random.random() < self.cooperation_probability else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        prev_payoffs = history.my_payoffs[-1] + np.sum(history.opponent_payoffs[-1, :])
        avg_prev_payoff = prev_payoffs / self.game_description.n_players
        cooperate = avg_prev_payoff >= self.game_description.k
        if state.round_number > 1:
            total_payoff_current_round = history.my_payoffs[-1] + np.sum(history.opponent_payoffs[-1, :])
            total_payoff_prev_round = history.my_payoffs[-2] + np.sum(history.opponent_payoffs[-2, :])
            if total_payoff_current_round > total_payoff_prev_round:
                self.cooperation_probability = min(0.9, self.cooperation_probability + 0.1)
            else:
                self.cooperation_probability = max(0.1, self.cooperation_probability - 0.05)
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_437(BaseStrategy):
    """
    This strategy, 'Adaptive Cooperativity with Social Learning', aims to balance individual payoffs 
    with collective well-being while adapting to diverse opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        if num_cooperators_prev_round >= self.game_description.n_players // 2:
            action = Action.C
        else:
            action = Action.D if random.random() < self.p else Action.C
        payoff_i_t = 1 - int(action == Action.C) + self.game_description.k / self.game_description.n_players * num_cooperators_prev_round
        avg_defector_payoff = np.mean(history.opponent_payoffs[-1, history.opponent_actions[-1, :] == 0])
        avg_cooperator_payoff = np.mean(history.opponent_payoffs[-1, history.opponent_actions[-1, :] == 1])
        if payoff_i_t > avg_defector_payoff:
            self.p = min(self.p + 0.1, self.game_description.k / self.game_description.n_players)
        elif payoff_i_t < avg_cooperator_payoff:
            self.p = max(self.p - 0.1, 0)
        return action

class Strategy_COLLECTIVE_438(BaseStrategy):
    """
    A mixed strategy combining elements of reciprocity, cooperation incentives, and exploitation detection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.r_initial = 3
        self.r_final = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < self.r_initial:
            return Action.C
        if state.round_number >= self.game_description.n_rounds - self.r_final:
            return Action.D
        previous_actions = history.opponent_actions[-1, :]
        cooperators = sum(previous_actions)
        if cooperators > len(previous_actions) / 2:
            self.cooperating = True
            return Action.C
        my_payoff = history.my_payoffs[-1]
        max_possible_payoff = self.game_description.k / self.game_description.n_players * self.game_description.n_players
        if my_payoff < max_possible_payoff and cooperators > len(previous_actions) / 2:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_440(BaseStrategy):
    """
    Adaptive Collective Maximization (ACM) strategy.
    
    This strategy initially cooperates, then observes opponents' actions. 
    It adapts by cooperating if most opponents cooperate and defecting otherwise.
    A punishment mechanism is implemented to deter widespread defection.
    In the endgame phase, it switches to cooperation if the average cooperation rate is high enough.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.rounds_cooperated = 0
        self.cooperation_threshold = game_description.n_players // 2
        self.observation_round = True
        self.punishment_trigger = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.observation_round:
            self.observation_round = False
            observed_cooperation_rate = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
            if observed_cooperation_rate >= 0.5:
                return Action.C
            else:
                return Action.D
        cooperators = sum(history.opponent_actions[-1, :])
        if cooperators < self.cooperation_threshold and history.my_payoffs[-1] < self.game_description.k / self.game_description.n_players * cooperators:
            self.punishment_trigger = True
        if self.punishment_trigger:
            return Action.D
        elif cooperators >= self.cooperation_threshold:
            return Action.C
        else:
            self.rounds_cooperated += 1
            if self.rounds_cooperated % 2 == 0:
                return Action.C
            else:
                return Action.D
        if state.round_number > self.game_description.n_rounds - self.game_description.n_rounds // 4:
            average_cooperation_rate = sum(history.my_actions) / (state.round_number + 1)
            if average_cooperation_rate > 0.5:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_441(BaseStrategy):
    """
    Adaptive Reciprocity with Endgame Cooperation strategy.
    Balances individual self-interest with collective well-being by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.w = min(self.n_rounds - 1, 5)
        self.theta = 0.6
        self.m = min(self.n_rounds // 2, 5)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_actions = np.array([action == Action.C for action in history.my_actions])
        opponent_actions = history.opponent_actions
        avg_coop_rate = self.calculate_average_cooperation(opponent_actions, my_actions, self.w)
        if state.round_number > self.n_players:
            if avg_coop_rate > self.theta:
                action = Action.C
            elif self.was_punished(my_actions, opponent_actions):
                action = Action.D
            else:
                action = Action.D
        else:
            action = Action.C
        if state.round_number >= self.n_rounds - self.m + 1:
            action = Action.C
        return action

    def calculate_average_cooperation(self, opponent_actions, my_actions, w):
        coop_rates = []
        for i in range(max(0, len(my_actions) - w), len(my_actions)):
            if not np.all(opponent_actions[i, :] == False):
                coop_rate = sum((1 for action in opponent_actions[i, :] if action)) / self.n_players
                coop_rates.append(coop_rate)
        return np.mean(coop_rates) if coop_rates else 0

    def was_punished(self, my_actions, opponent_actions):
        prev_round = opponent_actions[-2]
        return any((not action for action in prev_round)) and my_actions[-1] == True

class Strategy_COLLECTIVE_442(BaseStrategy):
    """
    Adaptive Cooperative Tit-for-Tat (ACT4T) strategy for N-Player Public Goods Game.
    
    This strategy adapts to the collective behavior of other players, rewarding cooperation and punishing defection.
    It encourages cooperation through initial cooperation and tit-for-tat reciprocity, fostering a culture of mutual cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialize ACT4T strategy with game description.

        Args:
            game_description (PublicGoodsDescription): Game parameters (n_players, n_rounds, k).
        """
        self.game_description = game_description
        self.last_round_coop_rate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide action based on current game state and history.

        Args:
            state (GameState): Current round number.
            history (None | PlayerHistory): Past actions and payoffs of all players.

        Returns:
            Action: Chosen action (C or D).
        """
        if state.round_number == 0:
            return Action.C
        prev_round_coop_rate = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        self.last_round_coop_rate = prev_round_coop_rate
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if sum(history.opponent_actions[-1, :] == False) == 1:
            return Action.C
        if prev_round_coop_rate >= self.game_description.k / self.game_description.n_players:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_443(BaseStrategy):
    """
    A collective strategy that adapts to various opponent behaviors while promoting cooperation.
    It balances individual self-interest with collective well-being, making it a robust and adaptive approach for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.forgiveness_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        cooperators = sum(history.opponent_actions[-1, :])
        defectors = self.game_description.n_players - 1 - cooperators
        if history.my_actions[-1] == False:
            self.forgiveness_counter += 1
        else:
            self.forgiveness_counter = 0
        if cooperators >= defectors or cooperators + self.forgiveness_counter >= defectors:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_444(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with group cooperation,
    adapting to the game's history and robust against various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_actions = history.opponent_actions[-1, :]
        prev_payoffs = history.my_payoffs[-1]

        def most_cooperated(actions):
            return sum(actions) > self.game_description.n_players / 2

        def punished_defector(actions, payoffs):
            if most_cooperated(actions):
                for i in range(self.game_description.n_players - 1):
                    if not actions[i]:
                        return True
            return False

        def prevent_exploitation(actions, payoffs):
            cooperators = sum(actions)
            if cooperators == self.game_description.n_players:
                return False
            shared_payoff = self.game_description.k / self.game_description.n_players * cooperators
            return shared_payoff > prev_payoffs

        def shared_payoff_increase():
            cooperators = sum(prev_actions)
            new_shared_payoff = self.game_description.k / self.game_description.n_players * (cooperators + 1)
            return new_shared_payoff > prev_payoffs
        if most_cooperated(prev_actions):
            action = Action.C
        elif punished_defector(prev_actions, prev_payoffs):
            action = Action.D
        elif prevent_exploitation(prev_actions, prev_payoffs):
            action = Action.C if shared_payoff_increase() else Action.D
        else:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_COLLECTIVE_445(BaseStrategy):
    """
    Adaptive Cooperative Threshold Strategy.
    
    This strategy balances individual self-interest with collective well-being, 
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.0
        self.prev_round_cooperators = 0
        self.alpha = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.k > self.game_description.n_players / 2 else Action.D
        n_cooperators = sum(history.opponent_actions[-1, :] != False)
        self.threshold = (1 - self.alpha) * self.threshold + self.alpha * n_cooperators / self.game_description.n_players
        if n_cooperators >= int(self.threshold):
            return Action.C
        else:
            avg_defector_payoff = np.mean(history.opponent_payoffs[-1, :][history.opponent_actions[-1, :] == False])
            my_last_round_payoff = history.my_payoffs[-1]
            return Action.D if avg_defector_payoff > my_last_round_payoff else Action.C
        self.prev_round_cooperators = n_cooperators

class Strategy_COLLECTIVE_446(BaseStrategy):
    """
    The Adaptive Cooperative Fingerprint (ACF) strategy for N-Player Public Goods Game.
    Aims to balance individual self-interest with cooperation to achieve higher total payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_fingerprints = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number <= 3:
            return Action.C
        if self.cooperation_fingerprints is None:
            opponent_cooperations = np.mean(history.opponent_actions[:4, :], axis=0)
            self.cooperation_fingerprints = opponent_cooperations
        most_recent_round_actions = history.opponent_actions[-1, :]
        cooperators_in_last_round = np.sum(most_recent_round_actions)
        if cooperators_in_last_round >= self.game_description.n_players // 2:
            return Action.C
        elif cooperators_in_last_round < self.game_description.n_players // 2:
            return Action.D
        total_contributions = np.sum(most_recent_round_actions)
        self_interest_balance = self.game_description.k / self.game_description.n_players * total_contributions - 1
        if self_interest_balance > 0:
            return Action.C
        return Action.D

    def update(self):
        pass

class Strategy_COLLECTIVE_447(BaseStrategy):
    """
    A collective strategy called "Adaptive Cooperation with Gradual Concession" (ACGC).
    ACGC aims to balance cooperation and self-interest while adapting to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate = True
        self.total_payoff_prev = 0
        self.num_cooperators_prev = 0

    def calculate_total_payoff(self, cooperate: bool, num_cooperators: int) -> float:
        return 1 - int(cooperate) + self.game_description.k / self.game_description.n_players * num_cooperators

    def count_cooperators(self, history: PlayerHistory) -> int:
        opponent_actions = history.opponent_actions[-1, :]
        return sum(opponent_actions)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.cooperate = True
            return Action.C
        if self.total_payoff_prev >= 0.5 * self.game_description.k:
            self.cooperate = True
        elif self.num_cooperators_prev < self.game_description.n_players / 2:
            self.cooperate = False
        else:
            self.cooperate = True
        if not self.cooperate and self.num_cooperators_prev > self.game_description.n_players / 2:
            self.cooperate = True
        if state.round_number == self.game_description.n_rounds - 1:
            self.cooperate = True
        if history is not None:
            self.total_payoff_prev = self.calculate_total_payoff(self.cooperate, self.count_cooperators(history))
            self.num_cooperators_prev = self.count_cooperators(history)
        return Action.C if self.cooperate else Action.D

class Strategy_COLLECTIVE_449(BaseStrategy):
    """
    Collective Strategy: Adaptive Tit-for-Tat with Community-Sensing.

    This strategy aims to balance individual self-interest with collective well-being by adapting to the community's behavior while encouraging cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if self.game_description.n_players == 2:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :])
        CCL = cooperators / self.game_description.n_players
        if CCL >= 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_450(BaseStrategy):
    """
    Adaptive Cooperation with Punishment strategy for the N-Player Public Goods Game.
    
    This strategy prioritizes cooperation while adapting to the behavior of other players. 
    By cooperating in the first round and punishing defectors, we promote a cooperative atmosphere.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def adaptive_cooperation(self, history: PlayerHistory) -> Action:
        prev_round = history.opponent_actions[-1]
        coop_count = sum((1 for action in prev_round if action))
        maj_coop = coop_count > self.game_description.n_players / 2
        if maj_coop:
            return Action.C
        else:
            p_defect = 1 - self.game_description.k / self.game_description.n_players * coop_count
            return Action.D if random.random() < p_defect else Action.C

    def punishment_mechanism(self, history: PlayerHistory) -> Action | None:
        for i, action in enumerate(history.opponent_actions[-1]):
            if not action and (history.my_actions[-2] if len(history.my_actions) > 1 else True):
                return Action.D if random.random() < 0.5 else Action.C
        return None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number >= self.game_description.n_rounds - 1:
            return Action.D
        punishment_action = self.punishment_mechanism(history)
        if punishment_action is not None:
            return punishment_action
        return self.adaptive_cooperation(history)

class Strategy_COLLECTIVE_451(BaseStrategy):
    """
    Adaptive Collective Optimism strategy for Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.8
        self.punish_defect_prob = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        prev_opponent_actions = history.opponent_actions[-1, :]
        num_cooperators = sum(prev_opponent_actions)
        if state.round_number <= 5:
            reciprocal_altruism_prob = 0.9
        else:
            reciprocal_altruism_prob = 1
        if num_cooperators >= self.game_description.n_players / 2:
            return Action.C if random.random() < reciprocal_altruism_prob else Action.D
        elif num_cooperators < self.game_description.n_players / 2 and (not all(prev_opponent_actions == False)):
            return Action.D if random.random() < self.punish_defect_prob else Action.C
        elif all(prev_opponent_actions == False):
            return Action.C if random.random() < 0.5 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.C

class Strategy_COLLECTIVE_452(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Gradual Concession.

    This strategy aims to balance individual self-interest with cooperation to achieve a mutually beneficial outcome.
    It adapts to the average cooperation rate of all players in the previous round and gradually concedes by increasing
    the probability of defection as the game progresses and the average cooperation rate remains low.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = game_description.k / game_description.n_players / 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_cooperation_rate = np.mean(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        p = (self.game_description.n_rounds - state.round_number) / self.game_description.n_rounds
        if avg_cooperation_rate >= self.theta:
            return Action.C
        else:
            return Action.D if random.random() < p else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_453(BaseStrategy):
    """
    Adaptive Cooperativeness (AC) strategy for N-Player Public Goods Game.
    Balances individual self-interest with collective well-being by adapting to the game's history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        previous_round_cooperation = np.sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        self.cooperation_history.append(previous_round_cooperation)
        if previous_round_cooperation >= 0.5:
            cooperate_probability = 1 - math.pow(1 - previous_round_cooperation, self.game_description.k / self.game_description.n_players)
        else:
            cooperate_probability = 0
        return Action.C if random.random() < cooperate_probability else Action.D

class Strategy_COLLECTIVE_455(BaseStrategy):
    """
    Collective Strategy: Adaptive Reciprocal Altruism (ARA)

    ARA is a collective strategy that balances individual self-interest with cooperative behavior,
    adapting to the game's history and opponent actions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = (game_description.k - 1) / game_description.n_players
        self.epsilon = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        avg_coop_rate = np.mean(np.concatenate((history.my_actions[:-1], history.opponent_actions[-1, :])))
        action = Action.D if avg_coop_rate < self.theta else Action.C
        for opponent_action in history.opponent_actions[-1, :]:
            if not opponent_action:
                action = Action.D
                break
        if random.random() < self.epsilon:
            action = random.choice([Action.C, Action.D])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_456(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation (AC)
    
    This strategy aims to balance individual payoff maximization with collective welfare by adapting to opponents' behaviors, punishing defectors, and promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.c_prev = 0
        self.avg_contribution_rate = 0
        self.w = 3
        self.theta = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        elif state.round_number < self.w:
            opponent_actions = history.opponent_actions[-1]
            if any(opponent_actions):
                action = Action.C
            else:
                action = Action.D
        else:
            avg_contribution_rates = np.mean(history.my_actions[max(0, state.round_number - self.w):state.round_number], axis=0)
            self.avg_contribution_rate = np.mean(avg_contribution_rates)
            if self.avg_contribution_rate > self.theta:
                action = Action.C
            else:
                c_prev = np.sum(history.opponent_actions[-1]) / n_players
                self.c_prev = c_prev
                if c_prev >= n_players / 2:
                    action = Action.C
                else:
                    action = Action.D
        opponent_defected = history.opponent_actions[-1].any() and (not history.my_actions[-1])
        if opponent_defected and self.c_prev == 1:
            action = Action.D
        if state.round_number == r - 1:
            action = Action.D
        return action

class Strategy_COLLECTIVE_457(BaseStrategy):
    """
    The ACD strategy balances individual self-interest with collective well-being 
    by adapting to the game's history and opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = min((game_description.n_players - 1) / game_description.k, 0.5)
        self.rounds_to_average = min(game_description.n_rounds // 2, 5)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_cooperators_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if total_cooperators_prev >= self.cooperation_threshold * self.game_description.n_players:
            return Action.C
        if state.round_number < self.rounds_to_average + 1:
            avg_payoff_diff = 0
        else:
            recent_coop_rounds = np.argwhere(history.my_actions[-self.rounds_to_average - 1:-1]).flatten()
            coop_avg_payoff = np.mean(history.my_payoffs[recent_coop_rounds])
            recent_defect_rounds = np.argwhere(~history.my_actions[-self.rounds_to_average - 1:-1]).flatten()
            defect_avg_payoff = np.mean(history.my_payoffs[recent_defect_rounds])
            avg_payoff_diff = coop_avg_payoff - defect_avg_payoff
        if avg_payoff_diff > 0:
            self.cooperation_threshold -= 0.05
        elif avg_payoff_diff < 0:
            self.cooperation_threshold += 0.05
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_458(BaseStrategy):
    """
    Adaptive Cooperation balances individual self-interest with collective well-being.
    It dynamically adjusts cooperation levels based on game history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_threshold = game_description.k / game_description.n_players
        self.cooperation_threshold_adjustment = 0.1
        self.threshold_history = [self.avg_payoff_threshold]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < math.ceil(self.game_description.n_players / 2):
            return Action.C
        avg_payoff_prev_rounds = self.calculate_average_payoff(history, state.round_number - 1)
        if avg_payoff_prev_rounds >= self.avg_payoff_threshold:
            action = Action.C
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            num_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            if num_cooperators_prev_round >= self.game_description.n_players / 2:
                action = Action.C
        if state.round_number % math.floor(self.game_description.n_players / 4) == 0:
            self.adjust_threshold(avg_payoff_prev_rounds, history)
        return action

    def calculate_average_payoff(self, history: PlayerHistory, round_num: int) -> float:
        total_payoff = sum(history.opponent_payoffs[round_num]) + history.my_payoffs[round_num]
        return total_payoff / self.game_description.n_players

    def adjust_threshold(self, avg_payoff_prev_rounds: float, history: PlayerHistory):
        num_cooperators = sum(history.opponent_actions[-1, :])
        if num_cooperators > self.game_description.n_players / 2:
            self.avg_payoff_threshold *= 1 + self.cooperation_threshold_adjustment
        elif num_cooperators < self.game_description.n_players / 2:
            self.avg_payoff_threshold *= 1 - self.cooperation_threshold_adjustment
        self.threshold_history.append(self.avg_payoff_threshold)

class Strategy_COLLECTIVE_459(BaseStrategy):
    """
    Collective strategy that balances individual payoff maximization with collective well-being by adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.memory = []
        self.threshold = 1.5
        self.punishment_window = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_payoff = np.mean(history.my_payoffs[-self.punishment_window:])
        if len(self.memory) >= self.punishment_window and state.round_number % self.punishment_window == 0:
            if avg_payoff > 1.5:
                self.threshold -= 0.1
            elif avg_payoff < 1.25:
                self.threshold += 0.1
        opponent_defections = np.sum(history.opponent_actions[-self.punishment_window:, :], axis=0)
        if np.any(opponent_defections > 1):
            return Action.D
        if avg_payoff >= self.threshold:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1 and np.mean(history.my_payoffs) >= 1.5:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_460(BaseStrategy):
    """
    Collective Adaptive Cooperation Strategy.

    This strategy focuses on balancing individual payoffs with collective welfare,
    adapting to the game's history and opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_ratio = 0.5
        self.threshold_cooperate = 0.6
        self.threshold_defect = 0.4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1]
        n_cooperators = sum(opponent_actions)
        average_payoff_cooperators = np.mean(history.my_payoffs[history.my_actions])
        average_payoff_defectors = np.mean(history.my_payoffs[~history.my_actions])
        if state.round_number > 2:
            recent_rounds_cooperate_ratio = np.sum(opponent_actions[-3:]) / (self.game_description.n_players * 3)
            if recent_rounds_cooperate_ratio > self.threshold_cooperate:
                self.cooperate_ratio *= 1.1
            elif recent_rounds_cooperate_ratio < self.threshold_defect:
                self.cooperate_ratio *= 0.9
        if average_payoff_cooperators > average_payoff_defectors or (average_payoff_cooperators == average_payoff_defectors and random.random() < self.cooperate_ratio):
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_COLLECTIVE_461(BaseStrategy):
    """
    An adaptive strategy that balances individual self-interest with collective cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.round_actions = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            prev_round_actions = history.opponent_actions[-1]
            self.round_actions = np.append(prev_round_actions, True)
            majority_action = self.get_majority_action(self.round_actions)
            if majority_action == Action.C:
                return Action.C
            elif self.is_punishment_needed(prev_round_actions, history):
                return Action.D
            elif self.can_exploit_greed(prev_round_actions, history):
                return Action.D
        return Action.C

    def get_majority_action(self, actions: NDArray[np.bool_]) -> Action:
        count_c = np.sum(actions)
        return Action.C if count_c >= len(actions) / 2 else Action.D

    def is_punishment_needed(self, prev_round_actions: NDArray[np.bool_], history: PlayerHistory) -> bool:
        opponent_payoffs = history.opponent_payoffs[-1]
        defector_payoff = np.max(opponent_payoffs[~prev_round_actions])
        avg_coop_payoff = np.mean(opponent_payoffs[prev_round_actions])
        return defector_payoff > avg_coop_payoff

    def can_exploit_greed(self, prev_round_actions: NDArray[np.bool_], history: PlayerHistory) -> bool:
        all_c = np.all(prev_round_actions)
        potential_defection_benefits = history.opponent_payoffs[-1] + self.game_description.k / self.game_description.n_players * np.sum(prev_round_actions)
        return np.any(potential_defection_benefits > history.opponent_payoffs[-1])

class Strategy_COLLECTIVE_462(BaseStrategy):
    """
    Adaptive Cooperative Tit-for-Tat (ACTT) strategy.
    
    This collective strategy aims to balance individual payoffs with collective well-being while adapting to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_coop_ratio_t_minus_2 = None
        self.opponent_coop_ratio_t_minus_1 = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            opponent_actions_t_minus_1 = history.opponent_actions[-1, :]
            total_cooperators_prev = np.sum(opponent_actions_t_minus_1) + (history.my_actions[-1] == 1)
            coop_ratio = total_cooperators_prev / self.game_description.n_players
            if self.opponent_coop_ratio_t_minus_2 is not None:
                self.opponent_coop_ratio_t_minus_2 = self.opponent_coop_ratio_t_minus_1
            self.opponent_coop_ratio_t_minus_1 = coop_ratio
            avg_opponent_coop_ratio = None
            if self.opponent_coop_ratio_t_minus_2 is not None:
                avg_opponent_coop_ratio = np.mean([self.opponent_coop_ratio_t_minus_1, self.opponent_coop_ratio_t_minus_2])
            if coop_ratio >= 0.5 or (state.round_number < self.game_description.n_rounds - 1 and avg_opponent_coop_ratio is not None and (avg_opponent_coop_ratio >= 0.5)):
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_463(BaseStrategy):
    """
    Adaptive Cooperative Threshold (ACT) strategy to balance individual self-interest with collective well-being.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = (game_description.k - 1) / (game_description.n_players - 1)
        self.window_size = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_cooperators = sum((1 for opponent_action in history.opponent_actions[-1, :] if opponent_action))
        proportion_cooperated = previous_cooperators / self.game_description.n_players
        if proportion_cooperated >= self.threshold:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif all((not opponent_action for opponent_action in history.opponent_actions[-2:, :].flatten())):
            return Action.C
        if state.round_number > self.window_size:
            my_average_payoff = np.mean(history.my_payoffs[-self.window_size:])
            opponent_average_payoffs = np.mean(history.opponent_payoffs[-self.window_size:, :], axis=0)
            if my_average_payoff < np.min(opponent_average_payoffs) - np.std(opponent_average_payoffs):
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_464(BaseStrategy):
    """
    A strategy that focuses on building trust and fostering cooperation while adapting to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_coop = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D]) if random.random() < self.p_coop else Action.D
        prev_actions = history.opponent_actions[-1, :]
        my_prev_action = history.my_actions[-1]
        if any(prev_actions):
            self.p_coop += 0.1
            self.p_coop = min(self.p_coop, 0.9)
        elif all((not action for action in prev_actions)):
            self.p_coop -= 0.2
            self.p_coop = max(self.p_coop, 0.1)
        if any(prev_actions) and (not my_prev_action):
            return Action.D
        if sum([action for action in prev_actions]) >= self.game_description.n_players / 2:
            self.p_coop += 0.05
            self.p_coop = min(self.p_coop, 0.95)
        if state.round_number > self.game_description.n_rounds * 0.8:
            return Action.C if random.random() < 0.8 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < self.p_coop else Action.D

class Strategy_COLLECTIVE_465(BaseStrategy):
    """
    A collective strategy that balances individual payoff maximization with collective welfare optimization.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.θ = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < self.game_description.n_rounds - 1:
            prev_round_coops = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
            if prev_round_coops / self.game_description.n_players >= self.θ:
                action = Action.C
            else:
                action = Action.D
            avg_payoff = np.mean(np.concatenate((history.my_payoffs[-1:], history.opponent_payoffs[-1, :])))
            if avg_payoff > 1 + self.game_description.k / self.game_description.n_players:
                self.θ -= 0.05
            else:
                self.θ += 0.05
            return action
        return Action.D

class Strategy_COLLECTIVE_466(BaseStrategy):
    """
    A collective strategy that adapts to the evolving behavior of opponents 
    and leverages reputation-based reciprocity in an N-player public goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.reputation = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        for i, action in enumerate(history.opponent_actions[-1]):
            if action:
                self.reputation[i] += 1
        num_cooperators = sum(history.opponent_actions[-1])
        p_C = 0.8 + self.game_description.k / self.game_description.n_players * (num_cooperators / self.game_description.n_players)
        if history.my_actions[-1]:
            return Action.C if random.random() < p_C else Action.D
        elif any((rep < 0.5 for rep in self.reputation)):
            p_D = 0.2 + (self.game_description.n_players - self.game_description.k) / self.game_description.n_players
            return Action.D if random.random() < p_D else Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_467(BaseStrategy):
    """
    A collective strategy that adapts to various opponent behaviors and is robust across different game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        T = self.game_description.k / self.game_description.n_players * (math.sqrt(self.game_description.n_players) - 1)
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        opponent_behavior = np.mean(history.opponent_actions[-3:, :], axis=0) if len(self.cooperation_history) >= 3 else np.zeros(self.game_description.n_players - 1)
        cooperate = False
        if previous_round_cooperators >= T:
            cooperate = True
        elif previous_round_cooperators < T:
            p = 0.5 * (1 - previous_round_cooperators / self.game_description.n_players)
            cooperate = random.random() < p
        for opponent in range(self.game_description.n_players - 1):
            if opponent_behavior[opponent] > 0.5:
                cooperate = True
                break
        self.cooperation_history.append(cooperate)
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_468(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperative Escalation (ACE)

    ACE aims to balance individual self-interest with collective well-being by adapting cooperation levels based on observed opponent behavior and game history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.escalation_level = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_coops = sum(history.opponent_actions[-1, :])
        if prev_coops >= self.game_description.n_players - 1:
            return Action.C
        elif prev_coops > self.game_description.n_players / 2:
            self.escalation_level += 1 / self.game_description.n_players
            if self.escalation_level >= 1 or state.round_number == self.game_description.n_rounds - 1:
                return Action.C
            else:
                return Action.D
        elif any((not opponent_action for opponent_action in history.opponent_actions[-1, :])):
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if sum((1 for round in range(-2, 0) if sum(history.opponent_actions[round, :]) < self.game_description.n_players / 2)) == 2:
            self.escalation_level = 0
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_469(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) strategy for N-Player Public Goods Game.
    
    This strategy adapts to various opponent behaviors and depends solely on game parameters and history.
    It aims to achieve a mutually beneficial outcome for all players involved by balancing individual self-interest with collective optimization.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < self.game_description.k / self.game_description.n_players else Action.D
        previous_round_payoff = np.mean(history.my_payoffs[-1:])
        if previous_round_payoff >= self.game_description.k:
            p_C = 0.7 + (previous_round_payoff - self.game_description.k) / (self.game_description.n_players * self.game_description.k)
            return Action.C if random.random() < p_C else Action.D
        else:
            return Action.D if random.random() > self.game_description.k / self.game_description.n_players else Action.C

class Strategy_COLLECTIVE_470(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and maximizes overall payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        average_payoff_prev_round = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1])
        average_cooperators_prev_round = sum(np.concatenate((history.my_actions[-1:], history.opponent_actions[-1])))
        if average_payoff_prev_round >= self.game_description.k:
            return Action.C
        else:
            p_d = 1 - self.game_description.k / self.game_description.n_players * (average_cooperators_prev_round / self.game_description.n_players)
            return Action.D if random.random() < p_d else Action.C

class Strategy_COLLECTIVE_471(BaseStrategy):
    """
    Adaptive Cooperation with Memory (ACM) strategy.
    
    This strategy balances individual self-interest with collective well-being,
    adapting to a wide range of opponent behaviors and game conditions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.memory = 0
        self.threshold = 0.5
        self.round_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.memory = sum(history.opponent_actions[-1, :]) + 1
        self.round_count += 1
        if self.round_count % 5 == 0:
            avg_c = np.mean(self.memory - 1 if history else 0)
            self.threshold = avg_c / self.game_description.n_players
        if state.round_number > 1 and history.my_actions[-2] == Action.D and (self.memory > self.threshold * self.game_description.n_players):
            return Action.D
        if self.memory >= self.game_description.n_players / 2:
            return Action.C
        elif self.memory == self.game_description.n_players / 2:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_472(BaseStrategy):
    """
    The Adaptive Collective Cooperation (ACC) strategy balances individual self-interest with collective well-being,
    adapting to opponent behaviors and promoting a cooperative environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.k / game_description.n_players
        self.prev_coop_levels = None
        self.defection_counts = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coop_levels = [action == Action.C for action in history.opponent_actions[-1, :]]
        any_opponent_cooperated = any(prev_coop_levels)
        self.defection_counts = [count + 1 if not action else 0 for count, action in zip(self.defection_counts, history.opponent_actions[-1, :])]
        punish_opponents = any((count >= 2 for count in self.defection_counts))
        if sum(prev_coop_levels) >= self.cooperation_threshold and any_opponent_cooperated or state.round_number == self.game_description.n_rounds - 1:
            cooperate = True
        elif punish_opponents or (history.my_actions[-1] == Action.C and (not any_opponent_cooperated)):
            cooperate = False
        else:
            cooperate = True
        self.prev_coop_levels = prev_coop_levels
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_473(BaseStrategy):
    """
    The Adaptive Cooperation strategy aims to balance individual payoffs with collective well-being by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tau = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        my_action = history.my_actions[-1]
        num_coop = np.sum(opponent_actions) + (my_action if my_action else 0)
        avg_payoff_C = self.game_description.k / self.game_description.n_players * num_coop
        avg_payoff_D = 1 + self.game_description.k / self.game_description.n_players * num_coop
        if num_coop >= self.game_description.n_players / 2:
            self.tau += 1 / self.game_description.n_players
        elif num_coop < self.game_description.n_players / 4 and state.round_number != self.game_description.n_rounds - 1:
            self.tau -= 1 / self.game_description.n_players
        if avg_payoff_C - self.tau >= avg_payoff_D:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_474(BaseStrategy):
    """
    This strategy adapts to the game's history and parameters by balancing individual self-interest with collective well-being.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_opponent_actions = history.opponent_actions[-1]
        opponent_cooperators = sum(previous_round_opponent_actions)
        if opponent_cooperators > 0:
            avg_payoff_coop = (history.my_payoffs[-1] + np.sum(history.opponent_payoffs[-1, :])) / opponent_cooperators
        else:
            avg_payoff_coop = float('inf')
        if avg_payoff_coop <= 1 + self.game_description.k / self.game_description.n_players:
            action = Action.D
        else:
            action = Action.C
        if np.all(~previous_round_opponent_actions):
            action = Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_COLLECTIVE_475(BaseStrategy):
    """
    Adaptive Cooperation with History-Based Adjustment (ACHA) strategy.
    
    This strategy adapts cooperation levels based on the previous round's outcomes, 
    promoting collective well-being through adaptive cooperation while punishing excessive defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(game_description.n_players / 2)
        self.defection_threshold = math.floor(game_description.n_players / 4)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :])
        num_defectors = self.game_description.n_players - num_cooperators - 1
        if num_cooperators >= self.cooperation_threshold:
            return Action.C
        elif num_defectors <= self.defection_threshold:
            return Action.D
        else:
            return Action.C if state.round_number % 2 == 0 else Action.D

class Strategy_COLLECTIVE_476(BaseStrategy):
    """
    The Adaptive Cooperative Threshold (ACT) strategy balances individual payoff maximization with collective welfare.
    It adapts cooperation based on the game's history and parameters, punishing exploitation through a temporary defection mechanism.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.avg_contributions = 0
        self.num_cooperators = 0
        self.k_threshold = (self.n_players / self.k) ** 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.n_players >= 4 and self.k > 2 else Action.D
        if history is not None:
            opponent_actions = history.opponent_actions[-1, :]
            num_cooperators_in_round = sum(opponent_actions) + (history.my_actions[-1] == True)
            self.num_cooperators = num_cooperators_in_round
            if state.round_number > 1:
                self.avg_contributions = (self.avg_contributions * (state.round_number - 2) + num_cooperators_in_round) / (state.round_number - 1)
        if self.num_cooperators < self.k_threshold and history is not None:
            return Action.D
        if self.avg_contributions >= self.k_threshold:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1 and history is not None:
            my_payoff_cooperating = self.k / self.n_players * sum(history.opponent_actions[-1, :]) + (history.my_payoffs[-1] if len(history.my_payoffs) > 0 else 0)
            my_payoff_defecting = 1
            return Action.C if my_payoff_cooperating > my_payoff_defecting else Action.D
        return Action.C if self.num_cooperators >= self.k_threshold else Action.D

class Strategy_COLLECTIVE_477(BaseStrategy):
    """
    A collective strategy that adapts to the game parameters and history.
    
    This strategy is based on the concept of adaptive collective convergence, 
    where players adjust their cooperation threshold based on the collective payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.k / 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        total_payoff = np.sum(history.my_payoffs) + np.sum(history.opponent_payoffs)
        avg_payoff = total_payoff / (self.game_description.n_players * state.round_number)
        if avg_payoff >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D
        collective_payoff = np.mean(np.concatenate((history.my_payoffs, history.opponent_payoffs)))
        if collective_payoff < self.cooperation_threshold and state.round_number % 5 == 0:
            self.cooperation_threshold += 0.05

class Strategy_COLLECTIVE_478(BaseStrategy):
    """
    Adaptive Cooperativity with Memory (ACM) Strategy.
    
    This strategy balances individual self-interest with collective benefits by adapting to the game's history and opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.s = 0.5
        self.M_t = None
        self.APC_t = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.M_t is None or self.APC_t is None:
            self.M_t = sum(history.opponent_actions[0, :] + [True])
            opponent_cooperators_payoffs = history.opponent_payoffs[0, :][history.opponent_actions[0, :]]
            if len(opponent_cooperators_payoffs) > 0:
                self.APC_t = np.mean(np.append(opponent_cooperators_payoffs, history.my_payoffs[0]))
            else:
                self.APC_t = 0
        M_prev = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        APC_prev = history.my_payoffs[-1] if history.my_actions[-1] else np.mean(history.opponent_payoffs[-1, :][history.opponent_actions[-1, :]])
        if M_prev > self.game_description.n_players / 2:
            return Action.C
        elif APC_prev > self.game_description.k / self.game_description.n_players + self.s:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        if opponent_cooperators >= self.game_description.n_players // 2:
            self.s *= 0.9
        elif opponent_cooperators < self.game_description.n_players // 2:
            self.s *= 1.1
        self.M_t = M_prev
        if history.my_actions[-1]:
            self.APC_t = APC_prev
        else:
            opponent_cooperators_payoffs = history.opponent_payoffs[-1, :][history.opponent_actions[-1, :]]
            if len(opponent_cooperators_payoffs) > 0:
                self.APC_t = np.mean(np.append(opponent_cooperators_payoffs, history.my_payoffs[-1]))
        return Action.D

class Strategy_COLLECTIVE_479(BaseStrategy):
    """
    A collective strategy called "Adaptive Collective Conscience" (ACC). 
    It aims to balance individual self-interest with collective well-being by adapting to the group's behavior over time.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.TC = 0
        self.CT = game_description.n_players * game_description.k / (2 * game_description.n_players + game_description.k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_contributors = sum(history.my_actions[:state.round_number])
        if state.round_number > 0 and len(history.opponent_actions) > 0 and (history.opponent_actions[-1].sum() < self.game_description.n_players / 2):
            p_cooperate = 0.5 + self.game_description.k / self.game_description.n_players
            if random.random() < p_cooperate:
                return Action.C
        ACP = num_contributors / state.round_number
        if ACP >= self.CT or (state.round_number == self.game_description.n_rounds - 1 and history.my_payoffs[-2] > history.my_payoffs[-3]):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_480(BaseStrategy):
    """
    A collective strategy that adapts to the cooperation rate of opponents,
    retaliating when it falls below a certain threshold and exploring defection 
    at regular intervals.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.history = []
        self.theta = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3 or (state.round_number == 0 and history is None):
            return Action.C
        opponent_cooperations = np.sum(history.opponent_actions[-1, :])
        avg_coop_rate = opponent_cooperations / self.game_description.n_players
        if avg_coop_rate >= self.theta:
            return Action.C
        elif state.round_number % 5 != 0:
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_481(BaseStrategy):
    """
    A collective strategy that balances individual payoff maximization with promoting cooperation among players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T = 0.5
        self.APC = []
        self.APD = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.n_players % 2 == 1 else Action.D
        my_payoff = history.my_payoffs[-1]
        opponent_payoffs = history.opponent_payoffs[-1, :]
        cooperators_payoffs = opponent_payoffs[history.opponent_actions[-1, :]]
        defectors_payoffs = opponent_payoffs[np.logical_not(history.opponent_actions[-1, :])]
        APC = np.mean(cooperators_payoffs) if len(cooperators_payoffs) > 0 else my_payoff
        APD = np.mean(defectors_payoffs) if len(defectors_payoffs) > 0 else my_payoff
        self.APC.append(APC)
        self.APD.append(APD)
        if APC > APD:
            self.T = min(self.T + 0.05, 1)
        elif APD > APC:
            self.T = max(self.T - 0.05, 0)
        if state.round_number < 5 and history.my_actions[-1] == False and ((history.my_actions[-2] if len(history.my_actions) >= 2 else True) == False):
            self.T += 0.1
        cooperators = sum(history.opponent_actions[-1, :])
        if cooperators / self.game_description.n_players >= self.T:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_482(BaseStrategy):
    """
    A strategy that promotes collective cooperation while adapting to the behavior of other players.
    Initially cooperates, then uses social learning and punishment mechanism to prevent exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_low_payoff = 0
        self.theta = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        T_c = sum(history.opponent_actions[-1, :])
        R_c = T_c / self.game_description.n_players
        if R_c >= self.game_description.k / self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        my_payoff = history.my_payoffs[-1]
        if my_payoff <= 1 and history.my_actions[-1]:
            self.consecutive_low_payoff += 1
            if self.consecutive_low_payoff >= self.theta:
                action = Action.D
                self.consecutive_low_payoff = 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_483(BaseStrategy):
    """
    Adaptive Cooperation with Memory (ACM) strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k_factor = game_description.k
        self.tau_threshold = (1 - self.k_factor / self.n_players) / (self.k_factor / self.n_players)
        self.cooperation_memory = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            opponent_cooperations = sum(history.opponent_actions[-1, :])
            if len(self.cooperation_memory) > 0 and self.cooperation_memory[-1] == False and (opponent_cooperations < self.tau_threshold * (self.n_players - 1)):
                return Action.C
            elif opponent_cooperations >= self.tau_threshold * (self.n_players - 1):
                return Action.C
            else:
                return Action.D
        action = self.__call__(state, history)
        self.cooperation_memory.append(action == Action.C)

class Strategy_COLLECTIVE_485(BaseStrategy):
    """
    The Adaptive Cooperative Threshold (ACT) strategy is designed to balance individual 
    self-interest with collective welfare by adapting to the evolving game environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.successful_cooperation_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if state.round_number > 1:
            tau = self.game_description.k / self.game_description.n_players * (self.successful_cooperation_rounds / (state.round_number - 1))
        else:
            tau = 0
        if history.my_actions[-1] == True and previous_round_cooperators > 0:
            self.successful_cooperation_rounds += 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if sum(history.opponent_actions[-1, :]) == 0 and history.my_actions[-1] == False:
            return random.choice([Action.C, Action.D], p=[0.1, 0.9])
        if previous_round_cooperators >= tau:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_486(BaseStrategy):
    """
    The Adaptive Cooperation (AC) strategy balances individual payoff maximization with collective welfare,
    adapting to various opponent behaviors. It relies on game parameters and history, making it robust 
    and suitable for a wide range of interactions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_probability = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperators = history.opponent_actions[-1, :] | history.my_actions[-1]
        avg_coop_payoff = np.mean(history.opponent_payoffs[-1, :][cooperators]) if any(cooperators) else 0.0
        avg_defect_payoff = np.mean(history.opponent_payoffs[-1, :][~cooperators]) if any(~cooperators) else 0.0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if avg_coop_payoff == avg_defect_payoff:
            return random.choice([Action.C, Action.D])
        action = Action.C if avg_coop_payoff > avg_defect_payoff else Action.D
        coop_ratio = np.sum(cooperators) / self.game_description.n_players
        if coop_ratio > 0.5:
            self.cooperation_probability += 0.1
        elif coop_ratio < 0.3:
            self.cooperation_probability -= 0.1
        return Action.C if random.random() < self.cooperation_probability else Action.D

class Strategy_COLLECTIVE_487(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.threshold = math.ceil(game_description.n_players / 2)
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_contributions_prev = sum(history.opponent_actions[-1, :] == True)
        avg_payoff_all_players_prev = np.mean(history.my_payoffs) if len(history.my_payoffs) > 0 else 0
        expected_defection_payoff = 1 + self.game_description.k / self.game_description.n_players * 0
        if avg_payoff_all_players_prev > expected_defection_payoff:
            self.threshold -= 1
        else:
            self.threshold += 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if total_contributions_prev >= self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_488(BaseStrategy):
    """
    Adaptive Cooperative Threshold (ACT) Strategy.

    This strategy aims to promote cooperation by maintaining a dynamic cooperation threshold.
    It adjusts its cooperation level based on the collective behavior of opponents and balances exploration and exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.previous_round_actions = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        cooperate = num_cooperators_prev_round >= self.cooperation_threshold * self.game_description.n_players
        alpha = 0.1
        self.cooperation_threshold = (1 - alpha) * self.cooperation_threshold + alpha * (num_cooperators_prev_round / self.game_description.n_players)
        if state.round_number == self.game_description.n_rounds - 1:
            cooperate = self.cooperation_threshold >= 0.5
        if num_cooperators_prev_round == math.floor(self.cooperation_threshold * self.game_description.n_players):
            cooperate = random.random() < 0.5
        opponent_defected = not history.opponent_actions[-1, :].all()
        if opponent_defected and history.my_actions[-1]:
            self.cooperation_threshold -= 0.1
            self.cooperation_threshold = max(0.0, self.cooperation_threshold)
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_489(BaseStrategy):
    """
    A collective strategy called "Adaptive Collective Optimism" (ACO) 
    that balances individual self-interest with collective well-being.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_count_history = []
        self.avg_cooperate_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :])
        if previous_round_cooperators >= self.game_description.k / self.game_description.n_players:
            return Action.C
        p = 1 - previous_round_cooperators / self.game_description.n_players
        if random.random() < p:
            return Action.D
        self.cooperate_count_history.append(previous_round_cooperators)
        self.avg_cooperate_count = np.mean(self.cooperate_count_history)
        if self.avg_cooperate_count > self.game_description.k / self.game_description.n_players:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif previous_round_cooperators == self.game_description.k / self.game_description.n_players:
            return random.choice([Action.C, Action.D])
        return Action.C

class Strategy_COLLECTIVE_490(BaseStrategy):
    """
    Collective strategy that adapts to the game's history and promotes cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.first_round_done = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if not self.first_round_done and state.round_number == 0:
            self.first_round_done = True
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        avg_coop_rate_prev_round = np.mean(history.opponent_actions[-1, :])
        if avg_coop_rate_prev_round > 0.5:
            return Action.C
        prob_defect = (1 - avg_coop_rate_prev_round) / 2
        action_probabilities = [1 - prob_defect, prob_defect]
        return random.choices([Action.C, Action.D], weights=action_probabilities)[0]

class Strategy_COLLECTIVE_491(BaseStrategy):
    """
    Adaptive Cooperative Escalation strategy.
    
    This strategy balances individual self-interest with collective well-being, 
    adapting to various opponent behaviors while promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.history = []
        self.avg_payoff = 0
        self.prev_coop_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.history.append(history.my_actions[-1])
            self.prev_coop_count = sum(history.opponent_actions[-1, :])
        n = self.game_description.n_players
        k = self.game_description.k
        r = self.game_description.n_rounds
        if len(self.history) == 1:
            return Action.C
        elif len(self.history) <= r // 2:
            if self.prev_coop_count >= n / 2:
                return Action.C
            else:
                return Action.D
        else:
            avg_payoff = np.mean(history.my_payoffs[-5:])
            expected_cooperative_payoff = k / n * n
            p = min(1, max(0, avg_payoff / expected_cooperative_payoff))
            if random.random() < p or (len(self.history) == r - 1 and avg_payoff >= expected_cooperative_payoff):
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_492(BaseStrategy):
    """
    Adaptive Cooperative Tit-for-Tat (ACTT) Strategy.
    
    Initially, cooperate to establish a baseline for reciprocal behavior.
    Adapt subsequent actions based on the average payoffs of cooperators and defectors in previous rounds.
    Cooperate with small probability if all players defected previously to encourage cooperation recovery.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        prev_round_payoffs = history.opponent_payoffs[-1, :]
        avg_coop_payoff = np.mean(prev_round_payoffs[prev_round_actions == True])
        avg_defect_payoff = np.mean(prev_round_payoffs[prev_round_actions == False])
        if all((not action for action in prev_round_actions)):
            return Action.C if random.random() < 1 / (self.game_description.n_players + 1) else Action.D
        if avg_coop_payoff >= avg_defect_payoff:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_493(BaseStrategy):
    """
    A collective strategy for N-Player Public Goods Game that balances individual payoffs with the promotion of overall cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.r_init = min(5, game_description.n_rounds // 2)
        self.n_coop_threshold = game_description.n_players // 2
        self.payoff_history = []
        self.cooperator_count_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        recent_payoff = history.my_payoffs[-1]
        cooperators_recent_round = sum(history.opponent_actions[-1, :])
        self.payoff_history.append(recent_payoff)
        self.cooperator_count_history.append(cooperators_recent_round)
        if state.round_number < self.r_init:
            return Action.C
        avg_coop_payoff = np.mean(self.payoff_history[-self.r_init:])
        if avg_coop_payoff > 1 - history.my_actions[-1] + self.game_description.k / self.game_description.n_players * cooperators_recent_round:
            return Action.C
        avg_payoff = np.mean(self.payoff_history)
        if cooperators_recent_round < self.n_coop_threshold or recent_payoff < 0.9 * avg_payoff:
            return Action.D
        if cooperators_recent_round > self.cooperator_count_history[-2] or recent_payoff > 1.1 * avg_payoff:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_494(BaseStrategy):
    """
    The Adaptive Cooperation strategy promotes cooperation while being robust to a wide range of opponent behaviors.
    It adapts to the game's history and punishes free-riding, encouraging players to contribute to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = 0.5
        self.punish_threshold = 1.0
        self.own_contribution_prev = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_coop_prev = np.mean(history.opponent_actions[-1, :])
        avg_payoff_prev = np.mean(history.opponent_payoffs[-1, :])
        if avg_coop_prev >= self.coop_threshold:
            action = Action.C
        elif self.own_contribution_prev and avg_payoff_prev < self.punish_threshold:
            action = Action.D
        else:
            action = Action.D
        if avg_payoff_prev > 1.5 * self.game_description.n_players:
            self.coop_threshold += 0.05
        elif avg_payoff_prev < 0.5 * self.game_description.n_players:
            self.punish_threshold -= 0.05
        self.own_contribution_prev = action == Action.C
        return action

class Strategy_COLLECTIVE_495(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Punishment (ACP)
    
    Aims to balance individual self-interest with collective welfare by 
    cooperating initially, punishing exploitation, and adapting to changing 
    opponent behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.C_count_history = []
        self.D_count_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        self.C_count_history.append(sum(opponent_actions))
        self.D_count_history.append(len(opponent_actions) - sum(opponent_actions))
        if self.C_count_history[-1] >= self.game_description.n_players // 2:
            return Action.C
        elif self.D_count_history[-1] > self.game_description.n_players // 2:
            return Action.D
        elif history.my_actions[-1] == True and history.opponent_actions[-1, 0] == False:
            return Action.D
        elif self.C_count_history[-1] == self.game_description.n_players // 2:
            return Action.C
        if state.round_number >= self.game_description.n_rounds - 2:
            cooperation_payoff = sum(history.my_payoffs[history.my_actions == True])
            defection_payoff = sum(history.my_payoffs[history.my_actions == False])
            if cooperation_payoff > defection_payoff:
                return Action.C
        return Action.C

class Strategy_COLLECTIVE_496(BaseStrategy):
    """
    A collective strategy called "Adaptive Cooperative Escalation" (ACE) 
    that balances individual payoffs with collective welfare by adapting to the game's history and promoting cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_cooperate = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_cooperate else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if prev_cooperators >= self.game_description.k / self.game_description.n_players * self.game_description.n_players / 2:
            self.p_cooperate = min(self.p_cooperate + 0.2, 0.9)
        if history.my_actions[-1]:
            self.p_cooperate = min(self.p_cooperate + 0.1, 0.9)
        if not history.opponent_actions[-1, 0] and (not history.opponent_actions[-2, 0] if state.round_number > 1 else True):
            self.p_cooperate = 0.5
        if state.round_number <= 2 and (not history.opponent_actions[-1, 0]):
            self.p_cooperate = 0.5
        if sum(history.opponent_actions[:, 0]) < self.game_description.n_rounds / 4:
            self.p_cooperate = max(self.p_cooperate - 0.2, 0.1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < self.p_cooperate else Action.D

class Strategy_COLLECTIVE_497(BaseStrategy):
    """
    Adaptive Cooperativity strategy for the Public Goods game.
    
    This strategy adapts to the game's history by adjusting a cooperation threshold based on 
    the number of cooperators in previous rounds. The goal is to balance individual payoffs with 
    collective well-being and promote a robust, cooperative environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = game_description.k / game_description.n_players
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        CR = num_cooperators / self.game_description.n_players
        if all((not action for action in history.opponent_actions[-1, :])) and (not history.my_actions[-1]):
            self.theta *= 0.9
        elif all((action for action in history.opponent_actions[-1, :])) and history.my_actions[-1]:
            self.theta *= 1.05
        if CR > self.theta:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif all((not history.my_actions[i] for i in range(max(0, len(history.my_actions) - 3), len(history.my_actions)))):
            self.consecutive_defections += 1
            if self.consecutive_defections > 2:
                return Action.D
        else:
            self.consecutive_defections = 0
        return Action.D

class Strategy_COLLECTIVE_498(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.k / 2
        self.rounds_since_threshold_update = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        if opponent_cooperators >= self.threshold:
            return Action.C
        elif opponent_cooperators < self.threshold:
            return random.choice([Action.D] * 7 + [Action.C])
        self.rounds_since_threshold_update += 1
        if self.rounds_since_threshold_update == 5:
            avg_cooperators = np.mean(sum(history.opponent_actions[-5:, :]))
            if avg_cooperators > self.threshold:
                self.threshold = min(self.game_description.n_players, self.threshold + 0.1)
            elif avg_cooperators < self.threshold:
                self.threshold = max(0, self.threshold - 0.1)
            self.rounds_since_threshold_update = 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        else:
            return history.my_actions[-1]

class Strategy_COLLECTIVE_499(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors.
    It encourages initial cooperation, responds positively to cooperation, punishes exploitation,
    and allows for exploration and adaptation to changing opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.5
        self.punish_defection = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < 0.8 else Action.D
        opponent_coop = sum(history.opponent_actions[-1, :])
        self.avg_coop_rate = (history.my_actions[-1] + opponent_coop) / self.game_description.n_players
        if self.avg_coop_rate > 0.5:
            coop_prob = self.avg_coop_rate
        else:
            coop_prob = 0
        if self.punish_defection and self.avg_coop_rate < 0.5:
            coop_prob = 0
        if random.random() < 0.1:
            coop_prob = random.random()
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        action = Action.C if random.random() < coop_prob else Action.D
        self.punish_defection = True if self.avg_coop_rate < 0.5 and history.my_actions[-1] else False
        return action

class Strategy_COLLECTIVE_500(BaseStrategy):
    """
    Adaptive Cooperate-and-Punish strategy.
    
    This strategy balances cooperation and self-interest by adapting to opponent behaviors.
    It promotes collective well-being without relying on explicit coordination.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.7 else Action.D
        cooperate_prob = 0.0
        defect_prob = 0.0
        for opponent in range(self.game_description.n_players):
            if opponent != state.round_number % self.game_description.n_players:
                prev_payoff = self.game_description.k / self.game_description.n_players * sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 0)
                if prev_payoff > 1:
                    cooperate_prob += 0.8
                else:
                    defect_prob += 0.6
        random_val = random.random()
        if random_val < cooperate_prob:
            return Action.C
        elif random_val < cooperate_prob + defect_prob:
            return Action.D
        else:
            return random.choice([Action.C, Action.D])

class Strategy_COLLECTIVE_501(BaseStrategy):
    """
    The Adaptive Cooperative Foresight strategy aims to promote cooperation while being robust against various opponent behaviors.
    It analyzes historical trends, responds to recent defections, and incorporates elements of tit-for-tat and punishment/forgiveness mechanisms.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.6 * game_description.n_players
        self.defection_trigger = 0.4 * game_description.n_players
        self.consecutive_defection_limit = 2
        self.endgame_window = min(3, game_description.n_rounds // 4)
        self.window_size = min(5, game_description.n_rounds // 2)
        self.contributions_history = []
        self.opponent_actions = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if history is not None:
            self.contributions_history.append(sum(history.opponent_actions[-1, :]))
            self.opponent_actions = history.opponent_actions
            avg_contributions = np.mean(self.contributions_history[-self.window_size:])
            recent_defectors = sum((1 for opponent in self.opponent_actions[-1, :] if not opponent))
            opponent_payoffs = history.opponent_payoffs
            cooperate = False
            if avg_contributions >= self.cooperation_threshold:
                cooperate = True
            if recent_defectors > 0 and sum(self.contributions_history[-1:]) < self.defection_trigger:
                cooperate = False
            opponent_last_action = self.opponent_actions[-1, 0]
            my_last_action = history.my_actions[-1]
            if not opponent_last_action and my_last_action:
                cooperate = False
            elif opponent_last_action and (not my_last_action):
                cooperate = True
            if state.round_number >= self.game_description.n_rounds - self.endgame_window:
                my_payoff = history.my_payoffs[-1]
                average_payoff = np.mean(opponent_payoffs[-1, :])
                if my_payoff < average_payoff:
                    cooperate = False
                else:
                    cooperate = True
            consecutive_defections = sum((1 for opponent in self.opponent_actions[-self.consecutive_defection_limit:, 0] if not opponent))
            if consecutive_defections >= self.consecutive_defection_limit:
                cooperate = False
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_502(BaseStrategy):
    """
    Adaptive Cooperation strategy for N-Player Public Goods Game.
    
    This strategy promotes a collective mindset by encouraging cooperation when others contribute, 
    punishing opportunistic behavior to maintain fairness, and adapting to the group's cooperation level.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.thresh = 0.5
        self.sum_cooperations = 0
        self.punish_opponent = False
        self.punishment_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        p_coop = num_cooperators_prev_round / self.game_description.n_players * self.game_description.k
        all_defected = not any(history.opponent_actions[-1, :])
        if all_defected:
            return Action.D
        for opponent in history.opponent_actions[-1, :]:
            if not opponent and any(history.opponent_actions[-1, :]):
                self.punish_opponent = True
                break
        if self.punish_opponent:
            self.punishment_rounds += 1
            if self.punishment_rounds < 2:
                return Action.D
            else:
                self.punish_opponent = False
                self.punishment_rounds = 0
        self.sum_cooperations += num_cooperators_prev_round
        if state.round_number > 1:
            self.thresh = 0.5 + self.game_description.k / self.game_description.n_players * (self.sum_cooperations / state.round_number)
        if num_cooperators_prev_round >= self.thresh * self.game_description.n_players:
            return Action.C
        elif random.random() < p_coop:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_503(BaseStrategy):
    """
    A collective strategy that prioritizes cooperation by adapting to observed cooperation rates.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.5
        self.threshold = game_description.k / game_description.n_players * (game_description.n_players / 2)
        self.payoff_diff = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        else:
            prev_cooperators = sum(history.opponent_actions[-1, :])
            if prev_cooperators > self.threshold:
                action = Action.C
            elif len(self.payoff_diff) > 0 and np.mean(self.payoff_diff) < 0:
                action = Action.D
            else:
                action = Action.C if random.random() < self.cooperate_prob else Action.D
            if history.my_payoffs.shape[0] > 1:
                prev_payoff = history.my_payoffs[-2]
                curr_payoff = self.game_description.k / self.game_description.n_players * prev_cooperators
                self.payoff_diff.append(curr_payoff - prev_payoff)
            if state.round_number % 5 == 0 and state.round_number > 0:
                cooperation_rate = prev_cooperators / self.game_description.n_players
                if cooperation_rate > 0.5:
                    self.threshold += self.game_description.k / self.game_description.n_players * 0.1
                else:
                    self.threshold -= self.game_description.k / self.game_description.n_players * 0.1
            return action

class Strategy_COLLECTIVE_504(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with group well-being.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_coops = 0
        self.coop_payoff = 0
        self.defect_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coops = sum(history.opponent_actions[-1, :] == True)
        self.prev_coops = prev_coops
        if self.game_description.n_players == 0:
            return Action.D
        else:
            T = self.game_description.k / self.game_description.n_players * prev_coops
        coop_payoff, defect_payoff = self.calculate_payoffs(history)
        self.coop_payoff = coop_payoff
        self.defect_payoff = defect_payoff
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if prev_coops > 0 else Action.D
        elif T > coop_payoff or (T <= coop_payoff and random.random() < 0.5):
            return Action.C
        else:
            return Action.D

    def calculate_payoffs(self, history: PlayerHistory) -> tuple[float, float]:
        """
        Calculate the expected payoffs for cooperating and defecting based on previous rounds.
        """
        my_prev_action = history.my_actions[-1]
        opponent_coops = sum(history.opponent_actions[-1, :] == True)
        coop_payoff = 1 - 1 + self.game_description.k / self.game_description.n_players * (opponent_coops + 1)
        defect_payoff = 1 - 0 + self.game_description.k / self.game_description.n_players * opponent_coops
        if my_prev_action == True:
            return (coop_payoff, defect_payoff)
        else:
            return (coop_payoff * 0.8, defect_payoff * 0.9)

class Strategy_COLLECTIVE_505(BaseStrategy):
    """
    Adaptive Cooperation with History-Based Threshold strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.beta = 0
        self.tau = game_description.k / game_description.n_players * (1 + self.beta)
        self.alpha = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coop_rate = np.mean(history.my_actions[-1:] + history.opponent_actions[-1, :])
        if prev_coop_rate > self.tau:
            self.beta += self.alpha
        elif prev_coop_rate < self.tau:
            self.beta -= self.alpha
            self.beta = max(self.beta, -1)
        self.tau = self.game_description.k / self.game_description.n_players * (1 + self.beta)
        if prev_coop_rate < self.tau:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_506(BaseStrategy):
    """
    Adaptive Cooperative Escalation (ACE) Strategy.

    This strategy promotes cooperation by initially cooperating, 
    gradually escalating response to defection, punishing chronic defectors, 
    and forgiving those who re-enter cooperative interactions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_probabilities = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        n_cooperators = sum(opponent_actions)
        n_defectors = self.game_description.n_players - 1 - n_cooperators
        if n_defectors > n_cooperators:
            defect_probability = (n_defectors - n_cooperators) / (2 * (self.game_description.n_players - 1))
            return Action.D if random.random() < defect_probability else Action.C
        for opponent, actions in enumerate(history.opponent_actions[:, :]):
            if sum(actions) < len(actions) / 2:
                self.punishment_probabilities[opponent] = max(self.punishment_probabilities.get(opponent, 0), 0.75)
            elif opponent in self.punishment_probabilities and sum(actions[-5:]) >= len(actions[-5:]):
                self.punishment_probabilities[opponent] = max(self.punishment_probabilities.get(opponent, 0) - 0.1, 0)
            if random.random() < self.punishment_probabilities.get(opponent, 0):
                return Action.D
        return Action.C

class Strategy_COLLECTIVE_507(BaseStrategy):
    """
    Collective Strategy: "Adaptive Tit-for-Tat with Endgame Cooperation"
    This strategy balances individual payoff maximization with collective welfare, 
    while adapting to various opponent behaviors and game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.n_players // 3
        self.opponents_last_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if history is not None:
            cooperators = sum(history.opponent_actions[-1, :])
            if cooperators > self.game_description.n_players // 2:
                self.threshold = self.game_description.n_players // 2
            elif cooperators < self.game_description.n_players // 4:
                self.threshold = self.game_description.n_players // 4
        if state.round_number <= self.game_description.n_rounds // 2:
            if history is not None and sum(history.opponent_actions[-1, :]) >= self.threshold:
                return Action.C
            else:
                return Action.D
        elif history is not None:
            if self.opponents_last_action == True:
                action = True
            else:
                action = False
            self.opponents_last_action = sum(history.opponent_actions[-1, :]) > 0
        else:
            return Action.C
        if action:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_508(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Retaliation (ACR)
    
    This strategy encourages initial cooperation, reciprocates cooperation when others cooperate,
    retaliates against exploitation to maintain fairness and deter free-riding, and forgives after retaliation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.retaliation_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        opponent_defectors = self.game_description.n_players - 1 - opponent_cooperators
        if self.retaliation_rounds > 0:
            self.retaliation_rounds -= 1
            if self.retaliation_rounds == 0:
                return Action.C
        was_exploited = history.my_actions[-1] and opponent_defectors > 0
        if was_exploited:
            self.retaliation_rounds += 1
            return Action.D
        if opponent_cooperators >= opponent_defectors:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_509(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_init = 0.5
        self.p_recip = 0.7
        self.p_defect = 0.3
        self.payoff_threshold = 1.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_cooperators >= self.game_description.n_players / 2:
            cooperate_prob = self.p_recip
        else:
            cooperate_prob = 1 - self.p_defect
        if state.round_number % 5 == 0 and history.my_payoffs.size > 4:
            avg_payoff_t_5t = np.mean(history.my_payoffs[-5:])
            if avg_payoff_t_5t < self.payoff_threshold:
                self.p_recip -= 0.1
                self.p_defect += 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if prev_cooperators == self.game_description.n_players / 2:
            cooperate_prob = 0.5
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_COLLECTIVE_510(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Social Learning.

    This strategy aims to balance individual self-interest with collective well-being by adapting to the evolving game dynamics and learning from others' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.c_t = 0
        self.π_i_t_minus_2 = self.π_i_t_minus_1 = self.π_i_t = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        self.c_t = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if state.round_number > 2:
            if self.c_t >= self.game_description.n_players / 2:
                action = Action.C
            else:
                action = Action.D
            avg_payoff = (self.π_i_t_minus_2 + self.π_i_t_minus_1 + self.π_i_t) / 3
            if avg_payoff < self.game_description.k / 2:
                action = Action.C
        else:
            action = Action.C
        if history:
            self.π_i_t_minus_2 = self.π_i_t_minus_1
            self.π_i_t_minus_1 = self.π_i_t
            self.π_i_t = 1 - int(history.my_actions[-1]) + self.game_description.k / self.game_description.n_players * self.c_t
        return action

class Strategy_COLLECTIVE_511(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Punishment (ACP)
    
    This strategy prioritizes cooperation while maintaining a balance between rewarding cooperative behavior and punishing defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        elif state.round_number == 1:
            if sum(history.opponent_actions[0, :]) < self.game_description.n_players // 2:
                return Action.D
        if sum(history.opponent_actions[-1, :] + (history.my_actions[-1],)) > self.game_description.n_players / 2:
            return Action.C
        elif history.my_actions[-1] == False:
            p = 0.5 + self.game_description.k / self.game_description.n_players * sum(history.opponent_actions[-1, :] + (history.my_actions[-1],)) / self.game_description.n_players
            return Action.D if random.random() < p else Action.C
        elif history.my_actions[-1] == True:
            p = 0.5 + self.game_description.k / self.game_description.n_players * sum(history.opponent_actions[-1, :] + (history.my_actions[-1],)) / self.game_description.n_players
            return Action.C if random.random() < p else Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.C if state.round_number % 2 != 0 else Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_512(BaseStrategy):
    """
    Adaptive Cooperative Threshold Strategy.
    
    This strategy aims to balance individual self-interest with collective welfare by adapting cooperation levels based on observed opponent behavior and game history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.delta = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or len(history.my_actions) == 0:
            return random.choice([Action.C, Action.D])
        ocr = np.mean(np.sum(history.opponent_actions, axis=1)) / self.game_description.n_players
        theta = max(0.5, min(1 - 1 / self.game_description.n_players, ocr + self.delta))
        cooperate_prob = 0 if ocr < theta else 1
        prev_opponent_actions = history.opponent_actions[-2:]
        for t in range(prev_opponent_actions.shape[0]):
            if np.all(~prev_opponent_actions[t, :]) and ocr < theta:
                cooperate_prob *= 0.25
                break
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_2(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behaviors by balancing cooperation and defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n = self.game_description.n_players
        k = self.game_description.k
        avg_payoff_prev_round = np.mean(history.opponent_payoffs[-1, :])
        if avg_payoff_prev_round >= k / n * (n - 1):
            action = Action.D
        else:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if n == 2 and state.round_number < self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_EXPLOITATIVE_3(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        if total_cooperators_prev_round <= self.game_description.n_players / 2:
            action = Action.D
        elif history.my_payoffs[-1] <= np.mean(history.opponent_payoffs[-1]):
            action = Action.C
        else:
            action = Action.D
        if not any(history.opponent_actions[-1, :]):
            action = Action.C
        elif all(history.opponent_actions[-1, :]):
            action = Action.D
        self.payoffs.append(action == Action.C)
        return action

class Strategy_EXPLOITATIVE_4(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_count_prev = 0
        self.defect_count_prev = 0
        self.payoff_history = []
        self.my_action_prev = None
        self.punishment_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 3 or (state.round_number == 4 and len(self.payoff_history) < 1):
            return Action.C
        elif state.round_number <= 6:
            avg_payoff = np.mean(self.payoff_history) if self.payoff_history else 0
            expected_value = 1 + self.game_description.k / self.game_description.n_players * (self.game_description.n_players / 2)
            if avg_payoff < expected_value:
                return Action.D
            else:
                return Action.C
        else:
            coop_count_curr = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            defect_count_curr = self.game_description.n_players - coop_count_curr
            if coop_count_curr > self.game_description.n_players / 2:
                action = Action.C
            elif defect_count_curr > self.game_description.n_players / 2:
                action = Action.D
            else:
                expected_payoff_C = 1 + self.game_description.k / self.game_description.n_players * coop_count_curr
                expected_payoff_D = 1 + self.game_description.k / self.game_description.n_players * coop_count_curr
                if expected_payoff_C > expected_payoff_D:
                    action = Action.C
                else:
                    action = Action.D
            if history.opponent_actions[-1, :].any() and (self.my_action_prev is not None and self.my_action_prev == True):
                action = Action.D
                self.punishment_rounds += 2
            return action

class Strategy_EXPLOITATIVE_5(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0
        self.theta = game_description.k / (2 * game_description.n_players)
        self.uncooperative_opponents = set()
        self.opponent_actions_history = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        opponent_actions = self.opponent_actions_history if self.opponent_actions_history is not None else np.full(self.game_description.n_players - 1, False)
        if history is not None and state.round_number > 0:
            opponent_actions = history.opponent_actions[-1]
        self.avg_coop_rate = sum(opponent_actions) / (self.game_description.n_players - 1)
        for i, action in enumerate(opponent_actions):
            if action == False and state.round_number >= 2:
                opponent_payoff = history.opponent_payoffs[-1][i]
                my_expected_payoff = self.avg_coop_rate * self.game_description.k + (1 - sum(opponent_actions)) / (self.game_description.n_players - 1)
                if my_expected_payoff > opponent_payoff:
                    self.uncooperative_opponents.add(i)
        if state.round_number < self.game_description.n_rounds - 1:
            if self.avg_coop_rate >= self.theta or len(self.uncooperative_opponents) == self.game_description.n_players - 1:
                action = Action.C
            else:
                action = Action.D
        else:
            action = Action.D
        if history is not None and state.round_number > 0:
            self.opponent_actions_history = history.opponent_actions[-1]
        return action

class Strategy_EXPLOITATIVE_6(BaseStrategy):
    """
    Exploitative Strategy: Adaptive Tit-for-Tat with Noise Tolerance.
    
    This strategy combines elements of tit-for-tat (TFT) and noise tolerance to effectively exploit a wide range of opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tau = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        cooperate_count = 0
        defect_count = 0
        for opponent_action in history.opponent_actions[-1, :]:
            if opponent_action and random.random() < 1 - self.tau:
                cooperate_count += 1
            elif not opponent_action and random.random() < 1 - self.tau:
                defect_count += 1
        if cooperate_count > defect_count:
            return Action.C
        elif cooperate_count < defect_count:
            return Action.D
        elif random.random() < 1 - self.tau:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_7(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its payoff by exploiting 
    the cooperative behavior of others while minimizing its own contributions 
    to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        cooperators = sum(history.opponent_actions[-1, :])
        defectors = self.game_description.n_players - 1 - cooperators
        if history is not None and state.round_number > 0:
            prev_defectors = sum(history.opponent_actions[state.round_number - 2, :])
            if prev_defectors > self.game_description.n_players / 3 - 1:
                self.punish_rounds = 2
        if self.punish_rounds > 0:
            self.punish_rounds -= 1
            return Action.D
        elif cooperators > self.game_description.n_players / 2 - 1:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_8(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' cooperation rates.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_prev = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        opponent_actions = history.opponent_actions[-1]
        avg_coop_prev = np.mean(opponent_actions)
        self.avg_coop_prev = avg_coop_prev
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if avg_coop_prev == 0:
            return random.choice([Action.C, Action.D])
        if history.my_actions.sum() == 0:
            return Action.D
        if avg_coop_prev > 0.5:
            return Action.D
        else:
            p_C = self.game_description.k / self.game_description.n_players * avg_coop_prev
            return Action.C if random.random() < p_C else Action.D

class Strategy_EXPLOITATIVE_9(BaseStrategy):
    """
    An adaptive strategy for the N-Player Public Goods Game that exploits cooperation while maximizing personal gain.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = 0.5 * game_description.k / game_description.n_players
        self.my_actions = np.empty(game_description.n_rounds, dtype=np.bool_)
        self.my_payoffs = np.empty(game_description.n_rounds, dtype=np.float64)
        self.opponent_actions = np.empty((game_description.n_rounds, game_description.n_players - 1), dtype=np.bool_)
        self.opponent_payoffs = np.empty((game_description.n_rounds, game_description.n_players - 1), dtype=np.float64)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return self.first_round(history)
        self.my_actions[state.round_number - 1] = history.my_actions[-1]
        self.my_payoffs[state.round_number - 1] = history.my_payoffs[-1]
        self.opponent_actions[state.round_number - 1, :] = history.opponent_actions[-1, :]
        self.opponent_payoffs[state.round_number - 1, :] = history.opponent_payoffs[-1, :]
        avg_coop_rate = np.mean(self.my_actions[:state.round_number])
        last_round_cooperators = sum(self.opponent_actions[state.round_number - 2, :] if state.round_number > 1 else self.opponent_actions[0, :])
        if state.round_number == self.game_description.n_rounds - 1:
            return self.last_round()
        avg_defect_payoff = np.mean(self.my_payoffs[self.my_actions == False] if np.any(self.my_actions == False) else [0])
        last_coop_payoff = self.my_payoffs[state.round_number - 1] if self.my_actions[state.round_number - 1] else avg_defect_payoff
        if avg_coop_rate > self.coop_threshold and last_round_cooperators >= self.game_description.n_players // 2:
            return Action.C
        elif last_round_cooperators == self.game_description.n_players // 2 and last_coop_payoff < avg_defect_payoff:
            return Action.D if self.my_actions[state.round_number - 1] else Action.C
        elif last_coop_payoff > avg_defect_payoff:
            return Action.C
        return Action.D

    def first_round(self, history: None | PlayerHistory) -> Action:
        if history is None or len(history.my_actions) == 0:
            return Action.C
        first_round_cooperators = sum([history.opponent_actions[0, i] for i in range(self.game_description.n_players - 1)])
        if first_round_cooperators > self.game_description.n_players // 2:
            return Action.C
        return Action.D

    def last_round(self) -> Action:
        if 1 + self.game_description.k / self.game_description.n_players * 1 > np.mean([p for p in self.my_payoffs if p != 0] or [0]):
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_10(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to a wide range of opponent behaviors and maximizes payoff based on game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_contributions = sum(history.my_actions[:state.round_number - 1]) + np.sum(history.opponent_actions[:state.round_number - 1], axis=1)
        avg_contribution_rate = np.mean(total_contributions)
        if avg_contribution_rate > self.game_description.k / self.game_description.n_players:
            return Action.D
        elif state.round_number < self.game_description.n_rounds / 2:
            return Action.C
        elif state.round_number >= self.game_description.n_rounds / 2 and (not history.my_actions[state.round_number - 2]):
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_11(BaseStrategy):
    """
    Exploits cooperative behavior while minimizing own contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.round_number = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.round_number is None or self.round_number < state.round_number:
            self.round_number = state.round_number
        if self.round_number == 0:
            return random.choice([Action.C, Action.D])
        elif history is not None and len(history.my_actions) > 0:
            c_prev = sum((1 for action in history.opponent_actions[-1, :] if action))
            k_value = self.game_description.k
            if c_prev > self.game_description.n_players / 2:
                return Action.D
            elif k_value > 2 * self.game_description.n_players / 3:
                return Action.C
            else:
                return Action.C
        if state.round_number >= self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_12(BaseStrategy):
    """
    An adaptive and exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.round_number = 0
        self.cooperators_prev = None
        self.avg_payoff_prev = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.round_number += 1
        if history is not None and len(history.my_actions) > 0:
            opponent_cooperators = sum(history.opponent_actions[-1, :])
            total_cooperators = opponent_cooperators + history.my_actions[-1]
            avg_payoff_prev = np.mean(np.concatenate(([history.my_payoffs[-1]], history.opponent_payoffs[-1])))
        else:
            opponent_cooperators = 0
            total_cooperators = 0
            avg_payoff_prev = 0
        self.cooperators_prev = total_cooperators
        self.avg_payoff_prev = avg_payoff_prev
        if state.round_number <= 5:
            if self.cooperators_prev <= self.game_description.n_players / 2:
                return Action.D
            else:
                return Action.C
        elif avg_payoff_prev >= (self.game_description.k - 1) / self.game_description.n_players:
            if state.round_number == self.game_description.n_rounds:
                return Action.D
            else:
                return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_13(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to the game's history.
    It balances cooperation and defection based on the average payoff per player and the number of cooperators in previous rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.epsilon = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_payoff_prev_round = np.mean(history.my_payoffs[-1:] + history.opponent_payoffs[-1, :])
        cooperators_prev_round = sum(np.logical_or(history.my_actions[-1], history.opponent_actions[-1, :]))
        if avg_payoff_prev_round >= self.epsilon and cooperators_prev_round > self.game_description.k / self.game_description.n_players * self.game_description.n_players:
            return Action.C
        elif avg_payoff_prev_round < self.epsilon or cooperators_prev_round < self.game_description.k / self.game_description.n_players * self.game_description.n_players:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_14(BaseStrategy):
    """
    Adaptive Exploiter strategy for N-Player Public Goods Game.
    
    This strategy initially cooperates with a probability of 0.5 in the first round,
    then enters an exploitation mode where it adapts to opponents' behaviors.
    It punishes opponents who exploit others and updates its cooperation probability
    based on observed opponent actions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_init = 0.5
        self.cooperation_probability = self.p_init
        self.opponent_punishments = []

    def calculate_avg_payoff(self, history: PlayerHistory) -> float:
        payoffs = np.concatenate(([history.my_payoffs[-1]], history.opponent_payoffs[-1]))
        return np.mean(payoffs)

    def get_opponent_actions(self, history: PlayerHistory) -> NDArray[np.bool_]:
        return history.opponent_actions[-1]

    def punish_opponents(self, opponent_actions: NDArray[np.bool_], payoffs: NDArray[np.float64]):
        avg_payoff = np.mean(payoffs)
        for i, (action, payoff) in enumerate(zip(opponent_actions, payoffs)):
            if action == 0 and payoff > avg_payoff:
                self.opponent_punishments.append((i, 2))

    def update_cooperation_probability(self, avg_payoff_prev_round: float, opponent_actions: NDArray[np.bool_]):
        for action in opponent_actions:
            if action == 1 and avg_payoff_prev_round < self.game_description.k / self.game_description.n_players:
                self.cooperation_probability = min(self.cooperation_probability + 0.1, 0.9)
            elif action == 0 and avg_payoff_prev_round >= self.game_description.k / self.game_description.n_players:
                self.cooperation_probability = max(self.cooperation_probability - 0.1, 0.1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choices([Action.C, Action.D], weights=[self.p_init, 1 - self.p_init])[0]
        for opponent, rounds in self.opponent_punishments[:]:
            if rounds > 0:
                self.opponent_punishments[self.opponent_punishments.index((opponent, rounds))] = (opponent, rounds - 1)
            else:
                self.opponent_punishments.remove((opponent, rounds))
        avg_payoff_prev_round = self.calculate_avg_payoff(history)
        opponent_actions = self.get_opponent_actions(history)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        self.punish_opponents(opponent_actions, np.concatenate(([history.my_payoffs[-1]], history.opponent_payoffs[-1])))
        self.update_cooperation_probability(avg_payoff_prev_round, opponent_actions)
        if avg_payoff_prev_round >= self.game_description.k / self.game_description.n_players:
            return Action.C
        elif any((opponent == i for i, rounds in self.opponent_punishments)):
            return Action.D
        else:
            return random.choices([Action.C, Action.D], weights=[self.cooperation_probability, 1 - self.cooperation_probability])[0]

class Strategy_EXPLOITATIVE_15(BaseStrategy):
    """
    Adaptive Exploiter strategy in N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploiters = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        cooperators_last_round = sum(history.opponent_actions[-1, :])
        avg_payoff_coop_last_round = np.mean([payoff if action else 0 for payoff, action in zip(history.opponent_payoffs[-1, :], history.opponent_actions[-1, :])])
        for i, (player_defected, player_payoff) in enumerate(zip(~history.opponent_actions[-1, :], history.opponent_payoffs[-1, :])):
            if player_defected and player_payoff > self.game_description.k / self.game_description.n_players * cooperators_last_round:
                self.exploiters.add(i)
        if cooperators_last_round <= self.game_description.n_players // 2 or any((history.opponent_actions[-1, i] == False for i in self.exploiters)):
            return Action.D
        elif avg_payoff_coop_last_round >= self.game_description.k / 2:
            return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_16(BaseStrategy):
    """
    Exploitative strategy designed to maximize personal payoffs by taking advantage of cooperation from others while minimizing contributions to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        self.total_cooperators = sum(history.opponent_actions[-1, :])
        if self.total_cooperators >= self.game_description.n_players / 2:
            return Action.D
        elif history.my_actions[-1] and (not all(history.opponent_actions[-1, :])):
            return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_17(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        total_cooperators_prev_round = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if self.n_players == 2:
            if history.opponent_actions[-1][0]:
                return Action.C
            else:
                return Action.D
        if total_cooperators_prev_round >= self.game_description.n_players // 2:
            return Action.D
        prob_cooperate = total_cooperators_prev_round / self.n_players * self.k
        return Action.C if random.random() < prob_cooperate else Action.D

class Strategy_EXPLOITATIVE_18(BaseStrategy):
    """
    The Adaptive Follower strategy aims to balance exploitation with adaptability,
    taking into account the game's parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.CR = 0.0
        self.Threshold = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        total_Cs = sum(history.opponent_actions[-3:, :].sum(axis=1))
        self.CR = total_Cs / (self.game_description.n_players * 3)
        if self.game_description.k / self.game_description.n_players < 0.5:
            self.Threshold = 0.2 + self.CR / 2
        else:
            self.Threshold = 0.4 - self.CR / 3
        if self.CR >= self.Threshold or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_19(BaseStrategy):
    """
    A robust and adaptive exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.round_phase = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        mid_round = math.ceil(self.game_description.n_rounds / 2)
        if self.round_phase is None:
            if state.round_number <= 3:
                self.round_phase = 'initial'
            elif state.round_number <= mid_round:
                self.round_phase = 'exploration'
            else:
                self.round_phase = 'exploitation'
        if self.round_phase == 'initial':
            return Action.C
        elif self.round_phase == 'exploration':
            return Action.C if state.round_number % 2 == 0 else Action.D
        elif self.round_phase == 'exploitation':
            num_cooperators = np.sum(history.opponent_actions, axis=1).mean()
            return Action.C if num_cooperators > self.game_description.n_players / 2 else Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_20(BaseStrategy):
    """
    Exploitative Strategy: "Conditional Cooperator with Adaptive Punishment"
    
    This strategy is designed to exploit opponents who tend to cooperate while also adapting to those who defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :])
        majority_cooperated = prev_round_cooperators > self.game_description.n_players / 2
        action = Action.D
        if majority_cooperated:
            action = Action.C
        punishment_threshold = math.ceil(self.game_description.n_players / 4)
        for opponent_action in history.opponent_actions[-1, :]:
            if not opponent_action and history.my_actions[-1] == True:
                self.consecutive_defections += 1
                if self.consecutive_defections >= punishment_threshold:
                    action = Action.D
        if state.round_number == self.game_description.n_rounds - 1 and history.my_actions[-1] == True:
            action = Action.C
        return action

class Strategy_EXPLOITATIVE_21(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to exploit the cooperative tendencies of opponents while minimizing its own contributions to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.round_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        self.round_count += 1
        if history is None:
            cooperators = 0
        else:
            cooperators = sum(history.opponent_actions[-1, :])
        if cooperators <= self.game_description.n_players / 2:
            p_c = self.game_description.k / self.game_description.n_players / (1 + self.game_description.k / self.game_description.n_players)
            return Action.C if random.random() < p_c else Action.D
        if cooperators > self.game_description.n_players / 2:
            return Action.D
        if self.round_count == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_22(BaseStrategy):
    """
    A strategy that balances exploration and exploitation by dynamically adjusting its cooperation level based on observed opponent behavior, the game's history, and its own performance.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff = 0.0
        self.cooperators_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_payoffs = np.mean(history.my_payoffs[:state.round_number])
        self.cooperators_prev = sum(history.opponent_actions[state.round_number - 1, :])
        exploitability = avg_payoffs / self.game_description.k * (self.game_description.n_players - 1)
        if exploitability > 0.5:
            action = Action.D
        elif self.cooperators_prev >= self.game_description.n_players / 2:
            action = Action.C
        else:
            action = Action.D
        if state.round_number > 1 and history.my_actions[state.round_number - 1] == False and (history.my_payoffs[state.round_number - 1] < avg_payoffs):
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_23(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.alpha = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == 1:
            return Action.C
        avg_cooperators = np.mean(history.my_actions[:state.round_number])
        opponent_defected = np.any(np.logical_and(history.opponent_actions[state.round_number - 1, :] == False, history.opponent_actions[:, :] != False), axis=0)
        if avg_cooperators > self.alpha:
            return Action.D
        elif np.any(opponent_defected):
            return Action.D
        else:
            return Action.C
        if state.round_number > 1 and avg_cooperators > self.alpha and (history.my_payoffs[state.round_number - 1] > np.mean(history.my_payoffs[:state.round_number])):
            self.alpha -= 0.05
        elif state.round_number > 1 and avg_cooperators < self.alpha and (history.my_payoffs[state.round_number - 1] < np.mean(history.my_payoffs[:state.round_number])):
            self.alpha += 0.05
        if state.round_number == self.game_description.n_rounds:
            return Action.D

class Strategy_EXPLOITATIVE_24(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior while prioritizing personal payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 and history is None:
            return Action.C if random.random() < 0.5 else Action.D
        prev_round_coops = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if prev_round_coops >= self.game_description.n_players / 2:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_25(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_opponent_coop = 0.5
        self.coop_rate = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        if state.round_number < self.game_description.n_rounds // 2:
            cooperators = sum(history.opponent_actions[-1, :])
            if cooperators >= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds // 2:
            opponent_coop_rates = np.mean(history.opponent_actions, axis=0)
            self.avg_opponent_coop = np.mean(opponent_coop_rates)
            if self.avg_opponent_coop >= 0.5:
                self.coop_rate = self.game_description.k / self.game_description.n_players * (1 - self.avg_opponent_coop)
        opponent_coop_rate = np.mean(history.opponent_actions[-1, :])
        if opponent_coop_rate > 0.5:
            return Action.D
        else:
            return Action.C if random.random() < self.coop_rate else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_26(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and maximizes payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.average_cooperation_rate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.k / self.game_description.n_players else Action.D
        if history is not None:
            opponent_cooperations = np.sum(history.opponent_actions, axis=1)
            total_rounds = state.round_number - 1
            self.average_cooperation_rate = np.mean(opponent_cooperations[:total_rounds])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        exploitation_threshold = self.game_description.k / self.game_description.n_players
        if self.average_cooperation_rate is not None and self.average_cooperation_rate < exploitation_threshold:
            return Action.D
        elif self.average_cooperation_rate == exploitation_threshold:
            return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_27(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponents = [None] * game_description.n_players
        self.avg_payoff = 0
        self.prev_action = None
        self.payoffs_history = []

    def calculate_payoff(self, action: Action, opponents: list[str]) -> float:
        c_i = int(action == Action.C)
        total_contributions = sum((1 for x in opponents if x == 'COOP')) + (c_i if self.prev_action is not None else 0)
        return 1 - c_i + self.game_description.k / self.game_description.n_players * total_contributions

    def classify_opponent(self, opponent_index: int, opponent_payoff: float, opponent_action: Action):
        if opponent_action == Action.C and opponent_payoff >= 1:
            return 'COOP'
        elif opponent_action == Action.D:
            return 'DEF'
        else:
            return 'FR'

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.prev_action = Action.C
            return Action.C
        for i, opponent_payoff in enumerate(history.opponent_payoffs[-1]):
            self.opponents[i] = self.classify_opponent(i, opponent_payoff, history.opponent_actions[-1, i])
        coop_count = sum((1 for x in self.opponents if x == 'COOP'))
        def_count = self.game_description.n_players - coop_count
        if coop_count >= self.game_description.n_players / 2:
            action = Action.C
        elif def_count >= self.game_description.n_players / 2:
            action = Action.D
        else:
            action = Action.C if self.prev_action == Action.C else Action.D
        payoff = self.calculate_payoff(action, self.opponents)
        self.payoffs_history.append(payoff)
        if len(self.payoffs_history) > 5:
            self.avg_payoff = np.mean(self.payoffs_history[-5:])
        else:
            self.avg_payoff = np.mean(self.payoffs_history)
        if payoff < self.avg_payoff and state.round_number != self.game_description.n_rounds - 1:
            action = Action.D if action == Action.C else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        self.prev_action = action
        return action

class Strategy_EXPLOITATIVE_28(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to exploit opponents by adapting to their behavior 
    while maintaining a balance between cooperation and defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        opponent_actions = np.sum(history.opponent_actions, axis=0) / len(history.opponent_actions)
        for i, opponent_action in enumerate(opponent_actions):
            if opponent_action < 0.5 and total_cooperators < self.game_description.k:
                return Action.D
            if opponent_action < 0.25:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if total_cooperators >= self.game_description.k:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_29(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to be exploitative and adaptive, 
    taking advantage of the game's structure and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.5
        self.avg_cooperator_payoff = 0
        self.avg_defector_payoff = 0
        self.opponent_frequencies = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None and len(history.my_payoffs) > 1:
            self.avg_cooperator_payoff = np.mean([payoff for payoff, action in zip(history.my_payoffs[:-1], history.my_actions[:-1]) if action])
            self.avg_defector_payoff = np.mean([payoff for payoff, action in zip(history.my_payoffs[:-1], history.my_actions[:-1]) if not action])
        if history is not None and len(history.opponent_actions) > 0:
            for i, actions in enumerate(history.opponent_actions.T):
                self.opponent_frequencies[i] = np.mean(actions)
        if self.avg_cooperator_payoff > self.avg_defector_payoff and state.round_number < self.game_description.n_rounds - 1:
            self.cooperate_prob = self.game_description.k / self.game_description.n_players * (self.avg_cooperator_payoff / self.avg_defector_payoff)
            action = Action.C if random.random() < self.cooperate_prob else Action.D
        else:
            exploit_opponents = [freq < 0.3 for freq in self.opponent_frequencies]
            if any(exploit_opponents):
                action = Action.D
            elif all(self.opponent_frequencies > 0.7):
                action = Action.C if random.random() < 0.8 else Action.D
            elif state.round_number == self.game_description.n_rounds - 1:
                action = Action.D
            elif history is not None and len(history.opponent_actions) > 0:
                punish_opponents = [actions[-2] and (not actions[-1]) for actions in history.opponent_actions.T]
                if any(punish_opponents):
                    self.cooperate_prob *= 0.8
                action = Action.C if random.random() < self.cooperate_prob else Action.D
            else:
                action = Action.C if random.random() < 0.4 else Action.D
        return action

class Strategy_EXPLOITATIVE_30(BaseStrategy):
    """
    An adaptive strategy that balances cooperation and defection to maximize payoffs while exploiting predictable opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_thresh = (game_description.n_players - 1) / game_description.n_players * game_description.k
        self.opponents_coop = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        total_cooperators = sum(self.opponents_coop)
        if total_cooperators > self.coop_thresh or self.detect_exploitable_opponent(history):
            action = Action.D
        else:
            action = Action.C
        if history is not None:
            for i, opponent in enumerate(history.opponent_actions[-1]):
                if opponent:
                    self.opponents_coop[i] += 1
        if state.round_number % 3 == 0 and state.round_number > 2:
            self.adjust_coop_thresh()
        return action

    def detect_exploitable_opponent(self, history: PlayerHistory) -> bool:
        for i, opponent in enumerate(history.opponent_actions[-1]):
            if opponent and (not history.my_actions[-1]):
                return True
        return False

    def adjust_coop_thresh(self):
        total_cooperators = sum(self.opponents_coop)
        if total_cooperators > self.game_description.n_players / 2:
            self.coop_thresh -= 0.05 * self.game_description.k / self.game_description.n_players
        else:
            self.coop_thresh += 0.05 * self.game_description.k / self.game_description.n_players
        self.coop_thresh = max(0.5 * self.game_description.k / self.game_description.n_players, min(self.coop_thresh, (self.game_description.n_players - 1) / self.game_description.n_players * self.game_description.k))

class Strategy_EXPLOITATIVE_31(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts based on observed cooperation rates and payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_contributions = 0
        self.my_total_payoff = 0
        self.opponents_cooperation_rate = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history.my_actions[state.round_number - 1]:
            self.total_contributions += 1
        self.my_total_payoff += history.my_payoffs[state.round_number - 1]
        opponents_coop_rate = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        self.opponents_cooperation_rate.append(opponents_coop_rate)
        if state.round_number < self.game_description.n_rounds // 2:
            avg_coop_rate = sum(self.opponents_cooperation_rate) / len(self.opponents_cooperation_rate)
            if avg_coop_rate > 0.5:
                return Action.D
            else:
                potential_payoff_defect = 1 + self.game_description.k / self.game_description.n_players * self.total_contributions
                potential_payoff_cooperate = self.game_description.k / self.game_description.n_players * (self.total_contributions + 1)
                return Action.C if potential_payoff_cooperate > potential_payoff_defect else Action.D
        elif state.round_number >= self.game_description.n_rounds // 2:
            coop_rate_trend = sum(self.opponents_cooperation_rate[-(self.game_description.n_rounds // 4):]) / (self.game_description.n_rounds // 4)
            if coop_rate_trend < 0.5:
                return Action.D
            else:
                return Action.C
        elif state.round_number > self.game_description.n_rounds // 2 and history.my_payoffs[state.round_number - 1] < max(history.opponent_payoffs[-1, :]):
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_32(BaseStrategy):
    """
    An adaptive strategy that balances exploiting opponents' cooperativeness and adapting to changing behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold_coop = game_description.k / game_description.n_players * (game_description.n_players / 2)
        self.window_size = 3
        self.coop_rate_window = [0.0] * self.window_size

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        avg_coop_rate_prev_round = np.mean(history.opponent_actions[-1, :])
        self.coop_rate_window[state.round_number % self.window_size] = avg_coop_rate_prev_round
        coop_rate_window_avg = sum(self.coop_rate_window) / len(self.coop_rate_window)
        if state.round_number < self.game_description.n_rounds - 1:
            if coop_rate_window_avg > self.threshold_coop:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_33(BaseStrategy):
    """
    Exploitative strategy that adapts to the game's history while punishing uncooperative behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators_prev_round = 0
        self.opponent_behavior = [False] * game_description.n_players
        self.punished_opponents = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        for i in range(self.game_description.n_players):
            if self.opponent_behavior[i] and (not history.opponent_actions[-1, i]):
                if i not in self.punished_opponents:
                    self.punished_opponents.append(i)
                if len(self.punished_opponents) >= 2:
                    self.punished_opponents.pop(0)
        self.total_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        if self.total_cooperators_prev_round <= self.game_description.n_players / 2 or state.round_number >= self.game_description.n_rounds - 2:
            return Action.D
        p = self.game_description.k / self.game_description.n_players * (self.total_cooperators_prev_round / self.game_description.n_players)
        if random.random() < p and (not any(self.punished_opponents)):
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_34(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Gradient Ascent (ATTA) strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.average_payoff_window_size = 5
        self.opponent_consistency_threshold = 0.9
        self.my_previous_actions = None
        self.my_previous_payoffs = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions_last_round = history.opponent_actions[-1, :]
        opponents_cooperated_last_round = sum(opponent_actions_last_round)
        my_payoffs = history.my_payoffs
        last_few_rounds_average_payoff = np.mean(my_payoffs[max(0, state.round_number - self.average_payoff_window_size):])
        if state.round_number >= 2:
            previous_few_rounds_average_payoff = np.mean(my_payoffs[max(0, state.round_number - 1 - self.average_payoff_window_size):-1])
            if last_few_rounds_average_payoff > previous_few_rounds_average_payoff:
                self.cooperation_threshold *= 1.1
            elif last_few_rounds_average_payoff < previous_few_rounds_average_payoff:
                self.cooperation_threshold /= 1.1
        if opponents_cooperated_last_round >= self.cooperation_threshold * self.game_description.n_players:
            return Action.C
        opponent_consistency = np.mean(~opponent_actions_last_round)
        if opponent_consistency > self.opponent_consistency_threshold or state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_35(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to exploit the opponent's tendencies 
    while being robust against a wide range of behaviors. It adjusts its actions based 
    on the cooperation rates and payoff differences in previous rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate_history = []
        self.payoff_diff_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        avg_coop_rate = opponent_cooperations / self.game_description.n_players
        self.cooperation_rate_history.append(avg_coop_rate)
        if len(self.payoff_diff_history) < 3:
            return Action.D
        last_three_rounds_payoffs = history.my_payoffs[-3:]
        avg_last_three_payoffs = np.mean(last_three_rounds_payoffs)
        opponent_avg_payoffs = np.mean(history.opponent_payoffs[-1, :])
        payoff_diff = avg_last_three_payoffs - opponent_avg_payoffs
        self.payoff_diff_history.append(payoff_diff)
        if state.round_number <= 5:
            return Action.C if avg_coop_rate > 0.5 else Action.D
        elif state.round_number < self.game_description.n_rounds // 2:
            avg_payoff_diff = np.mean(self.payoff_diff_history[-3:])
            return Action.C if avg_payoff_diff > 0 else Action.D
        else:
            last_three_coop_rates = self.cooperation_rate_history[-3:]
            avg_last_three_coop_rates = np.mean(last_three_coop_rates)
            if avg_last_three_coop_rates < 0.25:
                return Action.D
            prob_coop = avg_last_three_coop_rates / self.game_description.n_players
            return Action.C if random.random() < prob_coop else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

    def averagecooprate(self, round_number: int) -> float:
        if round_number < len(self.cooperation_rate_history):
            return self.cooperation_rate_history[round_number]
        else:
            opponent_cooperations = sum(history.opponent_actions[round_number, :])
            return opponent_cooperations / self.game_description.n_players

    def avg_payoff_diff_last_3_rounds(self) -> float:
        if len(self.payoff_diff_history) < 3:
            return 0
        else:
            return np.mean(self.payoff_diff_history[-3:])

class Strategy_EXPLOITATIVE_36(BaseStrategy):
    """
    An exploitative strategy that adapts to the game's dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        if state.round_number + 1 == self.game_description.n_rounds:
            my_payoff = np.sum(history.my_payoffs)
            avg_opponent_payoff = np.mean(np.sum(history.opponent_payoffs, axis=0))
            return Action.C if my_payoff < avg_opponent_payoff else Action.D
        prev_total_coop = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_total_coop <= self.game_description.n_players // 2:
            my_prev_payoff = history.my_payoffs[-1]
            avg_opponent_payoff = np.mean(history.opponent_payoffs[-1, :])
            return Action.C if my_prev_payoff > avg_opponent_payoff else Action.D
        coop_prob = self.game_description.k / self.game_description.n_players * (prev_total_coop - self.game_description.n_players // 2)
        return Action.D if random.random() < coop_prob else Action.C

class Strategy_EXPLOITATIVE_37(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators = 0
        self.avg_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.game_description.n_players == 1:
            return Action.C
        elif self.game_description.n_players == 2 and state.round_number % 2 == 1:
            return Action.C
        else:
            prev_round_opponents = history.opponent_actions[-1, :]
            total_cooperators_prev = sum(prev_round_opponents)
            if state.round_number < self.game_description.n_rounds // 2:
                if total_cooperators_prev >= self.game_description.n_players * 0.5:
                    action = Action.C
                else:
                    action = Action.D
            elif self.avg_payoff < self.game_description.k / self.game_description.n_players * self.game_description.n_players * 0.5:
                action = Action.D
            elif history.my_actions[-1]:
                action = Action.C
            else:
                action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        self.total_cooperators += total_cooperators_prev
        if history.my_payoffs.size > 0:
            self.avg_payoff = np.mean(history.my_payoffs)
        return action

class Strategy_EXPLOITATIVE_38(BaseStrategy):
    """
    An exploitative strategy that adapts to the behavior of other players.
    It initially cooperates, then adjusts based on observed payoffs and punishes defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.pi_C = 0.0
        self.pi_D = 0.0
        self.cooperation_count = 0
        self.exploited_players = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_payoff = self.calculate_my_payoff(history)
        opponent_payoffs = history.opponent_payoffs[-1, :]
        if state.round_number < self.game_description.n_players:
            self.pi_C += np.mean(opponent_payoffs[history.opponent_actions[-1, :] == 1])
            self.pi_D += np.mean(opponent_payoffs[history.opponent_actions[-1, :] == 0])
            self.cooperation_count += sum(history.opponent_actions[-1, :])
        if state.round_number >= self.game_description.n_players:
            if self.pi_C > self.pi_D and (not self.is_punishing()):
                return Action.C
            for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
                if opponent_action == 0 and history.my_actions[-1] == 1:
                    self.exploited_players.add(i)
            if not self.reciprocity_check(history):
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

    def calculate_my_payoff(self, history: PlayerHistory) -> float:
        """
        Calculate the payoff for this player in the previous round.
        """
        my_action = history.my_actions[-1]
        opponent_actions = history.opponent_actions[-1, :]
        total_cooperators = sum(opponent_actions) + int(my_action)
        return 1 - my_action + self.game_description.k / self.game_description.n_players * total_cooperators

    def is_punishing(self) -> bool:
        """
        Check if we are punishing any players in the next two rounds.
        """
        return len(self.exploited_players) > 0 and state.round_number < self.game_description.n_rounds - 2

    def reciprocity_check(self, history: PlayerHistory) -> bool:
        """
        Check if at least one opponent cooperated in the previous round.
        """
        return sum(history.opponent_actions[-1, :]) > 0

class Strategy_EXPLOITATIVE_39(BaseStrategy):
    """
    The Adaptive Exploiter strategy is based on a simple yet effective principle: 
    cooperate when others are likely to cooperate, and defect when others are likely to defect.
    This approach allows us to exploit the cooperation of others while minimizing our own losses.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5
        self.avg_coop_rate = 0

    def calculate_avg_coop_rate(self, history: PlayerHistory) -> float:
        coop_count = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        return coop_count / self.game_description.n_players

    def update_theta(self, avg_coop_rate_10: float) -> None:
        if avg_coop_rate_10 > 0.7:
            self.theta += 0.1
        else:
            self.theta -= 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return np.random.choice([Action.C, Action.D], p=[0.5, 0.5])
        self.avg_coop_rate = self.calculate_avg_coop_rate(history)
        if self.avg_coop_rate > self.theta:
            return np.random.choice([Action.C, Action.D], p=[0.8, 0.2])
        else:
            return np.random.choice([Action.C, Action.D], p=[0.1, 0.9])
        if state.round_number % 5 == 0 and state.round_number > 10:
            avg_coop_rate_10 = sum(history.my_actions[-10:]) / 10
            self.update_theta(avg_coop_rate_10)
        return Action.D

class Strategy_EXPLOITATIVE_40(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behaviors by tracking the average payoff difference between cooperating and defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.alpha = 0.1
        self.tau = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_payoff = history.my_payoffs[-1]
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        defecting_payoff = 1 + self.game_description.k / self.game_description.n_players * opponent_cooperators
        delta = prev_round_payoff - defecting_payoff
        self.tau = (1 - self.alpha) * self.tau + self.alpha * delta
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if abs(delta - self.tau) < 1e-06:
            prev_action = history.my_actions[-1]
            return Action.C if prev_action else Action.D
        return Action.D if delta > self.tau else Action.C

class Strategy_EXPLOITATIVE_41(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_c_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C
        opponent_actions = history.opponent_actions[:-1]
        avg_c = np.mean(opponent_actions) / self.game_description.n_players
        self.avg_c_history.append(avg_c)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif avg_c < 0.1:
            return Action.D
        elif avg_c > 0.9:
            return Action.C
        k = self.game_description.k
        n = self.game_description.n_players
        EC = k / n * avg_c + (1 - avg_c)
        ED = 1 + k / n * avg_c
        if EC > ED:
            return Action.C
        elif EC == ED:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_43(BaseStrategy):
    """
    Exploitative strategy that adapts to the cooperation level of other players while exploiting their cooperative behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        theta = self.game_description.k / self.game_description.n_players
        cooperators = sum(history.opponent_actions[-1, :])
        opponents_cooperation_rate = np.mean(history.opponent_actions)
        expected_defect_payoff = 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if cooperators >= theta or opponents_cooperation_rate > expected_defect_payoff:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_44(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.history_my_actions = None
        self.history_my_payoffs = None
        self.history_opponent_actions = None
        self.history_opponent_payoffs = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.history_my_actions = history.my_actions
        self.history_my_payoffs = history.my_payoffs
        self.history_opponent_actions = history.opponent_actions
        self.history_opponent_payoffs = history.opponent_payoffs
        avg_cooperators = np.sum(self.history_opponent_actions, axis=1).mean()
        if avg_cooperators >= self.game_description.n_players / 2:
            return Action.D
        pi_prev = history.my_payoffs[-1]
        avg_payoff = np.mean(history.opponent_payoffs, axis=0)[-1]
        if pi_prev < avg_payoff and any((not action for action in self.history_opponent_actions[-1])):
            return Action.D
        if self.game_description.k / self.game_description.n_players > 0.5 and any((not action for action in self.history_opponent_actions[-1])):
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_45(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to maximize its own payoff by 
    adaptively responding to the observed behavior of other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_rate_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        opponent_coop_rates = np.array([np.sum(history.opponent_actions[-2:, i]) for i in range(self.game_description.n_players - 1)])
        avg_coop_rate = np.mean(opponent_coop_rates)
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        elif avg_coop_rate >= self.game_description.k / self.game_description.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_46(BaseStrategy):
    """
    The Adaptive Raider strategy exploits cooperative tendencies in opponents while minimizing losses from mutual defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_coop = None
        self.total_coop = 0
        self.theta_t = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coop = self.prev_coop
        total_coop = self.total_coop
        theta_t = self.theta_t
        if history is not None:
            prev_payoff = history.my_payoffs[-1]
            opponent_actions = history.opponent_actions[-1, :]
            other_player_coop = np.any(opponent_actions)
            if prev_coop and prev_payoff >= 1:
                return Action.C
            elif not prev_coop and other_player_coop:
                return Action.D
            elif total_coop >= self.game_description.n_players / 2:
                return Action.D
        theta_t = total_coop / state.round_number
        if theta_t >= self.game_description.k / self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        self.prev_coop = action == Action.C
        self.total_coop += int(self.prev_coop)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_47(BaseStrategy):
    """
    Adaptive Cooperate-Defect (ACD) Strategy.
    
    This strategy aims to exploit opponents' behaviors by adapting to their cooperation rates 
    while maintaining a balance between short-term gains and long-term cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_ACRs = []
        self.acr_window_size = max(1, int(game_description.n_rounds / 3))
        self.alternate_count = 0

    def alternate(self) -> Action:
        """
        Alternate between cooperate and defect to maintain uncertainty.
        
        Returns:
            Action: The next action in the alternation sequence.
        """
        self.alternate_count += 1
        if self.alternate_count % 2 == 0:
            return Action.C
        else:
            return Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_opponents = self.game_description.n_players - 1
        opponent_coop_rates = history.opponent_actions[-min(self.acr_window_size, state.round_number), :]
        avg_ACR = np.mean(opponent_coop_rates)
        if len(self.opponent_ACRs) < self.game_description.n_rounds:
            self.opponent_ACRs.append(avg_ACR)
        if state.round_number < int(self.game_description.n_rounds / 3):
            return Action.C if avg_ACR > 0.5 else Action.D
        elif state.round_number >= int(self.game_description.n_rounds / 3) and state.round_number < int(2 * self.game_description.n_rounds / 3):
            prev_ACR = self.opponent_ACRs[state.round_number - 2] if state.round_number > 1 else avg_ACR
            if avg_ACR > 0.5:
                return Action.C
            elif avg_ACR < 0.5 and avg_ACR > prev_ACR:
                return Action.D
            else:
                return self.alternate()
        elif avg_ACR > 0.5:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_49(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop = 0.0
        self.theta = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C
        if state.round_number == 3:
            self.avg_coop = np.mean(history.my_actions[:3])
        if self.avg_coop > self.theta:
            action = Action.D
        else:
            action = Action.C
        if history is not None and state.round_number > 0:
            prev_opponent_actions = history.opponent_actions[-2, :]
            curr_opponent_actions = history.opponent_actions[-1, :]
            for i in range(self.game_description.n_players - 1):
                if prev_opponent_actions[i] and (not curr_opponent_actions[i]):
                    self.theta -= 0.1
                elif not prev_opponent_actions[i] and curr_opponent_actions[i]:
                    self.theta += 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_50(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rates = []
        self.opponent_models = [{} for _ in range(game_description.n_players)]
        self.exploitation_phase = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < 6:
            if state.round_number == 1:
                return random.choice([Action.C, Action.D])
            else:
                return Action.D
        if not self.exploitation_phase:
            avg_cooperation_rate = self.calculate_avg_cooperation_rate(history.opponent_actions, state.round_number - 1)
            self.cooperation_rates.append(avg_cooperation_rate)
            if avg_cooperation_rate > 0.5:
                return Action.C
            else:
                return Action.D
        most_cooperative_opponents = self.identify_most_cooperative_opponents(history.opponent_actions, state.round_number - 1)
        if len(most_cooperative_opponents) > 0 and state.round_number < self.game_description.n_rounds:
            return Action.D
        else:
            return Action.C

    def calculate_avg_cooperation_rate(self, opponent_actions: NDArray[np.bool_], round_number: int):
        cooperation_rates = np.mean(opponent_actions[:round_number + 1, :], axis=0)
        return np.mean(cooperation_rates)

    def identify_most_cooperative_opponents(self, opponent_actions: NDArray[np.bool_], round_number: int):
        if not self.exploitation_phase:
            avg_cooperation_rate = self.calculate_avg_cooperation_rate(opponent_actions, round_number - 1)
            self.cooperation_rates.append(avg_cooperation_rate)
            if len(self.cooperation_rates) == 5 and np.mean(self.cooperation_rates) > 0.5:
                self.exploitation_phase = True
        opponent_cooperations = np.sum(opponent_actions[:round_number + 1, :], axis=0) / (round_number + 1)
        top_25_percent_index = int(0.75 * len(opponent_cooperations))
        most_cooperative_opponents = np.argsort(-opponent_cooperations)[:top_25_percent_index]
        return most_cooperative_opponents

class Strategy_EXPLOITATIVE_51(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_defected_twice = [False] * game_description.n_players
        self.opponent_cooperated_twice = [False] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_payoff = np.mean(history.my_payoffs)
        if state.round_number < self.game_description.n_rounds // 2:
            num_cooperators_prev_round = sum((1 for action in history.opponent_actions[-1, :] if action))
            if num_cooperators_prev_round > self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        elif state.round_number >= self.game_description.n_rounds // 2:
            my_total_payoff = sum(history.my_payoffs)
            if my_total_payoff < avg_payoff:
                return Action.C
            else:
                return Action.D
        elif state.round_number > self.game_description.n_rounds * 3 // 4:
            top_players = [i for i, payoff in enumerate(history.opponent_payoffs[-1, :]) if payoff > avg_payoff]
            if len(top_players) <= self.game_description.n_players / 2 and history.my_payoffs[-1] in top_players:
                return Action.C
            else:
                return Action.D
        for opponent in range(self.game_description.n_players):
            if not history.opponent_actions[-1, opponent] and (state.round_number > 1 and (not history.opponent_actions[-2, opponent])):
                self.opponent_defected_twice[opponent] = True
                return Action.D
        for opponent in range(self.game_description.n_players):
            if history.opponent_actions[-1, opponent] and (state.round_number > 1 and history.opponent_actions[-2, opponent]):
                self.opponent_cooperated_twice[opponent] = True
                if self.opponent_defected_twice[opponent]:
                    self.opponent_defected_twice[opponent] = False
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_52(BaseStrategy):
    """
    An adaptive strategy that exploits opponents' cooperation while encouraging more cooperative behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_per_cooperator = 0
        self.exploitation_threshold = 1
        self.total_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_cooperators = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
        total_payoffs = np.sum(history.opponent_payoffs[-1, :]) + history.my_payoffs[-1]
        self.avg_payoff_per_cooperator = total_payoffs / num_cooperators if num_cooperators > 0 else 0
        if self.avg_payoff_per_cooperator > self.exploitation_threshold:
            return Action.D
        elif num_cooperators <= self.game_description.n_players // 2:
            return Action.C if random.random() < 0.25 else Action.D
        else:
            return Action.C
        self.total_payoff += history.my_payoffs[-1]
        if state.round_number % 5 == 0:
            self.adapt_exploitation_threshold()

    def adapt_exploitation_threshold(self):
        avg_performance = self.total_payoff / 5
        target_performance = self.game_description.n_players * self.game_description.k / 2
        if avg_performance < target_performance:
            self.exploitation_threshold -= 0.1
        else:
            self.exploitation_threshold += 0.1

class Strategy_EXPLOITATIVE_53(BaseStrategy):
    """
    Robust and adaptive exploitative strategy for the N-Player Public Goods Game.
    Leverages game parameters and history to maximize payoff without relying on cooperation or coordination with other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.n_players >= 10 else Action.D
        total_cooperators_t_minus_1 = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        action = Action.C if total_cooperators_t_minus_1 >= self.game_description.n_players / 2 else Action.D
        if history.my_payoffs[-1] < np.mean(history.opponent_payoffs[-1, :]) and history.my_actions[-1]:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_54(BaseStrategy):
    """
    The Adaptive Exploiter strategy balances adaptability with robustness against various opponent behaviors.
    It starts by cooperating, then adapts its behavior based on the average cooperation rate of opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_c = 0.5
        self.theta = game_description.k / game_description.n_players * 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number + 1 == self.game_description.n_rounds:
            return Action.C if state.round_number == 0 else Action.D
        opponent_actions = history.opponent_actions[-1, :]
        avg_c_t_minus_1 = np.mean(opponent_actions)
        if avg_c_t_minus_1 <= 0.001:
            return Action.D
        elif avg_c_t_minus_1 >= 0.999:
            return Action.C
        self.avg_c = avg_c_t_minus_1
        if self.avg_c > self.theta:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_55(BaseStrategy):
    """
    The Adaptive Exploiter is a robust and exploitative strategy for the N-Player Public Goods Game.
    It initially explores the opponents' behavior, then adapts its cooperation probability based on the number of cooperators.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_C = 0.5
        self.avg_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history.my_actions.size < 1 or history.opponent_actions.shape[0] < 1:
            return Action.C
        if state.round_number <= 2:
            if random.random() < self.p_C:
                return Action.C
            else:
                return Action.D
        T = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if T <= self.game_description.n_players / 2:
            self.p_C = max(0.01, min(self.game_description.k / self.game_description.n_players, 0.99))
        else:
            self.p_C = max(0.01, min(1 - self.game_description.k / self.game_description.n_players, 0.99))
        if random.random() < self.p_C:
            return Action.C
        else:
            return Action.D
        if state.round_number % 5 == 0 and state.round_number > 2:
            avg_payoff = np.mean(history.my_payoffs[-5:])
            self.avg_payoffs.append(avg_payoff)
            if avg_payoff > self.game_description.k / 2:
                self.p_C = min(self.p_C + 0.1, 0.99)
            elif avg_payoff < 1:
                self.p_C = max(self.p_C - 0.1, 0.01)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_56(BaseStrategy):
    """
    Exploitative strategy "Adaptive Predator" for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.previous_round_cooperators = 0
        self.previous_round_average_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        prev_opponent_actions = history.opponent_actions[-1]
        prev_my_payoff = history.my_payoffs[-1]
        self.previous_round_cooperators = sum(prev_opponent_actions) + (history.my_actions[-1] == Action.C)
        opponent_payoffs = np.sum(history.opponent_payoffs[-1])
        total_payoff = prev_my_payoff + opponent_payoffs
        self.previous_round_average_payoff = total_payoff / self.game_description.n_players
        if self.previous_round_cooperators >= self.game_description.n_players / 2 and self.previous_round_average_payoff >= self.game_description.k / 2:
            return Action.C
        opponent_defected_and_exploited = np.any(prev_opponent_actions == Action.D) and prev_my_payoff < np.max(history.opponent_payoffs[-1])
        if opponent_defected_and_exploited:
            return Action.C
        if state.round_number + 1 == self.game_description.n_rounds:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_57(BaseStrategy):
    """
    A dynamic strategy for the N-player public goods game that incorporates both reactive and proactive elements.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_init = 0.5
        self.p_react = 0.7
        self.th_payoff = 1.2
        self.p_explore = 0.1
        self.x_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        actions_prev = [Action.C if action else Action.D for action in history.my_actions]
        payoffs_prev = history.my_payoffs
        opponent_actions_prev = np.where(history.opponent_actions, Action.C, Action.D)
        opponent_payoffs_prev = history.opponent_payoffs
        avg_payoff_opponents = np.mean(opponent_payoffs_prev[-min(5, state.round_number):])
        num_cooperators_prev = sum((1 for action in opponent_actions_prev[-1] if action == Action.C))
        proactive_action = Action.D if avg_payoff_opponents > self.th_payoff else None
        explore_action = random.choice([Action.C, Action.D]) if random.random() < self.p_explore else None
        consecutive_defection_break = random.random() < 0.4 and self.x_defections > 2
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        elif num_cooperators_prev >= self.game_description.n_players / 2:
            action = Action.D
        elif random.random() < self.p_react or consecutive_defection_break:
            action = Action.C
        else:
            action = proactive_action or explore_action or Action.D
        self.x_defections += 1 if action == Action.D else 0
        return action

class Strategy_EXPLOITATIVE_58(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_C = game_description.k / game_description.n_players
        self.w = min(5, game_description.n_rounds)
        self.c_prev = 0
        self.avg_coop_rate = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_C else Action.D
        self.c_prev = sum(history.opponent_actions[-1, :])
        p_C_new = self.game_description.k / self.game_description.n_players * (self.game_description.n_players - self.c_prev) / (self.game_description.n_players - 1)
        opponent_cooperations = history.opponent_actions[max(0, state.round_number - self.w):state.round_number]
        avg_coop_rate = np.mean(opponent_cooperations)
        self.avg_coop_rate.append(avg_coop_rate)
        if avg_coop_rate > 0.6:
            p_D = 0.8
        else:
            p_D = 1 - p_C_new
        reciprocate = history.opponent_actions[-1, 0] if state.round_number > 0 else False
        if reciprocate:
            p_C_new += 0.1
        action = Action.C if random.random() < p_C_new else Action.D
        if state.round_number == self.game_description.n_rounds - 1 and (sum(history.my_payoffs) / state.round_number < np.mean(history.opponent_payoffs, axis=0).mean() or self.c_prev < self.game_description.n_players / 2):
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_59(BaseStrategy):
    """
    Adaptive Exploiter strategy for N-Player Public Goods Game.

    This strategy aims to exploit opponents who cooperate too much or too little.
    It cooperates when cooperators have higher average payoffs and defects otherwise,
    taking advantage of over-cooperative opponents while minimizing losses against
    over-defective ones.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        if self.game_description.n_players == 2:
            return Action.C if random.random() < 0.5 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_round_coops = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history is not None else False)
        avg_coop_payoff = np.mean(np.concatenate(([p for i, p in enumerate(history.opponent_payoffs[-1, :]) if history.opponent_actions[-1, i]], [history.my_payoffs[-1]] if history is not None and history.my_actions[-1] else [])))
        avg_defect_payoff = np.mean(np.concatenate(([p for i, p in enumerate(history.opponent_payoffs[-1, :]) if not history.opponent_actions[-1, i]], [history.my_payoffs[-1]] if history is not None and (not history.my_actions[-1]) else [])))
        avg_coop_payoff = 0 if np.isnan(avg_coop_payoff) else avg_coop_payoff
        avg_defect_payoff = 0 if np.isnan(avg_defect_payoff) else avg_defect_payoff
        return Action.C if avg_coop_payoff > avg_defect_payoff else Action.D

class Strategy_EXPLOITATIVE_60(BaseStrategy):
    """
    The K-Trigger strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punished_players = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 1)
        if previous_cooperators >= self.game_description.k:
            return Action.C
        for player in range(self.game_description.n_players - 1):
            if history.opponent_actions[-1, player] == 0 and previous_cooperators - (history.my_actions[-1] == 1) >= self.game_description.k:
                self.punished_players.append(player)
        if len(self.punished_players) > 0:
            if state.round_number <= history.my_payoffs.size + 2:
                return Action.D
            else:
                self.punished_players = []
        if previous_cooperators < self.game_description.k and history.my_actions[-1] == 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1 and previous_cooperators >= self.game_description.k:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_61(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history.
    This strategy balances cooperation and defection to maximize its own payoff in the tournament.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.history = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            return Action.C if random.random() < 0.5 else Action.D
        else:
            opponent_defected = any((not action for action in history.opponent_actions[-1, :]))
            if opponent_defected and random.random() < self.game_description.k / self.game_description.n_players:
                return Action.D
            cooperators = sum((1 for action in history.opponent_actions[-1, :] if action))
            if cooperators >= math.floor(self.game_description.k / 2):
                return Action.C
            else:
                return Action.D
            prev_cooperators = sum((1 for action in history.opponent_actions[-2, :] if action))
            curr_cooperators = sum((1 for action in history.opponent_actions[-1, :] if action))
            if curr_cooperators > prev_cooperators:
                return Action.C if random.random() < 0.6 else Action.D
            elif curr_cooperators < prev_cooperators:
                return Action.D if random.random() < 0.7 else Action.C
            for i, opponent_history in enumerate(history.opponent_actions):
                consecutive_defections = len(opponent_history) - next((j for j, x in enumerate(reversed(opponent_history)) if x), None)
                if consecutive_defections > 2:
                    return Action.D
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.D
            return Action.C

class Strategy_EXPLOITATIVE_62(BaseStrategy):
    """
    An exploitative strategy that adapts to various opponent behaviors and only depends on game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p = 0.5
        self.ct = 0
        self.prev_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p else Action.D
        n = self.game_description.n_players
        k = self.game_description.k
        opponent_actions = history.opponent_actions[-1, :]
        my_payoff = history.my_payoffs[-1]
        avg_c_payoff = 0
        avg_d_payoff = 0
        c_count = np.sum(opponent_actions) + (history.my_actions[-1] == Action.C)
        d_count = n - c_count
        if c_count > 0:
            avg_c_payoff = my_payoff / c_count
        if d_count > 0:
            opponent_avg_d_payoffs = history.opponent_payoffs[-1, :]
            avg_d_payoff = np.sum(opponent_avg_d_payoffs[opponent_actions == Action.D]) / d_count
        self.ct = (avg_c_payoff - avg_d_payoff) / k if k != 0 else 0
        self.p = max(0, min(1, self.p + self.ct / n))
        defect_trigger = np.sum(history.opponent_actions[-n + 2:-1, :] == Action.D) >= n - 2
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history.my_payoffs.shape[0] > n // 2 and np.all(history.my_actions[-(n // 2):] == Action.C):
            return Action.D
        if defect_trigger or random.random() >= self.p:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_65(BaseStrategy):
    """
    The Adaptive Wolfpack strategy exploits a wide range of opponent behaviors while adapting to the game's progression.
    It balances cooperation and defection to maximize payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = game_description.k / game_description.n_players
        self.punishment_rounds = 2
        self.predatory_opponents = []
        self.punishment_timer = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == 1:
            if history.opponent_actions[0, :].any():
                return Action.D
            else:
                return Action.C
        else:
            avg_coop_prev = np.mean(history.opponent_actions[-1, :] + (history.my_actions[-1] == True))
            if avg_coop_prev >= self.coop_threshold:
                action = Action.C
            else:
                action = Action.D
            for i in range(self.game_description.n_players - 1):
                opponent_defected = history.opponent_actions[-1, i]
                if opponent_defected and i not in self.predatory_opponents:
                    self.predatory_opponents.append(i)
                    self.punishment_timer[i] = self.punishment_rounds
            for opponent in self.predatory_opponents:
                if self.punishment_timer[opponent] > 0:
                    action = Action.D
                    self.punishment_timer[opponent] -= 1
            if state.round_number >= self.game_description.n_rounds - 2:
                expected_coop_payoff = self.game_description.k / self.game_description.n_players * np.sum(history.opponent_actions[-1, :] + (history.my_actions[-1] == True))
                expected_defect_payoff = 1 + self.game_description.k / self.game_description.n_players * np.sum(history.opponent_actions[-1, :])
                if expected_coop_payoff > expected_defect_payoff:
                    action = Action.C
                else:
                    action = Action.D
            return action

class Strategy_EXPLOITATIVE_66(BaseStrategy):
    """
    The Adaptive Predator strategy exploits a wide range of opponent behaviors while being robust and adaptive.
    It leverages game parameters and history to make informed decisions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cr_history = []
        self.opponent_cr = []

    def calculate_cooperation_ratio(self, round_number: int) -> float:
        """
        Calculate the cooperation ratio for a given round number.
        
        :param round_number: The round number to calculate the cooperation ratio for.
        :return: The cooperation ratio (between 0 and 1).
        """
        if round_number < len(self.cr_history):
            return self.cr_history[round_number]
        cooperators = sum(self.history.opponent_actions[round_number, :])
        cr = cooperators / self.game_description.n_players
        if len(self.cr_history) <= round_number:
            self.cr_history.extend([0.0] * (1 + round_number - len(self.cr_history)))
        self.cr_history[round_number] = cr
        return cr

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.history = history
        if state.round_number == 0:
            return Action.C
        cr_t_1 = self.calculate_cooperation_ratio(state.round_number - 1)
        opponent_cr_t_1 = np.mean(history.opponent_actions[max(0, state.round_number - 5):state.round_number, :])
        if len(self.opponent_cr) <= state.round_number:
            self.opponent_cr.extend([0.0] * (1 + state.round_number - len(self.opponent_cr)))
        self.opponent_cr[state.round_number] = opponent_cr_t_1
        if state.round_number < 3:
            return Action.C if cr_t_1 > 0.5 else Action.D
        action = Action.C if cr_t_1 > 0.5 else Action.D
        if opponent_cr_t_1 > 0.7:
            predatory_mode_rounds = min(3, state.round_number)
            if any((self.opponent_cr[state.round_number - i] > 0.7 for i in range(predatory_mode_rounds))):
                action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            my_total_payoff = sum(history.my_payoffs)
            opponent_total_payoff = np.sum(history.opponent_payoffs, axis=0).max()
            if opponent_total_payoff > my_total_payoff * 1.1:
                action = Action.D
        return action

class Strategy_EXPLOITATIVE_67(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior while exploiting their cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.n_players / 2
        self.opponent_cooperation_rates = [0.0] * (game_description.n_players - 1)
        self.total_payoffs_cooperating = 0.0
        self.total_payoffs_defecting = 0.0
        self.rounds_cooperated = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        for i in range(self.game_description.n_players - 1):
            self.opponent_cooperation_rates[i] = np.mean(history.opponent_actions[:, i])
            if history.my_payoffs[-1] > 0:
                if history.my_actions[-1]:
                    self.total_payoffs_cooperating += history.my_payoffs[-1]
                else:
                    self.total_payoffs_defecting += history.my_payoffs[-1]
                self.rounds_cooperated += int(history.my_actions[-1])
        if state.round_number >= self.game_description.n_rounds // 2:
            avg_payoff_coop = self.total_payoffs_cooperating / max(self.rounds_cooperated, 1)
            avg_payoff_defect = self.total_payoffs_defecting / (state.round_number - self.rounds_cooperated + 1)
            if avg_payoff_coop > avg_payoff_defect:
                self.cooperation_threshold -= 1
            elif avg_payoff_coop < avg_payoff_defect:
                self.cooperation_threshold += 1
        cooperators = sum(history.opponent_actions[-1, :])
        if cooperators >= self.cooperation_threshold:
            action = Action.C
            for i in range(self.game_description.n_players - 1):
                if self.opponent_cooperation_rates[i] > 0.75:
                    action = Action.D
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_68(BaseStrategy):
    """
    An exploitative strategy that adapts to the game's history and prioritizes self-interest.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.5
        self.total_cooperators_prev = 0
        self.avg_payoff_last_2_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if state.round_number <= 3:
            if state.round_number == 1:
                action = random.choice([Action.C, Action.D])
            else:
                opponent_prev_action = history.opponent_actions[-2, 0]
                action = Action.C if opponent_prev_action else Action.D
        else:
            if self.total_cooperators_prev > self.game_description.n_players / 2:
                action = Action.D
            else:
                action = Action.C
            if history.my_actions[-1] == Action.C and history.opponent_actions[-1, 0] == False:
                action = Action.D
            self.avg_payoff_last_2_rounds = np.mean(history.my_payoffs[-2:])
            if self.avg_payoff_last_2_rounds > self.game_description.k / 2:
                self.cooperation_probability += 0.1
            else:
                self.cooperation_probability -= 0.1
            if state.round_number == self.game_description.n_rounds - 1:
                action = Action.D
            if self.total_cooperators_prev == self.game_description.n_players / 2 and self.avg_payoff_last_2_rounds > self.game_description.k / 2:
                action = Action.C
        self.total_cooperators_prev = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        return action

class Strategy_EXPLOITATIVE_69(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game.
    Adapts to opponents' behavior by adjusting a threshold value.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_cooperators = 0
        self.θ = game_description.k / game_description.n_players * (game_description.n_players - 1)
        self.ε = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.prev_cooperators = sum(history.opponent_actions[-1, :])
        if self.prev_cooperators >= self.θ:
            action = Action.C
        else:
            action = Action.D
        if self.prev_cooperators >= self.game_description.n_players / 2:
            self.θ -= self.ε
        else:
            self.θ += self.ε
        return action

class Strategy_EXPLOITATIVE_70(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators_prev = 0
        self.avg_payoff_coop = 0
        self.avg_payoff_defect = 0
        self.opponent_behavior = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number <= 5:
            self.total_cooperators_prev = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if self.total_cooperators_prev > self.game_description.n_players / 2:
                return Action.D
            else:
                return Action.C
        else:
            recent_rounds = history.my_actions[max(0, state.round_number - 3):]
            recent_payoffs = history.my_payoffs[max(0, state.round_number - 3):]
            coop_payoffs = recent_payoffs[recent_rounds == True]
            defect_payoffs = recent_payoffs[recent_rounds == False]
            if len(coop_payoffs) > 0:
                self.avg_payoff_coop = np.mean(coop_payoffs)
            else:
                self.avg_payoff_coop = 0
            if len(defect_payoffs) > 0:
                self.avg_payoff_defect = np.mean(defect_payoffs)
            else:
                self.avg_payoff_defect = 0
            self.opponent_behavior.append(history.opponent_actions[-1, :])
            opponent_coop_rates = np.mean(self.opponent_behavior, axis=0)
            if state.round_number >= self.game_description.n_rounds - 2:
                return Action.D
            if opponent_coop_rates.max() > 0.75 and self.avg_payoff_coop > self.avg_payoff_defect:
                return Action.C
            elif opponent_coop_rates.min() < 0.25:
                return Action.D
            elif self.avg_payoff_coop > self.avg_payoff_defect:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_71(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to observed opponent behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def calculate_avg_cooperation_rates(self, history: PlayerHistory) -> NDArray[np.float64]:
        avg_cooperation_rates = np.mean(history.opponent_actions[:history.my_actions.size, :], axis=0)
        return avg_cooperation_rates

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None or state.round_number == 0:
            return Action.D
        elif 1 <= state.round_number <= 5:
            sum_cooperators_previous_round = np.sum(history.opponent_actions[-1, :])
            if sum_cooperators_previous_round / (self.game_description.n_players - 1) > 0.5:
                return Action.C
            else:
                return Action.D
        elif state.round_number < self.game_description.n_rounds - 1:
            avg_cooperation_rates = self.calculate_avg_cooperation_rates(history)
            if max(avg_cooperation_rates) > 0.5:
                return Action.C
            elif max(avg_cooperation_rates) == min(avg_cooperation_rates):
                return Action.D
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_72(BaseStrategy):
    """
    Exploitative strategy that adapts to the cooperation rate of opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate = 0.5
        self.total_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        elif state.round_number <= 4:
            self.total_cooperators += sum(history.opponent_actions[-1, :])
            self.cooperation_rate = self.total_cooperators / (self.game_description.n_players * state.round_number)
            return Action.D
        else:
            p_C = max(0.01, min(self.cooperation_rate, 0.99))
            if random.random() < p_C:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_73(BaseStrategy):
    """
    Exploitative Strategy: Adaptive Cooperation with Retaliation.
    
    This strategy adapts to opponents' behavior based on their recent actions, 
    retaliating against defectors to discourage exploitation while maximizing payoffs when possible.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        T = sum(opponent_actions) + int(history.my_actions[-1])
        AP_cooperators = self.game_description.k / self.game_description.n_players * T
        AP_defectors = 1 + self.game_description.k / self.game_description.n_players * T
        if history.my_payoffs.size > 0:
            my_previous_action = Action.C if history.my_actions[-1] else Action.D
            if my_previous_action == Action.C and AP_cooperators <= AP_defectors:
                AP_cooperators += self.game_description.k / self.game_description.n_players * T
                AP_defectors -= 1
            elif my_previous_action == Action.D and AP_cooperators > AP_defectors:
                AP_cooperators -= self.game_description.k / self.game_description.n_players * T
                AP_defectors += 1
        RF = 1 if any((not opponent_action for opponent_action in opponent_actions)) else 0
        if AP_cooperators > AP_defectors:
            if T >= self.game_description.n_players / 2 and RF == 0:
                return Action.C
            elif RF == 1:
                p = 1 - (T + int(history.my_actions[-1])) / self.game_description.n_players
                return Action.D if random.random() < p else Action.C
            else:
                return Action.D
        elif RF == 1:
            p = 1 - (T + int(history.my_actions[-1])) / self.game_description.n_players
            return Action.D if random.random() < p else Action.C
        else:
            return Action.D
        all_defected = not any((opponent_action for opponent_action in opponent_actions))
        single_cooperator = sum((1 for opponent_action in opponent_actions if opponent_action)) == 1
        if all_defected:
            return Action.C if random.random() < 0.5 else Action.D
        elif single_cooperator:
            return Action.D

class Strategy_EXPLOITATIVE_75(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Predator"

    This strategy is designed to exploit opponents' behaviors while adapting to changing game conditions.
    It uses a combination of history-based decision-making and adaptive thresholding to maximize payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if history is not None:
            opponent_coop_actions = np.sum(history.opponent_actions, axis=1)
            total_cooperators = np.sum(opponent_coop_actions) + (history.my_actions[:-1] == True).sum()
            self.avg_coop_rate = total_cooperators / ((state.round_number - 1) * self.game_description.n_players)
        theta = self.game_description.k / self.game_description.n_players + (1.0 - self.avg_coop_rate) / 2
        if history is not None:
            last_round_cooperators = np.sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
            if last_round_cooperators > theta * self.game_description.n_players:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1 and self.avg_coop_rate >= 0.5:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_76(BaseStrategy):
    """
    A hybrid strategy combining elements of Tit-for-Tat with a Pavlov-like adaptation mechanism.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_threshold = 1.5
        self.exploit_threshold = 1.2
        self.recent_rounds = min(3, game_description.n_rounds)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (history and len(history.my_actions) == 0):
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_defections = any((not action for action in history.opponent_actions[-1, :]))
        if opponent_defections:
            return Action.D
        my_payoffs = np.array(history.my_payoffs)
        avg_my_payoff = np.mean(my_payoffs[max(0, len(my_payoffs) - self.recent_rounds):])
        if avg_my_payoff > self.avg_payoff_threshold:
            return Action.C
        opponent_actions = np.array(history.opponent_actions[-1, :])
        num_cooperators = sum((action for action in opponent_actions))
        if num_cooperators > len(opponent_actions) // 2 and avg_my_payoff < self.exploit_threshold:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_77(BaseStrategy):
    """
    An exploitative strategy that adapts to the game's dynamics and exploits opportunities for high payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 1 - game_description.k / game_description.n_players
        self.coop_prob_first_round = min(0.5, game_description.k / game_description.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.coop_prob_first_round else Action.D
        prev_payoff_i = history.my_payoffs[-1]
        avg_payoff = np.mean(history.opponent_payoffs[-1])
        prev_action_i = Action.C if history.my_actions[-1] else Action.D
        if prev_payoff_i > avg_payoff:
            action = prev_action_i
        else:
            action = Action.D if prev_action_i == Action.C else Action.C
        total_cooperators = sum((1 for a in history.opponent_actions[-1, :] if a))
        if total_cooperators < self.theta * self.game_description.n_players - 1:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1 and prev_payoff_i > avg_payoff and (action == Action.C):
            action = Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_79(BaseStrategy):
    """
    An adaptive strategy that balances exploitation and cooperation.
    It gathers information about opponents' behaviors in the first round,
    then adapts to their actions while prioritizing individual payoff maximization.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.5
        self.my_coop_count = 0
        self.prev_avg_coop_rate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.game_description.n_players == 2 and history is not None:
            return Action.C if history.opponent_actions[0, 0] else Action.D
        coop_count = sum(history.opponent_actions[-1, :])
        self.avg_coop_rate = (self.avg_coop_rate * (state.round_number - 1) + coop_count) / state.round_number
        if state.round_number < self.game_description.n_rounds // 2:
            return Action.C if self.avg_coop_rate > 0.5 else Action.D
        if self.prev_avg_coop_rate is None or self.avg_coop_rate > self.prev_avg_coop_rate:
            return Action.C
        return Action.D
        self.prev_avg_coop_rate = self.avg_coop_rate
        self.my_coop_count += 1 if history.my_actions[-1] else 0

class Strategy_EXPLOITATIVE_80(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' cooperation levels and punishes defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_scores = [0] * (game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_cooperation_rate = sum(self.opponent_scores) / ((self.game_description.n_players - 1) * state.round_number)
        if avg_cooperation_rate >= self.game_description.k / self.game_description.n_players:
            play_action = Action.C
        else:
            play_action = Action.D
        for i in range(self.game_description.n_players - 1):
            if self.opponent_scores[i] == 0 and state.round_number > 2:
                return Action.D
        for i, action in enumerate(history.opponent_actions[-1, :]):
            if action:
                self.opponent_scores[i] += 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history and (not np.any(history.opponent_actions[0, :])):
            return Action.D
        return play_action

class Strategy_EXPLOITATIVE_81(BaseStrategy):
    """
    The Adaptive Free Rider strategy is designed to exploit a wide range of opponent behaviors while adapting to changing game conditions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or self.cooperation_threshold is None:
            self.cooperation_threshold = self.game_description.n_players * (self.game_description.k - 1) / self.game_description.k
            return Action.D
        total_cooperators_prev_round = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if total_cooperators_prev_round >= self.cooperation_threshold:
            action = Action.C
        else:
            action = Action.D
        if any(history.opponent_actions[-1, :]) or history.my_actions[-1]:
            self.cooperation_threshold = max(self.cooperation_threshold - 1 / self.game_description.n_players, 0)
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_82(BaseStrategy):
    """
    An adaptive strategy that exploits opponents' cooperative tendencies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.round_payoff = None
        self.total_cooperators = 0
        self.average_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.round_payoff = history.my_payoffs[-1]
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        self.total_cooperators = 1 + opponent_cooperators if history.my_actions[-1] else opponent_cooperators
        payoffs = np.append(history.my_payoffs[-1], history.opponent_payoffs[-1, :])
        self.average_payoff = np.mean(payoffs)
        if self.total_cooperators >= self.game_description.n_players / 2:
            return Action.D
        elif self.round_payoff < self.average_payoff:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_83(BaseStrategy):
    """
    Exploitative strategy "K-Tracker" for the N-player public goods game.
    
    This strategy aims to maximize its payoff by exploiting the game's structure 
    and adapting to opponents' behaviors. It uses a threshold-based approach to 
    decide whether to cooperate or defect in each round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.k_threshold = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        if state.round_number + 1 == self.game_description.n_rounds:
            return Action.D
        opponent_actions = history.opponent_actions[-1]
        avg_cooperators = sum(opponent_actions) / len(opponent_actions)
        if avg_cooperators > self.k_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_84(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game.
    It adapts to the game history and exploits opponents' willingness to cooperate.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def calculate_total_payoff(self, history: PlayerHistory) -> float:
        """
        Calculate the total payoff for all players in the previous round.

        Args:
            history (PlayerHistory): The game history up to this point.

        Returns:
            float: The total payoff for all players in the previous round.
        """
        prev_round = history.opponent_actions[-1, :]
        n_coop = sum(prev_round)
        return 1 + self.game_description.k / self.game_description.n_players * n_coop

    def calculate_expected_defect_payoff(self, history: PlayerHistory) -> float:
        """
        Calculate the expected payoff from defecting in the previous round.

        Args:
            history (PlayerHistory): The game history up to this point.

        Returns:
            float: The expected payoff from defecting in the previous round.
        """
        prev_round = history.opponent_actions[-1, :]
        n_coop = sum(prev_round)
        return 1 + self.game_description.k / self.game_description.n_players * (n_coop - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_round = history.opponent_actions[-1, :]
        avg_contrib_rate = sum(prev_round) / self.game_description.n_players
        total_payoff = self.calculate_total_payoff(history)
        expected_defect_payoff = self.calculate_expected_defect_payoff(history)
        if avg_contrib_rate > 0.5:
            return Action.D
        elif total_payoff >= expected_defect_payoff:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_85(BaseStrategy):
    """
    Adaptive Exploiter (AE) strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.classifications = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_players = self.game_description.n_players
        k = self.game_description.k
        for i, actions in enumerate(history.opponent_actions):
            if i not in self.classifications:
                self.classifications[i] = 'UNPRED'
            coop_count = sum((1 for action in actions[:state.round_number] if action))
            defect_count = state.round_number - coop_count
            if coop_count >= 0.5 * state.round_number:
                self.classifications[i] = 'COOP'
            elif defect_count >= 0.8 * state.round_number:
                self.classifications[i] = 'DEF'
        COOP_count = sum((1 for classification in self.classifications.values() if classification == 'COOP'))
        DEF_count = sum((1 for classification in self.classifications.values() if classification == 'DEF'))
        total_contributions = sum(history.opponent_actions[state.round_number - 1, :]) + history.my_actions[state.round_number - 1]
        threshold = 0.5 * n_players
        if COOP_count > DEF_count:
            return Action.C
        elif DEF_count > COOP_count:
            return Action.D
        elif total_contributions >= threshold:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_EXPLOITATIVE_86(BaseStrategy):
    """
    Adaptive Exploiter (AE) is designed to exploit a wide range of opponent behaviors while being robust and adaptive throughout the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 1.5
        self.current_payoff = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        proportion_cooperators = opponent_cooperators / self.game_description.n_players
        self.current_payoff += history.my_payoffs[-1]
        if proportion_cooperators > 0.5:
            action = Action.C
        else:
            action = Action.D
        if self.current_payoff < self.threshold:
            action = Action.C if action == Action.D else Action.D
        return action

class Strategy_EXPLOITATIVE_87(BaseStrategy):
    """
    An exploitative strategy for the N-player Public Goods Game that adapts to observed cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.memory = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        recent_cooperation_level = self.calculate_recent_cooperation(history, self.memory)
        T = self.game_description.k / self.game_description.n_players
        if recent_cooperation_level > T or (recent_cooperation_level >= T and self.significant_future_benefit_expected(history, state.round_number)):
            action = Action.C
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

    def calculate_recent_cooperation(self, history: PlayerHistory, M) -> float:
        cooperation_level_sum = sum(np.sum(history.opponent_actions[-M:], axis=0)) + np.sum(history.my_actions[-M:])
        recent_average_cooperation = cooperation_level_sum / (self.game_description.n_players * M)
        return recent_average_cooperation

    def significant_future_benefit_expected(self, history: PlayerHistory, current_round) -> bool:
        remaining_rounds = self.game_description.n_rounds - current_round
        prev_cooperation_level = np.sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        return remaining_rounds > 1 and prev_cooperation_level >= self.game_description.n_players - 1

class Strategy_EXPLOITATIVE_88(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game.
    It adapts to opponents' cooperation rates and adjusts its actions accordingly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_c_rates = [0.5] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        for opponent, action in enumerate(history.opponent_actions[-1]):
            self.opponent_c_rates[opponent] = (self.opponent_c_rates[opponent] * state.round_number + int(action)) / (state.round_number + 1)
        if state.round_number <= 4:
            cooperators = sum(history.opponent_actions[-1, :])
            return Action.C if cooperators >= self.game_description.n_players // 2 else Action.D
        avg_c_rate = np.mean(self.opponent_c_rates)
        opponent = history.opponent_actions.shape[1] - 1
        if state.round_number < self.game_description.n_rounds // 2:
            return Action.C if self.opponent_c_rates[opponent] > avg_c_rate else Action.D
        if state.round_number >= self.game_description.n_rounds // 2 + 1:
            return Action.D if self.opponent_c_rates[opponent] < avg_c_rate else Action.C
        return Action.D

class Strategy_EXPLOITATIVE_89(BaseStrategy):
    """
    An exploitative strategy for N-Player Public Goods Game that adapts to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.punishment_threshold = 0.2
        self.my_cooperation_rate = 1
        self.avg_opponent_cooperation_rates = np.zeros(game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_cooperation_rates = np.mean(history.opponent_actions[:state.round_number], axis=0)
        self.avg_opponent_cooperation_rates[:] = opponent_cooperation_rates
        avg_opponent_cooperation_rate = np.mean(self.avg_opponent_cooperation_rates)
        if avg_opponent_cooperation_rate < self.cooperation_threshold:
            return Action.D
        for i, coop_rate in enumerate(opponent_cooperation_rates):
            if coop_rate < self.punishment_threshold:
                return Action.D
        my_payoff = np.mean(history.my_payoffs[:state.round_number])
        expected_payoff = avg_opponent_cooperation_rate * (self.game_description.k / self.game_description.n_players)
        if my_payoff < expected_payoff:
            self.my_cooperation_rate -= 0.1
        else:
            self.my_cooperation_rate += 0.1
        return Action.C if random.random() < self.my_cooperation_rate else Action.D

class Strategy_EXPLOITATIVE_90(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behaviors over time.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_rate = 0.5
        self.threshold = (1 + self.coop_rate) / 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C
        if history is not None:
            opponent_cooperations = sum(history.opponent_actions[-1, :])
            self.coop_rate = (self.coop_rate * (state.round_number - 1) + opponent_cooperations / self.game_description.n_players) / state.round_number
            self.threshold = (1 + self.coop_rate) / 2
            if history.my_actions[-1] and sum(history.opponent_actions[-1, :]) < self.game_description.n_players - 1:
                self.coop_rate *= 0.9
        private_payoff = 1 - (self.coop_rate >= self.threshold)
        public_good_share = self.game_description.k / self.game_description.n_players * self.coop_rate * (self.game_description.n_players - 1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif private_payoff > public_good_share or random.random() < 0.01:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_91(BaseStrategy):
    """
    An adaptive exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.recent_window = []
        self.long_window = {}
        self.avg_coop_rate = 0
        self.coop_threshold = 0.5
        self.defect_threshold = 0.7
        self.adaptation_interval = 4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_coop_rate = sum(self.recent_window) / (state.round_number * self.game_description.n_players)
        recent_defector = all(history.opponent_actions[-2:, 0]) if len(history.opponent_actions) > 1 else False
        frequent_defector = sum(self.long_window.values()) / state.round_number >= self.defect_threshold
        if avg_coop_rate > self.coop_threshold:
            return Action.D
        elif recent_defector:
            return Action.D
        elif frequent_defector:
            return Action.C
        else:
            return Action.C
        self.recent_window.append(bool(history.my_actions[-1]))
        if len(self.recent_window) > 3:
            self.recent_window.pop(0)
        opponent_id = history.opponent_payoffs.shape[1]
        for i in range(opponent_id):
            self.long_window[i] = (self.long_window.get(i, 0) + int(history.opponent_actions[-1, i])) % 2
        if state.round_number % self.adaptation_interval == 0:
            avg_coop_rate = sum(self.long_window.values()) / self.game_description.n_players

class Strategy_EXPLOITATIVE_92(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior in an N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p = game_description.k / game_description.n_players
        self.round_payoffs = []
        self.coop_rate_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate = random.random() < self.p
            self.coop_rate_history.append(cooperate)
            return Action.C if cooperate else Action.D
        total_coop_prev_round = sum(history.opponent_actions[-1, :])
        if total_coop_prev_round >= self.game_description.n_players - self.game_description.k:
            cooperate = random.random() < self.p
        else:
            cooperate = False
        payoff_t = history.my_payoffs[-1]
        self.round_payoffs.append(payoff_t)
        self.coop_rate_history.append(cooperate)
        if state.round_number % 5 == 0:
            avg_coop_payoff = sum([payoff for coop, payoff in zip(self.coop_rate_history[-5:], self.round_payoffs[-5:]) if coop]) / len(self.round_payoffs[-5:])
            avg_defect_payoff = sum([payoff for coop, payoff in zip(self.coop_rate_history[-5:], self.round_payoffs[-5:]) if not coop]) / len(self.round_payoffs[-5:])
            if avg_coop_payoff > avg_defect_payoff:
                self.p = min(1, self.p + 0.1)
            else:
                self.p = max(0, self.p - 0.1)
        if state.round_number == self.game_description.n_rounds - 1:
            cooperate = False
        return Action.C if cooperate else Action.D

class Strategy_EXPLOITATIVE_93(BaseStrategy):
    """
    The Adaptive Exploiter strategy adapts to the opponents' behaviors while exploiting them to maximize its own payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_coop_prev = np.mean(np.concatenate(([history.my_actions[-1]], history.opponent_actions[-1, :])))
        if state.round_number < self.game_description.n_rounds - 1:
            if avg_coop_prev >= (self.game_description.n_players - self.game_description.k) / self.game_description.n_players:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_94(BaseStrategy):
    """
    Adaptive Greed strategy for N-Player Public Goods Game.
    This strategy adapts to various opponent behaviors and exploits opportunities to maximize individual payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0
        self.total_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if state.round_number < self.game_description.n_rounds // 2:
            avg_coop_rate = np.mean(history.my_actions)
            for opponent in history.opponent_actions.T:
                avg_coop_rate += np.mean(opponent)
            avg_coop_rate /= 1 + self.game_description.n_players - 1
            if avg_coop_rate > 0.5:
                return Action.D
            else:
                return Action.C
        elif state.round_number >= self.game_description.n_rounds // 2 and state.round_number < self.game_description.n_rounds - 2:
            total_payoff = history.my_payoffs.sum()
            for opponent in history.opponent_payoffs.T:
                total_payoff += opponent.sum()
            avg_payoff_per_player = total_payoff / (self.game_description.n_players * state.round_number)
            if avg_payoff_per_player > 1.5:
                return Action.D
            else:
                return Action.C
        elif state.round_number >= self.game_description.n_rounds - 2 and state.round_number < self.game_description.n_rounds - 1:
            num_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if num_cooperators > self.game_description.n_players // 2:
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_95(BaseStrategy):
    """
    Adaptive Exploiter (AE) strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.window_size = 5
        self.cooperation_threshold = 0.7
        self.lower_payoff_threshold = 1.5
        self.upper_payoff_threshold = 2.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_payoff = history.my_payoffs[-1]
        average_payoffs = np.mean(history.opponent_payoffs[-1, :])
        if previous_payoff >= average_payoffs:
            action = Action.C
        else:
            action = Action.D
        if state.round_number < self.n_rounds - 1:
            opponent_acrs = np.zeros(self.n_players - 1)
            for i in range(1, self.n_players):
                opponent_acr = np.mean(history.opponent_actions[-self.window_size:, i - 1])
                opponent_acrs[i - 1] = opponent_acr
                if opponent_acr > self.cooperation_threshold:
                    action = Action.C
                else:
                    action = Action.D
        if state.round_number == self.n_rounds - 1:
            return Action.D
        recent_payoffs = history.my_payoffs[-self.window_size:]
        average_recent_payoff = np.mean(recent_payoffs)
        if average_recent_payoff < self.lower_payoff_threshold:
            self.cooperation_threshold -= 0.05
        elif average_recent_payoff > self.upper_payoff_threshold:
            self.cooperation_threshold += 0.05
        return action

class Strategy_EXPLOITATIVE_96(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Predator"
    
    This strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history.
    It focuses on maximizing individual payoffs by identifying opportunities to defect against cooperators and retaliating against defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_defections = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        for i, opponent_action in enumerate(history.opponent_actions[-1]):
            if opponent_action == False:
                self.opponent_defections[i] += 1
        prev_cooperators = sum([action for action in history.opponent_actions[-1]]) + int(bool(history.my_actions[-1]))
        threshold = self.game_description.n_players / 2
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif prev_cooperators > threshold:
            return Action.D
        elif prev_cooperators < threshold:
            return Action.C
        for i, opponent_defection_count in enumerate(self.opponent_defections):
            if opponent_defection_count > 1:
                return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_97(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and maximizes payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_cooperation_rate = 0.0
        self.consecutive_exploitation_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperations = np.sum(history.opponent_actions, axis=1)
        total_cooperations = np.sum(opponent_cooperations) + history.my_actions.size
        self.avg_cooperation_rate = total_cooperations / (self.game_description.n_players * state.round_number)
        theta = self.game_description.k / self.game_description.n_players
        if self.avg_cooperation_rate > theta:
            action = Action.D
            self.consecutive_exploitation_rounds += 1
            exploitation_limit = 5
            if self.consecutive_exploitation_rounds >= exploitation_limit:
                action = Action.C
                self.consecutive_exploitation_rounds = 0
        else:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            opponent_payoffs_sum = np.sum(history.opponent_payoffs, axis=0)
            total_payoff = np.sum(history.my_payoffs) + self.game_description.k / self.game_description.n_players * opponent_payoffs_sum[-1]
            if total_payoff < self.game_description.k:
                action = Action.C
        return action

class Strategy_EXPLOITATIVE_98(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit the opponent's behavior while adapting to the game's dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rates = [0.0] * game_description.n_players
        self.long_term_cooperation_rates = [0.0] * game_description.n_players
        self.serial_defectors = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_actions = history.opponent_actions[-3:, :] if len(history.opponent_actions) >= 3 else history.opponent_actions
        for i, actions in enumerate(opponent_actions.T):
            recent_cooperations = sum(actions)
            self.cooperation_rates[i] = recent_cooperations / len(actions)
            long_term_cooperations = sum(history.opponent_actions[:, i])
            self.long_term_cooperation_rates[i] = long_term_cooperations / state.round_number
            if self.long_term_cooperation_rates[i] < 0.4:
                self.serial_defectors.add(i)
        for i, opponent_cr in enumerate(self.cooperation_rates):
            if opponent_cr >= 0.5 and i not in self.serial_defectors:
                return Action.C
            elif len(self.cooperation_rates) > 1:
                max_cr_opponent = np.argmax([self.long_term_cooperation_rates[j] if j not in self.serial_defectors else -np.inf for j, _ in enumerate(self.cooperation_rates)])
                return Action.C if max_cr_opponent == i and self.long_term_cooperation_rates[max_cr_opponent] >= 0.4 else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_100(BaseStrategy):
    """
    The Adaptive K-Threshold Strategy aims to exploit opponents' cooperative behavior 
    while minimizing its own contribution to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        k_threshold = self.game_description.k / self.game_description.n_players * (self.game_description.n_rounds - state.round_number + 1)
        if history.opponent_actions is not None:
            self.total_cooperators_prev = sum(history.opponent_actions[-1, :])
        if self.total_cooperators_prev >= k_threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_101(BaseStrategy):
    """
    Exploitative strategy "K-Threshold Punisher" that aims to maximize payoffs by adapting to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop = 0.0
        self.punishable_opponents = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        self.avg_coop = np.mean(history.my_actions[:state.round_number].astype(int))
        if self.avg_coop >= self.game_description.k / self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        for opponent_idx, (opponent_action, opponent_payoff) in enumerate(zip(history.opponent_actions[state.round_number - 1], history.opponent_payoffs[state.round_number - 1])):
            if opponent_action == Action.D and opponent_payoff > history.my_payoffs[-1]:
                self.punishable_opponents.add(opponent_idx)
        for opponent_idx, (opponent_action, _) in enumerate(zip(history.opponent_actions[state.round_number - 1], history.opponent_payoffs[state.round_number - 1])):
            if opponent_idx in self.punishable_opponents:
                action = Action.D
                break
        return action

class Strategy_EXPLOITATIVE_102(BaseStrategy):
    """
    An exploitative strategy that adapts to the behaviors of other players while 
    exploiting their actions over time.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploiters = []
        self.num_cooperators = 0
        self.num_defectors = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        prev_my_action = history.my_actions[-1]
        prev_opponent_actions = history.opponent_actions[-1]
        self.num_cooperators = sum(prev_opponent_actions)
        self.num_defectors = len(prev_opponent_actions) - self.num_cooperators
        for player, action in enumerate(prev_opponent_actions):
            if not action and prev_my_action:
                if player not in self.exploiters:
                    self.exploiters.append(player)
        if state.round_number < 4:
            return random.choice([Action.C, Action.D])
        if self.num_cooperators <= self.game_description.n_players / 2:
            return Action.D
        p_C = self.game_description.k / self.game_description.n_players * (self.num_cooperators / max(self.num_defectors, 1))
        if random.random() < p_C:
            for player in self.exploiters:
                if random.random() < 0.8:
                    return Action.D
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_104(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to maximize its own payoff by exploiting 
    the cooperation of other players while minimizing its own contribution to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            cooperate_prob = 1 / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        avg_payoff_prev = np.mean(history.my_payoffs[-1]) if history.my_payoffs is not None else 0
        if avg_payoff_prev >= self.game_description.k / self.game_description.n_players:
            return Action.D
        else:
            cooperate_prob = self.game_description.k / self.game_description.n_players / avg_payoff_prev if avg_payoff_prev != 0 else 1 / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        for opponent_action in history.opponent_actions[-1, :]:
            if not opponent_action and history.my_payoffs is not None and (np.mean(history.my_payoffs) < 1 / self.game_description.n_players):
                punish_prob = 0.5
                return Action.D if random.random() < punish_prob else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_actions = history.opponent_actions[-1, :]
        if not any(opponent_actions):
            cooperate_prob = 1 / self.game_description.n_players
            return Action.C if random.random() < cooperate_prob else Action.D
        elif sum(opponent_actions) == 1:
            punish_prob = 0.5
            return Action.D if random.random() < punish_prob else Action.C

class Strategy_EXPLOITATIVE_105(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy adapts to the game's history and parameters,
    initially defecting and then cooperating or defecting based on observed cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.acr_window_size = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        opponent_cooperations = np.sum(history.opponent_actions[:state.round_number - 1], axis=0)
        if state.round_number < self.acr_window_size:
            acr = np.mean(opponent_cooperations / (self.game_description.n_players - 1))
        else:
            start_round = max(0, state.round_number - self.acr_window_size)
            acr = np.mean(np.sum(history.opponent_actions[start_round:state.round_number], axis=0) / (self.game_description.n_players - 1))
        if acr >= 0.5:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_106(BaseStrategy):
    """
    An exploitative strategy that adapts to the observed cooperation rate among opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.observation_phase = True
        self.cooperation_rate = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        elif self.observation_phase and state.round_number < self.game_description.n_rounds // 2:
            return Action.C if state.round_number % 2 == 1 else Action.D
        elif self.observation_phase and state.round_number >= self.game_description.n_rounds // 2:
            self.observation_phase = False
            opponent_cooperations = history.opponent_actions[:self.game_description.n_rounds // 2, :].sum(axis=0)
            self.cooperation_rate = np.mean(opponent_cooperations)
        if not self.observation_phase and state.round_number < self.game_description.n_rounds - 1:
            return Action.C if self.cooperation_rate >= 0.5 else Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_107(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_count = {}
        self.c_history = []
        self.theta = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        k = self.game_description.k
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < k / n_players else Action.D
        last_round_actions = history.opponent_actions[-1, :] if history is not None else None
        cooperators_last_round = sum(last_round_actions) if last_round_actions is not None else 0
        avg_cooperation_rate = (cooperators_last_round + int(history.my_actions[-1])) / n_players
        for j in range(1, n_players):
            opponent_defected = history.opponent_actions[-1, j - 1] == Action.D if last_round_actions is not None else False
            others_cooperated = avg_cooperation_rate > 0
            if opponent_defected and others_cooperated:
                self.punish_count[j] = self.punish_count.get(j, 0) + 1
                if self.punish_count[j] >= self.theta:
                    return Action.D
        if avg_cooperation_rate >= k / n_players or (avg_cooperation_rate == k / n_players and random.random() < 0.5):
            action = Action.C
        else:
            action = Action.D
        if state.round_number + 1 == self.game_description.n_rounds:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_108(BaseStrategy):
    """
    A strategy for N-Player Public Goods Game, designed to exploit opponents' cooperative behavior.
    It cooperates in initial rounds and adapts to the overall level of cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_contribution_rates = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 3:
            return Action.C
        prev_round_contributions = np.sum(history.opponent_actions[-1, :] == True)
        avg_contribution_rate = (prev_round_contributions + history.my_actions[-1]) / self.game_description.n_players
        self.avg_contribution_rates.append(avg_contribution_rate)
        threshold = np.mean(self.avg_contribution_rates) * self.game_description.k / self.game_description.n_players
        prev_round_cooperators = np.sum(history.opponent_actions[-1, :] == True)
        if prev_round_cooperators >= threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_109(BaseStrategy):
    """
    Exploitative strategy for the N-Player Public Goods Game that adapts to game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] and 1 or 0)
        if self.game_description.n_players == 2:
            return Action.D
        if state.round_number + 1 == self.game_description.n_rounds:
            return Action.C if previous_round_cooperators >= self.game_description.k / 2 else Action.D
        if previous_round_cooperators >= self.game_description.k / 2:
            return Action.D
        else:
            p = self.game_description.k / self.game_description.n_players * (previous_round_cooperators / self.game_description.n_players)
            return Action.C if random.random() < p else Action.D

class Strategy_EXPLOITATIVE_110(BaseStrategy):
    """
    The Adaptive Exploiter strategy combines an initial exploratory phase with adaptive adjustments 
    to exploit opponents' behaviors while being robust to a wide range of opponent strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.cooperation_rate = 0.5
        self.opponent_ocr = [0] * (game_description.n_players - 1)
        self.window_size = min(10, game_description.n_rounds)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        previous_round_cooperators = sum(history.opponent_actions[-1, :])
        for opponent in range(self.n_players - 1):
            self.opponent_ocr[opponent] = self.update_opponent_ocr(opponent, history)
        if previous_round_cooperators <= self.game_description.n_players // 2:
            action = Action.C
        else:
            action = Action.D
        suckers = [opponent for opponent in range(self.n_players - 1) if self.opponent_ocr[opponent] > 0.8]
        free_riders = [opponent for opponent in range(self.n_players - 1) if self.opponent_ocr[opponent] < 0.2]
        if state.round_number < self.game_description.n_rounds:
            self.cooperation_rate = self.adjust_cooperation_rate(self.cooperation_rate, np.mean(self.opponent_ocr), history.my_actions[-1])
            if random.random() < self.cooperation_rate:
                action = Action.C
        return action

    def update_opponent_ocr(self, opponent: int, history: PlayerHistory) -> float:
        """Update opponent's cooperation rate over the last w rounds."""
        window_size = min(self.window_size, len(history.opponent_actions))
        ocr = sum([history.opponent_actions[i, opponent] for i in range(-window_size, 0)]) / window_size
        return ocr

    def adjust_cooperation_rate(self, cooperation_rate: float, average_opponent_ocr: float, my_last_action: bool) -> float:
        """Adjust cooperation rate based on opponents' behavior."""
        if average_opponent_ocr > 0.5 and my_last_action:
            cooperation_rate += 0.1
        elif average_opponent_ocr <= 0.5 or not my_last_action:
            cooperation_rate -= 0.1
        return max(0, min(cooperation_rate, 1))

class Strategy_EXPLOITATIVE_111(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior by monitoring their cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_opponent_coop = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_actions = history.opponent_actions[:-1]
        avg_opponent_coop_last_round = np.mean(opponent_actions[-1, :])
        self.avg_opponent_coop = (self.avg_opponent_coop * (state.round_number - 2) + avg_opponent_coop_last_round) / (state.round_number - 1)
        theta = self.game_description.k / self.game_description.n_players
        if self.avg_opponent_coop > theta:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_112(BaseStrategy):
    """
    Robust and adaptive exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_payoffs = None
        self.prev_actions = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number == 1 and history is None):
            return Action.C
        prev_payoffs = self.prev_payoffs if self.prev_payoffs is not None else np.array([0.0] * self.game_description.n_players)
        prev_actions = self.prev_actions if self.prev_actions is not None else np.full((self.game_description.n_players,), True, dtype=bool)
        if history:
            self.prev_payoffs = history.my_payoffs.copy() if state.round_number == 1 else np.append(history.my_payoffs[1:], history.opponent_payoffs[-1, 0])
            self.prev_actions = history.my_actions.copy() if state.round_number == 1 else np.append(history.my_actions[1:], True)
        else:
            self.prev_payoffs = prev_payoffs
            self.prev_actions = prev_actions
        avg_payoff_prev_round = np.mean(prev_payoffs)
        total_cooperators_prev_round = sum((1 for a in prev_actions if a))
        if state.round_number < 3:
            return Action.C
        elif avg_payoff_prev_round <= self.game_description.k / self.game_description.n_players:
            return Action.D
        elif total_cooperators_prev_round >= self.game_description.n_players // 2:
            return Action.C
        for i, opponent_action in enumerate(prev_actions):
            if not opponent_action and prev_payoffs[i] <= self.game_description.k / self.game_description.n_players:
                return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_113(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to exploit the cooperative behavior of opponents while adapting to their actions over time.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        n_players = self.game_description.n_players
        k = self.game_description.k
        last_round_actions = history.opponent_actions[-1]
        H_c = sum((1 for t in range(state.round_number) if all(history.opponent_actions[t])))
        H_d = state.round_number - H_c
        R = H_c / (H_c + H_d)
        T = k / n_players * R
        expected_payoff_coop = k / n_players * sum(last_round_actions)
        if expected_payoff_coop >= T:
            return Action.C
        elif math.isclose(expected_payoff_coop, T):
            return random.choice([Action.C, Action.D])
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_114(BaseStrategy):
    """
    Robust and adaptive exploitative strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5
        self.p_init = 0.5
        self.p_exploit = 0.8
        self.p_last_round = 0.2
        self.p_revive = 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        k = self.game_description.k
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        opponent_actions = history.opponent_actions[-1, :]
        avg_coop_t_minus_1 = sum(opponent_actions) / n_players
        if avg_coop_t_minus_1 >= self.theta:
            cooperate = True
        elif not any(opponent_actions):
            cooperate = random.random() < self.p_exploit
        else:
            opponent_defected = [not action and history.my_actions[-1] for action in opponent_actions]
            if any(opponent_defected):
                self.theta -= 0.1
            cooperate = False
        if state.round_number == self.game_description.n_rounds - 1:
            cooperate = random.random() < self.p_last_round
        if state.round_number >= 2 and (not any(history.opponent_actions[-2, :])) and (not any(history.opponent_actions[-1, :])):
            cooperate = random.random() < self.p_revive
        return Action.C if cooperate else Action.D

class Strategy_EXPLOITATIVE_115(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperation of other players while minimizing its own contributions to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        elif state.round_number < self.game_description.n_rounds - 1:
            total_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if total_cooperators >= self.game_description.n_players / 2:
                return Action.D
            else:
                return Action.C if random.random() < 0.5 else Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_116(BaseStrategy):
    """
    Adaptive Exploiter Strategy for N-Player Public Goods Game.
    
    This strategy aims to exploit the cooperation of others while minimizing its own contribution to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_coops = sum(history.opponent_actions[-1, :])
        prev_defs = self.game_description.n_players - 1 - prev_coops
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if prev_coops <= self.game_description.n_players / 2:
            return Action.D
        elif prev_defs == self.game_description.n_players - 1:
            return Action.C
        elif prev_coops == 1:
            return Action.D
        else:
            if self.game_description.k / self.game_description.n_players > 1 / (self.game_description.n_players - prev_defs):
                return Action.C
            return Action.D

class Strategy_EXPLOITATIVE_117(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.threshold = self.k / self.n_players * (self.n_players / 2)
        self.prev_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        prev_cooperators = sum(prev_round_actions)
        self.prev_cooperators = prev_cooperators
        prev_payoff = history.my_payoffs[-1]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if prev_cooperators >= self.threshold and prev_payoff < self.k / self.n_players * self.n_players:
            return Action.C
        elif prev_cooperators == 1:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_119(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' cooperation rates.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate_window_size = 5
        self.cooperation_rates = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_cooperators_previous_round = sum(history.opponent_actions[-1, :] + [True])
        cooperation_rate = total_cooperators_previous_round / self.game_description.n_players
        self.cooperation_rates.append(cooperation_rate)
        if len(self.cooperation_rates) > self.cooperation_rate_window_size:
            self.cooperation_rates.pop(0)
        averaged_cooperation_rate = np.mean(self.cooperation_rates)
        if 1 <= state.round_number < self.game_description.n_rounds // 2:
            return Action.D if averaged_cooperation_rate > 0.5 else Action.C
        elif self.game_description.n_rounds // 2 <= state.round_number < self.game_description.n_rounds - 1:
            return Action.D if total_cooperators_previous_round > self.game_description.n_players / 2 else Action.C if averaged_cooperation_rate < 0.5 else Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_120(BaseStrategy):
    """
    Adaptive Defection with Conditional Cooperation (ADCC) strategy.
    
    This strategy adapts to opponents' cooperation rates while minimizing our own contribution to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate_history = []
        self.dt = game_description.k / game_description.n_players * (game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        avg_coop_rate = np.mean(history.opponent_actions[-1, :] + [False])
        self.avg_coop_rate_history.append(avg_coop_rate)
        if avg_coop_rate >= self.dt:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_121(BaseStrategy):
    """
    A strategy that balances exploitation and adaptability in a public goods game.
    
    Initially, it cooperates to gather information about opponents' behaviors. 
    Then, it adapts to the average level of cooperation in previous rounds, defecting when others contribute more than a certain threshold. 
    In the endgame phase, it prioritizes individual payoffs by consistently defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperators = [0] * game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C
        avg_cooperators = sum(self.cooperators[max(0, state.round_number - 3):state.round_number]) / min(state.round_number, 3)
        threshold = self.game_description.n_players // 2 if self.game_description.k <= self.game_description.n_players / 2 else self.game_description.n_players // 4
        if avg_cooperators > threshold:
            action = Action.D
        else:
            action = Action.C
        if state.round_number < len(self.cooperators) and history is not None:
            self.cooperators[state.round_number] += sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == 1)
        if state.round_number >= self.game_description.n_rounds - 2:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_122(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_cooperators = sum(history.opponent_actions[-1, :])
        if total_cooperators <= self.game_description.n_players / 2:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_123(BaseStrategy):
    """
    Adaptive Exploiter strategy for the N-Player Public Goods Game.
    
    This strategy aims to maximize its own payoff by exploiting the behavior of other players while adapting to changing game conditions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_payoff_diff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        opponent_actions = history.opponent_actions[-1]
        avg_cooperation_rate = np.mean(opponent_actions)
        if avg_cooperation_rate > 0.6:
            return Action.D
        elif avg_cooperation_rate < 0.4:
            return Action.C
        prev_round_payoffs = history.opponent_payoffs[-1]
        cooperator_payoffs = prev_round_payoffs[opponent_actions]
        avg_cooperator_payoff = np.mean(cooperator_payoffs)
        if self.prev_payoff_diff is None:
            self.prev_payoff_diff = 0
        if history.my_payoffs[-1] - avg_cooperator_payoff > 0:
            cooperate_prob = 0.3
        else:
            cooperate_prob = 0.7
        for i, opponent in enumerate(opponent_actions):
            consecutive_defections = 0
            for j in range(1, min(len(history.opponent_actions), 3)):
                if not history.opponent_actions[-j, i]:
                    consecutive_defections += 1
                else:
                    break
            if consecutive_defections > 2:
                cooperate_prob = 0.9
                break
        if history.my_payoffs[-1] < avg_cooperator_payoff:
            return Action.C
        elif random.random() < cooperate_prob:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_124(BaseStrategy):
    """
    Adaptive Exploiter strategy: combines elements of tit-for-tat and reinforcement learning to exploit opponents' behaviors while being robust to various scenarios.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.confidence_scores = {'C': 0.5, 'D': 0.5}
        self.punishment_flag = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        if opponent_cooperators >= self.game_description.n_players // 2:
            tit_for_tat_action = Action.C
        else:
            tit_for_tat_action = Action.D
        if not self.punishment_flag and history.my_actions[-1] == True and (history.opponent_actions[-1, :] == False).any():
            self.punishment_flag = True
            return Action.D
        if self.punishment_flag:
            self.punishment_flag = False
            return Action.D
        my_payoff = history.my_payoffs[-1]
        opponent_avg_payoff = np.mean(history.opponent_payoffs[-1, :])
        if my_payoff > opponent_avg_payoff:
            self.confidence_scores['C' if history.my_actions[-1] else 'D'] += 0.1
        elif my_payoff < opponent_avg_payoff:
            self.confidence_scores['C' if history.my_actions[-1] else 'D'] -= 0.1
        if self.confidence_scores['C'] > self.confidence_scores['D']:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_125(BaseStrategy):
    """
    Adaptive Exploitation strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        opponent_actions_last_round = history.opponent_actions[-1, :]
        if state.round_number < n // 2:
            cooperators_prev_round = sum([1 for action in opponent_actions_last_round if action])
            if cooperators_prev_round >= n // 2:
                return Action.C
            else:
                return Action.D
        elif n // 2 <= state.round_number < 3 * n // 4:
            tft_cooperators = sum([1 for action in opponent_actions_last_round if action])
            if tft_cooperators >= (n - 1) // 2:
                return Action.C
            else:
                return Action.D
        elif state.round_number >= 3 * n // 4:
            return Action.D
        elif state.round_number == r - 1:
            return Action.D

class Strategy_EXPLOITATIVE_126(BaseStrategy):
    """
    Adaptive Exploiter strategy for N-Player Public Goods Game.
    
    This strategy adapts to various opponent behaviors and focuses on exploiting opportunities for higher payoffs while maintaining a balance between cooperation and defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_avg_payoff_threshold = game_description.k / game_description.n_players * game_description.n_players / 2
        self.dynamic_threshold = game_description.n_players / 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        elif state.round_number < self.game_description.n_rounds / 2:
            avg_payoff = np.mean(history.my_payoffs) if history else 0
            if avg_payoff > self.coop_avg_payoff_threshold:
                return Action.C
            else:
                return Action.D
        elif state.round_number >= self.game_description.n_rounds / 2 and state.round_number <= 3 * self.game_description.n_rounds / 4:
            avg_cooperators = np.mean(np.sum(history.opponent_actions, axis=1)) if history else 0
            dynamic_threshold = self.dynamic_threshold + state.round_number / self.game_description.n_rounds * (self.game_description.n_players / 4)
            if avg_cooperators > dynamic_threshold:
                defect_probability = min(1, (state.round_number / self.game_description.n_rounds) ** 2)
                return Action.D if random.random() < defect_probability else Action.C
            else:
                coop_probability = max(0.5, np.mean(history.my_payoffs[-10:]) / self.coop_avg_payoff_threshold) if history and len(history.my_payoffs) >= 10 else 0.5
                return Action.C if random.random() < coop_probability else Action.D
        else:
            total_payoff = np.sum(history.my_payoffs) if history else 0
            avg_cooperators = np.mean(np.sum(history.opponent_actions, axis=1)) if history else 0
            if total_payoff / state.round_number > self.coop_avg_payoff_threshold and avg_cooperators >= 2:
                return Action.C
            else:
                defect_probability = min(1, (state.round_number / self.game_description.n_rounds) ** 2)
                return Action.D if random.random() < defect_probability else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_127(BaseStrategy):
    """
    Adaptive Exploiter strategy for N-player public goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.alpha = 0.0
        self.tau = 0.0
        self.punished = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.k / self.game_description.n_players else Action.D
        opponent_cooperations = np.sum(history.opponent_actions, axis=1)
        total_cooperations = np.sum(opponent_cooperations) + np.sum(history.my_actions)
        self.alpha = total_cooperations / (state.round_number * self.game_description.n_players)
        self.tau = max(0.0, 1 - self.alpha * self.game_description.k / self.game_description.n_players)
        if self.punished:
            self.punished = False
            return Action.C
        opponent_cooperations_last_round = np.sum(history.opponent_actions[-1, :])
        if opponent_cooperations_last_round <= math.floor(self.tau * self.game_description.n_players):
            return Action.D
        elif opponent_cooperations_last_round == math.floor(self.tau * self.game_description.n_players):
            return Action.C if random.random() < 0.1 else Action.D
        return Action.C

class Strategy_EXPLOITATIVE_129(BaseStrategy):
    """
    Exploit the cooperation of others while minimizing own contribution to public good.
    Cooperate with probability k/n when enough cooperators, otherwise defect.
    Defect in first and last rounds for information gathering and maximum payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        num_cooperators = sum(history.opponent_actions[-1, :])
        if num_cooperators >= math.ceil(self.game_description.n_players / 2):
            p = min(1.0, self.game_description.k / self.game_description.n_players)
            return Action.C if random.random() < p else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_130(BaseStrategy):
    """
    Exploitative Strategy: Adaptive Defection with Self-Sacrificing Adjustment (ADS)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.threshold = 0.4
        self.adjust = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        avg_coop_rate = sum(history.my_actions[:state.round_number]) / state.round_number
        if avg_coop_rate < self.threshold and state.round_number < self.r / 3:
            return Action.D
        elif avg_coop_rate >= self.threshold and state.round_number < self.r / 3:
            return Action.C
        opponent_avg_coop_rates = [sum(opponent_actions[:state.round_number]) / state.round_number for opponent_actions in history.opponent_actions.T]
        if state.round_number >= self.r / 3 and state.round_number < 2 * self.r / 3:
            if sum(opponent_avg_coop_rates) / self.n_players > self.threshold:
                p = (1 + self.k / self.n_players) / 2
                return Action.C if random.random() < p else Action.D
            else:
                return Action.D
        if state.round_number >= self.r / 3 and history.my_actions[state.round_number - 1] and (sum(history.opponent_payoffs[state.round_number - 1, :]) / self.n_players < self.k / self.n_players * sum([int(action) for action in history.my_actions[:state.round_number]]) + 1):
            self.adjust = True
        elif self.adjust:
            self.adjust = False
            return Action.D
        opponent_coop_rates = [sum(opponent_actions[:state.round_number]) / state.round_number for opponent_actions in history.opponent_actions.T]
        if state.round_number >= 2 * self.r / 3:
            for opponent, coop_rate in enumerate(opponent_coop_rates):
                if coop_rate > 0.75:
                    return Action.D
                elif coop_rate < 0.25:
                    return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_131(BaseStrategy):
    """
    Exploitative strategy that adapts to the majority's actions and adjusts based on public goods contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_majority_action = sum(history.opponent_actions[-1, :]) > self.game_description.n_players / 2
        cooperate = not prev_majority_action
        total_contributions = sum((1 for action in history.opponent_actions[-1, :] if action))
        if total_contributions > self.game_description.k / 2:
            cooperate = True
        elif total_contributions < self.game_description.k / 2:
            cooperate = False
        if state.round_number == self.game_description.n_rounds - 1:
            cooperate = False
        return Action.C if cooperate else Action.D

class Strategy_EXPLOITATIVE_132(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behaviors and encourages cooperation when beneficial.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponents_CR = [0.0] * (game_description.n_players - 1)
        self.MCO_index = None
        self.exploited_opponent = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C if random.random() < 0.5 else Action.D
        for i in range(self.game_description.n_players - 1):
            self.opponents_CR[i] = np.mean(history.opponent_actions[:state.round_number, i])
        self.MCO_index = np.argmax(self.opponents_CR)
        if state.round_number < self.game_description.n_rounds - 1:
            if self.opponents_CR[self.MCO_index] > 0.5:
                return Action.D
            prev_cooperators = sum(history.opponent_actions[state.round_number - 1, :])
            return Action.C if prev_cooperators >= self.game_description.n_players // 2 else Action.D
        if history and len(history.my_actions) > 0 and (not self.exploited_opponent):
            for i in range(self.game_description.n_players - 1):
                if history.opponent_actions[-1, i] == False and history.opponent_payoffs[-1, i] < history.my_payoffs[-1]:
                    self.exploited_opponent = True
                    return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_133(BaseStrategy):
    """
    Exploitative strategy for the N-Player Public Goods Game that adapts to game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        elif state.round_number <= self.game_description.n_rounds // 2:
            avg_coop_payoff = np.mean(history.my_payoffs[history.opponent_actions[:, 0]] if history.my_payoffs[history.opponent_actions[:, 0]].size else 0)
            avg_defect_payoff = np.mean(history.my_payoffs[np.logical_not(history.opponent_actions[:, 0])])
            if avg_coop_payoff >= avg_defect_payoff:
                return Action.C
            else:
                return Action.D
        else:
            cr = np.mean(history.opponent_actions[-1, :])
            if cr > 0.5:
                return Action.D
            elif cr < 0.3:
                return Action.C
            else:
                prev_payoff = history.my_payoffs[-1]
                avg_coop_payoff_recent = np.mean(history.opponent_payoffs[-1, :][history.opponent_actions[-1, :]])
                if prev_payoff < avg_coop_payoff_recent:
                    return Action.C if random.random() < 0.6 else Action.D
                else:
                    return Action.D if random.random() < 0.4 else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_134(BaseStrategy):
    """
    Adaptive Greed Strategy: Exploits opponents' behavior while prioritizing exploitation over cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators = 0
        self.previous_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.total_cooperators = sum(history.opponent_actions[-1, :] | (history.my_actions[-1] == True))
        my_action = history.my_actions[-1]
        opponent_coops = sum(history.opponent_actions[-1, :])
        coop_ratio = opponent_coops / self.game_description.n_players if self.game_description.n_players > 0 else 0
        private_payoff = 1 - int(my_action) + self.game_description.k * coop_ratio
        self.previous_payoff = private_payoff
        n = self.game_description.n_players
        k = self.game_description.k
        cr = opponent_coops / (n - opponent_coops) if n - opponent_coops > 0 else float('inf')
        p = k * cr / n if n > 0 else 1
        if random.random() < p:
            action = Action.C
        else:
            action = Action.D
        if self.previous_payoff >= 1.5:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if opponent_coops > n / 2:
                action = Action.C
        return action

class Strategy_EXPLOITATIVE_135(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy, designed for the N-Player Public Goods Game.
    
    AE gathers information about opponents' behavior in initial rounds and identifies potential exploitability. 
    It then cooperates if expected payoff is greater than or equal to exploitation threshold ε and at least one opponent has cooperated previously.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_cooperation_rates = None
        self.exploitation_threshold = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.game_description.n_players:
            return Action.C
        if self.opponent_cooperation_rates is None:
            self.opponent_cooperation_rates = np.mean(history.opponent_actions[-self.game_description.n_players:, :], axis=0)
            self.exploitation_threshold = self.game_description.k / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if np.mean(self.opponent_cooperation_rates) >= self.exploitation_threshold and np.any(history.opponent_actions[-1, :] == True):
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_136(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game.
    It initially explores opponents' behavior by cooperating in early rounds, 
    then adapts its decision based on observed cooperation levels among other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T = game_description.k / 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number <= 3:
            return Action.C
        avg_cooperators = np.mean(np.sum(history.opponent_actions[-3:, :], axis=1) + history.my_actions[-3:])
        if state.round_number == self.game_description.n_rounds - 1:
            if avg_cooperators < self.game_description.n_players / 2:
                return Action.D
        if avg_cooperators > self.T:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_137(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and game histories.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defect_against_opponent = {}
        self.opponent_types = {}

    def calculate_avg_payoff(self, players: NDArray[np.bool_], history: PlayerHistory) -> float:
        payoffs = np.where(players, history.my_payoffs[-1], 0)
        return np.mean(payoffs)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        prev_round_actions = history.opponent_actions[-1]
        prev_round_payoffs = history.opponent_payoffs[-1]
        for i, (action, payoff) in enumerate(zip(prev_round_actions, prev_round_payoffs)):
            if action:
                self.opponent_types[i] = 'cooperator'
            elif payoff > 0:
                self.opponent_types[i] = 'defector'
            else:
                self.opponent_types[i] = 'unpredictable'
        avg_coop_payoff = self.calculate_avg_payoff(prev_round_actions, history)
        avg_defect_payoff = self.calculate_avg_payoff(~prev_round_actions, history)
        if avg_coop_payoff > avg_defect_payoff:
            action = Action.C
        else:
            action = Action.D
            for opponent, defect_against in self.defect_against_opponent.items():
                if defect_against and prev_round_actions[opponent]:
                    action = Action.D
                    break
        for i, opponent_type in self.opponent_types.items():
            if opponent_type == 'cooperator':
                return Action.C
            elif opponent_type == 'defector':
                return Action.D
            else:
                cooperate_prob = 0.5
                if random.random() < cooperate_prob:
                    return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if history.my_payoffs[-1] < avg_coop_payoff:
                return Action.C
        return action

class Strategy_EXPLOITATIVE_138(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior in the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = game_description.n_players * game_description.k / (game_description.n_players + game_description.k)
        self.adapt_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] != False)
        if prev_cooperators > self.coop_threshold:
            action = Action.D
        else:
            action = Action.C
        payoff_diff = self._calculate_payoff_diff(history, self.game_description.k, self.game_description.n_players)
        if abs(payoff_diff) > self.adapt_threshold:
            if payoff_diff > 0:
                self.coop_threshold *= 1.1
            else:
                self.coop_threshold *= 0.9
        return action

    def _calculate_payoff_diff(self, history: PlayerHistory, k: float, n_players: int) -> float:
        prev_round = len(history.my_actions) - 1
        my_prev_action = history.my_actions[prev_round]
        opponent_prev_actions = history.opponent_actions[prev_round]
        total_cooperators = sum(opponent_prev_actions != False) + (my_prev_action != False)
        share_public_good = k / n_players * total_cooperators
        if my_prev_action == True:
            my_payoff = share_public_good
        else:
            my_payoff = 1 + share_public_good
        payoff_diff = 0
        for opponent_action in opponent_prev_actions:
            if opponent_action == True:
                opponent_payoff = share_public_good
            else:
                opponent_payoff = 1 + share_public_good
            payoff_diff += opponent_payoff - my_payoff
        return payoff_diff

class Strategy_EXPLOITATIVE_139(BaseStrategy):
    """
    An exploitative strategy that balances cooperation and defection to maximize payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_cooperation_count = 0
        self.threshold = game_description.n_players / 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            previous_round_cooperations = sum(history.opponent_actions[-1, :])
            self.opponent_cooperation_count = previous_round_cooperations + (Action.C.value if history.my_actions[-1] else 0)
            if self.opponent_cooperation_count > self.threshold:
                self.threshold *= 1.1
            elif self.opponent_cooperation_count < self.game_description.n_players / 3:
                self.threshold *= 0.9
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.opponent_cooperation_count <= self.threshold:
            return Action.C
        else:
            return Action.D
        if history.my_actions[-1] and all(history.opponent_actions[-1, :]):
            return Action.D

class Strategy_EXPLOITATIVE_140(BaseStrategy):
    """
    An exploitative strategy that adapts to the game environment and exploits opponents' weaknesses.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.payoff_difference = 0
        self.opponent_behavior = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_coop_payoff = np.mean(history.my_payoffs[history.my_actions] if history.my_actions[-1] else [0])
        prev_round_defect_payoff = np.mean(history.my_payoffs[~history.my_actions] if not history.my_actions[-1] else [0])
        if state.round_number <= self.game_description.n_rounds // 4:
            if prev_round_coop_payoff > prev_round_defect_payoff:
                return Action.C
            return Action.D
        elif state.round_number <= 3 * self.game_description.n_rounds // 4:
            self.payoff_difference += prev_round_coop_payoff - prev_round_defect_payoff
            cooperation_rate = np.mean(history.opponent_actions[-1, :])
            if cooperation_rate >= self.cooperation_threshold:
                return Action.C
            else:
                self.cooperation_threshold -= 0.01 * (self.payoff_difference / (state.round_number - self.game_description.n_rounds // 4))
                return Action.D
        else:
            opponent_coop_rates = np.mean(history.opponent_actions, axis=0)
            for i, rate in enumerate(opponent_coop_rates):
                if rate > 0.5 and i not in self.opponent_behavior:
                    self.opponent_behavior[i] = True
            if any(self.opponent_behavior.values()):
                return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_141(BaseStrategy):
    """
    The Adaptive Exploiter strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.previous_cooperators = 0
        self.my_previous_action = Action.C

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        self.previous_cooperators = sum(history.opponent_actions[-1, :] == True)
        if self.previous_cooperators <= self.game_description.n_players / 2:
            action = Action.D
        elif self.game_description.k > 1.5 and self.my_previous_action == Action.D:
            action = Action.C
        else:
            action = Action.D
        opponent_defected_and_cooperators_greater_than_half = (history.opponent_actions[-1, :] == False).any() and self.previous_cooperators > self.game_description.n_players / 2
        if opponent_defected_and_cooperators_greater_than_half:
            action = Action.D
        if state.round_number + 1 == self.game_description.n_rounds:
            return Action.D
        self.my_previous_action = action
        return action

class Strategy_EXPLOITATIVE_142(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        prev_coops = sum(history.opponent_actions[-1, :] == True)
        if prev_coops == 0 and random.random() < 1 / self.n_players:
            return Action.C
        coop_thresh = self.k / self.n_players * (1 + prev_coops / self.n_players)
        expected_payoff_C = self.k / self.n_players * (prev_coops + 1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if expected_payoff_C >= coop_thresh:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_143(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Predator"
    
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors 
    while adapting to the game's history. This strategy depends only on the game parameters 
    (n, r, k) and the observed actions of other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.num_cooperators = 0
        self.threshold = game_description.n_players // 2
        self.rounds_for_threshold = max(3, int(0.1 * game_description.n_rounds))
        self.cooperator_counts = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        self.num_cooperators = sum(history.opponent_actions[-1, :])
        self.cooperator_counts.append(self.num_cooperators)
        if len(self.cooperator_counts) >= self.rounds_for_threshold:
            avg_cooperators = np.mean(self.cooperator_counts[-self.rounds_for_threshold:])
            if avg_cooperators > self.game_description.n_players / 2:
                self.threshold += 1
            else:
                self.threshold -= 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if self.num_cooperators <= self.threshold:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_144(BaseStrategy):
    """
    Exploitative strategy that balances cooperation and defection based on game history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :] + [True])
        if previous_round_cooperators >= self.game_description.n_players / 2:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_145(BaseStrategy):
    """
    An exploitative strategy that adapts to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.c_prev = 0
        self.d_prev = 0
        self.avg_coop_rate = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.c_prev = sum(history.opponent_actions[-1, :])
        self.d_prev = self.game_description.n_players - 1 - self.c_prev
        if self.c_prev > self.d_prev:
            action = Action.D
        else:
            action = Action.C
        if state.round_number >= self.game_description.n_rounds // 2 and self.avg_coop_rate > 0.6:
            action = Action.D
        if state.round_number % 5 == 1:
            recent_rounds = history.opponent_actions[max(0, state.round_number - 5):state.round_number, :]
            self.avg_coop_rate = np.mean(np.sum(recent_rounds, axis=0) / recent_rounds.shape[0])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_146(BaseStrategy):
    """
    The Adaptive Parasite strategy aims to exploit the cooperative behavior of other players 
    while minimizing its own contributions to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.m = 5
        self.theta = 0.6
        self.coop_count = 0
        self.total_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number <= 3:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_actions = history.opponent_actions[-self.m:, :]
        n_cooperators = np.sum(opponent_actions, axis=(0, 1))
        avg_coop_rate = n_cooperators / (self.m * self.game_description.n_players)
        if avg_coop_rate >= self.theta:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_147(BaseStrategy):
    """
    A robust and adaptive exploitative strategy for the N-Player Public Goods Game.
    It evaluates the average contribution of opponents in previous rounds to decide whether to cooperate or defect.
    Additionally, it detects and responds to exploitation by comparing expected and actual payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold_avg_contrib = 0.5
        self.tolerance_payoff_diff = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_contributions_prev_round = np.sum(history.opponent_actions[-1, :] + (history.my_actions[-1] == Action.C))
        avg_contrib = total_contributions_prev_round / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        expected_payoff_if_everyone_cooperates = self.game_description.k * self.game_description.n_players
        actual_payoff_prev_round = history.my_payoffs[-1]
        if avg_contrib > self.threshold_avg_contrib or (avg_contrib == self.threshold_avg_contrib and np.random.choice([True, False])):
            if abs(expected_payoff_if_everyone_cooperates - actual_payoff_prev_round) <= self.tolerance_payoff_diff:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_148(BaseStrategy):
    """
    An adaptive exploiter strategy for the N-player public goods game.
    It balances cooperation and defection to maximize payoffs by adapting to changing conditions over time.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.history = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.history = history
        ac = self.calculate_average_cooperation()
        p_coop = self.update_probability(ac)
        d = self.count_defections()
        if d > self.game_description.n_players / 2:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < p_coop else Action.D

    def calculate_average_cooperation(self) -> float:
        """
        Calculate the average cooperation rate of all opponents over the last three rounds.
        """
        opponent_actions = self.history.opponent_actions
        recent_rounds = opponent_actions[max(0, len(opponent_actions) - 3):]
        if not recent_rounds.size:
            return 0.5
        ac = np.mean(np.sum(recent_rounds, axis=1) / self.game_description.n_players)
        return ac

    def update_probability(self, ac: float) -> float:
        """
        Update the probability of cooperating based on the average cooperation rate and game parameters.
        """
        k = self.game_description.k
        n = self.game_description.n_players
        if ac > k / n:
            p_coop = 1 - k / n
        else:
            p_coop = min(0.5, k / n)
        return p_coop

    def count_defections(self) -> int:
        """
        Count the number of defections in the last round.
        """
        opponent_actions = self.history.opponent_actions
        if not opponent_actions.size:
            return 0
        d = np.sum(~opponent_actions[-1])
        return int(d)

class Strategy_EXPLOITATIVE_149(BaseStrategy):
    """
    A robust and exploitative strategy for N-Player Public Goods Game.
    Combines elements of cooperation and defection, adapting to game history while maximizing payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.m = math.ceil(game_description.n_rounds / 10)
        self.θ = 0.5
        self.p = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_coop_rate = np.mean(history.my_actions[-min(self.m, state.round_number):])
        if avg_coop_rate > self.θ:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_150(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Defector with Conditional Cooperation" (ADC)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        theta = 0.5 * self.game_description.k / self.game_description.n_players
        opponent_actions_prev_round = history.opponent_actions[-1, :]
        avg_opponent_cooperation_rate = np.mean(opponent_actions_prev_round)
        num_cooperators_prev_round = sum(opponent_actions_prev_round)
        p = self.game_description.k / self.game_description.n_players * (num_cooperators_prev_round / self.game_description.n_players)
        if avg_opponent_cooperation_rate > theta:
            if num_cooperators_prev_round >= theta:
                return Action.C if random.random() < p else Action.D
            else:
                return Action.D
        opponent_has_cooperated_more_than_50_percent = np.mean(history.opponent_actions) > 0.5 and avg_opponent_cooperation_rate > 0.5
        q = 1 - self.game_description.k / self.game_description.n_players * avg_opponent_cooperation_rate
        if opponent_has_cooperated_more_than_50_percent:
            return Action.D if random.random() < q else Action.C
        return Action.D

class Strategy_EXPLOITATIVE_151(BaseStrategy):
    """
    The 'Exploiter' strategy encourages cooperation initially, adapts to opponents' behavior,
    punishes exploiters, and identifies opportunities to exploit cooperators.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate_threshold = 0.5
        self.punishment_threshold = 0.5
        self.exploitation_threshold = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_cooperations = np.sum(history.opponent_actions, axis=0)
        cooperation_rates = opponent_cooperations / (state.round_number - 1)
        if state.round_number < int(0.1 * self.game_description.n_rounds):
            return Action.C
        overall_cooperation_rate = np.mean(cooperation_rates)
        action = Action.C if overall_cooperation_rate >= self.cooperation_rate_threshold else Action.D
        for i, (opponent_action, opponent_cooperation_rate) in enumerate(zip(history.opponent_actions[-1], cooperation_rates)):
            if opponent_action == 0 and opponent_cooperation_rate < self.punishment_threshold:
                action = Action.D
        for i, (opponent_action, opponent_cooperation_rate) in enumerate(zip(history.opponent_actions[-1], cooperation_rates)):
            if opponent_action == 1 and opponent_cooperation_rate > self.exploitation_threshold:
                action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_EXPLOITATIVE_152(BaseStrategy):
    """
    An exploitative strategy that adapts to observed cooperation levels in the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.trend_window_size = 5
        self.trend_coefficient = 0.4 * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_opponent_actions = history.opponent_actions[-1, :]
        total_cooperators_last_round = sum(previous_round_opponent_actions)
        if len(history.my_actions) >= self.trend_window_size:
            recent_cooperations = np.sum(history.opponent_actions[-self.trend_window_size:, :], axis=0)
            recent_total_cooperators = np.sum(recent_cooperations, axis=0) / self.game_description.n_players
            cooperation_trend = np.mean(recent_total_cooperators)
        else:
            cooperation_trend = total_cooperators_last_round / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif total_cooperators_last_round >= self.game_description.n_players // 2:
            return Action.C
        elif cooperation_trend > self.trend_coefficient / self.game_description.n_players:
            if len(history.my_actions) < self.trend_window_size + 1:
                return Action.D
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_153(BaseStrategy):
    """
    Exploitative strategy "Adaptive Follower with Endgame Manipulation" for N-Player Public Goods Game.
    
    Combines elements of tit-for-tat, win-stay-lose-shift, and endgame manipulation to create an adaptive 
    and exploitative approach. Monitors opponents' cooperation rates and adapts to their strategies to 
    maximize own payoffs while minimizing vulnerabilities to exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_ocrs = [0.5] * game_description.n_players
        self.total_payoffs_coop = 0
        self.total_payoffs_defect = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        last_round_actions = history.opponent_actions[-1]
        for i, action in enumerate(last_round_actions):
            self.opponent_ocrs[i] += (action - self.opponent_ocrs[i]) / state.round_number
        if state.round_number <= self.game_description.n_rounds // 2:
            coop_count_last_round = sum(last_round_actions)
            return Action.C if coop_count_last_round >= self.game_description.n_players / 2 else Action.D
        elif state.round_number <= self.game_description.n_rounds - 2:
            ocr_avg = sum(self.opponent_ocrs) / len(self.opponent_ocrs)
            return Action.C if ocr_avg > 0.5 else Action.D
        else:
            last_payoffs_coop = 1 - history.my_actions[-1] + self.game_description.k / self.game_description.n_players * sum(last_round_actions)
            last_payoffs_defect = 1 + self.game_description.k / self.game_description.n_players * sum(last_round_actions)
            self.total_payoffs_coop += last_payoffs_coop
            self.total_payoffs_defect += last_payoffs_defect
            return Action.C if self.total_payoffs_coop >= self.total_payoffs_defect else Action.D

class Strategy_EXPLOITATIVE_154(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_environment = False
        self.prev_round_coop = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < 3:
            return random.choice([Action.C, Action.D]) if random.random() < 0.5 else Action.C
        c = sum(history.opponent_actions[-1, :])
        self.coop_environment = c > self.game_description.n_players / 2
        if self.coop_environment:
            return Action.D if random.random() > self.game_description.k / self.game_description.n_players else Action.C
        elif self.prev_round_coop is not None and self.prev_round_coop == Action.C:
            return Action.D
        else:
            if state.round_number % 3 == 0:
                self.coop_environment = False
            return random.choice([Action.C, Action.D]) if random.random() < 0.5 else Action.C
        self.prev_round_coop = history.my_actions[-1]

class Strategy_EXPLOITATIVE_155(BaseStrategy):
    """
    The Adaptive Exploiter strategy balances exploration and exploitation while adapting to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploration_phase = True
        self.exploitation_phase = False
        self.previous_opponent_actions = []

    def calculate_own_payoff(self, round_number: int, history: PlayerHistory) -> float:
        my_action = history.my_actions[round_number]
        total_cooperators = sum(history.opponent_actions[round_number]) + my_action
        return 1 - my_action + self.game_description.k / self.game_description.n_players * total_cooperators

    def calculate_avg_opponent_payoff(self, round_number: int, history: PlayerHistory) -> float:
        opponent_payoffs = []
        for player in range(self.game_description.n_players):
            if player != 0:
                opponent_action = history.opponent_actions[round_number, player]
                total_cooperators = sum(history.opponent_actions[round_number]) + history.my_actions[round_number]
                payoff = 1 - opponent_action + self.game_description.k / self.game_description.n_players * total_cooperators
                opponent_payoffs.append(payoff)
        return np.mean(opponent_payoffs)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.previous_opponent_actions = []
            return Action.D
        if history is not None:
            self.previous_opponent_actions.append(history.opponent_actions[-1, :])
        if self.exploration_phase:
            if state.round_number % 2 == 0:
                return Action.C
            else:
                return Action.D
        majority_cooperated = sum(self.previous_opponent_actions[-1]) > self.game_description.n_players / 2
        minority_cooperated = sum(self.previous_opponent_actions[-1]) < self.game_description.n_players / 2
        if majority_cooperated:
            return Action.D
        elif minority_cooperated:
            return Action.C
        else:
            own_payoff = self.calculate_own_payoff(state.round_number - 1, history)
            avg_opponent_payoff = self.calculate_avg_opponent_payoff(state.round_number - 1, history)
            if own_payoff > avg_opponent_payoff:
                return Action.D
            else:
                return Action.C
        if state.round_number >= self.game_description.n_rounds // 4:
            self.exploration_phase = False
            self.exploitation_phase = True
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_156(BaseStrategy):
    """
    A hybrid exploitative strategy combining elements of Tit-for-Tat and Pavlov strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_round_coops = []
        self.avg_payoff_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_coops_prev = sum(history.opponent_actions[-1, :])
        self.avg_payoff_prev = np.mean(history.my_payoffs)
        if self.avg_payoff_prev < 1.5:
            if total_coops_prev >= self.game_description.n_players / 3:
                return Action.C
        elif self.avg_payoff_prev > 2.0:
            if total_coops_prev <= self.game_description.n_players / 3:
                return Action.D
        if total_coops_prev >= self.game_description.n_players // 2:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_EXPLOITATIVE_157(BaseStrategy):
    """
    A strategy that adapts to opponent behavior by cooperating when average payoffs are high and defecting otherwise.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        avg_payoff_last_round = np.mean(history.my_payoffs[-1] + np.sum(history.opponent_payoffs[-1, :], axis=0))
        self.threshold = max(self.threshold, avg_payoff_last_round / self.game_description.n_players)
        num_opponents_defected_last_round = sum((1 for opponent in history.opponent_actions[-1, :] if not opponent))
        p_defect = num_opponents_defected_last_round / self.game_description.n_players * (1 - self.game_description.k / self.game_description.n_players)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if num_opponents_defected_last_round == self.game_description.n_players - 1:
            return Action.D
        return Action.C if random.random() > p_defect else Action.D

class Strategy_EXPLOITATIVE_159(BaseStrategy):
    """
    Exploitative strategy, adapting to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        if self.game_description.n_players == 2:
            return Action.C if state.round_number % 2 == 0 else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :] + [Action.C.value]) if history.my_payoffs.size > 0 else 0
        if state.round_number < self.game_description.n_rounds - 1:
            return Action.C if random.random() < self.game_description.k / self.game_description.n_players and prev_cooperators >= self.game_description.n_players // 2 else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_160(BaseStrategy):
    """
    The Adaptive Follower strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators = 0
        self.opponent_cooperation_rate = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.total_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        opponent_rounds = history.opponent_actions.shape[0] - 1
        if opponent_rounds > 0:
            self.opponent_cooperation_rate = np.mean(history.opponent_actions[:opponent_rounds, :]) / self.game_description.n_players
        CT = self.game_description.k / self.game_description.n_players
        if self.total_cooperators >= CT:
            return Action.C
        elif self.opponent_cooperation_rate > 0.5:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.game_description.n_players == 2:
            if history.opponent_actions.shape[0] > 1:
                return Action(history.my_actions[-1])

class Strategy_EXPLOITATIVE_161(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game.
    Cooperates only when it's profitable, and defects otherwise.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        prev_round_actions = history.opponent_actions[-1]
        prev_round_payoffs = history.opponent_payoffs[-1]
        n_cooperators = sum(prev_round_actions)
        n_defectors = self.game_description.n_players - 1 - n_cooperators
        if n_cooperators > 0:
            avg_coop_payoff = np.mean(prev_round_payoffs[prev_round_actions])
        else:
            avg_coop_payoff = 0.0
        if n_defectors > 0:
            avg_defect_payoff = np.mean(prev_round_payoffs[~prev_round_actions])
        else:
            avg_defect_payoff = 0.0
        if avg_coop_payoff > avg_defect_payoff:
            return Action.C
        else:
            return Action.D
        if self.game_description.n_rounds - state.round_number <= math.ceil(self.game_description.n_players / 2):
            return Action.D

class Strategy_EXPLOITATIVE_162(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.weak_players = set()
        self.punished_opponents = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        avg_cooperators = sum(history.opponent_actions[-1]) / self.game_description.n_players
        if avg_cooperators >= self.game_description.n_players / 2:
            self.weak_players.clear()
            self.punished_opponents.clear()
            return Action.C
        for i, opponent_action in enumerate(history.opponent_actions[-1]):
            if opponent_action and i not in self.weak_players:
                self.weak_players.add(i)
        if len(self.punished_opponents) < 2:
            for i, opponent_action in enumerate(history.opponent_actions[-1]):
                if not opponent_action and history.my_actions[-1]:
                    self.punished_opponents.append(i)
        else:
            self.punished_opponents.pop(0)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if any((player in self.weak_players for player, action in enumerate(history.opponent_actions[-1]) if action)):
            return Action.D
        elif any((i in self.punished_opponents for i, opponent_action in enumerate(history.opponent_actions[-1]))):
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_163(BaseStrategy):
    """
    The Adaptive Predator strategy aims to exploit a wide range of opponent behaviors by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_coop = 0
        self.avg_payoff_defect = 0
        self.coop_count = 0
        self.defect_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        opponent_actions = history.opponent_actions[-1]
        cooperators = np.sum(opponent_actions)
        defectors = self.game_description.n_players - 1 - cooperators
        if cooperators > 0:
            avg_payoff_coop = np.mean(history.opponent_payoffs[-1, opponent_actions])
        else:
            avg_payoff_coop = 0
        if defectors > 0:
            avg_payoff_defect = np.mean(history.opponent_payoffs[-1, ~opponent_actions])
        else:
            avg_payoff_defect = 0
        if avg_payoff_coop > avg_payoff_defect:
            action = Action.C
        elif avg_payoff_coop < avg_payoff_defect:
            action = Action.D
        else:
            action = random.choice([Action.C, Action.D])
        if self.is_freeloader(history):
            action = Action.D
        elif self.is_cooperator(history):
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

    def is_freeloader(self, history: PlayerHistory) -> bool:
        """Check if an opponent has defected for a few consecutive rounds."""
        recent_actions = history.opponent_actions[-3:, :]
        for actions in recent_actions:
            if np.all(~actions):
                return True
        return False

    def is_cooperator(self, history: PlayerHistory) -> bool:
        """Check if an opponent has cooperated for a few consecutive rounds."""
        recent_actions = history.opponent_actions[-3:, :]
        for actions in recent_actions:
            if np.all(actions):
                return True
        return False

class Strategy_EXPLOITATIVE_164(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy aims to exploit the opponents' behavior by adapting to their cooperation rates.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.5
        self.avg_coop_rate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < self.cooperate_prob else Action.D
        opponent_actions = history.opponent_actions[-1]
        total_coop_prev_round = np.sum(opponent_actions)
        if total_coop_prev_round <= self.game_description.n_players / 2:
            cooperate = True
        else:
            cooperate = False
        if state.round_number % 5 == 0 and state.round_number > 5:
            opponent_actions_last_5_rounds = history.opponent_actions[-5:]
            self.avg_coop_rate = np.mean(np.sum(opponent_actions_last_5_rounds, axis=1)) / self.game_description.n_players
            if self.avg_coop_rate > 0.5:
                self.cooperate_prob += 0.1
            elif self.avg_coop_rate < 0.3:
                self.cooperate_prob -= 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            cooperate = False
        prev_round_opponent_actions = history.opponent_actions[-2] if len(history.opponent_actions) > 1 else None
        if prev_round_opponent_actions is not None and total_coop_prev_round == self.game_description.n_players / 2 and (np.sum(prev_round_opponent_actions) > self.game_description.n_players - np.sum(prev_round_opponent_actions)):
            cooperate = True
        return Action.C if cooperate else Action.D

class Strategy_EXPLOITATIVE_165(BaseStrategy):
    """
    An exploitative strategy that adapts to the cooperation rate of opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_cooperation_rate = np.mean(history.opponent_actions[-1, :] + (history.my_actions[-1] == Action.C))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if avg_cooperation_rate < 1 / self.game_description.n_players:
            return Action.D
        if avg_cooperation_rate > self.threshold:
            return Action.D
        p_coop = (state.round_number + 1) / self.game_description.n_rounds * (1 - self.threshold)
        return Action.C if random.random() < p_coop else Action.D

class Strategy_EXPLOITATIVE_166(BaseStrategy):
    """
    Adaptive Exploiter (AE) strategy balances short-term exploitation with long-term adaptability.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_payoff = 0.0
        self.avg_defect_payoff = 0.0
        self.coop_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.avg_coop_payoff = self.calculate_avg_coop_payoff(history)
        self.avg_defect_payoff = self.calculate_avg_defect_payoff(history)
        if self.avg_coop_payoff > self.avg_defect_payoff:
            self.coop_threshold += 0.1
        else:
            self.coop_threshold -= 0.1
        punish_opponent = self.identify_deviant_opponent(history)
        if punish_opponent:
            return Action.D
        if self.avg_coop_payoff > self.avg_defect_payoff + self.coop_threshold:
            return Action.C
        else:
            return Action.D

    def calculate_avg_coop_payoff(self, history: PlayerHistory) -> float:
        prev_round = history.my_actions.shape[0] - 1
        coop_mask = history.opponent_actions[prev_round, :] == True
        coop_payoffs = history.opponent_payoffs[prev_round, :][coop_mask]
        return np.mean(coop_payoffs) if coop_payoffs.size else 0.0

    def calculate_avg_defect_payoff(self, history: PlayerHistory) -> float:
        prev_round = history.my_actions.shape[0] - 1
        defect_mask = history.opponent_actions[prev_round, :] == False
        defect_payoffs = history.opponent_payoffs[prev_round, :][defect_mask]
        return np.mean(defect_payoffs) if defect_payoffs.size else 0.0

    def identify_deviant_opponent(self, history: PlayerHistory) -> bool:
        prev_round = history.my_actions.shape[0] - 1
        coop_count = np.sum(history.opponent_actions[prev_round, :])
        defect_count = self.game_description.n_players - 1 - coop_count
        if coop_count > self.game_description.n_players // 2 and defect_count == 1:
            return True
        elif defect_count > self.game_description.n_players // 2 and coop_count == 1:
            return True
        else:
            return False

class Strategy_EXPLOITATIVE_167(BaseStrategy):
    """
    A strategy that balances exploration and exploitation by adapting to the game's history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < 0.5 else Action.C
        n = self.game_description.n_players
        prev_round_cooperators = sum(history.opponent_actions[-1, :])
        if prev_round_cooperators > (n - 1) // 2:
            self.theta -= 0.05
        else:
            self.theta += 0.05
        self.theta = max(0, min(self.theta, 1))
        if prev_round_cooperators >= n * self.theta or state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif prev_round_cooperators == n * self.theta:
            return random.choice([Action.C, Action.D]) if random.random() < 0.5 else Action.C
        return Action.C

class Strategy_EXPLOITATIVE_168(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to opponents' cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.random_action_taken = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if history is None or len(history.my_actions) < state.round_number - 1:
            return Action.D
        prev_round_opponent_coops = sum(history.opponent_actions[state.round_number - 2, :])
        total_prev_round_coops = prev_round_opponent_coops + (history.my_actions[state.round_number - 2] == Action.C)
        if state.round_number < self.game_description.n_rounds:
            cooperation_threshold = self.game_description.k / 2
            if total_prev_round_coops >= cooperation_threshold:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_169(BaseStrategy):
    """
    An exploitative strategy that adapts to the opponents' behavior in a public goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2 or (state.round_number == 0 and history is None):
            return Action.D
        if self.avg_coop_rate is None:
            opponent_actions = np.concatenate([history.opponent_actions, history.my_actions[:, None]], axis=1)
            total_contributions = np.sum(opponent_actions, axis=1)
            self.avg_coop_rate = np.mean(total_contributions / self.game_description.n_players)
        if self.avg_coop_rate > 0.7:
            coop_prob = self.game_description.k / self.game_description.n_players
        elif self.avg_coop_rate >= 0.4 and self.avg_coop_rate <= 0.7:
            coop_prob = 0.5
        else:
            coop_prob = 1 - self.game_description.k / self.game_description.n_players
        if random.random() < coop_prob:
            return Action.C
        else:
            return Action.D
        if state.round_number >= self.game_description.n_rounds - 2:
            return Action.D

class Strategy_EXPLOITATIVE_170(BaseStrategy):
    """
    A robust and exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_frequencies = [0.5] * game_description.n_players
        self.learning_rate = 0.1
        self.cooperation_threshold = 0.3
        self.punishment_probability = 0.8
        self.self_protection_probability = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            self.cooperation_frequencies[i] = self.learning_rate * (1 - opponent_action) + (1 - self.learning_rate) * self.cooperation_frequencies[i]
        for i, cf in enumerate(self.cooperation_frequencies):
            if cf < self.cooperation_threshold:
                return Action.D
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if not opponent_action:
                return random.choices([Action.C, Action.D], weights=[0.2, self.punishment_probability])[0]
        high_cf_opponents = sum((cf > self.cooperation_threshold for cf in self.cooperation_frequencies))
        if high_cf_opponents > self.game_description.n_players // 2:
            return random.choices([Action.C, Action.D], weights=[0.8, self.self_protection_probability])[0]
        return Action.C

class Strategy_EXPLOITATIVE_171(BaseStrategy):
    """
    This strategy adapts to the game's dynamics and exploits opponent behaviors 
    while being robust to various strategies. It gathers information about opponents' 
    initial tendencies, observes the total number of cooperators and defectors from 
    the previous round, calculates the cooperation rate, sets a dynamic cooperation 
    threshold based on the game parameters, chooses actions accordingly, and adjusts 
    the threshold to punish or reward opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        cooperation_rate = total_cooperators / self.game_description.n_players
        if cooperation_rate > 0.5:
            self.cooperation_threshold = self.game_description.k / self.game_description.n_players
        else:
            self.cooperation_threshold = 0.5
        if cooperation_rate >= self.cooperation_threshold:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_172(BaseStrategy):
    """
    An adaptive exploitative strategy for the N-Player Public Goods Game.
    
    The strategy initially cooperates to encourage others to contribute and build a strong public good.
    Then it adjusts its behavior based on the cooperation rate of other players, defecting when 
    the expected payoff from cooperating is lower than that from defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defection_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round_cooperations = sum(history.opponent_actions[-1, :])
        e_c = self.game_description.k / self.game_description.n_players * (previous_round_cooperations + 1) - 1
        e_d = 1 + self.game_description.k / self.game_description.n_players * previous_round_cooperations
        if e_c >= e_d:
            action = Action.C
        else:
            action = Action.D
        average_cooperation_rate = (previous_round_cooperations + 1) / self.game_description.n_players
        self.defection_threshold = self.defection_threshold * 0.9 + average_cooperation_rate * 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_173(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to balance cooperation and defection based on the game's history.
    It adapts its decision rule based on the average payoffs from cooperation and defection, using an adaptive threshold.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.adaptive_threshold = 0.5
        self.learning_rate = 0.1
        self.previous_cooperation_payoff = None
        self.previous_defection_payoff = None

    def calculate_average_cooperation_payoff(self, history: PlayerHistory) -> float:
        total_contributions = sum(history.opponent_actions[-1, :])
        return self.game_description.k / self.game_description.n_players * total_contributions

    def calculate_average_defection_payoff(self, history: PlayerHistory) -> float:
        return 1 + self.game_description.k / self.game_description.n_players * sum(history.opponent_actions[-1, :])

    def update_adaptive_threshold(self, cooperation_payoff: float, defection_payoff: float) -> None:
        self.adaptive_threshold += self.learning_rate * (cooperation_payoff - defection_payoff)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperation_payoff = self.calculate_average_cooperation_payoff(history)
        defection_payoff = self.calculate_average_defection_payoff(history)
        if cooperation_payoff > defection_payoff:
            action = Action.C
        else:
            action = Action.D
        self.update_adaptive_threshold(cooperation_payoff, defection_payoff)
        total_contributions = sum(history.opponent_actions[-1, :])
        if total_contributions > self.game_description.n_players * self.adaptive_threshold:
            action = Action.C
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_174(BaseStrategy):
    """
    The Adaptive Predator strategy adapts to changing game dynamics, 
    exploiting opponents' weaknesses and manipulating endgame momentum.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_payoff = 0
        self.avg_defect_payoff = 0
        self.exploitable_opponents = set()
        self.opponent_actions = []
        self.my_actions = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        prev_round = state.round_number - 1
        my_prev_action = history.my_actions[prev_round]
        coop_payoff = 1 - int(my_prev_action) + self.game_description.k / self.game_description.n_players * sum(history.opponent_actions[prev_round, :])
        defect_payoff = 1 + self.game_description.k / self.game_description.n_players * sum(history.opponent_actions[prev_round, :])
        if my_prev_action:
            self.avg_coop_payoff += coop_payoff
        else:
            self.avg_defect_payoff += defect_payoff
        for i, opponent in enumerate(history.opponent_actions[prev_round, :]):
            if not opponent and coop_payoff < defect_payoff:
                self.exploitable_opponents.add(i)
        if any((opponent in self.exploitable_opponents for opponent in range(self.game_description.n_players - 1))):
            return Action.D
        if state.round_number < self.game_description.n_rounds - 1:
            if self.avg_coop_payoff > self.avg_defect_payoff:
                return Action.C
            else:
                return Action.D
        if sum(history.opponent_actions[prev_round, :]) >= self.game_description.n_players / 2:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_177(BaseStrategy):
    """
    The Adaptive Exploitation strategy adapts to various opponent behaviors and maximizes payoff.
    It encourages cooperation in initial rounds, then exploits opponents based on observed cooperation rates.
    In late rounds, it defects if its total payoff is significantly lower than others'.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.THRESHOLD_COOP = 0.6
        self.DEFECT_MARGIN = 1.0
        self.last_round_coop = []
        self.total_payoff = 0.0
        self.others_avg_payoff = 0.0
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        avg_coop_rate = np.mean(self.last_round_coop)
        if history is not None:
            self.last_round_coop.append(np.sum(history.opponent_actions[-1, :]))
            self.total_payoff += history.my_payoffs[-1]
            self.others_avg_payoff = np.mean(history.opponent_payoffs[-1, :])
        if avg_coop_rate > self.THRESHOLD_COOP:
            return Action.C
        if state.round_number > self.game_description.n_rounds // 2 and self.total_payoff < self.others_avg_payoff - self.DEFECT_MARGIN:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_178(BaseStrategy):
    """
    A dynamic strategy for the N-Player Public Goods Game that adapts to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        else:
            opponent_actions = history.opponent_actions[-1, :]
            C_prev = sum(opponent_actions)
            coop_ratio = C_prev / self.game_description.n_players
            if self.theta is None:
                self.theta = 0.5 if self.game_description.k > 2 else 0.3
            action = Action.C if coop_ratio >= self.theta else Action.D
            if abs(coop_ratio - self.theta) > 0.2:
                self.theta += 0.1 if coop_ratio > self.theta else -0.1
            if state.round_number == self.game_description.n_rounds - 1:
                action = Action.D
            return action

class Strategy_EXPLOITATIVE_179(BaseStrategy):
    """
    Exploitative Tit-for-Tat Pavlov Strategy.
    
    This strategy combines elements of tit-for-tat and Pavlov's strategy, 
    with an additional exploitation component to take advantage of opponents' overly cooperative behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = game_description.n_players * 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_prev_actions = history.opponent_actions[-1, :]
        opponent_payoffs_last_round = history.opponent_payoffs[-1, :]
        cooperators = sum(opponent_prev_actions) + 1 if history.my_actions[-1] else sum(opponent_prev_actions)
        opponent_exploitable = any(opponent_payoffs_last_round > self.game_description.k / 2)
        tft_component = all((opponent_prev_action == True for opponent_prev_action in opponent_prev_actions)) and (not opponent_exploitable)
        pavlov_component = cooperators > self.theta
        if tft_component or pavlov_component:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_180(BaseStrategy):
    """
    The Adaptive Exploiter is a history-dependent strategy that adjusts its behavior based on observed opponent actions and payoffs from previous rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_C_history = []
        self.avg_payoff_D_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if history.my_payoffs[-1] > 0:
            avg_payoff_C = np.mean(history.opponent_payoffs[-1, history.opponent_actions[-1]])
            avg_payoff_D = np.mean(history.opponent_payoffs[-1, ~history.opponent_actions[-1]])
        else:
            avg_payoff_C = 0
            avg_payoff_D = 0
        self.avg_payoff_C_history.append(avg_payoff_C)
        self.avg_payoff_D_history.append(avg_payoff_D)
        theta = self.game_description.k / self.game_description.n_players * 0.75
        if avg_payoff_C > avg_payoff_D and avg_payoff_C > theta:
            cooperate_probability = 1
        else:
            cooperate_probability = 0
        if avg_payoff_C == avg_payoff_D:
            cooperate_probability = 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if random.random() < cooperate_probability:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_181(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive.
    It leverages the game's history to inform its decisions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.c_history = []
        self.d_history = []
        self.alpha = 0.05
        self.beta = 0.05
        self.gamma = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        c_prev_round = sum(history.opponent_actions[-1, :])
        d_prev_round = self.n - c_prev_round
        T = self.k / self.n * (1 - self.alpha)
        P = 1 + self.beta * d_prev_round / self.n
        R = 1 - self.gamma * c_prev_round / self.n
        if c_prev_round / self.n >= T:
            action = Action.C
        else:
            action = Action.D
        if state.round_number != self.r - 1:
            if action == Action.C:
                self.c_history.append(1)
                self.d_history.append(0)
            else:
                self.c_history.append(0)
                self.d_history.append(1)
        if state.round_number == self.r - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_182(BaseStrategy):
    """
    Adaptive Exploiter strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = game_description.n_players / 2
        self.p_C = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number <= 5:
            num_cooperators_last_round = sum(history.opponent_actions[-1, :] + [True])
            most_players_cooperated_last_round = num_cooperators_last_round >= self.game_description.n_players / 2
            return Action.C if most_players_cooperated_last_round else Action.D
        num_cooperators_last_round = sum(history.opponent_actions[-1, :] + [True])
        if num_cooperators_last_round >= self.coop_threshold:
            self.p_C = self.game_description.k / self.game_description.n_players
            return Action.C if random.random() < self.p_C else Action.D
        elif state.round_number > 0.8 * self.game_description.n_rounds and num_cooperators_last_round >= self.coop_threshold:
            self.p_C = max(self.game_description.k / self.game_description.n_players - (self.game_description.n_rounds - state.round_number) / self.game_description.n_rounds, 0)
            return Action.C if random.random() < self.p_C else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_183(BaseStrategy):
    """
    An adaptive exploiter strategy that balances exploitation and adaptability.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.k / game_description.n_players
        self.last_round_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        last_round_avg_payoff = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1, :])
        if last_round_avg_payoff <= 1:
            self.cooperation_threshold -= 1 / (self.game_description.n_players * self.game_description.n_rounds)
        elif self.last_round_cooperators >= self.game_description.k / self.game_description.n_players:
            self.cooperation_threshold += 1 / (self.game_description.n_players * self.game_description.n_rounds)
        if self.last_round_cooperators >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D
        self.last_round_cooperators = sum(history.opponent_actions[-1, :])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_184(BaseStrategy):
    """
    Adaptive Exploiter (AE) strategy.

    This strategy initially cooperates with probability 0.5.
    Then, it calculates the percentage of cooperative actions taken by all opponents in previous rounds,
    and decides to cooperate or defect based on a cooperation threshold of 0.6.
    Additionally, if an opponent has defected in the previous round, AE defects with probability 0.8.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        opponent_cooperation_rate = self.calculate_opponent_cooperation_rate(history)
        if opponent_cooperation_rate > self.coop_threshold:
            action = Action.C
        elif opponent_cooperation_rate < self.coop_threshold:
            action = Action.D
        else:
            action = Action.C if random.random() < 0.5 else Action.D
        if history and self.has_opponent_defected(history, state.round_number - 1):
            if random.random() < 0.8:
                action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

    def calculate_opponent_cooperation_rate(self, history: PlayerHistory) -> float:
        opponent_actions = history.opponent_actions[:history.my_actions.size]
        cooperators_per_round = np.sum(opponent_actions, axis=1)
        cooperation_rate = np.mean(cooperators_per_round) / self.game_description.n_players
        return cooperation_rate

    def has_opponent_defected(self, history: PlayerHistory, round_number: int) -> bool:
        opponent_actions = history.opponent_actions[round_number]
        return not np.all(opponent_actions)

class Strategy_EXPLOITATIVE_185(BaseStrategy):
    """
    An adaptive strategy that initially cooperates and then exploits opponents based on their cooperation rates.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_labels = [None] * game_description.n_players
        self.coop_counts = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        for i in range(self.game_description.n_players):
            if history.my_actions.size > 1:
                coop_rate = self.coop_counts[i] / (state.round_number - 1)
                if coop_rate > 0.5:
                    self.opponent_labels[i] = Action.C
                else:
                    self.opponent_labels[i] = Action.D
        coop_opponents = sum((1 for label in self.opponent_labels[1:] if label == Action.C))
        def_opponents = self.game_description.n_players - 1 - coop_opponents
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if coop_opponents > (self.game_description.n_players - 1) / 2:
            action = Action.D
        elif def_opponents > (self.game_description.n_players - 1) / 2:
            action = Action.C
        else:
            action = Action.D
        if history.my_actions[-1]:
            self.coop_counts[0] += 1
        return action

class Strategy_EXPLOITATIVE_186(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its payoff by balancing cooperation and defection,
    leveraging the game's structure and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.n_players / 2
        self.avg_payoff_AE_last_5_rounds = None
        self.round_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3 or history is None:
            return Action.C
        num_cooperators_last_round = sum(history.opponent_actions[-1, :])
        avg_payoff_last_round = np.mean(np.concatenate((history.my_payoffs[-1:], history.opponent_payoffs[-1, :])))
        if num_cooperators_last_round >= self.threshold:
            return Action.D
        elif avg_payoff_last_round <= self.game_description.k:
            return Action.C
        else:
            return Action.D
        self.round_count += 1
        if self.round_count % 5 == 0 and history is not None:
            if self.avg_payoff_AE_last_5_rounds is None:
                self.avg_payoff_AE_last_5_rounds = np.mean(history.my_payoffs[-5:])
            avg_payoff_all_players_last_5_rounds = np.mean(np.concatenate((history.my_payoffs[-5:], history.opponent_payoffs[-5, :])))
            if self.avg_payoff_AE_last_5_rounds > avg_payoff_all_players_last_5_rounds:
                self.threshold -= 0.1 * self.game_description.n_players / 2
            elif self.avg_payoff_AE_last_5_rounds <= avg_payoff_all_players_last_5_rounds:
                self.threshold += 0.1 * self.game_description.n_players / 2
            self.avg_payoff_AE_last_5_rounds = np.mean(history.my_payoffs[-5:])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_188(BaseStrategy):
    """
    This strategy is designed to maximize individual payoff by exploiting the opponent's behavior while adapting to changing conditions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.acr = 0.0
        self.c_t = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        if state.round_number == 0 or state.round_number <= n_players / 2:
            return Action.C
        elif state.round_number <= 3 * n_players // 4:
            self.c_t.append(sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C))
            self.acr = sum(self.c_t) / (len(self.c_t) * n_players)
            return Action.C
        else:
            previous_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
            if self.acr >= 0.5 and previous_cooperators >= n_players * self.acr:
                return Action.C
            elif self.acr < 0.5 and previous_cooperators >= n_players / 2:
                return Action.D
        if state.round_number == n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_189(BaseStrategy):
    """
    Adaptive Exploiter Strategy.
    
    This strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_rate = 0.5
        self.w = 3
        self.alpha = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        opponent_coop_rate = np.mean(opponent_actions)
        self.coop_rate = (1 - self.alpha) * self.coop_rate + self.alpha * opponent_coop_rate
        theta = self.game_description.k / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif self.coop_rate > theta:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_190(BaseStrategy):
    """
    A robust and exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.round_number = 0
        self.coop_probability = 0.5
        self.p_punish = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.round_number = state.round_number
        if state.round_number == 0 or state.round_number < 3:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        cooperators = sum(opponent_actions)
        total_payoff_cooperators = sum([history.my_payoffs[-1] if action else 0 for action in opponent_actions])
        avg_payoff_per_cooperator = total_payoff_cooperators / max(cooperators, 1)
        defectors = self.game_description.n_players - cooperators
        total_payoff_defectors = sum(history.opponent_payoffs[-1]) - total_payoff_cooperators
        avg_payoff_per_defector = total_payoff_defectors / max(defectors, 1)
        if history.my_actions[-1] and (not all(history.opponent_actions[-1, :])):
            if random.random() < self.p_punish:
                return Action.D
        if avg_payoff_per_cooperator > avg_payoff_per_defector:
            if not history.my_actions[-1] and any(history.opponent_actions[-1, :]):
                self.coop_probability = min(1, self.coop_probability + 0.1)
        else:
            self.coop_probability = max(0, self.coop_probability - 0.01)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < self.coop_probability else Action.D

class Strategy_EXPLOITATIVE_191(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' cooperation levels.
    
    Cooperates initially, then defects or cooperates based on observed opponent behavior.
    In late rounds, always defects to maximize personal gain.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_payoff = 0.0
        self.defect_payoff = 0.0
        self.avg_cooperation_rate = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        self.avg_cooperation_rate = opponent_cooperations / self.game_description.n_players
        if 2 <= state.round_number < self.game_description.n_rounds // 2:
            if self.avg_cooperation_rate > 0.5:
                return Action.C
            else:
                return Action.D
        elif self.game_description.n_rounds // 2 <= state.round_number < 3 * self.game_description.n_rounds // 4:
            cooperate_payoff = 0.0 if history.my_actions[-1] else self.avg_cooperation_rate * self.game_description.k
            defect_payoff = 1.0 + self.avg_cooperation_rate * self.game_description.k / self.game_description.n_players
            self.cooperate_payoff += cooperate_payoff
            self.defect_payoff += defect_payoff
            if self.cooperate_payoff > self.defect_payoff:
                return Action.C
            else:
                return Action.D
        elif 3 * self.game_description.n_rounds // 4 <= state.round_number <= self.game_description.n_rounds:
            return Action.D
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        elif all(history.opponent_actions[-1, :]):
            return Action.C
        elif sum(history.opponent_actions[-1, :]) < self.game_description.n_players // 2:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_192(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Cooperate- Punish" (ACP)
    
    This strategy aims to maximize its payoff by balancing cooperation and defection, 
    while punishing opponents for non-cooperative behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = 0.5 + game_description.k / game_description.n_players / 2
        self.punish_rounds = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            if self.game_description.k / self.game_description.n_players <= 0.5:
                return Action.D
            else:
                return random.choice([Action.C, Action.D])
        opponent_cooperations = history.opponent_actions[-1, :]
        avg_coop = sum(opponent_cooperations) / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif avg_coop >= self.coop_threshold:
            return Action.C
        elif self.punish_rounds > 0:
            self.punish_rounds -= 1
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_194(BaseStrategy):
    """
    An exploitative strategy that adapts to changing circumstances throughout the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponents_defected = [False] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number <= 3:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        majority_cooperated = sum(opponent_actions) > self.game_description.n_players / 2
        p = self.game_description.k / self.game_description.n_players
        for i, opp in enumerate(opponent_actions):
            if opp == False and history.my_actions[-1] == True:
                self.opponents_defected[i] = True
        if majority_cooperated:
            return Action.D
        else:
            random_value = random.random()
            if random_value < p:
                return Action.C
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_195(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to exploit opponents' cooperative behavior 
    while minimizing its own contributions. By cooperating initially and adapting 
    to the cooperation ratio, AE gathers information about opponents' behaviors 
    and adjusts its actions accordingly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :])
        rho = prev_round_coops / (self.game_description.n_players - 1)
        tau = 1 - self.game_description.k / self.game_description.n_players if self.game_description.k / self.game_description.n_players > 0.5 else 0.5
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if rho >= tau or math.isclose(rho, tau):
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_196(BaseStrategy):
    """
    A strategy that adapts to the game's history and parameters by exploiting the behavior of opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_advantage_counter = 0
        self.threshold = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperators = history.opponent_actions[-1, :] == True
        opponent_defectors = ~opponent_cooperators
        n_cooperators = np.sum(opponent_cooperators)
        n_defectors = self.game_description.n_players - 1 - n_cooperators
        avg_cooperator_payoff = history.my_payoffs[-1] if history.my_actions[-1] else self.game_description.k / self.game_description.n_players * n_cooperators + 1
        avg_defector_payoff = self.game_description.k / self.game_description.n_players * n_cooperators if n_defectors > 0 else 0
        if avg_cooperator_payoff > avg_defector_payoff:
            action = Action.C
            self.cooperation_advantage_counter += 1
        elif avg_cooperator_payoff == avg_defector_payoff:
            action = Action.D
        else:
            action = Action.D
            self.cooperation_advantage_counter -= 1
        if self.cooperation_advantage_counter >= self.threshold:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_197(BaseStrategy):
    """
    Adaptive Exploiter (AE) strategy for N-Player Public Goods Game.
    Aims to maximize total payoff by adapting to opponents' behavior and exploiting their cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.previous_round_payoff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        total_cooperators = self.count_cooperators(history)
        if total_cooperators <= self.game_description.n_players / 2:
            return Action.D
        p = self.game_description.k / self.game_description.n_players * (total_cooperators / self.game_description.n_players)
        if random.random() < p:
            return Action.C
        return Action.D

    def count_cooperators(self, history: PlayerHistory):
        opponent_payoffs = history.opponent_payoffs[-1, :]
        total_cooperators = sum((1 for payoff in opponent_payoffs if payoff < 1 + self.game_description.k / self.game_description.n_players * self.game_description.n_players / 2))
        return total_cooperators + history.my_actions[-1]

class Strategy_EXPLOITATIVE_198(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to maximize its own payoff by adaptively responding 
    to the behavior of other players in the game. It cooperates with a probability p = 0.5 in the 
    initial round, then adjusts its cooperation threshold based on the average payoff and number of 
    cooperators in the previous rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < self.cooperation_prob else Action.D
        cooperators_prev = sum(history.opponent_actions[-1, :])
        avg_payoff_prev = np.mean(history.my_payoffs)
        theta = (avg_payoff_prev >= 1) * self.game_description.k / self.game_description.n_players * cooperators_prev
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if cooperators_prev >= theta:
            return Action.C
        elif cooperators_prev == theta:
            return random.choice([Action.C, Action.D])
        else:
            return Action.D

class Strategy_EXPLOITATIVE_199(BaseStrategy):
    """
    An exploitative strategy that adapts to the opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.c_prev = None
        self.defect_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D]) if random.random() < 0.5 else Action.C
        self.c_prev = sum(history.opponent_actions[-1, :])
        if self.c_prev >= self.game_description.n_players / 2:
            action = Action.D
        else:
            coop_prob = self.game_description.k / self.game_description.n_players * self.c_prev
            action = Action.C if random.random() < coop_prob else Action.D
        if state.round_number % 5 == 0 and state.round_number > 4:
            avg_cooperators = np.mean(history.opponent_actions[-5:, :].sum(axis=1))
            if avg_cooperators > self.game_description.n_players / 2:
                self.defect_prob += 0.1
            else:
                self.defect_prob -= 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_200(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to maximize payoffs by exploiting 
    the behaviors of opponents while adapting to the game's history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.c_avg = 0.5
        self.tau = (game_description.k + 1) / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_payoff = np.mean(history.my_payoffs)
        if state.round_number >= self.game_description.n_rounds - 1:
            return Action.D
        opponent_coops = history.opponent_actions[:-1].sum(axis=0) / state.round_number
        self.c_avg = np.mean(opponent_coops)
        if avg_payoff < self.tau:
            return Action.D
        else:
            p = self.c_avg if self.c_avg >= 0.5 else 0.5 - self.game_description.k / self.game_description.n_players * (1 - self.c_avg)
            return Action.C if random.random() < p else Action.D

class Strategy_EXPLOITATIVE_201(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its own payoff by adaptively responding 
    to the opponent's behavior while exploiting their cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_rate_history = [0] * game_description.n_players
        self.avg_coop_rate = 0
        self.window_size = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_cooperations = np.sum(history.opponent_actions[-1, :])
        self.coop_rate_history.append(opponent_cooperations / (self.game_description.n_players - 1))
        self.avg_coop_rate = np.mean(self.coop_rate_history[-self.window_size:])
        theta = self.game_description.k / self.game_description.n_players * (self.game_description.n_players / 2)
        if self.avg_coop_rate > theta or state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if all(history.opponent_actions[-1, :]) == False:
            return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_202(BaseStrategy):
    """
    Adaptive Exploiter strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_actions = history.opponent_actions[-1, :]
        avg_coop_rate = np.mean(opponent_actions)
        if avg_coop_rate > self.threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_203(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behaviors and adjusts its cooperation threshold.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tau = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1 or state.round_number == 2:
            return Action.C
        opponents_defected = sum((not action for action in history.opponent_actions[-1, :]))
        opponents_cooperated = self.game_description.n_players - 1 - opponents_defected
        if opponents_defected > self.game_description.n_players / 2:
            return Action.D
        else:
            total_payoff_C = self.game_description.k / self.game_description.n_players * opponents_cooperated + (1 - 1)
            total_payoff_D = 1 + self.game_description.k / self.game_description.n_players * opponents_cooperated
            if total_payoff_C > total_payoff_D and self.tau < opponents_cooperated / (self.game_description.n_players - 1):
                action = Action.C
            else:
                action = Action.D
            if opponents_cooperated > (self.game_description.n_players - 1) * self.tau:
                self.tau -= 0.1
            elif opponents_cooperated < (self.game_description.n_players - 1) * self.tau:
                self.tau += 0.1
            return action
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_EXPLOITATIVE_204(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors 
    while being robust and adaptive to changing environments.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_profiles = {}
        self.cooperation_rate = 0.5
        self.exploitation_rates = {'Cooperative': 1.0, 'Defector': 0.3, 'Unpredictable': 0.5}
        self.adjustment_thresholds = {'Cooperative': -10, 'Defector': 10}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round = state.round_number - 1
        my_prev_action = history.my_actions[prev_round]
        opponent_prev_actions = history.opponent_actions[prev_round, :]
        for i in range(self.game_description.n_players):
            if i == self.game_description.n_players - 1:
                continue
            opponent_cooperation_rate = np.mean(history.opponent_actions[:, i])
            opponent_type = 'Cooperative' if opponent_cooperation_rate >= 0.6 else 'Defector' if opponent_cooperation_rate <= 0.4 else 'Unpredictable'
            self.opponent_profiles[i] = {'type': opponent_type, 'cooperation_rate': opponent_cooperation_rate}
        opponent_types = [self.opponent_profiles[i]['type'] for i in range(self.game_description.n_players - 1)]
        if state.round_number <= 3:
            return Action.D if state.round_number > 1 else Action.C
        action = None
        for opponent_type in set(opponent_types):
            exploitation_rate = self.exploitation_rates[opponent_type]
            if opponent_type == 'Unpredictable':
                if state.round_number < self.game_description.n_rounds // 2:
                    action = Action.C if random.random() < 0.5 else Action.D
                else:
                    action = Action.D if random.random() < 0.5 else Action.C
            elif opponent_type == 'Cooperative':
                action = Action.D
            elif opponent_type == 'Defector':
                action = Action.C if random.random() < 0.3 else Action.D
        return action if action is not None else Action.D

class Strategy_EXPLOITATIVE_205(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to a wide range of opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        previous_round_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        previous_payoff = history.my_payoffs[-1]
        if previous_round_cooperators > self.n_players / 2 and previous_payoff <= self.k / self.n_players * previous_round_cooperators:
            return Action.C
        elif history.opponent_actions.shape[0] > 1 and (previous_round_cooperators <= self.n_players / 2 or previous_payoff < 1):
            return Action.D
        else:
            expected_coop_payoff = self.k / self.n_players * previous_round_cooperators + 1
            expected_defect_payoff = 1
            if expected_coop_payoff > expected_defect_payoff:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_206(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Public Goods Twist Strategy.
    
    This strategy cooperates in the first round, then uses a combination of Tit-for-Tat logic and public goods twist to determine its actions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.history = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.history = history
        opponent_actions_prev_round = self.history.opponent_actions[-1, :]
        total_coop_prev = sum(opponent_actions_prev_round)
        avg_payoff_coop_prev = np.mean(self.history.opponent_payoffs[-1, :][opponent_actions_prev_round])
        avg_payoff_defect_prev = np.mean(self.history.opponent_payoffs[-1, :][~opponent_actions_prev_round])
        if avg_payoff_coop_prev >= avg_payoff_defect_prev:
            return Action.C
        else:
            return Action.D
        if total_coop_prev >= self.game_description.n_players / 2:
            if avg_payoff_coop_prev >= self.game_description.k / self.game_description.n_players * total_coop_prev:
                return Action.C
            else:
                return Action.D
        if sum(~opponent_actions_prev_round) > 0:
            return Action.D

class Strategy_EXPLOITATIVE_207(BaseStrategy):
    """
    A strategy that aims to exploit opponents' cooperative tendencies while minimizing its own losses.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_initial_coop = 0.8
        self.threshold = game_description.n_players / 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_initial_coop else Action.D
        total_coops_prev_round = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if total_coops_prev_round > self.threshold:
            return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            p_coop = 1 - state.round_number / (self.game_description.n_rounds - 2)
            return Action.C if random.random() < p_coop else Action.D

class Strategy_EXPLOITATIVE_208(BaseStrategy):
    """
    Adaptive Cooperate-to-Defect (ACD) strategy for N-player Public Goods Game.

    This strategy balances cooperation and defection by adapting to opponents' behaviors,
    while being robust to a wide range of strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.C_count = 0
        self.D_count = 0
        self.CT = 0
        self.prev_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        self.C_count = sum(history.opponent_actions[-1, :] == True)
        self.D_count = self.game_description.n_players - self.C_count
        if self.C_count + self.D_count == 0:
            return Action.D
        self.CT = self.game_description.k / self.game_description.n_players * self.C_count / (self.C_count + self.D_count)
        if state.round_number < self.game_description.n_rounds - 1:
            if self.CT > 0.5 and self.prev_payoff >= np.mean(history.opponent_payoffs[-1, :]):
                return Action.C
            elif self.C_count / self.game_description.n_players > 0.3:
                return Action.C
        return Action.D

    def update(self, history: PlayerHistory):
        if history is not None:
            self.prev_payoff = np.sum(history.my_payoffs)

class Strategy_EXPLOITATIVE_209(BaseStrategy):
    """
    An exploitative strategy designed to adapt to changing conditions and exploit opponents' cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rates = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            if state.round_number == 0:
                return Action.D
            else:
                cooperators = sum(history.opponent_actions[-1, :])
                if cooperators > 0:
                    return Action.C
                return Action.D
        if self.cooperation_rates is None:
            self.cooperation_rates = np.zeros(self.game_description.n_players - 1)
        for i, actions in enumerate(history.opponent_actions[:-1].T):
            cooperations = sum(actions)
            rounds_played = len(actions)
            self.cooperation_rates[i] = cooperations / rounds_played
        avg_payoffs_opponents = np.mean(history.opponent_payoffs, axis=0)
        avg_payoff_self = np.mean(history.my_payoffs)
        cooperators = []
        for i, cr in enumerate(self.cooperation_rates):
            if cr > 0.6 and avg_payoffs_opponents[i] > avg_payoff_self:
                cooperators.append(i)
        prev_round_cooperators = sum(history.opponent_actions[-2, :])
        curr_round_cooperators = sum(history.opponent_actions[-1, :])
        increasing_cooperators = curr_round_cooperators > prev_round_cooperators
        if len(cooperators) > 0 or increasing_cooperators:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_210(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to changing game dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.w = 5
        self.coop_threshold = 0.5
        self.total_coop_payoff_diff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number == 1 and history is None):
            return Action.C
        prev_round_coop_payoff_avg = np.mean(history.opponent_payoffs[-1, history.opponent_actions[-1, :] == True])
        prev_round_defect_payoff_avg = np.mean(history.opponent_payoffs[-1, history.opponent_actions[-1, :] == False])
        if len(history.my_payoffs) >= self.w:
            payoff_diff = np.sum(history.my_payoffs[-self.w:]) - self.total_coop_payoff_diff
            self.total_coop_payoff_diff += prev_round_coop_payoff_avg * self.game_description.n_players - history.my_payoffs[-1]
            if payoff_diff > 0:
                self.coop_threshold += 0.1
        if prev_round_coop_payoff_avg >= prev_round_defect_payoff_avg and random.random() < self.coop_threshold:
            action = Action.C
        else:
            action = Action.D
        if self.game_description.n_players == 2:
            prev_opponent_action = history.opponent_actions[-1, 0]
            if prev_opponent_action:
                action = Action.C
            else:
                action = Action.D
        return action

class Strategy_EXPLOITATIVE_211(BaseStrategy):
    """
    The Adaptive Exploiter strategy maximizes payoffs by exploiting opponents' behaviors while adapting to changing game conditions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_prob = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < self.cooperate_prob else Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :])
        num_defectors = self.game_description.n_players - 1 - prev_cooperators
        if prev_cooperators <= self.game_description.n_players // 2:
            return Action.D
        elif num_defectors > 0 and prev_cooperators >= self.game_description.n_players // 2:
            return random.choice([Action.C, Action.D]) if random.random() < 1 - num_defectors / (self.game_description.n_players - 1) else Action.D
        else:
            cooperate_prob = np.mean(history.opponent_actions[-1, :])
            return random.choice([Action.C, Action.D]) if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_212(BaseStrategy):
    """
    The Adaptive Exploiter strategy adapts to opponents' behaviors while maximizing its own payoff.
    It balances cooperation and exploitation, reacting to opponents' cooperation and punishing their defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploit_threshold = 0.6
        self.react_threshold = 0.5
        self.punish_threshold = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 4:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        opponent_cooperation_rates = np.mean(history.opponent_actions, axis=0)
        opponent_last_actions = history.opponent_actions[-1, :]
        avg_opponent_cooperation_rate = np.mean(opponent_cooperation_rates)
        if avg_opponent_cooperation_rate > self.exploit_threshold:
            return Action.D
        for i in range(self.game_description.n_players - 1):
            if opponent_cooperation_rates[i] > self.react_threshold and opponent_last_actions[i]:
                return Action.C
        for i in range(self.game_description.n_players - 1):
            if 1 - opponent_cooperation_rates[i] > self.punish_threshold and (not opponent_last_actions[i]):
                return Action.D
        if state.round_number % 2 == 0:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_214(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behaviors by initially exploring their actions,
    classifying them as Cooperative or Defective, and then exploiting their weaknesses while trying
    to induce cooperation from Defective opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_prob = 0.5
        self.opp_classifications = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < 3:
            return Action.C if random.random() < self.coop_prob else Action.D
        if state.round_number == 3 and history is not None:
            for i in range(self.game_description.n_players - 1):
                coop_rate = sum(history.opponent_actions[:3, i]) / 3
                if coop_rate >= 2 / 3:
                    self.opp_classifications[i] = 'Cooperative'
                else:
                    self.opp_classifications[i] = 'Defective'
        if history is not None:
            for i in range(self.game_description.n_players - 1):
                coop_rate = sum(history.opponent_actions[-3:, i]) / 3
                if coop_rate >= 2 / 3 and self.opp_classifications.get(i, 'Defective') != 'Cooperative':
                    self.opp_classifications[i] = 'Cooperative'
                elif coop_rate < 2 / 3 and self.opp_classifications.get(i, 'Cooperative') != 'Defective':
                    self.opp_classifications[i] = 'Defective'
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        if all((value == 'Defective' for value in self.opp_classifications.values())):
            return Action.C
        return Action.D if any((value == 'Cooperative' for value in self.opp_classifications.values())) else Action.C

class Strategy_EXPLOITATIVE_215(BaseStrategy):
    """
    The Adaptive Predator strategy aims to maximize its payoff by exploiting the cooperative behavior of other players while minimizing its own contributions to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperators = []
        self.defectors = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.D
        if len(self.cooperators) == 0 or len(self.defectors) == 0:
            self._update_cooperator_defector_lists(history)
        if len(self.cooperators) >= self.game_description.n_players // 2:
            return Action.C
        for i in range(len(history.opponent_actions[-1])):
            if history.opponent_actions[-1, i] == False and i not in self.defectors:
                self.defectors.append(i)
        if state.round_number % 3 == 0:
            self._update_cooperator_defector_lists(history)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if len(self.cooperators) == 1:
            return Action.D
        if all([action == False for action in history.opponent_actions[-1]]):
            return Action.C
        return Action.D

    def _update_cooperator_defector_lists(self, history: PlayerHistory):
        self.cooperators = []
        self.defectors = []
        for i in range(len(history.opponent_actions[-1])):
            if history.opponent_actions[-1, i] == True:
                self.cooperators.append(i)
            else:
                self.defectors.append(i)
        if len(self.cooperators) > 0:
            for i in range(len(history.opponent_actions[-1])):
                if history.opponent_actions[-1, i] == False and i in self.cooperators:
                    self.defectors.append(i)
                    self.cooperators.remove(i)

class Strategy_EXPLOITATIVE_216(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to take advantage of other players' cooperative tendencies 
    while minimizing our own contributions. By cooperating only when it is beneficial and there are enough 
    other cooperators, we maximize our payoffs while avoiding exploitation by defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = math.ceil(game_description.n_players / 3)
        self.cooperators_last_5_rounds = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        opponent_actions = history.opponent_actions[-1, :]
        num_cooperators = sum(opponent_actions)
        self.cooperators_last_5_rounds.append(num_cooperators)
        if len(self.cooperators_last_5_rounds) > 5:
            self.cooperators_last_5_rounds.pop(0)
        my_payoff = history.my_payoffs[-1]
        opponent_payoffs = history.opponent_payoffs[-1, :]
        coop_payoffs = [payoff for action, payoff in zip(opponent_actions, opponent_payoffs) if action]
        def_payoffs = [payoff for action, payoff in zip(opponent_actions, opponent_payoffs) if not action]
        avg_coop_payoff = np.mean(coop_payoffs) if coop_payoffs else 0
        avg_def_payoff = np.mean(def_payoffs) if def_payoffs else 0
        if avg_coop_payoff > avg_def_payoff and num_cooperators >= self.threshold:
            return Action.C
        else:
            return Action.D
        if state.round_number % 5 == 0 and state.round_number != 0:
            avg_coop_last_5_rounds = np.mean(self.cooperators_last_5_rounds)
            if avg_coop_last_5_rounds > self.threshold:
                self.threshold -= 1
            elif avg_coop_last_5_rounds < self.threshold:
                self.threshold += 1
        return Action.D

class Strategy_EXPLOITATIVE_217(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        if self.punishment_rounds > 0:
            self.punishment_rounds -= 1
            return Action.D
        total_cooperators = sum(history.opponent_actions[-1, :])
        if total_cooperators <= self.game_description.n_players / 2:
            for opponent_action, my_action in zip(history.opponent_actions[-1, :], history.my_actions[-1:]):
                if not opponent_action and my_action:
                    self.punishment_rounds = 2
                    return Action.D
            return Action.C
        else:
            for opponent_action, my_action in zip(history.opponent_actions[-1, :], history.my_actions[-1:]):
                if not opponent_action and my_action:
                    self.punishment_rounds = 2
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            total_cooperators_last_round = sum(history.opponent_actions[-2, :])
            if total_cooperators_last_round < self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_218(BaseStrategy):
    """
    The Adaptive Exploiter strategy adapts to various opponent behaviors in the N-Player Public Goods Game,
    focusing on maximizing its own payoff by exploiting cooperation opportunities while minimizing vulnerability.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_cooperator_payoffs = []
        self.prev_defector_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C if state.round_number == 0 else Action.D
        prev_actions = history.opponent_actions[-1, :]
        my_prev_action = history.my_actions[-1]
        opponent_payoffs = history.opponent_payoffs[-1, :]
        my_prev_payoff = history.my_payoffs[-1]
        if my_prev_action:
            self.prev_cooperator_payoffs.append(my_prev_payoff)
            for i, (action, payoff) in enumerate(zip(prev_actions, opponent_payoffs)):
                if action:
                    self.prev_cooperator_payoffs.append(payoff)
                else:
                    self.prev_defector_payoffs.append(payoff)
        else:
            self.prev_defector_payoffs.append(my_prev_payoff)
            for i, (action, payoff) in enumerate(zip(prev_actions, opponent_payoffs)):
                if action:
                    self.prev_cooperator_payoffs.append(payoff)
                else:
                    self.prev_defector_payoffs.append(payoff)
        avg_coop_payoff = np.mean(self.prev_cooperator_payoffs) if self.prev_cooperator_payoffs else 0.0
        avg_defect_payoff = np.mean(self.prev_defector_payoffs) if self.prev_defector_payoffs else 0.0
        if avg_coop_payoff > avg_defect_payoff:
            return Action.C
        elif avg_coop_payoff < avg_defect_payoff:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_219(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to maximize its own payoff by exploiting 
    the cooperative tendencies of other players while minimizing its own contributions 
    to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.avg_payoff_window = []
        self.previous_avg_payoff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        if state.round_number <= 3:
            if state.round_number == 1:
                self.avg_payoff_window = []
                self.previous_avg_payoff = None
                return Action.C if random.random() < 0.5 else Action.D
            elif state.round_number == 2:
                pass
            else:
                fewer_than_half_cooperated = sum(history.opponent_actions[-1, :]) < self.game_description.n_players / 2
                return Action.D if fewer_than_half_cooperated else Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :])
        if prev_round_cooperators >= self.cooperation_threshold * self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        if len(self.avg_payoff_window) < 3:
            self.avg_payoff_window.append(np.mean(history.my_payoffs[-3:]))
        else:
            self.avg_payoff_window.pop(0)
            self.avg_payoff_window.append(np.mean(history.my_payoffs[-3:]))
        avg_payoff_window = np.mean(self.avg_payoff_window)
        if self.previous_avg_payoff is not None and avg_payoff_window > self.previous_avg_payoff * 1.1:
            self.cooperation_threshold += 0.1
        elif self.previous_avg_payoff is not None and avg_payoff_window < self.previous_avg_payoff * 0.9:
            self.cooperation_threshold -= 0.1
        self.previous_avg_payoff = avg_payoff_window
        return action

class Strategy_EXPLOITATIVE_220(BaseStrategy):
    """
    Exploitative strategy that adapts to game environment and opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        n_players = self.game_description.n_players
        k = self.game_description.k
        t = state.round_number
        T_t = k / n_players * (1 - (1 / k) ** t)
        C_prev = sum(history.opponent_actions[-1, :]) + 1 if history.my_actions[-1] else sum(history.opponent_actions[-1, :])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if C_prev >= T_t * n_players:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_221(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploitation_threshold_met = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number < 5:
            avg_coop_rate = np.mean(history.opponent_actions[-1, :])
            if avg_coop_rate > 0.5:
                return Action.C
            else:
                return Action.D
        avg_payoff_prev = np.mean(history.opponent_payoffs[-1, :])
        payoff_diff = avg_payoff_prev - history.my_payoffs[-1]
        if payoff_diff > 0.5:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_coop_rates = np.mean(history.opponent_actions[-3:, :], axis=0)
        if max(opponent_coop_rates) > 0.75 and (not self.exploitation_threshold_met):
            self.exploitation_threshold_met = True
            return random.choice([Action.C, Action.D], p=[0.2, 0.8])
        avg_coop_rate = np.mean(history.opponent_actions[-1, :])
        if avg_coop_rate < 0.25:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_222(BaseStrategy):
    """
    The Adaptive Predator strategy aims to exploit opponents' behaviors by adapting to their cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        my_previous_action = bool(history.my_actions[-1])
        avg_coop_prev = (my_previous_action + opponent_cooperators) / self.game_description.n_players
        if avg_coop_prev >= self.coop_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_223(BaseStrategy):
    """
    The Adaptive Free Rider strategy balances exploration, exploitation, and punishment to effectively navigate a wide range of opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_behavior = {}
        self.acr_window = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number <= 2:
            return Action.D
        if len(self.acr_window) >= 3:
            self.acr_window.pop(0)
        if history is not None:
            acr = sum(history.my_actions[-3:]) / 3 if len(history.my_actions) >= 3 else 1
            self.acr_window.append(acr)
        for opponent, actions in enumerate(history.opponent_actions.T):
            if opponent not in self.opponent_behavior:
                self.opponent_behavior[opponent] = []
            self.opponent_behavior[opponent].append(actions[-1])
        avg_acr = np.mean(self.acr_window) if self.acr_window else 0
        opponents_to_punish = set()
        for opponent, actions in self.opponent_behavior.items():
            if len(actions) >= 2 and (not any(actions[-2:])):
                opponents_to_punish.add(opponent)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if avg_acr > 0.5 or (avg_acr == 0.5 and random.random() < 0.5):
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_224(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its own payoff by adapting to 
    the behavior of other players while exploiting their cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :]) + int(self.my_action == Action.C)
        if prev_cooperators >= self.game_description.n_players // 2:
            action = Action.C
        else:
            action = Action.D
        if history is not None and state.round_number > 1:
            my_prev_payoff = history.my_payoffs[-1]
            opponents_prev_payoffs = history.opponent_payoffs[-1, :]
            avg_opponents_prev_payoff = np.mean(opponents_prev_payoffs)
            if my_prev_payoff < avg_opponents_prev_payoff:
                action = Action.C if self.my_action == Action.D else Action.D
        self.my_action = action
        return action

class Strategy_EXPLOITATIVE_225(BaseStrategy):
    """
    Adaptive Free-Rider strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        prev_round_coop_rate = np.mean(history.opponent_actions[-1, :])
        self.theta = max(self.theta, prev_round_coop_rate)
        p = 1 - self.game_description.k / self.game_description.n_players * self.theta
        if random.random() < p:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_226(BaseStrategy):
    """
    A strategy that adapts to opponents' behaviors and exploits their tendencies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_ratios = [0.0] * game_description.n_players
        self.previous_actions = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.cooperation_ratios = [np.mean(opponent_actions[:state.round_number]) for opponent_actions in history.opponent_actions.T]
        threshold = 0.5
        if any((cooperation_ratio >= threshold for cooperation_ratio in self.cooperation_ratios)):
            action = Action.C
        else:
            action = Action.D
        if history.my_actions[-1] and (not history.opponent_actions[-1, 0]):
            action = Action.D
            self.previous_actions.append(Action.D)
        else:
            self.previous_actions.append(action)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_227(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behaviors by initially cooperating, 
    identifying opportunities to exploit others' cooperation, and implementing a mild punishment mechanism.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperators_count = []
        self.T = None
        self.punishment_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-2, :] == True)
        avg_cooperators = np.mean(self.cooperators_count) if self.cooperators_count else 0
        self.T = avg_cooperators
        if len(self.cooperators_count) >= 3 and self.cooperators_count[-1] - self.cooperators_count[-2] >= 2:
            return Action.D
        elif prev_cooperators > self.T:
            return Action.C
        else:
            return Action.D
        if history.my_actions[-1] == True and np.any(history.opponent_actions[-1, :] == False):
            self.punishment_mode = True
        elif state.round_number - len(self.cooperators_count) >= 2:
            self.punishment_mode = False
        if self.punishment_mode and random.random() < 0.5:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        self.cooperators_count.append(sum(history.opponent_actions[-1, :] == True))
        return Action.C

class Strategy_EXPLOITATIVE_228(BaseStrategy):
    """
    A strategy that adapts to the average cooperation rate and current payoff to maximize overall payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def calculate_average_cooperation_rate(self, history: PlayerHistory) -> float:
        total_coop_count = 0
        for t_prime in range(history.my_actions.size):
            coop_count = sum(history.opponent_actions[t_prime, :]) + (history.my_actions[t_prime] == Action.C)
            total_coop_count += coop_count
        avg_coop_rate = total_coop_count / (self.game_description.n_players * history.my_actions.size)
        return avg_coop_rate

    def calculate_payoff_ratio(self, history: PlayerHistory) -> float:
        max_possible_payoff = self.game_description.k * self.game_description.n_players
        current_payoff = sum(history.my_payoffs)
        payoff_ratio = current_payoff / max_possible_payoff if max_possible_payoff != 0 else 1.0
        return payoff_ratio

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        avg_coop_rate = self.calculate_average_cooperation_rate(history)
        if avg_coop_rate >= 0.5 and state.round_number < self.game_description.n_rounds - 1:
            return Action.D
        else:
            payoff_ratio = self.calculate_payoff_ratio(history)
            cooperate_prob = payoff_ratio
            return Action.C if random.random() <= cooperate_prob else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C

class Strategy_EXPLOITATIVE_229(BaseStrategy):
    """
    Adaptive Exploiter strategy for N-Player Public Goods Game.

    This strategy balances adaptability and exploitation by cooperating when the average cooperation rate is high enough,
    and defecting otherwise.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_coop_rate = np.mean(history.opponent_actions[:-1, :].sum(axis=1)) / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if avg_coop_rate == self.game_description.k / self.game_description.n_players:
            return Action.C
        if avg_coop_rate >= self.game_description.k / self.game_description.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_230(BaseStrategy):
    """
    Exploitative strategy designed to maximize personal payoff by adapting to opponent behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_index = 0.0
        self.threshold = game_description.k / game_description.n_players * (game_description.n_players / 2)
        self.opponent_homogeneity_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        self.cooperation_index += 1.0 / self.game_description.n_players * opponent_cooperators - self.game_description.k / self.game_description.n_players
        if self.cooperation_index > 0:
            self.threshold -= 0.05
        elif self.cooperation_index < 0:
            self.threshold += 0.05
        if history.opponent_actions[-1, :].sum() == 0 or history.opponent_actions[-1, :].sum() == self.game_description.n_players - 1:
            self.opponent_homogeneity_count += 1
            if self.opponent_homogeneity_count >= 3:
                return Action.C if history.opponent_actions[-1, :].sum() > 0 else Action.D
        else:
            self.opponent_homogeneity_count = 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if opponent_cooperators <= self.threshold else Action.D

class Strategy_EXPLOITATIVE_231(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors 
    while adapting to the game's history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        if state.round_number <= self.game_description.n_rounds // 2:
            prev_cooperators = sum(history.opponent_actions[-1, :])
            p_t = self.game_description.k / self.game_description.n_players * (prev_cooperators / self.game_description.n_players)
            return Action.C if random.random() < p_t else Action.D
        else:
            prev_cooperators = sum(history.opponent_actions[-1, :])
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.D
            elif prev_cooperators == self.game_description.n_players // 2:
                return Action.C if random.random() < 0.5 else Action.D
            elif prev_cooperators <= self.game_description.n_players / 2:
                return Action.D
            else:
                p_t = prev_cooperators / self.game_description.n_players
                return Action.C if random.random() < p_t else Action.D

class Strategy_EXPLOITATIVE_232(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_cooperation_rate = 0.5
        self.my_payoff = 0
        self.opponent_payoffs = np.zeros(game_description.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C if state.round_number == 0 else self.exploration_phase(history)
        return self.exploitation_phase(history)

    def exploration_phase(self, history: PlayerHistory) -> Action:
        """
        Exploration phase decision.
        """
        previous_round_actions = history.opponent_actions[-1]
        majority_cooperated = sum(previous_round_actions) > len(previous_round_actions) // 2
        return Action.D if majority_cooperated else Action.C

    def exploitation_phase(self, history: PlayerHistory) -> Action:
        """
        Exploitation phase decision.
        """
        previous_rounds = history.my_payoffs.size - 1
        self.opponent_cooperation_rate = sum(history.opponent_actions[:previous_rounds].flatten()) / (self.game_description.n_players * previous_rounds)
        if self.opponent_cooperation_rate > 0.6:
            return Action.D
        elif self.opponent_cooperation_rate < 0.4:
            return Action.C
        else:
            p = self.game_description.k / self.game_description.n_players * self.opponent_cooperation_rate
            return Action.C if random.random() < p else Action.D

    def endgame_phase(self, history: PlayerHistory) -> Action:
        """
        Endgame phase decision.
        """
        opponent_payoff = sum(history.opponent_payoffs[-1])
        if opponent_payoff > self.my_payoff + 0.1 * self.my_payoff:
            return Action.D
        else:
            return Action.C

    def update_variables(self, history: PlayerHistory, action: Action):
        """
        Update instance variables.
        """
        previous_round_actions = history.opponent_actions[-1]
        payoff = 1 - int(action == Action.C) + self.game_description.k / self.game_description.n_players * sum(previous_round_actions)
        self.my_payoff += payoff
        self.opponent_payoffs += history.opponent_payoffs[-1]

class Strategy_EXPLOITATIVE_233(BaseStrategy):
    """
    Adaptive Defector with Conditional Cooperation strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds

    def calculate_average_cooperation_rate(self, history: PlayerHistory) -> float:
        """
        Calculate average cooperation rate among all players.
        """
        total_cooperations = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        return total_cooperations / self.n

    def calculate_opponent_average_cooperation_rate(self, opponent: int, history: PlayerHistory) -> float:
        """
        Calculate average cooperation rate of a specific opponent.
        """
        opponent_actions = history.opponent_actions[:, opponent]
        return sum(opponent_actions) / len(opponent_actions)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.n:
            return Action.D
        theta = self.k / self.n / (1 + self.k / self.n)
        if self.n <= state.round_number < 2 * self.n:
            avg_coop_rate = self.calculate_average_cooperation_rate(history)
            if avg_coop_rate > theta or math.isclose(avg_coop_rate, theta):
                return Action.C
            else:
                return Action.D
        opponent_avg_coop_rates = []
        for i in range(self.n - 1):
            opponent_avg_coop_rate = self.calculate_opponent_average_cooperation_rate(i, history)
            opponent_avg_coop_rates.append(opponent_avg_coop_rate)
        if all((avg_coop_rate > theta for avg_coop_rate in opponent_avg_coop_rates)):
            return Action.C
        else:
            for i, (avg_coop_rate, last_action) in enumerate(zip(opponent_avg_coop_rates, history.opponent_actions[-1, :])):
                if avg_coop_rate < theta and last_action:
                    new_opponent_avg_coop_rate = self.calculate_opponent_average_cooperation_rate(i, history)
                    if new_opponent_avg_coop_rate > theta:
                        return Action.C
            return Action.D
        if state.round_number == self.r - 1:
            return Action.D

class Strategy_EXPLOITATIVE_234(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game.
    This strategy adapts to the game's history and parameters, 
    cooperating when others cooperate and defecting otherwise.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        opponent_actions = history.opponent_actions[-1]
        num_cooperators = sum((1 for action in opponent_actions if action))
        if num_cooperators >= self.cooperation_threshold * self.game_description.n_players:
            cooperation_action = True
        else:
            cooperation_action = False
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        recent_cooperators = sum((1 for i in range(min(state.round_number, 3)) if history.opponent_actions[-i - 1].mean() > 0.5))
        avg_cooperation_rate = recent_cooperators / min(state.round_number + 1, 4)
        if avg_cooperation_rate > 0.6:
            self.cooperation_threshold += 0.1
        elif avg_cooperation_rate < 0.4:
            self.cooperation_threshold -= 0.1
        return Action.C if cooperation_action else Action.D

class Strategy_EXPLOITATIVE_235(BaseStrategy):
    """
    An adaptive strategy that exploits opponents' behaviors while maximizing its own payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploiters = []
        self.my_payoff_history = []
        self.opponent_payoff_history = [[] for _ in range(game_description.n_players)]
        self.defraud_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        prev_actions = history.opponent_actions[-1, :]
        prev_payoffs = history.opponent_payoffs[-1, :]
        self.exploiters.extend([i for i, (prev_action, prev_payoff) in enumerate(zip(prev_actions, prev_payoffs)) if prev_action == 0])
        self.my_payoff_history.append(history.my_payoffs[-1])
        for i, opponent_payoff in enumerate(prev_payoffs):
            self.opponent_payoff_history[i].append(opponent_payoff)
        num_cooperators_prev_round = sum((1 for action in history.my_actions if action == 1)) + sum((1 for prev_action in prev_actions if prev_action == 1))
        if not self.exploiters:
            return Action.C if random.random() < 0.5 else Action.D
        elif num_cooperators_prev_round <= self.game_description.n_players / 2:
            return Action.D
        else:
            return Action.C
        avg_payoffs = [sum(opponent_payoff) / len(self.opponent_payoff_history[i]) for i, opponent_payoff in enumerate(self.opponent_payoff_history)]
        if any((avg_payoff > sum(self.my_payoff_history) * 1.1 / len(self.my_payoff_history) for avg_payoff in avg_payoffs)):
            self.defraud_prob = min(0.9, self.defraud_prob + 0.1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_236(BaseStrategy):
    """
    Exploitative strategy combining elements of tit-for-tat and public goods punishment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_actions = history.opponent_actions[-1]
        c = sum((1 for action in prev_round_actions if action))
        d = len(prev_round_actions) - c
        threshold = self.game_description.k / self.game_description.n_players * self.game_description.n_players / 2
        prev_round_payoffs = []
        total_contributions = sum((1 for a in prev_round_actions if a))
        for i, action in enumerate(prev_round_actions):
            c_i = int(action)
            payoff = 1 - c_i + self.game_description.k / self.game_description.n_players * total_contributions
            prev_round_payoffs.append(payoff)
        if sum(prev_round_payoffs) < threshold:
            return Action.D
        if c >= d:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_237(BaseStrategy):
    """
    Adaptive Exploitation (AE) strategy for the N-Player Public Goods Game.
    
    This strategy adapts to changing opponent behaviors by alternating between cooperation and defection in the exploration phase,
    then switches to a greedy strategy that maximizes payoffs based on observed opponent behaviors in the exploitation phase.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5
        self.exploration_phase = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.exploration_phase and state.round_number < 5:
            if state.round_number % 2 != 0 or state.round_number == 1:
                action = Action.C
            else:
                action = Action.D
            if state.round_number == 4:
                self.exploration_phase = False
        else:
            opponent_cooperators = sum(history.opponent_actions[-1, :])
            average_payoff_in_last_rounds = np.mean(history.my_payoffs[state.round_number - 1:state.round_number + 1])
            if average_payoff_in_last_rounds < self.game_description.k / self.game_description.n_players * self.game_description.n_players:
                self.threshold -= 0.1
            else:
                self.threshold += 0.1
            if opponent_cooperators > self.game_description.n_players // 2:
                action = Action.D
            elif opponent_cooperators < self.game_description.n_players * self.threshold:
                action = Action.C
            else:
                action = Action.C if self.game_description.k / self.game_description.n_players * opponent_cooperators > average_payoff_in_last_rounds else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_238(BaseStrategy):
    """
    An exploitative strategy designed to maximize its own payoff by exploiting the cooperative tendencies of other players while minimizing its own contribution to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players, k, _ = (getattr(self.game_description, attr_name) for attr_name in ['n_players', 'k', 'n_rounds'])
        if state.round_number <= n_players // 2:
            return random.choice([Action.C, Action.D]) if random.random() < k / n_players else Action.D
        avg_coop_rate = history.my_actions.mean()
        if avg_coop_rate > 0.5:
            return Action.C if random.random() < k / n_players else Action.D
        else:
            return Action.D if random.random() < 1 - k / n_players else Action.C
        last_opponent_actions = history.opponent_actions[-1, :]
        cooperators_last_round = sum(last_opponent_actions)
        majority_cooperated = cooperators_last_round > len(last_opponent_actions) // 2
        if majority_cooperated:
            return Action.C
        else:
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.D
            return Action.D

class Strategy_EXPLOITATIVE_239(BaseStrategy):
    """
    An adaptive exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_coop_rate = np.mean(history.opponent_actions)
        if avg_coop_rate > 0.5:
            return Action.D
        else:
            return Action.C
        avg_payoff = np.mean(np.concatenate((history.my_payoffs, history.opponent_payoffs.flatten())))
        my_avg_payoff = np.mean(history.my_payoffs)
        if my_avg_payoff < avg_payoff:
            return Action.D
        elif avg_coop_rate > 0.3:
            return Action.C
        else:
            return Action.D
        if state.round_number >= self.game_description.n_rounds - 2:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if avg_payoff == my_avg_payoff and avg_coop_rate > 0.3:
            return Action.C
        opponent_history = history.opponent_actions
        for opponent in range(opponent_history.shape[1]):
            coop_streak = 0
            for t in range(1, state.round_number + 1):
                if opponent_history[t - 1, opponent]:
                    coop_streak += 1
                else:
                    coop_streak = 0
                if coop_streak >= 3:
                    return Action.D
        recent_defector = None
        for opponent in range(opponent_history.shape[1]):
            if not opponent_history[-1, opponent]:
                recent_defector = opponent
                break
        if recent_defector is not None:
            return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_240(BaseStrategy):
    """
    Adaptive Exploiter strategy for N-Player Public Goods Game.
    
    This strategy aims to maximize its own payoff by exploiting the cooperation of others while minimizing its own contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperators = []
        self.punish_opponents = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        for opponent in range(self.game_description.n_players - 1):
            if sum(history.opponent_actions[-2:, opponent]) > 0:
                self.cooperators.append(opponent)
        for opponent in range(self.game_description.n_players - 1):
            if history.opponent_actions[-1, opponent] == False and opponent not in self.punish_opponents and (sum(history.opponent_actions[-2:, opponent]) > 0):
                self.punish_opponents.append(opponent)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number < min(self.game_description.n_rounds // 4, 5):
            return Action.D
        if not self.cooperators and len(self.punish_opponents) == 0:
            return Action.D
        p = self.game_description.k / self.game_description.n_players * (len(self.cooperators) / self.game_description.n_players)
        if random.random() < p:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_241(BaseStrategy):
    """
    A K-adaptive exploitative strategy for the public goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.epsilon = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= math.floor(self.game_description.n_rounds / 2):
            p_C = self.game_description.k / self.game_description.n_players
            return Action.C if random.random() < p_C else Action.D
        elif history is not None:
            opponent_cooperation_rates = np.mean(history.opponent_actions, axis=1)
            avg_c = np.sum(opponent_cooperation_rates[:state.round_number]) / (self.game_description.n_players * state.round_number)
            if avg_c > self.game_description.k / self.game_description.n_players:
                p_C = min(1, self.game_description.k / self.game_description.n_players + self.epsilon)
            else:
                p_D = max(0, 1 - self.game_description.k / self.game_description.n_players - self.epsilon)
                p_C = 1 - p_D
            return Action.C if random.random() < p_C else Action.D
        elif state.round_number == self.game_description.n_rounds:
            return Action.D
        elif self.game_description.n_players % 2 == 0 and state.round_number <= math.floor(self.game_description.n_rounds / 2) + 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_242(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperation of others while minimizing its own contributions to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        threshold = self.game_description.n_players * self.game_description.k / (self.game_description.k + 1)
        total_cooperators_prev_round = sum((1 for action in history.opponent_actions[-1, :] if action))
        if total_cooperators_prev_round >= threshold:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_243(BaseStrategy):
    """
    The Adaptive Exploiter strategy adapts to opponents' behavior and exploits their cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.t = 0
        self.cooperate_count_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if self.t <= self.game_description.n_players:
            action = Action.C
        elif self.t <= 2 * self.game_description.n_players:
            action = Action.C if self.t % 2 == 0 else Action.D
        else:
            if history is not None and len(history.opponent_actions) > 0:
                self.cooperate_count_prev = sum(history.opponent_actions[-1, :])
            action = Action.D if self.cooperate_count_prev >= self.game_description.k / 2 else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history is not None and len(history.opponent_actions) > 0:
            for opponent_actions in history.opponent_actions[:, :]:
                if sum(opponent_actions) / len(opponent_actions) < 0.5 and self.t <= 2 * self.game_description.n_players:
                    return Action.D
        self.t += 1
        return action

class Strategy_EXPLOITATIVE_244(BaseStrategy):
    """
    An exploitative strategy that adapts to the opponent's behavior by maintaining a running average of their cooperation rate.
    
    It punishes opponents for defecting while free-riding on their cooperation, and defects in the last round regardless of opponent behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialize the strategy with the game description.

        :param game_description: A PublicGoodsDescription object containing the number of players, rounds, and multiplication factor.
        """
        self.game_description = game_description
        self.avg_coop = 0.5
        self.p_c = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide the action for the current round based on the game state and history.

        :param state: A GameState object containing the current round number.
        :param history: A PlayerHistory object containing the actions and payoffs of all players in previous rounds.
        :return: An Action enum value (C or D) representing the chosen action.
        """
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            opponent_cooperation_rate = np.mean(history.opponent_actions[-min(state.round_number // 2, 10), :])
            self.avg_coop = (self.avg_coop * (state.round_number - 1) + opponent_cooperation_rate) / state.round_number
            if history.opponent_actions[-1, 0] == False and history.my_actions[-1] == True:
                self.p_c += 0.1
            theta = self.game_description.k / self.game_description.n_players
            if self.avg_coop >= theta:
                self.p_c = 1 - self.game_description.n_players / self.game_description.k * (1 - self.avg_coop)
                return Action.C if random.random() < self.p_c else Action.D
            else:
                return Action.D

class Strategy_EXPLOITATIVE_245(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behaviors in the public goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_cooperation_frequencies = [0.5] * game_description.n_players
        self.classification_window = 5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number <= 3 and random.random() < 0.5):
            return Action.C
        opponents_cooperations = np.sum(history.opponent_actions[-self.classification_window:], axis=0)
        self.opponent_cooperation_frequencies = [coop / self.classification_window for coop in opponents_cooperations]
        most_common_type = 'Defector'
        cooperators, defectors, mixed = (0, 0, 0)
        for freq in self.opponent_cooperation_frequencies:
            if freq >= 0.7:
                cooperators += 1
            elif freq <= 0.3:
                defectors += 1
            else:
                mixed += 1
        if cooperators > defectors and cooperators > mixed:
            most_common_type = 'Cooperator'
        elif mixed > cooperators and mixed > defectors:
            most_common_type = 'Mixed'
        if most_common_type == 'Cooperator' or most_common_type == 'Mixed':
            cooperate_prob = 0.8
        else:
            cooperate_prob = 0.1
        if state.round_number > self.classification_window and random.random() < 0.5:
            max_defector_index = np.argmax(self.opponent_cooperation_frequencies)
            if self.opponent_cooperation_frequencies[max_defector_index] < 0.3:
                cooperate_prob = 0
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_246(BaseStrategy):
    """
    This strategy initially gathers information about opponents' behaviors and adapts to maximize collective payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_threshold = game_description.k / 2
        self.exploit_detection_threshold = 2
        self.consecutive_low_payoffs = 0

    def calculate_payoff(self, action, n, k, total_cooperators):
        if action:
            return k / n * total_cooperators
        else:
            return 1 + k / n * total_cooperators

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        total_cooperators = sum(history.opponent_actions[-1, :])
        my_payoff = self.calculate_payoff(True, self.game_description.n_players, self.game_description.k, total_cooperators)
        avg_prev_round_payoff = np.mean(history.my_payoffs) if len(history.my_payoffs) > 0 else 0
        if state.round_number > 1 and my_payoff < self.game_description.k:
            self.consecutive_low_payoffs += 1
            if self.consecutive_low_payoffs >= self.exploit_detection_threshold:
                return Action.D
        else:
            self.consecutive_low_payoffs = 0
        if avg_prev_round_payoff > self.avg_payoff_threshold:
            return Action.C if random.random() < 0.7 else Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_247(BaseStrategy):
    """
    The Adaptive Defector (AD) strategy exploits opponent behaviors while adapting to changing game conditions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = game_description.k / (game_description.n_players * (game_description.k - 1))
        self.beta = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_contributions = sum(history.opponent_actions[-1, :] + [Action.C]) / (self.game_description.n_players + 1)
        if prev_contributions < self.theta:
            action = Action.D
        else:
            action = Action.C
        self.beta = prev_contributions
        if self.beta > self.theta:
            self.theta += 0.05
        elif self.beta < self.theta:
            self.theta -= 0.05
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_248(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the opponent's behavior while adapting to changing conditions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_init = game_description.k / game_description.n_players
        if game_description.n_players == 2:
            self.p_init = min(game_description.k / 2, 0.5)
        self.c_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number <= math.floor(self.game_description.n_rounds / 2):
            cooperate_prob = self.p_init
        else:
            opponent_coop_sum = sum(history.opponent_actions[:-1].flatten())
            c_opponent = opponent_coop_sum / (state.round_number - 1)
            if c_opponent >= self.game_description.k / self.game_description.n_players:
                cooperate_prob = 0
            else:
                cooperate_prob = max(self.game_description.k / self.game_description.n_players, c_opponent)
        if state.round_number == self.game_description.n_rounds - 1:
            pi_coop = self.game_description.k * self.game_description.n_players / self.game_description.n_players
            pi_defect = 1 + self.game_description.k / self.game_description.n_players * opponent_coop_sum
            if pi_coop <= pi_defect:
                cooperate_prob = 0
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_249(BaseStrategy):
    """
    An exploitative strategy for N-Player Public Goods Game that adapts to opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 1.5
        self.history_coop_payoff = []
        self.history_defect_payoff = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        avg_payoff_prev = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1, :])
        num_cooperators_prev = sum(history.opponent_actions[-1, :] == 1) + (history.my_actions[-1] == 1)
        self.update_history(history)
        if state.round_number % 5 == 0:
            self.adjust_threshold()
        if avg_payoff_prev > self.threshold or num_cooperators_prev < self.game_description.n_players / 2:
            return Action.D
        else:
            return Action.C

    def update_history(self, history: PlayerHistory):
        if history.my_actions[-1] == 1:
            self.history_coop_payoff.append(history.my_payoffs[-1])
        else:
            self.history_defect_payoff.append(history.my_payoffs[-1])

    def adjust_threshold(self):
        coop_total = sum(self.history_coop_payoff) if self.history_coop_payoff else 0
        defect_total = sum(self.history_defect_payoff) if self.history_defect_payoff else 0
        if coop_total > defect_total:
            self.threshold += 0.1
        else:
            self.threshold -= 0.1

class Strategy_EXPLOITATIVE_250(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior in the public goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.epsilon = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_payoff = np.mean(history.opponent_payoffs[-1, :])
        if avg_payoff > 1:
            action = Action.D
        else:
            action = Action.C
            for opponent_payoff in history.opponent_payoffs[-1, :]:
                if opponent_payoff > avg_payoff + self.epsilon:
                    action = Action.D
                    break
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_251(BaseStrategy):
    """
    This strategy aims to maximize its total payoff by exploiting the cooperation of others while minimizing its own contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_init = 0.5
        self.punish_rounds = 2
        self.threshold = self.game_description.n_players / 2
        self.punished_opponents = []
        self.round_punishment_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        elif state.round_number >= self.game_description.n_rounds - 1:
            return Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if prev_cooperators >= self.threshold and (not any(self.punished_opponents)):
            action = Action.D
        elif history.my_payoffs.size > 0 and history.opponent_payoffs.size > 0:
            opponent_defected_and_was_punished = False
            for i, (opponent_action, opponent_payoff) in enumerate(zip(history.opponent_actions[-1], history.opponent_payoffs[-1])):
                if opponent_action == Action.D and opponent_payoff > history.my_payoffs[-1]:
                    self.punished_opponents.append(i)
                    opponent_defected_and_was_punished = True
            action = Action.D if opponent_defected_and_was_punished else Action.C
        elif any(self.punished_opponents):
            action = Action.D
        else:
            action = Action.C
        avg_payoff_cooperators = np.mean(history.my_payoffs[history.my_actions == Action.C]) if history.my_payoffs.size > 0 else 0
        avg_payoff_defectors = np.mean(history.my_payoffs[history.my_actions == Action.D]) if history.my_payoffs.size > 0 else 0
        if action == Action.D and (avg_payoff_cooperators == 0 or history.my_payoffs[-1] > avg_payoff_cooperators):
            self.threshold += 0.1
        elif action == Action.C and (avg_payoff_defectors == 0 or history.my_payoffs[-1] < avg_payoff_defectors):
            self.threshold -= 0.1
        if any(self.punished_opponents) and state.round_number - max([history.my_actions.size - np.where(history.opponent_actions[:, opponent] == Action.D)[0][-1] for opponent in self.punished_opponents]) > self.punish_rounds:
            self.punished_opponents = []
        return action

class Strategy_EXPLOITATIVE_252(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to maximize payoffs by adaptively responding to the opponent's behavior while minimizing cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = 0.7
        self.avg_coop_rate = 0
        self.avg_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        opponent_actions = history.opponent_actions
        my_payoffs = history.my_payoffs
        self.avg_coop_rate = (self.avg_coop_rate * (state.round_number - 1) + sum(opponent_actions[-1])) / state.round_number
        if state.round_number < 0.5 * self.game_description.n_rounds:
            opponent_coop_rate = np.mean(opponent_actions[-1])
            if self.avg_coop_rate > self.game_description.n_players / 2:
                return Action.D
            elif opponent_coop_rate > self.coop_threshold:
                return Action.C
            else:
                return Action.D
        expected_defect_payoff = 1 + self.game_description.k / self.game_description.n_players * sum(opponent_actions[-1])
        if state.round_number >= 0.5 * self.game_description.n_rounds and self.avg_payoff < expected_defect_payoff:
            return Action.D
        elif state.round_number > 0.8 * self.game_description.n_rounds:
            opponent_coop_rate = np.mean(opponent_actions[-1])
            if my_payoffs[-1] > self.game_description.k / self.game_description.n_players * self.game_description.n_players:
                return Action.D
            elif opponent_coop_rate > self.coop_threshold:
                return Action.C
        else:
            opponent_coop_rate = np.mean(opponent_actions[-1])
            if opponent_coop_rate > 0 and opponent_coop_rate < 1:
                return random.choices([Action.C, Action.D], weights=[40, 60])[0]
            else:
                self.avg_payoff = np.mean(my_payoffs)
                if state.round_number == self.game_description.n_rounds - 1:
                    return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_253(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior and exploits their cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.n_players / 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        total_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        if total_cooperators_prev_round >= self.cooperation_threshold:
            action = Action.C
        else:
            action = Action.D
        if total_cooperators_prev_round > self.cooperation_threshold:
            self.cooperation_threshold -= 1
        elif total_cooperators_prev_round < self.cooperation_threshold:
            self.cooperation_threshold += 1
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_254(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' actions and adjusts its behavior over time.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        last_round = history.opponent_actions[-1]
        if state.round_number <= self.n_players:
            majority_action = np.argmax(np.bincount(last_round.astype(int)))
            return Action(majority_action)
        avg_cooperation_rate = np.mean(history.opponent_actions[-self.n_players:, :])
        if avg_cooperation_rate > 0.5:
            return Action.D
        else:
            return Action.C
        if state.round_number > self.n_rounds - self.n_players:
            prev_cooperators = np.sum(last_round)
            if prev_cooperators > self.n_players / 2:
                return Action.D
            else:
                return Action.C
        if state.round_number == self.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_255(BaseStrategy):
    """
    The Adaptive Exploitation strategy balances cooperation and defection to maximize its payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tau = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        n = self.game_description.n_players
        k = self.game_description.k
        avg_coop_prev = np.mean(history.opponent_actions[-1, :])
        pi_C = k / n * avg_coop_prev + k / n
        pi_D = 1
        if avg_coop_prev < self.tau:
            p_exploit = 1 - self.tau
            return random.choices([Action.C, Action.D], weights=[p_exploit, 1 - p_exploit])[0]
        elif pi_C >= pi_D:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_256(BaseStrategy):
    """
    An exploitative strategy that adapts to various opponent behaviors and maximizes its own payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_coop_rates = [0.5] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 3 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        avg_opponent_coop = np.mean(history.opponent_actions[:-1, :].sum(axis=1)) / self.game_description.n_players - 1
        if avg_opponent_coop >= 0.5:
            cooperate_prob = self.game_description.k / self.game_description.n_players / (1 + self.game_description.k / self.game_description.n_players)
        else:
            cooperate_prob = 0
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_257(BaseStrategy):
    """
    A strategy that balances cooperation and defection based on the game's history and parameters.
    It aims to exploit opponents' cooperative tendencies while minimizing losses from potential defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.n_players / 2
        self.observation_phase = True
        self.punishment_dict = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.observation_phase and state.round_number < 5:
            self.observation_phase = False
            return Action.D
        avg_cooperators = np.mean(history.opponent_actions[-1, :])
        if avg_cooperators > self.threshold:
            action = Action.C
        else:
            action = Action.D
        for opponent, did_opponent_defect in enumerate(~history.opponent_actions[-1, :]):
            if did_opponent_defect and history.my_actions[-1]:
                self.punishment_dict[opponent] = 2
        for opponent, rounds_left in self.punishment_dict.items():
            if rounds_left > 0:
                action = Action.D
                self.punishment_dict[opponent] -= 1
        if state.round_number % 5 == 0 and state.round_number > 5:
            self.threshold = np.mean(np.sum(history.opponent_actions[-5:, :], axis=1))
        if state.round_number + 1 == self.game_description.n_rounds:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_258(BaseStrategy):
    """
    Exploit the behavior of other players while being robust to various opponent strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number < 3:
            last_round_actions = history.opponent_actions[-1, :]
            my_last_action = history.my_actions[-1]
            majority_cooperate = sum(last_round_actions) + (my_last_action == Action.C)
            if majority_cooperate >= self.game_description.n_players // 2:
                return Action.C
            else:
                return Action.D
        if len(self.cooperation_rate_history) < 3:
            my_actions = history.my_actions[:-1] == Action.C
            opponent_cooperate_rates = (history.opponent_actions[:-1, :] == True).mean(axis=0)
            self.cooperation_rate_history.extend([a * b for a, b in zip(my_actions, opponent_cooperate_rates)])
        else:
            my_last_action = history.my_actions[-1] == Action.C
            last_opponent_cooperate_rates = (history.opponent_actions[-1, :] == True).mean()
            self.cooperation_rate_history.append(my_last_action * last_opponent_cooperate_rates)
            self.cooperation_rate_history.pop(0)
        average_cooperation_rate = np.mean(self.cooperation_rate_history)
        if state.round_number < self.game_description.n_rounds - 1:
            return Action.C if average_cooperation_rate > 0.5 else Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_260(BaseStrategy):
    """
    Adaptive Exploiter strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prob_coop = 0.5
        self.prev_payoff_diff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.prob_coop else Action.D
        my_payoff = history.my_payoffs[-1]
        opponent_payoffs = history.opponent_payoffs[-1, :]
        total_contributions = sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        public_good_share = self.game_description.k / self.game_description.n_players * total_contributions
        coop_payoff = my_payoff - (1 if history.my_actions[-1] else 0) + public_good_share
        opponent_coop_payoff = max(opponent_payoffs)
        simulated_total_contributions = total_contributions - (1 if history.my_actions[-1] else 0) + sum(history.opponent_actions[-1, :])
        simulated_public_good_share = self.game_description.k / self.game_description.n_players * simulated_total_contributions
        defect_payoff = opponent_coop_payoff - (simulated_total_contributions > total_contributions) + simulated_public_good_share
        self.prev_payoff_diff = coop_payoff - defect_payoff
        self.prob_coop += 0.1 if self.prev_payoff_diff > 0 else -0.1 if self.prev_payoff_diff < 0 else 0
        opponent_actions = history.opponent_actions[-1, :]
        for i in range(self.game_description.n_players - 1):
            if opponent_actions[i]:
                return Action.C if random.random() < 0.6 else Action.D
            else:
                return Action.D if random.random() < 0.8 else Action.C
        return Action.C if random.random() < self.prob_coop else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_261(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_prob = 0.5
        self.cooperation_threshold = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperation_prob else Action.D
        opponent_cooperations = np.sum(history.opponent_actions[-1, :])
        opponent_defections = self.game_description.n_players - 1 - opponent_cooperations
        opponent_cooperation_rate = opponent_cooperations / (opponent_cooperations + opponent_defections)
        if opponent_cooperation_rate > self.cooperation_threshold:
            action = Action.C if random.random() < 0.2 else Action.D
        else:
            action = Action.D
        if action == Action.C:
            if opponent_cooperation_rate > 0.7:
                self.cooperation_prob += 0.1
            elif opponent_cooperation_rate < 0.3 and self.cooperation_prob > 0:
                self.cooperation_prob -= 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_262(BaseStrategy):
    """
    Exploitative strategy that adapts to the cooperation rates of opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_types = [None] * game_description.n_players
        self.cooperation_rates = [0.5] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if state.round_number < 3:
            return Action.C if random.random() < 0.5 else Action.D
        if all(self.opponent_types):
            for opponent in range(self.game_description.n_players):
                cooperation_rate = sum(history.opponent_actions[:state.round_number, opponent]) / state.round_number
                self.cooperation_rates[opponent] = cooperation_rate
                if cooperation_rate >= 0.66:
                    self.opponent_types[opponent] = 'C'
                elif cooperation_rate <= 0.33:
                    self.opponent_types[opponent] = 'D'
                else:
                    self.opponent_types[opponent] = 'U'
        cooperators = sum((opponent_type == 'C' for opponent_type in self.opponent_types))
        defectors = sum((opponent_type == 'D' for opponent_type in self.opponent_types))
        unpredictables = sum((opponent_type == 'U' for opponent_type in self.opponent_types))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if cooperators > self.game_description.n_players / 2:
            cooperate_prob = 0.8
        elif defectors > self.game_description.n_players / 2:
            cooperate_prob = 0.2
        else:
            cooperate_prob = 0.6
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_263(BaseStrategy):
    """
    Exploitative Strategy: Adaptive Defection with Self-Adjusting Threshold (ADSAT)

    This strategy adapts to opponents' behavior by initially cooperating, then updating a self-adjusting threshold based on the average cooperation rate.
    It defects when the total contributions are below this threshold and cooperates otherwise.
    The window size for calculating the average cooperation rate is also adjusted over time.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.ACR = 0.5
        self.tau = 0.5
        self.w = 3
        self.epsilon = 0.05
        self.delta = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_contributions = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        opponent_actions = history.opponent_actions[:, :-1] if state.round_number > 0 else np.zeros((self.game_description.n_players - 1, 0), dtype=bool)
        ACR_window = min(self.w, state.round_number)
        total_cooperators_in_window = sum(opponent_actions[-ACR_window:, :].sum(axis=1)) + int(sum(history.my_actions[-ACR_window:]))
        self.ACR = total_cooperators_in_window / (self.game_description.n_players * ACR_window)
        self.tau = max(0, min(1, self.ACR + self.epsilon))
        if total_contributions > self.tau * self.game_description.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_264(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Free-Rider"
    
    This strategy aims to maximize individual payoff by adapting to the group's cooperation level while minimizing personal contribution.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        prev_round = (history.my_payoffs[-1], history.opponent_actions[-1])
        avg_payoff_prev = np.mean(np.append(history.my_payoffs[-1], history.opponent_payoffs[-1]))
        total_cooperation_prev = sum((1 for action in history.opponent_actions[-1] if action)) + history.my_actions[-1]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif avg_payoff_prev > 1 + self.game_description.k / self.game_description.n_players * total_cooperation_prev:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_265(BaseStrategy):
    """
    Exploitative strategy that adapts to the behavior of other players 
    while minimizing cooperation when possible.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.THRESHOLD = game_description.k / game_description.n_players * (game_description.n_players - 1)
        self.adjustment = 0
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :] + [Action.C.value])
        if prev_cooperators > self.THRESHOLD + self.adjustment:
            action = Action.D
        else:
            action = Action.C
            num_consistent_cooperators = sum(np.all(history.opponent_actions[:, :], axis=0)) + 1
            self.adjustment += 1 / self.game_description.n_players * num_consistent_cooperators
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_266(BaseStrategy):
    """
    The Adaptive Predator strategy aims to exploit opponents' behavior by identifying and capitalizing on cooperative tendencies while minimizing losses when facing defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_quotient_threshold = 0.5
        self.payoff_difference_threshold = 0
        self.cooperation_quotient = 0
        self.payoff_difference = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponents_cooperated_last_round = sum(history.opponent_actions[-1, :])
        self.cooperation_quotient = opponents_cooperated_last_round / self.game_description.n_players
        opponent_payoffs = history.opponent_payoffs[-1, :]
        cooperators_payoffs = opponent_payoffs[history.opponent_actions[-1, :] == 1]
        defectors_payoffs = opponent_payoffs[history.opponent_actions[-1, :] == 0]
        if len(cooperators_payoffs) > 0 and len(defectors_payoffs) > 0:
            average_cooperator_payoff = np.mean(cooperators_payoffs)
            average_defector_payoff = np.mean(defectors_payoffs)
            self.payoff_difference = average_cooperator_payoff - average_defector_payoff
        else:
            self.payoff_difference = 0
        if state.round_number % 10 == 9:
            if self.cooperation_quotient > self.cooperation_quotient_threshold + 0.1:
                self.cooperation_quotient_threshold -= 0.05
            elif self.payoff_difference < self.payoff_difference_threshold - 0.5:
                self.payoff_difference_threshold += 0.05
        if self.cooperation_quotient > self.cooperation_quotient_threshold and self.payoff_difference > self.payoff_difference_threshold:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_267(BaseStrategy):
    """
    Exploit opponents' cooperative tendencies in an N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_actions_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :])
        if len(self.my_actions_history) > 0:
            cooperators += self.my_actions_history[-1]
        if cooperators >= self.game_description.n_players / 2:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_268(BaseStrategy):
    """
    Exploitative strategy that adapts to opponent behavior while maintaining a strong exploitative stance.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.defection_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        acr = self.calculate_average_cooperation_rate(history)
        theta = self.defection_threshold
        if acr < theta:
            return Action.D
        previous_payoff = history.my_payoffs[-1]
        average_payoff = np.mean(history.my_payoffs)
        if previous_payoff >= average_payoff and history.my_actions[-1]:
            return Action.C
        else:
            return Action.D

    def calculate_average_cooperation_rate(self, history: PlayerHistory) -> float:
        """
        Calculate the average cooperation rate of opponents over the last 'n' rounds.
        """
        cooperation_count = np.sum(history.opponent_actions[-self.n:, :])
        return cooperation_count / (self.n * self.game_description.n_players - 1)

class Strategy_EXPLOITATIVE_269(BaseStrategy):
    """
    This strategy balances cooperation and defection based on the observed behavior of opponents.
    It initially cooperates to gather information about potential cooperation levels, then adjusts its threshold accordingly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate_history = []
        self.thresh = game_description.k / (2 * game_description.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 4 or state.round_number == 0:
            return Action.C
        avg_coop_rate = self.calculate_avg_coop_rate(history, state.round_number)
        self.avg_coop_rate_history.append(avg_coop_rate)
        if avg_coop_rate > self.thresh:
            return Action.D
        else:
            return Action.C

    def calculate_avg_coop_rate(self, history: PlayerHistory, end_round: int):
        start_round = max(0, end_round - 4)
        total_cooperators = np.sum(history.opponent_actions[start_round:end_round + 1, :]) + sum(self.avg_coop_rate_history[max(-start_round, -len(self.avg_coop_rate_history)):])
        avg_coop_rate = total_cooperators / ((end_round - start_round + 1) * self.game_description.n_players)
        return avg_coop_rate

    def last_round_logic(self):
        return Action.D

class Strategy_EXPLOITATIVE_270(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior while maintaining a balance between cooperation and punishment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.m = 5
        self.theta = 0.6
        self.p = 2
        self.d = 3
        self.avgcoop = 0
        self.punished_opponents = [False] * game_description.n_players
        self.punishment_durations = [0] * game_description.n_players
        self.opponent_defection_streaks = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        self.avgcoop = self.calculate_avg_cooperation(history)
        if self.avgcoop >= self.theta:
            action = Action.C
        else:
            action = Action.D
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if not opponent_action and history.my_actions[-1]:
                self.punished_opponents[i] = True
                self.punishment_durations[i] += 1
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if not opponent_action:
                self.opponent_defection_streaks[i] += 1
            else:
                self.opponent_defection_streaks[i] = 0
        self.update_punishment_durations()
        self.update_avg_cooperation()
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif any(self.punished_opponents) or any((defection_streak >= self.d for defection_streak in self.opponent_defection_streaks)):
            return Action.D
        return action

    def calculate_avg_cooperation(self, history: PlayerHistory) -> float:
        """
        Calculate the average cooperation rate of all opponents over the last `m` rounds.
        """
        opponent_actions = history.opponent_actions[-self.m:, :]
        avgcoop = np.mean(opponent_actions)
        return avgcoop

    def update_punishment_durations(self):
        """
        Update punishment durations for each opponent.
        """
        for i in range(len(self.punishment_durations)):
            if self.punished_opponents[i] and self.punishment_durations[i] > 0:
                self.punishment_durations[i] -= 1
                if self.punishment_durations[i] == 0:
                    self.punished_opponents[i] = False

    def update_avg_cooperation(self):
        """
        Update average cooperation rate.
        """
        pass

class Strategy_EXPLOITATIVE_271(BaseStrategy):
    """
    The Adaptive Cooperate-Defect (ACD) strategy aims to balance cooperation and defection 
    to maximize individual payoff while adapting to the game's history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = min(0.5, game_description.k / (game_description.n_players * 1.5))
        self.opponent_defection_rates = [None] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < 0.5 else Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            previous_cooperators = sum(history.opponent_actions[-1, :])
            cooperators_including_self = history.my_actions[-1] + previous_cooperators
            if self.game_description.n_players * self.cooperation_threshold <= cooperators_including_self:
                probability_of_cooperating = 0.8
            else:
                probability_of_cooperating = 0.4
            for i, (opponent_actions, payoff) in enumerate(zip(history.opponent_actions.T, history.opponent_payoffs.T)):
                if self.game_description.n_rounds - state.round_number >= 10 and len(opponent_actions) > 1:
                    total_defections = np.count_nonzero(~opponent_actions[:-1])
                    total_actions = len(opponent_actions[:-1])
                    self.opponent_defection_rates[i] = total_defections / total_actions
                if self.opponent_defection_rates[i] is not None:
                    if self.opponent_defection_rates[i] >= 0.7:
                        probability_of_cooperating = min(probability_of_cooperating, 0.2)
                    elif self.opponent_defection_rates[i] <= 0.3:
                        probability_of_cooperating = max(probability_of_cooperating, 0.8)
            return Action.C if random.random() < probability_of_cooperating else Action.D

class Strategy_EXPLOITATIVE_272(BaseStrategy):
    """
    An adaptive strategy that balances exploitation with adaptability to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.n_players / (2 * game_description.k)
        self.consecutive_defections = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3 or history is None:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        for i, opponent_action in enumerate(opponent_actions):
            if not opponent_action and self.consecutive_defections.get(i, 0) >= 2:
                return Action.D
            elif not opponent_action:
                self.consecutive_defections[i] = self.consecutive_defections.get(i, 0) + 1
            else:
                self.consecutive_defections[i] = 0
        avg_cooperators_last_round = np.mean(history.opponent_actions[-1, :])
        if avg_cooperators_last_round > self.threshold:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_273(BaseStrategy):
    """
    Adaptive Exploiter strategy that adapts to various opponent behaviors while aiming to maximize payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.0
        self.theta = game_description.k / game_description.n_players * (game_description.n_players - 1) / 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        total_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        self.avg_coop_rate += total_cooperators / self.game_description.n_players - self.avg_coop_rate
        if self.avg_coop_rate > self.theta:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_274(BaseStrategy):
    """
    An adaptive exploitative strategy for the N-Player Public Goods Game.
    It balances between exploiting others and leveraging cooperation when beneficial.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.initial_rounds = 5
        self.final_rounds = 5
        self.cooperation_threshold_percentage = 0.6
        self.exploitation_logic_cooperation_rate = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.initial_rounds:
            return Action.C
        avg_cooperators = np.mean(np.sum(history.opponent_actions[-5:, :], axis=1))
        CT = self.cooperation_threshold_percentage * self.game_description.n_players
        defector_history = np.sum(history.my_actions[-3:] == 0)
        if defector_history > 2:
            return Action.C
        if avg_cooperators >= CT and history.my_payoffs[-1] >= np.mean(history.opponent_payoffs[-1, :]):
            return Action.C
        if np.sum(history.opponent_actions[-1, :]) < self.exploitation_logic_cooperation_rate * self.game_description.n_players:
            return Action.D
        if state.round_number >= self.game_description.n_rounds - self.final_rounds:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_275(BaseStrategy):
    """
    Exploitative strategy that adapts to opponent behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_rounds_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        if self.punish_rounds_remaining > 0:
            self.punish_rounds_remaining -= 1
            return Action.D
        if not all(history.opponent_actions[-1, :]):
            self.punish_rounds_remaining = 2
            return Action.D
        num_cooperators = sum(history.opponent_actions[-1, :])
        p = (1 + self.game_description.k / self.game_description.n_players * num_cooperators / self.game_description.n_players) ** (-1)
        if random.random() < p:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_276(BaseStrategy):
    """
    A robust and exploitative strategy for the N-Player Public Goods Game.
    Balances cooperation and exploitation, adapting to the game's dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        coop_ratio = (opponent_cooperators + int(history.my_actions[-1])) / self.game_description.n_players
        if history.my_actions[-1] == False and history.my_payoffs[-1] < np.mean(history.opponent_payoffs[-1, :]):
            return Action.C
        if coop_ratio > self.theta:
            action = Action.D
        else:
            action = Action.C
        if action == Action.D and history.my_payoffs[-1] < np.mean(history.opponent_payoffs[-1, :]):
            self.theta -= 0.05
        elif action == Action.C and history.my_payoffs[-1] > np.mean(history.opponent_payoffs[-1, :]):
            self.theta += 0.05
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_EXPLOITATIVE_277(BaseStrategy):
    """
    The Adaptive Exploiter strategy takes advantage of other players' cooperative behaviors 
    while minimizing losses when others defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 1.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        prev_round_payoffs = history.my_payoffs[-1:] + history.opponent_payoffs[-1, :]
        avg_payoff = np.mean(prev_round_payoffs)
        if avg_payoff > self.threshold:
            return Action.D if random.random() < 0.8 else Action.C
        else:
            return Action.C if random.random() < 0.4 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_278(BaseStrategy):
    """
    Adaptive Cooperate-Defect (ACD) strategy for Public Goods Game.
    
    The ACD strategy aims to exploit opponents by adapting to their behavior. 
    By initially cooperating with a probability of 50%, we create uncertainty about our intentions.
    As the game progresses, we adjust our cooperation probability based on the observed number of cooperators in the previous round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = game_description.k / game_description.n_players * (game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if history.opponent_actions.shape[1] == 1:
            if history.opponent_actions[-2, 0]:
                return Action.C
            else:
                return Action.D
        if prev_cooperators >= self.coop_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_280(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that is adaptive and robust.
    It observes the history of cooperation in the game to adjust its own level of cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.c_avg = 0.0
        self.n_c = 0
        self.n_d = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        self.n_c += sum(history.opponent_actions[-1, :] == True)
        self.n_d += sum(history.opponent_actions[-1, :] == False)
        self.c_avg = self.n_c / (state.round_number * self.game_description.n_players)
        if self.c_avg >= 0.5:
            return Action.C
        else:
            opponent_payoffs = history.opponent_payoffs[-1, :]
            my_payoff = history.my_payoffs[-1]
            avg_defect_payoff = np.mean(opponent_payoffs[history.opponent_actions[-1, :] == False])
            avg_coop_payoff = np.mean(opponent_payoffs[history.opponent_actions[-1, :] == True])
            if avg_defect_payoff > avg_coop_payoff:
                return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_281(BaseStrategy):
    """
    Exploits a wide range of opponent behaviors while adapting to changing cooperation levels and trends.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.delta = 0.1
        self.window_size = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        opponent_actions_array = np.array(history.opponent_actions)
        my_actions_array = np.array([a == Action.C for a in history.my_actions])
        all_cooperations = np.concatenate((opponent_actions_array, my_actions_array[:, np.newaxis]), axis=1)
        avg_coop_level = np.mean(all_cooperations[max(0, state.round_number - self.window_size + 1):state.round_number + 1, :])
        if avg_coop_level >= self.n_players / 2:
            return Action.C if avg_coop_level < self.n_players / 2 + self.delta else Action.D
        elif sum(history.opponent_actions[-1]) > self.n_players / 2:
            return Action.C
        elif state.round_number == self.n_rounds - 1:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_282(BaseStrategy):
    """
    Adaptive Exploiter strategy for the N-Player Public Goods Game.
    This strategy adapts to the game parameters and history by exploiting opponents' cooperative behavior early on,
    adapting to the average cooperation rate and payoffs, and using a "last-chance" cooperation attempt in late rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        avg_coop_rate = np.mean(history.opponent_actions)
        total_contributions = sum(history.opponent_actions[-1, :])
        if state.round_number < self.game_description.n_rounds // 3:
            return Action.C if avg_coop_rate > 0.5 else Action.D
        elif state.round_number >= self.game_description.n_rounds // 3 and state.round_number < 2 * self.game_description.n_rounds // 3:
            my_payoff = np.mean(history.my_payoffs)
            avg_payoff = np.mean(np.mean(history.opponent_payoffs, axis=0))
            return Action.C if my_payoff < avg_payoff else Action.D
        else:
            rounds_remaining = self.game_description.n_rounds - state.round_number
            return Action.C if rounds_remaining <= self.game_description.n_players else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_283(BaseStrategy):
    """
    A history-dependent strategy that adapts to opponents' behaviors while balancing cooperation and defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.p_c = self.k / self.n_players
        self.n_coop_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        self.n_coop_prev = opponent_cooperators + (history.my_actions[-1] == True)
        if self.n_players == 2:
            return Action.D
        if self.k > self.n_players - 1:
            self.p_c = min(self.k / self.n_players, 0.8)
        if state.round_number < self.game_description.n_rounds:
            if self.n_coop_prev >= self.n_players / 3:
                return Action.D
            else:
                return Action.C if random.random() < self.p_c else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_284(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Explorer"

    This strategy initially randomizes its action, then adapts to opponents' behavior.
    It cooperates if most opponents cooperate and defects otherwise, unless it can exploit them by defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_previous_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        cooperators = sum(history.opponent_actions[-1, :])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if cooperators >= math.ceil(self.game_description.k / self.game_description.n_players * self.game_description.n_players):
            return Action.C
        elif history.my_payoffs[-1] > np.mean(history.opponent_payoffs[-1, :]):
            return Action.D
        else:
            p = cooperators / self.game_description.n_players
            return Action.C if random.random() < p else Action.D

class Strategy_EXPLOITATIVE_285(BaseStrategy):
    """
    An exploitative player that adapts to the history of play and game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = 0.0
        self.exploit_counter = 0
        self.retaliation_flag = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        total_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        self.coop_threshold = self.game_description.k / self.game_description.n_players * (total_cooperators_prev_round / self.game_description.n_players)
        if self.retaliation_flag:
            action = Action.D
            self.retaliation_flag = False
        elif self.exploit_counter > 0:
            action = Action.D
        elif total_cooperators_prev_round >= self.coop_threshold * self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        if history.my_actions[-1] == Action.D and sum(history.opponent_actions[-1, :]) > 0:
            self.exploit_counter += 1
        elif history.my_actions[-1] == Action.C and any((action == Action.D for action in history.opponent_actions[-1, :])):
            self.retaliation_flag = True
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_286(BaseStrategy):
    """
    Exploitative strategy that adapts to the behaviors of other players by 
    identifying opportunities for cooperation and defecting when necessary.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :]) + 1
        coop_thresh = max(self.game_description.k / self.game_description.n_players, (1 + self.game_description.k / self.game_description.n_players * (self.game_description.n_rounds - state.round_number)) / (2 * (self.game_description.k / self.game_description.n_players)))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif cooperators >= coop_thresh:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_287(BaseStrategy):
    """
    Exploitative strategy that adapts to changing game dynamics and exploits opponents' cooperative tendencies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2 or state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif 3 <= state.round_number <= self.game_description.n_rounds // 2:
            num_cooperators = sum(history.opponent_actions[-1, :])
            if num_cooperators >= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        else:
            my_previous_payoff = history.my_payoffs[-1]
            avg_opponent_payoff = np.mean(history.opponent_payoffs[-1, :])
            if my_previous_payoff >= 1.5 * avg_opponent_payoff:
                return Action.C if history.my_actions[-1] else Action.D
            else:
                num_cooperators = sum(history.opponent_actions[-1, :])
                if history.my_actions[-1] and num_cooperators < self.game_description.n_players / 2 or (not history.my_actions[-1] and num_cooperators > self.game_description.n_players / 2):
                    return Action.D if history.my_actions[-1] else Action.C
                else:
                    return Action.D

class Strategy_EXPLOITATIVE_288(BaseStrategy):
    """
    Adaptive Exploiter strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.D_prev = 0
        self.π_avg_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        if self.D_prev > self.n_players / 2:
            return Action.D
        if self.π_avg_prev >= self.k / self.n_players:
            return Action.C
        else:
            return Action.D
        if state.round_number % 3 == 0:
            if history is not None:
                self.D_prev = sum((1 for opponent_action in history.opponent_actions[-2, :] if opponent_action == False))
                self.π_avg_prev = np.mean(history.my_payoffs[-2:] + history.opponent_payoffs[-2, :].flatten())

class Strategy_EXPLOITATIVE_289(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors 
    while adapting to the game's history. This strategy prioritizes self-interest and aims 
    to maximize its own payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.contribution_threshold = game_description.k / game_description.n_players
        self.opponent_cooperation_rates = [0.5] * (game_description.n_players - 1)
        self.cooperate_count = 0
        self.defect_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.D
        if history is not None:
            for i in range(self.game_description.n_players - 1):
                if history.opponent_actions[-1, i]:
                    self.opponent_cooperation_rates[i] += 0.1
                elif not history.my_actions[-1] and history.opponent_actions[-1, i]:
                    self.opponent_cooperation_rates[i] -= 0.1
        avg_ocr = sum(self.opponent_cooperation_rates) / (self.game_description.n_players - 1)
        if state.round_number < self.game_description.n_rounds:
            if avg_ocr > self.contribution_threshold:
                action = Action.C
            else:
                action = Action.D
            if avg_ocr == self.contribution_threshold:
                if self.cooperate_count > self.defect_count:
                    action = Action.C
                else:
                    action = Action.D
        else:
            action = Action.D
        if history is not None:
            if history.my_actions[-1]:
                self.cooperate_count += 1
            else:
                self.defect_count += 1
        return action

class Strategy_EXPLOITATIVE_290(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior while maintaining a balance between exploiting their cooperativeness and avoiding being exploited itself.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.n_rounds = game_description.n_rounds
        self.k = game_description.k
        self.m = min(game_description.n_rounds // 2, game_description.n_players)
        self.punish_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number <= self.n_players:
            return Action.D
        opponent_actions = history.opponent_actions[-self.m:, :]
        average_cooperation_rate = np.mean(opponent_actions)
        total_cooperators_previous_round = sum(history.opponent_actions[-1, :])
        if average_cooperation_rate >= 0.5 and total_cooperators_previous_round >= self.n_players // 2:
            if self.punish_counter > 0:
                self.punish_counter -= 1
                return Action.D
            else:
                opponent_payoffs = history.opponent_payoffs[-self.m:, :]
                average_opponent_payoff = np.mean(opponent_payoffs)
                if average_opponent_payoff >= self.k / self.n_players or np.any(np.diff(history.opponent_actions[-2:, :], axis=0) == 0):
                    return Action.D
            return Action.C
        else:
            if history.my_actions[-1] and (not history.my_actions[-2]):
                self.punish_counter = 2
            return Action.D

    def __str__(self):
        return 'Adaptive Defector with History-Based Cooperation'

class Strategy_EXPLOITATIVE_292(BaseStrategy):
    """
    A robust and adaptive exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate = 0.0
        self.average_payoff = 0.0
        self.standard_deviation = 0.0

    def calculate_cooperation_rate(self, history: PlayerHistory) -> float:
        opponent_actions = history.opponent_actions[:-1] if len(history.my_actions) > 1 else history.opponent_actions
        return np.mean(opponent_actions)

    def calculate_average_payoff(self, history: PlayerHistory) -> float:
        my_payoffs = history.my_payoffs[:-1] if len(history.my_actions) > 1 else history.my_payoffs
        return np.mean(my_payoffs)

    def calculate_standard_deviation(self, history: PlayerHistory) -> float:
        opponent_payoffs = history.opponent_payoffs[:-1] if len(history.my_actions) > 1 else history.opponent_payoffs
        flattened_payoffs = opponent_payoffs.flatten()
        return np.std(flattened_payoffs)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if len(history.my_actions) < self.game_description.n_rounds // 2:
            self.cooperation_rate = self.calculate_cooperation_rate(history)
            if self.cooperation_rate > 0.5:
                return Action.D
            else:
                return Action.C
        elif state.round_number == self.game_description.n_rounds // 2:
            self.average_payoff = self.calculate_average_payoff(history)
            self.standard_deviation = self.calculate_standard_deviation(history)
        elif len(history.my_actions) < self.game_description.n_rounds - 1:
            if self.cooperation_rate > 0.5 and self.average_payoff > history.my_payoffs[-2]:
                return Action.C
            elif self.cooperation_rate < 0.5 or self.average_payoff <= history.my_payoffs[-2]:
                return Action.D
            elif self.standard_deviation > 1:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_293(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy is designed to exploit a wide range of opponent behaviors by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions_last_round = history.opponent_actions[-1]
        total_coop_last_round = sum(opponent_actions_last_round)
        if total_coop_last_round >= self.game_description.n_players / 2:
            return Action.D
        elif self.game_description.k > 1.5 * self.game_description.n_players and total_coop_last_round < self.game_description.n_players / 2:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_294(BaseStrategy):
    """
    A robust and adaptive exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_cooperators = []
        self.avg_cooperators = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if history.my_actions.size > 0 and (not history.my_actions[-1]):
            prev_coop = sum(history.opponent_actions[-2, :])
        else:
            prev_coop = sum(history.opponent_actions[-1, :])
        self.prev_cooperators.append(prev_coop)
        if len(self.prev_cooperators) > 3:
            self.avg_cooperators = self.prev_cooperators[-3:]
        else:
            self.avg_cooperators = self.prev_cooperators
        avg_coop = sum(self.avg_cooperators) / len(self.avg_cooperators)
        if state.round_number <= 5:
            return Action.D if prev_coop > self.game_description.n_players / 2 else Action.C
        elif state.round_number < self.game_description.n_rounds:
            return Action.D if avg_coop > self.game_description.n_players / 2 else Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_295(BaseStrategy):
    """
    An adaptive strategy that balances cooperation and exploitation by responding to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop = [0.0] * game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        opponent_cooperations = np.sum(history.opponent_actions[-2, :])
        self.avg_coop[state.round_number - 1] = opponent_cooperations / self.game_description.n_players
        if self.avg_coop[state.round_number - 1] >= 0.5:
            action = Action.C
        elif self.game_description.k / self.game_description.n_players > 0.5:
            action = Action.D
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_296(BaseStrategy):
    """
    A robust and adaptive exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.COOP_count = 0
        self.DEF_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C
        if state.round_number == 3:
            for opponent_actions in history.opponent_actions[:3, :]:
                if sum(opponent_actions) >= 2:
                    self.COOP_count += 1
                else:
                    self.DEF_count += 1
        if state.round_number > 3 or (state.round_number == 3 and history is not None):
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.D
            for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
                if opponent_action and (self.COOP_count <= self.DEF_count or state.round_number == 3):
                    self.COOP_count += 1
                    if history is not None:
                        self.DEF_count -= 1
                elif not opponent_action and self.COOP_count > self.DEF_count:
                    self.DEF_count += 1
                    if history is not None:
                        self.COOP_count -= 1
            if self.COOP_count / self.game_description.n_players >= self.game_description.k / self.game_description.n_players:
                return Action.C
            elif self.COOP_count == int(self.game_description.k):
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_297(BaseStrategy):
    """
    An adaptive exploitative strategy for the N-Player Public Goods Game.
    
    The strategy initially cooperates to gather information, then exploits opponents' cooperation 
    while maintaining a positive image and creating uncertainty. In late rounds, it focuses on 
    maximizing personal payoff by defecting when possible.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.round_history = []

    def analyze_pattern(self, recent_rounds: NDArray[np.bool_]) -> str:
        """
        Analyze the pattern of opponents' actions in the last 5 rounds.
        
        Returns 'mostly_cooperate' if > 60% cooperated, 'mixed' if between 40% and 60%, 
        and 'mostly_defect' otherwise.
        """
        total_actions = recent_rounds.shape[0] * recent_rounds.shape[1]
        cooperators = np.sum(recent_rounds)
        cooperation_rate = cooperators / total_actions
        if cooperation_rate > 0.6:
            return 'mostly_cooperate'
        elif 0.4 <= cooperation_rate <= 0.6:
            return 'mixed'
        else:
            return 'mostly_defect'

    def estimate_expected_cooperators(self, history: PlayerHistory) -> int:
        """
        Estimate the expected number of cooperators in the next round.
        
        This is a simple implementation that assumes the average cooperation rate 
        in recent rounds will continue. You may want to improve this estimation.
        """
        total_actions = history.opponent_actions.shape[0] * history.opponent_actions.shape[1]
        cooperators = np.sum(history.opponent_actions)
        return math.ceil(cooperators / total_actions)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            self.round_history = []
            return Action.C
        recent_rounds = history.opponent_actions[max(0, len(history.opponent_actions) - 5):]
        majority_cooperated = np.sum(history.opponent_actions[-1]) > self.game_description.n_players / 2
        if state.round_number < self.game_description.n_rounds // 2:
            if not majority_cooperated:
                return Action.C
        pattern = self.analyze_pattern(recent_rounds)
        if pattern == 'mostly_cooperate':
            return Action.D
        elif pattern in ['mixed', 'mostly_defect']:
            return Action.C
        total_cooperators = np.sum(history.opponent_actions[-1])
        expected_cooperators = self.estimate_expected_cooperators(history)
        if state.round_number >= 3 * self.game_description.n_rounds // 4 and total_cooperators > self.game_description.n_players / 2:
            return Action.D
        elif expected_cooperators >= 1:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.game_description.n_players == 2:
            self.round_history.append(Action.C) if len(self.round_history) % 2 == 0 else self.round_history.append(Action.D)
            return self.round_history[-1]
        return Action.D

class Strategy_EXPLOITATIVE_298(BaseStrategy):
    """
    An exploitative strategy that adapts to various opponent behaviors in the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_init = 0.5
        self.p_defect = 0.7
        self.p_explore = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < self.p_init else Action.D
        n_players = self.game_description.n_players
        k = self.game_description.k
        theta = k / n_players * (n_players / 2)
        opponent_actions = history.opponent_actions[-1, :]
        my_payoffs = history.my_payoffs
        average_payoff = np.mean(my_payoffs)
        contributions = sum(opponent_actions) + int(history.my_actions[-1])
        theta_new = k / n_players * contributions
        theta = max(theta, theta_new)
        defect_prob = 0.0
        for opponent in range(n_players - 1):
            if not opponent_actions[opponent]:
                defect_prob += self.p_defect
                theta -= 0.1 * theta
        explore_action = random.random() < self.p_explore and average_payoff > np.mean(my_payoffs)
        cooperate = contributions >= theta or (defect_prob < 1.0 and random.random() < 1 - defect_prob) or explore_action
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if cooperate else Action.D

class Strategy_EXPLOITATIVE_299(BaseStrategy):
    """
    The Adaptive Exploiter strategy balances cooperation and exploitation by adapting to the average cooperation rate of opponents over time.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.rounds_since_adaptation = 0
        self.opponent_cooperation_rates = np.zeros(game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choices([Action.C, Action.D], weights=[self.k / self.n_players, 1 - self.k / self.n_players])[0]
        if self.rounds_since_adaptation >= 5:
            self.adapt_to_opponent_behavior(history)
            self.rounds_since_adaptation = 0
        opponent_cooperations = np.sum(history.opponent_actions[-1, :], axis=0)
        average_cooperation_rate = np.mean(opponent_cooperations)
        if average_cooperation_rate >= self.k / self.n_players:
            return random.choices([Action.C, Action.D], weights=[average_cooperation_rate, 1 - average_cooperation_rate])[0]
        else:
            for i in range(self.opponent_cooperation_rates.shape[0]):
                if self.opponent_cooperation_rates[i] > self.k / self.n_players:
                    return random.choices([Action.C, Action.D], weights=[1 - average_cooperation_rate, average_cooperation_rate])[0]
            return Action.D
        self.rounds_since_adaptation += 1
        return Action.D

    def adapt_to_opponent_behavior(self, history: PlayerHistory):
        for i in range(self.opponent_cooperation_rates.shape[0]):
            self.opponent_cooperation_rates[i] = np.mean(history.opponent_actions[:, i])

class Strategy_EXPLOITATIVE_300(BaseStrategy):
    """
    An adaptive strategy that initially gathers information about opponents' strategies,
    then exploits their cooperative tendencies while minimizing its own contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.observation_phase_cooperators = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.2 else Action.D
        if state.round_number <= 5:
            self.observation_phase_cooperators.append(sum(history.opponent_actions[-1, :]))
            return Action.D
        avg_cooperators = sum(self.observation_phase_cooperators) / len(self.observation_phase_cooperators)
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        if avg_cooperators > 0.5 * self.game_description.n_players:
            return Action.D
        elif avg_cooperators < 0.2 * self.game_description.n_players:
            p = 0.4 + (state.round_number - 6) * 0.1 / 2
            return Action.C if random.random() < p else Action.D
        else:
            p = self.game_description.k / self.game_description.n_players * (avg_cooperators / self.game_description.n_players)
            return Action.C if random.random() < p else Action.D

class Strategy_EXPLOITATIVE_301(BaseStrategy):
    """
    Adaptive Exploiter strategy: Cooperate with probability 0.5 in the first round, 
    then adapt to opponents' actions by cooperating only when beneficial.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators = []
        self.average_payoff_c = []
        self.average_payoff_d = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        prev_round_opponents = history.opponent_actions[-1, :]
        total_cooperators = sum(prev_round_opponents)
        self.total_cooperators.append(total_cooperators)
        prev_round_payoffs_c = [payoff for payoff, action in zip(history.opponent_payoffs[-1, :], prev_round_opponents) if action]
        avg_payoff_c = np.mean(prev_round_payoffs_c) if prev_round_payoffs_c else 0.0
        self.average_payoff_c.append(avg_payoff_c)
        prev_round_payoffs_d = [payoff for payoff, action in zip(history.opponent_payoffs[-1, :], prev_round_opponents) if not action]
        avg_payoff_d = np.mean(prev_round_payoffs_d) if prev_round_payoffs_d else 0.0
        self.average_payoff_d.append(avg_payoff_d)
        if state.round_number < self.game_description.n_rounds - 1:
            if total_cooperators <= self.game_description.n_players / 2 and avg_payoff_c > avg_payoff_d:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_302(BaseStrategy):
    """
    The Adaptive Exploiter strategy exploits opponents' cooperation while being robust to various opponent strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploitation_threshold = 0.5 * (game_description.k / game_description.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperation_frequencies = np.sum(history.opponent_actions, axis=0) / (state.round_number + 1)
        cooperative_opponents = np.sum(opponent_cooperation_frequencies > self.exploitation_threshold)
        defective_opponents = self.game_description.n_players - cooperative_opponents
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif cooperative_opponents > self.game_description.n_players / 2:
            return Action.C
        elif defective_opponents > self.game_description.n_players / 2:
            return Action.D
        else:
            probability_cooperate = cooperative_opponents / self.game_description.n_players
            return Action.C if random.random() < probability_cooperate else Action.D

class Strategy_EXPLOITATIVE_303(BaseStrategy):
    """
    An adaptive exploiter strategy that defects initially and then adapts based on the cooperation level of opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_history = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        opponent_cooperators = sum((1 for a in history.opponent_actions[-1, :] if a))
        total_rounds_played = len(history.my_payoffs)
        if self.opponent_history is not None and any((all((a == False for a in opponent)) for opponent in self.opponent_history)):
            return Action.D
        if total_rounds_played >= math.ceil(self.game_description.n_rounds / 2) and all((p <= 0 for p in history.my_payoffs[1:])):
            return Action.D
        self.opponent_history = history.opponent_actions[-self.game_description.n_players:, :]
        if opponent_cooperators >= self.game_description.n_players // 2:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_305(BaseStrategy):
    """
    Exploit others' cooperative behavior in the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_players = self.game_description.n_players
        k = self.game_description.k
        last_round_opponent_actions = history.opponent_actions[-1]
        cooperators_last_round = sum(last_round_opponent_actions)
        my_payoff_last_round = 1 - int(history.my_actions[-1]) + k / n_players * (cooperators_last_round + int(history.my_actions[-1]))
        avg_coop_payoff = my_payoff_last_round / (1 + cooperators_last_round) if history.my_actions[-1] else 0
        potential_defect_payoff = 1 + k / n_players * cooperators_last_round
        if avg_coop_payoff == 0 or potential_defect_payoff <= avg_coop_payoff:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_306(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game.
    It adapts its behavior based on the history of the game, aiming to maximize payoff by identifying opportunities to defect when others cooperate and vice versa.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if state.round_number <= 5:
            num_cooperators_prev_round = sum(history.opponent_actions[-1, :])
            if num_cooperators_prev_round >= self.game_description.n_players // 2:
                return Action.C if random.random() < 0.7 else Action.D
            return Action.D if random.random() < 0.7 else Action.C
        if state.round_number <= self.game_description.n_rounds // 2:
            avg_payoff_self = np.mean(history.my_payoffs[-5:])
            avg_payoffs_others = np.mean(history.opponent_payoffs[-5, :], axis=0)
            if avg_payoff_self < np.median(avg_payoffs_others):
                return Action.D if random.random() < 0.8 else Action.C
            return Action.C if random.random() < 0.6 else Action.D
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        if num_cooperators_prev_round >= self.game_description.n_players // 3:
            return Action.C if random.random() < 0.5 else Action.D
        return Action.D if random.random() < 0.9 else Action.C
        return Action.D

class Strategy_EXPLOITATIVE_307(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Tit-for-Tat with Punishment"

    This strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive.
    It cooperates initially, then adapts based on the total number of cooperators in the previous round.
    A punishment mechanism is implemented to prevent opponents from exploiting our cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            cooperators = sum(history.opponent_actions[-1, :])
            self.prev_cooperators = cooperators
            if self.prev_cooperators < self.game_description.k and history.my_actions[-1] == 1:
                return Action.D
            if cooperators >= self.game_description.k:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_308(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to exploit the opponent's behavior while adapting to the game's dynamics.
    This strategy depends only on the game parameters and history.

    Decision Rules:
    1. Initial Rounds (t < n): Play Cooperate (C) to gather information about the opponents' behavior and to potentially create a norm of cooperation.
    2. Observation Phase (t >= n): Observe the opponents' actions and calculate the average number of cooperators (`avg_coop`) in the last `n` rounds.

    Edge Cases:
    * First Round: Play Cooperate (C) to initiate a potential norm of cooperation.
    * Last Round: Play the same action as in the previous round, as there is no future opportunity for reciprocity.
    * Tiebreaker: In case of a tie (e.g., `avg_coop` equals `k/n`), play Cooperate (C) to maintain a cooperative stance.

    Exploitative Mindset:
    The Adaptive Exploiter strategy aligns with the exploitative mindset by:
    1. Initially cooperating to create a norm and gather information.
    2. Observing opponents' behavior to identify potential cooperation patterns.
    3. Defecting when opponents are cooperative enough (i.e., `avg_coop` exceeds `k/n`) to exploit their cooperative behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_history = []
        self.previous_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if len(self.avg_coop_history) < self.game_description.n_players:
            self.avg_coop_history.append(history.opponent_actions[-1, :].sum())
        else:
            self.avg_coop_history.pop(0)
            self.avg_coop_history.append(history.opponent_actions[-1, :].sum())
        avg_coop = sum(self.avg_coop_history) / len(self.avg_coop_history)
        if avg_coop > self.game_description.k / self.game_description.n_players:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_309(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors 
    while adapting to the game's history. This strategy focuses on maximizing its own payoff by 
    identifying and responding to opponents' cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_defects = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        prev_opponent_actions = history.opponent_actions[-1, :]
        avg_coop_prev = np.mean(prev_opponent_actions)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if avg_coop_prev >= self.game_description.k / self.game_description.n_players:
            self.consecutive_defects = 0
            return Action.C
        else:
            self.consecutive_defects += 1
            if self.consecutive_defects >= 2:
                return Action.C
            return Action.D
        for i in range(self.game_description.n_players - 1):
            if np.all(~history.opponent_actions[:, i]):
                return Action.D

class Strategy_EXPLOITATIVE_310(BaseStrategy):
    """
    Exploitative strategy that balances cooperation and exploitation by adapting to changing opponent behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        prev_round_opponent_actions = history.opponent_actions[-1]
        all_coop_prev_round = np.all(prev_round_opponent_actions)
        any_defect_prev_round = np.any(~prev_round_opponent_actions)
        if all_coop_prev_round:
            action = Action.C
            self.coop_count += 1
        elif self.coop_count > self.game_description.n_players / 2:
            action = Action.C
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_311(BaseStrategy):
    """
    An adaptive strategy that balances cooperation and exploitation in response to the dynamic environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.contribution_threshold = 0.5
        self.uncooperative_opponents = set()
        self.avg_contribution_rate = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.avg_contribution_rate = np.mean(np.sum(history.opponent_actions[:state.round_number - 1, :], axis=1)) / self.game_description.n_players
        opponent_recent_actions = history.opponent_actions[state.round_number - 3:state.round_number, :]
        for i in range(self.game_description.n_players - 1):
            if np.sum(opponent_recent_actions[:, i]) < opponent_recent_actions.shape[0] / 2:
                self.uncooperative_opponents.add(i)
        if self.avg_contribution_rate > self.contribution_threshold:
            return Action.C
        elif len(self.uncooperative_opponents) > 0 and self.avg_contribution_rate > 0:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_312(BaseStrategy):
    """
    An exploitative strategy that adapts to changing conditions in the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_prev = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            cooperators = sum(history.opponent_actions[-1, :] | history.my_actions[-1])
            self.p_prev = cooperators / self.game_description.n_players
            exp_defect = 1 + self.game_description.k / self.game_description.n_players * self.p_prev
            thresh_c = (self.game_description.n_players - self.game_description.k) / self.game_description.n_players
            if self.p_prev >= thresh_c:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_313(BaseStrategy):
    """
    Exploitative strategy that adapts to the game's history, cooperating when beneficial and defecting otherwise.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coops = sum((1 for action in history.opponent_actions[-1, :] if action)) + (history.my_actions[-1] == True)
        if prev_coops <= math.floor(self.game_description.n_players / 2):
            return Action.D
        else:
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.D
            return Action.C

class Strategy_EXPLOITATIVE_314(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to 
    opponent behavior and maximizes payoffs by balancing cooperation and defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.r = game_description.n_rounds
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :])
        avg_payoff_C = np.mean([payoff for payoff, action in zip(history.opponent_payoffs[-1], history.opponent_actions[-1]) if action])
        avg_payoff_D = np.mean([payoff for payoff, action in zip(history.opponent_payoffs[-1], history.opponent_actions[-1]) if not action])
        if state.round_number < 5:
            if prev_cooperators >= self.n / 2:
                return Action.D
            elif avg_payoff_C > avg_payoff_D:
                return Action.C
            else:
                return Action.D
        elif state.round_number <= self.r // 2:
            p = self.k / self.n * (prev_cooperators / self.n)
            if random.random() < p:
                return Action.C
            else:
                return Action.D
        else:
            if state.round_number == self.r - 1:
                return Action.D
            p = self.k / self.n * (prev_cooperators / self.n)
            if random.random() < p:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_315(BaseStrategy):
    """
    Exploitative strategy "Adaptive K-Trigger" for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.k_trigger = game_description.k
        self.punishment_counters = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1]
        opponent_payoffs = history.opponent_payoffs[-1]
        for i, (action, payoff) in enumerate(zip(opponent_actions, opponent_payoffs)):
            if not action and payoff > 1 + self.game_description.k / self.game_description.n_players:
                self.punishment_counters[i] += 1
        avg_payoff = np.mean(history.my_payoffs[-1:] + opponent_payoffs)
        if avg_payoff > 1.5:
            self.k_trigger -= 0.1
        elif avg_payoff < 1:
            self.k_trigger += 0.1
        total_cooperators = sum(history.my_actions[-1:]) + sum(opponent_actions)
        if total_cooperators >= self.k_trigger:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_316(BaseStrategy):
    """
    An exploitative strategy for N-Player Public Goods Game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_cooperators < self.game_description.n_players / 2:
            return Action.D
        else:
            π_C = self.game_description.k / self.game_description.n_players * prev_cooperators
            π_D = 1 + self.game_description.k / self.game_description.n_players * prev_cooperators
            if π_C == π_D:
                return Action.C
            elif π_C > π_D:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_317(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.consecutive_defects = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_coop_payoff_t_minus_1 = self.calculate_avg_coop_payoff(history)
        if avg_coop_payoff_t_minus_1 >= 1:
            action = Action.C
        else:
            action = Action.D
        if action == Action.D:
            self.consecutive_defects += 1
        else:
            self.consecutive_defects = 0
        if self.consecutive_defects > self.game_description.n_players // 2:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

    def calculate_avg_coop_payoff(self, history: PlayerHistory) -> float:
        total_payoffs = np.sum(history.my_payoffs[-1])
        cooperators = sum(np.logical_and(history.opponent_actions[-1, :], history.my_actions[-1]))
        if cooperators == 0:
            return 0
        avg_coop_payoff_t_minus_1 = total_payoffs / cooperators
        return avg_coop_payoff_t_minus_1

class Strategy_EXPLOITATIVE_318(BaseStrategy):
    """
    Exploit the behavior of opponents by adapting to their cooperation levels 
    and manipulating the public good to maximize its own payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return np.random.choice([Action.C, Action.D], p=[0.5, 0.5])
        opponent_cooperations = history.opponent_actions[-1]
        c_avg = sum(opponent_cooperations) / self.game_description.n_players
        if c_avg > self.game_description.k:
            return Action.D
        elif c_avg < self.game_description.k:
            return Action.C
        else:
            return np.random.choice([Action.C, Action.D], p=[0.5, 0.5])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        for opponent in range(self.game_description.n_players):
            if not history.opponent_actions[-1][opponent] and c_avg > self.game_description.k:
                return np.random.choice([Action.C, Action.D], p=[0.2, 0.8])
        for opponent in range(self.game_description.n_players):
            if history.opponent_actions[-1][opponent] and c_avg < self.game_description.k:
                return np.random.choice([Action.C, Action.D], p=[0.7, 0.3])

class Strategy_EXPLOITATIVE_319(BaseStrategy):
    """
    The Adaptive Exploiter is designed to maximize its own payoff by exploiting 
    the cooperative behavior of other players, while adapting to changes in their strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_C = 0.5
        self.tau = 0.5
        self.phi = 1.5
        self.alpha = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 and history is None:
            return random.choice([Action.C, Action.D])
        avg_coop_rate = np.mean(history.opponent_actions[-1, :])
        if self.game_description.n_players == 2:
            return Action.D
        if state.round_number < self.game_description.n_rounds - 1:
            if avg_coop_rate > self.tau:
                action = Action.D
            else:
                pi_avg = np.mean(history.opponent_payoffs[-1, :])
                pi_self = history.my_payoffs[-1]
                self.p_C += self.alpha * (pi_avg - pi_self)
                self.p_C = max(0.0, min(self.p_C, 1.0))
                action = Action.C if random.random() < self.p_C else Action.D
        else:
            our_payoff = history.my_payoffs[-1]
            if our_payoff > self.phi:
                action = Action.D
            else:
                action = Action.C if random.random() < self.p_C else Action.D
        return action

class Strategy_EXPLOITATIVE_320(BaseStrategy):
    """
    Adaptive Exploiter strategy for the N-Player Public Goods Game.
    
    This strategy is designed to be exploitative, adaptive, and robust against various opponent behaviors.
    It alternates between cooperate and defect in the exploration phase to gather information about opponents' strategies,
    then adapts to their actions in the exploitation phase to maximize its own payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialize the Adaptive Exploiter strategy.

        Args:
            game_description (PublicGoodsDescription): The game description containing n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.exploration_phase = True
        self.cooperate_next = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.exploration_phase and state.round_number <= 5:
            self.cooperate_next = not self.cooperate_next
            return Action.C if self.cooperate_next else Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        total_cooperators = opponent_cooperators + history.my_actions[-1]
        if total_cooperators <= self.game_description.n_players / 2:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_321(BaseStrategy):
    """
    Exploitative Strategy: Adaptive Cooperate-Defect (ACD)

    Balances cooperation and defection to maximize payoffs by adapting to the game's history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.adaptive_threshold = 0.5
        self.coop_count = 0
        self.defect_count = 0

    def calculate_avg_payoff(self, players: NDArray[np.bool_], history: PlayerHistory) -> float:
        """Calculate average payoff for a group of players."""
        total_payoff = np.sum(history.opponent_payoffs[-1, :][players])
        return total_payoff / np.count_nonzero(players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            self.coop_count = 1
            return Action.C
        cooperators = history.opponent_actions[-1, :] == True
        avg_coop_payoff = self.calculate_avg_payoff(cooperators, history)
        avg_defect_payoff = self.calculate_avg_payoff(~cooperators, history)
        self.adaptive_threshold += 1 / self.game_description.n_players * (np.count_nonzero(cooperators) - np.count_nonzero(~cooperators))
        if avg_coop_payoff > avg_defect_payoff + self.adaptive_threshold:
            action = Action.C
            self.coop_count += 1
        else:
            action = Action.D
            self.defect_count += 1
        return action

class Strategy_EXPLOITATIVE_322(BaseStrategy):
    """
    The Adaptive Free Rider strategy is an exploitative strategy that 
    takes advantage of opponents' cooperation while minimizing the risk 
    of being exploited itself. It monitors the history of opponents' actions 
    and adjusts its behavior accordingly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        avg_cooperators = sum((1 for actions in history.opponent_actions if any(actions))) / len(history.opponent_actions)
        if avg_cooperators >= self.game_description.n_players / 2:
            return random.choice([Action.C, Action.D], p=[0.3, 0.7])
        else:
            return Action.D
        opponent_actions = np.array(history.opponent_actions)
        cooperators = []
        defectors = []
        for i in range(self.game_description.n_players):
            if sum((1 for actions in opponent_actions[:, i] if actions)) / len(opponent_actions[:, i]) > 2 / 3:
                cooperators.append(i)
            elif sum((1 for actions in opponent_actions[:, i] if not actions)) / len(opponent_actions[:, i]) > 2 / 3:
                defectors.append(i)
        if any(cooperators):
            return Action.D
        elif any(defectors):
            return random.choice([Action.C, Action.D], p=[0.2, 0.8])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_323(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors 
    while adapting to the game's history. This strategy focuses on maximizing individual payoffs 
    by identifying opportunities to defect against cooperative opponents and cooperate when necessary 
    to avoid mutual defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_classification = {i: {'CO': 0, 'DO': 0, 'UO': 0} for i in range(1, game_description.n_players + 1)}
        self.history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        for opponent in range(1, self.game_description.n_players + 1):
            self.classify_opponent(self.opponent_classification[opponent], history)
        action = ''
        for opponent in range(1, self.game_description.n_players + 1):
            if self.opponent_classification[opponent]['CO'] >= 0.75:
                action = Action.D
            elif self.opponent_classification[opponent]['DO'] >= 0.75:
                action = Action.C
            elif history and history.my_actions[-1] == opponent:
                action = self.mirror_opponent_action(self.opponent_classification[opponent], history)
            else:
                action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        self.history.append(action.value)
        return action

    def classify_opponent(self, opponent: dict, history: PlayerHistory):
        recent_actions = history.opponent_actions[-5:, :] if len(history.opponent_actions) > 5 else history.opponent_actions
        cooperations = np.sum(recent_actions[:, :])
        total_actions = len(recent_actions)
        cooperation_rate = cooperations / total_actions
        opponent['CO'] = max(opponent['CO'], cooperation_rate)
        opponent['DO'] = min(opponent['DO'], 1 - cooperation_rate)

    def mirror_opponent_action(self, opponent: dict, history: PlayerHistory):
        if len(history.my_actions) == 0:
            return Action.C
        else:
            previous_action = history.opponent_actions[-2, :]
            return Action.C if np.any(previous_action) else Action.D

class Strategy_EXPLOITATIVE_324(BaseStrategy):
    """
    A strategic approach for the N-Player Public Goods Game that focuses on exploiting opponents while adapting to their behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        previous_round_cooperators = sum(history.opponent_actions[-1, :])
        total_players = self.game_description.n_players
        cooperators_proportion = previous_round_cooperators / total_players
        if cooperators_proportion > 0.5:
            return Action.C
        elif cooperators_proportion > self.threshold:
            return Action.D
        else:
            return Action.D
        if history is not None:
            self.update_threshold(cooperators_proportion)
            self.punish_or_reward_opponents(history)

    def update_threshold(self, cooperators_proportion: float):
        """
        Adjust the adaptive threshold (τ) to ensure it remains exploitative while adapting to game dynamics.
        """
        self.threshold = 0.5 * (self.threshold + cooperators_proportion)

    def punish_or_reward_opponents(self, history: PlayerHistory):
        """
        Punish or reward opponents based on their actions to maintain an exploitative strategy.
        """
        last_round_defectors = [i for i, x in enumerate(history.opponent_actions[-1, :]) if not x]
        if last_round_defectors:
            self.target_opponent = last_round_defectors[0]

    @property
    def target_opponent(self):
        try:
            return self._target_opponent
        except AttributeError:
            self._target_opponent = None
            return None

    @target_opponent.setter
    def target_opponent(self, value: int | None):
        self._target_opponent = value

class Strategy_EXPLOITATIVE_325(BaseStrategy):
    """
    The Adaptive Exploiter strategy exploits a wide range of opponent behaviors while being robust and adaptive.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperators_history = []
        self.avg_cooperators = 0
        self.exploitation_threshold = self.game_description.n_players / 2
        self.cooperation_threshold = self.game_description.n_players / 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if len(self.cooperators_history) >= 5 or state.round_number == self.game_description.n_rounds - 1:
            self.avg_cooperators = sum([sum(round) for round in self.cooperators_history[-5:]]) / min(5, len(self.cooperators_history))
            if self.avg_cooperators < self.game_description.n_players / 3:
                self.exploitation_threshold = 2 * self.game_description.n_players / 3
            elif self.avg_cooperators > 2 * self.game_description.n_players / 3:
                self.cooperation_threshold = self.game_description.n_players / 3
        if prev_cooperators <= self.exploitation_threshold:
            action = Action.D
        else:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        for opponent_actions in history.opponent_actions[:-1]:
            if all((action == False for action in opponent_actions)):
                return Action.D
        self.cooperators_history.append([int(history.my_actions[-1])] + list(history.opponent_actions[-1, :]))
        return action

class Strategy_EXPLOITATIVE_326(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        total_contributions_prev = sum(opponent_actions) + (history.my_actions[-1] if history else False)
        opponents_cooperation_rates = np.mean(history.opponent_actions, axis=0)
        if state.round_number >= self.game_description.n_rounds // 2:
            opponents_cooperation_rates = np.mean(history.opponent_actions[state.round_number - self.game_description.n_rounds // 2:, :], axis=0)
        cooperators = np.where(opponents_cooperation_rates > 0.75)[0]
        defectors = np.where(opponents_cooperation_rates < 0.25)[0]
        if total_contributions_prev <= self.game_description.n_players / 2:
            return Action.D
        p_cooperate = self.game_description.k / self.game_description.n_players
        if random.random() < p_cooperate:
            return Action.C
        for opponent in cooperators:
            if random.random() < 0.8:
                return Action.D
        for opponent in defectors:
            if random.random() < 0.2:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_327(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior and punishes defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.5
        self.prev_round_cooperators = None
        self.prev_round_payoff = None
        self.punish_opponents = [False] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        prev_round_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        avg_payoff_prev_round = np.mean(history.my_payoffs)
        if prev_round_cooperators >= self.game_description.n_players / 2:
            action = Action.D
        elif avg_payoff_prev_round < self.game_description.k / 2:
            action = Action.C
        else:
            action = Action.D
        for i, opponent_defected in enumerate(history.opponent_actions[-1, :]):
            if opponent_defected and history.my_actions[-1]:
                self.punish_opponents[i] = True
                action = Action.D
        self.prev_round_cooperators = prev_round_cooperators
        self.prev_round_payoff = avg_payoff_prev_round
        return action

class Strategy_EXPLOITATIVE_329(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive throughout the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C
        opponent_defectors = sum((1 - action for action in history.opponent_actions[-1, :]))
        if opponent_defectors > self.game_description.n_players / 2:
            return Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        payoff_C = self.game_description.k / self.game_description.n_players * opponent_cooperators
        payoff_D = 1 + self.game_description.k / self.game_description.n_players * opponent_cooperators
        if opponent_cooperators > self.game_description.n_players / 2 and payoff_C > payoff_D:
            return Action.C
        for i, action in enumerate(history.opponent_actions[-1, :]):
            if not action and history.my_payoffs[state.round_number - 1] < payoff_D:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            opponent_cooperators = sum(history.opponent_actions[-2, :])
            if opponent_cooperators > self.game_description.n_players / 2 and payoff_C > payoff_D:
                return Action.C
            else:
                return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_330(BaseStrategy):
    """
    An exploitative strategy that aims to maximize total game payoff by adapting to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tau = 0.5
        self.history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        avg_coop_rate = np.mean(history.opponent_actions[-1, :])
        if state.round_number % 5 == 0 and len(self.history) >= 10:
            coop_rates = np.array([np.mean(round) for round in self.history[-10:]])
            self.tau = np.mean(coop_rates)
        self.history.append(history.opponent_actions[-1, :])
        if avg_coop_rate > self.tau:
            return Action.C
        elif avg_coop_rate == self.tau:
            return Action.D
        else:
            return Action.D

    def _moving_average(self, data, window):
        return np.mean(data[-window:])

class Strategy_EXPLOITATIVE_331(BaseStrategy):
    """
    Exploitative strategy "Conditional Cooperator with Adaptive Threshold".
    
    Adapts to opponent behaviors by adjusting the threshold based on observed cooperation rates.
    Punishes defection in previous rounds to maintain a level of cooperation.
    Cooperates conditionally, ensuring that individual payoffs are maximized when others cooperate.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.k / game_description.n_players
        self.last_round_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperations = np.sum(history.opponent_actions[:state.round_number - 1, :], axis=1)
        total_cooperations = opponent_cooperations + (history.my_actions[:state.round_number - 1] == True)
        avg_cooperation_rate = np.mean(total_cooperations / self.game_description.n_players)
        if self.last_round_action is not None and history.opponent_actions[state.round_number - 2, 0]:
            return Action.D
        if avg_cooperation_rate >= self.threshold:
            action = Action.C
        else:
            action = Action.D
        self.threshold = max(self.threshold - 0.1 * (avg_cooperation_rate - self.threshold), self.game_description.k / self.game_description.n_players)
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        self.last_round_action = action
        return action

class Strategy_EXPLOITATIVE_332(BaseStrategy):
    """
    An adaptive defector that initially cooperates, then defects based on opponents' strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploitation_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < min(self.game_description.n_rounds // 2, 5):
            return Action.C
        avg_c_payoff = np.mean([payoff for payoff, action in zip(history.opponent_payoffs[-1], history.opponent_actions[-1]) if action])
        avg_d_payoff = np.mean([payoff for payoff, action in zip(history.opponent_payoffs[-1], history.opponent_actions[-1]) if not action])
        if avg_c_payoff > avg_d_payoff or (avg_c_payoff == 0 and avg_d_payoff == 0):
            return Action.C
        defectors = sum((1 for action in history.opponent_actions[-1] if not action))
        if defectors > self.game_description.n_players // 2:
            return Action.D
        opponent_strategies = [action for i, action in enumerate(history.opponent_actions[-1]) if i != state.round_number % self.game_description.n_players]
        if all((action or i % 2 == 0 for i, action in enumerate(opponent_strategies))):
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_333(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to maximize its total payoff by adaptively responding 
    to the opponents' behavior while being robust to various strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.5
        self.prev_actions = [None] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_opponent_actions = history.opponent_actions[-1]
        opponents_coop_rate = sum(prev_opponent_actions) / self.game_description.n_players
        if state.round_number % 5 == 0 or state.round_number > self.game_description.n_rounds / 2:
            new_avg_coop_rate = np.mean(history.opponent_actions[-5:, :])
            if abs(new_avg_coop_rate - self.avg_coop_rate) > 0.1:
                self.avg_coop_rate = new_avg_coop_rate
        if opponents_coop_rate < self.avg_coop_rate:
            action = Action.D
        elif opponents_coop_rate >= self.avg_coop_rate:
            if opponents_coop_rate >= 0.5:
                action = Action.C
            else:
                action = Action.D
        return action

class Strategy_EXPLOITATIVE_334(BaseStrategy):
    """
    Adaptive Defection with Cooperate-Defect Switching.
    
    This strategy aims to exploit opponents by adapting to their behavior while maintaining a balance between cooperation and defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        avg_cooperation_rate = np.mean(history.opponent_actions)
        cooperation_threshold = self.game_description.k / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if avg_cooperation_rate >= cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_335(BaseStrategy):
    """
    Adaptive Exploiter Strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if self.game_description.n_players == 2 and state.round_number == 1:
            return Action.C
        prev_round_opponents_actions = history.opponent_actions[-1, :]
        num_defectors = sum((1 for opponent_action in prev_round_opponents_actions if not opponent_action))
        num_cooperators = self.game_description.n_players - 1 - num_defectors
        if num_defectors >= self.game_description.n_players // 2:
            return Action.D
        punish_free_rider = False
        for i, opponent_action in enumerate(prev_round_opponents_actions):
            if not history.my_actions[-1] and opponent_action:
                punish_free_rider = True
                break
        if punish_free_rider:
            return random.choice([Action.D] * 8 + [Action.C])
        imitate_successful_cooperators = False
        total_payoff_for_cooperating = self.game_description.k / self.game_description.n_players * num_cooperators
        if total_payoff_for_cooperating > np.mean(history.my_payoffs):
            imitate_successful_cooperators = True
        if imitate_successful_cooperators:
            return random.choice([Action.C] * 6 + [Action.D])
        if num_cooperators >= self.game_description.n_players // 2:
            self.T += 0.1
        elif num_defectors >= self.game_description.n_players // 2:
            self.T -= 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        expected_payoff_for_cooperating = total_payoff_for_cooperating
        if expected_payoff_for_cooperating > self.T:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_336(BaseStrategy):
    """
    Exploitative strategy that adapts to opponent behavior in public goods games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        opponent_acr = np.mean(history.opponent_actions[:, 0])
        player_avg_payoff = np.mean(history.my_payoffs)
        expected_defect_payoff = 1.0
        expected_cooperate_payoff = self.game_description.k / self.game_description.n_players * self.game_description.n_players
        if player_avg_payoff < expected_defect_payoff:
            self.threshold -= 0.05
        elif player_avg_payoff > expected_cooperate_payoff:
            self.threshold += 0.05
        self.threshold = max(0, min(self.threshold, 1))
        if opponent_acr >= self.threshold or math.isclose(opponent_acr, 0.5):
            return Action.C
        else:
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.D
            return Action.D

class Strategy_EXPLOITATIVE_337(BaseStrategy):
    """
    Exploits opponents' cooperation while minimizing its own contribution.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5
        self.cooperation_rate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_cooperations = np.sum(history.opponent_actions[-1, :])
        self.cooperation_rate = (opponent_cooperations + 1) / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.cooperation_rate > self.threshold:
            return Action.D
        else:
            my_previous_action = history.my_actions[-1]
            payoff_C = 1 - my_previous_action + self.game_description.k / self.game_description.n_players * opponent_cooperations
            payoff_D = 1 + self.game_description.k / self.game_description.n_players * opponent_cooperations
            if payoff_C >= payoff_D:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_338(BaseStrategy):
    """
    The Adaptive Exploiter strategy balances cooperation and exploitation based on opponents' behavior.
    It monitors the average cooperation rate of opponents and adapts its action accordingly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.acr_threshold = 0.5
        self.punishment_rounds_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < self.game_description.n_players:
            return Action.C
        if self.punishment_rounds_remaining > 0:
            self.punishment_rounds_remaining -= 1
            return Action.D
        opponent_actions = history.opponent_actions[-self.game_description.n_players:, :]
        acr = np.mean(opponent_actions)
        if acr < self.acr_threshold:
            return Action.D
        if state.round_number >= self.game_description.n_rounds - self.game_description.n_players:
            return Action.D
        recent_actions = history.opponent_actions[-1, :]
        for i, action in enumerate(recent_actions):
            if not action and history.my_payoffs[-1] < history.opponent_payoffs[-1, i]:
                self.punishment_rounds_remaining = 3
                return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_339(BaseStrategy):
    """
    Exploitative Strategy: Adaptive Cooperation with Retaliation.
    
    Decision Rules:
    1. Initial Round: Cooperate to signal willingness and gather information.
    2. Subsequent Rounds: Calculate average payoffs of cooperators (APC) and defectors (APD),
        then select action based on thresholds CT and DT, retaliating against opponent defection.
    3. Last Round: Defect to maximize individual payoff.
    
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.APC = None
        self.APD = None
        self.CT = None
        self.DT = None
        self.opponent_defected = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        else:
            n_players = self.game_description.n_players
            k = self.game_description.k
            previous_round = history.my_payoffs.shape[0] - 1
            cooperators_actions = history.opponent_actions[previous_round, :]
            defectors_actions = ~cooperators_actions
            if np.sum(cooperators_actions) > 0:
                self.APC = np.mean(history.opponent_payoffs[previous_round, cooperators_actions])
            else:
                self.APC = 0
            if np.sum(defectors_actions) > 0:
                self.APD = np.mean(history.opponent_payoffs[previous_round, defectors_actions])
            else:
                self.APD = 0
            self.CT = self.APC
            self.DT = k / n_players * (n_players / 2)
            if self.APD > self.CT and self.APD > self.DT:
                return Action.D
            elif self.APC > self.APD and self.APC > self.DT:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        for action in history.opponent_actions[-1, :]:
            if not action:
                self.opponent_defected = True
        if self.opponent_defected:
            return Action.D

class Strategy_EXPLOITATIVE_341(BaseStrategy):
    """
    An adaptive exploitative strategy for the N-Player Public Goods Game.
    
    This strategy aims to maximize payoff by exploiting cooperation of others while minimizing personal contribution to the public good when advantageous.
    It adapts its strategy based on observed behavior in previous rounds, making it suitable for a tournament setting where opponents' strategies are unknown.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_cooperators = 0.0
        self.explore_phase = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.explore_phase and state.round_number <= self.game_description.n_players:
            return Action.C if random.random() < 0.6 else Action.D
        avg_cooperators = np.mean(history.opponent_actions[:, :])
        total_payoff_D = 1 + self.game_description.k / self.game_description.n_players * avg_cooperators
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if self.game_description.k / self.game_description.n_players * avg_cooperators > total_payoff_D else Action.D
        elif total_payoff_D >= self.game_description.k:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_players:
            self.explore_phase = False

class Strategy_EXPLOITATIVE_342(BaseStrategy):
    """
    The Adaptive Pessimist strategy aims to maximize its own payoff by exploiting the cooperation of others while minimizing its own contributions to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 1 / self.game_description.n_players else Action.D
        T = (self.game_description.k - 1) / self.game_description.n_players
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if prev_cooperators >= T and prev_cooperators >= (self.game_description.n_players + 1) / 2:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_343(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.fc = 0
        self.at = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_actions = history.opponent_actions[-1, :]
        self.fc = np.sum(opponent_actions) / self.game_description.n_players
        self.at = 0.4 if self.game_description.k / self.game_description.n_players > 0.5 else 0.6
        if math.isclose(self.fc, self.at):
            return Action.D
        elif self.fc < self.at:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_344(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior by defecting when possible and cooperating conditionally.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.c_prev = 0
        self.opponent_actions = []
        self.punish_defectors_list = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number > 1:
            self.c_prev = sum(history.opponent_actions[-2, :])
            has_defector = any((not action for action in history.opponent_actions[-2, :]))
        if self.punish_defectors_list and state.round_number < len(self.punish_defectors_list):
            return Action.D
        if state.round_number > 1:
            if self.c_prev <= self.game_description.n_players / 2:
                self.punish_defectors_list = [state.round_number + i for i in range(2)]
                return Action.D
            elif self.c_prev > self.game_description.n_players / 2 and has_defector:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_345(BaseStrategy):
    """
    An exploitative strategy that adapts to the game's progression and opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        avg_coop_rate = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if state.round_number < self.game_description.n_rounds // 3:
            if avg_coop_rate >= 0.5:
                return random.choices([Action.C, Action.D], weights=[0.8, 0.2])[0]
            else:
                return random.choices([Action.C, Action.D], weights=[0.2, 0.8])[0]
        elif state.round_number < 2 * self.game_description.n_rounds // 3:
            if avg_coop_rate >= 0.33:
                return random.choices([Action.C, Action.D], weights=[0.6, 0.4])[0]
            else:
                return random.choices([Action.C, Action.D], weights=[0.4, 0.6])[0]
        else:
            total_payoff = sum(history.my_payoffs)
            avg_total_payoff = np.mean(np.sum(history.opponent_payoffs, axis=0))
            if total_payoff < avg_total_payoff:
                return random.choices([Action.C, Action.D], weights=[0.4, 0.6])[0]
            else:
                return random.choices([Action.C, Action.D], weights=[0.2, 0.8])[0]
        if state.round_number == self.game_description.n_rounds - 1 and avg_coop_rate == 0.5:
            return random.choice([Action.C, Action.D])
        return Action.D

class Strategy_EXPLOITATIVE_347(BaseStrategy):
    """
    The Adaptive Defector strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_payoff = 0
        self.avg_defect_payoff = 0
        self.coop_count = 0
        self.threshold = game_description.n_players / 2
        self.punishment_phase = False
        self.rounds_punished = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_coop_payoff = np.mean(history.opponent_payoffs[-1, history.opponent_actions[-1] == True])
        avg_defect_payoff = np.mean(history.opponent_payoffs[-1, history.opponent_actions[-1] == False])
        coop_count = sum(history.opponent_actions[-1])
        if (avg_coop_payoff >= avg_defect_payoff or coop_count >= self.threshold) and (not self.punishment_phase):
            return Action.D
        if history.my_actions[-1] == True and sum(history.opponent_actions[-1]) < self.game_description.n_players - 1:
            self.punishment_phase = True
            self.rounds_punished = 0
            return Action.D
        if coop_count > self.coop_count:
            self.threshold -= 1
        elif coop_count < self.coop_count:
            self.threshold += 1
        if state.round_number == self.game_description.n_rounds - 1 and (self.punishment_phase or avg_coop_payoff > avg_defect_payoff):
            return Action.C
        if self.punishment_phase:
            self.rounds_punished += 1
            if self.rounds_punished >= 3:
                self.punishment_phase = False
        self.avg_coop_payoff = avg_coop_payoff
        self.avg_defect_payoff = avg_defect_payoff
        self.coop_count = coop_count
        return Action.C

class Strategy_EXPLOITATIVE_348(BaseStrategy):
    """
    The Adaptive Exploiter strategy capitalizes on opponents' behavior in a Public Goods Game,
    adapting to changing game conditions and maximizing individual payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if self.k > self.n_players / 2 else Action.D
        c_prev = sum(history.opponent_actions[-1, :] == True)
        if c_prev > self.n_players / 2:
            return Action.D
        elif self.sum_payoffs_for_coop(history) >= self.sum_payoffs_for_defect(history):
            return Action.C
        else:
            return Action.D

    def sum_payoffs_for_coop(self, history: PlayerHistory) -> float:
        total = 0
        for t in range(1, len(history.opponent_actions)):
            coop_count = sum([action == True for action in history.opponent_actions[t]])
            payoff = self.k / self.n_players * coop_count
            total += payoff
        return total

    def sum_payoffs_for_defect(self, history: PlayerHistory) -> float:
        total = 0
        for t in range(1, len(history.opponent_actions)):
            defect_count = sum([action == False for action in history.opponent_actions[t]])
            payoff = 1 + self.k / self.n_players * (self.n_players - defect_count)
            total += payoff
        return total

class Strategy_EXPLOITATIVE_349(BaseStrategy):
    """
    An adaptive strategy for the N-Player Public Goods Game that balances between exploiting 
    the public good and minimizing losses when others defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = 0.5
        self.avg_coop_rate = None
        self.memory = []
        self.defection_rate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number <= 2:
            self.memory.append(history.opponent_actions[-1, :])
            return Action.D
        else:
            if not history.my_actions.size:
                avg_coop_rate = 0.5
            elif not history.opponent_actions.size:
                avg_coop_rate = 0.5
            else:
                coop_rates = [sum(round) / self.game_description.n_players for round in history.opponent_actions]
                avg_coop_rate = np.mean(coop_rates)
            if len(self.memory) > 3:
                self.memory.pop(0)
            self.memory.append(history.opponent_actions[-1, :])
            defection_rate = 1 - avg_coop_rate
            if defection_rate > 0.5:
                self.coop_threshold *= 0.9
            if avg_coop_rate > self.coop_threshold:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_350(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Cooperator with Defection Penalties"
    
    This strategy balances cooperation with the need to adapt to exploitative opponents.
    By using a combination of thresholds, penalty mechanisms, and consecutive defection limits,
    it can effectively navigate various opponent behaviors while maintaining an overall cooperative stance.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_count = 0
        self.defect_payoffs = []
        self.coop_payoffs = []
        self.consecutive_defections = 0

    def calculatePayoff(self, n: int, k: float, action: Action, opponent_actions: NDArray[np.bool_]) -> float:
        c_i = 1 if action == Action.C else 0
        total_cooperators = np.sum(opponent_actions) + c_i
        return 1 - c_i + k / n * total_cooperators

    def avg_def_payoff(self, defect_payoffs: list[float]) -> float:
        if len(defect_payoffs) == 0:
            return 0.0
        return sum(defect_payoffs) / len(defect_payoffs)

    def avg_coop_payoff(self, coop_payoffs: list[float]) -> float:
        if len(coop_payoffs) == 0:
            return 0.0
        return sum(coop_payoffs) / len(coop_payoffs)

    def consecutive_defection_limit(self, m: int) -> bool:
        return self.consecutive_defections > m

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n = self.game_description.n_players
        k = self.game_description.k
        θ = 0.5 * n
        m = math.floor(2 * n / k)
        opponent_actions_last_round = history.opponent_actions[-1, :]
        avg_coop = self.coop_count / state.round_number
        if avg_coop > θ and (not self.consecutive_defection_limit(m)):
            action = Action.C
        elif self.avg_def_payoff(self.defect_payoffs) > self.avg_coop_payoff(self.coop_payoffs):
            action = Action.D
            self.consecutive_defections += 1
        else:
            action = Action.C
            self.consecutive_defections = 0
        if action == Action.C:
            self.coop_count += 1
            payoff = self.calculatePayoff(n, k, action, opponent_actions_last_round)
            self.coop_payoffs.append(payoff)
        else:
            payoff = self.calculatePayoff(n, k, action, opponent_actions_last_round)
            self.defect_payoffs.append(payoff)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_351(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy balances exploration, exploitation, and adaptation to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.0
        self.previous_cooperation_rates = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number < 3:
            opponents_cooperated = sum(history.opponent_actions[state.round_number - 1, :])
            if opponents_cooperated > 0:
                return Action.C
            else:
                return Action.D
        else:
            average_cooperation_rate = np.mean(self.previous_cooperation_rates[-3:]) if len(self.previous_cooperation_rates) >= 3 else 0.0
            action = Action.C if average_cooperation_rate > self.cooperation_threshold else Action.D
            if average_cooperation_rate > 0.5:
                self.cooperation_threshold -= 0.1
            else:
                self.cooperation_threshold += 0.1
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.D
            if sum(history.opponent_actions[state.round_number - 1, :]) == 0:
                if len(self.previous_cooperation_rates) > 0 and (not self.previous_cooperation_rates[-1]):
                    return Action.C
                else:
                    return Action.D
        opponents_cooperated = sum(history.opponent_actions[state.round_number - 1, :])
        self.previous_cooperation_rates.append(opponents_cooperated > 0)
        return action

class Strategy_EXPLOITATIVE_352(BaseStrategy):
    """
    A robust and adaptive exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        avg_cooperation_rate = np.mean(history.opponent_actions[-1, :])
        prev_payoff = self._calculate_payoff(history.my_actions[-1], history.opponent_actions[-1])
        avg_payoff = np.mean(history.opponent_payoffs[-1])
        num_cooperators_prev = sum(history.opponent_actions[-2, :] if state.round_number > 1 else [0])
        total_payoff = np.sum(history.my_payoffs)
        if state.round_number < self.game_description.n_rounds / 2:
            return Action.C if avg_cooperation_rate > 0.5 else Action.D
        elif state.round_number >= self.game_description.n_rounds / 2 and state.round_number < 3 * self.game_description.n_rounds / 4:
            return Action.C if prev_payoff > avg_payoff else Action.D
        elif state.round_number >= 3 * self.game_description.n_rounds / 4 and state.round_number < self.game_description.n_rounds:
            if num_cooperators_prev < sum(history.opponent_actions[-1, :]):
                return Action.D
            else:
                return Action.C if total_payoff > avg_payoff else Action.D
        else:
            return Action.D

    def _calculate_payoff(self, my_action: bool, opponent_actions: NDArray[np.bool_]) -> float:
        k = self.game_description.k
        n_players = self.game_description.n_players
        num_cooperators = sum(opponent_actions) + int(my_action)
        payoff = 1 - int(my_action) + k / n_players * num_cooperators
        return payoff

class Strategy_EXPLOITATIVE_353(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behaviors while prioritizing self-interest.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number < self.game_description.n_rounds // 2:
            cooperators = sum(history.opponent_actions[-1, :])
            if cooperators <= self.game_description.n_players // 2:
                return Action.C
            return Action.D
        elif state.round_number < 3 * self.game_description.n_rounds // 4:
            opponent_coop_payoffs = history.opponent_payoffs[-1, np.where(history.opponent_actions[-1, :])[0]]
            opponent_defect_payoffs = history.opponent_payoffs[-1, np.where(~history.opponent_actions[-1, :])[0]]
            if len(opponent_coop_payoffs) > 0 and len(opponent_defect_payoffs) > 0:
                avg_coop_payoff = np.mean(opponent_coop_payoffs)
                avg_defect_payoff = np.mean(opponent_defect_payoffs)
                if avg_coop_payoff > avg_defect_payoff:
                    return Action.C
            return Action.D
        else:
            cooperators = sum(history.opponent_actions[-1, :])
            if cooperators > self.game_description.n_players // 2:
                return Action.C
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        for i, opponent_actions in enumerate(history.opponent_actions[:, :]):
            is_tit_for_tat = np.all(opponent_actions[:-1] == history.my_actions[:-1])
            is_always_coop = np.all(opponent_actions)
            if is_tit_for_tat:
                return Action.C
            elif is_always_coop:
                return Action.D
        if state.round_number == self.game_description.n_rounds // 2 and len(history.my_actions) == 0:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_355(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior while prioritizing self-interest.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.opponent_cooperation_rates = [0] * (game_description.n_players - 1)
        self.lowered_thresholds = [False] * (game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        for i in range(self.game_description.n_players - 1):
            self.opponent_cooperation_rates[i] = history.opponent_actions[-1, i]
        average_cooperation_rate = sum(self.opponent_cooperation_rates) / (self.game_description.n_players - 1)
        if average_cooperation_rate >= 0.5:
            self.cooperation_threshold = 0.7
        else:
            self.cooperation_threshold = 0.3
        for i in range(self.game_description.n_players - 1):
            if state.round_number <= 3 and history.opponent_actions[state.round_number - 2, i]:
                self.lowered_thresholds[i] = True
                self.cooperation_threshold -= 0.1
        total_cooperators = sum(self.opponent_cooperation_rates) + history.my_actions[-1]
        if total_cooperators >= self.cooperation_threshold * self.game_description.n_players and total_cooperators > self.game_description.k / self.game_description.n_players * self.game_description.n_players / 2:
            action = Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        else:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_356(BaseStrategy):
    """
    An adaptive strategy that balances exploitation with adaptability.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_opponent_actions = history.opponent_actions[-1]
        previous_my_action = history.my_actions[-1]
        for opponent in range(self.game_description.n_players - 1):
            if not previous_opponent_actions[opponent]:
                return Action.D
        if sum(previous_opponent_actions) < self.game_description.n_players / 2:
            return Action.D
        max_payoff_opponent = None
        max_payoff = -math.inf
        for opponent in range(self.game_description.n_players - 1):
            if previous_opponent_actions[opponent] and history.opponent_payoffs[-1, opponent] > max_payoff:
                max_payoff_opponent = opponent
                max_payoff = history.opponent_payoffs[-1, opponent]
        if max_payoff_opponent is not None:
            return Action.C if previous_opponent_actions[max_payoff_opponent] else Action.D
        if random.random() < 0.1:
            return random.choice([Action.C, Action.D])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif self.game_description.n_players == 2:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_357(BaseStrategy):
    """
    An exploitative strategy that adapts to the game's history and parameters.
    It encourages cooperation while being prepared to defect when necessary.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.alpha = 0.0
        self.delta = (game_description.k - 1) / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.game_description.k / self.game_description.n_players else Action.D
        my_payoff = history.my_payoffs[-1]
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        total_cooperators = opponent_cooperators + (history.my_actions[-1] == True)
        if my_payoff > 1:
            self.alpha += 0.1
        else:
            self.alpha -= 0.1
        p_C = max(0, min(1, self.alpha + self.delta))
        return Action.C if random.random() < p_C else Action.D

    def __getstate__(self):
        state = self.__dict__.copy()
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)

class Strategy_EXPLOITATIVE_358(BaseStrategy):
    """
    Exploitative strategy that adapts to opponent behavior and punishes non-cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.pi_c = 0
        self.pi_d = 0
        self.rc = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        previous_round_cooperators = sum(history.opponent_actions[-1, :])
        self.rc = previous_round_cooperators / self.game_description.n_players
        my_last_action = history.my_actions[-1]
        opponent_payoffs = history.opponent_payoffs[-1, :]
        if my_last_action:
            my_last_payoff = self.game_description.k / self.game_description.n_players * previous_round_cooperators
        else:
            my_last_payoff = 1 + self.game_description.k / self.game_description.n_players * previous_round_cooperators
        if len(history.my_actions) >= 2:
            second_last_action = history.my_actions[-2]
            if second_last_action:
                second_last_payoff = self.game_description.k / self.game_description.n_players * sum(history.opponent_actions[-2, :])
            else:
                second_last_payoff = 1 + self.game_description.k / self.game_description.n_players * sum(history.opponent_actions[-2, :])
            if my_last_action and second_last_action:
                self.pi_c = (my_last_payoff + second_last_payoff) / 2
                self.pi_d = 0
            elif not my_last_action and (not second_last_action):
                self.pi_c = 0
                self.pi_d = (my_last_payoff + second_last_payoff) / 2
            else:
                self.pi_c = max(my_last_payoff, second_last_payoff)
                self.pi_d = min(my_last_payoff, second_last_payoff)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif self.rc > 0.5 and random.random() < 1 - self.game_description.k / self.game_description.n_players * self.rc:
            return Action.C
        elif self.pi_c < self.pi_d:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_360(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Public Goods Twist (ATP) strategy.
    
    This strategy exploits the public goods aspect of the game by adapting to 
    opponents' actions and payoffs. It incorporates elements of tit-for-tat and 
    aims to outperform other strategies in the tournament.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        C_t_minus_1 = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history.my_actions.size else 0)
        if C_t_minus_1 >= self.theta * self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        if history.my_payoffs.size and history.opponent_actions.size and history.opponent_payoffs.size:
            payoff_coop = 1 - int(history.my_actions[-1]) + self.game_description.k / self.game_description.n_players * C_t_minus_1
            payoff_defect = 1 - int(history.my_actions[-1]) + self.game_description.k / self.game_description.n_players * sum(history.opponent_actions[-1, :])
            if payoff_coop >= payoff_defect:
                self.theta += 0.05
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_361(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its own payoff by adapting to 
    the opponents' behavior and exploiting their cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.5
        self.imitate_strategy = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choices([Action.C, Action.D], weights=[self.cooperate_prob, 1 - self.cooperate_prob])[0]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        if opponent_cooperators > self.game_description.n_players // 2:
            return Action.D
        my_payoff = history.my_payoffs[-1]
        average_opponent_payoff = np.mean(history.opponent_payoffs[-1, :])
        if my_payoff < average_opponent_payoff and history.my_actions[-1] == False:
            self.cooperate_prob = 0.7
        if state.round_number >= self.game_description.n_rounds // 2:
            total_payoffs = np.sum(history.opponent_payoffs, axis=0)
            max_total_payoff = np.max(total_payoffs)
            if max_total_payoff > np.sum(history.my_payoffs):
                max_payoff_indices = np.where(total_payoffs == max_total_payoff)[0]
                if len(max_payoff_indices) > 1:
                    max_payoff_index = random.choice(max_payoff_indices)
                else:
                    max_payoff_index = max_payoff_indices[0]
                self.imitate_strategy = history.opponent_actions[-1, max_payoff_index]
        if self.imitate_strategy is not None:
            return Action.C if self.imitate_strategy else Action.D
        return random.choices([Action.C, Action.D], weights=[self.cooperate_prob, 1 - self.cooperate_prob])[0]

class Strategy_EXPLOITATIVE_362(BaseStrategy):
    """
    An adaptive exploitative strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.n_players / 3
        self.avg_cooperators = []
        self.last_round_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :])
        self.last_round_cooperators = [opponent_action for opponent_action in history.opponent_actions[-1, :]]
        if num_cooperators < self.threshold:
            action = Action.D
        elif num_cooperators >= self.threshold and self.get_payoff(Action.C, history) > self.get_payoff(Action.D, history):
            action = Action.C
        else:
            action = Action.D
        if self.is_opponent_defected(history) and self.get_payoff(Action.C, history) > self.get_payoff(Action.D, history):
            action = Action.D
        if state.round_number % 3 == 0:
            self.avg_cooperators.append(num_cooperators / self.game_description.n_players)
            if sum(self.avg_cooperators) < self.threshold:
                action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

    def get_payoff(self, action: Action, history: PlayerHistory) -> float:
        num_cooperators = sum(history.opponent_actions[-1, :])
        payoff = 1 - int(action == Action.C) + self.game_description.k / self.game_description.n_players * num_cooperators
        return payoff

    def is_opponent_defected(self, history: PlayerHistory) -> bool:
        opponent_actions = [opponent_action for opponent_action in history.opponent_actions[-1, :]]
        return any((not opponent_action for opponent_action in opponent_actions))

class Strategy_EXPLOITATIVE_363(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_cooperators = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_cooperators = sum(history.opponent_actions[-1, :]) + int(bool(history.my_actions[-1]))
        avg_cooperators = self.get_average_cooperators(n_cooperators, history)
        if state.round_number < self.game_description.n_players:
            if n_cooperators <= self.game_description.k - 1:
                return Action.D
            else:
                return Action.C
        elif avg_cooperators >= self.game_description.k / 2:
            return Action.D
        elif history.my_payoffs[-1] >= 1.5 * (self.game_description.k / self.game_description.n_players):
            return Action.C
        else:
            return Action.D
        if state.round_number >= self.game_description.n_rounds - self.game_description.n_players:
            if avg_cooperators <= self.game_description.k - 1:
                return Action.D

    def get_average_cooperators(self, n_cooperators: int, history: PlayerHistory):
        window_size = min(len(history.my_actions), self.game_description.n_players)
        total_coop = sum(history.opponent_actions[-window_size:, :].sum(axis=0)) + sum(history.my_actions[-window_size:])
        return total_coop / (self.game_description.n_players * window_size)

class Strategy_EXPLOITATIVE_364(BaseStrategy):
    """
    Adaptive Exploiter (AE) balances exploitation and adaptability to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.APC = None
        self.APD = None
        self.number_of_cooperators = None
        self.number_of_defectors = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        total_contributions = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        self.number_of_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        self.number_of_defectors = self.game_description.n_players - self.number_of_cooperators
        if self.number_of_cooperators == 0:
            APC = 0
        else:
            APC = self.game_description.k / self.game_description.n_players * total_contributions / self.number_of_cooperators
        if self.number_of_defectors == 0:
            APD = 0
        else:
            APD = (1 + self.game_description.k / self.game_description.n_players * total_contributions) / self.number_of_defectors
        if APC >= APD or math.isclose(APC, APD):
            action = Action.C
        else:
            action = Action.D
        for i in range(self.game_description.n_players - 1):
            if not history.opponent_actions[-1, i] and APC >= APD:
                action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_365(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior by tracking cooperation scores.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators = [0] * (game_description.n_players - 1)
        self.cooperation_scores = [0.0] * (game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        for i in range(self.game_description.n_players - 1):
            if history.opponent_actions[-1, i]:
                self.total_cooperators[i] += 1
                self.cooperation_scores[i] = self.total_cooperators[i] / state.round_number
        num_opponents = self.game_description.n_players - 1
        all_opponents_played_c = all(history.opponent_actions[-1, :])
        only_one_defector = sum((1 for j in range(num_opponents) if not history.opponent_actions[-1, j])) == 1
        if state.round_number < self.game_description.n_rounds - 1:
            if all_opponents_played_c or only_one_defector:
                return Action.C
            elif sum(self.total_cooperators) <= num_opponents // 2:
                return Action.C
            elif any((score > 0.5 for score in self.cooperation_scores)):
                return Action.D
            else:
                return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_366(BaseStrategy):
    """
    Exploitative strategy that adapts to opponent behavior by tracking average cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is None:
            self.avg_cooperators = 1.0
            return Action.C
        total_cooperators = sum(np.sum(history.opponent_actions[:-1], axis=1)) + state.round_number - 1
        self.avg_cooperators = total_cooperators / state.round_number
        threshold = min(1, max(0, self.game_description.k / self.game_description.n_players - 0.5))
        if self.avg_cooperators > threshold:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_367(BaseStrategy):
    """
    The Adaptive Explorer strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(game_description.n_players / 2)
        self.punishment_mode = False
        self.rounds_since_punishment = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions = history.opponent_actions[-1]
        my_action = Action.D
        for i, actions in enumerate(opponent_actions):
            if len(history.my_payoffs) > 1 and actions and (history.opponent_payoffs[-2][i] < self.game_description.k / self.game_description.n_players * sum((1 for a in history.opponent_actions[:, i] if a))):
                my_action = Action.D
                self.punishment_mode = True
                break
        if not self.punishment_mode:
            prev_round_cooperators = sum(opponent_actions)
            if prev_round_cooperators >= self.cooperation_threshold:
                my_action = Action.D
            elif self.game_description.k > self.game_description.n_players / 2 and (state.round_number - 3) % 3 != 0:
                my_action = Action.C
        if (state.round_number + 1) % 3 == 0:
            avg_payoff = np.mean(history.my_payoffs[-3:])
            expected_coop_payoff = self.game_description.k / self.game_description.n_players * self.game_description.n_players
            if avg_payoff < expected_coop_payoff and (not self.punishment_mode):
                self.cooperation_threshold -= 1
        if self.punishment_mode:
            self.rounds_since_punishment += 1
            if self.rounds_since_punishment >= 2:
                self.punishment_mode = False
                self.rounds_since_punishment = 0
        if state.round_number == self.game_description.n_rounds - 1:
            my_action = Action.D
        return my_action

class Strategy_EXPLOITATIVE_370(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to maximize payoffs by exploiting the cooperative behavior of other players while minimizing losses when others defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :])
        prev_defectors = self.game_description.n_players - 1 - prev_cooperators
        ACP = self.game_description.k / self.game_description.n_players * prev_cooperators
        ADP = 1 + self.game_description.k / self.game_description.n_players * prev_cooperators
        action = Action.C if ACP > ADP else Action.D
        if history.my_payoffs[-1] > ACP:
            self.threshold -= 0.1
        elif history.my_payoffs[-1] < ADP:
            self.threshold += 0.1
        if ACP == ADP:
            action = Action.C if random.random() < 0.5 else Action.D
        return action

class Strategy_EXPLOITATIVE_371(BaseStrategy):
    """
    A strategy that leverages the game's history and parameters to adaptively exploit opponents' behaviors while minimizing cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_actions = history.opponent_actions[-1, :]
        total_cooperators = sum(opponent_actions) + (history.my_actions[-1] == True)
        avg_payoff_prev = self.game_description.k / self.game_description.n_players * total_cooperators
        threshold = self.game_description.k / self.game_description.n_players
        if avg_payoff_prev >= threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_372(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_C = 0
        self.avg_payoff_D = 1
        self.num_cooperators_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if self.avg_payoff_C == 0 and self.avg_payoff_D == 1:
            self.avg_payoff_C = self.get_payoff_C(history)
            self.avg_payoff_D = self.get_payoff_D(history)
        elif history.my_actions[-1]:
            self.avg_payoff_C = (self.avg_payoff_C * state.round_number + self.get_payoff_C(history)) / (state.round_number + 1)
        else:
            self.avg_payoff_D = (self.avg_payoff_D * state.round_number + self.get_payoff_D(history)) / (state.round_number + 1)
        if history.opponent_actions.size > 0:
            self.num_cooperators_prev = np.sum(history.opponent_actions[-1, :])
        if state.round_number < self.game_description.n_rounds - 1:
            if self.avg_payoff_C >= self.avg_payoff_D and self.num_cooperators_prev >= math.floor(self.game_description.n_players / self.game_description.k):
                return Action.C
            else:
                return Action.D
        return Action.D

    def get_payoff_C(self, history: PlayerHistory) -> float:
        num_cooperators = np.sum(history.opponent_actions[-1, :]) + 1
        return self.game_description.k / self.game_description.n_players * num_cooperators

    def get_payoff_D(self, history: PlayerHistory) -> float:
        num_cooperators = np.sum(history.opponent_actions[-1, :])
        return 1 + self.game_description.k / self.game_description.n_players * num_cooperators

class Strategy_EXPLOITATIVE_373(BaseStrategy):
    """
    Exploitative strategy "K-Tracker" designed to adaptively track the effectiveness of cooperation and defect when beneficial.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defection_after_coop = False

    def calculate_avg_payoff_c(self, history: PlayerHistory) -> float:
        """Calculate average payoff per cooperator in the previous round."""
        prev_round_actions = history.opponent_actions[-1, :]
        cooperators = sum(prev_round_actions)
        if cooperators == 0:
            return 0.0
        prev_round_payoffs = history.opponent_payoffs[-1, :]
        total_payoff_c = sum([payoff for action, payoff in zip(prev_round_actions, prev_round_payoffs) if action])
        return total_payoff_c / cooperators

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """Choose an action based on the current state and history."""
        if state.round_number <= 2:
            return Action.C
        avg_payoff_c = self.calculate_avg_payoff_c(history)
        if avg_payoff_c >= self.game_description.k / self.game_description.n_players:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        for opponent_action in prev_round_actions:
            if opponent_action and (not self.defection_after_coop) or (not opponent_action and self.defection_after_coop):
                self.defection_after_coop = True
                return Action.D
        prev_round_payoffs = history.opponent_payoffs[-1, :]
        if all((action == 0.0 or payoff > 0.0 for action, payoff in zip(prev_round_actions, prev_round_payoffs))):
            self.defection_after_coop = False
            return Action.C
        return Action.D

    def __call__(self, state: GameState, history: None) -> Action:
        """Special case for initial (zeroth) round where history is None."""
        return Action.C

class Strategy_EXPLOITATIVE_374(BaseStrategy):
    """
    An adaptive exploitative strategy for the public goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_cooperation_rate = 0.0
        self.threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponents_actions = history.opponent_actions[1:, :]
        avg_cooperation_rate_opponents = np.mean(opponents_actions)
        self.avg_cooperation_rate = (self.avg_cooperation_rate * state.round_number + avg_cooperation_rate_opponents) / (state.round_number + 1)
        if history.my_payoffs[-1] > np.mean(history.opponent_payoffs[-1, :]):
            self.threshold -= 0.05
        else:
            self.threshold += 0.05
        self.threshold = max(0.01, min(self.threshold, 0.99))
        if self.avg_cooperation_rate < self.threshold or state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_375(BaseStrategy):
    """
    A strategy that balances exploitation with adaptability in a public goods game.
    It explores opponents' behavior initially, profiles them to identify potential "soft" targets,
    and adapts its cooperation/defection decision based on the expected number of cooperators and opponent pool's cooperation rate.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_coop_rate = 0.5

    def calculate_opponent_coop_rate(self, history: PlayerHistory) -> float:
        """Estimate the opponent pool's cooperation rate based on historical data."""
        if history.my_actions.size == 1:
            return self.opponent_coop_rate
        else:
            opponent_cooperations = np.sum(history.opponent_actions[:-1, :], axis=0)
            avg_opponent_cooperation = np.mean(opponent_cooperations) / (self.game_description.n_rounds - 1)
            return avg_opponent_cooperation

    def calculate_expected_cooperators(self, coop_rate: float, n_players: int) -> int:
        """Calculate the expected number of cooperators in the current round."""
        return math.ceil(coop_rate * n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.opponent_coop_rate = 0.5
            return Action.C
        self.opponent_coop_rate = self.calculate_opponent_coop_rate(history)
        expected_cooperators = self.calculate_expected_cooperators(self.opponent_coop_rate, self.game_description.n_players)
        if state.round_number <= 2:
            return Action.D if self.opponent_coop_rate > 0.5 else Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D if self.opponent_coop_rate > 0.7 else Action.C if expected_cooperators >= self.game_description.k / self.game_description.n_players else Action.D
        else:
            return Action.D if expected_cooperators < self.game_description.k / self.game_description.n_players else Action.C if self.opponent_coop_rate < self.game_description.k / self.game_description.n_players else Action.D

class Strategy_EXPLOITATIVE_376(BaseStrategy):
    """
    An exploitative strategy that adapts to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold_theta = 0.4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        avg_coop_prev_round = self.calculate_average_cooperation(history, state.round_number - 1)
        if avg_coop_prev_round > self.threshold_theta:
            return Action.D
        p = self.calculate_probability(history, state.round_number)
        if random.random() < p:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

    def calculate_average_cooperation(self, history: PlayerHistory, t: int):
        coop_count = sum((1 for opponent in history.opponent_actions[t] if opponent))
        return coop_count / (self.game_description.n_players - 1)

    def calculate_probability(self, history: PlayerHistory, t: int):
        total_coop_count = sum((sum(opponents) for opponents in history.opponent_actions[:t]))
        avg_num_cooperators = total_coop_count / ((t - 1) * (self.game_description.n_players - 1))
        return self.game_description.k / self.game_description.n_players * avg_num_cooperators

class Strategy_EXPLOITATIVE_377(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.OCR = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        OCR = sum(history.opponent_actions[-1, :]) / (self.game_description.n_players - 1)
        EPD = 1 + self.game_description.k / self.game_description.n_players * OCR * (self.game_description.n_players - 1)
        EPC = self.game_description.k / self.game_description.n_players * OCR * (self.game_description.n_players - 1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if EPD > EPC + 0.5:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_378(BaseStrategy):
    """
    The Adaptive Exploiter is a robust and exploitative strategy designed for the N-Player Public Goods Game.
    This strategy leverages the game's parameters and history to maximize its payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop = 0
        self.num_cooperators = 0
        self.T = game_description.k / game_description.n_players * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choices([Action.C, Action.D], weights=[1 / self.game_description.n_players, (self.game_description.n_players - 1) / self.game_description.n_players])[0]
        self.avg_coop = np.mean(history.opponent_actions[-1, :])
        self.num_cooperators = sum(history.opponent_actions[-1, :])
        if self.avg_coop == 0:
            return Action.D
        if self.avg_coop < 1 / self.game_description.n_players or state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.num_cooperators >= self.T:
            return Action.C
        else:
            return Action.D
        if state.round_number % (self.game_description.n_rounds // 5) == 0 and state.round_number != 0:
            if self.avg_coop > 0.5:
                self.T *= 1.05
            else:
                self.T /= 1.05

class Strategy_EXPLOITATIVE_379(BaseStrategy):
    """
    The Adaptive Raider strategy is designed to exploit opponents' cooperative tendencies 
    while minimizing losses from exploitation by others. This strategy leverages game history 
    and parameters to adapt its behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_coop_rate = np.mean(history.opponent_actions)
        t = len(history.my_actions)
        if t < self.game_description.n_rounds / 2:
            p = self.game_description.k / self.game_description.n_players
            return Action.C if random.random() < p else Action.D
        if avg_coop_rate > 0.5:
            return Action.C
        elif avg_coop_rate < 0.3:
            return Action.D
        else:
            p = self.game_description.k / self.game_description.n_players * avg_coop_rate
            return Action.C if random.random() < p else Action.D
        if t == self.game_description.n_rounds - 1:
            return Action.D
        if avg_coop_rate == 0:
            return Action.C
        elif avg_coop_rate == 1:
            return Action.D

class Strategy_EXPLOITATIVE_380(BaseStrategy):
    """
    An exploitative strategy designed to adapt to opponent behaviors in an N-player public goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_payoff = []
        self.avg_defect_payoff = []
        self.cooperate_prob = 0.5
        self.theta = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        elif state.round_number < self.game_description.n_rounds - 1:
            coop_avg = np.mean(self.avg_coop_payoff) if self.avg_coop_payoff else 0.0
            defect_avg = np.mean(self.avg_defect_payoff) if self.avg_defect_payoff else 0.0
            if coop_avg > defect_avg + self.theta:
                self.cooperate_prob = 0.7
            elif coop_avg < defect_avg - self.theta:
                self.cooperate_prob = 0.3
            else:
                self.cooperate_prob = 0.5
            return Action.C if random.random() < self.cooperate_prob else Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_381(BaseStrategy):
    """
    The Adaptive Defector strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive.
    This strategy relies on the game parameters (n and k) and the history of the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.last_round_history = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        observation_phase = state.round_number <= self.game_description.n_rounds // 2
        if observation_phase:
            return Action.C if state.round_number % 2 == 0 else Action.D
        opponent_actions = history.opponent_actions[:-1] if history is not None else []
        average_cooperation_rate = sum((1 for actions in opponent_actions for action in actions if action)) / (len(opponent_actions) * self.game_description.n_players - 1)
        if state.round_number == self.game_description.n_rounds - 1:
            self.last_round_history = history
        if average_cooperation_rate > 0.5:
            return Action.D
        else:
            p = self.game_description.k / self.game_description.n_players * (1 - average_cooperation_rate)
            return Action.C if random.random() < p else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_382(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' cooperative tendencies while minimizing losses from mutual defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defect_prob = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        prev_round_actions = history.opponent_actions[-1]
        prev_coops = sum(prev_round_actions)
        prev_avg_payoffcoop = np.mean([payoff for payoff, action in zip(history.opponent_payoffs[-1], prev_round_actions) if action])
        prev_avg_payoffdefect = np.mean([payoff for payoff, action in zip(history.opponent_payoffs[-1], prev_round_actions) if not action])
        if prev_avg_payoffdefect > prev_avg_payoffcoop:
            self.defect_prob += 0.1
        if prev_coops >= self.game_description.n_players / 2:
            coop_prob = (self.game_description.k - 1) / self.game_description.n_players
        else:
            coop_prob = 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if random.random() < coop_prob:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_383(BaseStrategy):
    """
    The Adaptive Parasite strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.observation_phase = True
        self.cooperation_rate = 0.0
        self.avg_payoff_coop = 0.0
        self.avg_payoff_defect = 0.0
        self.tau = game_description.n_players / (game_description.k + 1)
        self.adjusted_rounds_left = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        opponent_actions_prev_round = history.opponent_actions[-1, :]
        num_cooperators_prev_round = sum(opponent_actions_prev_round)
        my_action_prev_round = bool(history.my_actions[-1])
        my_payoff_prev_round = history.my_payoffs[-1]
        if self.observation_phase and state.round_number <= math.ceil(self.game_description.n_rounds / 3):
            self.cooperation_rate = num_cooperators_prev_round / self.game_description.n_players
            action_prob = random.random()
            return Action.C if action_prob < self.cooperation_rate else Action.D
        if state.round_number <= math.ceil(self.game_description.n_rounds / 3):
            self.avg_payoff_coop += my_payoff_prev_round * my_action_prev_round
            self.avg_payoff_defect += my_payoff_prev_round * (not my_action_prev_round)
        if state.round_number > math.ceil(self.game_description.n_rounds / 3) and self.observation_phase:
            self.observation_phase = False
        if not self.observation_phase:
            if self.avg_payoff_coop < self.avg_payoff_defect or num_cooperators_prev_round < self.tau:
                return Action.D
        if state.round_number > math.ceil(self.game_description.n_rounds / 3) and (not self.observation_phase):
            opponent_cooperation_rate = num_cooperators_prev_round / self.game_description.n_players
            if opponent_cooperation_rate > 0.25 + self.cooperation_rate:
                self.observation_phase = True
                self.adjusted_rounds_left = 1
        if state.round_number >= math.floor(self.game_description.n_rounds * (2 / 3)):
            return Action.D
        if self.adjusted_rounds_left > 0:
            action_prob = random.random()
            return Action.C if action_prob < opponent_cooperation_rate else Action.D
            self.adjusted_rounds_left -= 1
        return Action.C

class Strategy_EXPLOITATIVE_385(BaseStrategy):
    """
    Exploits cooperative tendencies of opponents while minimizing contributions to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.n_players / 2
        self.cooperation_rate = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C
        if history is not None:
            cooperators = sum(history.opponent_actions[-1, :])
            self.cooperation_rate = (self.cooperation_rate * (state.round_number - 1) + cooperators) / state.round_number
            if self.cooperation_rate > self.game_description.k:
                self.threshold += 0.1
            elif self.cooperation_rate < self.game_description.k:
                self.threshold -= 0.1
        if cooperators >= self.threshold:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_386(BaseStrategy):
    """
    A robust and exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < self.game_description.n_players // 2:
            avg_coop_rate = sum([1 if action else 0 for action in history.my_actions[:state.round_number]]) / (self.game_description.n_players * state.round_number)
            if avg_coop_rate > self.avg_coop_rate_threshold:
                return Action.C
            return Action.D
        coop_payoff_avg = np.mean([payoff for payoff, action in zip(history.my_payoffs[:state.round_number], history.my_actions[:state.round_number]) if action])
        defector_payoff_avg = np.mean([payoff for payoff, action in zip(history.my_payoffs[:state.round_number], history.my_actions[:state.round_number]) if not action])
        if coop_payoff_avg > defector_payoff_avg:
            return Action.C
        elif coop_payoff_avg == defector_payoff_avg:
            opponent_coop_count = sum([1 for actions in history.opponent_actions[:state.round_number, :] if np.any(actions)])
            if opponent_coop_count >= self.game_description.n_players // 2:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_387(BaseStrategy):
    """
    An exploitative strategy that adapts to the behavior of opponents by cooperating initially 
    and then responding to the average payoffs of cooperators and defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperators_payoff_sum = 0.0
        self.defectors_payoff_sum = 0.0
        self.cooperators_count = 0
        self.defectors_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        for opponent_payoff, opponent_action in zip(history.opponent_payoffs[-1], history.opponent_actions[-1]):
            if opponent_action:
                self.cooperators_payoff_sum += opponent_payoff
                self.cooperators_count += 1
            else:
                self.defectors_payoff_sum += opponent_payoff
                self.defectors_count += 1
        avg_cooperators_payoff = self.cooperators_payoff_sum / max(self.cooperators_count, 1)
        avg_defectors_payoff = self.defectors_payoff_sum / max(self.defectors_count, 1)
        if avg_cooperators_payoff > avg_defectors_payoff:
            return Action.C
        elif avg_cooperators_payoff < avg_defectors_payoff:
            return Action.D
        else:
            return Action.D if random.random() < 0.5 else Action.C

class Strategy_EXPLOITATIVE_388(BaseStrategy):
    """
    An exploitative strategy for N-Player Public Goods Game that adapts to game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_rate = 0.5
        self.consecutive_coops = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        CR = np.sum(prev_round_opponent_actions) / (self.game_description.n_players - 1)
        AP = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1, :])
        if CR > 0.5 and AP > 1:
            self.consecutive_coops += 1
            action = Action.C
        else:
            self.consecutive_coops = max(0, self.consecutive_coops - 1)
            action = Action.D
        if self.consecutive_coops >= 2:
            self.coop_rate += 0.1
        elif self.consecutive_coops <= 0 and len(history.my_actions) > 2:
            self.coop_rate -= 0.1
            self.coop_rate = max(self.coop_rate, 0)
        if random.random() < self.coop_rate:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_389(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game.
    
    This strategy initially defects to gather information, then adapts its behavior based on the average payoffs of cooperators and defectors in previous rounds. It also introduces a punishment mechanism to deter opponents from exploiting it.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploiters = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        avg_coop_payoff = np.mean(history.my_payoffs[history.my_actions])
        avg_defect_payoff = np.mean(history.my_payoffs[~history.my_actions])
        if avg_coop_payoff > avg_defect_payoff:
            action = Action.C
        else:
            action = Action.D
        for opponent_idx, (coop, payoff) in enumerate(zip(history.opponent_actions[-1, :], history.opponent_payoffs[-1, :])):
            if coop and len(self.exploiters) < self.game_description.n_players // 2:
                if sum(history.opponent_actions[-1, :]) < self.game_description.n_players // 2:
                    self.exploiters.add(opponent_idx)
            elif not coop and payoff > avg_defect_payoff * self.game_description.k / self.game_description.n_players:
                if sum(history.opponent_actions[-1, :]) >= self.game_description.n_players // 2:
                    self.exploiters.add(opponent_idx)
        for opponent_idx in self.exploiters:
            if history.opponent_actions[-1, opponent_idx]:
                action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_390(BaseStrategy):
    """
    Exploit a wide range of opponent behaviors by cooperating when the number of cooperators is low and defecting when it's high.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        previous_round_cooperators = sum((1 for action in history.opponent_actions[-1, :] if action))
        if state.round_number < self.game_description.n_rounds - 1:
            if previous_round_cooperators <= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_392(BaseStrategy):
    """
    Exploitative strategy that adapts to opponent behavior in the N-player public goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.6
        self.punishment_rounds = 2
        self.window_size = 5
        self.opponent_cooperation_rates = None
        self.punished_opponents = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.opponent_cooperation_rates is None:
            self.opponent_cooperation_rates = np.zeros(self.game_description.n_players - 1)
        for i, action in enumerate(history.opponent_actions[-1, :]):
            if action:
                self.opponent_cooperation_rates[i] += 1

        def punish(opponent_id):
            self.punished_opponents.add(opponent_id)
            return Action.D
        if state.round_number >= 3:
            average_opponents_cooperation_rate = np.mean(self.opponent_cooperation_rates[-self.window_size:])
            if average_opponents_cooperation_rate > self.cooperation_threshold:
                p_c = self.game_description.k / self.game_description.n_players
                if random.random() < p_c:
                    return Action.C
                else:
                    return Action.D
            else:
                for opponent_id in range(self.game_description.n_players - 1):
                    if history.opponent_actions[-2, opponent_id] and history.my_payoffs[-1] == 0:
                        return punish(opponent_id)
                return Action.D
        else:
            opponents_cooperation_rate = np.mean(history.opponent_actions[-1, :])
            if opponents_cooperation_rate > self.cooperation_threshold or state.round_number == 1:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            opponents_cooperation_rate_last_round = np.mean(history.opponent_actions[-2, :])
            if opponents_cooperation_rate_last_round > self.cooperation_threshold:
                return Action.C

class Strategy_EXPLOITATIVE_393(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior in an N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate = []
        self.exploitable_opponents = set()
        self.opponent_cooperation_rates = [0.0] * (game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 3:
            return Action.C
        self.cooperation_rate.append(history.my_actions[-1])
        avg_cooperation_rate = np.mean(self.cooperation_rate)
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if not opponent_action and history.my_actions[-1]:
                self.exploitable_opponents.add(i)
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            self.opponent_cooperation_rates[i] += (opponent_action - self.opponent_cooperation_rates[i]) / (state.round_number + 1)
            if self.opponent_cooperation_rates[i] > 0.8:
                return Action.C
            elif self.opponent_cooperation_rates[i] < 0.5 and i in self.exploitable_opponents:
                return Action.D
        if avg_cooperation_rate > 0.5:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_394(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior by cooperating 
    when the average payoff of cooperators is higher than defectors, and 
    defecting otherwise.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_probability = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_probability else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_cooperators = history.opponent_actions[-1, :] == True
        average_cooperator_payoff = np.mean(history.opponent_payoffs[-1, opponent_cooperators])
        average_defector_payoff = np.mean(history.opponent_payoffs[-1, ~opponent_cooperators])
        if math.isclose(average_cooperator_payoff, average_defector_payoff):
            return Action.C if random.random() < self.cooperate_probability else Action.D
        return Action.C if average_cooperator_payoff > average_defector_payoff else Action.D

class Strategy_EXPLOITATIVE_395(BaseStrategy):
    """
    Exploitative strategy designed to adapt to opponents' behaviors and maximize own payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_prob = game_description.k / game_description.n_players
        self.avg_payoff_per_cooperator = 0.0
        self.defect_next_round = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < self.cooperate_prob else Action.D
        opponent_cooperation_rate = np.mean(history.opponent_actions[-1, :])
        avg_opponent_payoff = np.mean(history.opponent_payoffs[-1, :])
        if state.round_number == 1:
            self.avg_payoff_per_cooperator = history.my_payoffs[0]
        else:
            cooperators = np.where(history.opponent_actions[-2, :] | history.my_actions[-2])[0]
            self.avg_payoff_per_cooperator = np.mean(history.opponent_payoffs[-2, :][cooperators])
        if opponent_cooperation_rate < 0.5 or avg_opponent_payoff < 0.5:
            return Action.D
        if self.avg_payoff_per_cooperator < 0.5 * self.game_description.k / self.game_description.n_players:
            return Action.D
        if self.avg_payoff_per_cooperator >= self.game_description.k / self.game_description.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_396(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_payoff_window_size = 5
        self.avg_defect_payoff_window_size = 5
        self.adaptive_adjustment_threshold = 0.1
        self.cooperative_trend_threshold = self.game_description.n_players // 2
        self.exploiter_detection_rounds = 3
        self.last_round_cooperation_threshold = 1.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            previous_round_opponent_actions = history.opponent_actions[-1, :]
            my_previous_action = history.my_actions[-1]
            cooperators_prev = np.sum(previous_round_opponent_actions)
            total_cooperators_prev = cooperators_prev + (1 if my_previous_action else 0)
        if total_cooperators_prev >= self.cooperative_trend_threshold:
            return Action.C
        avg_coop_payoff_window = history.my_payoffs[max(0, state.round_number - self.avg_coop_payoff_window_size):]
        avg_defect_payoff_window = np.mean(history.opponent_payoffs[max(0, state.round_number - self.avg_defect_payoff_window_size):], axis=0)
        if np.mean(avg_coop_payoff_window) < np.mean(avg_defect_payoff_window) * (1 - self.adaptive_adjustment_threshold):
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            total_my_payoffs = np.sum(history.my_payoffs)
            if total_my_payoffs > self.last_round_cooperation_threshold:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_397(BaseStrategy):
    """
    Adaptive Exploitation (AE) strategy for N-Player Public Goods Game.

    This strategy is designed to exploit the opponent's behavior while being robust to various strategies and adaptable to changing game conditions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.pi_avg = 0.0
        self.cooperate_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        opponent_payoffs = history.opponent_payoffs[-1]
        my_payoff = history.my_payoffs[-1]
        payoffs = np.append(opponent_payoffs, my_payoff)
        self.pi_avg = np.mean(payoffs)
        if self.pi_avg > 1:
            self.cooperate_prob = 0.3
        else:
            self.cooperate_prob = 0.7
        num_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if num_cooperators > self.game_description.n_players / 2:
            self.cooperate_prob = 0.2
        elif num_cooperators < self.game_description.n_players / 4:
            self.cooperate_prob = 0.8
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.C if random.random() < self.cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_399(BaseStrategy):
    """
    An adaptive exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        opponent_actions = history.opponent_actions[-1]
        avg_coop_rate = np.mean(opponent_actions)
        θ = 0.5 if self.game_description.k / self.game_description.n_players > 0.7 else 0.3
        action = Action.C if avg_coop_rate > θ else Action.D
        for opponent in range(self.game_description.n_players - 1):
            opponent_defections = np.sum(~history.opponent_actions[-2:, opponent])
            if opponent_defections >= 2:
                action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_400(BaseStrategy):
    """
    Exploitative strategy designed to adapt to opponents' behaviors and maximize payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_opponent_action = None
        self.prev_payoff = None
        self.punishment_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        prev_opponent_action = history.opponent_actions[-1, 0]
        prev_payoff = history.my_payoffs[-1]
        if prev_opponent_action and (not self.prev_opponent_action):
            self.punishment_mode = True
        if self.punishment_mode:
            return Action.D
        if state.round_number % 5 == 0:
            return Action.C if random.random() < 0.5 else Action.D
        if prev_opponent_action and prev_payoff > 1:
            return Action.C
        elif not prev_opponent_action or prev_payoff <= 1:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if not self.prev_opponent_action and prev_opponent_action:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_401(BaseStrategy):
    """
    An exploitative strategy that adapts to the cooperation level of opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] and 1)
        if cooperators >= self.n_players / self.k:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_402(BaseStrategy):
    """
    An adaptive strategy for the N-Player Public Goods Game that balances cooperation and exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_rate_window_size = max(1, int(0.2 * game_description.n_rounds))
        self.early_rounds_end = int(0.1 * game_description.n_rounds)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number < self.early_rounds_end:
            opponent_cooperations = history.opponent_actions[-1, :].sum()
            if opponent_cooperations > (self.game_description.n_players - 1) // 2:
                return Action.C
            else:
                my_payoff_defecting = self.calculate_payoff(history.my_payoffs, True)
                my_payoff_cooperating = self.calculate_payoff(history.my_payoffs, False)
                if my_payoff_defecting > my_payoff_cooperating:
                    return Action.D
                return Action.C
        start_index = max(0, state.round_number - self.coop_rate_window_size + 1)
        opponent_cooperations_window = history.opponent_actions[start_index:-1, :].sum(axis=1)
        coop_rate = np.mean(opponent_cooperations_window) / (self.game_description.n_players - 1)
        if state.round_number >= int(0.9 * self.game_description.n_rounds):
            my_payoff_defecting = self.calculate_payoff(history.my_payoffs, True)
            my_payoff_cooperating = self.calculate_payoff(history.my_payoffs, False)
            if my_payoff_defecting > my_payoff_cooperating:
                return Action.D
        my_payoffs_window = history.my_payoffs[start_index:-1]
        average_payoff_defecting = np.mean(my_payoffs_window[history.my_actions[start_index:-1] == False])
        average_payoff_cooperating = np.mean(my_payoffs_window[history.my_actions[start_index:-1] == True])
        payoff_gap = average_payoff_defecting - average_payoff_cooperating
        if coop_rate > 0.5 and abs(payoff_gap) < 0.1 * self.game_description.k:
            return Action.C
        elif coop_rate < 0.3 or abs(payoff_gap) > 0.2 * self.game_description.k:
            return Action.D
        if average_payoff_defecting > average_payoff_cooperating:
            return Action.D
        else:
            return Action.C

    def calculate_payoff(self, my_payoffs: NDArray[np.float64], is_defection: bool) -> float:
        payoffs = np.copy(my_payoffs)
        if is_defection:
            payoffs[my_payoffs == 0] += self.game_description.k / (self.game_description.n_players - 1)
        else:
            payoffs[my_payoffs > 0] -= 1
        return np.mean(payoffs)

class Strategy_EXPLOITATIVE_403(BaseStrategy):
    """
    Exploitative strategy: "Adaptive Predator".
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.k / game_description.n_players * (1 - 1 / game_description.n_rounds)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        opponent_cooperations = np.sum(history.opponent_actions[-1, :])
        avg_cooperation_rate = opponent_cooperations / self.game_description.n_players
        adjusted_threshold = self.game_description.k / self.game_description.n_players * (1 - 1 / (state.round_number + 1))
        if avg_cooperation_rate < adjusted_threshold:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_404(BaseStrategy):
    """
    The Adaptive Pessimist strategy exploits opponents by adapting to their cooperation rates.
    
    It initially cooperates with probability 0.5, then adjusts its cooperation based on the average opponent 
    cooperation rate in the previous round. If this rate falls below a threshold (θ), it defects; otherwise, 
    it cooperates. The threshold represents the point at which potential gains from cooperation are balanced by 
    the risk of exploitation.
    
    Additionally, if an opponent defects after cooperating in the previous round, Adaptive Pessimist defects 
    for two consecutive rounds before reassessing. In the final round, it always defects to maximize individual 
    payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = (game_description.k - 1) / game_description.n_players
        self.opponent_cooperation_rate = None
        self.triggered_defection = False
        self.rounds_since_trigger = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        opponent_actions = history.opponent_actions[-1, :]
        self.opponent_cooperation_rate = np.mean(opponent_actions)
        if self.triggered_defection:
            self.rounds_since_trigger += 1
            if self.rounds_since_trigger >= 2:
                self.triggered_defection = False
        if not self.triggered_defection and history.my_actions[-1] == Action.C and any((history.opponent_actions[-1, i] == False and history.opponent_actions[-2, i] == True for i in range(self.game_description.n_players))):
            self.triggered_defection = True
        if self.triggered_defection:
            return Action.D
        elif self.opponent_cooperation_rate < self.theta:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_405(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behaviors and occasionally probes their strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.epsilon = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number == 1 and self.game_description.n_players == 2):
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if random.random() < self.epsilon:
            return Action.C
        elif prev_cooperators >= self.game_description.n_players / 2:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_406(BaseStrategy):
    """
    An exploitative strategy designed to maximize payoff in public goods games.
    It adapts to opponents' behavior and punishes defectors while rewarding cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators_prev = 0
        self.opponent_actions = [None] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None and history.opponent_actions.size > 0:
            self.opponent_actions = history.opponent_actions[-1, :]
        total_cooperators_prev = sum(self.opponent_actions)
        if state.round_number < 5:
            if total_cooperators_prev >= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        else:
            defected_opponents = sum((1 for action in self.opponent_actions if not action))
            if defected_opponents > 0:
                return Action.D
            elif total_cooperators_prev >= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_407(BaseStrategy):
    """
    Exploitative strategy for the N-Player Public Goods Game that adapts to game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        T_c = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        E_th = self.game_description.k / self.game_description.n_players
        if T_c > E_th:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_408(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior by balancing cooperation and defection based on observed payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.APC = None
        self.APD = None
        self.PCR = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponents_actions = history.opponent_actions[-1]
        my_action = history.my_actions[-1]
        cooperators = sum(opponents_actions)
        defectors = len(opponents_actions) - cooperators
        if cooperators > 0:
            APC = (sum(history.opponent_payoffs[-1, opponents_actions]) + history.my_payoffs[-1] * my_action) / (cooperators + my_action)
        else:
            APC = 0.0
        if defectors > 0:
            APD = sum(history.opponent_payoffs[-1, ~opponents_actions]) / defectors
        else:
            APD = 0.0
        self.APC = APC
        self.APD = APD
        if state.round_number < self.game_description.n_rounds - 1:
            if APC > APD:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D
        self.PCR = cooperators / len(opponents_actions)

class Strategy_EXPLOITATIVE_409(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop = 0.5
        self.payoff_diff_count = 0
        self.theta = 0.5
        self.delta = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        opponent_coops = np.sum(history.opponent_actions[-1, :])
        avg_coop_t_1 = opponent_coops / self.game_description.n_players
        if avg_coop_t_1 > self.theta:
            action = Action.C
        else:
            action = Action.D
            if history.my_payoffs[-1] > np.mean(history.opponent_payoffs[-1, :]):
                self.payoff_diff_count += 1
        if avg_coop_t_1 > 0.7:
            self.avg_coop *= 1.2
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.payoff_diff_count > self.delta:
            action = random.choices([Action.C, Action.D], weights=[0.2, 0.8])[0]
        return action

class Strategy_EXPLOITATIVE_410(BaseStrategy):
    """
    An exploitative strategy that adapts to various opponent behaviors and maximizes its own payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.round_num = 0
        self.ACR = 0

    def calculate_ACR(self, history: PlayerHistory) -> float:
        """
        Calculate the average cooperation rate (ACR) of all players in a given round.
        
        :param history: The player's history containing opponent actions and payoffs.
        :return: ACR for the specified round.
        """
        if self.round_num == 0:
            return 0
        else:
            cooperators = sum(history.opponent_actions[-1, :] != False)
            total_players = self.game_description.n_players - 1
            if self.round_num <= 5:
                total_cooperators = sum(sum((round != False for round in history.opponent_actions[:self.round_num, :])))
                return total_cooperators / (total_players * self.round_num)
            else:
                return cooperators / total_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.round_num = 1
            return Action.D
        self.round_num += 1
        self.ACR = self.calculate_ACR(history)
        if state.round_number < self.game_description.n_rounds - 1:
            if self.ACR > self.game_description.k / self.game_description.n_players:
                return Action.C
            else:
                return Action.D
        else:
            cooperators_last_round = sum(history.opponent_actions[-2, :] != False)
            if cooperators_last_round > self.game_description.k / 2:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_411(BaseStrategy):
    """
    An exploitative strategy that adapts to game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coops = 0
        self.generous_players = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        n_coops_last_round = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        self.avg_coops = (self.avg_coops * (state.round_number - 2) + n_coops_last_round) / (state.round_number - 1)
        if self.avg_coops >= (self.game_description.n_players - 1) / self.game_description.k:
            action = Action.C
        else:
            action = Action.D
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if history.my_actions[-1] and (not opponent_action):
                action = Action.D
            elif opponent_action and i not in self.generous_players:
                opponent_contributions = sum(history.opponent_actions[:, i])
                avg_opponent_cooperation = opponent_contributions / state.round_number
                if avg_opponent_cooperation > self.avg_coops:
                    self.generous_players.append(i)
        for i in self.generous_players:
            if history.opponent_actions[-1, i]:
                action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_412(BaseStrategy):
    """
    Exploitative strategy that balances cooperation and defection based on observed opponent behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_payoff = 0.0
        self.avg_defect_payoff = 0.0
        self.opponent_behavior = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        if history is not None:
            coop_payoffs = np.mean(history.opponent_payoffs[-1, :] * (history.opponent_actions[-1, :] == True))
            defect_payoffs = np.mean(history.opponent_payoffs[-1, :] * (history.opponent_actions[-1, :] == False))
            self.avg_coop_payoff = coop_payoffs if math.isnan(coop_payoffs) else 0.5 * coop_payoffs + 0.5 * self.avg_coop_payoff
            self.avg_defect_payoff = defect_payoffs if math.isnan(defect_payoffs) else 0.5 * defect_payoffs + 0.5 * self.avg_defect_payoff
        action = Action.C if self.avg_coop_payoff > self.avg_defect_payoff else Action.D
        if history is not None and len(self.opponent_behavior) > 0:
            last_opponent_action = self.opponent_behavior[-1]
            if last_opponent_action == False:
                action = Action.D
            elif last_opponent_action == True:
                action = Action.C
        if state.round_number + 1 == self.game_description.n_rounds:
            return Action.D
        if history is not None and len(history.opponent_actions) > 0:
            last_opponent_action = np.any(history.opponent_actions[-1, :])
            self.opponent_behavior.append(last_opponent_action)
        return action

class Strategy_EXPLOITATIVE_413(BaseStrategy):
    """
    An exploitative strategy that adapts to the game environment by initially cooperating, 
    then defecting when it detects high cooperation rates among opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = game_description.n_players / 2
        self.phi = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C
        if state.round_number < self.game_description.n_rounds - 2:
            avg_cooperators = np.mean(history.my_actions[:state.round_number])
            if avg_cooperators > self.theta / self.game_description.n_players:
                return Action.D
            else:
                return Action.C
        elif state.round_number < self.game_description.n_rounds - 1:
            opponent_cooperation_rate = np.mean(history.opponent_actions[:state.round_number, :])
            if opponent_cooperation_rate > self.phi:
                return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_414(BaseStrategy):
    """
    A robust and adaptive exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cr = np.mean(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history.my_actions is not None else False)
        ap = np.mean(np.concatenate((history.opponent_payoffs[-1], [history.my_payoffs[-1]])))
        cooperate = cr > 0.5 and ap >= self.game_description.k / 2 or (state.round_number > 1 and np.mean(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history.my_actions is not None else False) > cr)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if cooperate else Action.D

class Strategy_EXPLOITATIVE_415(BaseStrategy):
    """
    A robust and exploitative strategy for the N-Player Public Goods Game.
    
    This strategy balances cooperation and exploitation based on game history,
    initially cooperating, then exploiting when the average payoff per cooperator
    exceeds a threshold. It also punishes defectors and revives cooperation with
    those who return to cooperative behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_coop = 0.0
        self.consecutive_defects_i = [0] * game_description.n_players
        self.cooperate_revival = [False] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        coop_payoffs = [payoff for i, payoff in enumerate(history.opponent_payoffs[-1]) if history.opponent_actions[-1, i]]
        self.avg_payoff_coop = sum(coop_payoffs) / len(coop_payoffs) if coop_payoffs else 0.0
        if self.avg_payoff_coop > 0.5 * self.game_description.k:
            return Action.D
        for i, action in enumerate(history.opponent_actions[-1]):
            if not action:
                self.consecutive_defects_i[i] += 1
                if self.consecutive_defects_i[i] >= 2:
                    self.cooperate_revival[i] = False
            else:
                self.consecutive_defects_i[i] = 0
        for i, revived in enumerate(self.cooperate_revival):
            if revived and history.opponent_actions[-1, i]:
                self.cooperate_revival[i] = True
        return Action.C

class Strategy_EXPLOITATIVE_416(BaseStrategy):
    """
    An adaptive strategy for the N-Player Public Goods Game that balances exploration, exploitation, and adaptability.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_probability = 0.5
        self.total_contributions = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperation_probability else Action.D
        self.total_contributions = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        total_payoff_cooperation = self.game_description.k / self.game_description.n_players * self.total_contributions
        if state.round_number >= 5 and state.round_number % 5 == 0:
            opponents_actions_last_5_rounds = history.opponent_actions[-5:, :]
            cooperators_last_5_rounds = np.sum(opponents_actions_last_5_rounds, axis=0)
            proportion_cooperators_last_5_rounds = np.mean(cooperators_last_5_rounds)
            if proportion_cooperators_last_5_rounds > 2 / 3:
                self.cooperation_probability = min(self.cooperation_probability + 0.1, 0.8)
            elif proportion_cooperators_last_5_rounds < 1 / 3:
                self.cooperation_probability = max(self.cooperation_probability - 0.1, 0.2)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif self.total_contributions >= self.game_description.n_players / 2 or total_payoff_cooperation < 1:
            return Action.D
        else:
            return Action.C if random.random() < self.cooperation_probability else Action.D

class Strategy_EXPLOITATIVE_417(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its own payoff by exploiting the cooperation levels of other players while adapting to their behavior over time.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.window_size = 3
        self.total_cooperators = []
        self.avg_cooperation_level = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        self.total_cooperators.append(cooperators)
        avg_window_size = min(self.window_size, len(self.total_cooperators))
        start_index = max(0, len(self.total_cooperators) - self.window_size)
        self.avg_cooperation_level.append(sum(self.total_cooperators[start_index:]) / (avg_window_size * self.game_description.n_players))
        if self.avg_cooperation_level[-1] >= self.game_description.k / self.game_description.n_players:
            return Action.D
        if len(self.total_cooperators) > 2 and self.total_cooperators[-1] - self.total_cooperators[-2] < 0:
            return Action.D
        return Action.C

    def __del__(self):
        pass

class Strategy_EXPLOITATIVE_418(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to maximize its own payoff by exploiting the cooperative tendencies of other players while minimizing its own contribution to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= self.game_description.n_players:
            return Action.C
        prev_coop_payoff = np.mean(history.my_payoffs[history.opponent_actions[-1, 0] == True]) if any(history.opponent_actions[-1, :]) else 0
        prev_defect_payoff = np.mean(history.my_payoffs[history.opponent_actions[-1, 0] == False]) if any(~history.opponent_actions[-1, :]) else 0
        if prev_coop_payoff > prev_defect_payoff and np.sum(history.opponent_actions[-1, :]) >= self.game_description.n_players / 2:
            return Action.D
        coop_rate_opponent = np.mean(history.opponent_actions) if history.opponent_actions.size > 0 else 0.5
        coop_rate_threshold = 0.25
        p = coop_rate_opponent / coop_rate_threshold
        return Action.C if random.random() < p else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_419(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' cooperative tendencies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.D
        elif 3 <= state.round_number <= self.r // 2:
            if history is None or len(history.opponent_actions) == 0:
                return Action.D
            avg_cooperators = sum(history.opponent_actions[-1, :]) / self.n
            p = self.k / self.n if avg_cooperators > self.n / 3 else 0
            return Action.C if random.random() < p else Action.D
        elif state.round_number > self.r // 2:
            if history is None or len(history.opponent_actions) == 0:
                return Action.D
            opponent_coop_rate = sum((sum(row) for row in history.opponent_actions[:state.round_number, :])) / (self.n * state.round_number)
            p = self.k / (self.n - 1) if opponent_coop_rate > 0.5 else 0
            return Action.C if random.random() < p else Action.D
        elif state.round_number == self.r - 1:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_420(BaseStrategy):
    """
    The Adaptive Parasite strategy aims to exploit the cooperative tendencies of opponents while minimizing its own contributions to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        elif 3 <= state.round_number <= self.n_players // 2:
            num_cooperators = sum(history.opponent_actions[-1, :] | [history.my_actions[-1]])
            try:
                avg_coop_payoff = np.mean([payoff for payoff, action in zip(history.opponent_payoffs[-1], history.opponent_actions[-1]) if action] + [history.my_payoffs[-1]] if history.my_actions[-1] else [])
                avg_defect_payoff = np.mean([payoff for payoff, action in zip(history.opponent_payoffs[-1], history.opponent_actions[-1]) if not action])
            except ZeroDivisionError:
                return Action.C
            return Action.C if avg_coop_payoff > avg_defect_payoff else Action.D
        elif state.round_number > self.n_players // 2:
            num_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            return Action.D if num_cooperators >= (self.n_players + 1) / 2 else Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_422(BaseStrategy):
    """
    The Adaptive Exploiter strategy balances exploitation with adaptability.
    It initially cooperates and then adjusts its behavior based on the game's history,
    aiming to take advantage of cooperative opponents while minimizing losses against defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 and history is None:
            return random.choice([Action.C, Action.D])
        previous_round = history.opponent_actions[-1, :]
        total_cooperators = np.sum(previous_round)
        average_payoff = np.mean(history.my_payoffs)
        expected_cooperation_payoff = self.game_description.k / self.game_description.n_players * self.game_description.n_players
        expected_defection_payoff = 1
        if average_payoff > expected_cooperation_payoff:
            self.cooperation_threshold += 1
        elif average_payoff < expected_defection_payoff and self.cooperation_threshold > math.ceil(self.game_description.n_players / 2):
            self.cooperation_threshold -= 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if total_cooperators >= self.cooperation_threshold:
            return Action.C
        elif total_cooperators < math.ceil(self.game_description.n_players / 2):
            return Action.D
        else:
            return random.choice([Action.C, Action.D])

class Strategy_EXPLOITATIVE_423(BaseStrategy):
    """
    The Adaptive Predator strategy aims to maximize its payoff by exploiting the cooperative tendencies of other players while minimizing its own contributions to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploitable_players = set()
        self.cooperative_players = set()

    def calculate_payoff(self, round_actions):
        n_cooperators = sum(round_actions)
        payoff = 1 - int(round_actions[0]) + self.game_description.k / self.game_description.n_players * n_cooperators
        return payoff

    def mark_exploitable(self, player, history):
        if len(history.my_payoffs) > 1 and history.my_payoffs[-1] > history.my_payoffs[-2]:
            self.exploitable_players.add(player)

    def cooperation_boost(self, player, history):
        if len(history.my_actions) > 1 and (not history.my_actions[-1]) and history.my_actions[-2]:
            self.cooperative_players.add(player)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        r = len(history.my_payoffs)
        n = self.game_description.n_players
        avg_coop_rate_prev_round = sum((1 for action in history.opponent_actions[-1, :] if action)) / (n - 1)
        if r <= self.game_description.n_rounds // 3:
            if avg_coop_rate_prev_round > 0.5:
                return Action.C
            else:
                return Action.D
        elif r <= 2 * self.game_description.n_rounds // 3:
            prev_payoff = self.calculate_payoff(history.opponent_actions[-1, :])
            avg_payoff_prev_round = sum((self.calculate_payoff(round) for round in history.opponent_actions[:-1])) / (r - 1)
            if prev_payoff < avg_payoff_prev_round:
                return Action.C
            else:
                return Action.D
        elif r < self.game_description.n_rounds:
            num_coop_prev_round = sum((1 for action in history.opponent_actions[-1, :] if action))
            if num_coop_prev_round > sum((1 for action in history.opponent_actions[-2, :] if action)):
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_424(BaseStrategy):
    """
    An exploitative strategy that gathers information about opponents' tendencies, 
    forms a "wolfpack" of likely cooperators, and exploits their cooperative behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponents = {'COOP': [], 'DEF': []}
        self.wolfpack_members = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        if state.round_number == 2:
            for i, action in enumerate(history.opponent_actions[0]):
                if action:
                    self.opponents['COOP'].append(i)
                else:
                    self.opponents['DEF'].append(i)
        previous_actions = history.opponent_actions[-1]
        self.wolfpack_members = [i for i in self.opponents['COOP'] if previous_actions[i]]
        if len(self.wolfpack_members) >= self.game_description.n_players // 2:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_425(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' cooperative tendencies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_behavior = {i: [0, 0] for i in range(game_description.n_players)}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        previous_round = history.opponent_actions[-1]
        C_count = sum(previous_round)
        D_count = len(previous_round) - C_count
        avg_payoff_C = np.mean(history.opponent_payoffs[-1, :])
        avg_payoff_D = np.sum(history.my_payoffs[-1]) + self.game_description.k / self.game_description.n_players * C_count
        for i, action in enumerate(previous_round):
            if action:
                self.opponent_behavior[i][0] += 1
            else:
                self.opponent_behavior[i][1] += 1
        cooperation_threshold = math.ceil(self.game_description.n_players / 2)
        for opponent in self.opponent_behavior.values():
            if opponent[0] >= state.round_number // 2:
                cooperation_threshold += 1
        if avg_payoff_C > avg_payoff_D and C_count >= cooperation_threshold:
            return Action.C
        elif avg_payoff_C == avg_payoff_D:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_426(BaseStrategy):
    """
    Hybrid strategy combining elements of Tit-for-Tat (TFT) and a novel "K-Greedy" approach.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        majority_cooperated = sum(prev_round_actions) > len(prev_round_actions) / 2
        avg_coop_payoff = self.calculate_avg_coop_payoff(history)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif majority_cooperated and avg_coop_payoff > self.game_description.k / 2:
            return Action.C
        elif avg_coop_payoff > self.game_description.k / 2:
            return Action.C
        else:
            return Action.D

    def calculate_avg_coop_payoff(self, history: PlayerHistory):
        x = 5
        coop_rounds = [round for round in history.opponent_actions[-x:] if sum(round) > 0]
        if not coop_rounds:
            return 0
        avg_coop_payoff = sum((sum(round) / len(round) for round in coop_rounds)) / len(coop_rounds)
        return avg_coop_payoff

class Strategy_EXPLOITATIVE_427(BaseStrategy):
    """
    An exploitative and adaptive strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.previous_payoffs = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2 or history is None:
            return Action.C
        my_actions = np.where(history.my_actions[:state.round_number], True, False)
        opponent_actions = history.opponent_actions[:state.round_number]
        if sum(my_actions[-2:]) == 0:
            return Action.D
        my_payoffs = np.where(history.my_payoffs[:state.round_number] > 0, 1, 0)
        average_coop_payoff = self.game_description.k / self.game_description.n_players * sum(my_payoffs[-3:])
        if average_coop_payoff > self.cooperation_threshold:
            action = Action.C
        else:
            action = Action.D
        if state.round_number >= self.game_description.n_rounds - 2:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_428(BaseStrategy):
    """
    An exploitative strategy that adapts to the opponents' behavior by cooperating conditionally.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.5
        self.opponent_cooperation_rates = {}
        self.recent_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        total_rounds = len(history.my_actions)
        if total_rounds < self.game_description.n_players and random.random() > 0.5:
            return Action.D
        elif total_rounds < 2 * self.game_description.n_players:
            return Action.C
        estimated_cooperation_rate = self._estimate_opponents_cooperation_rate(history)
        for opponent in range(self.game_description.n_players - 1):
            opponent_action_count = sum((1 for i in range(total_rounds) if history.opponent_actions[i, opponent]))
            opponent_cooperation_rate = opponent_action_count / total_rounds
            self.opponent_cooperation_rates[opponent] = opponent_cooperation_rate
        action = Action.D
        if estimated_cooperation_rate > self.cooperation_threshold:
            action = Action.C
        for opponent in range(self.game_description.n_players - 1):
            opponent_cooperation_rate = self.opponent_cooperation_rates.get(opponent, 0.5)
            if opponent_cooperation_rate > 0.8:
                action = Action.D
                break
        self.cooperation_threshold = self._update_cooperation_threshold(history)
        return action

    def _estimate_opponents_cooperation_rate(self, history: PlayerHistory) -> float:
        total_rounds = len(history.my_actions)
        opponent_action_count = sum((1 for i in range(total_rounds) if history.opponent_actions[i].any()))
        estimated_cooperation_rate = opponent_action_count / (total_rounds * (self.game_description.n_players - 1))
        return estimated_cooperation_rate

    def _update_cooperation_threshold(self, history: PlayerHistory):
        recent_payoffs = np.mean(history.my_payoffs[-5:]) if len(history.my_payoffs) >= 5 else np.mean(history.my_payoffs)
        overall_average_payoff = np.mean(history.my_payoffs)
        if recent_payoffs > overall_average_payoff:
            self.cooperation_threshold += 0.05
        elif recent_payoffs < overall_average_payoff:
            self.cooperation_threshold -= 0.05
        return max(0, min(self.cooperation_threshold, 1))

class Strategy_EXPLOITATIVE_429(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Parasite"

    The Adaptive Parasite strategy aims to exploit the cooperative behavior of opponents while minimizing its own contributions to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.5
        self.exploit_prob = 0.8
        self.punish_prob = 0.9
        self.num_exploited_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < self.cooperate_prob else Action.D
        opponent_defected = history.opponent_actions[-1, 0] == False and history.my_actions[-1] == True
        if opponent_defected:
            return Action.D if random.random() < self.punish_prob else Action.C
        num_cooperators = sum(history.opponent_actions[-1, :])
        if num_cooperators >= self.game_description.n_players // 2:
            action = Action.D if random.random() < self.exploit_prob else Action.C
        else:
            action = Action.C if random.random() < self.cooperate_prob else Action.D
        self.num_exploited_rounds += opponent_defected
        if self.num_exploited_rounds > self.game_description.n_rounds // 3:
            if num_cooperators < self.game_description.n_players // 2:
                self.cooperate_prob -= 0.2
            else:
                self.exploit_prob += 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_430(BaseStrategy):
    """
    Exploitative strategy "Adaptive Predator" that adapts to changing game dynamics and exploits opportunities for cooperation or defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.probability = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        total_cooperators_previous_round = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if len(history.my_payoffs) > 1:
            self.probability = self.update_probability(self.probability, history, total_cooperators_previous_round)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        p = self.game_description.k / self.game_description.n_players * (total_cooperators_previous_round / self.game_description.n_players)
        if random.random() < max(0, min(self.probability, p)):
            return Action.C
        else:
            return Action.D

    def update_probability(self, p: float, history: PlayerHistory, total_cooperators_previous_round: int) -> float:
        """
        Update the probability p based on the previous round's outcomes.
        
        Args:
        - p (float): The current probability value.
        - history (PlayerHistory): A dataclass containing this player's actions and payoffs as well as opponents' actions and payoffs.
        - total_cooperators_previous_round (int): Total number of cooperators in the previous round.

        Returns:
        float: Updated probability p.
        """
        if total_cooperators_previous_round > sum(history.opponent_actions[-2, :]) + history.my_actions[-2] and history.my_payoffs[-1] > history.my_payoffs[-2]:
            return min(1, p * 1.1)
        elif total_cooperators_previous_round < sum(history.opponent_actions[-2, :]) + history.my_actions[-2] or history.my_payoffs[-1] <= history.my_payoffs[-2]:
            return max(0, p * 0.9)
        else:
            return p

class Strategy_EXPLOITATIVE_432(BaseStrategy):
    """
    The Adaptive Exploiter strategy balances exploration, adaptation, and exploitation 
    to maximize payoffs in the N-player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.actions = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            self.actions.append(Action.C)
            return Action.C
        if state.round_number <= self.game_description.n_rounds // 4:
            cooperators = sum(history.opponent_actions[-1, :])
            if cooperators > self.game_description.n_players / 2:
                self.actions.append(Action.C)
                return Action.C
            else:
                self.actions.append(Action.D)
                return Action.D
        elif state.round_number <= 3 * self.game_description.n_rounds // 4:
            avg_payoff_C = np.mean([self.game_description.k / self.game_description.n_players if a else 1 for a in history.my_actions[:-1]])
            avg_payoff_D = np.mean([0 if a else 1 + self.game_description.k / self.game_description.n_players * sum(history.opponent_actions[i, :]) for i, a in enumerate(history.my_actions[:-1])])
            if avg_payoff_C > avg_payoff_D:
                self.actions.append(Action.C)
                return Action.C
            else:
                self.actions.append(Action.D)
                return Action.D
        else:
            cooperators = sum(history.opponent_actions[-1, :])
            if cooperators < self.game_description.n_players / 2 and state.round_number != self.game_description.n_rounds - 1:
                self.actions.append(Action.D)
                return Action.D
            else:
                self.actions.append(Action.C)
                return Action.C

class Strategy_EXPLOITATIVE_434(BaseStrategy):
    """
    Adaptive Exploiter strategy for N-Player Public Goods Game.
    
    This strategy is designed to exploit a wide range of opponent behaviors in the N-Player Public Goods Game. 
    It uses a combination of observation, adaptation, and strategic manipulation to maximize its payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploration_rounds = 3
        self.theta = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= self.exploration_rounds:
            return random.choices([Action.C, Action.D], weights=[0.5, 0.5])[0]
        opponent_cooperations = history.opponent_actions[-2:, :]
        avg_opponent_cooperation_rate = np.mean(opponent_cooperations)
        if avg_opponent_cooperation_rate > self.theta:
            return random.choices([Action.C, Action.D], weights=[0.2, 0.8])[0]
        else:
            return random.choices([Action.C, Action.D], weights=[0.8, 0.2])[0]
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history.opponent_actions[-1, :].sum() > 0 and (not np.all(history.opponent_actions[-1, :])):
            return random.choices([Action.C, Action.D], weights=[0.1, 0.9])[0]
        if avg_opponent_cooperation_rate == self.theta:
            return random.choices([Action.C, Action.D], weights=[0.6, 0.4])[0]

class Strategy_EXPLOITATIVE_435(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_rate = 0.5
        self.avg_payoff = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        opponent_actions = history.opponent_actions[:-1]
        opponent_payoffs = history.opponent_payoffs[:-1]
        total_coop = np.sum(opponent_actions)
        avg_opponent_coop = total_coop / (state.round_number * (self.game_description.n_players - 1))
        if opponent_payoffs.size > 0:
            avg_opponent_payoff = np.mean(np.sum(opponent_payoffs, axis=1))
        else:
            avg_opponent_payoff = self.avg_payoff
        self.coop_rate = avg_opponent_coop
        self.avg_payoff = avg_opponent_payoff
        if self.coop_rate > 0.5 and self.avg_payoff > 1:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            if self.coop_rate > 0.5 and self.avg_payoff > 1:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_436(BaseStrategy):
    """
    The ACD strategy is designed to exploit a wide range of opponent behaviors by adapting its cooperation level based on the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_behavior = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        T = self.game_description.k / self.game_description.n_players * (self.game_description.n_players / 2)
        for i, action in enumerate(history.opponent_actions[-1, :]):
            if i not in self.opponent_behavior:
                self.opponent_behavior[i] = [action]
            else:
                self.opponent_behavior[i].append(action)
        prev_round_cooperators = sum(history.opponent_actions[-1, :] == True)
        avg_coop_rates = {i: np.mean(actions) for i, actions in self.opponent_behavior.items()}
        unreliable_opponents = [i for i, rate in avg_coop_rates.items() if rate < 0.5]
        if prev_round_cooperators >= T:
            return Action.C
        elif len(unreliable_opponents) > 0:
            return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif len(self.opponent_behavior) == 1:
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_437(BaseStrategy):
    """
    An exploitative strategy for N-Player Public Goods Game, adapting to opponent behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_c = 0.0
        self.history = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number >= r - 1:
            return Action.D
        c_t = np.sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        self.avg_c = (self.avg_c * 2 + c_t) / 3 if state.round_number > 1 else c_t / n
        θ = k / n * (1 - self.avg_c)
        π_C = 0 + k / n * (self.avg_c * n)
        if π_C > θ:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_438(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy is designed to exploit various opponent behaviors 
    and maximize its payoff in an N-Player Public Goods Game. It adapts to different 
    cooperation levels by setting a contribution threshold.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = (game_description.n_players - 1) / (game_description.k - 1)
        self.prev_cooperators = None
        self.exploiting = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperators_t_minus_1 = sum(history.opponent_actions[-1, :])
        self.exploiting = state.round_number > 3 and all((cooperators >= self.theta for cooperators in [cooperators_t_minus_1] + history.my_payoffs[-2:]))
        if self.exploiting or (not self.prev_cooperators is None and self.prev_cooperators < self.theta):
            return Action.D
        else:
            return Action.C
        self.prev_cooperators = cooperators_t_minus_1

class Strategy_EXPLOITATIVE_439(BaseStrategy):
    """
    Exploitative strategy that adapts to the behavior of other players while maintaining a level of cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop = 0
        self.prev_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        avg_coop = np.mean(np.sum(history.opponent_actions[:-1], axis=1))
        if avg_coop >= self.game_description.n_players / 2:
            action = Action.C
        else:
            action = Action.D
        if history.my_payoffs[-1] > 0 and np.sum(history.opponent_actions[-1, :]) < self.game_description.n_players / 2:
            action = Action.D
        if avg_coop > self.game_description.n_players / 2 and history.opponent_payoffs[-1, 0] > history.my_payoffs[-1]:
            action = Action.C
        if state.round_number >= self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_440(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Predator" for N-Player Public Goods Game.
    
    Initially explores opponents' behavior, then adapts to their cooperation rates 
    and responds with either cooperation or defection to maximize its own payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_rates = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        if self.coop_rates is None or len(self.coop_rates) != self.game_description.n_players:
            self.coop_rates = np.zeros((self.game_description.n_players,))
        opponent_actions = history.opponent_actions[-1]
        for i in range(opponent_actions.shape[0]):
            if opponent_actions[i]:
                self.coop_rates[i] += 1
        if state.round_number >= 3:
            opponent_types = np.where(self.coop_rates / state.round_number > 2 / 3, 'Cooperator', np.where(self.coop_rates / state.round_number < 1 / 3, 'Defector', 'Unpredictable'))
            if len(np.where(opponent_types == 'Cooperator')[0]) > self.game_description.n_players // 2:
                return Action.D
            elif len(np.where(opponent_types == 'Defector')[0]) > self.game_description.n_players // 2:
                return Action.C
            else:
                avg_payoff_prev_round = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1].sum(axis=0))
                if avg_payoff_prev_round > 1.5:
                    return Action.C
                return Action.D
        return Action.C if random.random() < 0.5 else Action.D

    def __del__(self):
        pass

class Strategy_EXPLOITATIVE_441(BaseStrategy):
    """
    The Adaptive Exploiter strategy combines elements of exploration, exploitation, and punishment.
    It balances short-term self-interest with long-term adaptability and robustness.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate_history = []
        self.recent_trend = 0

    def calculate_cooperation_rate(self, history: PlayerHistory) -> float:
        """
        Calculate the average number of cooperators in the last 3 rounds (ACR).
        """
        recent_rounds = history.opponent_actions[-min(3, len(history.my_actions)):, :]
        num_cooperators = np.sum(recent_rounds, axis=(0, 1))
        return num_cooperators / (self.game_description.n_players - 1)

    def update_recent_trend(self, history: PlayerHistory) -> None:
        """
        Calculate the Recent Cooperation Trend (RCT).
        """
        recent_rounds = history.opponent_actions[-min(3, len(history.my_actions)):, :]
        if len(recent_rounds) < 2:
            self.recent_trend = 0
        else:
            current_cooperators = np.sum(recent_rounds[-1, :])
            previous_cooperators = np.sum(recent_rounds[-2, :])
            self.recent_trend = int(current_cooperators > previous_cooperators) * 2 - 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.cooperation_rate_history.append(self.calculate_cooperation_rate(history))
        cooperation_rate = np.mean(self.cooperation_rate_history[-min(3, len(self.cooperation_rate_history)):])
        self.update_recent_trend(history)
        if state.round_number < 3:
            return Action.C if history.opponent_actions[-1, :].sum() > self.game_description.n_players // 2 else Action.D
        if cooperation_rate > self.game_description.n_players / 2 and self.recent_trend >= 0:
            return Action.C
        elif self.game_description.k / self.game_description.n_players < 1.5 and history.opponent_actions[-1, :].sum() >= self.game_description.n_players // 2:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        low_cooperation_threshold = self.game_description.n_players / 4
        if len(self.cooperation_rate_history) >= 3 and all((rate < low_cooperation_threshold for rate in self.cooperation_rate_history[-3:])):
            return Action.C if history.opponent_actions[-1, :].sum() > 0 else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_442(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate = 0
        self.defection_rate = 0
        self.payoffs_cooperate = []
        self.payoffs_defect = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 and history is None:
            return Action.C
        if state.round_number <= 5:
            if self.cooperation_rate > self.game_description.n_players / 2:
                action = Action.C
            else:
                action = Action.D
        else:
            avg_payoff_coop = np.mean(self.payoffs_cooperate) if self.payoffs_cooperate else 0
            avg_payoff_defect = np.mean(self.payoffs_defect) if self.payoffs_defect else 0
            if avg_payoff_coop > avg_payoff_defect:
                action = Action.C
            else:
                action = self.adaptive_adjustment()
        if history is not None:
            self.cooperation_rate = np.mean(history.opponent_actions[-1, :])
            self.defection_rate = 1 - self.cooperation_rate
            if action == Action.C:
                payoff = self.calculate_payoff(history)
                self.payoffs_cooperate.append(payoff)
            else:
                payoff = self.calculate_payoff(history)
                self.payoffs_defect.append(payoff)
        return action

    def adaptive_adjustment(self):
        if len(self.payoffs_cooperate) > 0 and np.mean(self.payoffs_cooperate[-3:]) > np.mean(self.payoffs_defect[-3:]):
            return Action.C
        elif self.defection_rate < 0.5:
            return Action.D
        else:
            return random.choice([Action.C, Action.D])

    def calculate_payoff(self, history: PlayerHistory):
        my_action = history.my_actions[-1]
        opponents_cooperate = np.sum(history.opponent_actions[-1, :])
        payoff = 1 - int(my_action) + self.game_description.k / self.game_description.n_players * opponents_cooperate
        return payoff

class Strategy_EXPLOITATIVE_444(BaseStrategy):
    """
    An adaptive exploitative strategy for the N-Player Public Goods Game.
    It balances exploitation with cooperation based on observed opponent behavior and payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop = 0.5
        self.avg_payoff_C = 1.0
        self.avg_payoff_D = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        last_actions = history.opponent_actions[-1]
        avg_coop_round = np.mean(last_actions)
        self.avg_coop = (self.avg_coop * state.round_number + avg_coop_round) / (state.round_number + 1)
        if history.my_payoffs.size > 0:
            my_last_action = history.my_actions[-1]
            last_payoff = history.my_payoffs[-1]
            if my_last_action == True:
                self.avg_payoff_C = (self.avg_payoff_C * state.round_number + last_payoff) / (state.round_number + 1)
            else:
                self.avg_payoff_D = (self.avg_payoff_D * state.round_number + last_payoff) / (state.round_number + 1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        k_over_n = self.game_description.k / self.game_description.n_players
        avg_payoff_ratio_C = self.avg_payoff_C / max(self.avg_payoff_D, 1e-06)
        avg_payoff_ratio_D = self.avg_payoff_D / max(self.avg_payoff_C, 1e-06)
        adapt_prob_C = min(1.0, k_over_n + avg_payoff_ratio_C)
        adapt_prob_D = max(0.0, k_over_n - avg_payoff_ratio_D)
        if self.avg_coop > 0.7:
            return Action.C if random.random() < adapt_prob_C else Action.D
        elif k_over_n > avg_payoff_ratio_D:
            return Action.C if random.random() < adapt_prob_D else Action.D
        else:
            return Action.D if random.random() < 0.8 else Action.C

class Strategy_EXPLOITATIVE_446(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Public Goods Twist strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators = 0
        self.opponent_strategies = {i: None for i in range(1, game_description.n_players + 1)}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self.total_cooperators = sum(history.opponent_actions[-1, :])
        for i, opp_action in enumerate(history.opponent_actions[-1, :]):
            if not opp_action and self.opponent_strategies[i + 1] is None:
                self.opponent_strategies[i + 1] = 'defector'
            elif opp_action and self.opponent_strategies[i + 1] is None:
                self.opponent_strategies[i + 1] = 'cooperator'
        if self.total_cooperators >= math.ceil(self.game_description.n_players / 2):
            action = Action.C
        else:
            action = Action.D
        for i, strategy in self.opponent_strategies.items():
            if strategy == 'defector':
                action = Action.D
                break
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_EXPLOITATIVE_447(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its own payoff by adapting to the opponents' behavior and exploiting their cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.n_players:
            return Action.C
        opponent_actions = history.opponent_actions[-self.n_players:, :]
        opponent_payoffs = history.opponent_payoffs[-self.n_players:, :]
        avg_payoffs = np.mean(opponent_payoffs, axis=0)
        max_coop_opponent = np.argmax(avg_payoffs)
        if avg_payoffs[max_coop_opponent] > 0.8:
            return Action.D
        coop_rates = np.sum(opponent_actions, axis=0) / self.n_players
        if np.max(coop_rates) > 0.5:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_448(BaseStrategy):
    """
    A strategy that leverages game history to maximize payoff by adapting to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_payoffs = np.zeros(game_description.n_rounds)
        self.opponent_actions = np.zeros((game_description.n_rounds, game_description.n_players - 1), dtype=bool)

    def calculate_average_payoff_cooperators(self, history: PlayerHistory) -> float:
        """
        Calculate the average payoff of cooperators in a given round.
        
        Args:
            history (PlayerHistory): A dataclass containing this player's and opponents' actions and payoffs.

        Returns:
            float: The average payoff of cooperators in the most recent round.
        """
        last_round = len(history.my_payoffs) - 1
        total_contributions = np.sum(np.append([True], history.opponent_actions[last_round, :]))
        if total_contributions > 0:
            avg_pi_C = self.game_description.k / self.game_description.n_players * total_contributions + (1 - 1)
        else:
            avg_pi_C = 1
        return avg_pi_C

    def calculate_average_payoff_defectors(self, history: PlayerHistory) -> float:
        """
        Calculate the average payoff of defectors in a given round.
        
        Args:
            history (PlayerHistory): A dataclass containing this player's and opponents' actions and payoffs.

        Returns:
            float: The average payoff of defectors in the most recent round.
        """
        last_round = len(history.my_payoffs) - 1
        total_contributions = np.sum(np.append([True], history.opponent_actions[last_round, :]))
        if self.game_description.n_players > total_contributions:
            avg_pi_D = 1 + self.game_description.k / self.game_description.n_players * total_contributions
        else:
            avg_pi_D = 1
        return avg_pi_D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide on an action based on the game's current state and this player's history.
        
        Args:
            state (GameState): The current game state.
            history (None | PlayerHistory): A dataclass containing this player's and opponents' actions and payoffs.

        Returns:
            Action: This player's chosen action for the current round.
        """
        if state.round_number == 0 or history is None:
            return Action.C
        self.my_payoffs[state.round_number - 1] = history.my_payoffs[-1]
        self.opponent_actions[state.round_number - 1, :] = history.opponent_actions[-1, :]
        pi_C = self.calculate_average_payoff_cooperators(history)
        pi_D = self.calculate_average_payoff_defectors(history)
        if pi_C > pi_D:
            return Action.C
        elif state.round_number < self.game_description.n_rounds - 2 and history.my_actions[-1] == True and history.opponent_actions[-1, :].any():
            return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_449(BaseStrategy):
    """
    Exploitative strategy: "Adaptive Defection with Conditional Cooperation" (ADCC)
    
    This strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history.
    It aims to maximize our total payoff by defecting whenever possible and cooperating only when necessary to encourage others to cooperate.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        prev_round_coops = sum(history.opponent_actions[-1, :])
        if prev_round_coops >= self.game_description.k / 2:
            return Action.C
        return Action.D

    def last_round(self) -> Action:
        return Action.D

class Strategy_EXPLOITATIVE_450(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game.
    It adapts to opponents' behavior and exploits defectors while punishing free-riders.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        num_defectors = sum((1 for action in prev_round_actions if not action))
        total_contributions = self.n_players - num_defectors
        if num_defectors >= self.n_players / 2:
            return Action.D
        for i, opponent_action in enumerate(prev_round_actions):
            if not opponent_action and total_contributions < self.n_players / 2:
                return Action.D
        p = self.k / self.n_players * (total_contributions / self.n_players)
        if random.random() < p:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_452(BaseStrategy):
    """
    Exploitative strategy that adapts to opponent behavior in the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_coop = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_coop else Action.D
        avg_coop_t_minus_1 = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]]) / self.game_description.n_players
        if avg_coop_t_minus_1 >= self.game_description.k / self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        if history.my_payoffs[-1] > 1 + self.game_description.k / self.game_description.n_players * self.game_description.n_players / 2:
            self.p_coop = min(0.9, self.p_coop + 0.1)
        elif history.my_payoffs[-1] < 1 + self.game_description.k / self.game_description.n_players * self.game_description.n_players / 4:
            self.p_coop = max(0.1, self.p_coop - 0.1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_454(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game.
    It adapts to changing opponent behaviors and retaliates against opponents who defect while cooperating.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_round_actions = []
        self.num_cooperators = 0

    def calculate_threshold(self, num_cooperators, prev_round_actions):
        if len(prev_round_actions) == 1:
            return 0
        T = sum([sum([player for player in actions if player]) for actions in prev_round_actions[:-1]]) / (len(prev_round_actions) - 1)
        return T

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n = self.game_description.n_players
        k = self.game_description.k
        r = self.game_description.n_rounds
        if state.round_number == 0:
            self.prev_round_actions.append([Action.C.value] * n)
            return Action.C
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        my_prev_action = history.my_actions[-1]
        num_cooperators = sum(prev_round_opponent_actions) + my_prev_action
        self.num_cooperators += num_cooperators
        if state.round_number < r - 1:
            T = self.calculate_threshold(num_cooperators, self.prev_round_actions)
            if num_cooperators >= T:
                action = Action.C
            elif any((player == Action.D.value and my_prev_action for player in prev_round_opponent_actions)):
                action = Action.D
            else:
                action = Action.D
        else:
            action = Action.D
        self.prev_round_actions.append(list(prev_round_opponent_actions) + [my_prev_action])
        return action

class Strategy_EXPLOITATIVE_455(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and maximizes its payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.acr_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < 3:
            return Action.C
        opponent_actions = history.opponent_actions
        n_players = self.game_description.n_players
        avg_opponent_cooperation_rate = np.mean(opponent_actions[-1, :])
        if len(self.acr_history) < state.round_number:
            self.acr_history.append(avg_opponent_cooperation_rate)
        if avg_opponent_cooperation_rate > 0.5:
            return Action.D
        if state.round_number % 5 == 0 and len(self.acr_history) >= 2:
            prev_acr = self.acr_history[-2]
            acr_diff = avg_opponent_cooperation_rate - prev_acr
            if acr_diff > 0.1:
                return Action.C
            elif prev_acr - avg_opponent_cooperation_rate > 0.1:
                return Action.D
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_457(BaseStrategy):
    """
    Adaptive Exploiter strategy for N-Player Public Goods Game.
    Balances adaptability with exploitative tendencies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_rate = 1.0
        self.total_payoff = 0
        self.round_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        avg_payoff_prev = (history.my_payoffs[-1] + np.mean(history.opponent_payoffs[-1])) / self.game_description.n_players
        if avg_payoff_prev <= 1 / self.game_description.k:
            return Action.D
        self.round_count += 1
        if self.round_count % 3 == 0:
            total_payoff_last_3_rounds = np.sum(history.my_payoffs[-3:])
            avg_payoff_last_3_rounds = total_payoff_last_3_rounds / 3
            if avg_payoff_last_3_rounds < 1:
                self.coop_rate *= 0.9
            elif avg_payoff_last_3_rounds > self.game_description.k * (self.game_description.n_players - 1) / self.game_description.n_players:
                self.coop_rate *= 1.1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif random.random() < self.coop_rate:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_458(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators = None
        self.avg_payoff_per_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.total_cooperators is None and self.avg_payoff_per_round is None:
            self.total_cooperators = sum(history.opponent_actions[0, :]) + 1
            self.avg_payoff_per_round = np.mean([self.game_description.k * (history.my_actions[0] + sum(history.opponent_actions[0, :])) / self.game_description.n_players])
        if state.round_number <= math.floor(self.game_description.n_players / 2):
            if self.total_cooperators <= math.floor(self.game_description.n_players / 4):
                action = Action.C
            else:
                action = Action.D
        elif math.ceil(self.game_description.n_players / 2) < state.round_number <= math.ceil(3 * self.game_description.n_players / 4):
            if self.avg_payoff_per_round >= self.game_description.k / 2:
                if sum(history.opponent_actions[-1, :]) + history.my_actions[-1] < self.game_description.n_players // 2:
                    action = Action.C
                else:
                    action = Action.D
            else:
                action = Action.D
        elif state.round_number > 1 and sum(history.opponent_actions[-2, :]) > sum(history.opponent_actions[-1, :]):
            action = Action.D
        elif self.avg_payoff_per_round >= self.game_description.k / 2:
            action = Action.C
        else:
            action = Action.D
        if history is not None and state.round_number > 0:
            self.total_cooperators += history.my_actions[-1]
            payoffs = [self.game_description.k * (history.my_actions[i] + sum(history.opponent_actions[i, :])) / self.game_description.n_players for i in range(state.round_number)]
            self.avg_payoff_per_round = np.mean(payoffs)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_459(BaseStrategy):
    """
    An adaptive exploiter strategy that initially defects to observe opponent behavior,
    then adapts its strategy based on observed cooperation level and average payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_level = 0.0
        self.average_payoff = 0.0
        self.previous_actions = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number <= 5:
            opponent_cooperations = sum(history.opponent_actions[-1, :])
            self.cooperation_level = opponent_cooperations / self.game_description.n_players
            if self.cooperation_level > 0.5:
                return Action.C
            else:
                return Action.D
        opponent_payoffs = np.mean(history.opponent_payoffs[-1, :], axis=0)
        self.average_payoff = np.mean(opponent_payoffs)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.average_payoff > 1.5:
            return Action.D
        elif self.average_payoff < 1.0:
            return Action.C
        else:
            p = self.cooperation_level
            if random.random() < p:
                return Action.C
            else:
                return Action.D
        if len(self.previous_actions) >= 2 and sum(self.previous_actions[-1]) / self.game_description.n_players - sum(self.previous_actions[-2]) / self.game_description.n_players > 0.2:
            return Action.D
        if len(history.opponent_actions) >= 2 and np.any(np.diff(history.opponent_actions, axis=0)):
            self.cooperation_level = sum(opponent_cooperations) / self.game_description.n_players
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        self.previous_actions.append([a == Action.C for a in history.my_actions])

class Strategy_EXPLOITATIVE_460(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' cooperation rate in an N-Player Public Goods Game.
    
    The strategy aims to maximize its own payoff by balancing short-term gains with long-term adaptability.
    It cooperates initially, then adjusts its cooperation probability based on the opponent's average cooperation rate.
    If an opponent's cooperation rate drops below a certain threshold, it switches to unconditional defection for a few rounds.
    
    Parameters:
    game_description (PublicGoodsDescription): Game parameters (number of players, rounds, and multiplication factor)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_c_rate = 0.0
        self.punish_rounds_left = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C
        r = self.game_description.n_rounds
        n = self.game_description.n_players
        window_size = min(r // 2, 5)
        opponent_cooperations = sum(history.opponent_actions[-window_size:, :].sum(axis=1))
        self.opponent_c_rate = opponent_cooperations / (n * window_size)
        k = self.game_description.k
        cooperation_probability = max(0.0, 1 - k / n * self.opponent_c_rate)
        if random.random() < cooperation_probability:
            action = Action.C
        else:
            action = Action.D
        punish_threshold = 0.3
        punishment_rounds = 2
        epsilon = 0.01
        if self.opponent_c_rate < punish_threshold - epsilon and self.punish_rounds_left == 0:
            self.punish_rounds_left = punishment_rounds
            action = Action.D
        if state.round_number == r - 1:
            action = Action.D
        if self.punish_rounds_left > 0:
            self.punish_rounds_left -= 1
        return action

class Strategy_EXPLOITATIVE_461(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.c_rate = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.c_rate else Action.D
        previous_round_decisions = history.opponent_actions[-1, :] != 0
        c_avg = np.mean(previous_round_decisions)
        π_cavg = np.mean([history.opponent_payoffs[-1, j] for j in range(self.game_description.n_players - 1) if previous_round_decisions[j]])
        π_davg = np.mean([history.opponent_payoffs[-1, j] for j in range(self.game_description.n_players - 1) if not previous_round_decisions[j]])
        if π_cavg >= π_davg:
            action = Action.D
        else:
            action = Action.C
        self.c_rate += 0.1 if np.sum(previous_round_decisions) >= self.game_description.n_players / 2 else -0.1
        self.c_rate = max(0, min(self.c_rate, 1))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if math.isclose(π_cavg, π_davg):
            return Action.C if random.random() < 0.5 else Action.D
        return action

class Strategy_EXPLOITATIVE_462(BaseStrategy):
    """
    A robust and adaptive exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        prev_round = history.my_actions.shape[0] - 1
        coop_count = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if coop_count > self.game_description.n_players / 2:
            return Action.D
        our_prev_payoff = history.my_payoffs[-1]
        avg_coop_payoff = self.game_description.k / self.game_description.n_players * coop_count
        if our_prev_payoff > avg_coop_payoff and (not history.my_actions[-1]):
            return Action.D
        elif our_prev_payoff <= avg_coop_payoff:
            return random.choice([Action.C, Action.D])
        for opponent_payoffs in history.opponent_payoffs[-1, :]:
            if opponent_payoffs > our_prev_payoff:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return random.choice([Action.C, Action.D])

class Strategy_EXPLOITATIVE_463(BaseStrategy):
    """
    Exploitative Strategy: Adaptive Greed.

    Initially cooperate, then alternate between cooperating and defecting to gather information.
    After exploration phase, adapt to the environment by contributing if average public goods contribution is high,
    otherwise exploit the situation by defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.last_round_cooperators = 0
        self.deviation_restart_rounds = 0

    def calculate_average_public_goods(self, history: PlayerHistory) -> float:
        """
        Calculate average public goods contribution up to the previous round.

        Args:
            history (PlayerHistory): Player's game history.

        Returns:
            float: Average public goods contribution.
        """
        total_cooperators = sum(history.my_actions[:history.my_actions.size]) + np.sum(history.opponent_actions)
        avg_contributions = total_cooperators / (self.game_description.n_players * history.my_actions.size)
        return avg_contributions

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide the action for the current round based on game state and history.

        Args:
            state (GameState): Current game state.
            history (None | PlayerHistory): Player's game history or None for the first round.

        Returns:
            Action: Chosen action for this round.
        """
        if state.round_number == 0:
            return Action.C
        if self.deviation_restart_rounds > 0:
            self.deviation_restart_rounds -= 1
            return Action.D if state.round_number % 2 != 0 else Action.C
        elif 2 <= state.round_number <= 5:
            return Action.D if state.round_number % 2 != 0 else Action.C
        else:
            avg_contributions = self.calculate_average_public_goods(history)
            current_cooperators = sum(history.opponent_actions[-1, :])
            if abs(current_cooperators - self.last_round_cooperators) > 1:
                self.deviation_restart_rounds = min(3, self.game_description.n_rounds - state.round_number)
            return Action.C if avg_contributions > self.game_description.k / 2 else Action.D
        if state.round_number == self.game_description.n_rounds:
            return Action.D

class Strategy_EXPLOITATIVE_464(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        opponent_actions = history.opponent_actions[-1, :]
        avg_coop_rate = np.mean(opponent_actions) + history.my_actions[-1] / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number <= 3:
            return random.choice([Action.C, Action.D])
        if avg_coop_rate > self.theta:
            p_exploit = 1 - self.game_description.k / self.game_description.n_players * avg_coop_rate
            return Action.D if random.random() < p_exploit else Action.C
        else:
            p_cooperate = self.game_description.k / self.game_description.n_players * avg_coop_rate
            return Action.C if random.random() < p_cooperate else Action.D

class Strategy_EXPLOITATIVE_465(BaseStrategy):
    """
    Exploitative strategy designed to adapt to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.epsilon = 0.01
        self.theta = game_description.k / game_description.n_players * (1 - 1 / game_description.n_players)
        self.exploitable_opponents = []
        self.average_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        previous_round_cooperation_rate = np.mean(history.opponent_actions[-1, :])
        opponent_defections = [i for i, action in enumerate(history.opponent_actions[-1, :]) if not action]
        if previous_round_cooperation_rate > 0.5 and any((i not in self.exploitable_opponents for i in opponent_defections)):
            self.exploitable_opponents.extend(opponent_defections)
        average_payoff = np.mean(np.concatenate((self.average_payoffs, [np.mean(history.my_payoffs)])))
        if len(self.average_payoffs) > 0:
            previous_average_payoff = self.average_payoffs[-1]
            if average_payoff > previous_average_payoff:
                self.theta -= self.epsilon
            elif average_payoff < previous_average_payoff:
                self.theta += self.epsilon
        self.average_payoffs.append(average_payoff)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if previous_round_cooperation_rate > self.theta or any((i in self.exploitable_opponents for i, action in enumerate(history.opponent_actions[-1, :]))):
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_467(BaseStrategy):
    """
    A hybrid exploitative strategy that combines elements of tit-for-tat and gradient-based optimization.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate_history = []
        self.payoff_diff_history = []
        self.action_against = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if state.round_number < self.game_description.n_rounds // 2:
            avg_cooperation_rate = np.mean(self.cooperation_rate_history)
            if avg_cooperation_rate > 0.5:
                action = Action.C
            else:
                action = Action.D
        elif state.round_number >= self.game_description.n_rounds // 2:
            delta_payoff = np.mean(np.array(self.payoff_diff_history))
            if delta_payoff > 0:
                action = Action.C
            else:
                action = Action.D
        else:
            num_cooperators = sum(history.opponent_actions[-1, :])
            if num_cooperators < self.game_description.n_players // 2:
                action = Action.D
            else:
                action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if np.mean(history.opponent_actions[:, i]) > 0.8:
                self.action_against[i] = Action.D
            elif np.mean(history.opponent_actions[:, i]) < 0.2:
                self.action_against[i] = Action.C
        self.cooperation_rate_history.append(action == Action.C)
        if len(self.payoff_diff_history) > 0:
            payoff_diff = np.sum(np.where(history.my_payoffs[-1] > 0, history.my_payoffs[-1], 0)) - np.sum(np.where(history.opponent_payoffs[-1, :] > 0, history.opponent_payoffs[-1, :], 0))
            self.payoff_diff_history.append(payoff_diff)
        return action

class Strategy_EXPLOITATIVE_468(BaseStrategy):
    """
    Adaptive Exploiter strategy for N-Player Public Goods Game.
    
    This strategy aims to maximize its payoff by exploiting the cooperative behavior of other players while minimizing its own contributions to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        opponent_defectors = self.game_description.n_players - 1 - opponent_cooperators
        if history.my_payoffs[-1] > np.mean(history.opponent_payoffs[-1, :]):
            self.threshold += 0.1
        else:
            self.threshold -= 0.1
        self.threshold = max(0, min(self.threshold, 1))
        cooperation_probability = self.game_description.k / self.game_description.n_players * (opponent_cooperators / (opponent_cooperators + opponent_defectors)) + self.threshold
        if opponent_cooperators <= opponent_defectors or random.random() >= cooperation_probability:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_469(BaseStrategy):
    """
    The Adaptive Exploiter strategy exploits cooperative behavior while adapting to changing circumstances.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators = 0
        self.total_defectors = 0
        self.k_threshold = 1.5 * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if prev_cooperators >= self.game_description.n_players / 2:
            action = Action.D
        elif self.game_description.k > self.k_threshold and history.my_actions[-1]:
            action = Action.C
        else:
            action = Action.D
        self.total_cooperators += history.my_actions[-1] == True
        self.total_defectors += history.my_actions[-1] == False
        if prev_cooperators > self.game_description.n_players / 2:
            self.k_threshold *= 1.1
        if history.opponent_payoffs.size > 0 and history.opponent_actions.size > 0:
            for i in range(self.game_description.n_players - 1):
                if history.opponent_actions[-1, i] == False and history.my_actions[-1]:
                    action = Action.D
                    break
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_470(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior by adjusting a cooperation threshold.
    Probes opponents' strategies through occasional cooperation and adjusts the threshold based on their responses.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.apc_history = []
        self.ct = game_description.k / game_description.n_players

    def calculate_apc(self) -> float:
        if not self.apc_history:
            return 0.0
        total_payoff = sum((apc for apc, _ in self.apc_history))
        num_cooperators = sum((1 for _, cooperators in self.apc_history if cooperators > 0))
        return total_payoff / num_cooperators if num_cooperators > 0 else 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponents_defected = not all(history.opponent_actions[-1])
        consecutive_cooperation = len(self.apc_history) > 1 and self.apc_history[-2][1] == self.game_description.n_players - 1 and (history.my_payoffs[-1] > 0)
        if opponents_defected:
            self.ct = max(self.ct - 0.1, 0)
        elif consecutive_cooperation:
            self.ct = min(self.ct + 0.1, self.game_description.k / self.game_description.n_players)
        apc = self.calculate_apc()
        if apc <= self.ct:
            action = Action.D
        elif random.random() < 0.1:
            action = Action.C
        else:
            action = Action.D
        num_cooperators = sum((1 for opponent_action in history.opponent_actions[-1] if opponent_action))
        self.apc_history.append((apc, num_cooperators))
        return action

class Strategy_EXPLOITATIVE_471(BaseStrategy):
    """
    Exploitative strategy "Adaptive Predator" for N-Player Public Goods Game.
    
    This strategy is designed to exploit the cooperation of others while minimizing our own contributions.
    By adapting to the level of cooperation in the group, we can maximize our payoffs throughout the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        total_cooperators = sum(history.opponent_actions[-1, :]) + (1 if history.my_actions[-1] else 0)
        if total_cooperators >= self.game_description.n_players / 2:
            return Action.D
        cooperate_prob = self.game_description.k / self.game_description.n_players * (total_cooperators / self.game_description.n_players)
        if self.game_description.n_players % 2 == 1 and total_cooperators == (self.game_description.n_players - 1) // 2:
            cooperate_prob = 0.5
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_472(BaseStrategy):
    """
    The Adaptive Exploiter strategy balances exploitation with adaptability,
    while respecting the game's constraints and assumptions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.round_count = 0
        self.CI = 0.0
        self.DT = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.round_count += 1
        if history is not None and len(history.opponent_actions) > 0:
            opponents_cooperated = sum(history.opponent_actions[-1, :])
            self.CI = (self.CI * (self.round_count - 2) + opponents_cooperated / self.game_description.n_players) / (self.round_count - 1)
        if self.round_count <= 3:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.CI >= self.DT:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_473(BaseStrategy):
    """
    Adaptive Exploiter strategy for the N-Player Public Goods Game.
    
    This strategy aims to maximize its own payoff by adapting to the behavior of other players 
    while exploiting their cooperation. It uses a combination of exploration and exploitation 
    to navigate the game's dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.n_players / 2
        self.opponent_cooperation_rate = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 3:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :])
        self.opponent_cooperation_rate = num_cooperators / self.game_description.n_players
        if history.my_payoffs.size > 0 and history.my_payoffs[-1] < 1 - math.ceil(self.game_description.k * self.opponent_cooperation_rate):
            return Action.D
        if num_cooperators > self.threshold:
            return Action.C
        elif num_cooperators < self.threshold and self.opponent_cooperation_rate < 2 / 3:
            return Action.D
        return Action.C

    def update_threshold(self, history: PlayerHistory):
        opponent_cooperation_rate = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if opponent_cooperation_rate > 0.5:
            self.threshold += 0.05
        elif opponent_cooperation_rate < 0.3:
            self.threshold -= 0.05
        self.threshold = max(1, min(self.game_description.n_players - 1, self.threshold))

class Strategy_EXPLOITATIVE_474(BaseStrategy):
    """
    Exploitative strategy "Adaptive Predator" that adapts to the game's history and opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.history = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        self.history = history
        prev_round_cooperators = history.opponent_actions[-1, :] | history.my_actions[-1]
        avg_c = np.mean(history.opponent_payoffs[-1, prev_round_cooperators])
        avg_d = np.mean(history.opponent_payoffs[-1, ~prev_round_cooperators]) + (1 - history.my_actions[-1])
        if avg_c > avg_d + self.game_description.k / self.game_description.n_players:
            action = Action.C
        else:
            action = Action.D
        if random.random() < 0.01:
            return Action.C
        return action

class Strategy_EXPLOITATIVE_475(BaseStrategy):
    """
    Exploitative strategy that adapts to the cooperative behavior of opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number == 1 and self.game_description.n_players == 2):
            return Action.D
        total_cooperators_prev_round = sum(history.opponent_actions[-1, :])
        avg_opponent_payoff_prev_round = np.mean(history.opponent_payoffs[-1, :])
        my_avg_payoff_prev_round = history.my_payoffs[-1]
        if total_cooperators_prev_round <= self.game_description.n_players / 2:
            return Action.D
        p = self.game_description.k / self.game_description.n_players * (total_cooperators_prev_round / self.game_description.n_players)
        if avg_opponent_payoff_prev_round > my_avg_payoff_prev_round:
            p = min(p + 0.1, 0.9)
        return Action.C if random.random() < p else Action.D

class Strategy_EXPLOITATIVE_477(BaseStrategy):
    """
    Exploitative strategy that takes advantage of the public good while minimizing contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        total_cooperators = sum((1 for action in history.opponent_actions[-1, :] if action))
        threshold = self.game_description.n_players / 2
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif total_cooperators <= threshold or (total_cooperators == threshold and self.game_description.k > 1.5):
            return Action.D
        else:
            p = self.game_description.k / self.game_description.n_players * (total_cooperators / self.game_description.n_players)
            return Action.C if random.random() < p else Action.D

class Strategy_EXPLOITATIVE_478(BaseStrategy):
    """
    The Adaptive Exploiter strategy combines exploration, exploitation, and adaptation to effectively navigate the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_categories = {}
        for i in range(game_description.n_players - 1):
            self.opponent_categories[i] = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        if state.round_number <= 5:
            last_action = history.my_actions[-1]
            return Action.D if last_action else Action.C
        for opponent_index in range(self.game_description.n_players - 1):
            opponent_cooperation_rate = sum(history.opponent_actions[:, opponent_index]) / state.round_number
            opponent_defection_rate = 1 - opponent_cooperation_rate
            recent_behavior = history.opponent_actions[-3:, opponent_index] if state.round_number >= 3 else history.opponent_actions[:state.round_number, opponent_index]
            if opponent_cooperation_rate > 0.6 and opponent_defection_rate < 0.4:
                self.opponent_categories[opponent_index] = 'Cooperator'
            elif opponent_defection_rate > 0.6 and opponent_cooperation_rate < 0.4:
                self.opponent_categories[opponent_index] = 'Defector'
            else:
                self.opponent_categories[opponent_index] = 'Unpredictable'
        if state.round_number % 5 == 0:
            for opponent_index, category in self.opponent_categories.items():
                opponent_cooperation_rate = sum(history.opponent_actions[:, opponent_index]) / state.round_number
                opponent_defection_rate = 1 - opponent_cooperation_rate
                if category == 'Cooperator' and opponent_cooperation_rate > 0.6 and (opponent_defection_rate < 0.4):
                    pass
                elif category == 'Defector' and opponent_defection_rate > 0.6 and (opponent_cooperation_rate < 0.4):
                    pass
        for opponent_index, category in self.opponent_categories.items():
            if category == 'Cooperator':
                return Action.C
            elif category == 'Defector':
                return Action.D
            else:
                return Action.C if random.random() < 0.5 else Action.D
        return Action.C if random.random() < 0.5 else Action.D

class Strategy_EXPLOITATIVE_479(BaseStrategy):
    """
    An exploitative strategy for N-Player Public Goods Game that adapts to various opponent behaviors and maximizes its own payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_rate = [0] * game_description.n_players
        self.prev_coop_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        theta = self.game_description.k / self.game_description.n_players
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        self.coop_rate = [int(opponent_action) for opponent_action in history.opponent_actions[-1]]
        self.prev_coop_count = sum(self.coop_rate)
        avg_coop_rate = np.mean(history.my_payoffs[:state.round_number]) / (self.game_description.n_players * state.round_number)
        if avg_coop_rate < theta or self.prev_coop_count <= self.game_description.n_players // 2:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_480(BaseStrategy):
    """
    The Adaptive Exploiter strategy initially cooperates to gather information about opponents.
    It then adapts its behavior based on the average cooperation rate of opponents, exploiting both cooperative and competitive environments.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.acr = 0
        self.round_num = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.round_num = state.round_number
        if state.round_number == 0:
            return Action.C
        if state.round_number < 5:
            opponent_actions_last_round = history.opponent_actions[-1]
            self.acr += sum(opponent_actions_last_round) / len(opponent_actions_last_round)
            self.acr /= state.round_number
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        k = self.game_description.k
        n = self.game_description.n_players
        if self.acr >= 0.5:
            p = 1 - k / n
            return Action.D if random.random() < p else Action.C
        elif self.acr < 0.2:
            return Action.D
        elif self.acr > 0.8:
            return Action.C
        else:
            p = k / n
            return Action.C if random.random() < p else Action.D

class Strategy_EXPLOITATIVE_481(BaseStrategy):
    """
    Exploitative strategy "Adaptive Threshold" to balance cooperation and exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = None

    def calculate_avg_cooperation(self, opponent_actions):
        coop_count = sum((1 for a in opponent_actions if a))
        return coop_count / (self.game_description.n_players - 1)

    def interpolate(self, avg_coop_prev, k):
        return (0.5 * (2 - k) + avg_coop_prev * (k - 1)) / (3 - k)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        opponent_actions = history.opponent_actions[-1]
        avg_coop_prev = self.calculate_avg_cooperation(opponent_actions)
        if self.game_description.k < 2:
            self.coop_threshold = 0.5
        elif self.game_description.k >= 3:
            self.coop_threshold = avg_coop_prev + 0.1
        else:
            self.coop_threshold = self.interpolate(avg_coop_prev, self.game_description.k)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if sum((1 for a in opponent_actions if not a)) == self.game_description.n_players - 1:
            return Action.D
        if avg_coop_prev > self.coop_threshold:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_482(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive K-Threshold"
    
    This strategy aims to maximize individual payoffs by adapting to the opponents' behavior and exploiting their cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop = 0.0
        self.coop_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C
        if state.round_number == 0 or history is None:
            self.avg_coop = 0.0
            self.coop_count = 0
            return Action.C
        if (state.round_number - 1) % 5 == 0 and state.round_number > 3:
            self.coop_count = sum(history.opponent_actions[-5:, :].sum(axis=1)) + history.my_actions[-5:].sum()
            self.avg_coop = self.coop_count / (self.game_description.n_players * 5)
        if (state.round_number - 1) % 5 == 0 and self.avg_coop < self.game_description.k / 2:
            return Action.C
        if self.avg_coop >= self.game_description.k / 2:
            action = Action.D
        else:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        elif (state.round_number - 3) % 5 == 0 and history.my_actions[-5:].sum() + sum(history.opponent_actions[-5:, :].sum(axis=1)) == 0:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_484(BaseStrategy):
    """
    An exploitative strategy for N-Player Public Goods Game that leverages game parameters 
    and history without relying on communication or specific opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.all_cooperate_or_defect = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.all_cooperate_or_defect is None:
            self.check_all_cooperate_or_defect(history)
        if self.all_cooperate_or_defect == True:
            return Action.C if state.round_number % 2 == 1 else Action.D
        elif self.all_cooperate_or_defect == False:
            return Action.D if state.round_number % 2 == 1 else Action.C
        T = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if T > self.game_description.n_players / 2:
            return Action.D
        APP = np.mean(history.opponent_payoffs[-1, :] + [history.my_payoffs[-1]])
        if APP < self.game_description.k / 2:
            return Action.C
        return Action.D

    def check_all_cooperate_or_defect(self, history: PlayerHistory):
        """
        Check if all players consistently cooperate or defect and update the flag accordingly.
        """
        opponent_actions = history.opponent_actions[:, 0]
        if np.all(opponent_actions) or not np.any(opponent_actions):
            self.all_cooperate_or_defect = np.all(opponent_actions)

class Strategy_EXPLOITATIVE_485(BaseStrategy):
    """
    An exploitative strategy that adapts to the game parameters and history.
    
    The strategy starts by cooperating in the first three rounds, then enters an exploitation phase where it defects if the average cooperation rate is above a certain threshold. It also introduces a punishment mechanism to discourage opponents from defecting. The threshold value is adjusted based on the game's history to balance between cooperation and defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_prev = 0.5
        self.coop_threshold = 0.5

    def calculate_average_cooperation_rate(self, history: PlayerHistory) -> float:
        if len(history.my_actions) == 1:
            return 0
        opponent_cooperators = sum(history.opponent_actions[-2, :])
        total_players = self.game_description.n_players - 1
        avg_coop_prev = (history.my_actions[-2] + opponent_cooperators) / (total_players + 1)
        return avg_coop_prev

    def opponent_defected_last_two_rounds(self, history: PlayerHistory, opponent_index: int) -> bool:
        if len(history.opponent_actions) < 3:
            return False
        return not history.opponent_actions[-2, opponent_index] and (not history.opponent_actions[-1, opponent_index])

    def update_coop_threshold(self, avg_payoffs: NDArray[np.float64]) -> None:
        if len(avg_payoffs) < 3:
            return
        payoff_diff = avg_payoffs[-1] - avg_payoffs[-2]
        if payoff_diff > 0:
            self.coop_threshold -= 0.1
        elif payoff_diff < 0:
            self.coop_threshold += 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number <= 3:
            return Action.C
        self.avg_coop_prev = self.calculate_average_cooperation_rate(history)
        for opponent_index in range(self.game_description.n_players - 1):
            if self.opponent_defected_last_two_rounds(history, opponent_index):
                return Action.D
        if self.avg_coop_prev > self.coop_threshold:
            return Action.D
        else:
            return Action.C
        avg_payoffs = np.mean(history.my_payoffs)
        self.update_coop_threshold(avg_payoffs)

class Strategy_EXPLOITATIVE_486(BaseStrategy):
    """
    A robust and exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        total_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if total_cooperators < self.n_players / 2:
            p = self.k / self.n_players
            if random.random() < p:
                return Action.C
            else:
                return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.D
        if total_cooperators == 0:
            p = self.k / self.n_players
            if random.random() < p:
                return Action.C
        elif total_cooperators == self.n_players:
            return Action.D

class Strategy_EXPLOITATIVE_487(BaseStrategy):
    """
    Exploitative strategy that adapts to the collective cooperation level and identifies opportunities to defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.theta = self.k / self.n_players
        self.theta_low = 0.2 * self.theta

    def calculate_expected_payoffs(self, avg_coop_rate: float) -> tuple[float, float]:
        private_payoff = 1
        coop_benefit = self.k / self.n_players * avg_coop_rate * self.n_players
        E_C = coop_benefit
        E_D = private_payoff + self.k / self.n_players * avg_coop_rate * (self.n_players - 1)
        return (E_C, E_D)

    def decide_action(self, avg_coop_rate: float, last_round: bool) -> Action:
        if last_round:
            return Action.D
        E_C, E_D = self.calculate_expected_payoffs(avg_coop_rate)
        if E_C > E_D:
            return Action.C
        else:
            return Action.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_coop_rates = np.mean(history.opponent_actions, axis=1)
        avg_coop_rate = np.mean(opponent_coop_rates[-1:])
        last_round = state.round_number == self.game_description.n_rounds - 1
        if avg_coop_rate < self.theta_low:
            return Action.D
        return self.decide_action(avg_coop_rate, last_round)

class Strategy_EXPLOITATIVE_488(BaseStrategy):
    """
    Exploits a wide range of opponent behaviors while being robust and adaptive.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.round_num = 0
        self.avg_coop_rate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.round_num = state.round_number
        if history is not None:
            opponent_actions = history.opponent_actions
        if self.round_num <= 3:
            if self.round_num == 1:
                return Action.C
            else:
                return random.choice([Action.C, Action.D])
        elif self.avg_coop_rate is None:
            self.avg_coop_rate = np.mean(opponent_actions[:3, :].sum(axis=1)) / self.game_description.n_players
            opponent_behavior = 'cooperative' if self.avg_coop_rate > 0.5 else 'defective'
            if opponent_behavior == 'cooperative':
                p = min(max(self.game_description.k / self.game_description.n_players * self.avg_coop_rate, 0), 1)
                return Action.C if random.random() < p else Action.D
            else:
                return Action.D
        elif self.round_num == self.game_description.n_rounds - 1:
            return Action.D
        elif abs(self.avg_coop_rate - 0.5) <= 0.1:
            return random.choice([Action.C, Action.D])
        else:
            p = min(max(self.game_description.k / self.game_description.n_players * self.avg_coop_rate, 0), 1)
            return Action.C if random.random() < p else Action.D

class Strategy_EXPLOITATIVE_489(BaseStrategy):
    """
    The Adaptive Exploiter is a robust and adaptive strategy designed to exploit a wide range of opponent behaviors in the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_history = None
        self.cooperation_rate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if not self.opponent_history:
            self.opponent_history = [[] for _ in range(self.game_description.n_players)]
            self.cooperation_rate = []
        opponent_actions_last_round = history.opponent_actions[-1]
        my_action_last_round = history.my_actions[-1]
        for i, action in enumerate(opponent_actions_last_round):
            self.opponent_history[i].append(action)
        if state.round_number < 4:
            avg_cooperation_rate = np.mean(self.cooperation_rate) if self.cooperation_rate else 0.5
            threshold = 0.4
            if all((len(opponent_actions) == len(history.my_payoffs) for opponent_actions in self.opponent_history)):
                threshold = 0.9 if avg_cooperation_rate == 1 else 0.1 if avg_cooperation_rate == 0 else threshold
            if avg_cooperation_rate > threshold:
                return Action.C
            else:
                return random.choice([Action.C, Action.D]) if np.random.rand() < 0.6 else Action.D
        elif state.round_number < self.game_description.n_rounds // 2 + 1:
            majority_action = Action.C if sum(opponent_actions_last_round) > len(opponent_actions_last_round) / 2 else Action.D
            return random.choice([Action.C, Action.D]) if np.random.rand() < 0.5 and sum(opponent_actions_last_round) == len(opponent_actions_last_round) / 2 else majority_action
        else:
            for opponent in range(self.game_description.n_players):
                if self.opponent_consistent_cooperator(opponent, self.opponent_history[opponent]):
                    return Action.D
            majority_action = Action.C if sum(opponent_actions_last_round) > len(opponent_actions_last_round) / 2 else Action.D
            return random.choice([Action.C, Action.D]) if np.random.rand() < 0.5 and sum(opponent_actions_last_round) == len(opponent_actions_last_round) / 2 else majority_action
        self.cooperation_rate.append(my_action_last_round)

    def opponent_consistent_cooperator(self, opponent, history):
        return np.mean(history) > 0.7 if history else False

class Strategy_EXPLOITATIVE_491(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = (game_description.k - 1) / game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        avg_coop_rate = cooperators / self.game_description.n_players
        prev_coop_rate = np.mean(history.my_actions[:-1])
        if state.round_number < self.game_description.n_rounds - 1:
            if avg_coop_rate < self.threshold or (avg_coop_rate == self.threshold and random.random() < 0.5):
                return Action.D
            elif cooperators == 0 and random.random() < 0.5:
                return Action.C
            else:
                return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_492(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy adapts to the game's history and parameters.
    It aims to capitalize on opponents' cooperative behavior while minimizing its own contributions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        opponent_actions = history.opponent_actions
        if state.round_number < 5:
            majority_cooperated = sum(opponent_actions[-1, :]) > self.game_description.n_players / 2
            return Action.C if majority_cooperated else Action.D
        acr = sum([sum(round) for round in opponent_actions[-3:, :]]) / (3 * self.game_description.n_players)
        if acr > self.game_description.k / self.game_description.n_players:
            return Action.C
        if state.round_number >= self.game_description.n_rounds - 1:
            total_payoff = sum(history.my_payoffs[:-1])
            avg_payoff_per_round = total_payoff / (state.round_number - 1)
            return Action.C if total_payoff > avg_payoff_per_round else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_493(BaseStrategy):
    """
    A strategy for the N-Player Public Goods Game that adapts based on the game's history and parameters,
    aiming to maximize personal payoff while minimizing cooperation from others.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_coop = np.mean(history.opponent_actions, axis=1).mean() + history.my_actions.mean()
        if avg_coop >= self.game_description.n_players / 2:
            return Action.D
        if avg_coop < self.game_description.n_players / 2 and self.game_description.k < 1.5:
            return Action.C
        return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_495(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to exploit the behavior of opponents 
    while adapting to their actions over time. This strategy does not rely on coordination 
    mechanisms or shared norms.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.TC_prev = 0
        self.APPC_prev = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        TC_curr = sum(history.opponent_actions[-1, :])
        APPC_curr = self.game_description.k / self.game_description.n_players * TC_curr if TC_curr > 0 else 0
        if self.APPC_prev > 1:
            if TC_curr >= self.TC_prev or (TC_curr == 1 and state.round_number < self.game_description.n_rounds - 1):
                return Action.C
            else:
                return Action.D
        return Action.D
        self.TC_prev = TC_curr
        self.APPC_prev = APPC_curr
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_496(BaseStrategy):
    """
    Exploitative strategy that adapts to the game's history and exploits opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        if state.round_number < self.game_description.n_rounds // 2:
            prev_coops = sum(history.opponent_actions[-1, :])
            if prev_coops > self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        coop_threshold = self.game_description.n_players / (self.game_description.k + 1)
        prev_coops = sum(history.opponent_actions[-1, :])
        if prev_coops >= coop_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_497(BaseStrategy):
    """
    An exploitative and adaptive strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_coop_rate = np.mean(history.opponent_actions)
        if state.round_number < self.game_description.n_rounds // 2 and avg_coop_rate > 0.4:
            return Action.D
        my_cumulative_payoff = np.sum(history.my_payoffs)
        avg_payoff = np.mean(np.sum(history.opponent_payoffs, axis=0))
        if state.round_number >= self.game_description.n_rounds // 2 and my_cumulative_payoff < avg_payoff:
            return Action.D
        prev_round_cooperators = sum(history.opponent_actions[-1, :])
        if prev_round_cooperators > self.game_description.n_players / 2:
            return Action.C
        else:
            return Action.D
        if state.round_number > int(0.8 * self.game_description.n_rounds) and my_cumulative_payoff < avg_payoff:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_498(BaseStrategy):
    """
    An adaptive strategy that exploits opponents' behavior in a public goods game.
    It starts by cooperating to probe opponents, then adapts its decision based on the number of cooperators and expected payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 1 / game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_coop_count = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        coop_payoff = self.game_description.k / self.game_description.n_players * prev_coop_count
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif prev_coop_count == 0 and state.round_number > 0:
            return Action.C
        if history.my_payoffs.size > 0:
            total_payoff = np.sum(history.my_payoffs)
            avg_payoff = total_payoff / (state.round_number + 1)
            if total_payoff > avg_payoff * (state.round_number + 1):
                self.threshold -= 0.05
            else:
                self.threshold += 0.05
        if coop_payoff > self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_499(BaseStrategy):
    """
    A strategy that balances cooperation and exploitation in a public goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.m = 5
        self.coop_threshold = 0.6
        self.defect_threshold = 0.3
        self.opponent_actions = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponents_coop_rates = []
        for opponent_action in history.opponent_actions[:, :]:
            coop_rate = self.calculate_avg_coop_rate(opponent_action, self.m)
            opponents_coop_rates.append(coop_rate)
        all_cooperated = all((rate == 1 for rate in opponents_coop_rates))
        if all_cooperated:
            return Action.C
        avg_coop_rate = sum(opponents_coop_rates) / len(opponents_coop_rates)
        if avg_coop_rate >= self.coop_threshold:
            return Action.C
        elif any((rate < self.defect_threshold for rate in opponents_coop_rates)):
            return Action.D
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

    @staticmethod
    def calculate_avg_coop_rate(actions: NDArray[np.bool_], m: int) -> float:
        recent_actions = actions[-m:]
        return np.mean(recent_actions)

class Strategy_EXPLOITATIVE_500(BaseStrategy):
    """
    Adaptive Exploiter strategy for Public Goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.explore_phase = True
        self.punishment_mode = False
        self.punishment_rounds_remaining = 0
        self.estimated_average_cooperation_rate = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or self.explore_phase:
            self.explore_phase = state.round_number < 3
            return Action.C
        opponent_actions = history.opponent_actions[:state.round_number, :]
        self.estimated_average_cooperation_rate = np.mean(opponent_actions)
        if self.punishment_mode:
            self.punishment_rounds_remaining -= 1
            if self.punishment_rounds_remaining == 0:
                self.punishment_mode = False
            return Action.D
        elif self.estimated_average_cooperation_rate < 0.5:
            self.punishment_mode = True
            self.punishment_rounds_remaining = 2
            return Action.D
        elif self.estimated_average_cooperation_rate > self.game_description.k / self.game_description.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_501(BaseStrategy):
    """
    An adaptive strategy that exploits opponents' behavior while adapting to changes in their actions over time.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.num_cooperators = None
        self.num_defectors = None
        self.avg_payoff_coop = None
        self.avg_payoff_defect = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        opponent_actions = history.opponent_actions[-1, :]
        my_action = bool(history.my_actions[-1])
        self.num_cooperators = sum(opponent_actions) + int(my_action)
        self.num_defectors = len(opponent_actions) - self.num_cooperators
        if self.avg_payoff_coop is None:
            self.avg_payoff_coop = history.my_payoffs[-1] if my_action else 0
            self.avg_payoff_defect = history.my_payoffs[-1] if not my_action else 0
        else:
            self.avg_payoff_coop += (history.my_payoffs[-1] - self.avg_payoff_coop) / state.round_number
            self.avg_payoff_defect += (history.my_payoffs[-1] - self.avg_payoff_defect) / state.round_number
        if self.avg_payoff_coop >= self.avg_payoff_defect:
            return Action.C
        else:
            return Action.D
        for player in range(len(opponent_actions)):
            opponent_action = bool(opponent_actions[player])
            opponent_payoff = history.opponent_payoffs[-1, player]
            if opponent_action == False and opponent_payoff > self.avg_payoff_coop:
                if state.round_number >= 2 and bool(history.opponent_actions[-2, player]):
                    return Action.C
            elif opponent_action == True and opponent_payoff < self.avg_payoff_defect:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if math.isclose(self.avg_payoff_coop, self.avg_payoff_defect):
            return Action.C

class Strategy_EXPLOITATIVE_502(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to exploit the cooperation of others while minimizing its own contributions,
    adapting to the evolving game dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        prev_round_coop = np.sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        cr_t_minus_1 = prev_round_coop / self.game_description.n_players
        avg_payoff_coop_t_minus_1 = self.game_description.k / self.game_description.n_players * cr_t_minus_1 * self.game_description.n_players
        avg_payoff_defect_t_minus_1 = 1 + self.game_description.k / self.game_description.n_players * cr_t_minus_1 * self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if avg_payoff_coop_t_minus_1 > avg_payoff_defect_t_minus_1:
            p_coop = max(0.5, min(1, avg_payoff_coop_t_minus_1 / (avg_payoff_coop_t_minus_1 + avg_payoff_defect_t_minus_1)))
            return random.choice([Action.C, Action.D], p=[p_coop, 1 - p_coop])
        else:
            return Action.D

class Strategy_EXPLOITATIVE_504(BaseStrategy):
    """
    An adaptive exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def calculate_average_cooperation_rate(self, history: PlayerHistory) -> float:
        opponent_actions = history.opponent_actions[:-1]
        average_cooperation_rate = np.mean(opponent_actions)
        return average_cooperation_rate

    def calculate_my_payoff(self, history: PlayerHistory) -> float:
        my_payoffs = history.my_payoffs[:-1]
        if len(my_payoffs) == 0:
            my_payoff = 0.0
        else:
            my_payoff = np.sum(my_payoffs)
        return my_payoff

    def calculate_average_payoff(self, history: PlayerHistory) -> float:
        opponent_payoffs = history.opponent_payoffs[:-1]
        if len(opponent_payoffs) == 0:
            average_payoff = 0.0
        else:
            average_payoff = np.mean(np.sum(opponent_payoffs, axis=0))
        return average_payoff

    def get_previous_round_total_contribution(self, history: PlayerHistory) -> int:
        opponent_actions = history.opponent_actions[-1]
        total_cooperators = sum(opponent_actions)
        if self.game_description.n_players > 1:
            total_cooperators += history.my_actions[-1]
        return total_cooperators

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        average_cooperation_rate = self.calculate_average_cooperation_rate(history)
        my_payoff = self.calculate_my_payoff(history)
        average_payoff = self.calculate_average_payoff(history)
        previous_round_total_contribution = self.get_previous_round_total_contribution(history)
        if 1 <= state.round_number <= self.game_description.n_rounds // 2:
            if average_cooperation_rate > 0.5:
                return Action.D
            else:
                return Action.C
        elif self.game_description.n_rounds // 2 < state.round_number <= 3 * self.game_description.n_rounds // 4:
            if my_payoff < average_payoff:
                return Action.D
            elif previous_round_total_contribution > self.game_description.n_players // 2:
                return Action.C
            else:
                return Action.D
        elif 3 * self.game_description.n_rounds // 4 < state.round_number <= self.game_description.n_rounds - 1:
            if my_payoff > average_payoff:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_505(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' cooperation levels and times defections to maximize payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.0
        self.round_1_cooperators = 0
        self.round_2_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        if state.round_number == 2:
            self.cooperation_threshold = (self.round_1_cooperators + self.round_2_cooperators) / (2 * self.game_description.n_players)
        if history is not None:
            if state.round_number == 1:
                self.round_1_cooperators = sum(history.opponent_actions[0, :])
            elif state.round_number == 2:
                self.round_2_cooperators = sum(history.opponent_actions[-1, :])
        if history is not None:
            expected_cooperators = sum(np.mean(history.opponent_actions[-3:, :], axis=0) > 0.5)
            if expected_cooperators >= self.game_description.n_players * self.cooperation_threshold:
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_506(BaseStrategy):
    """
    Adaptive Exploiter strategy for N-Player Public Goods Game.

    This strategy initially explores opponents' behavior by cooperating,
    then adapts to exploit or cooperate based on the average cooperation rate.
    It also punishes exploitable opponents with a high probability.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = None
        self.exploitable_opponents = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None and len(history.opponent_actions) > 0:
            opponent_coop_rates = np.mean(history.opponent_actions[-1, :], axis=0)
            self.avg_coop_rate = np.sum(opponent_coop_rates) / self.game_description.n_players
        if state.round_number <= 3:
            return Action.C
        if self.avg_coop_rate is None or math.isnan(self.avg_coop_rate):
            return Action.D
        if self.avg_coop_rate > 0.5:
            action = Action.D
        elif self.avg_coop_rate < 0.5:
            action = Action.C
        else:
            action = random.choices([Action.C, Action.D], weights=[0.6, 0.4])[0]
        if state.round_number > 1 and self.avg_coop_rate is not None and (self.avg_coop_rate > 0.5):
            opponent_defectors = np.where(history.opponent_actions[-2, :] == Action.D)[0]
            for i in opponent_defectors:
                self.exploitable_opponents.add(i)
        if state.round_number < self.game_description.n_rounds:
            if action == Action.D and random.random() < 0.8 and (len(self.exploitable_opponents) > 0):
                opponent = random.choice(list(self.exploitable_opponents))
        return action

    def __getstate__(self):
        return {'game_description': self.game_description, 'avg_coop_rate': self.avg_coop_rate, 'exploitable_opponents': list(self.exploitable_opponents)}

    def __setstate__(self, state):
        self.__dict__.update(state)

class Strategy_EXPLOITATIVE_507(BaseStrategy):
    """
    An adaptive exploitative strategy for the N-Player Public Goods Game.
    It gathers information in initial rounds, adapts to opponents' behavior,
    exploits high cooperation rates, and punishes defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.5
        self.punishment_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        if state.round_number <= 3:
            return Action.C if random.random() < 0.5 else Action.D
        recent_coop_rates = np.mean(history.opponent_actions[-2:, :], axis=0)
        self.avg_coop_rate = np.mean(recent_coop_rates)
        if self.avg_coop_rate > 0.5:
            return Action.D if random.random() < 0.7 else Action.C
        else:
            return Action.C if random.random() < 0.3 else Action.D
        opponent_coop_rates = np.mean(history.opponent_actions, axis=0)
        for i, rate in enumerate(opponent_coop_rates):
            if rate > 0.8:
                return Action.D
            elif history.opponent_actions[-1, i] == False and history.my_payoffs[-1] < self.game_description.k / self.game_description.n_players:
                self.punishment_rounds = 2
                break
        if self.punishment_rounds > 0:
            self.punishment_rounds -= 1
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_508(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Predator" to maximize payoff by adapting to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_coop = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            return Action.C if random.random() < 0.5 else Action.D
        opponent_coop = sum(history.opponent_actions[-2:, :].flatten())
        self.total_coop += opponent_coop
        coop_rate = self.total_coop / ((state.round_number + 1) * self.game_description.n_players)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if coop_rate > 0.5:
            return Action.C if random.random() < self.game_description.k / self.game_description.n_players * coop_rate else Action.D
        else:
            return Action.D if random.random() < 1 - self.game_description.k / self.game_description.n_players * coop_rate else Action.C

class Strategy_EXPLOITATIVE_509(BaseStrategy):
    """
    An adaptive strategy that exploits opponents' behavior while being robust to various playing styles.
    Cooperates in the initial round, then defects if the total number of cooperators is high enough,
    and always defects in the last round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number + 1 == self.game_description.n_rounds:
            return Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if prev_cooperators >= self.game_description.k:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_510(BaseStrategy):
    """
    An adaptive strategy that balances exploration and exploitation in the Public Goods Game.
    
    Initially explores opponents' behaviors with a random cooperation probability, 
    then adapts to the average number of cooperators in previous rounds, 
    exploiting situations where cooperation is more beneficial. 
    Finally, exploits the last round by defecting.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k
        self.r = game_description.n_rounds
        self.cooperation_probabilities = [0.5] * self.n

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < 3:
            return Action.C if random.random() < self.cooperation_probabilities[state.round_number % self.n] else Action.D
        elif state.round_number > 0 and state.round_number < self.r - 1:
            previous_cooperators = np.sum(history.opponent_actions[-3:, :], axis=0).mean()
            if previous_cooperators >= self.n / 2:
                cooperation_probability = self.k / self.n
            else:
                cooperation_probability = 0
            return Action.C if random.random() < cooperation_probability else Action.D
        elif state.round_number == self.r - 1:
            return Action.D
        for opponent in range(self.n):
            if history.opponent_actions[-1, opponent]:
                self.cooperation_probabilities[opponent] = min(1.0, self.cooperation_probabilities[opponent] + 0.1)
            else:
                self.cooperation_probabilities[opponent] = max(0.0, self.cooperation_probabilities[opponent] - 0.1)
        return Action.D

class Strategy_EXPLOITATIVE_511(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperation levels of other players while adapting to their behavior over time.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploit_mode = False
        self.punishment_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        n_cooperators = np.sum(history.opponent_actions, axis=1)
        avg_cooperators = np.mean(n_cooperators)
        CT = self.game_description.k / self.game_description.n_players * avg_cooperators
        if not self.exploit_mode and CT > 1:
            return Action.C
        elif not self.exploit_mode and CT <= 1:
            self.exploit_mode = True
            return Action.D
        elif self.exploit_mode:
            opponents_defected = np.any(history.opponent_actions[-1, :] == False)
            if opponents_defected:
                self.punishment_rounds += 2
                return Action.D
            else:
                self.exploit_mode = False
                self.punishment_rounds = 0
                return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if avg_cooperators / self.game_description.n_players > 0.8:
            return Action.C

class Strategy_EXPLOITATIVE_512(BaseStrategy):
    """
    An adaptive strategy that exploits opponents while adapting to their behaviors over time.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5
        self.total_cooperators = None
        self.my_payoff_history = []
        self.opponent_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_opponent_actions = history.opponent_actions[-1]
        self.total_cooperators = np.sum(prev_round_opponent_actions) + (history.my_actions[-1] == True)
        public_good_share = self.game_description.k / self.game_description.n_players * self.total_cooperators
        private_payoff = 1 - history.my_actions[-1]
        if public_good_share >= private_payoff:
            action = Action.C
        else:
            action = Action.D
        my_avg_payoff = np.mean(self.my_payoff_history) if self.my_payoff_history else 0
        opponent_payoffs_avg = [np.mean(opponent_payoffs) for opponent_payoffs in history.opponent_payoffs.T]
        max_opponent_payoff_avg = max(opponent_payoffs_avg)
        if max_opponent_payoff_avg > my_avg_payoff + self.threshold:
            mirrored_strategy = np.argmax(opponent_payoffs_avg)
            action = Action.C if history.opponent_actions[-1, mirrored_strategy] else Action.D
        return action

class Strategy_COLLECTIVE_53(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Reputation-Based Trust

    This strategy aims to balance individual self-interest with collective welfare by adaptively cooperating based on the reputation of fellow players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.reputations = np.zeros(game_description.n_players)
        self.theta = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        for i in range(self.game_description.n_players):
            if i != self.game_description.n_players - 1:
                self.reputations[i] += 1 if opponent_actions[i] else -1
                self.reputations[i] = max(self.reputations[i], -self.game_description.n_players)
        avg_reputation = np.mean(self.reputations)
        if avg_reputation > self.theta and self.reputations[self.game_description.n_players - 1] >= 0:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_78(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Learning (ACL)
    
    ACL is a collective strategy that balances individual self-interest with collective well-being. 
    It adapts to the game's history and adjusts its cooperation level based on the observed behavior of other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = (game_description.k - 1) / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_payoff_prev_round = np.mean(history.opponent_payoffs[-1, :])
        cooperation_index = np.sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if cooperation_index > 0.5:
            self.threshold = min(self.threshold + 0.01, self.game_description.k - 1)
        else:
            self.threshold = max(0, self.threshold - 0.01)
        if avg_payoff_prev_round > self.threshold:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_176(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) Strategy for N-Player Public Goods Game.
    
    This strategy balances individual self-interest with collective good, adapting to observed game history.
    It initially cooperates to establish an initial cooperative tone and then responds based on the number of cooperators in previous rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.recently_punished = False
        self.punishment_rounds_remaining = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        last_round_opponent_actions = history.opponent_actions[-1, :]
        num_coop_last = sum(last_round_opponent_actions)
        if num_coop_last > self.game_description.n_players / 2:
            return Action.C
        elif num_coop_last <= self.game_description.n_players / 2 and any(last_round_opponent_actions):
            coop_payoff = self.game_description.k / self.game_description.n_players * num_coop_last
            if coop_payoff >= self.game_description.k / self.game_description.n_players * history.my_actions[-1]:
                return Action.C
            else:
                return Action.D
        elif sum(history.opponent_actions[-1, :]) < self.game_description.n_players // 2 and (not self.recently_punished):
            self.recently_punished = True
            self.punishment_rounds_remaining = 3
            return Action.D
        else:
            if self.punishment_rounds_remaining > 0:
                self.punishment_rounds_remaining -= 1
                return Action.D
            if sum(history.opponent_actions[-1, :]) > num_coop_last:
                self.recently_punished = False
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_215(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy.
    
    This collective strategy adapts to changing opponent behaviors, 
    balancing individual self-interest with collective cooperation and adaptability.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_rate = 0.5
        self.prev_actions = [None] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum((1 for action in history.opponent_actions[-1, :] + [history.my_actions[-1]] if action))
        if prev_cooperators > self.game_description.n_players / 2:
            return Action.C
        elif prev_cooperators < self.game_description.n_players / 2 and history.my_payoffs[-1] < np.mean(history.opponent_payoffs[-1, :]):
            opponent_actions = history.opponent_actions[-1, :]
            return Action.C if sum(opponent_actions) >= len(opponent_actions) / 2 else Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_225(BaseStrategy):
    """
    Adaptive Reciprocal Altruism strategy. Initially promotes cooperation, 
    adapts to majority behavior, punishes defectors, and prioritizes self-interest.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponents = {}
        self.majority_cooperated = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < min(5, self.game_description.n_rounds // 2):
            return Action.C
        opponent_actions = history.opponent_actions[-1]
        cooperators = sum(opponent_actions)
        self.majority_cooperated = cooperators > self.game_description.n_players / 2
        for i, action in enumerate(opponent_actions):
            if action and history.my_payoffs[-1] < self.game_description.k / self.game_description.n_players * (self.game_description.n_players / 2):
                self.opponents[i] = 'defector'
                return Action.D
        if history.my_payoffs[-1] < self.game_description.k / self.game_description.n_players * (self.game_description.n_players / 2):
            return Action.D
        if self.majority_cooperated:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_290(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop = 0.0
        self.total_defect = 0
        self.coop_rate_i = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        coop_threshold = self.game_description.n_players / 2 + self.game_description.k / 4
        defect_limit = self.game_description.n_rounds / 4
        if history:
            opponent_coop = sum(history.opponent_actions[-1, :])
            self.avg_coop += (opponent_coop - self.avg_coop) / state.round_number
            self.total_defect += int(not history.my_actions[-1])
        if state.round_number < 3:
            return Action.C
        elif self.avg_coop >= coop_threshold:
            return Action.C
        elif self.total_defect > defect_limit and state.round_number % 2 == 0:
            return Action.D
        else:
            avg_coop_rate = sum(self.coop_rate_i) / self.game_description.n_players
            if history and history.my_payoffs.size > 0:
                my_coop_rate = sum(history.my_actions) / state.round_number
                return Action.C if my_coop_rate >= avg_coop_rate else Action.D
        if state.round_number == self.game_description.n_rounds - 2 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C if self.avg_coop >= coop_threshold else Action.D
        return Action.D

class Strategy_COLLECTIVE_395(BaseStrategy):
    """
    A strategy that prioritizes cooperation while adaptively responding to the group's behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_coop_count = 0
        self.prev_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        prev_coop_count = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        self.prev_coop_count = prev_coop_count
        self.prev_action = history.my_actions[-1]
        if prev_coop_count > self.game_description.n_players / 2:
            return Action.C
        elif prev_coop_count <= self.game_description.n_players / 2 and self.prev_action == True:
            return Action.D
        elif state.round_number > 1 and self.prev_action == False and (prev_coop_count > self.game_description.n_players / 2):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_439(BaseStrategy):
    """
    A collective strategy called "Adaptive Cooperative Threshold" (ACT).
    ACT is designed to balance individual self-interest with collective well-being,
    while being robust to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        opponent_actions_prev = history.opponent_actions[-1, :]
        avg_coop_prev = np.mean(opponent_actions_prev)
        if avg_coop_prev >= self.game_description.k / self.game_description.n_players:
            return Action.C
        else:
            return random.choice([Action.C, Action.D])

class Strategy_COLLECTIVE_448(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective well-being.
    It encourages initial cooperation, adapts to opponents' behavior based on their average cooperation rate,
    and retaliates against defectors to maintain cooperation among other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.0
        self.coop_prob = game_description.k / game_description.n_players
        self.retaliation_rounds = [-1] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        elif state.round_number <= 4:
            self.avg_coop_rate = np.mean(history.opponent_actions[1:state.round_number, :])
            return Action.D
        else:
            if self.avg_coop_rate >= 0.5 and random.random() < self.coop_prob:
                action = Action.C
            elif state.round_number == self.game_description.n_rounds - 1:
                return Action.C
            else:
                action = Action.D
            for i, opponent_defected in enumerate(history.opponent_actions[state.round_number - 1, :]):
                if not opponent_defected and state.round_number - self.retaliation_rounds[i] <= 2:
                    action = Action.D
                    break
        return action

class Strategy_COLLECTIVE_454(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with cooperation.
    Adapts to the game's history, punishes defectors, and forgives cooperators.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punish_opponents = [False] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        ci = total_cooperators / self.game_description.n_players
        for opponent_idx in range(self.game_description.n_players - 1):
            if history.opponent_actions[-1, opponent_idx] == False and ci > 0.75:
                self.punish_opponents[opponent_idx] = True
            elif self.punish_opponents[opponent_idx] and history.opponent_actions[-1, opponent_idx] == True:
                self.punish_opponents[opponent_idx] = False
        if ci >= 0.5 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif any(self.punish_opponents):
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_484(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Retaliation (ACR)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.num_coop = None
        self.avg_coop_rate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.avg_coop_rate is None:
            n = self.game_description.n_players
            r = self.game_description.n_rounds
            min_rounds = min(3, r)
            num_coop_per_round = np.sum(history.opponent_actions[:min_rounds, :], axis=1) + history.my_actions[:min_rounds]
            self.num_coop = num_coop_per_round / n
            self.avg_coop_rate = np.mean(self.num_coop)
        if state.round_number < min(3, self.game_description.n_rounds):
            action = Action.C
        elif self.avg_coop_rate >= 0.5:
            action = Action.C
        else:
            action = Action.D
        opponents_defected = np.sum(history.opponent_actions[-1, :] == False)
        if opponents_defected > 0 and self.num_coop[-1] > 0:
            coop_rates = np.mean(history.opponent_actions[:state.round_number - 1, :], axis=0) + history.my_actions[state.round_number - 2]
            min_rate_idx = np.argmin(coop_rates)
            action = Action.D if history.opponent_actions[-1, min_rate_idx] == False else action
        return action

class Strategy_EXPLOITATIVE_1(BaseStrategy):
    """
    An exploitative strategy designed to adapt to opponents' behavior in an N-player Public Goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_labels = [None] * game_description.n_players
        self.cooperation_rates = [0.0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < 0.5 else Action.D
        wacr = self.calculate_wacr()
        if wacr > 0.5:
            return Action.C
        else:
            return Action.D

    def calculate_wacr(self) -> float:
        """
        Calculate the weighted average cooperation rate (WACR).
        """
        total_cooperation_rate = 0.0
        for i, opponent_label in enumerate(self.opponent_labels):
            if opponent_label == 'COOP':
                total_cooperation_rate += self.game_description.k / self.game_description.n_players
            elif opponent_label == 'DEF':
                total_cooperation_rate -= (1 - self.game_description.k) / self.game_description.n_players
        return total_cooperation_rate / self.game_description.n_players

    def update_opponent_labels(self, history: PlayerHistory):
        """
        Update opponent labels and cooperation rates based on their actions.
        """
        for i in range(self.game_description.n_players):
            if self.opponent_labels[i] == 'COOP' and (not history.opponent_actions[-1, i]):
                self.opponent_labels[i] = 'DEF'
                self.cooperation_rates[i] = 0.0

class Strategy_EXPLOITATIVE_48(BaseStrategy):
    """
    An adaptive exploiter strategy for public goods games.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.c_rate = None
        self.exp_threshold = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 1 - 1 / self.game_description.n_players else Action.D
        self.c_rate = np.mean(history.opponent_actions[:, :], axis=0)
        self.exp_threshold = 1 - self.game_description.k / self.game_description.n_players
        opponent_c_rates_above_threshold = self.c_rate > self.exp_threshold
        if opponent_c_rates_above_threshold.any():
            defect_prob = (self.c_rate[opponent_c_rates_above_threshold] - self.exp_threshold) / (1 - self.exp_threshold)
            return Action.D if random.random() < np.mean(defect_prob) else Action.C
        cooperate_prob = (1 - self.c_rate) / (1 - self.exp_threshold)
        return Action.C if random.random() < np.mean(cooperate_prob) else Action.D
        opponent_defected_and_self_cooperated_last_round = history.opponent_actions[-1, :] == 0 and history.my_actions[-1] == True
        if opponent_defected_and_self_cooperated_last_round.any():
            self.c_rate[opponent_defected_and_self_cooperated_last_round] += 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_64(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behaviors and estimates their cooperation rates.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponents = {i: 'UNREL' for i in range(1, game_description.n_players + 1)}
        self.cooperation_rates = {i: 0.5 for i in range(1, game_description.n_players + 1)}
        self.history = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if history is None:
            self.history = []
            return Action.C
        for opponent, action in enumerate(history.opponent_actions[-1, :]):
            coop_rate = self.cooperation_rates[opponent + 1]
            if action:
                coop_rate += 1 / (state.round_number + 1)
            else:
                coop_rate -= 1 / (state.round_number + 1)
            coop_rate = max(0, min(coop_rate, 1))
            self.cooperation_rates[opponent + 1] = coop_rate
            if coop_rate >= 0.7:
                self.opponents[opponent + 1] = 'COOP'
            elif coop_rate <= 0.3:
                self.opponents[opponent + 1] = 'DEF'
        if state.round_number < 0.9 * self.game_description.n_rounds:
            for opponent, profile in self.opponents.items():
                if profile == 'COOP':
                    coop_prob = (1 + len([opp for opp, prof in self.opponents.items() if prof == 'COOP'])) / self.game_description.n_players
                    return Action.C if random.random() < coop_prob else Action.D
                elif profile == 'DEF':
                    return Action.D
                else:
                    expected_coop_payoff = self.game_description.k / self.game_description.n_players * len([opp for opp, prof in self.opponents.items() if prof == 'COOP'])
                    if expected_coop_payoff > 1 - self.game_description.k / self.game_description.n_players * len([opp for opp, prof in self.opponents.items() if prof == 'COOP']):
                        return Action.C
                    else:
                        return Action.D
        cooperators = sum(self.cooperation_rates.values())
        if cooperators >= self.game_description.n_players // 2:
            return Action.D
        else:
            expected_coop_payoff = self.game_description.k / self.game_description.n_players * len([opp for opp, prof in self.opponents.items() if prof == 'COOP'])
            if expected_coop_payoff > 1 - self.game_description.k / self.game_description.n_players * len([opp for opp, prof in self.opponents.items() if prof == 'COOP']):
                return Action.C
            else:
                return Action.D
        for opponent, action in enumerate(history.opponent_actions[-1, :]):
            if not action and (self.history is None or self.history[-1][0] == Action.C):
                return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_78(BaseStrategy):
    """
    Adaptive Exploiter Strategy: 
    A history-dependent, adaptive strategy that aims to exploit opponents' behaviors while minimizing losses.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_crs = np.zeros(game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_cooperators = np.sum(history.opponent_actions[-1, :])
        self._update_opponent_crs(history)
        if state.round_number % 5 == 0:
            self._adapt_to_changes()
        if prev_round_cooperators >= self.game_description.n_players // 2:
            return Action.C
        for opponent, cr in enumerate(self.opponent_crs):
            if cr > 0.7:
                return Action.D
        return Action.D

    def _update_opponent_crs(self, history: PlayerHistory) -> None:
        for opponent in range(len(self.opponent_crs)):
            opponent_actions = history.opponent_actions[:, opponent]
            self.opponent_crs[opponent] = np.mean(opponent_actions)

    def _adapt_to_changes(self) -> None:
        for opponent, cr in enumerate(self.opponent_crs):
            if cr < 0.7:
                pass

    def _last_round_cooperate(self, state: GameState) -> Action:
        return Action.C

class Strategy_EXPLOITATIVE_103(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and maximizes payoffs based on game history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rates = None
        self.observation_phase = True

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        elif self.observation_phase and (state.round_number == 3 or state.round_number == 4):
            if history is not None:
                opponent_actions = history.opponent_actions
                num_rounds = state.round_number + 1
                self.cooperation_rates = np.sum(opponent_actions[:num_rounds, :], axis=0) / num_rounds
            return Action.C
        elif self.observation_phase and state.round_number == 5:
            self.observation_phase = False
            if history is not None:
                opponent_actions = history.opponent_actions
                num_rounds = state.round_number
                self.cooperation_rates = np.sum(opponent_actions[:num_rounds, :], axis=0) / num_rounds
        else:
            if history is not None and (state.round_number - 1) % 5 == 0:
                opponent_actions = history.opponent_actions
                num_rounds = state.round_number
                self.cooperation_rates = np.sum(opponent_actions[:num_rounds, :], axis=0) / num_rounds
            if self.cooperation_rates is not None:
                if np.any(self.cooperation_rates > 0.7):
                    return Action.D
                else:
                    return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_118(BaseStrategy):
    """
    Adaptive Exploiter strategy for N-Player Public Goods Game.
    Balances cooperation and exploitation, adapting to various opponent behaviors while prioritizing self-interest.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.ocr = np.zeros(game_description.n_players)
        self.theta = game_description.k / game_description.n_players + 0.01
        self.exploiting = False
        self.exploit_rounds_left = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        avg_ocr = np.mean(self.ocr)
        for i, action in enumerate(history.opponent_actions[-1, :]):
            self.ocr[i] = ((state.round_number - 2) * self.ocr[i] + action) / (state.round_number - 1)
        if self.exploiting:
            self.exploit_rounds_left -= 1
            if self.exploit_rounds_left == 0:
                self.exploiting = False
            return Action.D
        if avg_ocr >= self.theta or (state.round_number == self.game_description.n_rounds - 1 and (not self.exploiting)):
            return Action.C
        else:
            self.exploiting = True
            self.exploit_rounds_left = max(2, min(self.game_description.n_rounds - state.round_number, 3))
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_175(BaseStrategy):
    """
    An adaptive strategy that exploits opponents' cooperation in the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.0
        self.opp_behavior = [None] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            if state.round_number == 0:
                return Action.C
            else:
                return Action(random.choice([True, False]))
        avg_coop_rate = self._update_avg_coop_rate(history)
        if avg_coop_rate > 0.5:
            return Action(False)
        else:
            return Action(True)
        for i, opp_action in enumerate(history.opponent_actions[-1]):
            if opp_action:
                self.opp_behavior[i] = True
                return Action(random.choice([True, False]))
            else:
                self.opp_behavior[i] = False
                return Action(False)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action(False)
        elif sum(self.opp_behavior) == len(self.opp_behavior):
            return Action(False)
        elif any((behavior is None for behavior in self.opp_behavior)):
            return Action(False)

    def _update_avg_coop_rate(self, history: PlayerHistory) -> float:
        """Update the average cooperation rate based on previous rounds' data."""
        if history is None:
            return self.avg_coop_rate
        total_contributions = sum(history.opponent_actions[:-1].sum(axis=1))
        n_rounds = len(history.my_payoffs)
        avg_coop_rate = total_contributions / (n_rounds * self.game_description.n_players - 1)
        alpha = 0.2
        self.avg_coop_rate = alpha * avg_coop_rate + (1 - alpha) * self.avg_coop_rate
        return self.avg_coop_rate

class Strategy_EXPLOITATIVE_193(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.w = 5
        self.θ = 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        opponent_actions = history.opponent_actions[-1, :]
        total_cooperators_prev_round = sum(opponent_actions)
        if total_cooperators_prev_round <= self.n_players / 2:
            total_payoff = self.k / self.n_players * total_cooperators_prev_round
            cooperate_prob = total_payoff / self.k * (self.n_players - total_cooperators_prev_round)
            action = Action.C if random.random() < cooperate_prob else Action.D
        else:
            action = Action.D
        opponent_cooperation_rates = []
        for i in range(self.n_players):
            coop_rate = sum(opponent_actions[i::self.n_players]) / min(state.round_number, self.w)
            opponent_cooperation_rates.append(coop_rate)
            if coop_rate >= self.θ:
                action = Action.D
        return action

class Strategy_EXPLOITATIVE_259(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy focuses on adaptability and exploitativeness.
    It starts by cooperating in the initial round, then observes opponents' actions 
    to calculate an average cooperation rate (ACR). Based on ACR and the game's k/n value,
    it decides whether to cooperate or defect. AE also implements a punishment mechanism
    for frequent defectors and defects in the final round.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.acr_threshold = 0.5 if game_description.n_players > 2 else 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperations = sum(history.opponent_actions[-1, :])
        acr = opponent_cooperations / self.game_description.n_players
        opponent_defections_last_round = np.sum(np.logical_not(history.opponent_actions[-1, :]))
        punish_probability = 0.7 if opponent_defections_last_round > self.game_description.n_players / 2 else 0.3
        punish = random.random() < punish_probability
        kn_value = self.game_description.k / self.game_description.n_players
        if acr > self.acr_threshold:
            return Action.C
        elif acr <= self.acr_threshold and kn_value < acr:
            return Action.D
        else:
            cooperate_probability = kn_value / acr if acr != 0 else 1.0
            return Action.C if random.random() < cooperate_probability else Action.D
        if punish:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_340(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy balances individual payoffs with collective benefits 
    while adapting to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_contributions = 0
        self.avg_cooperator_payoff = 0
        self.avg_defector_payoff = 0
        self.opponent_behavior = {}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_players = self.game_description.n_players
        k = self.game_description.k
        threshold = k / n_players * (n_players / 2)
        for i in range(n_players):
            if i not in self.opponent_behavior:
                self.opponent_behavior[i] = []
        for i in range(n_players):
            if i != self.game_description.n_players - 1:
                self.opponent_behavior[i].append(history.opponent_actions[-1, i])
                if history.opponent_actions[-1, i]:
                    self.avg_cooperator_payoff += history.opponent_payoffs[-1, i]
                else:
                    self.avg_defector_payoff += history.opponent_payoffs[-1, i]
        avg_cooperator_payoff = self.avg_cooperator_payoff / (state.round_number + 1)
        avg_defector_payoff = self.avg_defector_payoff / (state.round_number + 1)
        self.total_contributions += sum(history.opponent_actions[-1, :])
        if self.total_contributions >= threshold:
            action = Action.C
        elif avg_defector_payoff > avg_cooperator_payoff:
            action = Action.D
        else:
            action = Action.C
        for i in range(n_players):
            if i != self.game_description.n_players - 1 and all(self.opponent_behavior[i]):
                action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if self.total_contributions >= threshold:
                action = Action.C
            else:
                action = Action.D
        return action

class Strategy_EXPLOITATIVE_368(BaseStrategy):
    """
    Adaptive Exploiter strategy combines elements of Tit-for-Tat, Win-Stay-Lose-Shift, and Pavlov's strategy.
    It adapts to opponents' behaviors while maintaining an exploitative approach.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_profiles = [None] * game_description.n_players
        self.previous_actions = []
        self.successful_exploits = 0
        self.aggression_threshold = 0.75

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or len(self.previous_actions) == 0:
            return Action.C
        if len(self.previous_actions) >= self.game_description.n_rounds // 3:
            self.update_profiles(history)
            action = None
            for i in range(self.game_description.n_players):
                if self.opponent_profiles[i] == 'Cooperator':
                    action = Action.D
                elif self.opponent_profiles[i] == 'Defector':
                    action = Action.C
                else:
                    action = self.mirror_previous_action(history, i)
        else:
            cooperators = sum(self.previous_actions[-1])
            if cooperators >= self.game_description.n_players // 2:
                return Action.C
            for i in range(self.game_description.n_players):
                if history.opponent_payoffs[-1, i] > np.mean(history.my_payoffs):
                    action = Action.D
                else:
                    action = Action.C
        if state.round_number > 0 and (history is not None and history.my_payoffs[-1] > 1.5 * np.mean(history.my_payoffs)):
            return self.maintain_action(action)
        elif state.round_number > 0 and (history is not None and history.my_payoffs[-1] < 0.75 * np.mean(history.my_payoffs)):
            return self.switch_action(action)
        if self.successful_exploits >= 2 * self.game_description.n_rounds / 3:
            self.adjust_aggression(0.9)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        self.previous_actions.append([action.value for _ in range(self.game_description.n_players)])
        return action

    def update_profiles(self, history):
        opponent_cooperation_rates = []
        for i in range(self.game_description.n_players):
            cooperation_rate = sum(history.opponent_actions[:, i]) / len(history.opponent_actions)
            if cooperation_rate >= self.aggression_threshold:
                self.opponent_profiles[i] = 'Cooperator'
            elif cooperation_rate <= 1 - self.aggression_threshold:
                self.opponent_profiles[i] = 'Defector'
            else:
                self.opponent_profiles[i] = 'Mixed'

    def mirror_previous_action(self, history, opponent_index):
        if history.opponent_actions[-1, opponent_index]:
            return Action.C
        else:
            return Action.D

    def maintain_action(self, action):
        return action

    def switch_action(self, action):
        if action == Action.C:
            return Action.D
        else:
            return Action.C

    def adjust_aggression(self, new_threshold):
        self.aggression_threshold = new_threshold

class Strategy_EXPLOITATIVE_369(BaseStrategy):
    """
    Exploit opponent behavior by tracking average payoff per cooperator and adapting to changes.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.appc_values = None
        self.max_appc = 0
        self.prev_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        k = self.game_description.k
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_payoffs = history.my_payoffs[-1]
        opponent_prev_round_payoffs = history.opponent_payoffs[-1]
        self.appc_values = np.array([opp_payoff / self.prev_cooperators if self.prev_cooperators > 0 else 0 for opp_payoff in opponent_prev_round_payoffs])
        if self.prev_cooperators == 0:
            avg_defector_payoff = prev_round_payoffs + np.sum(opponent_prev_round_payoffs) / n_players
            self.appc_values[:] = avg_defector_payoff
        max_appc_idx = np.argmax(self.appc_values)
        self.max_appc = self.appc_values[max_appc_idx]
        at_least_one_defected = not history.opponent_actions[-1, :].all()
        expected_payoff_from_cooperation = k / n_players * np.sum(history.opponent_actions[-1, :]) if self.prev_cooperators > 0 else prev_round_payoffs + k
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if self.prev_cooperators >= n_players / 2 else Action.D
        return Action.C if expected_payoff_from_cooperation >= self.max_appc or at_least_one_defected else Action.D
        self.prev_cooperators = np.sum(history.opponent_actions[-1, :])

class Strategy_EXPLOITATIVE_398(BaseStrategy):
    """
    An exploitative strategy for the N-player public goods game, 
    aiming to maximize payoff by adapting to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_action = None
        self.last_round_payoff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        k_factor = self.game_description.k
        rounds = self.game_description.n_rounds
        if state.round_number == 0 or history is None:
            return random.choices([Action.C, Action.D], weights=[0.5, 0.5])[0]
        opponent_actions = history.opponent_actions
        opponent_payoffs = history.opponent_payoffs
        prev_round_opponents = opponent_actions[-1]
        avg_coop_prev_round = sum(prev_round_opponents) / n_players
        if avg_coop_prev_round >= k_factor / n_players:
            return random.choices([Action.C, Action.D], weights=[0.8, 0.2])[0]
        prev_payoff = history.my_payoffs[-1]
        coop_avg_payoff = sum((payoff for action, payoff in zip(opponent_actions[-1], opponent_payoffs[-1]) if action)) / n_players
        if prev_payoff < coop_avg_payoff and self.prev_action == Action.D:
            return Action.C
        opponent_prev_defect = False
        for i, (action, payoff) in enumerate(zip(opponent_actions[-1], opponent_payoffs[-1])):
            if action is False and payoff > prev_payoff:
                opponent_prev_defect = True
                break
        if prev_payoff <= coop_avg_payoff and opponent_prev_defect:
            return Action.D
        if state.round_number == rounds - 1:
            return Action.D
        self.prev_action = history.my_actions[-1]
        self.last_round_payoff = prev_payoff
        return random.choices([Action.C, Action.D], weights=[0.5, 0.5])[0]

class Strategy_EXPLOITATIVE_431(BaseStrategy):
    """
    A robust and exploitative strategy for the N-Player Public Goods Game.
    Adapts to various opponent behaviors while maximizing payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.5
        self.history = {i: [] for i in range(1, game_description.n_players + 1)}
        self.payoffs = {i: [] for i in range(1, game_description.n_players + 1)}

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        for i in range(1, self.game_description.n_players + 1):
            self.history[i].append(history.opponent_actions[-1, i - 1] if i != self.game_description.n_players else history.my_actions[-1])
            self.payoffs[i].append(history.opponent_payoffs[-1, i - 1] if i != self.game_description.n_players else history.my_payoffs[-1])
        num_coop_prev = sum((self.history[i][-1] for i in range(1, self.game_description.n_players + 1)))
        if num_coop_prev < self.game_description.n_players / 2:
            return Action.D
        for j in range(1, self.game_description.n_players + 1):
            if not self.history[j][-1] and self.history[self.game_description.n_players][-1]:
                defect_prob = 0.7
                if random.random() < defect_prob:
                    return Action.D
        max_payoff_player = max(range(1, self.game_description.n_players + 1), key=lambda i: self.payoffs[i][-1])
        imitate_coop_prob = 0.8 if self.history[max_payoff_player][-1] else 0.2
        if random.random() < imitate_coop_prob:
            return Action.C
        coop_rates = {i: sum(self.history[i][-3:]) / min(3, state.round_number) for i in range(1, self.game_description.n_players + 1)}
        coop_opponent = next((i for i in range(1, self.game_description.n_players + 1) if coop_rates[i] > 0.6), None)
        if coop_opponent:
            cooperate_prob = 0.9
        else:
            cooperate_prob = 0.7
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_445(BaseStrategy):
    """
    An exploitative strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_classification = [None] * game_description.n_players
        self.coop_counts = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        for i in range(self.game_description.n_players):
            if history.my_actions[-1]:
                self.coop_counts[i] += 1
        for i in range(self.game_description.n_players - 1):
            coop_rate = self.coop_counts[i + 1] / state.round_number
            if coop_rate > 0.5:
                self.opponent_classification[i] = 'COOP'
            elif coop_rate < 0.4:
                self.opponent_classification[i] = 'DEF'
            else:
                self.opponent_classification[i] = 'UNPRED'
        coop_or_unpred_opponents = sum((1 for opponent in self.opponent_classification if opponent in ['COOP', 'UNPRED']))
        defector_opponents = sum((1 for opponent in self.opponent_classification if opponent == 'DEF'))
        if coop_or_unpred_opponents > (self.game_description.n_players - 1) // 2:
            return Action.C
        elif defector_opponents > (self.game_description.n_players - 1) // 2:
            return Action.D
        elif self.game_description.k > 1.5:
            return Action.C
        else:
            return Action.D
        if state.round_number > 0.8 * self.game_description.n_rounds:
            average_payoff = history.my_payoffs.mean()
            expected_value_coop = self.game_description.k / self.game_description.n_players * self.game_description.n_players
            if average_payoff < expected_value_coop:
                return Action.D

class Strategy_EXPLOITATIVE_456(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.phase = 'Initial'
        self.history = [[] for _ in range(game_description.n_players)]
        self.observation_phase_rounds = math.ceil(game_description.n_rounds / 2)
        self.exploitation_phase_rounds = math.floor(game_description.n_rounds / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        if self.phase == 'Observation' and state.round_number <= self.observation_phase_rounds:
            for i in range(self.game_description.n_players):
                self.history[i].append(history.opponent_actions[-1, i])
            if state.round_number % 2 == 0:
                return Action.C
            else:
                return Action.D
        if self.phase == 'Observation' and state.round_number > self.observation_phase_rounds:
            self.phase = 'Exploitation'
        if self.phase == 'Exploitation':
            for i in range(self.game_description.n_players):
                coop_rate = sum(self.history[i]) / len(self.history[i])
                if coop_rate > 0.5:
                    return Action.D
                else:
                    return Action.C
        if state.round_number >= self.game_description.n_rounds - self.exploitation_phase_rounds:
            for i in range(self.game_description.n_players):
                coop_rate = sum(self.history[i][-self.exploitation_phase_rounds:]) / self.exploitation_phase_rounds
                if coop_rate > 0.5 and self(history, state) == Action.D:
                    return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_466(BaseStrategy):
    """
    Adaptive Exploiter strategy for the N-Player Public Goods Game.
    Cooperates when others cooperate and defects when they don't, while adapting to changes in total payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_cooperators = 0
        self.prev_total_payoff = 0
        self.threshold = game_description.n_players // 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if self.prev_cooperators >= self.threshold:
            action = Action.C
        else:
            action = Action.D
        curr_total_payoff = sum(history.my_payoffs) + np.sum(history.opponent_payoffs, axis=0).sum()
        if curr_total_payoff > self.prev_total_payoff * 1.1:
            adjustment = (curr_total_payoff - self.prev_total_payoff) / self.prev_total_payoff
            if adjustment > 0 and self.prev_cooperators >= self.game_description.n_players // 2:
                self.threshold -= 1
            elif adjustment < 0 and self.prev_cooperators < self.game_description.n_players // 2:
                self.threshold += 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        self.prev_total_payoff = curr_total_payoff
        return action

class Strategy_EXPLOITATIVE_483(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' cooperation rates while prioritizing self-interest.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_coop_history = np.zeros(game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        elif state.round_number <= self.game_description.n_rounds // 2:
            avg_coop_rate = np.mean(history.opponent_actions[-1, :])
            if avg_coop_rate > 0.5:
                return Action.D
            else:
                p_coop = avg_coop_rate / (self.game_description.n_players - 1)
                return Action.C if random.random() < p_coop else Action.D
        else:
            for i, coop_count in enumerate(self.opponent_coop_history):
                if coop_count >= state.round_number / 2:
                    action_against_i = Action.D
                else:
                    p_coop = 1 / (self.game_description.n_players - 1 - coop_count)
                    action_against_i = Action.C if random.random() < p_coop else Action.D
            return action_against_i
        self.opponent_coop_history += history.opponent_actions[-1, :]

class Strategy_EXPLOITATIVE_490(BaseStrategy):
    """
    A robust and exploitative strategy for the N-Player Public Goods Game.
    
    Decision Rules:
    1. Initial Round: Cooperate with a probability of 50%.
    2. Middle Rounds: 
       - If the average cooperation rate among all players in the previous round is above k/n, Defect.
       - Otherwise, Cooperate with a probability adjusted based on game parameters.
    3. Last Round: Defect.

    Adaptive Component:
    Monitor the average cooperation rate among opponents over the last few rounds and adjust the cooperation probability accordingly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.move_window_size = min(5, game_description.n_rounds)
        self.cooperation_probabilities = []
        self.opponent_actions_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        opponent_actions_last_round = history.opponent_actions[-1, :]
        opponent_cooperations_last_round = sum(opponent_actions_last_round)
        avg_coop_rate = (opponent_cooperations_last_round + int(history.my_actions[-1])) / self.game_description.n_players
        if state.round_number < self.game_description.n_rounds - 1:
            p = self.game_description.k / self.game_description.n_players / (1 + self.game_description.k / self.game_description.n_players)
            opponent_cooperations_window = [sum(self.opponent_actions_history[i]) for i in range(max(0, len(self.opponent_actions_history) - self.move_window_size), len(self.opponent_actions_history))]
            avg_coop_rate_window = np.mean(opponent_cooperations_window) / self.game_description.n_players
            if avg_coop_rate_window > self.game_description.k / self.game_description.n_players:
                p /= 2
            elif avg_coop_rate_window < (self.game_description.k - 1) / self.game_description.n_players:
                p *= 1.5
            p = max(0, min(p, 1))
            if random.random() < p:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D
        self.cooperation_probabilities.append(p)
        self.opponent_actions_history.append(opponent_actions_last_round)
        return Action.D

class Strategy_EXPLOITATIVE_494(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.contribution_threshold = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < self.game_description.n_players:
            return Action.D
        avg_contribution_rate = np.mean(history.opponent_actions)
        cooperation_probability = 1 - 1 / self.contribution_threshold
        if avg_contribution_rate > self.contribution_threshold:
            cooperation_probability *= 0.9
        return Action.C if random.random() < cooperation_probability else Action.D

class Strategy_COLLECTIVE_513(BaseStrategy):
    """
    K-Trigger strategy for N-Player Public Goods Game.
    
    This strategy adapts to various opponent behaviors by cooperating in the first round, 
    then calculating a trigger value based on the number of cooperators in the previous round. 
    It cooperates if the total payoff from cooperation is greater than or equal to the trigger value.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate = True
        self.trigger_value = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        c_prev = sum(history.opponent_actions[-1, :] == True)
        self.trigger_value = self.game_description.k * c_prev / self.game_description.n_players
        cooperate_payoff = self.game_description.k / self.game_description.n_players * c_prev
        if cooperate_payoff >= self.trigger_value:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_514(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Reciprocity (ACGR) strategy.
    Prioritizes cooperation and reciprocity while adapting to opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_threshold = 0.5
        self.p_coop_initial = 0.6
        self.p_punish = 0.8

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            opponent_actions = history.opponent_actions[-2, :]
            proportion_cooperators = np.mean(opponent_actions)
            if proportion_cooperators > self.p_threshold:
                p_coop = min(self.p_coop_initial + 0.05 * state.round_number, 0.9)
                for opponent_actions_last_round in history.opponent_actions[-2, :]:
                    if not opponent_actions_last_round:
                        p_coop -= 0.1
                    else:
                        p_coop += 0.05
                        p_coop = min(p_coop, 0.9)
                return Action.C if random.random() < p_coop else Action.D
            elif proportion_cooperators <= self.p_threshold:
                opponent_defections = np.sum(~history.opponent_actions[-2:, :], axis=0)
                for i, opponent_defection in enumerate(opponent_defections):
                    if opponent_defection == 2:
                        return Action.D if random.random() < self.p_punish else Action.C
            return Action.D

class Strategy_COLLECTIVE_515(BaseStrategy):
    """
    Adaptive Cooperation with Retaliation (ACR) strategy.
    
    Cooperates initially, then cooperates if at least half of opponents cooperate,
    defects otherwise. Also includes a retaliation mechanism to punish persistent 
    non-cooperation and a forgiveness mechanism to re-establish cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators_prev = 0
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if total_cooperators >= self.game_description.n_players / 2:
            action = Action.C
            self.consecutive_defections = 0
        elif self.consecutive_defections < 2:
            action = Action.D
            self.consecutive_defections += 1
        else:
            action = Action.D
        if action == Action.D and total_cooperators > self.total_cooperators_prev:
            self.consecutive_defections = 0
        self.total_cooperators_prev = total_cooperators
        return action

class Strategy_COLLECTIVE_517(BaseStrategy):
    """
    A collective strategy that adapts to the group's behavior, rewarding cooperation 
    when it is beneficial to the collective and punishing exploitation to maintain cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punishment_rounds = 2
        self.punished_opponents = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round = state.round_number - 1
        total_cooperators = sum(history.opponent_actions[previous_round, :]) + int(history.my_actions[previous_round])
        if state.round_number < self.game_description.n_rounds // 3:
            return Action.C if total_cooperators >= self.game_description.n_players * self.game_description.k / (2 * self.game_description.n_players) else Action.D
        elif state.round_number < 2 * self.game_description.n_rounds // 3:
            previous_cooperators = history.opponent_actions[previous_round, :] + history.my_actions[previous_round]
            previous_defectors = ~previous_cooperators
            average_cooperator_payoff = np.mean(history.opponent_payoffs[previous_round, :][previous_cooperators]) if sum(previous_cooperators) > 0 else 0
            average_defector_payoff = np.mean(history.opponent_payoffs[previous_round, :][previous_defectors]) if sum(previous_defectors) > 0 else 0
            if average_cooperator_payoff >= average_defector_payoff:
                return Action.C
            else:
                p = (average_defector_payoff - average_cooperator_payoff) / (self.game_description.k - 1)
                return Action.D if random.random() < p else Action.C
        elif state.round_number >= self.game_description.n_rounds // 3:
            return Action.C if total_cooperators >= self.game_description.n_players * self.game_description.k / (2 * self.game_description.n_players) else Action.D
        for i, opponent_action in enumerate(history.opponent_actions[previous_round, :]):
            if opponent_action and total_cooperators >= self.game_description.n_players * self.game_description.k / (2 * self.game_description.n_players):
                self.punished_opponents.add(i)
        if state.round_number - previous_round < self.punishment_rounds and history.my_actions[previous_round]:
            for opponent in self.punished_opponents:
                if not history.opponent_actions[previous_round, opponent]:
                    return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_COLLECTIVE_518(BaseStrategy):
    """
    A collective strategy that balances individual payoff maximization with promoting cooperation among all players.
    It adapts to the game's history and opponents' behaviors while maintaining a robust and fair approach.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number % (self.game_description.n_rounds // 4) == 2:
            return Action.C
        majority_cooperated = sum(history.opponent_actions[-1, :]) > self.game_description.n_players / 2
        if majority_cooperated:
            return Action.C
        else:
            p = (self.game_description.k - 1) / (self.game_description.n_players - self.game_description.k)
            return Action.D if random.random() < p else Action.C

class Strategy_COLLECTIVE_519(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that balances cooperation and defection based on game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_payoff_prev = np.mean(history.my_payoffs[-1:] + history.opponent_payoffs[-1, :].tolist())
        if avg_payoff_prev >= self.game_description.k / 2 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        else:
            p_defect = 1 - avg_payoff_prev / self.game_description.k
            return Action.D if random.random() < p_defect else Action.C

class Strategy_COLLECTIVE_520(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperativity with History-Based Adjustment (ACHA)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(game_description.n_players / 2) - 1
        self.punishment_flag = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        prev_payoff = history.my_payoffs[-1]
        avg_payoff = np.mean(history.my_payoffs)
        if prev_cooperators >= self.game_description.n_players / 2:
            return Action.C
        elif prev_cooperators < self.game_description.n_players / 2 and prev_payoff < avg_payoff:
            self.punishment_flag = True
            return Action.D
        else:
            self.punishment_flag = False
            return Action.C
        if state.round_number % 5 == 0:
            total_payoff = sum(history.my_payoffs[-5:]) / 5
            if total_payoff < self.game_description.k * (self.cooperation_threshold + 1) / self.game_description.n_players:
                self.cooperation_threshold += 1
            elif total_payoff >= self.game_description.k * (self.cooperation_threshold + 1) / self.game_description.n_players:
                self.cooperation_threshold -= 1
        if state.round_number == self.game_description.n_rounds - 1 and prev_cooperators >= self.game_description.n_players / 2:
            return Action.C

class Strategy_COLLECTIVE_521(BaseStrategy):
    """
    Adaptive Cooperative Equilibrium (ACE) Strategy.
    
    A Collective Strategy for N-Player Public Goods Game that adapts to opponents' strategies,
    incentivizing cooperation while deterring defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.ACR = 0.0
        self.MACR = np.zeros(10)
        self.coop_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D]) if random.random() < 0.5 else Action.D
        if state.round_number <= 3:
            return Action.C if random.random() < self.coop_prob else Action.D
        opponent_actions_last_round = history.opponent_actions[-1, :]
        n_opponents = len(opponent_actions_last_round)
        self.ACR = np.sum(opponent_actions_last_round) / n_opponents
        if self.ACR >= 0.5:
            action = Action.C
        else:
            action = Action.D
        if state.round_number % 5 == 0:
            self.MACR[:-1] = self.MACR[1:]
            self.MACR[-1] = self.ACR
            macr_avg = np.mean(self.MACR)
            if macr_avg > 0.6:
                self.coop_prob += 0.1
            elif macr_avg < 0.4:
                self.coop_prob -= 0.1
            self.coop_prob = max(0.0, min(self.coop_prob, 1.0))
        return action if state.round_number != self.game_description.n_rounds else Action.D

class Strategy_COLLECTIVE_522(BaseStrategy):
    """
    Adaptive Cooperation with Self-Interest (ACSI) strategy.

    This strategy prioritizes cooperation when it benefits the group but adapts to self-interest when others defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.prev_round_payoffs = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number < 6:
            action = Action.C if state.round_number % 2 == 0 else Action.D
            self.prev_round_payoffs = history.my_payoffs[:-1] if self.prev_round_payoffs is None else self.prev_round_payoffs[1:] + [history.my_payoffs[-1]]
            return action
        avg_payoff = np.mean(self.prev_round_payoffs) if self.prev_round_payoffs is not None else 0
        action = Action.C if avg_payoff >= self.game_description.k else Action.D
        if history.my_actions[-1] == Action.C and np.any(history.opponent_actions[-1, :] == Action.D):
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.C
        self.prev_round_payoffs = history.my_payoffs[:-1] if self.prev_round_payoffs is None else self.prev_round_payoffs[1:] + [history.my_payoffs[-1]]
        return action

class Strategy_COLLECTIVE_523(BaseStrategy):
    """
    A dynamic strategy that balances individual self-interest with collective well-being.
    It adapts to various opponent behaviors and game conditions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = max(game_description.n_players / 4, 1)
        self.exploration_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        total_cooperators = sum(opponent_actions) + (history.my_actions[-1] if state.round_number > 1 else True)
        if total_cooperators < self.threshold:
            return Action.D
        if total_cooperators > self.game_description.n_players / 2:
            return Action.C
        self.exploration_rounds += 1
        if self.exploration_rounds == 5:
            action = random.choice([Action.C, Action.D])
            self.exploration_rounds = 0
            return action
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if total_cooperators > self.game_description.n_players / 2 else Action.D
        if state.round_number % 10 == 0 and state.round_number > 0:
            avg_cooperation_rate = sum(history.my_actions[state.round_number - 10:state.round_number]) / (self.game_description.n_rounds * 10)
            for i in range(state.round_number - 10, state.round_number):
                avg_cooperation_rate += sum(history.opponent_actions[i, :])
            avg_cooperation_rate /= self.game_description.n_players * 10
            if avg_cooperation_rate > 0.6:
                self.threshold -= 1
            elif avg_cooperation_rate < 0.4:
                self.threshold += 1
        return Action.C

class Strategy_COLLECTIVE_524(BaseStrategy):
    """
    This strategy adapts to the game's history by adjusting cooperation levels 
    based on the average number of cooperators and a dynamic threshold.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop = 0.0
        self.coop_thresh = game_description.k / game_description.n_players * 0.3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        else:
            prev_round_actions = history.opponent_actions[-1, :]
            num_coop_prev_round = sum(prev_round_actions)
            self.avg_coop = num_coop_prev_round / self.game_description.n_players
            if self.avg_coop >= self.game_description.k / self.game_description.n_players * 0.5:
                self.coop_thresh = self.game_description.k / self.game_description.n_players * 0.7
            else:
                self.coop_thresh = self.game_description.k / self.game_description.n_players * 0.3
            if num_coop_prev_round >= self.coop_thresh:
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_525(BaseStrategy):
    """
    Adaptive Collective Cooperation Strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_threshold = 0.5
        self.p_c = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2 or state.round_number == self.game_description.n_rounds:
            return Action.C
        avg_coop = np.mean(history.opponent_actions[-1, :])
        if avg_coop >= self.avg_coop_threshold:
            action = Action.C
        else:
            self.p_c = min(1, state.round_number / self.game_description.n_rounds * 0.5)
            action = Action.D if random.random() > self.p_c else Action.C
            if np.all(history.opponent_actions[-2:, :] == False):
                p_c_high = min(1, self.p_c + 0.2)
                action = Action.C if random.random() < p_c_high else Action.D
        return action

class Strategy_COLLECTIVE_526(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective well-being 
    by adapting to the average cooperation level of others.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_c = [0.5]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        theta = self.game_description.k / self.game_description.n_players
        avg_c_t_1 = np.mean(history.opponent_actions[-1, :])
        self.avg_c.append(avg_c_t_1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if avg_c_t_1 >= theta:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_527(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with the need for cooperation.
    It adapts to the level of cooperation observed and adjusts its own behavior accordingly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        c_prev = sum(history.opponent_actions[-1, :] & history.my_actions[-1])
        coop_ratio = (c_prev + int(history.my_actions[-1])) / self.game_description.n_players
        if coop_ratio > 0.5:
            theta = (self.game_description.k - 1) / self.game_description.k
        elif coop_ratio == 0.5:
            theta = 0.5
        else:
            theta = (1 - self.game_description.k) / (self.game_description.n_players - 1)
        if len(history.opponent_actions) > 2 and state.round_number > 1:
            c_prev_avg = sum((1 for i in range(-3, 0) for action in [history.my_actions[i]] + history.opponent_actions[i].tolist() if action))
            current_coop_ratio = c_prev_avg / (self.game_description.n_players * 2)
        else:
            current_coop_ratio = coop_ratio
        if current_coop_ratio >= theta:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_528(BaseStrategy):
    """
    Adaptive Collective Optimism Strategy.
    
    This strategy encourages cooperation by initially cooperating and reciprocating cooperative actions.
    It punishes defectors, imitates the majority, and adapts to changing game dynamics by adjusting its threshold for cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.8
        self.threshold = game_description.n_players // 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        opponent_actions = history.opponent_actions[-1, :]
        num_cooperators_prev_round = sum(opponent_actions)
        defected_and_got_high_payoff = any((not opponent_action and payoff > 1.5 for opponent_action, payoff in zip(opponent_actions, history.opponent_payoffs[-1, :])))
        if num_cooperators_prev_round > self.game_description.n_players // 2:
            action = Action.C
        elif defected_and_got_high_payoff:
            action = Action.D
        else:
            if num_cooperators_prev_round > self.game_description.n_players // 2:
                action = Action.C
            else:
                action = Action.D
            my_payoff = history.my_payoffs[-1]
            opponent_avg_payoff = np.mean(history.opponent_payoffs[-1, :])
            if my_payoff > opponent_avg_payoff:
                self.threshold -= 1
            elif my_payoff < opponent_avg_payoff:
                self.threshold += 1
        if state.round_number == self.game_description.n_rounds - 1 and num_cooperators_prev_round >= self.threshold:
            action = Action.C
        return action

class Strategy_COLLECTIVE_529(BaseStrategy):
    """
    Adaptive Collective Conscience (ACC) strategy.
    
    This strategy balances individual self-interest with the greater good, adapting to the game's history and opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :] == True)
        if cooperators > self.game_description.n_players / 2:
            return Action.C
        p = 1 - self.game_description.k / self.game_description.n_players
        if random.random() < p:
            return Action.D
        two_thirds_rounds = max(0, state.round_number // 3 * 2)
        biased_coop_count = sum([sum(history.opponent_actions[i, :] == True) > self.game_description.n_players / 2 for i in range(max(0, state.round_number - two_thirds_rounds), state.round_number)])
        if biased_coop_count >= two_thirds_rounds:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_530(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation.

    Balances individual self-interest with collective well-being by adapting to the game's history and exploiting the benefits of cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_payoff_prev_round = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1])
        if avg_payoff_prev_round == self.game_description.k:
            return Action.C
        elif avg_payoff_prev_round > self.game_description.k:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_531(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective well-being,
    adapting to changes in the Cooperation Quotient.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5
        self.previous_CQ = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        cooperators = sum(history.opponent_actions[-1, :]) + int(bool(history.my_actions[-1]))
        CQ = cooperators / self.game_description.n_players
        if self.previous_CQ is not None:
            if CQ > self.previous_CQ:
                self.theta -= 0.05
            elif CQ < self.previous_CQ:
                self.theta += 0.05
        self.previous_CQ = CQ
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif CQ >= self.theta:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_532(BaseStrategy):
    """
    The Adaptive Community Focus (ACF) strategy prioritizes collective benefits over individual gains.
    It encourages cooperation when it's beneficial to the community and punishes defection to promote a stable environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round = history.opponent_actions[-1, :]
        cooperators = np.sum(prev_round)
        defectors = self.game_description.n_players - 1 - cooperators
        if cooperators >= defectors:
            return Action.C
        elif self.game_description.k / self.game_description.n_players * cooperators >= 1:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if np.sum(history.opponent_actions[-2, :]) >= (self.game_description.n_players - 1) // 2:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_533(BaseStrategy):
    """
    A collective strategy called "Adaptive Cooperative Threshold" (ACT) 
    designed to balance individual self-interest with collective welfare, 
    while adapting to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = 0.0
        self.total_contributions = 0.0
        self.trigger_defect = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.k > 2.5 else Action.D
        total_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        self.total_contributions += total_cooperators
        self.cooperation_threshold = self.total_contributions / state.round_number * (self.game_description.k / self.game_description.n_players)
        if 2 <= state.round_number < 5:
            self.cooperation_threshold *= 0.8
        expected_cooperation_payoff = 1 - int(history.my_actions[-1]) + self.cooperation_threshold
        expected_defection_payoff = 1 + self.game_description.k / self.game_description.n_players * (total_cooperators / self.game_description.n_players)
        if expected_cooperation_payoff >= expected_defection_payoff:
            action = Action.C
        else:
            action = Action.D
        if history.opponent_actions[-1, 0] == False and history.my_actions[-1]:
            self.trigger_defect = True
            action = Action.D
        if self.trigger_defect and state.round_number % 2 == 0:
            action = Action.D
        elif not self.trigger_defect or (self.trigger_defect and state.round_number % 2 != 0):
            self.trigger_defect = False
        return action

class Strategy_COLLECTIVE_534(BaseStrategy):
    """
    Collective Strategy: Adaptive Tit-for-Tat with Public Goods Twist.

    This strategy balances individual self-interest with collective well-being by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop = 0.0
        self.threshold = 1 - game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_cooperators = np.sum(history.opponent_actions[:state.round_number - 1])
        self.avg_coop = total_cooperators / (self.game_description.n_players * (state.round_number - 1))
        if self.avg_coop >= self.threshold:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_535(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with History-Driven Adaptation.
    
    This strategy adapts to the collective behavior of players while promoting cooperation.
    It cooperates in the first round and responds to the average payoffs of cooperators and defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperator_avg_payoff = None
        self.defector_avg_payoff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        previous_round = history.my_payoffs.shape[0] - 1
        cooperators = np.where(history.opponent_actions[previous_round, :] | history.my_actions[previous_round])[0]
        defectors = np.where(~history.opponent_actions[previous_round, :] & ~history.my_actions[previous_round])[0]
        if len(cooperators) > 0:
            self.cooperator_avg_payoff = np.mean(history.opponent_payoffs[previous_round, cooperators])
        else:
            self.cooperator_avg_payoff = 0
        if len(defectors) > 0:
            self.defector_avg_payoff = np.mean(history.opponent_payoffs[previous_round, defectors] + history.my_payoffs[previous_round])
        else:
            self.defector_avg_payoff = 0
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if self.cooperator_avg_payoff >= self.defector_avg_payoff:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_536(BaseStrategy):
    """
    Collective strategy that balances individual self-interest with cooperation benefits,
    adapting to opponents' behavior using a simple learning mechanism.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_coop_prev = np.mean(history.opponent_actions[-1, :])
        if avg_coop_prev > self.coop_threshold or (avg_coop_prev == self.coop_threshold and random.random() < 0.5):
            action = Action.C
        else:
            action = Action.D
        if state.round_number < self.game_description.n_rounds - 1:
            avg_payoff = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1, :])
            if avg_payoff > 1.5:
                self.coop_threshold += 0.05
            elif avg_payoff < 1.2:
                self.coop_threshold -= 0.05
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_COLLECTIVE_537(BaseStrategy):
    """
    A strategy that adapts to the cooperation rate of other players.
    It starts by cooperating, then adjusts its behavior based on the average cooperation rate.
    If the cooperation rate falls below a certain threshold, it punishes non-cooperators by defecting.
    In the endgame, it cooperates if the cooperation rate is high enough.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.5
        self.cooperators_t_minus_1 = 0
        self.punishment_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.cooperators_t_minus_1 = sum(history.opponent_actions[-1, :] + [Action.C.value])
        avg_coop_rate_window_size = max(1, math.floor(self.game_description.n_rounds * 0.1))
        recent_cooperation_rates = []
        for t in range(max(0, state.round_number - avg_coop_rate_window_size), state.round_number):
            cooperators_t = sum(history.opponent_actions[t, :] + [Action.C.value if history.my_actions[t] else Action.D.value])
            recent_cooperation_rates.append(cooperators_t / self.game_description.n_players)
        self.avg_coop_rate = np.mean(recent_cooperation_rates)
        if self.avg_coop_rate > 0.5:
            action = Action.C
        else:
            action = Action.D
        if self.avg_coop_rate < 0.3 and (not self.punishment_rounds):
            self.punishment_rounds = max(1, math.floor(self.game_description.n_rounds * 0.1))
            return Action.D
        elif self.punishment_rounds > 0:
            self.punishment_rounds -= 1
        if state.round_number > math.floor(self.game_description.n_rounds * 0.8):
            if self.avg_coop_rate > 0.5:
                return Action.C
            else:
                return Action.D
        return action

class Strategy_COLLECTIVE_538(BaseStrategy):
    """
    Collective strategy using adaptive cooperativity.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.quorum = math.ceil((game_description.n_players + game_description.k) / 2)
        self.w_quorum = 0.0
        self.w_tit_for_tat = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_coop_count = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if prev_coop_count >= self.quorum:
            self.w_quorum += 1 / self.game_description.n_rounds
            self.w_tit_for_tat -= 1 / self.game_description.n_rounds
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if random.random() < self.w_quorum:
            return Action.C
        elif random.random() < self.w_tit_for_tat:
            opponent_cooperated = any(history.opponent_actions[-1, :] == True)
            return Action.C if opponent_cooperated else Action.D
        else:
            quorum_weight = self.w_quorum / (self.w_quorum + self.w_tit_for_tat)
            return Action.C if random.random() < quorum_weight else Action.D

class Strategy_COLLECTIVE_539(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation (AC)
    
    The AC strategy aims to balance individual payoff maximization with collective welfare by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperators_t_1 = 0
        self.avg_payoff_t_1 = 0
        self.cooperators_threshold = math.ceil(game_description.n_players / 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        self.cooperators_t_1 = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if history.my_payoffs.size > 0:
            self.avg_payoff_t_1 = np.mean(history.my_payoffs)
        if self.avg_payoff_t_1 > self.game_description.k / self.game_description.n_players * self.game_description.n_players / 2:
            self.cooperators_threshold += 1
        elif self.avg_payoff_t_1 < self.game_description.k / self.game_description.n_players * self.game_description.n_players / 4 and self.cooperators_threshold > math.ceil(self.game_description.n_players / 2):
            self.cooperators_threshold -= 1
        if history.my_actions[-1] == False and self.cooperators_t_1 >= self.game_description.n_players // 2:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if self.cooperators_t_1 >= math.ceil(self.game_description.n_players / 2):
                return Action.C
            else:
                return Action.D
        if self.cooperators_t_1 >= self.cooperators_threshold:
            return Action.C
        elif random.random() < 0.5:
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_540(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Retaliation (ACR)

    This strategy balances cooperation and self-interest by adaptively responding to the behavior of others.
    It combines cooperation with retaliation to encourage mutual cooperation and deter exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.alpha = self.game_description.n_players / 2
        self.beta = 0.5
        self.gamma = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_cooperators = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        was_exploited = history.my_actions[-1] and (not any(history.opponent_actions[-1, :]))
        if was_exploited:
            self.retaliation_rounds_left = self.gamma - 1
            return Action.D
        if hasattr(self, 'retaliation_rounds_left') and self.retaliation_rounds_left > 0:
            self.retaliation_rounds_left -= 1
            return Action.D
        if total_cooperators >= self.alpha:
            return Action.C
        elif total_cooperators < self.alpha and random.random() < self.beta:
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_542(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective cooperation,
    adapting to the game's history and opponents' behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        my_action = history.my_actions[-1]
        T = sum(opponent_actions) + (my_action == Action.C)
        for i, opponent_action in enumerate(opponent_actions):
            if opponent_action and history.opponent_payoffs[-1, i] > 0 and (my_action == Action.D):
                return Action.D
        p = self.game_description.k / self.game_description.n_players * (1 - T / self.game_description.n_players)
        if T >= self.game_description.n_players // 2:
            return Action.C
        elif random.random() < p:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if T == self.game_description.n_players // 2:
            return Action.D

class Strategy_COLLECTIVE_544(BaseStrategy):
    """
    An adaptive strategy that balances individual self-interest with collective well-being.
    Cooperates initially and when the average cooperation rate is high, defects otherwise.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.6

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperations = np.sum(history.opponent_actions, axis=1)
        avg_coop_rate = np.mean(opponent_cooperations / self.game_description.n_players)
        if math.isclose(avg_coop_rate, self.theta):
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if avg_coop_rate >= self.theta:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_545(BaseStrategy):
    """
    Collective Strategy: "Adaptive Cooperator"

    Our collective strategy, "Adaptive Cooperator," aims to balance individual self-interest with the benefits of cooperation.
    We prioritize adaptability and robustness against various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_cooperators = sum(history.opponent_actions[-1, :] | [history.my_actions[-1]])
        majority_cooperated = prev_round_cooperators > self.game_description.n_players / 2
        avg_cooperator_payoff = np.mean([payoff for payoff, action in zip(history.opponent_payoffs[-1, :], history.opponent_actions[-1, :]) if action])
        defector_payoffs = [payoff for payoff, action in zip(history.opponent_payoffs[-1, :], history.opponent_actions[-1, :]) if not action]
        if majority_cooperated:
            return Action.C
        if defector_payoffs and max(defector_payoffs) > avg_cooperator_payoff:
            return random.choice([Action.C, Action.D])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_546(BaseStrategy):
    """
    Adaptive Cooperation Strategy.

    This strategy adapts to changing opponent behaviors, prioritizing cooperation 
    when possible while protecting individual payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        if state.round_number > 1:
            prev_prev_cooperators = sum(history.opponent_actions[-2, :]) + history.my_actions[-2]
        else:
            prev_prev_cooperators = prev_cooperators
        increased_cooperation = prev_cooperators > prev_prev_cooperators
        if prev_cooperators >= self.game_description.n_players / 2 or (prev_cooperators < self.game_description.n_players / 2 and increased_cooperation):
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_547(BaseStrategy):
    """
    The Adaptive Harmony strategy promotes cooperation while being robust to various opponent behaviors.
    It adapts to the number of cooperators and incorporates a Tit-for-Tat component, encouraging mutual 
    cooperation and punishing lone defectors. The initial cooperation rule and last round exception ensure 
    that the strategy is not overly exploitable.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.k / self.n else Action.D
        prev_coops = sum((1 for action in history.opponent_actions[-1] if action))
        coop_prob = max(0.5, self.k / self.n) if prev_coops >= (self.n + 1) // 2 else min(0.5, 1 - self.k / self.n)
        if any((not action for action in history.opponent_actions[-1])) and history.my_actions[-1]:
            coop_prob *= 0.5
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < coop_prob else Action.D

class Strategy_COLLECTIVE_548(BaseStrategy):
    """
    A collective strategy that balances individual payoff maximization with 
    collective cooperation by tracking the average contribution rate of opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = game_description.k / game_description.n_players * 0.5 + (1 - game_description.k / game_description.n_players) * 0.25

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or self.game_description.n_players == 2:
            return Action.C if random.random() < 0.5 else Action.D
        avg_contribution_rate = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if avg_contribution_rate >= self.theta:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_549(BaseStrategy):
    """
    Adaptive Cooperation with Punishment (ACP) strategy for Public Goods game.
    This strategy adapts to the game's history, punishes non-cooperative behavior,
    and aims to create a stable cooperative environment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :] | [False])
        avg_defector_payoff = np.mean([payoff for payoff, action in zip(history.my_payoffs if state.round_number == 1 else history.opponent_payoffs[-2, :], history.my_actions if state.round_number == 1 else history.opponent_actions[-2, :]) if not action])
        avg_cooperator_payoff = np.mean([payoff for payoff, action in zip(history.my_payoffs if state.round_number == 1 else history.opponent_payoffs[-2, :], history.my_actions if state.round_number == 1 else history.opponent_actions[-2, :]) if action])
        if state.round_number > 1 and num_cooperators < self.game_description.n_players // 2 and (sum(history.opponent_actions[-2, :] | [False]) < self.game_description.n_players // 2):
            return Action.D
        if num_cooperators >= self.game_description.n_players // 2:
            action = Action.C
        elif avg_defector_payoff > avg_cooperator_payoff:
            action = Action.D
        else:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            if num_cooperators >= self.game_description.n_players // 2:
                return Action.C
        return action

class Strategy_COLLECTIVE_550(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors.
    
    This strategy prioritizes cooperation and encourages others to contribute by:
    1. Setting a cooperative tone in the first round.
    2. Adapting to the group's behavior based on average cooperation rates.
    3. Implementing a punishment mechanism to deter low cooperation.
    4. Forgiving the group when cooperation improves.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tau = 0.5
        self.punishment_threshold = 0.5 * game_description.n_players * game_description.k / game_description.n_players
        self.forgiveness_threshold = 0.5 * game_description.n_players * game_description.k / game_description.n_players
        self.punishment_triggered = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_coop_rate = np.mean(np.concatenate((history.my_actions[:state.round_number], history.opponent_actions[:state.round_number, :].flatten())))
        if avg_coop_rate >= self.tau:
            return Action.C
        total_contributions = np.sum(history.my_actions[state.round_number - 1]) + np.sum(history.opponent_actions[state.round_number - 1, :])
        if not self.punishment_triggered and total_contributions < self.punishment_threshold:
            self.punishment_triggered = True
            return Action.D
        if self.punishment_triggered and total_contributions > self.forgiveness_threshold:
            self.punishment_triggered = False
            return Action.C
        return Action.D

class Strategy_COLLECTIVE_551(BaseStrategy):
    """
    Adaptive Cooperative Escalation strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.8 else Action.D
        c_prev = sum(history.opponent_actions[-1, :])
        ratio = c_prev / self.game_description.n_players
        p_coop = 0.9 if c_prev >= self.game_description.n_players // 2 else 0.95 if ratio >= 2 / 3 else 0.5 if 1 / 3 <= ratio < 2 / 3 else 0.1
        return Action.C if random.random() < p_coop and state.round_number != self.game_description.n_rounds - 1 else Action.D

class Strategy_COLLECTIVE_552(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Tit-for-Tat and Punishment (ACTP)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.punished_opponents = set()

    def majority_action(self, history: PlayerHistory) -> Action:
        cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
        if cooperators > self.game_description.n_players // 2:
            return Action.C
        else:
            return Action.D

    def opponent_defected(self, history: PlayerHistory, opponent_index: int) -> bool:
        return not history.opponent_actions[-1, opponent_index]

    def self_cooperated(self, history: PlayerHistory) -> bool:
        return history.my_actions[-1]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        majority = self.majority_action(history)
        if majority == Action.C:
            action = Action.C
        else:
            action = Action.D
        for opponent_index in range(self.game_description.n_players - 1):
            if self.opponent_defected(history, opponent_index) and self.self_cooperated(history):
                action = Action.D
                self.punished_opponents.add(opponent_index)
            elif opponent_index in self.punished_opponents and history.opponent_actions[-1, opponent_index]:
                action = Action.C
                self.punished_opponents.remove(opponent_index)
        return action

class Strategy_COLLECTIVE_554(BaseStrategy):
    """
    A collective strategy that adapts to various opponent behaviors in a public goods game.
    
    Initially, it encourages cooperation with a random choice. Then, it gradually shifts towards 
    a Tit-for-Tat approach and eventually adapts to late-round dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.5
        self.tft_counter = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if state.round_number < 5:
            prev_round_coop = np.mean(history.opponent_actions[-1, :] == True)
            self.avg_coop_rate = (self.avg_coop_rate + prev_round_coop) / 2
            return Action.C if self.avg_coop_rate > 0.5 else Action.D
        elif state.round_number < self.game_description.n_rounds // 2:
            coop_count = np.sum(history.opponent_actions[-1, :] == True) + (history.my_actions[-1] == True)
            if coop_count >= self.game_description.n_players / 2:
                action = Action.C
                self.tft_counter += 1
            else:
                action = Action.D
                self.tft_counter -= 1
            return action
        else:
            opponents_behavior = np.mean(history.opponent_actions[-1, :] == True)
            if opponents_behavior > 0.5:
                return Action.C
            elif opponents_behavior < 0.5:
                return Action.D
            else:
                return Action.C if self.tft_counter >= 0 else Action.D

class Strategy_COLLECTIVE_555(BaseStrategy):
    """
    A collective strategy that balances individual payoff maximization with collective welfare,
    adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        sum_cooperators_prev_round = sum(history.opponent_actions[-1, :] | [history.my_actions[-1]])
        theta = self.game_description.k / self.game_description.n_players * (sum_cooperators_prev_round / self.game_description.n_players)
        cooperate = theta > 0.5
        if history.my_actions[-1] and (not all(history.opponent_actions[-1, :])):
            self.theta += (1 - self.theta) / self.game_description.n_players
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_556(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to the game parameters and history.
    
    The strategy, Adaptive Collective Optimism (ACO), aims to create a positive feedback loop by cooperating when 
    the collective payoff is high, which encourages others to cooperate as well. By reciprocating altruism and 
    exploring different actions, ACO adapts to various opponent behaviors while maintaining a cooperative stance.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_prev_round = None
        self.num_cooperators_prev_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            my_payoffs = np.array(history.my_payoffs)
            opponent_actions = np.array(history.opponent_actions)
            self.avg_payoff_prev_round = np.mean(my_payoffs[-1] + np.sum(opponent_actions[-1, :] * (self.game_description.k / self.game_description.n_players)))
            self.num_cooperators_prev_round = sum(opponent_actions[-1, :]) + 1
        if self.avg_payoff_prev_round >= self.game_description.k:
            return Action.C
        else:
            p = self.game_description.k / self.game_description.n_players * (self.num_cooperators_prev_round / self.game_description.n_players)
            if random.random() < p:
                return Action.D
            else:
                return Action.C
        if state.round_number % 5 == 0:
            return random.choice([Action.C, Action.D])
        if state.round_number == self.game_description.n_rounds - 1 and self.avg_payoff_prev_round >= self.game_description.k:
            return Action.C
        return Action.C

class Strategy_COLLECTIVE_557(BaseStrategy):
    """
    Adaptive Cooperative Escalation (ACE) strategy for N-player public goods games.
    
    This strategy balances individual payoff maximization with cooperative behavior,
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate = True
        self.previous_payoff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None and len(history.my_payoffs) > 0:
            self.previous_payoff = history.my_payoffs[-1]
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        average_payoff_per_player = np.mean([self.previous_payoff] + [payoff for payoff in history.opponent_payoffs[-1]])
        if self.previous_payoff > average_payoff_per_player and opponent_cooperators >= self.game_description.k / 2:
            self.cooperate = True
        else:
            self.cooperate = False
        if not self.cooperate:
            if any((action for action in history.opponent_actions[-1])):
                self.cooperate = True
        if history is not None and len(history.my_payoffs) > 1 and (self.previous_payoff == 0 or all((payoff == 0 for payoff in history.opponent_payoffs[-2]))):
            return Action.D
            return Action.D
        return Action.C if self.cooperate else Action.D

class Strategy_COLLECTIVE_558(BaseStrategy):
    """
    Adaptive Reciprocity with Social Learning strategy.
    Balances individual self-interest with collective well-being by adapting to the group's behavior and learning from its history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_window = []
        self.w = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        avg_coop_prev = np.mean(opponent_actions)
        num_defect_prev = self.game_description.n_players - sum(opponent_actions)
        if len(self.avg_coop_window) >= self.w:
            self.avg_coop_window.pop(0)
        self.avg_coop_window.append(avg_coop_prev)
        avg_coop_window_avg = np.mean(self.avg_coop_window)
        if avg_coop_window_avg < 0.5 and self.w > 1:
            self.w -= 1
        elif avg_coop_window_avg > 0.75:
            self.w += 1
        if avg_coop_prev >= 0.5 or (state.round_number == self.game_description.n_rounds - 1 and state.round_number % 2 == 0):
            return Action.C
        else:
            prob_defect = num_defect_prev / self.game_description.n_players
            return Action.D if random.random() < prob_defect else Action.C

class Strategy_COLLECTIVE_559(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation = True
        self.previous_payoff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_contributions = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        payoff = 1 - int(self.cooperation) + self.game_description.k / self.game_description.n_players * total_contributions
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        exploration_probability = 0.1
        random_choice = random.random()
        if payoff >= self.game_description.k / self.game_description.n_players and self.cooperation:
            action = Action.C
        elif payoff < self.game_description.k / self.game_description.n_players or not self.cooperation:
            action = Action.D
        elif random_choice < exploration_probability:
            action = Action.C
        else:
            action = Action.D
        self.cooperation = action == Action.C
        self.previous_payoff = payoff
        return action

class Strategy_COLLECTIVE_560(BaseStrategy):
    """
    The Adaptive Cooperate-then-Punish strategy.
    
    This strategy cooperates initially and continues to do so if the average payoff 
    is above a certain threshold. If the average payoff falls below this threshold, 
    it defects for a fixed number of rounds before re-evaluating its decision.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5
        self.p = 2
        self.cooperate_phase = True
        self.punish_phase_count = 0
        self.round_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_actions = history.opponent_actions[-1]
        num_cooperators = sum(prev_round_actions)
        total_payoff = self.game_description.k / self.game_description.n_players * num_cooperators + 1
        avg_payoff_prev_round = total_payoff
        self.round_payoffs.append(avg_payoff_prev_round)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        if not self.cooperate_phase:
            if self.punish_phase_count > 0:
                self.punish_phase_count -= 1
                return Action.D
            else:
                self.cooperate_phase = True
        if avg_payoff_prev_round < self.theta and (not self.cooperate_phase):
            self.cooperate_phase = False
            self.punish_phase_count = self.p
        return Action.C

class Strategy_COLLECTIVE_561(BaseStrategy):
    """
    Adaptive Collective Cooperation (ACC) strategy for N-Player Public Goods Game.
    
    This strategy fosters cooperation while protecting against exploitation by incorporating elements of cooperation, punishment, and forgiveness.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def calculate_cooperation_probability(self, defectors: int) -> float:
        if self.game_description.n_players == 2:
            return 0.5
        else:
            return max(0, 1 - defectors / (self.game_description.n_players - 1))

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        if opponent_cooperators >= self.game_description.n_players / 2:
            return Action.C
        elif 0 < opponent_cooperators < self.game_description.n_players / 2:
            defectors = self.game_description.n_players - opponent_cooperators - 1
            cooperation_probability = self.calculate_cooperation_probability(defectors)
            return Action.C if random.random() < cooperation_probability else Action.D
        else:
            total_payoffs = np.sum(history.my_payoffs)
            average_payoff_per_round = total_payoffs / state.round_number
            punishment_threshold = self.game_description.k * (self.game_description.n_players - 1) / self.game_description.n_players
            if average_payoff_per_round < punishment_threshold:
                return Action.D
            else:
                cooperation_probability = 0.5
                return Action.C if random.random() < cooperation_probability else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            total_cooperations = np.sum(history.my_actions)
            if total_cooperations >= self.game_description.n_players / 2 * (self.game_description.n_rounds - 1):
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_563(BaseStrategy):
    """
    Adaptive Reciprocity with Public Goods Incentives strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] if history else 0)
        if prev_cooperators >= self.k / self.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_564(BaseStrategy):
    """
    Adaptive Cooperation with Self-Interest (ACSI) Strategy.

    Balances individual self-interest with cooperative behavior to maximize overall payoff.
    Adapts to the game's history and parameters, making it robust against various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_init = 0.5
        self.avg_payoff_prev = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_init else Action.D
        avg_payoff_prev = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1, :])
        total_cooperators_prev = sum(history.opponent_actions[-1, :] == True)
        if total_cooperators_prev >= self.game_description.n_players // 2:
            p_punish = 0.7
            avg_payoff_prev *= (1 - p_punish) * random.random()
        if self.avg_payoff_prev is None:
            self.avg_payoff_prev = avg_payoff_prev
        else:
            self.avg_payoff_prev = 0.3 * self.avg_payoff_prev + 0.7 * avg_payoff_prev
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif total_cooperators_prev == 0:
            p_reset = 0.4
            return Action.C if random.random() < p_reset else Action.D
        elif self.avg_payoff_prev >= self.game_description.k / self.game_description.n_players * self.game_description.n_players:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_565(BaseStrategy):
    """
    A collective strategy that balances individual payoffs with the benefits of contributing to the public good.
    It adapts to the game's history and opponent behaviors, encouraging cooperation while being cautious of exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.acr_history = []
        self.total_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.game_description.n_players:
            return Action.C
        if state.round_number < 2 * self.game_description.n_players:
            if history is not None:
                cooperators = sum(history.opponent_actions[-1, :] + [True])
                acr = cooperators / self.game_description.n_players
                self.acr_history.append(acr)
                self.total_payoffs.append(sum(history.my_payoffs))
            return Action.C
        if state.round_number < self.game_description.n_rounds - 2 * self.game_description.n_players:
            acr = self.acr_history[-1]
            total_payoff = sum(self.total_payoffs)
            if acr >= self.game_description.k / self.game_description.n_players:
                return Action.C
            elif total_payoff > self.game_description.n_rounds * 1 and acr < self.game_description.k / self.game_description.n_players:
                return Action.D
            else:
                return Action.C
        if state.round_number >= self.game_description.n_rounds - 2 * self.game_description.n_players:
            acr = self.acr_history[-1]
            if acr < self.game_description.k / self.game_description.n_players:
                p = 0.5 + acr * self.game_description.n_players / (2 * self.game_description.k)
                return Action.C if random.random() < p else Action.D
            else:
                return Action.C
        acr = self.acr_history[-1]
        return Action.C if acr >= self.game_description.k / self.game_description.n_players else Action.D

class Strategy_COLLECTIVE_567(BaseStrategy):
    """
    Adaptive Cooperative Escalation strategy for the N-player public goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.patience = 0
        self.avg_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (history is not None and state.round_number == 1):
            return Action.C
        prev_cooperators = sum(history.opponent_actions[-2, :] + [False])
        self.patience += int(prev_cooperators <= sum(history.opponent_actions[-1, :]))
        if self.game_description.n_rounds <= 3:
            return Action.C
        if state.round_number >= self.game_description.n_rounds // 2:
            self.avg_payoff = np.mean([sum(row) * self.game_description.k for row in history.opponent_actions[-(self.game_description.n_rounds // 2):]])
        if prev_cooperators >= self.game_description.n_players / 2:
            return Action.C
        elif self.patience >= self.game_description.n_players / 3:
            self.patience = 0
            return Action.C
        elif self.avg_payoff < self.game_description.k / 2:
            return Action.D
        else:
            return Action.D if prev_cooperators < self.game_description.n_players / 2 else Action.C

class Strategy_COLLECTIVE_568(BaseStrategy):
    """
    A collective strategy called "Adaptive Collective Optimism" (ACO) for the N-Player Public Goods Game.
    It balances individual self-interest with collective well-being by adaptively responding to the game's history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (state.round_number == 1 and history is None):
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        total_contributions = sum(history.opponent_actions[-1, :] != False) + (Action.C == history.my_actions[-1])
        if total_contributions >= self.game_description.k / 2:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_569(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Gradual Reciprocity (ACGR).
    
    This strategy prioritizes cooperation and reciprocity while adapting to the game's dynamics.
    By cooperating initially and reciprocating based on majority behavior, ACGR encourages others to cooperate,
    promoting collective welfare. The gradual adjustment mechanism allows ACGR to respond to changes in the game's environment,
    ensuring a balance between individual and collective interests.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperative_rounds = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_coops = sum(history.opponent_actions[-1, :] != False)
        if prev_round_coops > self.game_description.n_players / 2:
            return Action.C
        if history is not None and len(history.my_payoffs) > 0:
            self.cooperative_rounds = sum((1 for round in range(len(history.opponent_actions)) if sum(history.opponent_actions[round, :]) > self.game_description.n_players / 2))
            c_ratio = self.cooperative_rounds / (state.round_number + 1)
        threshold = math.ceil(self.game_description.n_players / 2)
        if hasattr(self, 'cooperative_rounds') and len(history.my_payoffs) > 0:
            if c_ratio > 0.5:
                threshold -= 1
            elif c_ratio < 0.3:
                threshold += 1
        if prev_round_coops >= threshold:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_570(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Concession (ACGC) strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = None
        self.avg_coop = None
        self.previous_action = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif 1 <= state.round_number <= self.game_description.n_rounds // 2:
            opponent_actions = history.opponent_actions[state.round_number - 1, :]
            self.avg_coop = np.mean(opponent_actions)
            if self.coop_threshold is None:
                self.coop_threshold = self.game_description.k / self.game_description.n_players * (self.game_description.n_players - 1)
            if self.avg_coop >= self.coop_threshold:
                return Action.C
            else:
                return Action.D
        elif state.round_number > self.game_description.n_rounds // 2:
            if self.previous_action == Action.D and history.my_actions[state.round_number - 2]:
                return Action.C
            elif self.previous_action == Action.C and (not history.my_actions[state.round_number - 2]):
                return Action.C
            opponent_actions = history.opponent_actions[state.round_number - 1, :]
            self.avg_coop = np.mean(opponent_actions)
            if self.avg_coop >= self.coop_threshold:
                return Action.C
            else:
                return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        elif state.round_number <= 2 and sum(history.opponent_actions[state.round_number - 1, :]) > self.game_description.n_players // 2:
            return Action.D
        self.previous_action = history.my_actions[state.round_number - 1]

class Strategy_COLLECTIVE_571(BaseStrategy):
    """
    Adaptive Cooperation with Social Learning (ACS) strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :] == True)
        total_opponents = self.game_description.n_players - 1
        if total_opponents == 0:
            return Action.C
        avg_coop_payoff = np.sum([history.opponent_payoffs[-1, i] for i in range(total_opponents) if history.opponent_actions[-1, i]]) / num_cooperators
        avg_defect_payoff = np.sum([history.opponent_payoffs[-1, i] for i in range(total_opponents) if not history.opponent_actions[-1, i]]) / (total_opponents - num_cooperators)
        if avg_defect_payoff == 0 or avg_coop_payoff >= avg_defect_payoff:
            return Action.C
        defect_prob = (total_opponents - num_cooperators) / total_opponents
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.D if random.random() < defect_prob else Action.C

class Strategy_COLLECTIVE_572(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation with Social Learning (ACSL)
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.T_c = 0
        self.APC = 0
        self.APD = 0
        self.consecutive_unrewarded_coop = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self.T_c = sum(history.opponent_actions[-1, :])
        opponent_cooperators_payoff = np.mean([payoff for action, payoff in zip(history.opponent_actions[-1, :], history.opponent_payoffs[-1, :]) if action])
        opponent_defectors_payoff = np.mean([payoff for action, payoff in zip(history.opponent_actions[-1, :], history.opponent_payoffs[-1, :]) if not action])
        if self.T_c > 0:
            self.APC = opponent_cooperators_payoff
        else:
            self.APC = 0
        if self.game_description.n_players - self.T_c > 0:
            self.APD = opponent_defectors_payoff
        else:
            self.APD = 0
        if self.APC < self.APD and state.round_number > 1:
            self.consecutive_unrewarded_coop += 1
        else:
            self.consecutive_unrewarded_coop = 0
        if self.APC >= self.APD or (self.consecutive_unrewarded_coop <= 2 and state.round_number > 1):
            return Action.C
        else:
            p = 1 - self.game_description.k / self.game_description.n_players * self.T_c / self.game_description.n_players
            if random.random() < p or self.consecutive_unrewarded_coop > 2:
                return Action.D
            return Action.C

class Strategy_COLLECTIVE_573(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with cooperation, 
    adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = game_description.k / (2 * game_description.n_players)
        self.defect_streaks = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_contribution = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        if avg_contribution >= self.theta or (state.round_number > 1 and any((streak >= 2 for streak in self.defect_streaks))):
            action = Action.C
        else:
            action = Action.D
        for i, a in enumerate(history.opponent_actions[-1, :]):
            if not a:
                self.defect_streaks[i] += 1
            else:
                self.defect_streaks[i] = 0
        return action

class Strategy_COLLECTIVE_574(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to the game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.k_over_n = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        prev_round_history = (history.my_actions[-1], history.opponent_actions[-1])
        avg_payoff_prev_round = np.mean(history.my_payoffs[-1] + self.k_over_n * np.sum(history.opponent_actions[-1]))
        cooperate_prob = 1 if avg_payoff_prev_round > self.k_over_n else 0
        num_defectors_prev_round = np.sum(~history.opponent_actions[-1])
        if num_defectors_prev_round > self.game_description.n_players / 2:
            cooperate_prob = 0
        if state.round_number >= 2 and state.round_number <= 5:
            coop_level_prev_round = np.sum(history.opponent_actions[-1]) / self.game_description.n_players
            if coop_level_prev_round > 0.5:
                cooperate_prob += 0.1
            elif coop_level_prev_round < 0.5:
                cooperate_prob -= 0.1
            cooperate_prob = max(0, min(cooperate_prob, 1))
        if state.round_number == self.game_description.n_rounds - 1:
            avg_payoff_overall = np.mean(history.my_payoffs)
            cooperate_prob = 1 if avg_payoff_overall > self.k_over_n else 0
        return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_COLLECTIVE_575(BaseStrategy):
    """
    Adaptive Cooperation with History-Dependent Threshold (ACHT) strategy.
    This collective strategy encourages cooperation in the initial round, 
    then adapts to opponents' behaviors through history-dependent threshold calculation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_contributions = 0
        self.my_contributions = 0
        self.opponent_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        opponent_actions = history.opponent_actions[:-1, :]
        total_cooperators = np.sum(opponent_actions)
        threshold = self.game_description.k / self.game_description.n_players * (total_cooperators / state.round_number)
        self.total_contributions += np.sum(history.opponent_actions[-1, :] == True)
        self.my_contributions += 1 if history.my_actions[-1] else 0
        self.opponent_defections += np.sum(history.opponent_actions[-1, :] == False)
        if self.opponent_defections > threshold:
            return Action.D
        elif self.opponent_defections < threshold:
            return Action.C
        else:
            return Action.C

class Strategy_COLLECTIVE_576(BaseStrategy):
    """
    A collective strategy aiming to balance individual self-interest with the collective goal of maximizing public goods.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tau = 0.5
        self.p_C = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_coop_t_minus_1 = np.mean(history.opponent_actions[-1, :] + history.my_actions[-1])
        if avg_coop_t_minus_1 >= self.tau:
            self.tau += 0.05
        else:
            self.tau -= 0.05
        payoff_diff = history.my_payoffs[-1] - np.mean(history.opponent_payoffs[-1, :])
        if payoff_diff > 0:
            self.p_C = min(1, self.p_C + 0.05)
        elif payoff_diff < 0:
            self.p_C = max(0, self.p_C - 0.05)
        return Action.C if random.random() < self.p_C else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_COLLECTIVE_578(BaseStrategy):
    """
    A strategy for the N-Player Public Goods Game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        avg_coop_prev = np.mean(history.opponent_actions[-1, :])
        if avg_coop_prev > self.theta:
            action = Action.C
        else:
            action = Action.D
        coop_payoffs = history.opponent_payoffs[-1, :]
        coop_mask = history.opponent_actions[-1, :]
        defect_mask = ~coop_mask
        payoff_coop_prev = np.mean(coop_payoffs[coop_mask]) if np.any(coop_mask) else 0.0
        payoff_defect_prev = np.mean(coop_payoffs[defect_mask]) if np.any(defect_mask) else 1.0
        if payoff_coop_prev < payoff_defect_prev:
            action = Action.D
        if state.round_number % 3 == 0:
            action = random.choice([Action.C, Action.D])
        if self.game_description.n_rounds == state.round_number + 1:
            return Action.D
        elif self.game_description.n_players == 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_579(BaseStrategy):
    """
    A collective strategy that balances individual payoff maximization with collective well-being.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if self.game_description.k > self.game_description.n_players / 2 else Action.D
        opponent_cooperation_rate = np.mean(history.opponent_actions[-1, :])
        cooperators = sum(history.my_actions[:state.round_number - 1]) + sum(history.opponent_actions[:-1, :].sum(axis=1))
        self.theta = 1 / state.round_number * (self.game_description.k / self.game_description.n_players * cooperators)
        if opponent_cooperation_rate >= 0.5 or self.theta >= 1:
            return Action.C
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_COLLECTIVE_580(BaseStrategy):
    """
    The Adaptive Cooperative Tracker (ACT) strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_coop = np.mean(history.my_actions[:state.round_number] | history.opponent_actions[:state.round_number, :].any(axis=1))
        coop_thresh = self.dynamic_threshold(avg_coop)
        prev_high_coop_rounds = np.sum(np.mean(history.my_actions[:state.round_number] | history.opponent_actions[:state.round_number, :].any(axis=1), axis=0) > avg_coop)
        if state.round_number > 2 * self.game_description.n_rounds // 3 and prev_high_coop_rounds >= 2 * state.round_number // 3:
            return Action.C
        elif coop_thresh >= avg_coop:
            return Action.C
        return Action.D

    def dynamic_threshold(self, avg_coop: float) -> float:
        """
        Calculates the dynamic cooperation threshold based on game parameters.
        
        :param avg_coop: The average cooperation rate among all players in previous rounds.
        :return: The dynamic cooperation threshold.
        """
        if self.game_description.k / self.game_description.n_players > 0.75:
            return avg_coop + 0.25
        else:
            return avg_coop

class Strategy_COLLECTIVE_581(BaseStrategy):
    """
    Collective Strategy: Adaptive Cooperation (AC)
    
    Decision Rules:
    1. Initial Round: Cooperate to initiate cooperation.
    2. Reciprocity-Based Cooperation: Cooperate if cooperators >= n/2 in previous round.
    3. Punishment for Defection: Defect next round if others cooperate and player defects.
    4. Exploration: Randomly choose action with probability ε = 0.1.
    
    Edge Cases:
    * Last Round: Cooperate if cooperators >= n/2, else defect.
    * Tie-Breaking: Cooperate in case of tie.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperators_history = []
        self.defectors_history = []
        self.epsilon = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        prev_round_actions = history.opponent_actions[-1]
        self.cooperators_history.append(sum(prev_round_actions))
        self.defectors_history.append(len(prev_round_actions) - sum(prev_round_actions))
        if random.random() < self.epsilon:
            return Action(random.choice([Action.C, Action.D]))
        if self.cooperators_history[-1] >= self.game_description.n_players / 2:
            return Action.C
        if history.my_actions[-1] == False and self.cooperators_history[-1] > self.defectors_history[-1]:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if self.cooperators_history[-1] >= self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_COLLECTIVE_582(BaseStrategy):
    """
    Adaptive Cooperative Escalation (ACE) strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tau = game_description.k / game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        c_t = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if c_t >= self.tau:
            action = Action.C
        elif random.random() < 1 - c_t / self.tau:
            action = Action.D
        else:
            action = Action.C
        if action == Action.D and history.my_payoffs[-1] > 0:
            p_c = history.my_payoffs[-1] / (self.game_description.k * self.game_description.n_players)
            if random.random() < p_c:
                action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            c_t_prev = sum(history.opponent_actions[-2, :] + [history.my_actions[-2]])
            if c_t_prev >= self.tau:
                action = Action.C
        return action

class Strategy_COLLECTIVE_583(BaseStrategy):
    """
    A collective strategy that adapts to various opponent behaviors by initially cooperating, 
    then using a mixed strategy based on observed behavior, and punishing opponents for defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 5:
            return Action.C
        num_players = self.game_description.n_players
        prev_round_actions = history.opponent_actions[-1, :] if history else None
        if prev_round_actions is not None:
            prev_round_cooperators = sum(prev_round_actions)
        else:
            return Action.C
        coop_threshold = 0.6 * num_players
        p_C = 0.8 if prev_round_cooperators >= coop_threshold else 1 - prev_round_cooperators / num_players
        opponent_defected = any((action == Action.D for action in history.opponent_actions[-1, :])) if history else False
        punishment_margin = 0.05
        if opponent_defected:
            p_C -= punishment_margin
            p_D = 1 - p_C
        last_round = state.round_number == self.game_description.n_rounds
        if last_round:
            return Action.C
        consecutive_cooperation = all((action == Action.C for action in history.opponent_actions[-2, :])) if history and len(history.opponent_actions) > 1 else False
        if consecutive_cooperation:
            p_D -= 0.1
        return Action.C if random.random() < p_C else Action.D

class Strategy_COLLECTIVE_584(BaseStrategy):
    """
    Adaptive Cooperative Escalation strategy for Public Goods Game.
    
    This collective strategy balances individual self-interest with the desire 
    for mutual cooperation, adapting to the game's history and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_contributions = None
        self.avg_coop_rate = None
        self.theta = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.total_contributions is None:
            self.total_contributions = [0] * (self.game_description.n_rounds + 1)
        if history is not None:
            recent_cooperators = sum(history.opponent_actions[-1, :]) + int(history.my_actions[-1])
            self.total_contributions[state.round_number] = recent_cooperators
        if state.round_number > 1:
            self.avg_coop_rate = self.total_contributions[state.round_number - 1] / self.game_description.n_players
        if self.avg_coop_rate is not None and self.avg_coop_rate >= self.theta:
            return Action.C
        elif state.round_number > 2:
            prev_avg_coop_rate = self.total_contributions[state.round_number - 2] / self.game_description.n_players
            if prev_avg_coop_rate > self.avg_coop_rate:
                return Action.D
            else:
                return Action.C
        elif state.round_number == self.game_description.n_rounds:
            return Action.D
        else:
            return Action.D

class Strategy_COLLECTIVE_586(BaseStrategy):
    """
    A collective strategy promoting cooperation through adaptive response to opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.probability_of_cooperation = 0.8

    def _calculate_average_cooperation_rate(self, history: PlayerHistory) -> float:
        """Calculate the average cooperation rate among all players in previous rounds."""
        if len(history.my_actions) == 1:
            return self.probability_of_cooperation
        opponent_cooperations = np.sum(history.opponent_actions[:-1], axis=0)
        total_cooperators = np.sum(opponent_cooperations) + np.sum(history.my_actions[:-1])
        return total_cooperators / (self.game_description.n_players * len(history.my_actions))

    def _punishment_mechanism(self, history: PlayerHistory) -> None:
        """Punish opponents who defect while others cooperate."""
        if len(history.opponent_actions) > 0 and np.sum(history.opponent_actions[-1]) == self.game_description.n_players - 2:
            self.probability_of_cooperation = max(self.probability_of_cooperation - 0.2, 0)

    def _single_defector_handling(self, history: PlayerHistory) -> None:
        """Increase the probability of cooperation against a single defector."""
        if len(history.opponent_actions) > 0 and np.sum(1 - history.opponent_actions[-1]) == 1:
            self.probability_of_cooperation += 0.15

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number < 5:
            average_cooperation_rate = self._calculate_average_cooperation_rate(history)
            if average_cooperation_rate > 0.5:
                return Action.C
            else:
                return Action.D
        previous_round_cooperation_rate = self._calculate_average_cooperation_rate(history)
        if previous_round_cooperation_rate > 0.6:
            self.probability_of_cooperation += 0.1
        elif previous_round_cooperation_rate < 0.4:
            self.probability_of_cooperation -= 0.1
        self._punishment_mechanism(history)
        self._single_defector_handling(history)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < self.probability_of_cooperation else Action.D

class Strategy_COLLECTIVE_587(BaseStrategy):
    """
    Adaptive Cooperative Maximization Strategy.

    This strategy balances individual self-interest with the goal of maximizing overall group payoff.
    It starts by cooperating in the first round, then adapts its cooperation threshold based on collective behavior and payoff comparisons.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialize the ACM strategy.

        Args:
            game_description (PublicGoodsDescription): Game parameters.
        """
        self.game_description = game_description
        self.m = 3
        self.theta = 0.6
        self.d = 2
        self.avg_coop_rate = 0
        self.last_defectors = game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Decide on an action for the current round.

        Args:
            state (GameState): Current game state.
            history (PlayerHistory): Player's past actions and payoffs.

        Returns:
            Action: Cooperation (C) or Defection (D).
        """
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        defectors = sum((1 for action in opponent_actions if not action))
        coop_rates = np.mean(history.my_actions[max(-self.m, -state.round_number):], axis=0)
        self.avg_coop_rate = coop_rates
        if defectors > self.last_defectors + self.d:
            action = Action.D
        elif self.avg_coop_rate >= self.theta:
            action = Action.C
        else:
            action = Action.D
        my_payoff = history.my_payoffs[-1]
        avg_coop_payoff = np.mean([payoff for action, payoff in zip(history.opponent_actions[-1], history.opponent_payoffs[-1]) if action])
        if my_payoff < avg_coop_payoff:
            self.theta += 0.05
        elif my_payoff > avg_coop_payoff:
            self.theta -= 0.05
        self.last_defectors = defectors
        return action

class Strategy_COLLECTIVE_588(BaseStrategy):
    """
    A collective strategy that adapts to the game's history and parameters,
    balancing individual self-interest with collective well-being.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5

    def calculate_avg_contribution(self, history: PlayerHistory) -> float:
        """
        Calculate the average contribution rate of all players in the previous round.
        """
        prev_round_actions = history.opponent_actions[-1, :]
        avg_contrib = np.mean(prev_round_actions)
        return avg_contrib

    def update_threshold(self, avg_payoff: float, prev_avg_payoff: float):
        """
        Adjust the threshold value based on the game's history.
        """
        if avg_payoff > prev_avg_payoff:
            self.theta += 0.05
        elif avg_payoff < prev_avg_payoff:
            self.theta -= 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        avg_contrib = self.calculate_avg_contribution(history)
        prev_round_payoffs = history.my_payoffs[-1]
        if len(history.my_payoffs) > 1:
            prev_prev_round_payoffs = history.my_payoffs[-2]
            avg_payoff = np.mean(prev_round_payoffs)
            prev_avg_payoff = np.mean(prev_prev_round_payoffs)
            self.update_threshold(avg_payoff, prev_avg_payoff)
        if avg_contrib >= self.theta:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_589(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) strategy for the N-Player Public Goods Game.
    
    ACO aims to create an environment where cooperation is encouraged through adaptive optimism.
    By initially cooperating and then responding positively to successful collective outcomes,
    ACO fosters a culture of mutual support.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_payoff_coop_prev_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        total_contributions_prev_round = sum(history.opponent_actions[-1, :] + history.my_actions[-1])
        private_payoff_defect = 1
        self.total_payoff_coop_prev_round = self.game_description.k / self.game_description.n_players * total_contributions_prev_round
        if self.total_payoff_coop_prev_round >= private_payoff_defect:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_590(BaseStrategy):
    """
    Adaptive Reciprocity with Forgiveness (ARF) strategy for the Public Goods game.
    
    This strategy prioritizes collective cooperation while allowing for adaptability and forgiveness. 
    It reciprocates cooperation, punishes non-cooperation, and forgives non-cooperative behavior to promote overall cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperators = []
        self.total_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            last_round_opponents_cooperated = sum(history.opponent_actions[-1, :])
            self.total_cooperators += 1 if history.my_actions[-1] else 0
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.C
            elif last_round_opponents_cooperated >= math.ceil(self.game_description.k / self.game_description.n_players):
                return Action.C
            elif last_round_opponents_cooperated < math.ceil(self.game_description.k / self.game_description.n_players) and last_round_opponents_cooperated >= math.ceil(self.game_description.k / (2 * self.game_description.n_players)):
                return Action.C
            else:
                return Action.D
        return Action.C

class Strategy_COLLECTIVE_591(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and aligns with the collective mindset.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_prev = 0
        self.num_coop_neighbors = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history is None:
            return Action.C
        avg_payoff_prev = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1, :])
        action = Action.C if avg_payoff_prev > 1.5 else Action.D
        num_coop_neighbors = sum(history.opponent_actions[-1, :] == True)
        if num_coop_neighbors > self.game_description.n_players / 3:
            action = Action.C
        if avg_payoff_prev < 1 and np.mean(history.opponent_actions[-1, :]) > 0.5:
            action = Action.D
        self.avg_payoff_prev = avg_payoff_prev
        return action

class Strategy_COLLECTIVE_592(BaseStrategy):
    """
    Adaptive Cooperation with Gradual Withdrawal (ACGW) strategy.
    
    Aims to balance individual payoff maximization with collective welfare by adapting to the game's history and gradually adjusting cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_ratio = 0.0
        self.previous_payoff = 0.0
        self.consecutive_defection = 0
        self.defect_probability = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number <= 4:
            avg_cooperators = np.mean(history.opponent_actions[-1, :] + history.my_actions[-1])
            if avg_cooperators > self.game_description.n_players / 2:
                return Action.C
            else:
                return Action.D
        last_3_rounds = max(0, state.round_number - 3)
        avg_cooperators_last_3_rounds = np.mean(np.sum(history.opponent_actions[last_3_rounds:, :], axis=1) + history.my_actions[last_3_rounds:])
        self.cooperation_ratio = avg_cooperators_last_3_rounds / self.game_description.n_players
        if self.cooperation_ratio > 0.5:
            return Action.C
        elif self.previous_payoff < np.mean(history.opponent_payoffs[-1, :]) and self.cooperation_ratio <= 0.5:
            self.consecutive_defection += 1
            return Action.D
        else:
            probability = min(1, max(0, round(self.cooperation_ratio + 0.2)))
            if random.random() < probability:
                self.consecutive_defection = 0
                return Action.C
            else:
                self.consecutive_defection += 1
                return Action.D
        if self.consecutive_defection >= 2 and state.round_number > 4:
            self.defect_probability += 0.2
            if random.random() < self.defect_probability:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return Action.C

class Strategy_COLLECTIVE_593(BaseStrategy):
    """
    Collective Strategy: Adaptive Tit-for-Tat with Public Goods Consideration.
    
    This strategy aims to balance individual self-interest with collective well-being.
    It initially cooperates, promotes reciprocity, and considers public goods benefits.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        prev_coop_count = sum(prev_round_actions)
        prev_defect_count = self.game_description.n_players - 1 - prev_coop_count
        cooperate = prev_coop_count >= prev_defect_count
        public_goods_benefit = self.game_description.k / self.game_description.n_players * (prev_coop_count + 1)
        if public_goods_benefit > 1 and (not cooperate):
            cooperate = True
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if cooperate else Action.D

class Strategy_COLLECTIVE_594(BaseStrategy):
    """
    Adaptive Cooperation with Social Norms strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = math.ceil(game_description.n_players / 2)
        self.punish_defectors = False
        self.expected_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_cooperators = sum(history.opponent_actions[-1, :])
        majority_cooperated = previous_round_cooperators >= self.cooperation_threshold
        if majority_cooperated and (not self.punish_defectors):
            return Action.C
        elif majority_cooperated and self.punish_defectors:
            self.punish_defectors = False
            return Action.D
        self.expected_cooperators = previous_round_cooperators + (1 if history.my_actions[-1] else -1)
        payoff_C = self.game_description.k / self.game_description.n_players * self.expected_cooperators
        payoff_D = 1 + self.game_description.k / self.game_description.n_players * previous_round_cooperators
        if history.my_actions[-1] and (not majority_cooperated):
            self.punish_defectors = True
        return Action.C if payoff_C > payoff_D else Action.D

class Strategy_COLLECTIVE_595(BaseStrategy):
    """
    A strategy that balances individual self-interest with collective benefits by adaptively responding to the game's history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 1.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n = self.game_description.n_players
        r = self.game_description.n_rounds
        k = self.game_description.k
        prev_coop_frac = sum(history.opponent_actions[-1, :]) / n
        if state.round_number < r // 3:
            if prev_coop_frac > 0.5 or random.random() < 0.1:
                return Action.C
            else:
                return Action.D
        elif state.round_number >= r // 3 and state.round_number <= 2 * r // 3:
            avg_payoff = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1, :])
            if avg_payoff > self.threshold:
                return Action.C
            else:
                return Action.D
        elif state.round_number > 2 * r // 3:
            prev_my_payoff = history.my_payoffs[-1]
            avg_opponent_payoff = np.mean(history.opponent_payoffs[-1, :])
            if prev_coop_frac > 0.5 and prev_my_payoff >= avg_opponent_payoff:
                return Action.C
            else:
                return Action.D
        elif state.round_number == r - 1:
            second_last_round_actions = history.opponent_actions[-2, :]
            if np.any(second_last_round_actions):
                return Action.C
            else:
                return Action.D

class Strategy_COLLECTIVE_596(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploration_probability = 0.1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        previous_round_actions = history.my_actions[-1:] + history.opponent_actions[-1]
        cooperation_level = sum((1 for action in previous_round_actions if action)) / self.game_description.n_players
        punishment_probability = 0.7
        defect_probability = max(1 - cooperation_level, punishment_probability * (history.my_actions[-1] and history.my_payoffs[-1] <= 0.5))
        probabilities = [defect_probability + self.exploration_probability / 2, 1 - defect_probability - self.exploration_probability / 2]
        return random.choices([Action.D, Action.C], weights=probabilities)[0]

class Strategy_COLLECTIVE_597(BaseStrategy):
    """
    Collective strategy that balances individual self-interest with the pursuit of collective welfare.
    Adapts to the behavior of other players and adjusts its cooperation level based on observed payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators_last_round = None
        self.total_payoff_last_round = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if history is not None and len(history.my_actions) > 0:
            total_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
        else:
            total_cooperators = self.game_description.n_players // 2
        if history is not None and len(history.my_payoffs) > 0:
            total_payoff = sum(history.opponent_payoffs[-1, :]) + history.my_payoffs[-1]
        else:
            total_payoff = self.game_description.k * (self.game_description.n_players - 1) / self.game_description.n_players
        self.total_cooperators_last_round = total_cooperators
        self.total_payoff_last_round = total_payoff
        if total_cooperators > self.game_description.n_players / 2:
            return Action.C
        elif total_payoff > self.game_description.k * (self.game_description.n_players - 1) / self.game_description.n_players:
            return Action.C
        elif history is not None and len(history.opponent_actions) > 0 and (sum((not x for x in history.opponent_actions[-1, :])) > self.game_description.n_players / 3):
            return Action.D
        else:
            return Action.C

class Strategy_COLLECTIVE_598(BaseStrategy):
    """
    Collective strategy "Adaptive Cooperation with Threshold" (ACT) 
    that balances individual payoffs with collective well-being while adapting to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        tau = self.game_description.k / self.game_description.n_players
        num_cooperators_prev_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if state.round_number > 1:
            self.avg_coop = (self.avg_coop * (state.round_number - 2) + num_cooperators_prev_round) / (state.round_number - 1)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if math.isclose(self.avg_coop, tau):
            return Action.C
        elif self.avg_coop >= tau:
            return Action.C
        else:
            return Action.D

class Strategy_COLLECTIVE_599(BaseStrategy):
    """
    This strategy balances cooperation and self-interest while adapting to the game's history.
    It aims to encourage collective cooperation while safeguarding against exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.c_prev = 0
        self.retaliate = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.c_prev = sum(history.opponent_actions[-1, :])
        if not history.my_actions[-1] and self.c_prev > 0:
            self.retaliate = True
        if self.retaliate:
            action = Action.D
            self.retaliate = False
        elif self.c_prev >= self.game_description.n_players / 2:
            action = Action.C
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        return action

class Strategy_COLLECTIVE_600(BaseStrategy):
    """
    A collective strategy for the N-Player Public Goods Game that adapts to opponents' cooperation levels.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_threshold = 0.5
        self.punishment_probability = 0.8
        self.recent_cooperation_level = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        previous_round_cooperation = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]]) / self.game_description.n_players
        self.recent_cooperation_level.append(previous_round_cooperation)
        if len(self.recent_cooperation_level) > 1 and history.my_payoffs[-1] <= 1 - int(history.my_actions[-1]):
            return Action.D
        if state.round_number % 5 == 0:
            self.adjust_reciprocity()
        if previous_round_cooperation >= self.cooperate_threshold:
            return Action.C
        elif random.random() < 1 - self.game_description.k / self.game_description.n_players:
            return Action.D
        else:
            return Action.C

    def adjust_reciprocity(self):
        overall_cooperation_level = sum(self.recent_cooperation_level) / len(self.recent_cooperation_level)
        if overall_cooperation_level > self.cooperate_threshold:
            self.cooperate_threshold *= 0.9
        else:
            self.cooperate_threshold *= 1.1
        self.cooperate_threshold = min(max(self.cooperate_threshold, 0), 1)

class Strategy_EXPLOITATIVE_513(BaseStrategy):
    """
    The Adaptive Exploiter strategy balances cooperation and defection to maximize payoffs while adapting to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.5
        self.coop_probability = 0.5
        self.punishment_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        prev_round_opponent_actions = history.opponent_actions[-1]
        num_cooperators_last_round = sum(prev_round_opponent_actions)
        total_contributions_last_round = num_cooperators_last_round + (history.my_actions[-1] == Action.C)
        if self.punishment_mode:
            self.punishment_mode = False
            return Action.D
        for opponent_action in prev_round_opponent_actions:
            if opponent_action and history.my_actions[-1] == Action.C:
                pass
            elif not opponent_action and history.my_actions[-1] == Action.C:
                self.punishment_mode = True
        expected_coop_payoff = self.game_description.k / self.game_description.n_players * num_cooperators_last_round + (history.my_actions[-1] == Action.C)
        expected_defect_payoff = 1 - (history.my_actions[-1] == Action.D) + self.game_description.k / self.game_description.n_players * total_contributions_last_round
        if expected_coop_payoff > expected_defect_payoff:
            action = Action.C
        elif expected_coop_payoff < expected_defect_payoff:
            action = Action.D
        else:
            action = random.choice([Action.C, Action.D])
        self.avg_coop_rate = (self.avg_coop_rate * (state.round_number - 1) + num_cooperators_last_round / self.game_description.n_players) / state.round_number
        if self.avg_coop_rate > 0.5:
            self.coop_probability += 0.05
        elif self.avg_coop_rate < 0.3:
            self.coop_probability -= 0.05
        self.coop_probability = max(0, min(self.coop_probability, 1))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if random.random() < self.coop_probability:
            action = Action.C
        else:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_514(BaseStrategy):
    """
    Adaptive Tit-for-Tat with Public Goods Twist.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploitable_opponents = set()
        self.avg_payoffs = [[] for _ in range(game_description.n_players)]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_cooperators_prev_round = sum([1 if action else 0 for action in history.opponent_actions[-1]])
        opponent_actions_prev_round = [action for action in history.opponent_actions[-1]]
        for i, action in enumerate(opponent_actions_prev_round):
            self.avg_payoffs[i].append(action)
            if len(self.avg_payoffs[i]) > 3:
                self.avg_payoffs[i].pop(0)
            if sum([1 if x else 0 for x in self.avg_payoffs[i]]) / len(self.avg_payoffs[i]) < self.game_description.k / self.game_description.n_players:
                self.exploitable_opponents.add(i)
        if any((opponent in self.exploitable_opponents for opponent in range(self.game_description.n_players))):
            return Action.D
        if total_cooperators_prev_round >= self.game_description.k:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_515(BaseStrategy):
    """
    An adaptive strategy that balances short-term gains with long-term adaptability in the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_cooperation_rates = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        elif state.round_number <= 5:
            opponent_cooperation_rate = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
            self.opponent_cooperation_rates.append(opponent_cooperation_rate)
            if opponent_cooperation_rate >= 0.5:
                return Action.C
            else:
                return Action.D
        else:
            avg_opponent_cooperation_rate = np.mean(self.opponent_cooperation_rates[-3:]) if len(self.opponent_cooperation_rates) >= 3 else np.mean(self.opponent_cooperation_rates)
            our_payoff = sum(history.my_payoffs)
            opponent_payoffs = history.opponent_payoffs
            avg_opponent_payoff = np.mean(opponent_payoffs, axis=1)[-3:] if opponent_payoffs.shape[0] >= 3 else np.mean(opponent_payoffs, axis=1)
            if state.round_number == self.game_description.n_rounds - 1:
                return Action.D
            elif avg_opponent_cooperation_rate > 0.5:
                return Action.D
            elif our_payoff < np.mean(avg_opponent_payoff):
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_516(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behaviors in an N-player public goods game.
    Cooperates initially, then uses the average number of cooperators and a threshold value 
    to decide whether to cooperate or defect. Defects in the last round for maximum payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.previous_cooperators = []
        self.C_avg = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        self.previous_cooperators = [opp for opp in history.opponent_actions[-1, :] if opp]
        self.C_avg = len(self.previous_cooperators) / self.game_description.n_players
        threshold_value = 1 - 1 / self.game_description.k
        if self.game_description.k * self.C_avg >= threshold_value:
            action = Action.C
        else:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_517(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to exploit opponents' behaviors while adapting to the game's dynamics.
    It combines elements of Tit-for-Tat (TFT) and Win-Stay-Lose-Shift (WSLS) with a twist to handle the public goods game specifics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_payoffs = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        last_round_cooperators = sum(history.opponent_actions[-1, :] + [True])
        total_payoff = history.my_payoffs[-1]
        opponent_payoff = self.opponent_payoffs[history.opponent_actions.shape[0] % self.game_description.n_players]
        if last_round_cooperators >= (self.game_description.n_players + 1) / 2:
            return Action.C
        elif total_payoff > self.game_description.k * last_round_cooperators / self.game_description.n_players:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_518(BaseStrategy):
    """
    Exploitative strategy "Adaptive Predator" in N-Player Public Goods Game.
    
    This strategy aims to exploit non-cooperative behavior while adapting to the opponents' strategies.
    By defecting in initial rounds, we gather information about opponents and identify potential cooperators.
    As the game progresses, we adjust our cooperation threshold based on the average cooperation rate among opponents.
    If opponents are cooperative, we cooperate to maximize payoffs; otherwise, we defect to exploit non-cooperators.
    
    In late-game scenarios, we prioritize cooperating if it leads to higher expected payoffs, considering no remaining rounds.
    This approach balances short-term exploitation with long-term cooperation, making our strategy robust and adaptive to a wide range of opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = game_description.k / game_description.n_players
        self.opponent_cooperation_rates = np.zeros(game_description.n_players)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        k = self.game_description.k
        if state.round_number == 0:
            return Action.D
        avg_opponent_cooperation_rate = np.mean(self.opponent_cooperation_rates)
        if state.round_number < n_players // 2:
            return Action.D
        elif n_players // 2 <= state.round_number < self.game_description.n_rounds - 1:
            if avg_opponent_cooperation_rate > self.theta:
                return Action.C
            else:
                return Action.D
        else:
            expected_defect_payoff = 1 + k / n_players * np.sum(self.opponent_cooperation_rates)
            expected_cooperate_payoff = k / n_players * (n_players - 1)
            if expected_cooperate_payoff > expected_defect_payoff:
                return Action.C
            else:
                return Action.D
        if history is not None and state.round_number > 0:
            for i in range(n_players):
                if history.opponent_actions[-1, i]:
                    self.opponent_cooperation_rates[i] += 1 / state.round_number

class Strategy_EXPLOITATIVE_519(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behaviors while being robust and adaptive throughout the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 3:
            if state.round_number == 0:
                return Action.C
            else:
                opponent_actions = history.opponent_actions[-1, :]
                defectors = sum((1 - action for action in opponent_actions))
                return Action.D if defectors > self.game_description.n_players / 2 else Action.C
        else:
            opponent_actions = history.opponent_actions[-1, :]
            cooperators = sum((action for action in opponent_actions))
            cr = cooperators / self.game_description.n_players
            return Action.C if cr > self.game_description.k / self.game_description.n_players else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_520(BaseStrategy):
    """
    An adaptive strategy for the N-Player Public Goods Game that balances 
    exploitativeness and cooperation based on opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_actions = [[] for _ in range(game_description.n_players)]
        self.payoffs = []
        self.ties_bias = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        prev_opponent_actions = np.array(history.opponent_actions, dtype=bool)
        num_cooperators = sum(prev_opponent_actions[-1, :])
        if num_cooperators < self.game_description.k / self.game_description.n_players:
            action = Action.D
        elif num_cooperators >= math.ceil(self.game_description.k / self.game_description.n_players):
            action = Action.C
        else:
            action = Action.C if random.random() < self.ties_bias else Action.D
        for i, opponent_action in enumerate(prev_opponent_actions[-1, :]):
            if not opponent_action and sum(prev_opponent_actions[-1, :]) > math.ceil(self.game_description.k / self.game_description.n_players):
                action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            if num_cooperators > math.ceil(self.game_description.k / self.game_description.n_players):
                action = Action.C
            else:
                action = Action.D
        return action

class Strategy_EXPLOITATIVE_521(BaseStrategy):
    """
    Exploitative strategy that adapts to the level of cooperation in the game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_cooperators_prev = 0.0
        self.total_payoff_prev = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        avg_cooperators = sum(history.opponent_actions[-2, :]) / self.game_description.n_players
        total_payoff_prev = history.my_payoffs[-2]
        if history.opponent_actions[-2, 0] == False and history.my_actions[-2]:
            return Action.D
        punishment = (state.round_number - 2) % 3 < 2 if state.round_number > 2 else False
        if avg_cooperators > self.game_description.k / 2:
            return Action.D
        elif total_payoff_prev > 1 - int(history.my_actions[-2]):
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds:
            return Action.D

class Strategy_EXPLOITATIVE_522(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to exploit the cooperative tendencies of opponents 
    while minimizing its own contributions to the public good when cooperation levels are low.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_cooperators = 0
        self.threshold = game_description.n_players / 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_actions = np.ones(state.round_number + 1, dtype=bool)
        if history is not None:
            my_actions[1:] = history.my_actions[:state.round_number]
        avg_cooperators = sum(my_actions) / (self.game_description.n_players * (state.round_number + 1))
        if state.round_number % 5 == 0 and state.round_number > 0:
            if avg_cooperators > self.threshold:
                self.threshold -= 0.1 * self.game_description.n_players
            elif avg_cooperators < self.threshold:
                self.threshold += 0.1 * self.game_description.n_players
        if avg_cooperators >= self.threshold or state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_523(BaseStrategy):
    """
    Exploitative strategy that balances cooperation and exploitation while adapting to changing opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_payoff = 0.0
        self.avg_defect_payoff = 0.0
        self.defectors = []
        self.prev_round_number = None

    def calculate_avg_payoff(self, history: PlayerHistory, action: Action) -> float:
        if self.prev_round_number is None or len(history.my_actions) < 2:
            return 0.0
        avg_payoff = 0.0
        for round_num in range(max(0, len(history.my_actions) - 2), len(history.my_actions)):
            payoff = history.my_payoffs[round_num]
            opponent_action = history.opponent_actions[round_num][0] if action == Action.C else not history.opponent_actions[round_num][0]
            avg_payoff += payoff if action == Action.C and opponent_action or (action == Action.D and (not opponent_action)) else 0.0
        return avg_payoff / min(2, len(history.my_actions))

    def update_avg_payoffs(self, history: PlayerHistory):
        self.avg_coop_payoff = self.calculate_avg_payoff(history, Action.C)
        self.avg_defect_payoff = self.calculate_avg_payoff(history, Action.D)

    def update_defectors(self, history: PlayerHistory):
        if len(history.my_actions) > 1:
            for opponent in range(len(history.opponent_actions[0])):
                if history.opponent_actions[-1][opponent] == False and all((history.opponent_actions[-2][j] for j in range(len(history.opponent_actions[0])) if j != opponent)):
                    self.defectors.append(opponent)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2 or (history is not None and len(history.my_actions) < 2):
            return Action.C
        for opponent in self.defectors:
            if all((not action for action in history.opponent_actions[-1])):
                return Action.D
        self.prev_round_number = state.round_number
        self.update_avg_payoffs(history)
        self.update_defectors(history)
        if self.avg_coop_payoff > self.avg_defect_payoff:
            return Action.C
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds:
            return Action.D

class Strategy_EXPLOITATIVE_524(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy for playing the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.payoff_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        else:
            cooperators = self.game_description.n_players // 2
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif cooperators < self.game_description.n_players / 2:
            return Action.D
        else:
            previous_payoff = 1 - int(history.my_actions[-1]) + self.game_description.k / self.game_description.n_players * sum(history.opponent_actions[-1, :] + [int(history.my_actions[-1])])
            self.payoff_history.append(previous_payoff)
            if previous_payoff >= 1.5:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_525(BaseStrategy):
    """
    The Adaptive Parasite strategy aims to exploit the public goods game by defecting when possible and cooperating when the benefits of doing so are substantial.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defection_success_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        prev_round_actions = history.opponent_actions[-1, :]
        my_payoff = history.my_payoffs[-1]
        self.defection_success_count = self.update_defection_success_count(self.defection_success_count, state.round_number - 1, my_payoff)
        action = self.adaptive_parasite(state.round_number, prev_round_actions)
        if self.defection_success_count > self.game_description.n_rounds / 4:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

    def adaptive_parasite(self, round_num: int, prev_round_actions: NDArray[np.bool_]) -> Action:
        total_cooperators = sum((1 for action in prev_round_actions if action))
        if total_cooperators == self.game_description.n_players // 2:
            return Action.D
        if total_cooperators >= self.game_description.n_players / 2:
            return Action.C
        else:
            return Action.D

    def update_defection_success_count(self, defection_success_count: int, round_num: int, payoff: float) -> int:
        if payoff >= 2:
            defection_success_count += 1
        if (round_num + 1) % (self.game_description.n_rounds / 2) == 0:
            defection_success_count = 0
        return defection_success_count

class Strategy_EXPLOITATIVE_526(BaseStrategy):
    """
    An adaptive exploitative strategy for N-Player Public Goods Game.
    It combines reactive and proactive elements to maximize total payoff by 
    exploiting cooperative tendencies while considering game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = 0.5
        self.payoff_diff_threshold = 1.0
        self.majority_defect_threshold = 50.0
        self.payoff_gap_threshold = 1.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_payoffs = np.sum(history.my_payoffs[:state.round_number])
        opponent_payoffs = np.mean(np.sum(history.opponent_payoffs[:state.round_number], axis=1))
        if state.round_number == self.game_description.n_rounds - 1 and my_payoffs >= opponent_payoffs + self.payoff_gap_threshold:
            return Action.C
        opponent_actions_last_round = history.opponent_actions[-1, :]
        avg_coop_rate = np.mean(opponent_actions_last_round)
        if avg_coop_rate > self.coop_threshold:
            action = 'C'
        elif my_payoffs < opponent_payoffs - self.payoff_diff_threshold:
            action = 'D'
        elif sum(~opponent_actions_last_round) / self.game_description.n_players * 100 > self.majority_defect_threshold:
            action = 'D'
        else:
            action = 'C'
        return Action.C if action == 'C' else Action.D

class Strategy_EXPLOITATIVE_527(BaseStrategy):
    """
    Exploitative strategy, adapting to opponent cooperation rates.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators = 0
        self.acr_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number == 1 or state.round_number == 2:
            return Action.C if random.random() < 0.5 else Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif state.round_number == self.game_description.n_rounds:
            acr = sum(self.acr_history) / len(self.acr_history)
            if acr >= 0.5:
                return Action.D
            else:
                return Action.C
        else:
            opponent_cooperators = sum(history.opponent_actions[-1, :])
            self.total_cooperators += opponent_cooperators
            acr = self.total_cooperators / (self.game_description.n_players * state.round_number)
            self.acr_history.append(acr)
            if acr >= 0.5:
                probability = 1 - self.game_description.k / self.game_description.n_players * acr
                return Action.D if random.random() < probability else Action.C
            else:
                probability = self.game_description.k / self.game_description.n_players * acr
                return Action.C if random.random() < probability else Action.D

class Strategy_EXPLOITATIVE_528(BaseStrategy):
    """
    An exploitative strategy for N-Player Public Goods Game that adapts to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.round_counter = 0
        self.avg_coop_rate = None
        self.total_payoffs = [0.0] * (game_description.n_players - 1)
        self.target_id = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        self.round_counter = state.round_number
        if history is None or len(history.opponent_actions) < 2:
            return Action.C
        opponent_actions = history.opponent_actions
        opponent_payoffs = history.opponent_payoffs
        recent_coop_rates = np.mean(opponent_actions[-2:, :], axis=0)
        self.avg_coop_rate = np.mean(recent_coop_rates)
        if self.round_counter < self.game_description.n_rounds - 1:
            if self.avg_coop_rate >= 0.5:
                return Action.C
            self.total_payoffs = np.sum(opponent_payoffs[-3:, :], axis=0)
            self.target_id = np.argmax(self.total_payoffs)
            if opponent_actions[-1, self.target_id] == True:
                return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_529(BaseStrategy):
    """
    An exploitative strategy in an N-player public goods game.
    It adapts its cooperation probability based on the number of opponents cooperating.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        total_cooperators = sum(history.opponent_actions[-1, :]) + int(bool(history.my_actions[-1]))
        n = self.game_description.n_players
        k = self.game_description.k
        if state.round_number < self.game_description.n_rounds - 1:
            if total_cooperators <= n / 2:
                cooperate_prob = k / n * (total_cooperators / n)
                return Action.C if random.random() < cooperate_prob else Action.D
            else:
                return Action.D
        if total_cooperators > n / 2:
            return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            cooperate_prob = k / n * (total_cooperators / n)
            return Action.C if random.random() < cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_530(BaseStrategy):
    """
    Adaptive Exploiter strategy for N-Player Public Goods Game.
    
    This strategy balances exploiting opponents' cooperative behavior with adapting to changes in their actions.
    By analyzing opponents' past behavior and adjusting its own cooperation threshold, the AE strategy can effectively navigate various environments and exploit any available opportunities for higher payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_cooperation_rates = [0.5] * (game_description.n_players - 1)
        self.cooperation_threshold = 0.5
        self.previous_overall_coop_rate = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        for i in range(len(self.opponent_cooperation_rates)):
            self.opponent_cooperation_rates[i] = (self.opponent_cooperation_rates[i] * state.round_number + int(history.opponent_actions[state.round_number - 1, i])) / (state.round_number + 1)
        opponent_avg_coop_rate = np.mean(self.opponent_cooperation_rates)
        if self.previous_overall_coop_rate is not None:
            overall_cooperation_rate = (self.previous_overall_coop_rate * (state.round_number - 1) + opponent_avg_coop_rate) / state.round_number
        else:
            overall_cooperation_rate = opponent_avg_coop_rate
        if state.round_number % 10 == 0 and self.previous_overall_coop_rate is not None:
            if overall_cooperation_rate > self.previous_overall_coop_rate:
                self.cooperation_threshold += 0.1
            elif overall_cooperation_rate < self.previous_overall_coop_rate:
                self.cooperation_threshold -= 0.1
        total_cooperators_prev_round = sum((int(action) for action in history.opponent_actions[state.round_number - 1, :]))
        if total_cooperators_prev_round >= self.game_description.n_players // 2:
            return Action.D
        elif opponent_avg_coop_rate > self.cooperation_threshold:
            return Action.C
        else:
            return Action.D if state.round_number == self.game_description.n_rounds - 1 else random.choice([Action.C, Action.D])
        self.previous_overall_coop_rate = overall_cooperation_rate

class Strategy_EXPLOITATIVE_531(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to balance exploration and exploitation in the public goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = math.floor(0.5 * game_description.n_players)
        self.round_data = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        prev_opponent_actions = history.opponent_actions[-1]
        cooperators = sum(prev_opponent_actions)
        defectors = len(prev_opponent_actions) - cooperators
        if cooperators > self.threshold:
            self.threshold += 1
        elif cooperators < self.threshold:
            self.threshold -= 1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if state.round_number < 3:
            if random.random() < 0.5:
                return Action.C
            else:
                return Action.D
        for opponent_action in prev_opponent_actions:
            if not opponent_action:
                return Action.D
            elif opponent_action and cooperators >= 2:
                return Action.D
        if cooperators >= self.threshold:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_532(BaseStrategy):
    """
    Exploits opponents' cooperative tendencies while adapting to changing game conditions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.previous_payoff_defecting = None
        self.total_cooperators_previous_round = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.total_cooperators_previous_round = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        opponent_payoffs = history.opponent_payoffs[:, 0]
        my_payoff_defecting = 1 + self.game_description.k * self.total_cooperators_previous_round / self.game_description.n_players
        my_payoff_cooperating = self.game_description.k * self.total_cooperators_previous_round / self.game_description.n_players
        self.previous_payoff_defecting = my_payoff_defecting > my_payoff_cooperating
        p = self.game_description.k / self.game_description.n_players * (self.total_cooperators_previous_round / self.game_description.n_players)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif self.total_cooperators_previous_round <= self.game_description.n_players // 2 or self.previous_payoff_defecting:
            return Action.D
        elif random.random() < p:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_533(BaseStrategy):
    """
    Exploitative strategy designed to capitalize on cooperative opponents.
    Adapts its own actions based on the cooperation rate of opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5
        self.avg_coop_rate = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        k = self.game_description.k
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        elif state.round_number <= 3:
            previous_opponent_actions = history.opponent_actions[-2, :]
            if any(previous_opponent_actions):
                return Action.D
            else:
                return Action.C
        else:
            recent_rounds = min(3, state.round_number)
            opponent_coop_rates = history.opponent_actions[-recent_rounds:, :]
            self.avg_coop_rate = np.mean(opponent_coop_rates)
            if self.avg_coop_rate > self.theta:
                return Action.D
            elif any(history.opponent_actions[-1, :]):
                return Action.C
            else:
                return Action.D
        self.theta = max(0, min(1, self.theta + (self.avg_coop_rate - self.theta) / 10))

class Strategy_EXPLOITATIVE_534(BaseStrategy):
    """
    Adaptive Exploiter strategy that combines elements of game theory and machine learning 
    to exploit opponents' behaviors while being robust to various scenarios.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.C = Action.C.value
        self.D = Action.D.value
        self.CR_history = []

    def calculate_CR(self, opponent_actions):
        """
        Calculate Cooperation Rate (CR)
        
        :param opponent_actions: Actions of all opponents in the current round
        :return: Average number of cooperators in the previous rounds
        """
        if not self.CR_history:
            return 0.5
        window_size = max(2, math.floor(len(self.CR_history) * 0.1))
        recent_CRs = self.CR_history[-window_size:]
        return np.mean(recent_CRs)

    def get_previous_payoff(self, action, my_payoffs):
        """
        Get previous payoff for a given action
        
        :param action: Action to check (C or D)
        :param my_payoffs: Payoffs from all rounds
        :return: Previous payoff for the specified action
        """
        if len(my_payoffs) == 0:
            return 1.0
        previous_round_action = Action(int(my_payoffs[-1] > 0))
        if previous_round_action == action:
            return my_payoffs[-1]
        else:
            return 1.0 - my_payoffs[-1]

    def punish_defector(self, opponent_actions):
        """
        Punish defectors by defecting for 2-3 rounds
        
        :param opponent_actions: Actions of all opponents in the current round
        :return: Action to take (D)
        """
        return self.D

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        opponent_actions = history.opponent_actions[-1, :]
        my_payoffs = history.my_payoffs
        CR = self.calculate_CR(opponent_actions)
        previous_payoff_C = self.get_previous_payoff(self.C, my_payoffs)
        previous_payoff_D = self.get_previous_payoff(self.D, my_payoffs)
        if CR < 0.5:
            return Action.D
        elif CR >= 0.5 and previous_payoff_C >= previous_payoff_D:
            self.CR_history.append(np.mean(opponent_actions))
            return Action.C
        else:
            for opponent_action in opponent_actions:
                if not opponent_action:
                    return self.punish_defector(opponent_actions)
            return Action.D

class Strategy_EXPLOITATIVE_535(BaseStrategy):
    """
    An adaptive strategy that balances short-term gains with long-term cooperation,
    while exploiting opponents' behaviors in an N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.rounds = game_description.n_rounds
        self.my_payoffs = []
        self.opponent_coop_rates = [0.5] * self.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        elif state.round_number < 5:
            cooperators = sum(history.opponent_actions[-1, :])
            return Action.C if cooperators >= self.n_players / 2 else Action.D
        elif state.round_number < self.rounds // 2:
            apc, apd = self.calculate_average_payoffs(history)
            return Action.C if apc > apd else Action.D
        else:
            cooperators = sum(history.opponent_actions[-1, :])
            if cooperators >= 3 * self.n_players / 4:
                return Action.D
            elif state.round_number == self.rounds - 1:
                return Action.D
            else:
                return Action.C

    def calculate_average_payoffs(self, history: PlayerHistory):
        my_coop_count = sum(history.my_actions)
        opponent_coop_counts = np.sum(history.opponent_actions, axis=0)
        apc = np.mean([self.k / self.n_players * (my_coop_count + coop_count) if coop else 0 for coop, coop_count in zip(history.my_actions, opponent_coop_counts)])
        apd = np.mean([1 - coop + self.k / self.n_players * coop_count for coop, coop_count in zip(~history.my_actions, opponent_coop_counts)])
        return (apc, apd)

    def update_opponent_coop_rates(self, history: PlayerHistory):
        if history:
            for i, actions in enumerate(history.opponent_actions.T):
                coop_rate = np.mean(actions)
                self.opponent_coop_rates[i] = coop_rate

class Strategy_EXPLOITATIVE_536(BaseStrategy):
    """
    An adaptive strategy that prioritizes exploitation by cooperating when cooperation is more rewarding and defecting otherwise.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_payoff_avg = 0.0
        self.defect_payoff_avg = 0.0
        self.cooperation_rate = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_coop_payoffs = history.my_payoffs[-1] * history.my_actions[-1]
        prev_round_defect_payoffs = history.my_payoffs[-1] * ~history.my_actions[-1]
        self.cooperate_payoff_avg = (self.cooperate_payoff_avg * (state.round_number - 1) + prev_round_coop_payoffs) / state.round_number
        self.defect_payoff_avg = (self.defect_payoff_avg * (state.round_number - 1) + prev_round_defect_payoffs) / state.round_number
        opponent_actions = history.opponent_actions[-1]
        self.cooperation_rate = np.mean(opponent_actions)
        if self.cooperate_payoff_avg > self.defect_payoff_avg:
            action = Action.C
        else:
            action = Action.D
        if self.cooperation_rate > 0.75:
            action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_537(BaseStrategy):
    """
    An exploitative strategy that adapts to the cooperation of other players.
    It starts by cooperating in the first round, then adjusts its behavior 
    based on the number of cooperators and average payoff in subsequent rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        k = self.game_description.k
        r = self.game_description.n_rounds
        round_number = state.round_number
        if round_number == 0:
            return Action.C
        opponent_actions_prev_round = history.opponent_actions[-1, :]
        cooperators_prev_round = sum(opponent_actions_prev_round)
        avg_payoff_prev_round = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1])
        if round_number <= n_players // 2:
            return Action.C if cooperators_prev_round >= n_players / 2 else Action.D
        elif n_players // 2 < round_number < r - n_players // 2:
            return Action.C if avg_payoff_prev_round >= k / n_players else Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_538(BaseStrategy):
    """
    An adaptive strategy that balances cooperation and exploitation in the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.my_payoff_history = []
        self.opponents_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        total_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        avg_opponents_payoff = np.mean(np.delete(history.opponent_payoffs[-1], 0))
        if state.round_number < self.game_description.n_rounds - 1:
            if total_cooperators >= self.game_description.k:
                return Action.C
            elif len(self.my_payoff_history) > 0 and self.my_payoff_history[-1] > avg_opponents_payoff and (total_cooperators < self.game_description.k):
                return Action.D
            elif len(self.my_payoff_history) > 0 and self.my_payoff_history[-1] < avg_opponents_payoff and any((d_i == Action.D for d_i in history.opponent_actions[-1, :])):
                return Action.D
            else:
                return Action.C if total_cooperators >= self.game_description.k else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_539(BaseStrategy):
    """
    An adaptive exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.in_exploitation_phase = False
        self.exploitation_phase_count = 0
        self.in_punishment_phase = False
        self.punishment_phase_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        previous_round_avg_payoff = np.mean(history.my_payoffs[-1])
        any_player_cooperated = np.any(history.opponent_actions[-1, :])
        if not self.in_exploitation_phase and previous_round_avg_payoff >= self.game_description.k / self.game_description.n_players and any_player_cooperated:
            return Action.C
        elif self.in_exploitation_phase:
            action = Action.D
            self.exploitation_phase_count += 1
            if self.exploitation_phase_count >= 3:
                self.in_punishment_phase = True
                self.in_exploitation_phase = False
                self.exploitation_phase_count = 0
            return action
        elif self.in_punishment_phase:
            action = Action.C
            self.punishment_phase_count += 1
            if self.punishment_phase_count == 1:
                self.in_punishment_phase = False
                self.punishment_phase_count = 0
                self.in_exploitation_phase = False
                return random.choice([Action.C, Action.D])
            return action
        elif state.round_number + 1 == self.game_description.n_rounds:
            return Action.D
        else:
            self.in_exploitation_phase = True
            return Action.D

class Strategy_EXPLOITATIVE_540(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior by cooperating 
    when the number of cooperators exceeds the multiplication factor and 
    defecting otherwise.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        self.cooperation_threshold = np.mean([self.game_description.k, prev_cooperators / self.game_description.n_players])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if prev_cooperators >= self.cooperation_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_541(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior while maximizing its own payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.THR = game_description.k / (2 * game_description.n_players)
        self.CR_history = []
        self.opponent_payoffs = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        C_count = sum((1 for i in range(self.game_description.n_players - 1) if history.opponent_actions[-1, i]))
        CR = C_count / (self.game_description.n_players - 1)
        if CR > self.THR:
            self.THR += 0.01
        elif CR < self.THR:
            self.THR -= 0.01
        if CR >= self.THR:
            action = Action.C
        else:
            action = Action.D
        if history.my_actions[-1] == True and any(history.opponent_actions[-1, :] == False):
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_542(BaseStrategy):
    """
    A strategy that exploits opponents by defecting when they cooperate excessively.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.5
        self.num_cooperators = []
        self.default_coop_rate = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or state.round_number < len(self.num_cooperators):
            return random.choice([Action.C, Action.D])
        self.update_avg_coop_rate(history)
        avg_coop_rate = self.avg_coop_rate
        theta_high = 0.6
        epsilon = 0.05
        k_over_n = self.game_description.k / self.game_description.n_players
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        elif avg_coop_rate > theta_high:
            return Action.D
        else:
            p = min(1, k_over_n * avg_coop_rate + epsilon)
            return Action.C if random.random() < p else Action.D

    def update_avg_coop_rate(self, history: PlayerHistory):
        self.num_cooperators.append(sum(history.opponent_actions[-1, :]))
        total_rounds = len(self.num_cooperators) - 1
        total_cooperators = sum(self.num_cooperators)
        n_players = self.game_description.n_players
        self.avg_coop_rate = total_cooperators / (n_players * total_rounds)

class Strategy_EXPLOITATIVE_543(BaseStrategy):
    """
    Adaptive Cooperate-Defect (ACD) Strategy.
    
    This strategy aims to exploit a wide range of opponent behaviors while being robust and adaptive.
    The decision rules are based on the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_behavior = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        T = self.game_description.k / self.game_description.n_players * (1 - (state.round_number + 1) / self.game_description.n_rounds)
        CR = self.calculate_average_cooperation_rate(self.opponent_behavior)
        if history is not None:
            opponent_actions = np.delete(history.opponent_actions[-1, :], 0)
            self.opponent_behavior.append(opponent_actions)
        if CR > T or state.round_number + 1 == self.game_description.n_rounds:
            return Action.D
        if self.opponent_has_defected_in_more_than_half_rounds(self.opponent_behavior):
            return Action.D
        return Action.C

    def calculate_average_cooperation_rate(self, opponent_behavior):
        cooperations = sum((np.sum(actions) for actions in opponent_behavior))
        total_actions = len(opponent_behavior) * (self.game_description.n_players - 1)
        if total_actions == 0:
            return 0
        return cooperations / total_actions

    def opponent_has_defected_in_more_than_half_rounds(self, opponent_behavior):
        recent_opponent_behavior = [actions for actions in reversed(opponent_behavior) if np.any(actions)]
        for actions in recent_opponent_behavior:
            defections_since_last_cooperation = sum((1 for action in actions if not action))
            if defections_since_last_cooperation > (self.game_description.n_players - 1) / 2:
                return True
        return False

class Strategy_EXPLOITATIVE_546(BaseStrategy):
    """
    A robust and exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_memory = 0
        self.p_coop = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D]) if random.random() < self.p_coop else Action.D
        c_prev = sum(history.opponent_actions[-1, :])
        self.coop_memory = 1 if c_prev > self.game_description.n_players / 2 else -1
        self.p_coop += self.coop_memory * 0.1
        self.p_coop = max(0, min(self.p_coop, 1))
        avg_coop_payoff = self.game_description.k / self.game_description.n_players * c_prev
        avg_defect_payoff = 1 + self.game_description.k / self.game_description.n_players * c_prev
        if state.round_number < self.game_description.n_rounds - 1:
            return Action.C if avg_coop_payoff > avg_defect_payoff else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_547(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and maximizes its payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.mode = 'exploration'

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if self.mode == 'exploration':
            if state.round_number <= 3:
                cooperate_prob = 0.5
                if random.random() < cooperate_prob:
                    action = Action.C
                else:
                    action = Action.D
            else:
                cooperate_prob = 0.5
                if random.random() < cooperate_prob:
                    action = Action.C
                else:
                    action = Action.D
            if state.round_number == 3 and (sum(history.opponent_actions[-2, :]) >= self.game_description.n_players / 2 or history.my_payoffs[-1] > 1):
                self.mode = 'exploitation'
        else:
            TC = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
            π_i = history.my_payoffs[-1]
            if TC > self.game_description.n_players / 2 or π_i > self.game_description.k / self.game_description.n_players * TC:
                action = Action.D
            else:
                action = Action.C
        return action

class Strategy_EXPLOITATIVE_549(BaseStrategy):
    """
    The Adaptive Predator strategy exploits opponent behaviors while adapting to game dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_payoff = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1])
        num_cooperators = sum((1 for opponent_action in history.opponent_actions[-1, :] if opponent_action))
        punish_opponent = False
        for opponent_idx, opponent_payoff in enumerate(history.opponent_payoffs[-1]):
            if opponent_payoff > self.k and (not history.opponent_actions[-1, opponent_idx]):
                punish_opponent = True
                break
        if avg_payoff >= self.k:
            return Action.D
        elif num_cooperators <= math.floor(self.n_players / 2):
            return Action.C
        else:
            return Action.D
        if punish_opponent and history.my_actions[-1]:
            return Action.D
        return Action.C if not history.my_actions[-1] else Action.D

class Strategy_EXPLOITATIVE_550(BaseStrategy):
    """
    History-Driven Adaptive Exploitation strategy.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff = 0
        self.num_cooperators = 0
        self.defector_list = []
        self.cooperate_prob = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        avg_payoff_t = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1, :])
        self.cooperate_prob = avg_payoff_t if avg_payoff_t >= 1 else 0
        num_cooperators_t = sum(history.opponent_actions[-1, :])
        if num_cooperators_t > self.game_description.n_players / 2:
            return Action.D
        if history.my_payoffs.size > 0 and np.mean(history.opponent_payoffs[-1, :]) < 1:
            self.defector_list.append(True)
            if random.random() < 0.7:
                return Action.D
        return Action.C if random.random() < self.cooperate_prob else Action.D

class Strategy_EXPLOITATIVE_551(BaseStrategy):
    """
    An exploitative strategy that adapts to the cooperation rate and payoff difference of opponents.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_coop = 0.0
        self.avg_payoff_defect = 0.0
        self.cooperation_rate = 0.0
        self.payoff_diff = 0.0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        n = self.game_description.n_players
        recent_rounds = history.my_payoffs[-n:]
        opponent_actions_recent = history.opponent_actions[-n:, :]
        avg_coop_payoff, avg_defect_payoff = self.calculate_avg_payoffs(recent_rounds, opponent_actions_recent)
        if state.round_number <= n:
            action = Action.C if state.round_number == 1 or avg_coop_payoff > avg_defect_payoff else Action.D
        elif state.round_number < self.game_description.n_rounds - n + 1:
            cooperation_rate = np.mean(opponent_actions_recent)
            payoff_diff = avg_coop_payoff - avg_defect_payoff
            action = Action.D if cooperation_rate < 0.5 or payoff_diff > 0 else Action.C
        else:
            total_payoff_coop, total_payoff_defect = self.calculate_total_payoffs(history)
            action = Action.C if total_payoff_coop > total_payoff_defect else Action.D
        return action

    def calculate_avg_payoffs(self, recent_rounds: NDArray[np.float64], opponent_actions_recent: NDArray[np.bool_]) -> tuple[float, float]:
        coop_mask = np.where(recent_rounds > 0, True, False)[:, None] & opponent_actions_recent
        defect_mask = ~coop_mask
        avg_coop_payoff = np.mean(np.where(coop_mask, recent_rounds[:, None], np.nan), axis=1).mean()
        avg_defect_payoff = np.mean(np.where(defect_mask, recent_rounds[:, None], np.nan), axis=1).mean()
        return (avg_coop_payoff, avg_defect_payoff)

    def calculate_total_payoffs(self, history: PlayerHistory) -> tuple[float, float]:
        my_actions = np.where(history.my_actions, Action.C, Action.D)
        opponent_actions_recent = history.opponent_actions[-self.game_description.n_players:, :]
        coop_payoff_sum = 0.0
        defect_payoff_sum = 0.0
        for i in range(self.game_description.n_players):
            if my_actions[i] == Action.C:
                coop_payoff_sum += history.my_payoffs[i]
            else:
                defect_payoff_sum += history.my_payoffs[i]
        return (coop_payoff_sum, defect_payoff_sum)

class Strategy_EXPLOITATIVE_553(BaseStrategy):
    """
    Exploitative strategy, aiming to maximize own payoff by adapting to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.0
        self.coop_threshold = 0.5 * game_description.k / game_description.n_players
        self.coop_scores = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        total_cooperations = sum(history.my_actions[:-1]) + sum((sum(opponent_coop) for opponent_coop in history.opponent_actions[:-1]))
        self.avg_coop_rate = total_cooperations / (state.round_number * self.game_description.n_players)
        if state.round_number > 1:
            my_prev_action = history.my_actions[-2]
            for i, opponent in enumerate(history.opponent_actions[-2]):
                opponent_prev_action = opponent
                if opponent_prev_action and (not my_prev_action):
                    self.coop_scores[i] += 1
                elif not opponent_prev_action and my_prev_action:
                    self.coop_scores[i] -= 1
        if state.round_number < self.game_description.n_rounds - 1:
            if self.avg_coop_rate > self.coop_threshold:
                return Action.C
            else:
                return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_554(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D]) if random.random() < 0.5 else Action.C
        prev_round = history.my_actions.shape[0] - 1
        my_action_prev = history.my_actions[prev_round]
        opponent_actions_prev = history.opponent_actions[prev_round, :]
        my_payoff_prev = history.my_payoffs[prev_round]
        avg_c_payoff = np.mean(history.my_payoffs[:prev_round + 1][history.my_actions[:prev_round + 1] == 1])
        avg_d_payoff = np.mean(history.my_payoffs[:prev_round + 1][history.my_actions[:prev_round + 1] == 0])
        if avg_c_payoff == 0 or avg_d_payoff == 0:
            return Action.D
        theta = min(0.5, avg_c_payoff / avg_d_payoff)
        cooperators_prev_round = np.sum(opponent_actions_prev) + my_action_prev
        if cooperators_prev_round >= self.game_description.n_players * theta:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if theta == 0.5:
            return random.choice([Action.C, Action.D]) if random.random() < 0.5 else Action.C
        return Action.D

class Strategy_EXPLOITATIVE_555(BaseStrategy):
    """
    A hybrid strategy that balances cooperation and defection based on observed opponent behavior and game history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.observation_rounds = 5
        self.high_threshold = 0.6
        self.low_threshold = 0.4
        self.avg_cooperation_rate = None
        self.penultimate_round_actions = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        elif state.round_number <= self.observation_rounds + 1:
            opponent_cooperations = np.sum(history.opponent_actions, axis=1)
            total_cooperators = np.sum(opponent_cooperations) + 1
            if self.avg_cooperation_rate is None:
                self.avg_cooperation_rate = total_cooperators / (self.game_description.n_players * state.round_number)
            else:
                self.avg_cooperation_rate = ((self.game_description.n_players - 1) * self.avg_cooperation_rate * (state.round_number - 1) + total_cooperators) / (self.game_description.n_players * state.round_number)
        if state.round_number > self.observation_rounds:
            if self.avg_cooperation_rate is not None and self.avg_cooperation_rate > self.high_threshold:
                return Action.D
            elif self.avg_cooperation_rate is not None and self.avg_cooperation_rate < self.low_threshold:
                return Action.C
            else:
                if random.random() < self.avg_cooperation_rate if self.avg_cooperation_rate is not None else 0.5:
                    return Action.C
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            self.penultimate_round_actions = history.opponent_actions[-1, :]
        if state.round_number == self.game_description.n_rounds:
            if np.sum(self.penultimate_round_actions) >= (self.game_description.n_players - 1) // 2:
                return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_556(BaseStrategy):
    """
    The Adaptive Predator strategy exploits opponent weaknesses in the N-Player Public Goods Game.
    
    In initial rounds, it cooperates to gather information. During observation phase, 
    it adapts its behavior based on the number of cooperating opponents. 
    Finally, it identifies and targets players with low average payoffs for exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.past_payoffs = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        n_players = self.game_description.n_players
        k = self.game_description.k
        r = self.game_description.n_rounds
        round_num = state.round_number
        cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if 0 < round_num <= 3:
            return Action.C
        elif 4 <= round_num < r // 2:
            return Action.C if cooperators >= n_players / 2 else Action.D
        else:
            avg_payoff = np.mean(self.past_payoffs, axis=0) if self.past_payoffs else None
            if avg_payoff is not None:
                lowest_avg_payoff_players = np.where(avg_payoff == np.min(avg_payoff))[0]
                my_index = n_players - 1
                if len(lowest_avg_payoff_players) > 1:
                    cooperation_counts = np.sum(history.opponent_actions[:, lowest_avg_payoff_players], axis=0)
                    max_cooperation_count = np.max(cooperation_counts)
                    prioritized_indices = lowest_avg_payoff_players[cooperation_counts == max_cooperation_count]
                    return Action.C if my_index in prioritized_indices else Action.D
                return Action.C if my_index in lowest_avg_payoff_players else Action.D
            else:
                return Action.D
        self.past_payoffs.append(history.my_payoffs)

class Strategy_EXPLOITATIVE_557(BaseStrategy):
    """
    An adaptive exploitative strategy for N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.noise_tolerance = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        avg_cooperators = sum((1 for round in history.opponent_actions[:-1] if np.count_nonzero(round)))
        avg_defectors = len(history.opponent_actions[:-1]) - avg_cooperators
        if avg_defectors == 0:
            return Action.C
        ratio = avg_cooperators / (avg_cooperators + avg_defectors)
        noise = random.uniform(-self.noise_tolerance, self.noise_tolerance)
        ratio += noise
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D if ratio > 0.5 else Action.C
        return Action.D if ratio > 0.5 else Action.C

class Strategy_EXPLOITATIVE_558(BaseStrategy):
    """
    A strategy that adapts to the game's dynamics by initially cooperating,
    then defecting more frequently in exploitative environments and adjusting
    to maximize personal gain in final rounds.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_coop_count = sum(history.opponent_actions[-1, :] + [True])
        coop_env = prev_coop_count > self.game_description.n_players / 2
        if state.round_number >= self.game_description.n_rounds * 0.8:
            if coop_env:
                p_D = 0.5 + 0.5 * (self.game_description.n_rounds - state.round_number) / self.game_description.n_rounds
                return Action.D if random.random() < p_D else Action.C
            else:
                pass
        if coop_env:
            p_C = self.game_description.k / self.game_description.n_players - 0.5
            return Action.C if random.random() < p_C else Action.D
        else:
            p_D = 0.8 + 0.2 * (self.game_description.n_players - prev_coop_count) / self.game_description.n_players
            return Action.D if random.random() < p_D else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_559(BaseStrategy):
    """
    A strategic approach that exploits the game's structure and adapts to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_ocr = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions
        total_cooperators = np.sum(opponent_actions[-1, :])
        self.opponent_ocr = np.mean(np.any(opponent_actions[:-1, :], axis=0))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if total_cooperators >= self.game_description.n_players / 2:
            return Action.D
        elif self.opponent_ocr > 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_560(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to maximize its own payoff by exploiting 
    the cooperation levels of other players while minimizing the risk of being exploited itself.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.tau = 0.0
        self.opponent_coop_rate = 0.0
        self.punish = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        self.tau = self.game_description.k / self.game_description.n_players * np.mean(history.opponent_actions[:, :])
        self.opponent_coop_rate = np.mean(history.opponent_actions[-1, :])
        if self.opponent_coop_rate > self.tau:
            return Action.D
        elif self.opponent_coop_rate < 0.3 * self.tau:
            return Action.C
        if history.my_payoffs[-1] == 0 and history.my_actions[-1]:
            self.punish = True
        else:
            self.punish = False
        if self.punish:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_561(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperation of others while minimizing its own contributions to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_cooperation_rate = 0.5

    def calculate_opponent_cooperation_rate(self, history: PlayerHistory) -> float:
        """
        Calculate the average cooperation rate of all opponents over the previous rounds.
        """
        total_cooperations = sum(sum(history.opponent_actions[:history.my_actions.size]))
        return total_cooperations / (self.game_description.n_players * self.game_description.n_rounds)

    def calculate_expected_payoff_defect(self, opponent_cooperation_rate: float) -> float:
        """
        Calculate the expected payoff if we defect in this round.
        """
        return 1 + self.game_description.k / self.game_description.n_players * opponent_cooperation_rate

    def calculate_expected_payoff_cooperate(self, opponent_cooperation_rate: float) -> float:
        """
        Calculate the expected payoff if we cooperate in this round.
        """
        return self.game_description.k / self.game_description.n_players * (opponent_cooperation_rate + 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return random.choice([Action.C, Action.D])
        self.opponent_cooperation_rate = self.calculate_opponent_cooperation_rate(history)
        expected_payoff_defect = self.calculate_expected_payoff_defect(self.opponent_cooperation_rate)
        expected_payoff_cooperate = self.calculate_expected_payoff_cooperate(self.opponent_cooperation_rate)
        if expected_payoff_defect > expected_payoff_cooperate:
            return Action.D
        else:
            return Action.C
        if self.opponent_cooperation_rate == 0:
            return Action.D
        elif self.opponent_cooperation_rate == 1:
            return Action.C
        if state.round_number == self.game_description.n_rounds:
            if sum(history.my_actions) > len(history.my_actions) / 2:
                return Action.C
            else:
                return Action.D

class Strategy_EXPLOITATIVE_563(BaseStrategy):
    """
    Adaptive Exploiter (AE) strategy for the N-Player Public Goods Game.

    This strategy balances cooperation and exploitation to maximize payoffs while adapting to opponents' behavior over time.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.rounds = game_description.n_rounds
        self.N_C = [0] * self.n_players
        self.N_D = [0] * self.n_players
        self.opponents = ['Unpredictable'] * self.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 2:
            return Action.C
        for i in range(self.n_players):
            if self.opponents[i] == 'Cooperator' or (self.opponents[i] == 'Unpredictable' and self.N_C[i] / state.round_number > 0.75):
                self.N_C[i] += int(history.opponent_actions[-1, i])
            elif self.opponents[i] == 'Defector' or (self.opponents[i] == 'Unpredictable' and self.N_D[i] / state.round_number > 0.75):
                self.N_D[i] += int(not history.opponent_actions[-1, i])
        for i in range(self.n_players):
            if self.N_C[i] / state.round_number > 0.75:
                self.opponents[i] = 'Cooperator'
            elif self.N_D[i] / state.round_number > 0.75:
                self.opponents[i] = 'Defector'
        total_cooperators = sum(self.N_C)
        if total_cooperators >= self.n_players // 2:
            return Action.C
        elif total_cooperators <= self.n_players // 2:
            return Action.D
        else:
            public_good_value = self.k / self.n_players * total_cooperators
            if public_good_value > 0.5:
                return Action.C
            else:
                return Action.D
        if state.round_number >= self.rounds - 2:
            if total_cooperators > sum(self.N_D):
                return Action.D
            elif sum(self.N_D) > total_cooperators and self.k / self.n_players * total_cooperators <= 0.5:
                return Action.D
        if random.random() < 0.5:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_564(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        total_cooperators_previous_round = sum(history.opponent_actions[-1, :] + [history.my_actions[-1]])
        if total_cooperators_previous_round <= self.game_description.n_players / 2:
            return Action.D
        else:
            p = self.game_description.k / self.game_description.n_players * (total_cooperators_previous_round / self.game_description.n_players)
            return Action.C if random.random() < p else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        for opponent_actions in history.opponent_actions.T:
            if sum(opponent_actions) / len(opponent_actions) > 0.75:
                return Action.D
        for opponent_actions in history.opponent_actions.T:
            if sum((1 - x for x in opponent_actions)) / len(opponent_actions) > 0.75:
                return Action.C if random.random() < 0.5 else Action.D

class Strategy_EXPLOITATIVE_565(BaseStrategy):
    """
    Balances exploitation with adaptability and robustness.
    Exploits opponents who tend to free-ride or are overly greedy, 
    while encouraging cooperation by mirroring cooperators and punishing defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.exploitation_threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        n_players = self.game_description.n_players
        n_rounds = self.game_description.n_rounds
        prev_round_opponent_actions = history.opponent_actions[-1, :]
        prev_round_my_payoff = history.my_payoffs[-1]
        if state.round_number == n_rounds - 1:
            return Action.D
        num_cooperators = sum(prev_round_opponent_actions)
        if num_cooperators < n_players * self.exploitation_threshold:
            return Action.D
        if any(prev_round_opponent_actions):
            return Action.C
        else:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_566(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Predator"

    This strategy is designed to exploit opponents' behaviors while being robust to various strategies and adapting to changing game dynamics.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rates = [0.5] * game_description.n_players
        self.payoffs_deviation = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        k = self.game_description.k
        if state.round_number == 0 or state.round_number < 3:
            cooperate_prob = 0.5
            if state.round_number > 1:
                majority_cooperated = sum(history.opponent_actions[-2, :]) / n_players >= 0.5
                cooperate_prob = majority_cooperated * 1 + (not majority_cooperated) * 0
        else:
            avg_cooperation_rate = np.mean(self.cooperation_rates)
            cooperate_prob = avg_cooperation_rate > 0.5 * 0.7 + avg_cooperation_rate <= 0.5 * 0.3
            for i in range(n_players):
                opponent_deviation = self.payoffs_deviation[i]
                if opponent_deviation > 0:
                    cooperate_prob -= 0.1
                elif opponent_deviation < 0:
                    cooperate_prob += 0.1
                cooperate_prob = max(0, min(cooperate_prob, 1))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        random.seed()
        return Action.C if random.random() < cooperate_prob else Action.D

    def update(self, history: PlayerHistory):
        n_players = self.game_description.n_players
        k = self.game_description.k
        round_number = history.my_actions.size - 1
        for i in range(n_players):
            opponent_cooperated = history.opponent_actions[round_number - 2, i] if round_number > 2 else True
            self.cooperation_rates[i] = (self.cooperation_rates[i] * 2 + int(opponent_cooperated)) / 3
        for i in range(n_players):
            opponent_payoff = history.opponent_payoffs[round_number - 1, i]
            expected_payoff = k * np.mean(history.my_actions[:round_number]) if round_number > 0 else 1 - Action.C + k / n_players
            self.payoffs_deviation[i] = opponent_payoff - expected_payoff

class Strategy_EXPLOITATIVE_567(BaseStrategy):
    """
    A strategic design for the N-Player Public Goods Game that is exploitative and adaptive.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_rate = None
        self.exploit_threshold = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if self.coop_rate is None:
            self.coop_rate = self.calculate_average_cooperation(history)
            self.exploit_threshold = self.calculate_exploit_threshold(self.game_description.k)
        if state.round_number >= math.ceil(5) or state.round_number * 2 >= self.game_description.n_rounds:
            if self.coop_rate > self.exploit_threshold:
                action = Action.C
            else:
                action = Action.D
            self.coop_rate = self.update_coop_rate(self.coop_rate, history.opponent_actions[-1, :])
            self.exploit_threshold = self.update_exploit_threshold(self.exploit_threshold, self.game_description.k)
        else:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        elif self.coop_rate < 0.2:
            action = Action.D
        return action

    def calculate_average_cooperation(self, history: PlayerHistory):
        """
        Calculate average cooperation rate over past rounds.
        """
        total_cooperations = np.sum(history.opponent_actions[:, :], axis=1)
        coop_rates = (total_cooperations + 1) / self.game_description.n_players
        return np.mean(coop_rates)

    def calculate_exploit_threshold(self, k: float):
        """
        Set exploitation threshold based on the game parameters and history.
        """
        if k > self.game_description.n_players * 0.7:
            return 0.4
        else:
            return 0.7

    def update_coop_rate(self, coop_rate: float, opponent_actions: NDArray[np.bool_]):
        """
        Update cooperation rate based on new information.
        """
        if sum(opponent_actions) > self.game_description.n_players / 2:
            coop_rate += 0.1
        elif sum(opponent_actions) < self.game_description.n_players / 2:
            coop_rate -= 0.1
        return max(0, min(coop_rate, 1))

    def update_exploit_threshold(self, exploit_threshold: float, k: float):
        """
        Update exploitation threshold based on new information and game parameters.
        """
        if k > self.game_description.n_players * 0.7:
            exploit_threshold = max(0.4, exploit_threshold - 0.05)
        else:
            exploit_threshold = min(0.7, exploit_threshold + 0.05)
        return exploit_threshold

class Strategy_EXPLOITATIVE_569(BaseStrategy):
    """
    Exploitative strategy that prioritizes defection but incorporates conditional cooperation to maximize payoffs.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.defect_likelihood = 0.8
        self.exploration_probability = 0.2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        opponent_actions = history.opponent_actions[-1, :]
        prev_cooperators = np.sum(opponent_actions)
        n_players = self.game_description.n_players
        k = self.game_description.k
        if prev_cooperators <= n_players / 2:
            return Action.D
        elif prev_cooperators > n_players / 2 and k > n_players - 1:
            return Action.C
        else:
            return Action.D
        if state.round_number % 5 == 0:
            return random.choices([Action.C, Action.D], weights=[self.exploration_probability, 1 - self.exploration_probability])[0]

class Strategy_EXPLOITATIVE_570(BaseStrategy):
    """
    Adaptive Exploiter strategy for N-Player Public Goods Game.

    This strategy balances short-term gains with long-term adaptability by monitoring opponent behavior and adjusting cooperation probability.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_prob = 0.5
        self.threshold_high = game_description.n_players // 2
        self.threshold_low = game_description.n_players // 4
        self.rounds_to_average = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        opponent_cooperation_rate = np.mean(history.opponent_actions[-self.rounds_to_average:, :])
        self.cooperation_prob += (opponent_cooperation_rate - 0.6) * 0.1
        self.cooperation_prob = max(0, min(self.cooperation_prob, 1))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        total_cooperators_last_round = sum(history.opponent_actions[-1, :] == True)
        if total_cooperators_last_round > self.threshold_high:
            return Action.D
        elif total_cooperators_last_round < self.threshold_low:
            return Action.C
        else:
            return Action.C if random.random() < self.cooperation_prob else Action.D

class Strategy_EXPLOITATIVE_571(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and only depends on game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = 0.5

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C if random.random() < 0.5 else Action.D
        opponent_actions = history.opponent_actions[-1, :]
        own_action = history.my_actions[-1]
        num_cooperators_prev_round = sum(opponent_actions) + (1 if own_action else 0)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if num_cooperators_prev_round <= self.game_description.n_players // 2:
            return Action.D
        elif num_cooperators_prev_round > self.game_description.n_players // 2 and history.my_payoffs[-1] != np.max(history.opponent_payoffs[-1, :]):
            return Action.C
        elif not own_action and num_cooperators_prev_round > self.game_description.n_players // 2:
            return Action.D
        if num_cooperators_prev_round <= self.threshold * self.game_description.n_players:
            self.threshold -= 0.1
        elif num_cooperators_prev_round > self.threshold * self.game_description.n_players and history.my_payoffs[-1] != np.max(np.concatenate((history.my_payoffs[-1:], history.opponent_payoffs[-1, :]))):
            self.threshold += 0.1
        return Action.D if random.random() < 0.5 else Action.C

class Strategy_EXPLOITATIVE_572(BaseStrategy):
    """
    An exploitative strategy that adapts to the cooperation rate of other players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.alpha = 0.5

    def calculate_avg_coop_rate(self, history: PlayerHistory) -> float:
        coop_count = sum((1 for action in history.opponent_actions[-1, :] if action))
        avg_coop_rate = coop_count / (self.game_description.n_players - 1)
        return avg_coop_rate

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            self.alpha = 0.5
            return Action.D
        avg_coop_rate_prev_round = self.calculate_avg_coop_rate(history)
        if avg_coop_rate_prev_round > self.alpha:
            action = Action.C
        else:
            action = Action.D
        if avg_coop_rate_prev_round > 0.5:
            self.alpha += 0.05
        elif avg_coop_rate_prev_round < 0.5:
            self.alpha -= 0.05
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_574(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors in the N-Player Public Goods Game.
    It combines elements of cooperation and defection to maximize its payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.n_players / 2
        self.punishment_list = []
        self.exploitation_targets = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperators = np.sum(history.opponent_actions[-1, :])
        opponent_defectors = self.game_description.n_players - 1 - opponent_cooperators
        if opponent_cooperators > self.cooperation_threshold:
            action = Action.C
        else:
            action = Action.D
        for i in range(self.game_description.n_players - 1):
            if history.opponent_actions[-1, i] == False and history.opponent_payoffs[-1, i] > np.mean(history.my_payoffs):
                self.punishment_list.append(i)
                action = Action.D
        for i in range(self.game_description.n_players - 1):
            if all((history.opponent_actions[j, i] == True for j in range(state.round_number))):
                self.exploitation_targets.append(i)
                action = Action.D
        recent_cooperation_rate = np.mean(np.sum(history.opponent_actions[-5:, :], axis=1)) / (self.game_description.n_players - 1)
        self.cooperation_threshold = self.game_description.n_players * recent_cooperation_rate / 2
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if opponent_cooperators == self.cooperation_threshold:
            action = Action.C
        return action

class Strategy_EXPLOITATIVE_575(BaseStrategy):
    """
    Exploitative strategy that adapts to the game's dynamics by identifying opportunities to defect and minimize losses when cooperating.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.THRESHOLD_COOP = 0.6
        self.PUNISHMENT_ROUNDS = 2

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        opponent_coop_rates = []
        for i in range(self.game_description.n_players - 1):
            coop_rate = sum(history.opponent_actions[max(0, state.round_number - 3):state.round_number, i]) / min(state.round_number, 3)
            opponent_coop_rates.append(coop_rate)
        avg_coop_rate = sum(opponent_coop_rates) / len(opponent_coop_rates)
        if history.my_actions[state.round_number - 1] and any((not action for action in history.opponent_actions[state.round_number - 1, :])):
            punishment_rounds_left = self.PUNISHMENT_ROUNDS - sum((1 for i in range(state.round_number - 1) if not history.my_actions[i]))
            return Action.D if punishment_rounds_left > 0 else Action.C
        if avg_coop_rate > self.THRESHOLD_COOP:
            return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_576(BaseStrategy):
    """
    The Adaptive Exploiter strategy exploits opponents by cooperating in early rounds,
    then adapting to their behavior based on the number of cooperators and the game's parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.explore_rounds = 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.explore_rounds:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        previous_cooperators = sum(history.opponent_actions[-1, :])
        if previous_cooperators < self.game_description.n_players / 2:
            return Action.D
        always_defectors = np.all(~history.opponent_actions, axis=0)
        if np.any(always_defectors):
            return Action.D
        always_cooperators = np.all(history.opponent_actions, axis=0)
        if self.game_description.k / self.game_description.n_players > 0.5 and np.any(allays_cooperators):
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_577(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to maximize individual payoffs 
    while adapting to the behaviors of other players. By estimating opponent 
    cooperation rates and adjusting actions accordingly, this strategy aims to 
    exploit situations where others are likely to cooperate or defect.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.k / game_description.n_players * (game_description.n_players / 2)
        self.estimated_opponent_cooperation_rate = 0
        self.average_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        num_cooperators = sum(history.opponent_actions[-1, :])
        self.estimated_opponent_cooperation_rate = num_cooperators / self.game_description.n_players
        my_payoffs = history.my_payoffs[:-1]
        if len(my_payoffs) > 0:
            self.average_payoff = np.mean(my_payoffs)
        current_payoff = history.my_payoffs[-1]
        if self.estimated_opponent_cooperation_rate > self.cooperation_threshold and current_payoff <= self.average_payoff:
            return Action.C
        elif self.estimated_opponent_cooperation_rate < self.cooperation_threshold or current_payoff > self.average_payoff:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_578(BaseStrategy):
    """
    A hybrid exploitative strategy that combines elements of tit-for-tat (TFT) and gradient-based strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.C_count = 0
        self.D_count = 0
        self.APC_sum = 0
        self.APD_sum = 0
        self.consecutive_defections = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or len(history.my_actions) == 0:
            return Action.C
        if state.round_number <= self.game_description.n_rounds // 2:
            opponent_action = history.opponent_actions[-1, 0]
            if opponent_action:
                return Action.C
            else:
                return Action.D
        my_last_action = history.my_actions[-1]
        if my_last_action:
            self.APC_sum += history.my_payoffs[-1]
            self.C_count += 1
        else:
            self.APD_sum += history.my_payoffs[-1]
            self.D_count += 1
        APC = self.APC_sum / max(self.C_count, 1)
        APD = self.APD_sum / max(self.D_count, 1)
        if APC > APD:
            return Action.C
        else:
            return Action.D
        opponent_last_actions = history.opponent_actions[-3:, 0]
        if not any(opponent_last_actions):
            self.consecutive_defections += 1
        else:
            self.consecutive_defections = 0
        if self.consecutive_defections >= 3:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_579(BaseStrategy):
    """
    Exploitative strategy that adapts to various opponent behaviors while prioritizing its own payoff maximization.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_prob = 0.5
        self.avg_coop_payoff = 0
        self.avg_defect_payoff = 0
        self.opponent_cooperation_count = 0
        self.recent_opponent_actions = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D]) if random.random() < self.coop_prob else Action.D
        cooperators = history.opponent_actions[-1, :] | np.array(history.my_actions)[-1]
        defectors = ~cooperators
        avg_coop_payoff = np.mean(history.opponent_payoffs[-1, :][cooperators])
        avg_defect_payoff = np.mean(history.opponent_payoffs[-1, :][defectors])
        self.opponent_cooperation_count += 1 if np.any(cooperators) else 0
        self.recent_opponent_actions.append(np.any(cooperators))
        if len(self.recent_opponent_actions) > 3:
            self.recent_opponent_actions.pop(0)
        if avg_coop_payoff > avg_defect_payoff:
            self.coop_prob += (avg_coop_payoff - avg_defect_payoff) / self.game_description.k
        else:
            self.coop_prob -= 0.1
            self.coop_prob = max(0, min(self.coop_prob, 1))
        if self.opponent_cooperation_count > state.round_number // 2:
            self.coop_prob += 0.1
        total_payoff_diff = np.sum(history.my_payoffs[history.my_actions]) - np.sum(history.my_payoffs[~history.my_actions])
        if abs(total_payoff_diff) > 10 * state.round_number:
            self.coop_prob += 0.1
        if any(self.recent_opponent_actions) and (not all(self.recent_opponent_actions)):
            return Action.D if random.random() < 0.7 else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D if random.random() < 0.9 else Action.C
        return random.choice([Action.C, Action.D]) if random.random() < self.coop_prob else Action.D

class Strategy_EXPLOITATIVE_580(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to balance exploiting others' cooperativeness with maintaining a reasonable level of cooperation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.threshold = game_description.k / game_description.n_players * (game_description.n_players - 1)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        cooperators = sum(history.opponent_actions[-1, :])
        if cooperators == self.threshold:
            return Action.D
        if cooperators > self.threshold:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_583(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.mode = 'Initial Cooperation'
        self.history = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        self.history = history if history is not None else self.history
        my_actions = np.where(self.history.my_actions, True, False)
        opponent_actions = np.where(self.history.opponent_actions, True, False)
        if self.mode == 'Initial Cooperation' and state.round_number > 0 and (sum(opponent_actions[-1, :]) < self.game_description.n_players / 2):
            self.mode = 'Exploitation Mode'
        if self.mode == 'Exploitation Mode':
            my_payoff = self.history.my_payoffs[-1]
            avg_payoff = np.mean(self.history.opponent_payoffs[-1, :])
            if sum(my_actions) < 2:
                if random.random() < 0.3 and sum(opponent_actions[-1, :]) >= self.game_description.n_players / 2:
                    self.mode = 'Cooperation Mode'
            if my_payoff < avg_payoff or (my_payoff == avg_payoff and random.random() < 0.5):
                return Action.D
            else:
                return Action.C
        return Action.C if random.random() < 0.5 else Action.D

class Strategy_EXPLOITATIVE_584(BaseStrategy):
    """
    A strategy that adapts to the game's history and exploits opponents' behavior while being robust to various opponent strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        elif state.round_number < self.game_description.n_rounds // 2:
            total_cooperators = sum(history.opponent_actions[-1, :]) + history.my_actions[-1]
            if total_cooperators > self.game_description.n_players / 2:
                return Action.D
            else:
                return Action.C
        elif state.round_number < 3 * self.game_description.n_rounds // 4:
            opponent_cooperation_rate = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
            if opponent_cooperation_rate > 0.5:
                return Action.D
            else:
                return Action.C
        elif state.round_number < self.game_description.n_rounds:
            payoff_from_cooperating = self.game_description.k / self.game_description.n_players * sum(history.opponent_actions[-1, :])
            expected_payoff_from_defecting = 1 - payoff_from_cooperating
            if payoff_from_cooperating > expected_payoff_from_defecting:
                return Action.C
            else:
                return Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_585(BaseStrategy):
    """
    The K-Adaptive strategy aims to maximize individual payoff by adaptively exploiting the opponent's behavior 
    while considering the game parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperators = []
        self.defectors = []
        self.k_threshold = game_description.k / game_description.n_players * (game_description.n_players - 1)
        self.cooperation_rate_history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        opponent_cooperations = sum(opponent_actions)
        cooperation_rate = opponent_cooperations / self.game_description.n_players
        if len(self.cooperation_rate_history) > 0 and state.round_number < self.game_description.n_rounds - 1:
            prev_cooperation_rate = self.cooperation_rate_history[-1]
            if cooperation_rate > prev_cooperation_rate:
                self.k_threshold += 0.05
            elif cooperation_rate < prev_cooperation_rate:
                self.k_threshold -= 0.05
        self.cooperation_rate_history.append(cooperation_rate)
        for i, action in enumerate(opponent_actions):
            if action and i not in self.defectors:
                self.cooperators.append(i)
            elif not action and i not in self.cooperators:
                self.defectors.append(i)
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if cooperation_rate > self.k_threshold:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_586(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.theta = 0.5
        self.previous_round_cooperators = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        self.previous_round_cooperators = sum(history.opponent_actions[-1, :])
        self.theta = self.game_description.k / self.game_description.n_players * (self.previous_round_cooperators / self.game_description.n_players)
        if self.previous_round_cooperators >= self.theta * self.game_description.n_players:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_587(BaseStrategy):
    """
    An exploitative strategy that takes advantage of other players' cooperation in a Public Goods game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponents = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.opponents is None:
            self.opponents = ['Cooperative' if action else 'Defective' for action in history.opponent_actions[-1, :]]
        if state.round_number < self.game_description.n_rounds - 1:
            cooperative_count = sum((1 for opponent in self.opponents if opponent == 'Cooperative'))
            if cooperative_count >= self.game_description.k / self.game_description.n_players * self.game_description.n_players:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_588(BaseStrategy):
    """
    This strategy balances between encouraging cooperation and exploiting opponents' contributions.
    It adapts to changes in opponent behavior by adjusting its cooperation requirements based on the observed level of public good contribution.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.c_prev = 0
        self.avg_payoff = 0
        self.delta = max(1, math.floor(self.game_description.k / 10))
        self.last_round = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if not self.last_round and state.round_number == self.game_description.n_rounds - 1:
            self.last_round = True
            return Action.D
        opponent_cooperators = sum(history.opponent_actions[-1, :])
        total_cooperators = opponent_cooperators + (history.my_actions[-1] == Action.C)
        self.c_prev = total_cooperators
        if state.round_number <= self.game_description.n_players:
            return Action.C
        m = min(self.game_description.n_rounds // 2, 5)
        recent_payoffs = history.my_payoffs[max(0, state.round_number - m):]
        if len(recent_payoffs) > 0:
            self.avg_payoff = np.mean(recent_payoffs)
        theta = self.game_description.k / self.game_description.n_players + 0.5
        if self.avg_payoff > theta and (not self.last_round):
            self.delta += 1
        condition = total_cooperators >= self.game_description.k / 2 + self.delta
        return Action.C if condition else Action.D

class Strategy_EXPLOITATIVE_589(BaseStrategy):
    """
    The Adaptive Exploiter (AE) strategy for N-Player Public Goods Game.
    
    This exploitative strategy balances cooperation and exploitation. 
    It cooperates when there are enough other cooperators, contributing to the public good,
    and exploits by defecting when there aren't enough cooperators, keeping its endowment privately.

    By monitoring opponent behavior and adapting coop_threshold, it adjusts its strategy
    to respond to changing circumstances.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_threshold = math.ceil(game_description.n_players / game_description.k)
        self.randomization_component = 0.05

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        prev_cooperators = sum(history.opponent_actions[-1, :])
        if prev_cooperators > self.game_description.n_players / 2:
            self.coop_threshold += 1
        elif prev_cooperators < self.game_description.n_players / 4:
            self.coop_threshold -= 1
        self.coop_threshold = max(1, min(self.coop_threshold, self.game_description.n_players))
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        if prev_cooperators >= self.coop_threshold or random.random() < self.randomization_component:
            return Action.C
        else:
            return Action.D

class Strategy_EXPLOITATIVE_590(BaseStrategy):
    """
    Adaptive Defector with Punishment Strategy.

    This strategy adapts to the game's progress and opponents' behaviors by adjusting its cooperation threshold 
    and punishment criteria. It exploits cooperators who consistently contribute to the public good while punishing 
    defectors who don't reciprocate. In the endgame, it prioritizes cooperation if most opponents are cooperating 
    to maximize collective payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        """
        Initialize strategy state variables.

        Args:
            game_description (PublicGoodsDescription): Game description containing n_players, n_rounds, and k.
        """
        self.game_description = game_description
        self.punishment_threshold = 0.7
        self.cooperation_threshold = 0.5
        self.opponents_behavior = [1] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        """
        Determine the action based on the current state and history.

        Args:
            state (GameState): Current game state containing round number.
            history (None | PlayerHistory): Game history for this player - None in the first round.

        Returns:
            Action: Chosen action, either COOPERATE or DEFECT.
        """
        if state.round_number == 0:
            return Action.C
        self.opponents_behavior = [1 if action else 0 for action in history.opponent_actions[-1, :]]
        avg_cooperation_rate = np.mean(self.opponents_behavior)
        if state.round_number < self.game_description.n_rounds // 3:
            self.punishment_threshold -= 0.05
        elif state.round_number > 2 * self.game_description.n_rounds // 3:
            self.punishment_threshold += 0.1
        for i in range(self.game_description.n_players):
            if self.opponents_behavior[i] < self.punishment_threshold or (avg_cooperation_rate > 0.8 and self.opponents_behavior[i] > 0.9):
                return Action.D
        if avg_cooperation_rate > self.cooperation_threshold:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_591(BaseStrategy):
    """
    A strategy that balances exploitation with adaptability.
    It initially defects, gathers information about opponents' strategies,
    and updates its cooperation probability based on previous round outcomes.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        total_cooperators = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == True)
        if history.my_payoffs[-1] > np.mean(history.opponent_payoffs[-1, :]):
            self.p = min(self.game_description.k / self.game_description.n_players, self.p + 0.1)
        else:
            self.p = max(0, self.p - 0.1)
        if total_cooperators <= self.game_description.n_players / 2:
            return Action.C if random.random() < self.p else Action.D
        return Action.D

class Strategy_EXPLOITATIVE_592(BaseStrategy):
    """
    An adaptive strategy that exploits opponent cooperation while punishing free-riding.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_cooperators_prev = 0
        self.prev_payoff = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.total_cooperators_prev = sum(history.opponent_actions[-1, :]) + (history.my_actions[-1] == Action.C)
        if history.my_actions[-1] == Action.D:
            my_contribution = 0
        else:
            my_contribution = 1
        total_contributions = self.total_cooperators_prev
        public_good_share = self.game_description.k / self.game_description.n_players * total_contributions
        private_payoff = 1 - my_contribution
        if private_payoff + public_good_share <= 1:
            return Action.D
        if self.total_cooperators_prev >= math.ceil((self.game_description.n_players + 1) / 2):
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_593(BaseStrategy):
    """
    The Adaptive Exploiter strategy is designed to take advantage of cooperative opponents 
    while minimizing the risk of being exploited. By cooperating initially and adapting 
    to the average payoff, AE encourages cooperation and gathers information about 
    opponents' behavior.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_payoff = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1])
        n_C = sum((1 for action in history.opponent_actions[-1, :] if action))
        n_D = self.game_description.n_players - 1 - n_C
        if avg_payoff > 1:
            return Action.C if n_C >= self.game_description.k / 2 else Action.D
        elif n_D >= self.game_description.n_players - self.game_description.k / 2:
            return Action.D
        else:
            return Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_594(BaseStrategy):
    """
    An exploitative strategy for the N-Player Public Goods Game.
    
    This strategy adapts to the opponent's behavior and maximizes payoff by 
    exploiting opponents with low cooperation rates and punishing defectors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_profiles = {i: {'cooperation_rate': 0.0, 'last_action': None} for i in range(1, self.game_description.n_players)}
        self.last_round_number = -1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or (history is not None and len(history.my_actions) < 2):
            return Action.C
        if history is not None:
            for i, action in enumerate(history.opponent_actions[-1]):
                opponent_index = i + 1
                self.opponent_profiles[opponent_index]['last_action'] = action
                cooperation_rate = np.mean(history.opponent_actions[:, i])
                self.opponent_profiles[opponent_index]['cooperation_rate'] = cooperation_rate
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        for i, profile in self.opponent_profiles.items():
            if profile['last_action'] is False and self.last_round_number != state.round_number - 1:
                return Action.D
            if profile['cooperation_rate'] < 0.5:
                return Action.D
        return Action.C

    def update_last_round(self, state: GameState):
        self.last_round_number = state.round_number

class Strategy_EXPLOITATIVE_595(BaseStrategy):
    """
    The Adaptive Predator strategy aims to exploit opponents' behaviors while adapting to the game's history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.round_history = []
        self.avg_cooperators_window_size = min(10, game_description.n_rounds // 2)

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        self.round_history.append(history.opponent_actions[-1])
        avg_cooperators = np.mean(self.round_history[-self.avg_cooperators_window_size:], axis=0)
        avg_total_cooperators = np.sum(avg_cooperators)
        if state.round_number < self.game_description.n_rounds // 2:
            if avg_total_cooperators >= self.game_description.n_players / 2:
                return Action.C
        opponent_behavior = np.mean(self.round_history[-self.avg_cooperators_window_size:], axis=0)
        is_free_rider = (opponent_behavior < 0.1) & (avg_total_cooperators > self.game_description.n_players / 2)
        if state.round_number >= self.game_description.n_rounds // 2:
            for i, is_fr in enumerate(is_free_rider):
                if is_fr:
                    return Action.D
            if np.all(opponent_behavior < 0.1) or np.all(opponent_behavior > 0.9):
                if opponent_behavior[0] < 0.5:
                    return Action.C
                else:
                    return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_596(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' cooperation levels.
    Cooperates initially, then defects unless the average number of cooperators exceeds a threshold.
    Punishes opponents who defect excessively.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_thresholds = np.zeros(game_description.n_rounds)
        for t in range(1, game_description.n_rounds + 1):
            self.cooperation_thresholds[t - 1] = game_description.k / game_description.n_players * (game_description.n_rounds - t + 1) / game_description.n_rounds

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_cooperators = np.sum(history.opponent_actions, axis=1)
        avg_cooperators = np.mean(opponent_cooperators + history.my_actions.astype(int))
        if history.my_payoffs is not None and len(history.my_payoffs) > 0:
            max_round_with_data = min(len(history.my_payoffs), state.round_number)
            total_cooperation_in_last_max_round = sum(history.opponent_actions[max_round_with_data - 1, :])
        else:
            total_cooperation_in_last_max_round = 0
        if avg_cooperators >= self.cooperation_thresholds[state.round_number - 1]:
            return Action.C
        elif state.round_number < self.game_description.n_rounds and total_cooperation_in_last_max_round < self.cooperation_thresholds[state.round_number - 2] and (history.my_actions[-1] == False):
            return Action.D
        elif state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        else:
            return Action.C

class Strategy_EXPLOITATIVE_597(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behaviors while maximizing individual payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.last_round_payoff_difference = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        n_players = self.game_description.n_players
        k = self.game_description.k
        cooperators_last_round = sum(history.opponent_actions[-1, :] == True)
        if cooperators_last_round >= n_players / 2:
            return Action.C
        last_round_payoffs = history.my_payoffs[-1] if len(history.my_payoffs) > 0 else 0
        opponent_defect_payoff = np.mean([payoff for payoff, action in zip(history.opponent_payoffs[-1], history.opponent_actions[-1]) if not action])
        self.last_round_payoff_difference = opponent_defect_payoff - last_round_payoffs
        if self.last_round_payoff_difference > 0 and state.round_number > 2 and (self.last_round_payoff_difference is not None):
            return Action.D
        my_last_action = history.my_actions[-1]
        opponent_defected_last_round = any([not action for action in history.opponent_actions[-1]])
        if my_last_action == True and opponent_defected_last_round:
            p = k / n_players * cooperators_last_round / n_players
            return Action.D if random.random() < p else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            total_payoff = sum(history.my_payoffs)
            expected_value = n_players * k / 2
            if total_payoff < expected_value:
                return Action.C
            else:
                return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_598(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behaviors while maximizing personal payoffs in the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.window_size = 5
        self.coop_threshold = game_description.k / game_description.n_players
        self.defect_tolerance = 2
        self.repeated_defectors = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.D
        avg_coop_rate = np.mean(history.my_actions[max(0, state.round_number - self.window_size):state.round_number])
        if avg_coop_rate > self.game_description.k / self.game_description.n_players:
            self.coop_threshold = 1
        else:
            self.coop_threshold = self.game_description.k / self.game_description.n_players
        prev_round_coop_count = sum(history.opponent_actions[state.round_number - 1, :])
        for opponent_idx, actions in enumerate(history.opponent_actions[:, :state.round_number].T):
            if not any(actions[-self.defect_tolerance:]) and opponent_idx not in self.repeated_defectors:
                self.repeated_defectors.add(opponent_idx)
        if prev_round_coop_count >= self.coop_threshold and (not self.repeated_defectors):
            return Action.C
        elif any((opponent_idx in self.repeated_defectors for opponent_idx, actions in enumerate(history.opponent_actions[:, :state.round_number].T))):
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_599(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior while maintaining a balance between cooperation and exploitation.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.n_players = game_description.n_players
        self.k = game_description.k
        self.n_rounds = game_description.n_rounds
        self.cooperation_ratios = [0] * self.n_players
        self.total_cooperation_ratio = 0
        self.cooperation_threshold = 0.7
        self.exploitation_threshold = 0.4

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= 1:
            return Action.C
        elif state.round_number == 2:
            opponents_defected = [not action for action in history.opponent_actions[-2]]
            if any(opponents_defected):
                return Action.D
            else:
                return Action.C
        for i, opponent_action in enumerate(history.opponent_actions[-1]):
            self.cooperation_ratios[i] += opponent_action
            self.cooperation_ratios[i] /= state.round_number - 1
            self.total_cooperation_ratio += self.cooperation_ratios[i]
        average_cr = self.total_cooperation_ratio / self.n_players
        if average_cr < self.exploitation_threshold:
            return Action.D if random.random() < 0.6 else Action.C
        else:
            for i, opponent_cr in enumerate(self.cooperation_ratios):
                if opponent_cr > self.cooperation_threshold:
                    return Action.C
                else:
                    return Action.D
        if state.round_number == self.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_600(BaseStrategy):
    """
    An adaptive exploitative strategy for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_coop_rate = 0.5
        self.defect_prob = 0.0
        self.coop_reduce_factor = 0.5
        self.punishment_threshold = 0.75

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if history is not None:
            self.update_avg_coop_rate(history)
            self.update_defect_prob()
            opponent_defect_rates = np.mean(~history.opponent_actions, axis=0)
            punished_opponents = opponent_defect_rates > self.punishment_threshold
            if np.any(punished_opponents):
                adjusted_coop_prob = max(0, self.avg_coop_rate - self.coop_reduce_factor)
                return Action.C if random.random() < adjusted_coop_prob else Action.D
            elif state.round_number == self.game_description.n_rounds - 1:
                return Action.D
        return Action.D if random.random() < self.defect_prob else Action.C

    def update_avg_coop_rate(self, history: PlayerHistory):
        recent_actions = history.opponent_actions[-min(5, history.my_actions.size):]
        self.avg_coop_rate = np.mean(~recent_actions)

    def update_defect_prob(self):
        if self.avg_coop_rate > 0.5:
            self.defect_prob = min(1, self.avg_coop_rate - 0.2)

class Strategy_COLLECTIVE_137(BaseStrategy):
    """
    Adaptive Cooperation with Punishment (ACP) Strategy.

    This strategy prioritizes cooperation while adapting to the evolving dynamics of the game.
    It initially cooperates, reciprocates cooperation, punishes defections, and forgives past mistakes,
    creating a positive feedback loop that encourages cooperation among players.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.history = []

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        prev_round_actions = history.opponent_actions[-1, :]
        my_prev_action = history.my_actions[-1]
        num_cooperators = sum(prev_round_actions) + int(my_prev_action)
        if num_cooperators >= self.game_description.n_players / 2:
            action = Action.C
        else:
            action = Action.D
        for i in range(self.game_description.n_players - 1):
            opponent_prev_action = prev_round_actions[i]
            if opponent_prev_action and (not my_prev_action) or (not opponent_prev_action and my_prev_action):
                action = Action.C if action == Action.D else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.C
        if num_cooperators == self.game_description.n_players / 2:
            if state.round_number % 2 == 0:
                action = Action.C
            else:
                action = Action.D
        return action

class Strategy_EXPLOITATIVE_63(BaseStrategy):
    """
    Adaptive Exploitation strategy for N-Player Public Goods Game.
    
    This strategy balances adaptability and exploitativeness by monitoring the group's overall contribution level 
    and adjusting cooperation/defection probabilities accordingly. It also adapts to opponents' behavior, 
    exploiting those who cooperate excessively while maintaining a balanced approach against others.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_contributions = None
        self.opponents_cooperation_levels = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < 0.5 else Action.D
        if self.total_contributions is None or self.opponents_cooperation_levels is None:
            self.total_contributions = history.my_actions.sum()
            self.opponents_cooperation_levels = np.zeros(self.game_description.n_players - 1)
        for i, opponent_action in enumerate(history.opponent_actions[-1, :]):
            if opponent_action:
                self.opponents_cooperation_levels[i] += 1
        self.total_contributions += history.my_actions[-1]
        T = self.total_contributions / self.game_description.n_players
        p = 0.7 if T >= self.game_description.k / 2 * self.game_description.n_players else 0.8
        for i, opponent_cooperation_level in enumerate(self.opponents_cooperation_levels):
            if opponent_cooperation_level > (state.round_number - 1) / 2:
                p -= 0.1
            elif opponent_cooperation_level < (state.round_number - 1) / 2:
                pass
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.C if random.random() < p else Action.D

class Strategy_EXPLOITATIVE_99(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' behavior and punishes excessive defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff_per_coop = 0
        self.defect_count_i = [0] * game_description.n_players

    def majority_defected(self, history: PlayerHistory) -> bool:
        opponent_actions = history.opponent_actions[-1]
        return sum(opponent_actions) < len(opponent_actions) / 2

    def calculate_avg_payoff_per_coop(self, history: PlayerHistory) -> float:
        total_cooperators = np.sum(history.my_actions[-3:] | (history.opponent_actions[-3:] == True).any(axis=1))
        avg_payoffs = np.mean(history.my_payoffs[-3:])
        return avg_payoffs / total_cooperators if total_cooperators > 0 else 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        if state.round_number <= 3:
            if state.round_number == 1:
                action = Action.C
            elif self.majority_defected(history):
                action = Action.D
            else:
                action = Action.C
        else:
            self.avg_payoff_per_coop = self.calculate_avg_payoff_per_coop(history)
            if self.avg_payoff_per_coop > self.game_description.k / self.game_description.n_players:
                action = Action.C
            else:
                action = Action.D
        opponent_actions = history.opponent_actions[-1]
        for i, action_i in enumerate(opponent_actions):
            if not action_i and action == Action.C:
                self.defect_count_i[i] += 1
            if self.defect_count_i[i] >= 2:
                action = Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            action = Action.D
        return action

class Strategy_EXPLOITATIVE_176(BaseStrategy):
    """
    Exploitative strategy that adapts to opponents' cooperation patterns.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.ACR = None
        self.previous_payoff = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        n_players = self.game_description.n_players
        k = self.game_description.k
        CT = k / n_players
        if state.round_number == 0 or state.round_number == 1:
            return Action.C
        if self.ACR is None:
            opponent_actions = history.opponent_actions[:2, :]
            self.ACR = np.mean(opponent_actions, axis=0)
        my_payoff = history.my_payoffs[-1]
        average_payoff = np.mean(history.opponent_payoffs[-1, :])
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        for opponent_ACR in self.ACR:
            if opponent_ACR > CT:
                return Action.D
        if my_payoff < average_payoff and (self.previous_payoff is None or my_payoff < self.previous_payoff):
            return Action.C
        else:
            return Action.D
        if history.my_actions[-1] == True and any((not action for action in history.opponent_actions[-1, :])):
            self.previous_payoff = my_payoff
            return Action.D
        return Action.C

class Strategy_EXPLOITATIVE_291(BaseStrategy):
    """
    Exploitative strategy: "Adaptive Raider"
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_profiles = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        if self.opponent_profiles is None:
            self.opponent_profiles = {i: [0, 0] for i in range(self.game_description.n_players)}
        for player, profile in self.opponent_profiles.items():
            if history is not None:
                profile[0] += int(history.opponent_actions[-1, player - 1])
                profile[1] += 1
        exploitation_threshold = self.game_description.k / self.game_description.n_players
        opponent_cooperation_rates = {player: profile[0] / profile[1] for player, profile in self.opponent_profiles.items()}
        if any((opponent_cooperation_rates[player] > exploitation_threshold for player in range(self.game_description.n_players))):
            action = Action.D
        else:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return action

class Strategy_EXPLOITATIVE_304(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors in the N-Player Public Goods Game.
    It uses a combination of observation, adaptation, and opportunistic exploitation to maximize its total payoff.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.window_size = 5
        self.punishment_probability = 0.8

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return random.choice([Action.C, Action.D])
        if state.round_number < 3:
            return random.choice([Action.C, Action.D]) if random.random() < 0.5 else Action.D
        opponent_cooperations = np.sum(history.opponent_actions[-self.window_size:], axis=0)
        avg_cooperation_rate = np.mean(opponent_cooperations / self.window_size)
        if avg_cooperation_rate > 0.5:
            return Action.D
        else:
            p = 1 - self.game_description.k / self.game_description.n_players * (avg_cooperation_rate - 0.5)
            return Action.C if random.random() < p else Action.D
        for i, opponent_action in enumerate(history.opponent_actions[-1]):
            if opponent_action == False and history.my_actions[-1] == True:
                return Action.D if random.random() < self.punishment_probability else Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D
        return Action.D

class Strategy_EXPLOITATIVE_346(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors in the N-Player Public Goods Game.
    It adapts to the game's history and parameters, making it robust against various strategies.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.avg_payoff = 0
        self.coop_prob = 0.5
        self.opponents_coop = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.coop_prob else Action.D
        prev_avg_payoff = self.avg_payoff
        opponent_cooperations = history.opponent_actions[-1, :]
        self.opponents_coop = [coop + 1 if action else coop for coop, action in zip(self.opponents_coop, opponent_cooperations)]
        total_contributions = sum((1 for x in opponent_cooperations if x)) + (history.my_actions[-1] == Action.C)
        self.avg_payoff = np.mean([1 - int(action == Action.D) + self.game_description.k / self.game_description.n_players * total_contributions for action, _ in zip(history.opponent_actions[-1, :], range(self.game_description.n_players))])
        if state.round_number == self.game_description.n_rounds:
            return Action.D
        if self.avg_payoff > 1 + self.game_description.k / self.game_description.n_players * (self.game_description.n_players - 1):
            action = Action.C
        else:
            action = Action.D
        if math.isclose(self.avg_payoff, 1 + self.game_description.k / self.game_description.n_players * (self.game_description.n_players - 1)):
            action = Action.C if random.random() < self.coop_prob else Action.D
        return action

class Strategy_EXPLOITATIVE_359(BaseStrategy):
    """
    The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperation of others while minimizing its own contributions to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_prob = 1 / game_description.k
        self.avg_cooperators = None
        self.free_riders = []
        self.punishment_period = 2
        self.round_count = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperation_prob else Action.D
        self.update_avg_cooperators(history)
        self.identify_free_riders(history)
        if self.avg_cooperators > self.game_description.n_players / 2:
            return Action.C if random.random() < self.cooperation_prob else Action.D
        else:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if self.avg_cooperators > self.game_description.n_players / 2 else Action.D

    def update_avg_cooperators(self, history: PlayerHistory):
        opponent_actions = np.array(history.opponent_actions)
        avg_cooperators = (opponent_actions.sum(axis=1) + 1).mean()
        self.round_count += 1
        if self.avg_cooperators is None:
            self.avg_cooperators = avg_cooperators
        else:
            self.avg_cooperators = (self.avg_cooperators * (self.round_count - 1) + avg_cooperators) / self.round_count

    def identify_free_riders(self, history: PlayerHistory):
        opponent_payoffs = np.array(history.opponent_payoffs)
        avg_payoff = opponent_payoffs.mean()
        for i in range(opponent_payoffs.shape[1]):
            if (opponent_payoffs[:, i] < avg_payoff).mean() > 0.5 and i not in self.free_riders:
                self.free_riders.append(i)

    def defect_against_free_rider(self, opponent_id: int):
        return Action.D

class Strategy_EXPLOITATIVE_433(BaseStrategy):
    """
    Exploitative Strategy: "Adaptive Predator"
    
    Decision Rules:
    1. **Initial Round**: Defect (D) in the first round to gather information about opponents' behaviors.
    2. **Observation Phase**: For rounds 2 to r/2, cooperate (C) if the majority of opponents cooperated in the previous round, and defect (D) otherwise.
    3. **Exploitation Phase**: After the observation phase, switch to a more exploitative strategy:
        * If an opponent has consistently cooperated (C) for at least 75% of the rounds in the observation phase, defect (D) against them.
        * If an opponent has consistently defected (D) or shown mixed behavior, cooperate (C) with a probability proportional to their cooperation rate in the observation phase.
    4. **Last Round**: Defect (D) in the last round to maximize payoffs, regardless of opponents' previous behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.observation_phase_cooperation_rates = None

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        if state.round_number <= self.game_description.n_rounds // 2:
            majority_cooperated = sum(history.opponent_actions[-1, :]) > self.game_description.n_players / 2
            return Action.C if majority_cooperated else Action.D
        if self.observation_phase_cooperation_rates is None:
            observation_rounds = history.my_actions[:self.game_description.n_rounds // 2]
            self.observation_phase_cooperation_rates = np.mean(observation_rounds[:, np.newaxis] == history.opponent_actions[:self.game_description.n_rounds // 2, :], axis=0)
        opponent_behaviors = np.where(self.observation_phase_cooperation_rates >= 0.75, 'consistent_C', np.where(self.observation_phase_cooperation_rates < 0.25, 'consistent_D', 'mixed'))
        for opponent in range(self.game_description.n_players):
            if opponent_behaviors[opponent] == 'consistent_C':
                return Action.D
            elif opponent_behaviors[opponent] != 'consistent_C':
                cooperate_prob = self.observation_phase_cooperation_rates[opponent]
                return Action.C if random.random() < cooperate_prob else Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_453(BaseStrategy):
    """
    The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to changing game conditions.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.CR = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < self.game_description.n_players:
            if state.round_number == 0:
                return Action.C
            else:
                if history is not None and np.any(history.opponent_actions[-1, :]):
                    return Action.D
                return Action.C
        if history is not None:
            self.CR = [CR + (opponent_action == 1) for CR, opponent_action in zip(self.CR, history.opponent_actions[-1, :])]
        MCO = np.argmax(self.CR)
        alpha = 1 - self.game_description.k / self.game_description.n_players * ((state.round_number + 1) / self.game_description.n_rounds)
        action = Action.C if self.CR[MCO] > alpha else Action.D
        if state.round_number >= self.game_description.n_rounds // 2:
            if history is not None and np.all(history.opponent_actions[-1, :]):
                action = Action.C
        return action

class Strategy_COLLECTIVE_516(BaseStrategy):
    """
    A collective strategy that balances individual self-interest with collective welfare by adapting to the game's history and parameters.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.ρ = 0
        self.τ = game_description.k / (2 * game_description.n_players)
        self.δ = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number <= self.game_description.n_players:
            return Action.C
        total_contributions = np.sum(history.my_actions)
        for opponent_history in history.opponent_actions.T:
            total_contributions += np.sum(opponent_history)
        self.ρ = total_contributions / (state.round_number * self.game_description.n_players)
        for i, opponent_history in enumerate(history.opponent_actions.T):
            self.δ[i] = 0 if opponent_history[-1] else self.δ[i] + 1
        if self.ρ >= self.τ:
            cooperate_against = [defection > self.game_description.n_players // 2 for defection in self.δ]
            return Action.D if any(cooperate_against) else Action.C
        return Action.D

    def update_average_contribution_rate(self, t: int, ρ: float):
        total_contributions = np.sum(history.my_actions)
        for opponent_history in history.opponent_actions.T:
            total_contributions += np.sum(opponent_history)
        return total_contributions / (t * self.game_description.n_players)

    def update_consecutive_defections(self, t: int, δ: list):
        for i, opponent_history in enumerate(history.opponent_actions.T):
            δ[i] = 0 if opponent_history[-1] else δ[i] + 1
        return δ

class Strategy_COLLECTIVE_541(BaseStrategy):
    """
    Collective Strategy: "Adaptive Cooperation with Reputation Feedback"
    
    This strategy aims to balance individual payoff maximization with collective welfare, 
    adapting to the evolving game environment and opponent behaviors.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate = 0.5
        self.opponent_behavior = [[0, 0] for _ in range(game_description.n_players)]

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        my_actions = np.where(history.my_actions, True, False)
        opponent_cooperations = np.count_nonzero(history.opponent_actions, axis=1)
        self.cooperation_rate = np.mean(my_actions + opponent_cooperations[-1] / (self.game_description.n_players - 1))
        if self.cooperation_rate > 0.5:
            return Action.C
        for i in range(self.game_description.n_players):
            if i == history.my_payoffs.size:
                continue
            cooperation_count = np.count_nonzero(history.opponent_actions[:, i])
            defection_count = state.round_number - cooperation_count
            self.opponent_behavior[i] = [cooperation_count, defection_count]
        adapt_factor = 1
        for opponent in range(self.game_description.n_players):
            if opponent == history.my_payoffs.size:
                continue
            if self.opponent_behavior[opponent][0] > self.opponent_behavior[opponent][1]:
                adapt_factor += (self.opponent_behavior[opponent][0] - self.opponent_behavior[opponent][1]) / state.round_number
        prev_payoffs = history.my_payoffs[:-1]
        if len(prev_payoffs) > 0:
            avg_payoff = np.mean(prev_payoffs)
            total_cooperators = np.count_nonzero(my_actions + opponent_cooperations[-2])
            payoff = self.game_description.k / self.game_description.n_players * total_cooperators
            if state.round_number == 1:
                return Action.C
            elif avg_payoff >= payoff:
                return Action.D
            else:
                return Action.C
        return Action.D

class Strategy_COLLECTIVE_543(BaseStrategy):
    """
    Adaptive Cooperation (AC) Strategy.

    This strategy initially cooperates with a probability of 50% and then adapts based on reciprocity, punishment,
    and adaptive adjustment.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperate_prob = 0.5
        self.prev_round_coops = 0
        self.prev_round_defs = 0

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.cooperate_prob else Action.D
        prev_round_coops = sum(history.opponent_actions[-1, :])
        prev_round_defs = self.game_description.n_players - prev_round_coops - 1
        if prev_round_coops >= prev_round_defs:
            return Action.C
        elif prev_round_coops < prev_round_defs:
            return Action.D if random.random() < 0.5 else Action.C
        coop_ratio = history.my_payoffs[-1] / np.mean(history.opponent_payoffs[-1, :])
        if coop_ratio > 1:
            self.cooperate_prob += 0.1
        elif coop_ratio <= 1:
            self.cooperate_prob -= 0.1
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C if prev_round_coops >= prev_round_defs else Action.D
        self.prev_round_coops = prev_round_coops
        self.prev_round_defs = prev_round_defs

class Strategy_COLLECTIVE_566(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) strategy for the N-Player Public Goods Game.
    This strategy adapts to various opponent behaviors and relies solely on game parameters and history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.total_payoff = 0
        self.history = []
        self.punishment_targets = set()

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions_last_round = history.opponent_actions[-1, :]
        my_action_last_round = history.my_actions[-1]
        my_payoff_last_round = history.my_payoffs[-1]
        avg_payoff_last_round = np.mean(np.concatenate((history.my_payoffs[-1:], history.opponent_payoffs[-1, :])))
        if my_action_last_round and (not all(opponent_actions_last_round)):
            self.punishment_targets.add(np.where(~opponent_actions_last_round)[0][0])
        exploration = state.round_number % 5 == 0
        if len([x for x in opponent_actions_last_round if x]) >= self.game_description.n_players // 2 + 1 and my_payoff_last_round < avg_payoff_last_round or (any(self.punishment_targets) and state.round_number % 2 == (list(self.punishment_targets)[0] + 1) % 2):
            action = Action.C
        elif exploration:
            action = random.choice([Action.C, Action.D])
        else:
            action = Action.D
        self.history.append(action)
        return action

class Strategy_COLLECTIVE_577(BaseStrategy):
    """
    Adaptive Collective Optimism (ACO) strategy for the N-Player Public Goods Game.
    
    This collective strategy prioritizes cooperation when it benefits the group as a whole,
    adapting to the group's behavior and adjusting its own actions accordingly.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.p_initial = 0.5
        self.theta_initial = 1

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C if random.random() < self.p_initial else Action.D
        pi_avg = np.mean(history.my_payoffs[-1] + history.opponent_payoffs[-1])
        p_coop = 0.7 if pi_avg > 1 else 0.3
        c_opponent = sum(history.opponent_actions[-1, :]) / self.game_description.n_players
        p_coop *= 0.8 if c_opponent > 0.5 else 0.2
        theta_t = 1 - state.round_number / self.game_description.n_rounds
        if history.my_payoffs[-1] < theta_t:
            return Action.D
        if state.round_number == self.game_description.n_rounds - 1 and pi_avg > 1:
            return Action.C
        return Action.C if random.random() < p_coop else Action.D

class Strategy_EXPLOITATIVE_544(BaseStrategy):
    """
    An exploitative strategy that adapts to opponents' behavior and punishes excessive defection.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_rate = 0.0
        self.consecutive_defections = [0] * game_description.n_players
        self.exploitation_mode = False

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0:
            return Action.C
        opponent_actions = history.opponent_actions[-1, :]
        self.coop_rate = np.mean(opponent_actions)
        for i, action in enumerate(opponent_actions):
            if not action:
                self.consecutive_defections[i] += 1
            else:
                self.consecutive_defections[i] = 0
        for opponent, defections in enumerate(self.consecutive_defections):
            if defections > 2:
                return Action.D
        if any((defection > 2 for defection in self.consecutive_defections)):
            self.exploitation_mode = True
        if self.coop_rate >= 0.5:
            return Action.C
        else:
            expected_coop_payoff = 1 - 1 + self.game_description.k / self.game_description.n_players * self.game_description.n_players
            expected_defect_payoff = 1 - 0 + self.game_description.k / self.game_description.n_players * np.sum(opponent_actions)
            if expected_coop_payoff > expected_defect_payoff:
                return Action.C
            else:
                return Action.D
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.D

class Strategy_EXPLOITATIVE_552(BaseStrategy):
    """
    An adaptive free-rider strategy in an N-player public goods game.
    The strategy balances cooperation and defection based on opponents' behavior,
    aiming to maximize individual payoffs while minimizing contributions to the public good.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_threshold = game_description.n_players / 3

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.D
        avg_cooperators = np.mean(history.opponent_actions[:state.round_number])
        if 2 <= state.round_number < self.game_description.n_rounds / 2:
            if avg_cooperators < self.cooperation_threshold:
                p_c = (1 + self.game_description.k / self.game_description.n_players * avg_cooperators) / (self.game_description.n_players + self.game_description.k / self.game_description.n_players * state.round_number)
                return Action.C if random.random() < p_c else Action.D
            else:
                return Action.D
        elif self.game_description.n_rounds / 2 <= state.round_number < self.game_description.n_rounds:
            if np.sum(history.opponent_actions[-3:]) > np.sum(history.opponent_actions[-6:-3]):
                return Action.D
            else:
                p_c = (1 + self.game_description.k / self.game_description.n_players * avg_cooperators) / (self.game_description.n_players + self.game_description.k / self.game_description.n_players * state.round_number)
                return Action.C if random.random() < p_c else Action.D
        else:
            return Action.D

class Strategy_EXPLOITATIVE_562(BaseStrategy):
    """
    The Adaptive Exploiter is a history-dependent, exploitative strategy designed for the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.opponent_cooperation_rates = [0.0] * (game_description.n_players - 1)
        self.recent_payoffs = []
        self.threshold_low = 0.4
        self.threshold_high = 0.7

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number < 2:
            return Action.C
        for i in range(self.game_description.n_players - 1):
            self.opponent_cooperation_rates[i] = (self.opponent_cooperation_rates[i] * (state.round_number - 2) + history.opponent_actions[-1, i]) / (state.round_number - 1)
        self.recent_payoffs.append(history.my_payoffs[-1])
        if len(self.recent_payoffs) > 2:
            payoff_comparison = [payoff < np.median([p for p in self.recent_payoffs[-3:]]) for payoff in self.recent_payoffs[-3:]]
            if any(payoff_comparison):
                return Action.C
        opponent_cooperation_rate_avg = sum(self.opponent_cooperation_rates) / (self.game_description.n_players - 1)
        if opponent_cooperation_rate_avg > self.threshold_low:
            return Action.C
        elif opponent_cooperation_rate_avg < self.threshold_high and history.my_actions[-1] != True:
            return Action.D
        if opponent_cooperation_rate_avg > self.threshold_high:
            return Action.D
        if any((self.opponent_cooperation_rates[i] < 0.3 for i in range(self.game_description.n_players - 1) if self.opponent_cooperation_rates[i] > 0.4)):
            return Action.C
        return Action.C

class Strategy_EXPLOITATIVE_568(BaseStrategy):
    """
    Exploits a wide range of opponent behaviors while adapting to the game's history.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.coop_count = [0] * game_description.n_players
        self.defect_count = [0] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        opponent_actions = history.opponent_actions[-1]
        for i, action in enumerate(opponent_actions):
            if action:
                self.coop_count[i] += 1
                self.defect_count[i] = 0
            else:
                self.coop_count[i] = 0
                self.defect_count[i] += 1
        majority_cooperated = sum(self.coop_count) > self.game_description.n_players / 2
        if majority_cooperated:
            return Action.D
        for i, coop_history in enumerate(self.coop_count):
            if coop_history >= 2 and self.defect_count[i] == 0:
                return Action.D
        for i, defect_history in enumerate(self.defect_count):
            if defect_history == 1 and self.coop_count[i] > 1:
                return Action.C
        last_round = state.round_number + 1 == self.game_description.n_rounds
        if last_round:
            return Action.D
        tiebreaker = sum(self.coop_count) == self.game_description.n_players / 2
        if tiebreaker:
            return Action.C
        return Action.D

class Strategy_EXPLOITATIVE_573(BaseStrategy):
    """
    Exploitative strategy designed to adapt and maximize payoff in the N-Player Public Goods Game.
    """

    def __init__(self, game_description: PublicGoodsDescription):
        self.game_description = game_description
        self.cooperation_rate_opp = [0.5] * game_description.n_players
        self.last_round_defection = [False] * game_description.n_players

    def __call__(self, state: GameState, history: None | PlayerHistory) -> Action:
        if state.round_number == 0 or history is None:
            return Action.C
        avg_coop_rate_all = np.mean(history.opponent_actions[-1])
        opponent_cooperations = history.opponent_actions.shape[1]
        self.cooperation_rate_opp = [(coop + rate * state.round_number) / (state.round_number + 1) if coop else rate * state.round_number / (state.round_number + 1) for coop, rate in zip(history.opponent_actions[-1], self.cooperation_rate_opp)]
        for i in range(opponent_cooperations):
            opponent_defected = history.opponent_actions[-1, i] is False
            if self.last_round_defection[i]:
                if random.random() < 0.8:
                    return Action.D
            opponent_cooperation_rate = self.cooperation_rate_opp[i]
            if opponent_cooperation_rate > 0.7 and state.round_number < self.game_description.n_rounds // 2:
                return Action.D
        if avg_coop_rate_all > 0.5:
            action = Action.D
        else:
            action = Action.C
        if state.round_number == self.game_description.n_rounds - 1:
            return Action.C
        self.last_round_defection = [coop is False for coop in history.opponent_actions[-1]]
        return action
