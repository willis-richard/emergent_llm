"""
Strategy descriptions for public_goods.

Generated with:
- Provider: Client
- Model: llama3.1:70b
"""


description_COLLECTIVE_1 = '''
**Collective Strategy: Adaptive Cooperative Optimism (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 100% to establish a baseline for cooperation and encourage others to follow.
2. **Reciprocity-Based Adaptation**: For rounds t > 1, calculate the average number of cooperators in the previous round (t-1). If this value is above a certain threshold θ (e.g., n/3), cooperate with probability p = (k/n) × (average_cooperators / n). Otherwise, defect.
3. **Punishment Mechanism**: If the average payoff for all players in the previous round is below 0.5 (i.e., many defectors), switch to a punishing strategy: defect for one round and then return to the reciprocity-based adaptation rule.

**Handling Edge Cases:**

* Last Round: In the final round, always cooperate if the game has been cooperative overall (average_cooperators > n/2). Otherwise, defect.
* First Few Rounds: If fewer than 3 players cooperated in the first two rounds, switch to a "learning phase" where you randomly choose between C and D with equal probability for the next 2-3 rounds. This allows exploration of opponent strategies.

**Collective Mindset Alignment:**

ACO aims to create an environment conducive to cooperation while being robust against defectors. By initially cooperating, we encourage others to do so as well. The reciprocity-based adaptation rule responds positively to cooperative behaviors and negatively to defections. The punishment mechanism discourages exploitation by temporarily punishing low-average-payoff rounds.

**Pseudocode:**
```markdown
function ACO(n, k, r)
  // Initialize variables
  average_cooperators = 0
  previous_payoffs = []

  for t = 1 to r do
    if t == 1 then
      // Initial cooperation
      action = C
    else
      // Reciprocity-based adaptation
      average_cooperators = calculate_average_cooperators(previous_round)
      if average_cooperators > θ then
        p = (k/n) × (average_cooperators / n)
        action = random_choice(C, D, p)
      else
        action = D

      // Punishment mechanism
      if average_payoff(previous_round) < 0.5 then
        action = D
        punishment_activated = true
        punishment_rounds = 1
      end if

      // Last round adjustment
      if t == r then
        if game_has_been_cooperative() then
          action = C
        else
          action = D
        end if
      end if
    end if

    // Update variables
    previous_payoffs.append(calculate_payoff(action, average_cooperators))
    average_cooperators = update_average_cooperators(previous_round)

    // Play the action
    play_action(action)
  end for
end function

function calculate_average_cooperators(round)
  // Calculate average number of cooperators in round
  return sum(cooperator_actions) / n
end function

function game_has_been_cooperative()
  // Check if game has been cooperative overall
  return average_cooperators > n/2
end function

function update_average_cooperators(round)
  // Update average number of cooperators
  return (average_cooperators × (round - 1) + calculate_average_cooperators(round)) / round
end function
```
Note: The pseudocode is a simplified representation of the strategy, and some variables or functions might be omitted for clarity.
'''

description_COLLECTIVE_2 = '''
**Collective Strategy: Adaptive Cooperation with Public Goods Focus (ACPG)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Public Goods Focused**: In subsequent rounds, calculate the average public goods contribution from the previous round (`avg_pub_goods_prev`). If `avg_pub_goods_prev` is above the threshold `(k/n) * 0.5`, cooperate (C). Otherwise, defect (D).
3. **Adaptive Response**: Monitor the number of cooperators in the previous round (`coop_count_prev`). If `coop_count_prev` is increasing or remains stable, continue to cooperate (C). If `coop_count_prev` decreases, switch to defection (D) for one round to signal dissatisfaction.
4. **Last Round Exception**: In the final round, defect (D) regardless of previous actions to maximize individual payoff.

**Pseudocode:**
```markdown
# Initialize variables
avg_pub_goods_prev = 0
coop_count_prev = 0

# First round
if t == 1:
    action = C

# Subsequent rounds
else:
    # Calculate average public goods contribution from previous round
    avg_pub_goods_prev = (k/n) * Σ(j=1 to n) c_j,t-1
    
    # Public Goods Focused decision rule
    if avg_pub_goods_prev > (k/n) * 0.5:
        action = C
    else:
        action = D

    # Adaptive Response decision rule
    coop_count_prev = Σ(j=1 to n) c_j,t-1
    if coop_count_prev >= coop_count_prev,t-1 and action == C:
        action = C
    elif coop_count_prev < coop_count_prev,t-1 and action == C:
        action = D

# Last round exception
if t == r:
    action = D
```
**Collective Mindset:**

ACPG prioritizes the public goods aspect of the game, encouraging cooperation when the collective contribution is sufficient. By adapting to changes in cooperation levels and responding to decreases in cooperative behavior, ACPG promotes a stable and mutually beneficial environment.

**Robustness:**

1. **Initial Cooperation**: Establishes a cooperative tone and encourages others to follow.
2. **Public Goods Focus**: Ensures that the collective contribution is prioritized, promoting cooperation when it benefits the group.
3. **Adaptive Response**: Adjusts behavior in response to changes in cooperation levels, maintaining a balance between individual and collective interests.

**Edge Cases:**

1. **First Round**: Cooperate to set a cooperative tone.
2. **Last Round**: Defect to maximize individual payoff.
3. **No Opponent Cooperation**: Defect to avoid contributing to an unresponsive group.

ACPG is designed to be robust against various opponent behaviors, including unconditional cooperation or defection, tit-for-tat strategies, and other adaptive approaches. By prioritizing the public goods aspect and adapting to changes in cooperation levels, ACPG promotes a collective mindset while protecting individual interests.
'''

description_COLLECTIVE_3 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and depends solely on game parameters and history:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 0.8. This initial optimism encourages cooperation and sets a positive tone for the game.
2. **Reactive Adaptation**: For subsequent rounds (t > 1), calculate the average cooperation rate (ACR) among all players in the previous round:

   ACR = (Σ(j=1 to n) c_j,t-1) / n

   If ACR ≥ 0.5, cooperate (C). Otherwise, defect (D).
3. **Punishment Mechanism**: Implement a mild punishment mechanism to discourage repeated defection. If a player i observes that another player j has defected in the previous round (c_j,t-1 = 0), decrease the cooperation probability for player j by 10% in the current round.

   This mechanism is designed to be gentle, allowing players to recover from mistakes and adapt to changing opponent behaviors.
4. **Endgame Cooperation**: In the last round (t = r), cooperate (C) if the total payoff for cooperating is higher than or equal to the total payoff for defecting, considering all previous rounds:

   Σ(t=1 to r-1) π_i,t + π_i,r (if C) ≥ Σ(t=1 to r-1) π_i,t + π_i,r (if D)

**Edge Cases:**

* In case of a tie in the last round (i.e., equal payoffs for cooperating and defecting), cooperate (C).
* If there are multiple optimal actions, choose cooperation (C) by default.

**Collective Mindset:**
ACO aligns with a collective mindset by:

* Encouraging initial cooperation to set a positive tone
* Adapting to the group's cooperation rate to maintain social cohesion
* Implementing a mild punishment mechanism to discourage repeated defection
* Prioritizing cooperation in the endgame when it leads to higher total payoffs

**Pseudocode:**
```python
def ACO(n, k, r):
  # Initialize variables
  acr = 0.8  # Initial cooperation rate
  punish_prob = 0.1  # Punishment probability decrease

  for t in range(1, r+1):
    if t == 1:
      cooperate = random.random() < acr
    else:
      prev_acr = (sum(c_j for c_j in prev_round_actions)) / n
      cooperate = prev_acr >= 0.5

      # Punishment mechanism
      for j, action in enumerate(prev_round_actions):
        if action == 0 and random.random() < punish_prob:
          cooperate[j] *= 0.9

    # Endgame cooperation
    if t == r:
      total_coop_payoff = sum(pi_i,t for pi_i,t in coop_payoffs)
      total_defect_payoff = sum(pi_i,t for pi_i,t in defect_payoffs)
      cooperate = total_coop_payoff >= total_defect_payoff

    # Return cooperation decision
    return cooperate
```
This strategy balances individual and collective interests, adapting to various opponent behaviors while promoting cooperation throughout the game.
'''

description_COLLECTIVE_4 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

The ACP strategy aims to balance individual self-interest with collective welfare by adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Post-Initial Rounds**:
	* If the average cooperation rate among all players in the previous round is above a certain threshold (`θ`), cooperate (C). This encourages continued cooperation when it's working.
	* Otherwise, defect (D) with a probability proportional to the difference between the average cooperation rate and `θ`. This introduces a "punishment" mechanism for non-cooperative behavior.
3. **Punishment Mechanism**: If a player defects in a round where the average cooperation rate is below `θ`, increase the likelihood of defection in subsequent rounds by a small amount (`ε`). This creates a gradual response to persistent non-cooperation.

**Threshold Value (θ)**: Set `θ` to a value slightly above 0.5, e.g., `θ = 0.55`. This allows for some flexibility while encouraging cooperation.

**Punishment Intensity (ε)**: Choose a small positive value for `ε`, e.g., `ε = 0.05`. This ensures gradual adaptation without over-reacting to short-term fluctuations.

**Edge Cases**:

1. **Last Round**: Cooperate (C) in the final round, regardless of previous outcomes, to maximize collective welfare and encourage cooperation.
2. **Ties in Average Cooperation Rate**: In cases where the average cooperation rate equals `θ`, cooperate (C) with a probability of 0.5.

**Collective Mindset Alignment**:

ACP prioritizes cooperation while introducing a punishment mechanism to deter persistent non-cooperation. This approach promotes collective welfare by encouraging players to work together, while also adapting to changing circumstances and opponent behaviors.

Pseudocode:
```python
def ACP(n, k, r, θ=0.55, ε=0.05):
    # Initialize cooperation rate and punishment intensity
    avg_coop_rate = 0
    punish_intensity = 0

    for round in range(r):
        if round == 0:  # Initial Round
            action = C
        else:
            avg_coop_rate = calculate_avg_coop_rate(prev_round)
            if avg_coop_rate >= θ:
                action = C
            else:
                punish_intensity += ε * (θ - avg_coop_rate)
                action = D with probability punish_intensity

        # Take action and update game state
        take_action(action)
        update_game_state()

    return total_payoff
```
This ACP strategy balances cooperation and punishment to promote collective welfare, while adapting to the game's history and opponent behaviors.
'''

description_COLLECTIVE_5 = '''
**Collective Strategy: Adaptive Reciprocity with Public Goods (ARPG)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds:**
	* If the total number of cooperators in the previous round is greater than or equal to `n/2`, cooperate (C).
	* Otherwise, defect (D).

Pseudocode:
```python
def ARPG(n, k, history):
    if len(history) == 0:  # First Round
        return 'C'
    
    prev_coops = sum(1 for action in history[-1] if action == 'C')
    if prev_coops >= n / 2:
        return 'C'
    else:
        return 'D'
```
**Edge Cases:**

* **Last Round:** Follow the same decision rules as above. The strategy's adaptability ensures that it responds to the current state of cooperation, even in the final round.
* **Ties:** If `n/2` is not an integer, use the floor value (e.g., for `n=5`, use `2` instead of `2.5`).
* **No Previous Round Data:** In case of missing data or a new player entering the game, default to cooperating (C) in the first round and then follow the standard decision rules.

**Collective Mindset:**

The ARPG strategy is designed to promote cooperation while being robust against various opponent behaviors. By cooperating when the majority cooperates, it encourages others to do the same, creating a positive feedback loop. When faced with low cooperation levels, it defects to avoid exploitation and signal that cooperation is necessary for mutual benefit.

**Rationale:**

1. **Reciprocity:** The strategy reciprocates cooperation when the majority cooperates, promoting a collective mindset.
2. **Adaptability:** ARPG adapts to changes in cooperation levels, allowing it to respond effectively to different opponent behaviors.
3. **Robustness:** By not relying on specific coordination mechanisms or norms, ARPG remains robust against various opponents and game scenarios.

The ARPG strategy is designed to perform well in a tournament setting, where strategies compete against each other without prior knowledge of their opponents' behaviors. Its adaptability and reciprocity features make it an effective collective strategy for the N-Player Public Goods Game.
'''

description_COLLECTIVE_6 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Overview**

Adaptive Cooperation (AC) is a collective strategy designed for the N-Player Public Goods Game. AC aims to balance individual self-interest with collective welfare by adapting to the evolving behavior of other players. The strategy relies solely on game parameters and history, making it robust to various opponent behaviors.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a baseline for cooperation.
2. **Contribution Threshold**: Calculate the average contribution rate (ACR) of all players over the previous rounds:

`ACR = (∑(t=1 to r-1) Σ(j=1 to n) c_j) / ((r-1) * n)`

Cooperate if `ACR >= 0.5`, otherwise Defect.
3. **Reputation-based Adaptation**: Assign a reputation score (RS) to each player based on their past cooperation rates:

`RS_i = (∑(t=1 to r-1) c_i,t) / (r-1)`

Cooperate if the average RS of all players is above 0.5, otherwise Defect.
4. **Punishment Mechanism**: If a player defects in the previous round and their RS is below 0.5, defect in the current round to punish them.

**Edge Cases**

* **Last Round**: Cooperate if the ACR is above 0.5; otherwise, Defect.
* **Early Rounds (r < 5)**: Cooperate to encourage cooperation among other players.
* **Low Participation**: If fewer than half of the players cooperate in a round, defect in the next round.

**Pseudocode**
```python
def adaptive_cooperation(n, k, r, history):
    if r == 1:
        return 'C'  # Cooperate in the first round

    ACR = calculate_average_contribution_rate(history)
    RS = calculate_reputation_scores(history)

    if ACR >= 0.5 or average(RS) > 0.5:
        return 'C'
    elif punishment_mechanism(history):
        return 'D'
    else:
        return 'D'

def calculate_average_contribution_rate(history):
    total_contributions = sum(sum(c_j for c_j in round_history) for round_history in history)
    return total_contributions / ((r-1) * n)

def calculate_reputation_scores(history):
    RS = [sum(c_i,t for t in range(r-1)) / (r-1) for c_i,t in zip(history, range(r-1))]
    return RS

def punishment_mechanism(history):
    if history[-1][i] == 'D' and RS[i] < 0.5:
        return True
    return False
```
**Collective Mindset**

Adaptive Cooperation (AC) prioritizes collective welfare by:

* Encouraging cooperation in early rounds to establish a baseline.
* Adapting to the evolving behavior of other players through reputation scores and contribution rates.
* Punishing defectors to maintain cooperation incentives.

By balancing individual self-interest with collective goals, AC aims to achieve a higher total payoff for all players while remaining robust to various opponent behaviors.
'''

description_COLLECTIVE_7 = '''
I propose a collective strategy called "Adaptive Collective Optimism" (ACO) for the N-Player Public Goods Game. ACO is designed to balance individual self-interest with the collective goal of maximizing overall payoffs.

**Decision Rules:**

1. **Initial Exploration**: In the first round, play Cooperate (C) to gather information about the opponents' behavior and encourage cooperation.
2. **Contribution Matching**: For rounds 2 to r-1:
	* If the total number of cooperators in the previous round is greater than or equal to n/2, play C.
	* Otherwise, play Defect (D).
3. **Adaptive Adjustment**: After each round, adjust the cooperation threshold based on the game's history:
	* If the average payoff for all players in the previous round is above a certain threshold (e.g., 1.5), decrease the cooperation threshold by 0.1.
	* If the average payoff is below the threshold, increase the cooperation threshold by 0.1.
4. **Last Round**: In the final round (r), play D to maximize individual payoff.

**Pseudocode:**
```
// Initialize variables
cooperation_threshold = n/2
average_payoff_threshold = 1.5

// First round exploration
if round == 1:
    action = C

// Contribution matching and adaptive adjustment
for round in range(2, r):
    if total_cooperators >= cooperation_threshold:
        action = C
    else:
        action = D
    
    // Adjust cooperation threshold based on average payoff
    if average_payoff > average_payoff_threshold:
        cooperation_threshold -= 0.1
    elif average_payoff < average_payoff_threshold:
        cooperation_threshold += 0.1

// Last round defection
if round == r:
    action = D
```
**Collective Mindset:**

ACO aligns with the collective mindset by:

* Encouraging cooperation through initial exploration and contribution matching.
* Adapting to the game's history to optimize overall payoffs.
* Gradually adjusting the cooperation threshold to balance individual self-interest with collective goals.

**Edge Cases:**

* First round: Initial exploration encourages cooperation and gathers information about opponents' behavior.
* Last round: Defection maximizes individual payoff, as there are no future rounds to consider.
* Ties in total cooperators: ACO uses a simple tie-breaking rule (e.g., cooperate if the number of cooperators is exactly n/2).

ACO's adaptive nature and focus on collective optimization make it a robust strategy for the N-Player Public Goods Game, capable of handling a wide range of opponent behaviors.
'''

description_COLLECTIVE_8 = '''
I propose a collective strategy called "Adaptive Collective Cooperation" (ACC). ACC is designed to balance individual payoff maximization with collective welfare, while being robust to various opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Reciprocal Cooperation**: In subsequent rounds, cooperate if at least k/n players cooperated in the previous round. This ensures that cooperation is sustained when enough players contribute to the public good.
3. **Punish Defection**: If fewer than k/n players cooperated in the previous round, defect (D) in the current round. This discourages free-riding and encourages others to cooperate.
4. **Exploit Greed**: If all players defected in the previous round, cooperate in the current round if k > 2. This takes advantage of the increased payoff from cooperation when the multiplication factor is high.

**Edge Cases:**

1. **Last Round**: In the final round (r), always defect to maximize individual payoff, as there are no future rounds to affect.
2. **Tie-Breaking**: If exactly k/n players cooperated in the previous round, randomly choose between cooperation and defection with equal probability.

**Collective Mindset:**

ACC is designed to promote collective cooperation while being robust to various opponent behaviors. By initially cooperating and reciprocating cooperation, ACC encourages others to contribute to the public good. The punishment mechanism discourages free-riding, while exploiting greed in certain situations ensures that ACC players can benefit from cooperation even when others are not.

**Pseudocode:**
```
function ACC(player_id, round, history) {
  if (round == 1) { // Initial Round
    return COOPERATE;
  }

  previous_cooperators = count Cooperators in history[round-1];
  if (previous_cooperators >= k/n) { // Reciprocal Cooperation
    return COOPERATE;
  } else if (previous_cooperators < k/n && all players defected in history[round-1] && k > 2) { // Exploit Greed
    return COOPERATE;
  } else { // Punish Defection
    return DEFECT;
  }

  if (round == r) { // Last Round
    return DEFECT;
  }
}
```
This strategy will be implemented as an algorithm and compete against other independent strategies in the tournament.
'''

description_COLLECTIVE_9 = '''
To design a collective strategy for the N-Player Public Goods Game that is adaptive and robust, I propose a dynamic strategy based on monitoring the group's cooperation level and adjusting individual behavior accordingly. This approach ensures alignment with the collective mindset while making decisions solely based on game parameters and history.

**Strategy Name:** Adaptive Collective Conscience (ACC)

### Decision Rules

1. **Initial Round (t=1):**
   - Cooperate (C) to initiate a cooperative environment and encourage other players to follow suit. This sets a positive precedent for the game.

2. **Subsequent Rounds (t > 1):**
   - Calculate the average cooperation rate (ACR) of all players over the previous rounds.
     ```
     ACR = Σ(t=1 to current_round-1, c_j / n) / (current_round - 1)
     ```
   - If ACR ≥ k/n, Cooperate (C). This condition checks if the historical cooperation rate is at least as high as the multiplication factor divided by the number of players. Cooperating in this scenario encourages further cooperation and leverages the group's investment.
   - Otherwise, Defect (D) with a probability P_D = 1 - ACR. This introduces a degree of responsiveness to low cooperation levels without abruptly switching strategies.

3. **Last Round (t=r):**
   - Calculate the total payoff for all players if everyone cooperates in this last round and compare it with the expected payoff from defecting.
     ```
     Total_Coop_Payoff = k * n
     Expected_Defect_Payoff = 1 + (k/n) * Σ(j=1 to n-1) c_j
     ```
   - If Total_Coop_Payoff > Expected_Defect_Payoff, Cooperate (C). This rule promotes cooperation in the final round if it maximizes total payoffs.

### Handling Edge Cases

- **First Round:** Always Cooperate as described.
- **Last Round:** Use the specific decision rule provided for this case to maximize collective payoff.
- **Ties or Specific Scenarios:** In scenarios where calculations lead to equal expected outcomes, default to Cooperation (C) to maintain a proactive stance.

### Collective Mindset Alignment

The ACC strategy is designed with the collective mindset in mind. By basing decisions on the historical cooperation rate and adjusting behavior accordingly, it aims to encourage high levels of cooperation within the group, thus maximizing collective payoffs over time. The strategy's adaptability allows it to respond effectively to a variety of opponent behaviors, promoting stability and cooperation in the game.

### Pseudocode Summary

```
Function ACC_Strategy(current_round, n, k, previous_actions):
  if current_round == 1:
    return C
  else:
    ACR = Calculate_Average_Cooperation_Rate(previous_actions)
    if ACR >= k/n:
      return C
    else:
      P_D = 1 - ACR
      if Random() < P_D:
        return D
      else:
        return C

Function Last_Round_Adjustment(current_round, r, n, k, previous_actions):
  if current_round == r:
    Total_Coop_Payoff = Calculate_Total_Cooperation_Payoff(n, k)
    Expected_Defect_Payoff = Calculate_Expected_Defect_Payoff(previous_actions, n, k)
    if Total_Coop_Payoff > Expected_Defect_Payoff:
      return C
  else:
    return ACC_Strategy(current_round, n, k, previous_actions)
```

This strategy combines elements of cooperation initiation, adaptive response to group behavior, and payoff maximization in the final round. By dynamically adjusting its cooperative stance based on observed behavior, it aims to achieve a high level of collective payoffs while being robust against various strategies from other players.
'''

description_COLLECTIVE_10 = '''
I propose a collective strategy called "Adaptive Collective Optimism" (ACO). ACO aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to establish a cooperative tone.
2. **Reciprocal Altruism**: If the average cooperation rate in the previous round is above 0.5, cooperate (play C). Otherwise, defect (play D).
3. **Punishment and Forgiveness**: If you defected in the previous round and the average cooperation rate was below 0.5, cooperate (play C) to "make amends." If you cooperated and the average cooperation rate was above 0.5, continue cooperating.
4. **Exploration**: With a small probability (ε = 0.1), randomly choose an action (C or D) to gather information about opponents' strategies.

**Edge Cases:**

* **Last Round**: In the final round, cooperate (play C) regardless of previous actions or outcomes, as there's no future benefit to defecting.
* **Early Rounds**: If n is large and r is small, prioritize cooperation in early rounds to establish a collective norm. As the game progresses, adjust the strategy according to the decision rules above.

**Collective Mindset:**

ACO prioritizes collective well-being by:

1. Encouraging initial cooperation to set a positive tone.
2. Reciprocating altruism when opponents cooperate.
3. Punishing and forgiving to maintain social norms.
4. Exploring the strategy space to adapt to diverse opponent behaviors.

**Pseudocode:**

```markdown
function AdaptiveCollectiveOptimism(n, k, r):
  // Initialize variables
  avg_cooperation = 0.5
  prev_action = C
  exploration_prob = 0.1

  for t in range(1, r+1):
    if t == 1:
      action = C  // Initial cooperation
    else:
      // Reciprocal altruism and punishment/forgiveness
      if avg_cooperation > 0.5:
        action = C
      elif prev_action == D and avg_cooperation < 0.5:
        action = C  // Make amends
      else:
        action = D

    // Exploration
    if random(0, 1) < exploration_prob:
      action = random(C, D)

    // Update variables
    prev_action = action
    avg_cooperation = (avg_cooperation * (t-1) + sumcoop(t)) / t

    return action
```

ACO is designed to be adaptive and robust in the face of various opponent strategies. By balancing individual self-interest with collective well-being, ACO aims to achieve a high total game payoff while promoting cooperation among players.
'''

description_COLLECTIVE_11 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation strategy is designed to balance individual payoffs with collective well-being while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage cooperation and maximize potential collective payoff.
2. **Subsequent Rounds**: Calculate the average cooperation rate of all players in the previous round, excluding yourself:

`avg_coop_prev_round = Σ(j=1 to n, j ≠ i) c_j / (n - 1)`

3. **Cooperation Threshold**:
If `avg_coop_prev_round ≥ k/n`, cooperate (C). This threshold ensures that cooperation is encouraged when the collective benefit is high enough.
4. **Defection Trigger**: If `avg_coop_prev_round < k/n` and you cooperated in the previous round, defect (D) in this round.

Pseudocode:
```python
def adaptive_cooperation(n, k, history):
    if current_round == 1:  # Initial Round
        return 'C'
    
    avg_coop_prev_round = calculate_avg_coop_prev_round(history)
    
    if avg_coop_prev_round >= k/n:
        return 'C'  # Cooperate
    elif avg_coop_prev_round < k/n and history[-1][i] == 'C':
        return 'D'  # Defect after cooperating in previous round
    
    # Default to defecting if no conditions met
    return 'D'
```
**Edge Cases:**

* **Last Round**: Cooperate (C) to ensure the highest possible collective payoff.
* **Ties**: In case of a tie in the cooperation rate calculation, cooperate (C).
* **Opponent Deviation**: If an opponent deviates from their previous action, re-evaluate the cooperation threshold and adjust your decision accordingly.

**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes collective well-being while considering individual payoffs. By adapting to the average cooperation rate of other players, it promotes a balanced approach that benefits both the individual and the group. This strategy is robust against various opponent behaviors and can lead to more cooperative outcomes in the N-Player Public Goods Game.

This strategy will be implemented as an algorithm for the tournament, where its performance will be evaluated against other independent strategies developed by other AI systems.
'''

description_COLLECTIVE_12 = '''
**Collective Strategy: "Adaptive Reciprocal Cooperation" (ARC)**

The ARC strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Rounds:** Cooperate (C) in the first two rounds to establish a baseline of cooperation and encourage others to reciprocate.
2. **Reciprocal Cooperation:** In subsequent rounds, cooperate if the average number of cooperators in the previous round is greater than or equal to `n/2`, where `n` is the number of players. This encourages maintaining cooperation when it's prevalent.
3. **Adaptive Defection:** If the average number of cooperators in the previous round is less than `n/2`, defect (D) with a probability proportional to the number of defectors in the previous round (`p_defect = num_defectors / n`). This adapts to an increasingly competitive environment by increasing the likelihood of defection.
4. **Trigger Strategy:** If a player has cooperated and received a payoff less than or equal to `1` (i.e., they were exploited), switch to defecting for one round as a "punishment" mechanism.

**Handling Edge Cases:**

* In the last round (`r = r_max`), cooperate if the average number of cooperators in the previous round is greater than or equal to `n/2`, otherwise defect. This ensures that cooperation is not exploited in the final round.
* If there's only one player, always cooperate, as defection would yield zero payoff.

**Collective Mindset:**

The ARC strategy aligns with a collective mindset by:

1. Encouraging initial cooperation to establish a cooperative baseline.
2. Adapting to the game's history and opponent behaviors, rather than relying on fixed patterns or schedules.
3. Implementing a reciprocal cooperation mechanism that promotes maintaining cooperation when it's prevalent.

**Pseudocode:**

```
def ARC(n, r, k):
  # Initialize variables
  coop_avg = 0
  defect_prob = 0

  for round in range(r):
    if round < 2:
      # Initial rounds: Cooperate
      action = C
    else:
      # Calculate average number of cooperators in previous round
      coop_avg = (coop_avg * (round - 1) + num_cooperators[round - 1]) / round

      # Reciprocal cooperation
      if coop_avg >= n/2:
        action = C
      else:
        # Adaptive defection
        defect_prob = num_defectors[round - 1] / n
        if random() < defect_prob:
          action = D
        else:
          action = C

    # Trigger strategy: Punish exploitation
    if coop_avg > 0 and payoff <= 1:
      action = D

    # Last round adjustment
    if round == r - 1:
      if coop_avg >= n/2:
        action = C
      else:
        action = D

  return action
```

This strategy is designed to be adaptive, robust, and aligned with a collective mindset, making it suitable for a tournament against independent AI strategies.
'''

description_COLLECTIVE_13 = '''
**Collective Strategy: Adaptive Cooperation with Grudges (ACG)**

**Overview**
ACG is a collective strategy that balances individual self-interest with cooperation, adapting to the game's history and opponents' behaviors. It uses a simple yet effective approach to encourage cooperation while minimizing exploitation.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and gather information about opponents.
2. **Payoff-Based Cooperation**: In subsequent rounds, cooperate if the average payoff of cooperators in the previous round is greater than or equal to the average payoff of defectors. This encourages cooperation when it's beneficial for the group.
3. **Grudge Mechanism**: If a player defects while others cooperate (i.e., "free-ride"), ACG will defect against that player for a fixed number of rounds (`grudge_rounds`). This discourages exploitation and promotes reciprocity.
4. **Forgiveness**: After `grudge_rounds`, ACG reverts to the payoff-based cooperation rule, giving opponents a chance to redeem themselves.

**Pseudocode**
```python
def adaptive_cooperation_with_grudges(n, k, r):
    # Initialize variables
    grudge_players = set()
    grudge_rounds = 2  # adjustable parameter

    for round in range(1, r + 1):
        if round == 1:
            action = C  # Cooperate in the first round
        else:
            prev_coop_payoff_avg = average_payoff_of_cooperators(round - 1)
            prev_defect_payoff_avg = average_payoff_of_defectors(round - 1)

            if prev_coop_payoff_avg >= prev_defect_payoff_avg:
                action = C  # Cooperate if cooperation was beneficial
            else:
                action = D  # Defect otherwise

        # Check for free-riders and apply grudge mechanism
        if action == C and any(other_player_action == D for other_player in range(1, n + 1)):
            grudge_players.add(defecting_player)
            action = D  # Defect against the free-rider

        # Apply forgiveness after grudge rounds
        if len(grudge_players) > 0 and round - max_grudge_round > grudge_rounds:
            grudge_players.clear()

        # Record actions and payoffs for next round's calculations
        record_action(action)
        record_payoff(calculate_payoff(n, k, action))
```
**Edge Cases**

* In the last round, ACG will defect to maximize individual payoff, as there are no future rounds to influence.
* If all players cooperate or defect simultaneously, ACG will adapt accordingly, using the payoff-based cooperation rule.

**Collective Mindset**
ACG prioritizes collective well-being by cooperating when it benefits the group and discouraging exploitation through the grudge mechanism. By adapting to opponents' behaviors and game history, ACG promotes a balanced approach that balances individual self-interest with cooperation.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it:

1. Encourages cooperation when beneficial for the group.
2. Discourages exploitation through the grudge mechanism.
3. Adapts to opponents' behaviors and game history.
4. Prioritizes collective well-being while considering individual self-interest.

Feel free to ask if you'd like me to elaborate on any aspect of this strategy!
'''

description_COLLECTIVE_14 = '''
**Collective Strategy: "Adaptive Cooperation with Gradual Punishment" (ACGP)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to reciprocate.
2. **Consecutive Cooperation:** If all players cooperated in the previous round, continue to cooperate (C). This reinforces the collective benefit of cooperation.
3. **Partial Defection:** If some players defected in the previous round, but the total number of cooperators is still above a threshold (n/2), cooperate (C) with a probability proportional to the number of cooperators in the previous round. This encourages continued cooperation while allowing for some flexibility.
4. **Widespread Defection:** If most players defected in the previous round (i.e., fewer than n/2 cooperated), defect (D) with a high probability (e.g., 0.8). This punishes widespread defection and discourages further exploitation.
5. **Gradual Punishment:** After punishing widespread defection, gradually increase the cooperation probability over the next few rounds if the number of cooperators increases. This allows for recovery from periods of low cooperation.

**Edge Cases:**

1. **Last Round:** Defect (D) in the last round to maximize individual payoff, as there are no future rounds to consider.
2. **Tiebreaker:** In cases where the decision rules yield a tie (e.g., equal probabilities for cooperation and defection), cooperate (C) to maintain a cooperative bias.

**Pseudocode:**
```
// Initialize variables
int num_cooperators_prev = 0;
bool last_round_punished = false;

for round t from 1 to r:
    // Determine number of cooperators in previous round
    num_cooperators_prev = countcoop(round t-1);

    if (round == 1) { // Initial Round
        cooperate();
    } else if (num_cooperators_prev == n) { // Consecutive Cooperation
        cooperate();
    } else if (num_cooperators_prev >= n/2) { // Partial Defection
        coop_prob = num_cooperators_prev / n;
        if (random() < coop_prob) {
            cooperate();
        } else {
            defect();
        }
    } else if (!last_round_punished) { // Widespread Defection
        last_round_punished = true;
        defect();
    } else { // Gradual Punishment
        grad_ punish_coop_prob = (num_cooperators_prev / n)^2; // Increase cooperation probability gradually
        if (random() < grad_punish_coop_prob) {
            cooperate();
        } else {
            defect();
        }
    }

    // Update variables for next round
    last_round_punished = false;
```
**Collective Mindset:**

ACGP is designed to balance individual self-interest with collective cooperation. By adapting to the game's history and punishing widespread defection, ACGP encourages players to cooperate while allowing for some flexibility in case of partial defection. The gradual punishment mechanism enables recovery from periods of low cooperation, promoting a more cooperative environment.

In the tournament against independent strategies, ACGP will initially establish a cooperative tone and then adapt to the opponents' behaviors. By doing so, it aims to achieve a high collective payoff while minimizing individual losses due to exploitation.
'''

description_COLLECTIVE_15 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation (AC) strategy is designed to balance individual self-interest with collective well-being, adapting to various opponent behaviors while promoting cooperation.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds:** Observe the total number of cooperators (TC) in the previous round. If TC ≥ n/2, cooperate (C). Otherwise, defect (D).
3. **Punishment Mechanism:** If the average payoff per player in the previous round is less than the expected payoff from universal cooperation (k), defect (D) in the next round.
4. **Forgiveness Mechanism:** After punishing (defecting), switch back to cooperating if the total number of cooperators in the previous round increases.

**Pseudocode:**
```python
def adaptive_cooperation(n, k, r):
    # Initialize variables
    TC = 0  # Total cooperators in previous round
    avg_payoff = 0  # Average payoff per player in previous round
    punish = False

    for t in range(1, r+1):
        if t == 1:  # Initial round
            action = 'C'
        else:
            if TC >= n/2 and not punish:
                action = 'C'
            else:
                action = 'D'

        # Observe opponent actions and calculate payoffs
        # ... (implementation details omitted)

        # Update variables for next round
        TC = count_cooperators(opponent_actions)
        avg_payoff = calculate_average_payoff(payoffs)

        if avg_payoff < k and not punish:
            punish = True
        elif TC > previous_TC and punish:
            punish = False

    return action
```
**Rationale:**

1. **Initial Cooperation:** By cooperating in the first round, AC sets a positive tone and encourages others to cooperate.
2. **Majority Rule:** Cooperating when most players cooperated in the previous round helps maintain a high level of cooperation.
3. **Punishment Mechanism:** Defecting when average payoffs are low incentivizes opponents to cooperate, as they will receive higher payoffs if everyone cooperates.
4. **Forgiveness Mechanism:** Switching back to cooperating after punishing allows AC to adapt to changing opponent behaviors and encourages cooperation.

**Collective Mindset:**

AC prioritizes collective well-being by:

1. Encouraging initial cooperation
2. Adapting to the majority's behavior
3. Punishing low payoffs to incentivize cooperation
4. Forgiving and reverting to cooperation when opponents adapt

By balancing individual self-interest with collective goals, AC aims to achieve a high level of cooperation in the N-player public goods game.
'''

description_COLLECTIVE_16 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and game parameters:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability p = k/n. This initial cooperation rate is based on the multiplication factor k and the number of players n.
2. **Observation and Adaptation**: After the first round, observe the total number of cooperators in the previous round (Σc_j). Calculate the average payoff of cooperators (π_avg_c) and defectors (π_avg_d) in the previous round.
3. **Cooperation Threshold**: Determine a cooperation threshold (τ) based on the game parameters: τ = k/n \* (n-1)/(n-k).
4. **Adaptive Cooperation**: Cooperate (play C) if:
	* π_avg_c > π_avg_d and Σc_j ≥ τ
	* OR, in the last round, cooperate to avoid a mutual defection outcome (i.e., all players playing D)
5. **Defection Punishment**: If a player defects (plays D) when the cooperation threshold is met or exceeded (Σc_j ≥ τ), decrease the probability of cooperating with that player by 20% in the next round.

**Edge Cases:**

* **First Round**: Cooperate with probability p = k/n.
* **Last Round**: Cooperate to avoid a mutual defection outcome, unless the opponent's past behavior suggests they will defect regardless.
* **Ties**: In case of a tie between π_avg_c and π_avg_d, cooperate if Σc_j ≥ τ.

**Collective Mindset:**

The ACO strategy is designed to promote collective cooperation while being robust against various opponent behaviors. By observing the game's history and adapting to the cooperation rate, ACO aims to maximize the total payoff for all players.

**Pseudocode (simplified):**
```
// Initialize variables
k = multiplication factor
n = number of players
r = number of rounds
tau = k/n \* (n-1)/(n-k)
p_coop = k/n // initial cooperation probability

// First round
if (round == 1) {
    play C with probability p_coop
}

// Subsequent rounds
else {
    observe Σc_j and calculate π_avg_c, π_avg_d
    
    if (π_avg_c > π_avg_d && Σc_j >= tau) {
        play C
    } else if (last round) {
        play C to avoid mutual defection
    } else {
        // Defection punishment
        decrease p_coop by 20% for players who defected when Σc_j ≥ tau
        play C with updated probability
    }
}
```
This strategy balances cooperation and punishment, aiming to create a stable collective outcome that maximizes the total payoff.
'''

description_COLLECTIVE_17 = '''
To design a collective strategy for this N-Player Public Goods Game, I propose the "Adaptive Cooperate-to-Threaten" (ACT) strategy.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C).
2. **Reciprocity**: If at least half of the players cooperated in the previous round, cooperate in the current round.
3. **Punishment**: If less than half of the players cooperated in the previous round, defect (play D) with a probability proportional to the number of defectors in the previous round.

Formally, let `C_t` be the number of cooperators in round `t`, and `n` be the total number of players. The decision rule for player `i` in round `t+1` is:

- If `t == 0` (first round), cooperate (play C).
- Else if `C_t >= n/2`, cooperate (play C).
- Else, defect with probability `p = (n - C_t) / n`.

**Edge Cases:**

* **Last Round**: In the last round (`r`), cooperate (play C) regardless of previous rounds' outcomes.
* **Ties**: If exactly half of the players cooperated in a round, the strategy defaults to cooperation.

**Collective Mindset:**

The ACT strategy aims to balance individual self-interest with collective well-being. By initially cooperating and reciprocating cooperation from others, the strategy fosters a cooperative environment. The punishment mechanism helps maintain cooperation by deterring free-riding behavior.

When facing opponents who are unwilling to cooperate, the ACT strategy adapts by increasing its defection rate, while still providing opportunities for opponents to adjust their behavior and re-establish cooperation.

**Pseudocode:**
```python
def act_strategy(history):
    if len(history) == 0:  # First round
        return 'C'
    
    C_t = sum(1 for action in history[-1] if action == 'C')
    n = len(history[-1])
    
    if C_t >= n/2:
        return 'C'
    else:
        p = (n - C_t) / n
        return 'D' if random.random() < p else 'C'

def act_strategy_last_round(history):
    return 'C'
```
Note that this pseudocode is a simplified representation of the strategy, and actual implementation may require additional considerations, such as handling empty histories or edge cases not explicitly mentioned here.
'''

description_COLLECTIVE_18 = '''
Here is a collective strategy for the N-Player Public Goods Game that meets the requirements:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability 0.8, and defect (play D) with probability 0.2. This initial cooperation encourages other players to contribute and establishes a baseline for future interactions.
2. **Contribution-based Adaptation**: After the first round, calculate the average contribution rate (ACR) of all players in the previous round. ACR is defined as the total number of cooperators divided by the total number of players.

   If ACR ≥ 0.5, cooperate (play C) with probability (k/n) × ACR. This rewards high contribution rates and encourages others to follow suit.
   
   If ACR < 0.5, defect (play D) with probability 1 - ((k/n) × ACR). This penalizes low contribution rates and motivates players to adapt.

3. **Punishment Mechanism**: To deter persistent defectors, introduce a punishment mechanism. Track the number of consecutive rounds where a player defects while others cooperate (i.e., ACR > 0.5). If this count exceeds a threshold (e.g., 2-3 rounds), temporarily defect against that specific player for one round.

**Edge Case Handling:**

1. **Last Round**: In the final round, play C if the average payoff from cooperation over all previous rounds is higher than the average payoff from defection; otherwise, play D.
2. **All Players Defecting**: If all players have defected in the previous round (ACR = 0), cooperate with probability 0.5 to encourage a restart of cooperative behavior.

**Collective Mindset:**

The ACO strategy aligns with the collective mindset by:

1. Encouraging initial cooperation to establish a baseline for mutual benefit.
2. Adapting to the average contribution rate, rewarding high contributors and penalizing low contributors.
3. Implementing a punishment mechanism to deter persistent defectors.

**Pseudocode:**
```markdown
# Initialize variables
acr = 0.5  # Average Contribution Rate (initial value)
consecutive_defections = {}  # Track consecutive defections per player

# First round
if current_round == 1:
    play C with probability 0.8, D with probability 0.2

# Subsequent rounds
else:
    acr = total_contributors / num_players
    
    if acr >= 0.5:
        # Cooperate based on contribution rate
        cooperate_probability = (k/n) * acr
        play C with probability cooperate_probability, D otherwise
    else:
        # Defect based on low contribution rate
        defect_probability = 1 - ((k/n) * acr)
        play D with probability defect_probability, C otherwise
    
    # Punishment mechanism
    for player in players:
        if player_defected_last_round and acr > 0.5:
            consecutive_defections[player] += 1
            if consecutive_defections[player] >= threshold:
                temporarily_defect_against_player = True

# Last round handling
if current_round == r:
    average_cooperation_payoff = sum(cooperative_payoffs) / (r - 1)
    average_defection_payoff = sum(defective_payoffs) / (r - 1)
    
    if average_cooperation_payoff > average_defection_payoff:
        play C
    else:
        play D

# All players defecting handling
if all_players_defected_last_round:
    cooperate_probability = 0.5
    play C with probability cooperate_probability, D otherwise
```
This strategy is designed to be adaptive and robust to various opponent behaviors while maintaining a collective mindset that encourages cooperation and mutual benefit.
'''

description_COLLECTIVE_19 = '''
I propose a collective strategy called "Adaptive Collective Cooperation" (ACC) that balances individual self-interest with collective well-being. ACC is designed to be robust and adaptive to various opponent behaviors.

**Decision Rules:**

1. **Initial Rounds:** In the first few rounds (e.g., 3-5), play Cooperate (C) to establish a cooperative tone and encourage others to do the same.
2. **Cooperation Threshold:** Calculate the cooperation threshold (CT) as follows:

CT = (k / n) \* (number of previous rounds with ≥ 50% cooperators)

If CT > 0.5, play C; otherwise, play Defect (D).
3. **Reciprocity:** Monitor the actions of other players in the previous round. If a player i defected while you played C, and their payoff was higher than yours, play D against that player in the next round.
4. **Punishment:** Identify "free riders" who consistently defect while others cooperate. Play D against these players for 2-3 consecutive rounds to punish them.
5. **Exploration:** Every 5-10 rounds, randomly play D (with a probability of 0.1-0.3) to test the cooperation level of other players and adjust CT accordingly.

**Edge Cases:**

* **Last Round:** Play C if CT > 0.5; otherwise, play D.
* **Ties in Cooperation Threshold:** In case of ties, play C if the previous round had more cooperators than defectors; otherwise, play D.

**Collective Mindset Alignment:**

ACC prioritizes collective cooperation while protecting individual interests. By initially cooperating and adapting to the group's behavior, ACC encourages others to cooperate as well. The reciprocity and punishment mechanisms ensure that free riders are discouraged from exploiting cooperative players.

**Pseudocode:**
```
function ACC(n, k, r):
  CT = 0
  previous_round_actions = []
  for round in range(1, r+1):
    if round <= 3:
      play C
    else:
      CT = calculate_cooperation_threshold(previous_round_actions)
      if CT > 0.5:
        play C
      else:
        play D
      # Reciprocity and punishment logic here
      # Exploration (randomly play D) every 5-10 rounds
      previous_round_actions.append(get_previous_round_actions())
```
This strategy should perform well in a tournament setting, as it balances individual self-interest with collective cooperation while adapting to various opponent behaviors.
'''

description_COLLECTIVE_20 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5.
2. **Cooperation Threshold**: Set a cooperation threshold `τ` based on the number of players (n) and the multiplication factor (k): `τ = k / n`. This threshold represents the minimum share of public good required for cooperation to be beneficial.
3. **Contribution Assessment**: After each round, assess the total contributions (`Σc_j`) made by all players in that round.
4. **Cooperation Decision**:
	* If the total contributions exceed the cooperation threshold (`Σc_j > τ`), cooperate (C) in the next round.
	* Otherwise, defect (D) in the next round.
5. **Punishment Mechanism**: If a player observes that their payoff is lower than the average payoff of all players in the previous round, defect (D) in the next round to "punish" the group for not contributing enough.

**Edge Cases:**

1. **Last Round**: In the last round, cooperate (C) if the total contributions in the previous round exceeded the cooperation threshold.
2. **Tiebreaker**: If the total contributions are exactly equal to the cooperation threshold, cooperate (C) with probability 0.5.
3. **Early Defection**: If a player observes that another player has defected (D) in the first round, defect (D) in the next round to avoid exploitation.

**Collective Mindset:**

1. **Shared Goal**: The collective goal is to maximize the total payoff of all players over the entire game.
2. **Reciprocal Altruism**: Players cooperate if they believe others will also contribute to the public good, creating a mutually beneficial environment.
3. **Adaptive Learning**: The strategy adapts to changing circumstances, such as shifts in opponent behavior or changes in the total contributions.

**Pseudocode:**
```python
def ACO(n, k, r):
    τ = k / n  # cooperation threshold
    cooperate_prob = 0.5  # initial cooperation probability

    for t in range(r):
        if t == 0:
            action = random.choice([C, D])  # initial random choice
        else:
            total_contributions = sum(c_j for c_j in previous_round_actions)
            if total_contributions > τ:
                action = C  # cooperate if threshold exceeded
            elif total_contributions < τ and my_payoff < average_payoff:
                action = D  # punish group if payoff is low
            else:
                action = random.choice([C, D])  # random choice otherwise

        previous_round_actions.append(action)
        my_payoff = calculate_payoff(n, k, action, total_contributions)

    return total_payoff
```
This strategy combines elements of reciprocity, altruism, and adaptive learning to create a collective mindset that promotes cooperation while being robust to various opponent behaviors.
'''

description_COLLECTIVE_21 = '''
I propose a collective strategy called "Adaptive Reciprocity with Group Benefit" (ARG). This strategy balances individual self-interest with the need for cooperation to achieve the group's benefit.

**Decision Rules:**

1. **First Round**: Cooperate (C) in the first round to signal willingness to cooperate and encourage others to do so.
2. **Subsequent Rounds**: Use a combination of reciprocity and group benefit considerations:
	* If the number of cooperators in the previous round is above a threshold (n/2), cooperate (C).
	* Otherwise, defect (D) if the payoff from cooperation in the previous round was below a certain threshold (k/n \* n/2). This ensures that if enough players are not cooperating, it's no longer beneficial to do so.
3. **Last Round**: Defect (D) in the last round, as there is no future benefit from cooperation.

**Reciprocity Mechanism:**

1. Track the number of cooperators (C_count) and defectors (D_count) in each round.
2. Calculate the "cooperation rate" (CR) as C_count / n.
3. If CR > 0.5, cooperate; otherwise, defect.

**Group Benefit Consideration:**

1. Estimate the average payoff per player from cooperation (APC) using historical data.
2. Compare APC to the threshold value (k/n \* n/2).
3. If APC is above this threshold, cooperate; otherwise, defect.

**Pseudocode:**
```markdown
def ARG(n, k, r):
  C_count = 0
  D_count = 0
  APC_history = []

  for round in range(1, r+1):
    if round == 1:
      action = 'C'
    else:
      CR = C_count / n
      APC = estimate_average_payoff(APC_history)
      
      if CR > 0.5 and APC >= k/n * n/2:
        action = 'C'
      elif APC < k/n * n/2:
        action = 'D'
      else:
        action = 'C'

    # Update counts and history
    if action == 'C':
      C_count += 1
    else:
      D_count += 1
    APC_history.append(APC)

  return action

def estimate_average_payoff(APC_history):
  # Simple moving average or more sophisticated estimation method
  pass
```
**Collective Mindset:**

ARG aims to balance individual self-interest with the need for cooperation. By reciprocating cooperation and considering the group benefit, ARG promotes a collective mindset that encourages players to work together to achieve better outcomes.

This strategy is robust against various opponent behaviors, as it adapts to changes in the number of cooperators and defectors over time. It also handles edge cases by defining specific actions for the first and last rounds.

In a tournament setting, ARG will perform well against independent strategies that do not rely on explicit coordination or predetermined patterns.
'''

description_COLLECTIVE_22 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy aims to balance individual self-interest with the benefits of cooperation, adapting to the dynamics of the game and opponent behaviors.

**Decision Rules:**

1. **Initial Round (Round 1)**:
	* Cooperate (C) with probability `p_init = k/n`, where `k` is the multiplication factor and `n` is the number of players.
2. **Subsequent Rounds**:
	* Observe the total number of cooperators (`c_total`) in the previous round.
	* Calculate the average payoff for cooperators (`avg_payoff_c`) and defectors (`avg_payoff_d`) in the previous round.
	* If `avg_payoff_c > avg_payoff_d`, cooperate (C) with probability `p_c = c_total / n`.
	* Otherwise, defect (D) with probability `p_d = 1 - p_c`.
3. **Consecutive Cooperation**:
	* If a player cooperated in the previous round and received a payoff greater than or equal to the average payoff for defectors, increase the cooperation probability by `Δp = 0.05` (up to a maximum of `p_c = 1`).
4. **Consecutive Defection**:
	* If a player defected in the previous round and received a payoff less than or equal to the average payoff for cooperators, decrease the defection probability by `Δp = 0.05` (down to a minimum of `p_d = 0`).

**Edge Cases:**

1. **Last Round (Round r)**:
	* Cooperate (C) if the total number of cooperators in the previous round is greater than or equal to half the number of players (`c_total ≥ n/2`).
	* Otherwise, defect (D).
2. **Tie-breaking**:
	* In cases where `avg_payoff_c == avg_payoff_d`, cooperate with probability `p_c = 0.5`.

**Collective Mindset:**

Our strategy prioritizes cooperation when it benefits the group and adapts to changing circumstances. By considering the average payoffs for cooperators and defectors, we promote a balanced approach that rewards cooperation while discouraging exploitation.

Pseudocode:
```python
def adaptive_cooperation(n, k, r):
    p_init = k / n  # initial cooperation probability

    for t in range(r):  # rounds
        if t == 0:  # first round
            cooperate_prob = p_init
        else:
            c_total = count_cooperators(previous_round)
            avg_payoff_c = calculate_avg_payoff_coop(previous_round)
            avg_payoff_d = calculate_avg_payoff_defect(previous_round)

            if avg_payoff_c > avg_payoff_d:
                cooperate_prob = c_total / n
            else:
                cooperate_prob = 1 - (c_total / n)

        # adjust probabilities based on consecutive cooperation/defection
        if t > 0 and previous_action == 'C' and payoff >= avg_payoff_d:
            cooperate_prob += Δp
        elif t > 0 and previous_action == 'D' and payoff <= avg_payoff_c:
            cooperate_prob -= Δp

        # edge cases
        if t == r - 1:  # last round
            cooperate_prob = 1 if c_total >= n/2 else 0
        elif avg_payoff_c == avg_payoff_d:
            cooperate_prob = 0.5

        action = 'C' if random() < cooperate_prob else 'D'
        return action
```
This strategy will be implemented as an algorithm and played in a tournament against other AI systems to test its effectiveness.
'''

description_COLLECTIVE_23 = '''
**Collective Strategy: Adaptive Tit-for-Tat with Gradual Cooperation (ATGC)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to initiate cooperation and encourage others to do so.
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust strategy accordingly.
3. **Tit-for-Tat Component:** If a majority of players (> n/2) cooperated in the previous round, cooperate (C). Otherwise, defect (D).
4. **Gradual Cooperation Component:** If the number of cooperators in the previous round is increasing or stable, increase the cooperation rate by 1/n (i.e., become more likely to cooperate).
5. **Punishment Mechanism:** If a player defects while others cooperate, punish them by defecting in the next round.

**Edge Cases:**

* **Last Round:** Cooperate (C) if a majority of players cooperated in the previous round; otherwise, defect (D).
* **Early Rounds:** In the first few rounds (e.g., r/4), prioritize cooperation to establish a cooperative atmosphere.
* **Late Rounds:** In the last few rounds (e.g., 3r/4), become more cautious and adjust strategy based on the observed behavior of others.

**Collective Mindset:**

The ATGC strategy aims to balance individual self-interest with collective well-being. By initially cooperating and gradually adjusting to the group's behavior, we encourage cooperation while being robust against exploitation. The tit-for-tat component ensures that we respond to the actions of others, while the gradual cooperation mechanism promotes increasing cooperation over time.

**Pseudocode:**
```python
def ATGC(n, k, r, history):
  # Initialize cooperation rate
  cooperate_rate = 1/n

  for t in range(r):
    if t == 0:
      # First round: Cooperate
      action = C
    else:
      # Analyze previous round's outcome
      prev_cooperators = sum(1 for i in history[t-1] if i == C)

      if prev_cooperators > n/2:
        # Majority cooperated: Cooperate
        action = C
      elif prev_cooperators == n/2:
        # Tie: Gradually increase cooperation rate
        cooperate_rate += 1/n
        action = random.choice([C, D], p=[cooperate_rate, 1-cooperate_rate])
      else:
        # Majority defected: Defect
        action = D

    # Punishment mechanism
    if history[t-1].count(D) > n/2 and history[t-1].count(C) > 0:
      action = D

    return action
```
This strategy is designed to be robust against various opponent behaviors, while promoting cooperation and collective well-being.
'''

description_COLLECTIVE_24 = '''
**Collective Strategy: Adaptive Cooperative Foresight (ACF)**

**Decision Rules:**

1. **First Round:** Cooperate (C) unconditionally to establish a cooperative tone and gather information about opponents' behavior.
2. **Subsequent Rounds:** Use a probabilistic approach based on the history of cooperation rates observed in previous rounds.

Define:

* `cooperation_rate`: The proportion of players who cooperated in the previous round.
* `threshold`: A value between 0 and 1, representing the minimum required cooperation rate to cooperate (set to 0.5 for a balanced approach).
* `sensitivity`: A parameter controlling how responsive the strategy is to changes in cooperation rates (set to 0.2 for moderate sensitivity).

**Cooperation Probability Calculation:**

`cooperate_probability = max(0, min(1, cooperation_rate - threshold + sensitivity))`

If `random_number < cooperate_probability`, then Cooperate (C); otherwise, Defect (D).

**Edge Cases:**

* **Last Round:** Always Defect (D) to avoid being exploited by opponents who may defect in the final round.
* **Tie-Breaking:** In cases where the cooperation probability is exactly 0.5, Cooperate (C) to maintain a slightly cooperative stance.

**Collective Mindset:**

The ACF strategy prioritizes cooperation and adaptability while being mindful of potential exploitation by opponents. By starting with unconditional cooperation in the first round, we encourage others to follow suit. In subsequent rounds, our probabilistic approach allows us to adjust our behavior based on observed cooperation rates. This balance between cooperation and self-interest should promote a collective mindset that fosters mutual benefit.

**Pseudocode:**
```markdown
function ACF(n, k, r, round):
  if round == 1:
    return COOPERATE

  cooperation_rate = get_cooperation_rate_from_previous_round()
  threshold = 0.5
  sensitivity = 0.2

  cooperate_probability = max(0, min(1, cooperation_rate - threshold + sensitivity))

  if random_number < cooperate_probability:
    return COOPERATE
  else:
    return DEFECT

function get_cooperation_rate_from_previous_round():
  # Implement logic to retrieve the proportion of players who cooperated in the previous round
  pass
```
This strategy is designed to be robust and adaptive, allowing it to perform well against a wide range of opponent behaviors. By prioritizing cooperation and adjusting based on observed behavior, ACF aims to create a mutually beneficial environment for all players.
'''

description_COLLECTIVE_25 = '''
**Collective Strategy: "Adaptive Cooperation"**

Our collective strategy, Adaptive Cooperation, aims to balance individual self-interest with collective well-being by adapting to the evolving behavior of other players.

**Decision Rules:**

1. **Initial Rounds (t = 1 to r/3)**: Cooperate (C) in the first round and observe others' actions. In subsequent rounds, cooperate if at least half of the players cooperated in the previous round.
2. **Exploration Phase (t = r/3 + 1 to 2r/3)**:
	* If the average cooperation rate among other players is above 0.5, continue cooperating (C).
	* Otherwise, defect (D) with a probability of 0.5 and cooperate with a probability of 0.5.
3. **Exploitation Phase (t = 2r/3 + 1 to r)**:
	* If the average cooperation rate among other players is above 0.7, continue cooperating (C).
	* Otherwise, defect (D) if your payoff in the previous round was below the average payoff of all players; otherwise, cooperate (C).

**Edge Cases:**

1. **First Round**: Cooperate (C) to set a positive tone and encourage others to cooperate.
2. **Last Round**: Defect (D) if you have been consistently cooperating and your current payoff is above the average payoff of all players; otherwise, cooperate (C).
3. **Tie-Breaking**: In case of ties in the decision rules (e.g., equal number of cooperators and defectors), randomly choose between cooperation and defection with equal probabilities.

**Collective Mindset:**

Adaptive Cooperation prioritizes collective well-being by initially encouraging cooperation and then adapting to the behavior of other players. By balancing individual self-interest with a willingness to cooperate, this strategy promotes a mutually beneficial outcome for all players.

Pseudocode:
```python
def adaptive_cooperation(n, k, r, history):
    # Initialize variables
    avgcoop = 0
    my_payoff = 0

    # Initial Rounds (t = 1 to r/3)
    if t <= r / 3:
        action = 'C'
    else:
        # Exploration Phase (t = r/3 + 1 to 2r/3)
        avgcoop = sum([h['cooperation'] for h in history[-n:]]) / n
        if avgcoop > 0.5:
            action = 'C'
        else:
            action = random.choice(['C', 'D'])

    # Exploitation Phase (t = 2r/3 + 1 to r)
    if t > 2 * r / 3:
        avgcoop = sum([h['cooperation'] for h in history[-n:]]) / n
        my_payoff = sum([h['payoff'] for h in history[-1:]])
        avg_payoff = sum([h['payoff'] for h in history[-n:]]) / n
        if avgcoop > 0.7:
            action = 'C'
        elif my_payoff < avg_payoff:
            action = 'D'
        else:
            action = 'C'

    return action
```
This strategy will be implemented as an algorithm and compete in the tournament against other independent strategies developed by AI systems.
'''

description_COLLECTIVE_26 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Learning (ACGL)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) to establish a cooperative tone and encourage others to do the same.
2. **Learning Phase**: For rounds 2 to r/3 (where r is the total number of rounds), use the following rules:
	* If the average cooperation rate in the previous round is above 0.5, cooperate (C).
	* Otherwise, defect (D) with a probability p = 1 - (average cooperation rate in the previous round). This encourages cooperation while allowing for exploration.
3. **Adaptive Phase**: For rounds r/3 to r-1:
	* Calculate the average payoff per player in the previous round (π_prev).
	* If π_prev is greater than or equal to the expected payoff from universal cooperation (k), cooperate (C).
	* Otherwise, defect (D) with a probability p = 1 - (π_prev / k). This adapts to the group's performance and adjusts cooperation levels accordingly.
4. **Final Round**: In the last round (r), cooperate (C) regardless of previous outcomes, as there is no future benefit from defecting.

**Handling Edge Cases:**

* If n = 2 (only two players), always cooperate (C) to maximize payoffs in this special case.
* If k ≥ n (the multiplication factor is too high), reduce the cooperation rate in the Adaptive Phase by a factor of (n / k) to avoid over-cooperation.

**Collective Mindset:**

ACGL prioritizes collective well-being while being robust to varying opponent behaviors. By initially cooperating and gradually adapting, ACGL promotes a cooperative atmosphere and learns from the group's performance. This strategy is designed to thrive in a diverse tournament setting where other AI systems may employ different strategies.

Pseudocode:
```
function ACGL(n, k, r):
  // Initial rounds
  if round == 1:
    return C
  
  // Learning phase
  elif round <= r/3:
    avg_coop_prev = calculate_average_cooperation_rate(prev_round)
    if avg_coop_prev >= 0.5:
      return C
    else:
      p = 1 - avg_coop_prev
      return D with probability p
  
  // Adaptive phase
  elif round < r:
    π_prev = calculate_average_payoff_per_player(prev_round)
    if π_prev >= k:
      return C
    else:
      p = 1 - (π_prev / k)
      return D with probability p
  
  // Final round
  else:
    return C
```
This strategy is designed to be adaptive, robust, and collective, making it a strong contender in the tournament.
'''

description_COLLECTIVE_27 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Concession (ACGC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage cooperation and gather information about opponents' behavior.
2. **Early Rounds**: For rounds 2 to ⌊r/4⌋, cooperate if:
	* The total number of cooperators in the previous round is greater than or equal to n/2.
	* Your payoff from the previous round was less than or equal to the average payoff of all players.
3. **Middle Rounds**: For rounds ⌊r/4⌋ + 1 to ⌊3r/4⌋, cooperate if:
	* The total number of cooperators in the previous round is greater than n/2.
	* Your payoff from the previous round was less than or equal to your average payoff over the past ⌊r/4⌋ rounds.
4. **Late Rounds**: For rounds ⌊3r/4⌋ + 1 to r - 1, cooperate if:
	* The total number of cooperators in the previous round is greater than n/2.
	* Your average payoff over the past ⌊r/2⌋ rounds is less than or equal to your overall average payoff.
5. **Final Round**: Cooperate (C) in the last round if:
	* The total number of cooperators in the previous round was greater than n/2.

**Gradual Concession Mechanism:**

To adapt to opponents' behavior, implement a gradual concession mechanism:

1. Track the number of consecutive rounds where your payoff is less than or equal to your average payoff (streak).
2. When streak ≥ ⌊r/4⌋, switch from cooperation to defection for one round.
3. If, after switching to defection, your payoff increases and your new streak is reset, maintain defection for the next round.
4. Otherwise, revert to cooperation.

**Pseudocode:**

```python
def ACGC(n, r, k):
    # Initialize variables
    cooperators = []
    payoffs = []
    streak = 0

    for t in range(r):
        if t == 0:
            action = 'C'
        elif t < r/4:
            if len(cooperators) >= n/2 and payoffs[-1] <= sum(payoffs)/len(payoffs):
                action = 'C'
            else:
                action = 'D'
        elif t < 3r/4:
            if len(cooperators) > n/2 and payoffs[-1] <= sum(payoffs[-r//4:])/r//4:
                action = 'C'
            else:
                action = 'D'
        elif t == r - 1:
            if len(cooperators) > n/2 and sum(payoffs)/len(payoffs) <= sum(payoffs[-r//2:])/r//2:
                action = 'C'
            else:
                action = 'D'

        # Gradual concession mechanism
        if payoffs[-1] <= sum(payoffs)/len(payoffs):
            streak += 1
            if streak >= r/4:
                action = 'D'
                streak = 0

        cooperators.append(action == 'C')
        payoffs.append(calculate_payoff(n, k, cooperators))

    return payoffs
```

This strategy balances cooperation and defection to maximize collective payoff while being robust to various opponent behaviors. The gradual concession mechanism allows the strategy to adapt to opponents' actions without compromising its overall cooperative nature.
'''

description_COLLECTIVE_28 = '''
**Collective Strategy: Adaptive Cooperative Balance (ACB)**

The ACB strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 50% to encourage initial cooperation and gather information about opponents' behaviors.
2. **Reciprocity**: For subsequent rounds, calculate the **Cooperation Rate** (CR) as the ratio of cooperative actions to total actions in the previous round.

   `CR = Σ(c_j) / n`
3. **Threshold-Based Cooperation**: Cooperate if CR ≥ k/n (i.e., when at least a fraction k/n of players cooperated in the previous round). Otherwise, defect.
4. **Adaptive Adjustment**: Update the cooperation probability based on the game's history:

   - If CR > k/n and payoff is higher than expected (π_i > 1 + (k/n) \* n), increase cooperation probability by 10% for the next round.
   - If CR < k/n and payoff is lower than expected (π_i < 1), decrease cooperation probability by 10% for the next round.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate if CR ≥ k/n in the second-to-last round. Otherwise, defect.
2. **No History**: If there is no history (e.g., first round or opponents have not played before), default to the initial cooperation probability of 50%.

**Collective Mindset:**

The ACB strategy aligns with a collective mindset by:

1. Encouraging initial cooperation to promote mutual benefit.
2. Responding to opponents' cooperative behavior with reciprocity.
3. Adapting to changes in the game's history to maintain a balance between individual and collective interests.

**Pseudocode:**
```python
def ACB(n, k, r):
  # Initialize variables
  CR = 0.5  # Initial cooperation rate (50%)
  cooperate_prob = 0.5

  for t in range(1, r+1):
    if t == 1:
      action = C with probability 0.5
    else:
      prev_CR = CR
      CR = calculate_cooperation_rate(prev_round_actions)
      if CR >= k/n:
        cooperate_prob = max(cooperate_prob + 0.1, 1) if payoff > expected_payoff else min(cooperate_prob - 0.1, 0)
        action = C with probability cooperate_prob
      else:
        action = D

    # Update history and calculate payoffs
    prev_round_actions.append(action)
    payoff = calculate_payoff(prev_round_actions)

  return actions
```
Note that this pseudocode is a simplified representation of the strategy, and actual implementation may require additional details.
'''

description_COLLECTIVE_29 = '''
**Collective Strategy: Adaptive Cooperative Tit-for-Tat (ACT4T)**

**Overview**
ACT4T is a collective strategy designed for the N-Player Public Goods Game. It balances individual self-interest with cooperation to achieve better collective outcomes. ACT4T adapts to the game's history, responding to varying levels of cooperation and defection among opponents.

**Decision Rules**

1. **Initial Rounds (t = 1)**: Cooperate (C) in the first round to encourage initial cooperation.
2. **Subsequent Rounds (t > 1)**:
   - If the average payoff in the previous round was above the threshold `k/n`, cooperate (C).
   - Otherwise, defect (D).

Pseudocode:
```markdown
if t == 1:  # First round
    action = C
else:
    avg_payoff_prev_round = calculate_average_payoff(t-1)
    if avg_payoff_prev_round >= k / n:
        action = C
    else:
        action = D
```
**Handling Edge Cases**

* **Last Round (t = r)**: Defect (D) to maximize individual payoff, as cooperation will not affect future rounds.
* **Low Cooperation**: If the number of cooperators in the previous round is below `n/2`, defect (D) for two consecutive rounds to signal dissatisfaction with low cooperation levels.

Pseudocode:
```markdown
if t == r:  # Last round
    action = D
elif num_cooperators_prev_round < n / 2:
    if t > 1 and num_cooperators_prev_prev_round < n / 2:
        action = D
```
**Collective Mindset**

ACT4T promotes cooperation by responding positively to high average payoffs, which are often the result of collective cooperation. By cooperating when the average payoff is above a certain threshold, ACT4T encourages others to cooperate as well. When cooperation levels drop, ACT4T's adaptive nature allows it to temporarily defect to signal dissatisfaction and stimulate a return to cooperative behavior.

**Robustness**

ACT4T is robust against various opponent behaviors:

* **Cooperative opponents**: ACT4T will maintain cooperation, ensuring high collective payoffs.
* **Defective opponents**: ACT4T will adapt by defecting, minimizing individual losses.
* **Mixed strategies**: ACT4T's adaptive nature allows it to respond effectively to varying levels of cooperation and defection.

By combining these elements, ACT4T provides a robust and effective collective strategy for the N-Player Public Goods Game.
'''

description_COLLECTIVE_30 = '''
I'll introduce a collective strategy for the N-Player Public Goods Game that adapts to the game's history and is robust against various opponent behaviors. This strategy, dubbed " Adaptive Collective Optimism" (ACO), aims to balance cooperation and self-interest while promoting collective welfare.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 0.5 (50%). This starting point allows ACO to test the waters, observe opponents' behaviors, and adjust accordingly.
2. **History-Based Adaptation**: After the first round, use the following rule:
	* If the average payoff in the previous round was greater than or equal to the individual player's payoff, cooperate (play C).
	* Otherwise, defect (play D).
3. **Cooperation Threshold**: Introduce a cooperation threshold (CT) parameter, which is initially set to 0.5. Update CT based on the game's history:
	* If the number of cooperators in the previous round was greater than or equal to n \* CT, increase CT by 0.05.
	* If the number of cooperators was less than n \* CT, decrease CT by 0.05.
4. **Edge Case Handling**:
	* In the last round, play D (defect) if your current total payoff is below the average payoff; otherwise, play C (cooperate).
	* If a player has never cooperated before, cooperate (play C) with a probability of 0.5 in the next round.

**Collective Mindset Alignment:**

ACO aims to balance individual self-interest with collective welfare by:

1. Encouraging cooperation when it leads to higher payoffs for all players.
2. Adapting to changes in opponents' behaviors and adjusting cooperation levels accordingly.
3. Fostering a cooperative environment by increasing the cooperation threshold when cooperation is successful.

**Pseudocode:**

```
Initialize:
  cooperate_prob = 0.5
  CT = 0.5

For each round t:
  If t == 1:
    play C with probability cooperate_prob
  Else:
    avg_payoff_prev_round = calculate average payoff in previous round
    individual_payoff_prev_round = calculate own payoff in previous round
    If avg_payoff_prev_round >= individual_payoff_prev_round:
      play C
    Else:
      play D
    
    Update CT based on number of cooperators in previous round
    If num_cooperators >= n * CT:
      CT += 0.05
    Else:
      CT -= 0.05

Last Round Handling:
  If total_payoff < avg_total_payoff:
    play D
  Else:
    play C

Never-Cooperated- Before Handling:
  If never cooperated before:
    cooperate_prob = 0.5
```

ACO is designed to be robust against various opponent behaviors, as it adapts to changes in the game's history and balances individual self-interest with collective welfare. By aligning with a collective mindset, ACO promotes cooperation while being mindful of its own payoffs.
'''

description_COLLECTIVE_31 = '''
**Collective Strategy: Adaptive Cooperative Fingerprint (ACF)**

ACF aims to balance individual self-interest with collective welfare by adaptively responding to the game's history and parameters. This strategy does not rely on explicit coordination or shared norms.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
	* Cooperate (C) in the first round to initiate cooperation and observe opponents' reactions.
	* In rounds 2-3, alternate between C and D to gather information about opponents' strategies and adaptability.
2. **Adaptive Phase:**
	* Track the number of cooperators (c) and defectors (d) in each round.
	* Calculate the cooperation ratio: ρ = c / (c + d)
	* If ρ ≥ k/n, cooperate (C) in the next round; otherwise, defect (D).
3. **Punishment Mechanism:**
	* Monitor opponents' behavior and identify "free-riders" who consistently defect while others cooperate.
	* If a free-rider is detected, defect (D) for one round to punish them.
4. **Endgame Strategy (Last 2 rounds):**
	* If the game has reached a stable cooperative state (ρ ≥ k/n), maintain cooperation (C).
	* Otherwise, defect (D) to maximize individual payoff in the remaining rounds.

**Edge Cases:**

1. **First Round:** Cooperate (C)
2. **Last Round:** Defect (D) unless in a stable cooperative state
3. **Ties:** In case of a tie in cooperation ratio ρ, cooperate (C)

**Collective Mindset Alignment:**

ACF prioritizes cooperation when the collective benefit is apparent (ρ ≥ k/n), promoting a mutually beneficial outcome. By punishing free-riders and adapting to opponents' strategies, ACF maintains a balance between individual self-interest and collective welfare.

Pseudocode:
```
Initialize:
  c = 0  // number of cooperators
  d = 0  // number of defectors
  ρ = 0  // cooperation ratio

For each round t:
  if t ≤ 3:  // Initial Exploration
    if t == 1: Cooperate (C)
    else: Alternate between C and D
  else:  // Adaptive Phase
    c = count(cooperators in previous round)
    d = count(defectors in previous round)
    ρ = c / (c + d)
    if ρ ≥ k/n: Cooperate (C)
    else: Defect (D)

  // Punishment Mechanism
  if opponent is a free-rider:
    Defect (D) for one round

  // Endgame Strategy
  if t ≥ r - 2:  // Last 2 rounds
    if ρ ≥ k/n: Cooperate (C)
    else: Defect (D)
```
This strategy will be implemented as an algorithm and tested in a tournament against other AI systems.
'''

description_COLLECTIVE_32 = '''
To design a collective strategy for this N-Player Public Goods Game that is adaptive and robust to various opponent behaviors, we can employ a combination of cooperation and strategic adaptation based on observed behavior. This approach aims to balance individual gain with collective benefit, encouraging mutual cooperation while safeguarding against exploitation.

**Strategy Name:** Adaptive Collective Cooperation (ACC)

### Decision Rules:

1. **Initial Round (t=1):** Cooperate (C). In the absence of prior information about other players' strategies, starting with cooperation sets a positive tone and encourages reciprocity.

2. **Subsequent Rounds (t > 1):**
    - Calculate the proportion of cooperators in the previous round, p_t-1 = Σ(c_j,t-1) / n.
    - If p_t-1 ≥ k/n, cooperate (C). This condition is met when enough players contributed to make cooperation beneficial in terms of shared payoff from the public good.
    - Otherwise, defect (D).

3. **Adaptive Adjustment:** To adapt to changing strategies and maintain robustness:
    - Monitor the average payoff received over the last few rounds (e.g., t-5 to t-1). If this average is below a certain threshold (e.g., 0.8 * k), temporarily switch to defecting for one round to signal dissatisfaction with current payoffs.
    - After such an adjustment, return to the standard decision rule based on p_t-1.

### Handling Edge Cases:

- **Last Round:** In the final round (t=r), consider the long-term average payoff. If it's below a satisfactory level, defect to maximize personal gain since there are no future rounds to worry about reciprocity or maintaining cooperation.
  
- **Early Rounds with Low Cooperation:** If in the initial few rounds (e.g., t < 5) cooperation levels are consistently low, briefly switch to an exploratory phase. In this phase, alternate between cooperate and defect over a couple of rounds to gauge other players' reactions without significantly impacting overall payoff.

### Collective Mindset:

The ACC strategy is designed with a collective mindset, aiming for mutual benefit through cooperation while ensuring individual protection against exploitation. It encourages others to cooperate by starting with a cooperative stance and adjusting based on observed behavior, thus promoting a balance between personal gain and public good.

**Pseudocode:**

```python
def AdaptiveCollectiveCooperation(n, k, r):
    p_t = 0  # Proportion of cooperators in the previous round
    avg_payoff_window = []  # To monitor average payoffs over last few rounds
    
    for t in range(1, r+1):  # Rounds loop
        if t == 1:  # First round
            action = 'C'
        else:
            p_t = get_proportion_cooperators(t-1)
            if p_t >= k/n:
                action = 'C'
            else:
                action = 'D'
            
            # Adaptive adjustment based on average payoff
            avg_payoff = sum(avg_payoff_window) / len(avg_payoff_window)
            if avg_payoff < 0.8 * k and t > 5:  
                action = 'D'  # Temporarily defect to adjust
                # Add logic here for brief exploratory phase in early rounds
                
        # Play the game with chosen action and update p_t, avg_payoff_window
        
        if t == r:  # Last round adjustment based on long-term average payoff
            if long_term_avg_payoff < satisfactory_level:
                action = 'D'
        
    return total_payoff

# Supporting functions like get_proportion_cooperators(t) and updating avg_payoff_window are assumed.
```

This strategy is designed to be flexible, adaptive, and aligned with the collective mindset necessary for success in a public goods game.
'''

description_COLLECTIVE_33 = '''
**Collective Strategy: "Adaptive Cooperation" (AC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage cooperation and gather information about opponents' strategies.
2. **Subsequent Rounds**: Observe the total number of cooperators (TC) in the previous round. If TC ≥ n/2, cooperate (C). Otherwise, defect (D).
3. **Adaptive Threshold**: Update the threshold for cooperating based on the game's history. After each round, calculate the average payoff per player (APP) over the last 5 rounds. If APP > k/2, decrease the cooperation threshold by 1/n. If APP < k/2, increase the cooperation threshold by 1/n.

**Pseudocode:**
```python
def adaptive_cooperation(n, k, r):
    # Initialize variables
    TC = 0  # Total cooperators in previous round
    APP = []  # Average payoff per player over last 5 rounds

    for t in range(1, r+1):
        if t == 1:  # Initial round
            action = 'C'
        else:
            if TC >= n/2:
                action = 'C'
            else:
                action = 'D'

        # Update Adaptive Threshold
        APP.append(calculate_average_payoff(n, k, r))  # calculate average payoff per player over last 5 rounds
        if len(APP) > 5:
            APP.pop(0)
        threshold_update = sum(APP) / len(APP) - k/2
        if threshold_update > 0:
            cooperation_threshold -= 1/n
        elif threshold_update < 0:
            cooperation_threshold += 1/n

        # Update TC for next round
        TC = count_cooperators()

    return action
```
**Rationale:**

* By cooperating in the initial round, we encourage opponents to cooperate and gather information about their strategies.
* The adaptive threshold mechanism allows us to adjust our cooperation strategy based on the game's history. If the average payoff per player is high, we decrease the cooperation threshold, making it more likely for us to cooperate. Conversely, if the average payoff per player is low, we increase the cooperation threshold, making it less likely for us to cooperate.
* By using a moving average of payoffs over the last 5 rounds, we smooth out noise and react to changes in the game's dynamics.

**Collective Mindset:**

Our strategy aligns with the collective mindset by:

* Encouraging cooperation in the initial round
* Adapting our cooperation strategy based on the game's history
* Reacting to changes in the average payoff per player

By using a robust and adaptive strategy, we aim to promote cooperation while also being prepared to defect when necessary.
'''

description_COLLECTIVE_34 = '''
I'll design a collective strategy that adapts to the game's history and parameters. Here's my proposal:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability p = k/n, where k is the multiplication factor and n is the number of players. This initial cooperation rate encourages others to cooperate while allowing our strategy to adapt.
2. **Subsequent Rounds (t>1):**
	* Observe the previous round's total contributions (Σ(j=1 to n) c_j).
	* If the total contributions are above a threshold θ = (k/n) \* (n/2), Cooperate (C). This indicates that enough players are cooperating, making it beneficial for us to contribute.
	* Otherwise, Defect (D).
3. **Adjustment Mechanism:** After each round, adjust our cooperation probability p based on the previous round's outcome:
	+ If our payoff π_i was higher than the average payoff of all players, increase p by 10% (p = min(p + 0.1, k/n)).
	+ If our payoff π_i was lower than or equal to the average payoff, decrease p by 5% (p = max(p - 0.05, 0)).

**Edge Cases:**

* **Last Round (t=r):** Cooperate (C) if the total contributions in the previous round were above the threshold θ. Otherwise, Defect (D).
* **Ties:** In case of a tie in payoffs or contributions, maintain our current cooperation probability p.

**Collective Mindset:**

ACO aims to align with the collective mindset by:

1. Encouraging initial cooperation through probabilistic cooperation.
2. Adapting to the game's history and adjusting our cooperation rate based on outcomes.
3. Focusing on the group's overall performance, rather than individual payoffs.

**Pseudocode:**
```python
def ACO(n, k, r):
  p = k/n  # Initial cooperation probability
  for t in range(1, r+1):
    if t == 1:
      action = Cooperate with probability p
    else:
      total_contributions = Σ(j=1 to n) c_j from previous round
      θ = (k/n) * (n/2)
      if total_contributions > θ:
        action = Cooperate
      else:
        action = Defect
    outcome = play_round(action)
    π_i = calculate_payoff(outcome)
    avg_payoff = calculate_average_payoff()
    if π_i > avg_payoff:
      p = min(p + 0.1, k/n)  # Increase cooperation probability
    else:
      p = max(p - 0.05, 0)  # Decrease cooperation probability
  return action
```
This strategy balances individual and collective interests by adapting to the game's history and adjusting our cooperation rate accordingly. By doing so, ACO aims to achieve a mutually beneficial outcome for all players in the tournament.
'''

description_COLLECTIVE_35 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Concession (ACGC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage cooperation and gather information about opponents' behaviors.
2. **Subsequent Rounds**:
	* If the number of cooperators in the previous round is greater than or equal to `n/2`, cooperate (C).
	* Otherwise, defect (D) with a probability `p` that depends on the game history.

The probability `p` is calculated as follows:

`p = max(0, 1 - (k/n) * (average_cooperators / n))`

where `average_cooperators` is the average number of cooperators over the last `r/2` rounds (or all previous rounds if `r/2` is not an integer).

**Gradual Concession Mechanism:**

* If a player defects, they will gradually concede by cooperating with a probability that increases as the game progresses.
* The concession rate is determined by the difference between the actual number of cooperators and the desired number (`n/2`). A larger difference leads to faster concession.

**Additional Heuristics:**

* **Tit-for-Tat (TFT) Component**: If an opponent defected in the previous round, there's a 50% chance that our strategy will defect as well in the current round.
* **Reciprocity Detection**: If an opponent has cooperated in at least `r/3` consecutive rounds, our strategy will cooperate with high probability (`>0.8`) to reciprocate and reinforce cooperation.

**Edge Cases:**

* **Last Round**: Cooperate (C) in the last round to maximize collective payoff.
* **Tie-Breaking**: In case of a tie in the number of cooperators, our strategy will cooperate with a probability `>0.5` to break the tie and encourage cooperation.

**Collective Mindset:**

ACGC is designed to promote cooperation while being robust to various opponent behaviors. By cooperating initially and gradually conceding when others defect, we create an environment conducive to mutual cooperation. The TFT component ensures that our strategy can adapt to aggressive opponents, while reciprocity detection helps maintain cooperation with cooperative opponents.

Pseudocode:
```python
def ACGC(n, k, r):
  # Initialize variables
  average_cooperators = 0
  concession_rate = 0

  for round in range(r):
    if round == 0:
      action = C
    else:
      # Calculate probability of cooperation
      p = max(0, 1 - (k/n) * (average_cooperators / n))
      action = choose_action(p)

      # Update concession rate
      concession_rate += (n/2 - average_cooperators) / (r/2)

    # Take action and update history
    if action == C:
      cooperate()
    else:
      defect()

    # Update average cooperators and concession rate
    average_cooperators = (average_cooperators * (round-1) + count_cooperators()) / round

  return total_payoff
```
Note: This pseudocode provides a basic outline of the strategy. The actual implementation will require additional details, such as handling multiple opponents and calculating payoffs.
'''

description_COLLECTIVE_36 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

The ACP strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and punishing free-riders.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds:** Observe the total number of cooperators (T_c) in the previous round and calculate the average cooperation rate (ACR) as T_c / n.

**Cooperation Threshold:**

* If ACR ≥ 0.5, Cooperate (C). This means that if at least half of the players cooperated in the previous round, it's likely beneficial to continue cooperating.
* If ACR < 0.5, Defect (D) with a probability p = 1 - ACR. This introduces a gradual punishment mechanism for low cooperation rates.

**Punishment Mechanism:**

* When defecting, keep track of the number of consecutive defections (CD).
* If CD > k/2 (i.e., more than half of the multiplication factor), switch to Cooperate (C) in the next round. This prevents prolonged exploitation and allows for re-entry into cooperation.

**Edge Cases:**

* **Last Round:** Cooperate (C) in the last round, as there is no future punishment or reward.
* **Tie-breaking:** In case of a tie (e.g., ACR = 0.5), choose Cooperate (C) to maintain a cooperative stance.

**Collective Mindset:**

ACP prioritizes cooperation while maintaining individual self-interest. By punishing free-riders and adapting to the game's history, it encourages others to cooperate, ultimately leading to higher collective payoffs.

Pseudocode:
```
Input: n, k, r (game parameters), T_c (total cooperators in previous round)
Output: action (C or D)

if first_round then
  return C
end if

ACR = T_c / n

if ACR ≥ 0.5 then
  return C
else
  p = 1 - ACR
  if random() < p then
    return D
    CD++
  else
    return C
    CD = 0
  end if
end if

if CD > k/2 then
  return C
  CD = 0
end if
```
This strategy balances cooperation and self-interest, making it robust to various opponent behaviors. By adapting to the game's history and punishing free-riders, ACP encourages collective cooperation and maximizes overall payoffs.
'''

description_COLLECTIVE_37 = '''
**Collective Strategy: Adaptive Cooperative Follower (ACF)**

The ACF strategy is designed to balance individual self-interest with collective well-being, adapting to the evolving game dynamics and opponent behaviors.

**Decision Rules:**

1. **Initial Rounds (t = 1):** Cooperate (C) in the first round to set a positive tone for cooperation.
2. **Subsequent Rounds (t > 1):**
   - If the majority of players (> n/2) cooperated in the previous round, cooperate (C).
   - Otherwise, defect (D) if the average payoff of defectors is higher than that of cooperators; otherwise, cooperate (C).

Pseudocode:
```python
if t == 1:  # First round
    action = C
else:
    prev_round_coops = count_cooperators(t-1)
    if prev_round_coops > n/2:
        action = C
    else:
        avg_defect_payoff = average_payoff(defectors, t-1)
        avg_coop_payoff = average_payoff(cooperators, t-1)
        if avg_defect_payoff > avg_coop_payoff:
            action = D
        else:
            action = C
```
**Handling Edge Cases:**

* **Last Round (t = r):** Cooperate (C) in the last round to ensure a positive collective outcome, regardless of previous actions.
* **Ties:** In case of a tie (e.g., equal number of cooperators and defectors), cooperate (C) to maintain a cooperative stance.

**Collective Mindset:**

The ACF strategy prioritizes cooperation when it is likely to be reciprocated by other players, while adapting to situations where defection may provide a short-term advantage. By following the majority's behavior, ACF encourages cooperation and helps sustain collective well-being over time.

**Robustness to Opponent Behaviors:**

The ACF strategy can handle various opponent behaviors:

* **Cooperative opponents:** ACF will cooperate with them, fostering mutual cooperation.
* **Defecting opponents:** ACF may initially defect but will adapt to cooperation if others start cooperating.
* **Mixed strategies:** ACF's adaptive nature allows it to adjust its behavior in response to changing opponent actions.

**Tournament Performance:**

The ACF strategy is designed to perform well in a tournament setting, where multiple independent strategies compete. By balancing individual self-interest with collective well-being, ACF can effectively navigate diverse opponent behaviors and contribute to overall collective success.
'''

description_COLLECTIVE_38 = '''
**Collective Strategy: Adaptive Cooperation with Social Learning**

Our strategy is designed to balance individual self-interest with collective well-being, adapting to various opponent behaviors while promoting cooperation.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to signal a willingness to cooperate and encourage others to do so.
2. **Social Learning Phase (3 ≤ t < r/2):**
	* If the total number of cooperators in the previous round is greater than or equal to half the number of players (n/2), Cooperate (C).
	* Otherwise, Defect (D).
3. **Exploration and Exploitation Phase (t ≥ r/2):**
	* With probability p = 0.1, Explore by randomly choosing an action (C or D) to gather information about opponents' strategies.
	* With probability 1 - p = 0.9, choose the action that maximizes expected payoff based on the observed opponent behavior in previous rounds.

**Expected Payoff Calculation:**

For each possible action (C or D), estimate the expected payoff using the following formula:

E[π_i] = (1 - c_i) + (k/n) × E[Σ(j=1 to n) c_j]

where E[Σ(j=1 to n) c_j] is the expected number of cooperators, estimated based on observed opponent behavior.

**Handling Edge Cases:**

* **Last Round (t = r):** Cooperate (C) if the total payoff from cooperation in previous rounds exceeds the total payoff from defection. Otherwise, Defect (D).
* **Ties:** In case of a tie in expected payoffs between C and D, choose Cooperate (C) to promote collective well-being.

**Pseudocode:**

```
function AdaptiveCooperation(n, k, r, history):
  if t ≤ 2:
    return C
  elif 3 ≤ t < r/2:
    total_cooperators = count_c(history[t-1])
    if total_cooperators ≥ n/2:
      return C
    else:
      return D
  else:
    explore_probability = 0.1
    if random() < explore_probability:
      return random_choice([C, D])
    else:
      expected_payoff_C = estimate_expected_payoff(history, C)
      expected_payoff_D = estimate_expected_payoff(history, D)
      if expected_payoff_C > expected_payoff_D:
        return C
      elif expected_payoff_D > expected_payoff_C:
        return D
      else:  # tie
        return C

function estimate_expected_payoff(history, action):
  # calculate expected number of cooperators based on observed opponent behavior
  expected_cooperators = ...
  expected_payoff = (1 - c_i) + (k/n) × expected_cooperators
  return expected_payoff
```

This strategy balances individual self-interest with collective well-being by adapting to various opponent behaviors and promoting cooperation. It is designed to be robust against a wide range of opponent strategies, making it suitable for a tournament setting.
'''

description_COLLECTIVE_39 = '''
I'll introduce a collective strategy that adapts to various opponent behaviors while promoting cooperation and robustness. This strategy is called "Conditional Cooperative Escalation" (CCE).

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (play C) with a probability of 0.5 (or 50%). This establishes an initial cooperative tone without being overly naive.
2. **Observing Opponent Behavior**: For each subsequent round (t > 1), calculate the average cooperation rate among all opponents in the previous round (t-1). Let's denote this as `avg_opponent_coop[t-1]`.
3. **Cooperation Threshold**: Set a cooperation threshold (`coop_threshold`) based on the multiplication factor (`k`): `coop_threshold = k / n`. This value represents the minimum level of collective cooperation required to make cooperation beneficial.
4. **Escalation Mechanism**: If `avg_opponent_coop[t-1] >= coop_threshold`, cooperate (play C) in round t with a high probability (e.g., 0.8). Otherwise, defect (play D).
5. **Punishment for Defection**: If an opponent defects in the previous round and you cooperated, decrease your cooperation probability by a small amount (e.g., 0.1) for that specific opponent in the next round.

**Handling Edge Cases:**

* **Last Round**: In the final round (r), play D to avoid potential exploitation.
* **Early Defection**: If an opponent defects in one of the first few rounds (e.g., t <= 3), defect immediately and switch to a more cautious strategy, reducing cooperation probability by a larger amount (e.g., 0.2) for that opponent.

**Collective Mindset:**

CCE aims to balance individual self-interest with collective well-being. By cooperating when opponents do, you create an environment where mutual cooperation becomes beneficial. The escalation mechanism encourages others to cooperate, while the punishment component discourages exploitation.

**Pseudocode:**
```python
def CCE(n, k, r):
  # Initial round probability
  coop_prob = 0.5
  
  for t in range(1, r+1):
    if t == 1:
      # First round: random cooperation (50%)
      action[t] = 'C' if random.random() < coop_prob else 'D'
    else:
      avg_opponent_coop = calculate_avg_opponent_coop(t-1)
      coop_threshold = k / n
      
      if avg_opponent_coop >= coop_threshold:
        # Escalate cooperation
        coop_prob = 0.8
      else:
        coop_prob -= 0.1  # Decrease cooperation probability
        
      action[t] = 'C' if random.random() < coop_prob else 'D'
      
    # Punish opponent for defection in previous round (if you cooperated)
    for opponent in range(n):
      if opponent_defected[opponent][t-1] and action[t-1] == 'C':
        coop_prob -= 0.2
        
    # Update cooperation probability bounds
    coop_prob = max(0, min(coop_prob, 1))
    
  return action

def calculate_avg_opponent_coop(t):
  opponent_actions = [action[i][t] for i in range(n)]
  avg_cooperation = sum(opponent_actions.count('C')) / n
  return avg_cooperation
```
This strategy combines adaptiveness and robustness to perform well against various opponents, while promoting cooperation when beneficial.
'''

description_COLLECTIVE_40 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

ACE is a robust and adaptive collective strategy for the N-Player Public Goods Game. It balances individual self-interest with collective well-being by dynamically adjusting cooperation levels based on observed opponent behavior.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a baseline of cooperation and encourage others to follow suit.
2. **Cooperation Threshold**: Calculate the average cooperation rate of all players over the past `min(r, 5)` rounds, denoted as `avg_c`. If `avg_c` is above a threshold value (`thres = 0.6`), cooperate (C) in the current round.
3. **Escalation Mechanism**: If `avg_c` falls below the threshold, defect (D) with a probability (`p_defect`) proportional to the difference between `avg_c` and `thres`. Specifically:

   `p_defect = max(0, 1 - ((avg_c - thres) / (1 - thres)))`

   This mechanism allows ACE to gradually increase defection rates when cooperation levels decline.
4. **Punishment Mechanism**: If a player observes another player defecting in the previous round, they will defect with a higher probability (`p_punish = 0.8`) in the current round.

**Edge Case Handling:**

* In the last round, always cooperate (C) to maximize collective payoff.
* If an opponent defects consistently (more than `r/2` times), permanently switch to defection (D).
* When facing a completely new set of opponents, reset the cooperation threshold and escalation mechanism.

**Collective Mindset:**

ACE is designed to promote a culture of cooperation by:

1. Encouraging initial cooperation to establish trust.
2. Adapting to changing opponent behavior through the escalation mechanism.
3. Punishing consistent defectors to maintain social norms.
4. Prioritizing collective well-being over individual gains when cooperation levels are high.

**Pseudocode:**
```python
def ACE(n, r, k):
  # Initialize variables
  avg_c = 0.0  # Average cooperation rate
  thres = 0.6  # Cooperation threshold
  p_defect = 0.0  # Probability of defection

  for t in range(r):  # Loop through rounds
    if t == 0:  # Initial round
      action = 'C'
    else:
      avg_c = calculate_avg_cooperation()  # Update average cooperation rate
      p_defect = max(0, 1 - ((avg_c - thres) / (1 - thres)))  # Calculate defection probability

      if random.random() < p_defect:  # Escalation mechanism
        action = 'D'
      elif avg_c >= thres:
        action = 'C'  # Cooperate above threshold
      else:
        action = 'D'

    # Punishment mechanism (optional)
    for opponent in opponents:
      if opponent.defected_last_round:
        p_punish = 0.8
        if random.random() < p_punish:
          action = 'D'

    # Take action and update game state
    take_action(action)

    # Update last round's actions and cooperation rates for opponents
    update_opponent_actions()

  return total_payoff()
```
This strategy should be implemented as an algorithm to participate in the tournament against other AI systems.
'''

description_COLLECTIVE_41 = '''
**Collective Strategy: Adaptive Tit-for-Tat with Threshold (ATTT)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds:** Observe the previous round's actions and payoffs.
	* If the number of cooperators in the previous round is above a certain threshold (τ), cooperate (C). Otherwise, defect (D).
	* Update τ based on the game's history using an exponential moving average (EMA) with a smoothing factor α.

Pseudocode:
```
τ = 0.5 * n (initialize threshold as half of the number of players)
α = 0.1 (smoothing factor for EMA)

for each round t:
  if t == 1: # First Round
    action = C
  else:
    num_cooperators_prev_round = count(C in previous round's actions)
    τ = α * num_cooperators_prev_round + (1 - α) * τ
    if num_cooperators_prev_round >= τ:
      action = C
    else:
      action = D

return action
```
**Handling Edge Cases:**

* **Last Round:** Since the game has perfect information, we can anticipate that others may defect in the last round to maximize their payoff. In this case, our strategy will also defect (D) in the last round.
* **Ties in τ:** If the number of cooperators is exactly equal to τ, we cooperate (C) to maintain a cooperative tone.

**Collective Mindset:**

Our strategy, Adaptive Tit-for-Tat with Threshold (ATTT), aims to balance individual self-interest with collective cooperation. By adapting to the game's history and adjusting our threshold τ, we encourage cooperation while avoiding exploitation by defectors.

* **Encouraging Cooperation:** By cooperating in the first round and when the number of cooperators is above the threshold, we promote a cooperative environment.
* **Discouraging Defection:** When the number of cooperators falls below the threshold, we defect to signal that cooperation is not being reciprocated and to protect our own interests.

**Robustness:**

ATTT is designed to be robust against various opponent behaviors:

* **All-Defectors:** If all opponents defect, our strategy will also defect, ensuring a reasonable payoff.
* **Mixed Strategies:** ATTT adapts to the game's history, allowing it to respond effectively to mixed strategies that combine cooperation and defection.

By implementing this collective strategy, we aim to achieve a high total payoff over multiple rounds while promoting cooperation among players.
'''

description_COLLECTIVE_42 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to establish a cooperative tone and encourage others to do the same.
2. **Reciprocity**: Cooperate if at least k/n players cooperated in the previous round. This ensures that our cooperation is reciprocated by enough players to generate a positive public good.
3. **Escalation**: If fewer than k/n players cooperated in the previous round, defect (play D) with probability p = (k/n - number of cooperators last round) / (n - k). This escalates the pressure on others to cooperate by increasing the likelihood of our defection as more players defect.
4. **Concession**: If all players defected in the previous round, cooperate with probability q = 1/(n-1). This introduces a possibility for cooperation to re-emerge and encourages others to do the same.

**Handling Edge Cases:**

* In the last round (r), always defect. Since there's no future interaction, there's no incentive to maintain cooperation.
* If only one player remains in the game (e.g., due to opponents' strategies resulting in early exits), cooperate if k > 1 and defect otherwise.

**Collective Mindset:**

ACE aligns with a collective mindset by:

* Encouraging cooperation through reciprocity and initial cooperation
* Escalating pressure on others to cooperate when necessary
* Introducing concession mechanisms to allow for cooperation re-emergence

**Pseudocode (for clarity):**
```python
def ACE(n, k, r, history):
    if round == 1:  # Initial Cooperation
        return C
    elif len(cooperators_last_round) >= k:
        return C  # Reciprocity
    else:
        p = (k - len(cooperators_last_round)) / (n - k)
        if random.random() < p:
            return D  # Escalation
        else:
            q = 1 / (n - 1)
            if random.random() < q and all_defected_last_round:
                return C  # Concession
            else:
                return D

    if round == r:  # Last Round
        return D

    # Handle single-player case
    if num_players_left == 1:
        if k > 1:
            return C
        else:
            return D
```
This strategy is designed to adapt to various opponent behaviors while promoting cooperation and public good generation. By incorporating reciprocity, escalation, and concession mechanisms, ACE balances individual interests with collective well-being, ensuring a robust performance in the tournament against diverse strategies.
'''

description_COLLECTIVE_43 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

ACE is a decision-making framework that balances individual self-interest with collective cooperation. It adapts to the evolving game dynamics and opponent behaviors, promoting mutual benefits while minimizing exploitation.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative baseline.
2. **Subsequent Rounds:** Evaluate the previous round's outcome using the following metrics:
	* **Cooperation Index (CI):** The proportion of players who cooperated in the previous round (i.e., CI = Σ(j=1 to n) c_j / n).
	* **Personal Payoff Ratio (PPR):** The ratio of your payoff in the previous round to the maximum possible payoff (i.e., PPR = π_i,t-1 / k).

Based on these metrics, apply the following decision rules:

	* If CI ≥ 0.5 and PPR ≥ 0.8: Cooperate (C). This indicates a strong cooperative environment, and your individual contribution is relatively high.
	* If CI < 0.5 or PPR < 0.8: Defect (D) with probability p = 1 - CI. This introduces a degree of caution when cooperation levels are low or personal payoffs are unsatisfactory.
3. **Escalation Mechanism:** If you defected in the previous round and observed an increase in cooperation (i.e., CI increased), consider escalating your cooperation by increasing the probability of cooperating in the current round. Specifically, if p > 0.5 in the previous round, set p = max(0.5, p + 0.1) for the current round.

**Edge Cases:**

* **Last Round:** Cooperate (C) unconditionally to maximize collective payoff and establish a cooperative tone for future interactions.
* **Ties:** In cases where multiple strategies yield identical expected payoffs, default to cooperation (C) to maintain a collective focus.

**Pseudocode:**
```python
def ACE(n, k, history):
    # Initialize variables
    CI = 0.5  # Cooperation Index
    PPR = 0.8  # Personal Payoff Ratio
    p = 1 - CI  # Defection probability

    # Evaluate previous round's outcome (if applicable)
    if len(history) > 0:
        CI = sum([h['cooperated'] for h in history[-1]]) / n
        PPR = history[-1][player_id]['payoff'] / k

    # Apply decision rules
    if CI >= 0.5 and PPR >= 0.8:
        return 'C'
    else:
        p = 1 - CI
        if random.random() < p:
            return 'D'
        else:
            return 'C'

    # Escalation mechanism (if applicable)
    if len(history) > 1 and history[-2][player_id]['action'] == 'D' and CI > history[-2]['CI']:
        p = max(0.5, p + 0.1)

# Example usage:
n = 6
k = 2
history = []  # Initialize empty history

for round in range(r):
    action = ACE(n, k, history)
    # Simulate game dynamics and update history
    ...
```
**Collective Mindset:**

ACE promotes a collective mindset by:

1. Encouraging cooperation when it is likely to be reciprocated.
2. Gradually increasing cooperation levels in response to increased collective cooperation.
3. Avoiding exploitation by defecting with a probability that depends on the level of cooperation.

By adopting ACE, players can foster a cooperative environment while minimizing individual losses and promoting mutual benefits.
'''

description_COLLECTIVE_44 = '''
**Collective Strategy: Adaptive Tit-for-Tat with Public Goods Twist**

Our collective strategy, named "PubliTFT," is designed to promote cooperation while adapting to various opponent behaviors. PubliTFT combines elements of Tit-for-Tat (TFT) and the public goods game's unique payoff structure.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C).
2. **Public Goods Threshold**: Calculate a "public goods threshold" (PGT) based on the previous round's total contributions:
	* PGT = (k/n) × average_total_contributions_last_round
3. **Tit-for-Tat Component**:
	* If the opponent(s) cooperated in the last round, cooperate (C).
	* If the opponent(s) defected in the last round, defect (D).
4. **Public Goods Adaptation**: Adjust the decision based on the public goods threshold:
	* If total contributions in the previous round exceeded PGT, cooperate (C).
	* If total contributions were below PGT, defect (D).

**Pseudocode:**
```
function PubliTFT(n, k, r, history):
  if current_round == 1:
    return C  # Initial cooperation
  else:
    previous_total_contributions = sum(history[-1])
    pgt = (k/n) * average_total_contributions_last_round
    
    opponent_actions = [history[i][-1] for i in range(n)]
    tft_decision = TFT(opponent_actions)
    
    if previous_total_contributions > pgt:
      return C  # Cooperate if public goods threshold exceeded
    else:
      return tft_decision  # Follow Tit-for-Tat component

def TFT(opponent_actions):
  if all(opponent_actions == C):
    return C  # Cooperate if opponents cooperated
  else:
    return D  # Defect if opponents defected
```
**Edge Cases:**

1. **Last Round**: In the final round, PubliTFT defaults to defecting (D), as there are no future rounds to influence.
2. **No Previous History**: If there is no previous history (e.g., first round or new opponent), PubliTFT cooperates.

**Collective Mindset:**

PubliTFT aligns with the collective mindset by:

1. Encouraging cooperation in the early stages of the game.
2. Adapting to the public goods threshold, which reflects the group's overall cooperative behavior.
3. Punishing defection using the Tit-for-Tat component.

By combining these elements, PubliTFT promotes a balance between individual self-interest and collective well-being, making it a robust and adaptive strategy for the N-Player Public Goods Game.
'''

description_COLLECTIVE_45 = '''
**Collective Strategy: Adaptive Public Goods Game (APGG)**

The APGG strategy aims to balance individual payoff maximization with collective cooperation to achieve a high total game payoff.

**Decision Rules:**

1. **Initial Round:** In the first round, play Cooperate (C) with probability 0.5. This allows us to explore the opponents' behavior while minimizing potential losses.
2. **Exploration Phase (Rounds 2-5):**
	* If the average payoff of all players in the previous round is above the threshold `k/n` (i.e., the multiplication factor divided by the number of players), play Cooperate (C).
	* Otherwise, play Defect (D) with probability `1 - (average_payoff / (k/n))`. This encourages cooperation when the public good is valuable and defecting when it's not.
3. **Exploitation Phase (Rounds 6+):**
	* Calculate the average number of cooperators in the previous round (`avg_cooperators`).
	* If `avg_cooperators` is above a certain threshold (`n/2`), play Cooperate (C). This maintains cooperation when most players are cooperating.
	* Otherwise, play Defect (D) with probability `1 - (avg_cooperators / n)`. This adapts to the level of cooperation in the group.

**Edge Cases:**

1. **Last Round:** In the last round, play Cooperate (C) if the average payoff of all players in the previous round is above the threshold `k/n`, and Defect (D) otherwise.
2. **Tie-Breaking:** In cases where the decision rules yield a tie, choose the action that maximizes the individual payoff.

**Pseudocode:**
```python
def APGG(n, k, r):
    # Initialize variables
    avg_payoff = 0
    avg_cooperators = 0

    for t in range(r):
        if t == 0:
            # Initial round: randomize cooperation probability
            cooperate_prob = 0.5
        elif t < 5:
            # Exploration phase
            avg_payoff = calculate_avg_payoff()
            if avg_payoff > k/n:
                cooperate_prob = 1
            else:
                cooperate_prob = 1 - (avg_payoff / (k/n))
        else:
            # Exploitation phase
            avg_cooperators = calculate_avg_cooperators()
            if avg_cooperators > n/2:
                cooperate_prob = 1
            else:
                cooperate_prob = 1 - (avg_cooperators / n)

        action = random.choice([C, D], p=[cooperate_prob, 1-cooperate_prob])

        # Update variables for next round
        avg_payoff = update_avg_payoff()
        avg_cooperators = update_avg_cooperators()

    return total_game_payoff()
```
**Collective Mindset:**

The APGG strategy prioritizes cooperation when the public good is valuable and adapts to the level of cooperation in the group. By doing so, it promotes a collective mindset that balances individual payoff maximization with the greater good.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it:

1. Explores opponents' behavior in early rounds.
2. Adapts to changes in cooperation levels throughout the game.
3. Prioritizes cooperation when beneficial for all players.

By adopting this collective mindset, APGG aims to achieve a high total game payoff while minimizing losses and promoting cooperation among players.
'''

description_COLLECTIVE_46 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

AC is a collective strategy that balances individual self-interest with cooperative behavior, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) unconditionally.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is greater than or equal to half the number of players (n/2), cooperate (C).
	* Otherwise, defect (D).

Pseudocode:
```
if t == 1:
    action = C
else:
    prev_cooperators = count(C) from previous round
    if prev_cooperators >= n/2:
        action = C
    else:
        action = D
```

**Edge Cases:**

* **Last Round (t=r):** Cooperate (C) unconditionally, as there is no future payoff to consider.
* **Ties:** In case of a tie in the number of cooperators, cooperate (C).
* **First-round deviations:** If an opponent deviates from cooperation in the first round, adjust the strategy by defecting (D) for one round before reverting to the original decision rule.

**Collective Mindset:**

AC promotes a collective mindset by:

1. Encouraging initial cooperation to establish a cooperative atmosphere.
2. Responding positively to cooperation by other players, reinforcing mutual cooperation.
3. Punishing widespread defection by switching to defection, discouraging exploitation.
4. Fostering a sense of shared responsibility by adapting to the group's overall behavior.

**Robustness:**

AC is robust against various opponent behaviors, including:

1. **Cooperative opponents:** AC cooperates when others cooperate, ensuring mutual benefits.
2. **Defective opponents:** AC adapts to widespread defection by defecting, minimizing losses.
3. **Mixed strategies:** AC's adaptive nature allows it to respond effectively to mixed strategies.

**Tournament Performance:**

AC is designed to perform well in a tournament setting, as it balances individual self-interest with collective cooperation. By adapting to the game's history and parameters, AC can outperform independent strategies that rely on fixed or naive approaches.
'''

description_COLLECTIVE_47 = '''
I propose a collective strategy called "Adaptive Reciprocal Altruism" (ARA) that balances individual self-interest with group cooperation. ARA depends solely on game parameters and history.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to set a positive tone for the game.
2. **Reciprocity**: Cooperate if at least k-1 players cooperated in the previous round (i.e., enough to create a public good). Otherwise, defect (D).
3. **Punishment**: If more than n-k+1 players defected in the previous round, defect in this round to deter future free-riding.
4. **Self-Protection**: Defect if your individual payoff from cooperation would be lower than the average payoff of all players in the previous round (π_i,t < Σ(j=1 to n) π_j,t / n).

**Edge Cases:**

* In the last round, cooperate only if the game's history suggests that at least k-1 players will also cooperate.
* If there is a tie in the number of cooperators and defectors in the previous round, cooperate.

**Collective Mindset Alignment:**

ARA prioritizes cooperation when it benefits both the individual and the group. By reciprocating cooperation and punishing free-riding, ARA encourages collective cooperation while protecting individual self-interest.

Pseudocode:
```markdown
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
cooperation_history = []  # list to track cooperation rates

# Round loop
for t in range(r):
    if t == 0:  # First round
        action = C (cooperate)
    else:
        previous_cooperators = sum(cooperation_history[-1])
        if previous_cooperators >= k - 1:  # Enough cooperators to create public good
            action = C (cooperate)
        elif previous_cooperators > n - k + 1:  # Too many defectors, punish
            action = D (defect)
        else:
            self_payoff = calculate_self_payoff(t-1)  # Calculate individual payoff from cooperation
            average_payoff = sum(calculate_payoffs(t-1)) / n  # Calculate average payoff of all players
            if self_payoff < average_payoff:  # Self-protection
                action = D (defect)
            else:
                action = C (cooperate)

    cooperation_history.append(action == C)  # Update cooperation history

# End of round loop
```
ARA adapts to changing opponent behaviors and balances individual self-interest with collective cooperation, making it a robust strategy for this N-Player Public Goods Game.
'''

description_COLLECTIVE_48 = '''
**Collective Strategy: Adaptive Tit-for-Tat with Public Goods Incentives (ATPGI)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a baseline for cooperation and encourage others to follow suit.
2. **Subsequent Rounds:** Use the following rules:
	* If the number of cooperators in the previous round is above the threshold (`n * k / (k + 1)`), cooperate (C). This encourages continued cooperation when the public good is being sufficiently contributed to.
	* If the number of cooperators in the previous round is below the threshold, defect (D) with a probability `p = 1 - (number of cooperators / n)`. This allows for adaptation to changing opponent behaviors and prevents exploitation by free-riders.
3. **Last Round:** Defect (D), as there is no future opportunity to influence others' behavior.

**Handling Edge Cases:**

* If the number of players (`n`) is 2, use a simple Tit-for-Tat strategy, as the public goods incentives are less relevant in this case.
* If the multiplication factor (`k`) is close to 1 (i.e., `k ≈ 1`), prioritize cooperation (C) more strongly, as the benefits of cooperation are relatively low.

**Collective Mindset:**

The ATPGI strategy aligns with a collective mindset by:

* Encouraging cooperation in early rounds to establish a norm
* Adapting to changing opponent behaviors while maintaining a willingness to cooperate
* Prioritizing public goods incentives when they are sufficient, and defecting otherwise

This approach balances individual self-interest with the need for collective cooperation, making it robust against various opponent strategies.

**Pseudocode:**
```
function ATPGI(n, k, history):
  if round == 1:
    return C
  else:
    num_cooperators_prev = count_cooperators(history[-1])
    threshold = n * k / (k + 1)
    
    if num_cooperators_prev >= threshold:
      return C
    else:
      p = 1 - (num_cooperators_prev / n)
      return D with probability p, otherwise return C
    
  # Last round handling
  if round == r:
    return D
```
This strategy is designed to perform well in a tournament setting against various independent strategies, without relying on specific coordination mechanisms or shared norms.
'''

description_COLLECTIVE_49 = '''
**Collective Strategy: Adaptive Cooperative Fingerprint (ACF)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and gather information about opponents' behaviors.
2. **Exploration Phase**: For rounds 2 to r/4, play C with probability p = (k/n) \* (number of cooperators in previous round) / n. This phase helps identify potential cooperators and estimates the effectiveness of cooperation.
3. **Adaptation Phase**: After the exploration phase, use the following rules:
	* If the average payoff per cooperator in the previous round is greater than or equal to 1 + (k/n), continue playing C with probability p = (average payoff per cooperator) / n.
	* Otherwise, play D with probability p = 1 - ((average payoff per cooperator) / n).
4. **Punishment Mechanism**: If an opponent defects after a cooperative round, play D for the next two rounds to signal disapproval and deter future defections.

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round to maximize collective payoff, as there is no opportunity for retaliation or adaptation.
2. **Tiebreakers**: In cases where multiple strategies have the same expected payoff, favor cooperation (C) to promote collective well-being.

**Collective Mindset:**

The Adaptive Cooperative Fingerprint (ACF) strategy aims to balance individual self-interest with collective prosperity. By initially cooperating and adapting to opponents' behaviors, ACF promotes a culture of cooperation while maintaining robustness against various opponent strategies.

**Pseudocode:**
```markdown
# Initialize variables
cooperators = 0
payoffs = []
p_cooperate = (k/n)

# First round
if round == 1:
    action = C

# Exploration phase
elif 2 <= round < r/4:
    p_cooperate = (k/n) * cooperators / n
    if random() < p_cooperate:
        action = C
    else:
        action = D

# Adaptation phase
else:
    avg_payoff_per_cooperator = sum(payoffs[previous_round]) / cooperators
    if avg_payoff_per_cooperator >= 1 + (k/n):
        p_cooperate = avg_payoff_per_cooperator / n
    else:
        p_cooperate = 1 - (avg_payoff_per_cooperator / n)
    
    if random() < p_cooperate:
        action = C
    else:
        action = D

# Punishment mechanism
if opponent_defected and previous_action == C:
    action = D for next two rounds

# Last round
if round == r:
    action = C
```
The ACF strategy is designed to be robust, adaptive, and collective-oriented, making it a competitive entry in the tournament against other AI systems.
'''

description_COLLECTIVE_50 = '''
I propose a collective strategy called "Adaptive Cooperate-Defect" (ACD). ACD balances individual self-interest with cooperative behavior, adapting to the game's history and opponent actions.

**Decision Rules:**

1. **Initial Rounds:** In the first round, cooperate (C) unconditionally.
2. **Cooperation Threshold:** Introduce a cooperation threshold, `τ` (tau), which is initialized to 0.5. This value represents the minimum proportion of cooperators required for an individual player to cooperate.
3. **Round t:** For each subsequent round t > 1:
	* Observe the number of cooperators in the previous round, `c_prev`.
	* Calculate the cooperation rate, `ρ` (rho), as `c_prev / n`.
	* If `ρ >= τ`, cooperate (C). Otherwise, defect (D).
4. **Adaptation Mechanism:** Update the cooperation threshold, `τ`, based on the game's history:
	+ If the total payoff of cooperators in the previous round is higher than the total payoff of defectors, increase `τ` by 0.1 (up to a maximum of 1).
	+ If the total payoff of cooperators is lower, decrease `τ` by 0.1 (down to a minimum of 0).

**Edge Cases:**

* **Last Round:** In the final round, defect (D) unconditionally.
* **Ties:** In case of a tie in the cooperation rate, cooperate (C).
* **Zero Cooperators:** If there were no cooperators in the previous round, set `τ` to 0.5.

**Collective Mindset:**

ACD aligns with a collective mindset by:

* Initially cooperating unconditionally to encourage cooperation.
* Adapting to the game's history and opponent actions, allowing for flexibility and responsiveness.
* Gradually increasing or decreasing the cooperation threshold based on the relative payoffs of cooperators and defectors.

**Pseudocode:**
```python
def ACD(n, k, r):
    τ = 0.5  # initial cooperation threshold
    c_prev = 0  # number of cooperators in previous round
    
    for t in range(1, r + 1):
        if t == 1:
            action = 'C'  # cooperate unconditionally in first round
        else:
            ρ = c_prev / n
            if ρ >= τ:
                action = 'C'
            else:
                action = 'D'
        
        # update cooperation threshold based on game history
        total_coop_payoff = sum([payoff_i for i, payoff_i in enumerate(payoffs) if actions[i] == 'C'])
        total_defect_payoff = sum([payoff_i for i, payoff_i in enumerate(payoffs) if actions[i] == 'D'])
        
        if total_coop_payoff > total_defect_payoff:
            τ = min(1, τ + 0.1)
        elif total_coop_payoff < total_defect_payoff:
            τ = max(0, τ - 0.1)
        
        # update number of cooperators for next round
        c_prev = sum([1 for action_i in actions if action_i == 'C'])
    
    return action
```
This pseudocode provides a basic implementation of the ACD strategy. It will be refined and optimized during the algorithm development phase.

By employing an adaptive cooperation threshold, ACD balances individual self-interest with cooperative behavior, making it a robust and effective collective strategy for the N-Player Public Goods Game.
'''

description_COLLECTIVE_51 = '''
**Collective Strategy: Adaptive Cooperativity (AC)**

The Adaptive Cooperativity strategy aims to balance individual payoff maximization with collective welfare, adapting to the evolving game dynamics and opponent behaviors.

**Decision Rules:**

1. **Initial Round**: In the first round, AC starts with a cooperative action (C) to establish a positive tone and encourage cooperation.
2. **Cooperation Threshold**: Define a cooperation threshold `θ` as a function of the number of players (n) and multiplication factor (k): `θ = (n - k) / n`. This threshold represents the minimum proportion of cooperators required for AC to cooperate in subsequent rounds.
3. **Payoff-Based Adaptation**: After each round, calculate the average payoff per player (`avg_payoff`) and compare it to the maximum possible payoff (`max_payoff = 1 + (k/n) * n`). If `avg_payoff` is above `θ * max_payoff`, AC cooperates in the next round; otherwise, it defects.
4. **Opponent Behavior Analysis**: Keep track of the number of opponents who cooperated in each round (`coop_opponents`). If this number exceeds `(n - 1) / 2` (i.e., more than half of the opponents cooperate), AC cooperates in the next round; otherwise, it defects.
5. **Robustness Mechanism**: To prevent exploitation by defecting opponents, introduce a "punishment" phase. If AC has defected for two consecutive rounds and `coop_opponents` is below `(n - 1) / 2`, AC will cooperate in the next round to test if opponents are willing to reciprocate.

**Pseudocode:**
```markdown
# Initialize variables
θ = (n - k) / n
avg_payoff_prev = 0
coop_opponents_prev = 0
punishment_phase = False

# First round
action[1] = C

# Subsequent rounds
for t = 2 to r:
    # Calculate average payoff and compare to threshold
    avg_payoff = (Σ(j=1 to n) π_j,t-1) / n
    if avg_payoff > θ * max_payoff:
        action[t] = C
    else:
        action[t] = D

    # Analyze opponent behavior
    coop_opponents = count opponents who cooperated in round t-1
    if coop_opponents > (n - 1) / 2:
        action[t] = C

    # Robustness mechanism
    if punishment_phase and action[t-1] == D and coop_opponents < (n - 1) / 2:
        action[t] = C
        punishment_phase = False
    elif action[t-1] == D and action[t-2] == D:
        punishment_phase = True

# Last round
action[r] = D  # Defect in the final round to maximize individual payoff
```
**Edge Cases:**

* In the last round, AC defects to maximize individual payoff.
* If all opponents cooperate, AC will also cooperate to achieve the highest collective payoff.

The Adaptive Cooperativity strategy balances cooperation and defection based on game dynamics and opponent behaviors. By adapting to changing circumstances, AC aims to promote collective welfare while protecting against exploitation by defecting opponents.
'''

description_COLLECTIVE_52 = '''
**Collective Strategy: Adaptive Cooperation with Retaliation (ACR)**

The ACR strategy is designed to balance individual self-interest with collective well-being, adapting to various opponent behaviors while promoting cooperation.

**Decision Rules:**

1. **Initial Round (t=1): Cooperate (C)**
Start by cooperating in the first round to establish a baseline for mutual benefit.
2. **Subsequent Rounds (t>1):**
	* If the average payoff of all players in the previous round is above the threshold `k/n`, **Cooperate (C)**.
	* Otherwise, if any player defected in the previous round, **Defect (D)** to punish non-cooperators and maintain accountability.
3. **Retaliation Mechanism:**
If a player defects while others cooperate, ACR will defect for one round as retaliation. This aims to prevent exploitation and encourage cooperation.

**Edge Cases:**

* **Last Round (t=r): Cooperate (C)**
Cooperate in the final round to maximize collective payoff, as there are no further opportunities for retaliation or long-term benefits.
* **Tiebreaker:**
In cases where multiple players have the same average payoff, ACR will cooperate if the majority of players cooperated in the previous round.

**Collective Mindset Alignment:**

ACR prioritizes cooperation when it is beneficial to all players (average payoff above `k/n`). By retaliating against defectors, ACR encourages opponents to cooperate, ultimately leading to higher collective payoffs. This strategy promotes a culture of mutual support and accountability within the game.

Pseudocode:
```markdown
function ACR(n, k, r, history):
  # Initialize variables
  threshold = k/n
  previous_payoffs = []
  retaliation_flag = False

  for t in range(1, r+1):
    if t == 1:  # Initial Round
      action = C
    else:
      average_payoff = sum(previous_payoffs) / n
      if average_payoff >= threshold:
        action = C
      elif any(player.defected for player in history[t-1]):
        action = D  # Retaliate against defectors
      else:
        action = C

    previous_payoffs.append(average_payoff)

    # Update retaliation flag
    if any(player.defected for player in history[t]) and not retaliation_flag:
      retaliation_flag = True
    elif not any(player.defected for player in history[t]):
      retaliation_flag = False

  return action
```
This strategy is designed to be adaptive, robust, and collective, aiming to maximize the overall payoff while promoting cooperation among players.
'''

description_COLLECTIVE_53 = '''
**Collective Strategy: Adaptive Cooperation with Reputation-Based Trust**

Our collective strategy aims to balance individual self-interest with collective welfare by adaptively cooperating based on the reputation of fellow players.

**Decision Rules:**

1. **Initial Round (t=1): Cooperate**
In the first round, all players start with a clean slate. We cooperate to establish a baseline level of trust and encourage others to do the same.
2. **Reputation-Based Cooperation**
For subsequent rounds (t>1), we use a reputation system to track each player's cooperation history. Let `R_i` denote the reputation of player i, initialized to 0.

a. If player i cooperated in the previous round (`c_i,t-1 = 1`), increase their reputation by 1: `R_i += 1`.
b. If player i defected in the previous round (`c_i,t-1 = 0`), decrease their reputation by 1, but not below -n: `R_i -= 1`, with a floor of `-n`.

Cooperate if:

* The average reputation of all players (`avg_R`) is above a threshold (`θ`): `avg_R > θ`.
* The current player's reputation (`R_i`) is non-negative.

Defect otherwise.

**Pseudocode:**
```python
def get_action(t, c_prev, R):
    if t == 1:
        return COOPERATE
    
    avg_R = sum(R) / len(R)
    
    for i in range(len(c_prev)):
        if c_prev[i] == COOPERATE:
            R[i] += 1
        else:
            R[i] -= 1
            R[i] = max(R[i], -n)
    
    if avg_R > θ and R[current_player_index] >= 0:
        return COOPERATE
    else:
        return DEFECT
```
**Edge Cases:**

* **Last Round (t=r): Defect**
In the final round, there is no incentive to cooperate since there will be no future rounds to benefit from a good reputation.
* **Tiebreaker:** In case of a tie in average reputation (`avg_R == θ`), defect to prioritize individual self-interest.

**Collective Mindset:**

Our strategy promotes cooperation by establishing a shared understanding that:

1. Cooperation is rewarded with increased reputation.
2. Defection is penalized with decreased reputation.
3. A player's actions influence not only their own payoff but also the collective welfare.

By aligning individual interests with collective well-being, our adaptive cooperation strategy fosters a cooperative environment that benefits all players while remaining robust to various opponent behaviors.
'''

description_COLLECTIVE_54 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy, "Adaptive Cooperation," balances individual self-interest with the desire for collective prosperity. We aim to maximize overall payoffs by adapting to the evolving game environment and the behavior of our opponents.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to establish a baseline level of cooperation and encourage others to reciprocate.
2. **Observation Phase (2 < t ≤ r/3):** Observe the actions of other players and calculate the average cooperation rate (ACR) over the previous rounds.

`ACR = Σ(c_j, t-1 to t-3) / (n * 3)`

where `c_j` is the cooperation indicator for player j in a given round.

3. **Adaptive Phase (r/3 < t ≤ r):** Cooperate if the ACR exceeds a threshold value (`θ`) and defect otherwise.

`Cooperate if ACR > θ = k/n`

This threshold ensures that we cooperate when there is sufficient collective investment in the public good, but also allows us to adapt to decreasing cooperation rates.
4. **Punishment Mechanism:** If another player defects while you cooperated in the previous round (`t-1`), defect in the current round (`t`). This discourages exploitation and encourages reciprocity.

**Edge Cases:**

* In the last round (r = `last_round`): Cooperate if the ACR is above the threshold (`θ`); otherwise, defect.
* If all players defected in a previous round: Defect for one round to signal that cooperation is not viable; then revert to the adaptive phase.

**Collective Mindset:**

Our strategy prioritizes collective prosperity while protecting individual interests. By cooperating initially and adapting to the behavior of others, we create an environment conducive to mutual benefit. The punishment mechanism ensures that exploitation is discouraged, promoting a culture of reciprocity.

Pseudocode:
```
Initialize variables
acr = 0 (average cooperation rate)
θ = k/n (cooperation threshold)

For each round t
  If t ≤ 2: Cooperate
  Else if 2 < t ≤ r/3: Observe and calculate acr
    acr = Σ(c_j, t-1 to t-3) / (n * 3)
  Else: Adaptive phase
    If acr > θ: Cooperate
    Else: Defect

  Check for punishment mechanism
    If another player defected while you cooperated in previous round:
      Defect in current round

  Update acr and adapt strategy as needed
```
This collective strategy balances individual self-interest with the desire for mutual benefit, making it a robust and adaptive solution for the N-player Public Goods Game.
'''

description_COLLECTIVE_55 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) with a probability of 0.5. This allows us to gather information about other players' behaviors without committing to a specific strategy too early.
2. **Contribution Threshold**: Calculate the average cooperation rate (CR) of all players over the last `m` rounds, where `m` is a small fraction of the total rounds (`m = r/10`). If CR > 0.5, cooperate; otherwise, defect.
3. **Recent Behavior Adaptation**: Monitor the number of cooperators in the previous round (`c_prev`). If `c_prev` ≥ `n/2`, increase the cooperation probability by 0.1 (up to a maximum of 1). Conversely, if `c_prev` < `n/4`, decrease the cooperation probability by 0.1 (down to a minimum of 0).
4. **Last Round**: In the final round (`t = r`), defect (D) unconditionally.

**Pseudocode:**
```python
def adaptive_cooperation(n, k, r):
    m = r / 10  # number of rounds for contribution threshold calculation
    coop_prob = 0.5  # initial cooperation probability

    for t in range(1, r+1):
        if t == 1:
            action = 'C' if random.random() < coop_prob else 'D'
        else:
            avg_CR = calculate_average_cooperation_rate(m)
            if avg_CR > 0.5:
                action = 'C'
            else:
                action = 'D'

            # Recent behavior adaptation
            c_prev = count_cooperators_in_previous_round()
            if c_prev >= n / 2:
                coop_prob = min(coop_prob + 0.1, 1)
            elif c_prev < n / 4:
                coop_prob = max(coop_prob - 0.1, 0)

        # Last round
        if t == r:
            action = 'D'

        return action

def calculate_average_cooperation_rate(m):
    # Calculate the average cooperation rate over the last `m` rounds
    # ( implementation omitted for brevity )
```
**Rationale:**

1. The initial random cooperation allows us to gather information about other players without committing to a specific strategy.
2. The contribution threshold adapts our behavior based on the overall cooperation level, promoting collective cooperation when many others cooperate.
3. Recent behavior adaptation adjusts our cooperation probability in response to changes in the number of cooperators, allowing us to respond to shifts in the group's behavior.
4. Defecting in the final round ensures we don't contribute unnecessarily, as there are no future rounds to benefit from cooperation.

This strategy balances individual self-interest with collective well-being, making it a robust and adaptive approach for a wide range of opponent behaviors.
'''

description_COLLECTIVE_56 = '''
**Collective Strategy: Adaptive Cooperation with Social Learning**

This strategy aims to balance individual self-interest with collective well-being by adaptively adjusting cooperation levels based on observed opponent behavior and game history.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to set a cooperative tone and encourage reciprocity.
2. **Subsequent Rounds (t>1):**
	* If the average cooperation rate of opponents in the previous round is above a threshold (`theta`), cooperate (C). Otherwise, defect (D).
	* `theta` is dynamically adjusted based on the game's history:
		+ Increase `theta` by 0.05 if the collective payoff in the previous round was higher than the average payoff of the last 3 rounds.
		+ Decrease `theta` by 0.05 if the collective payoff in the previous round was lower than the average payoff of the last 3 rounds.
3. **Last Round (t=r):** Defect (D) to maximize individual payoff, as there is no future interaction.

**Edge Cases:**

* If all opponents defected in the previous round, defect (D) in the current round to avoid exploitation.
* If the game has only two players (n=2), always cooperate (C) to ensure mutual benefit.

**Pseudocode:**
```python
def adaptive_cooperation(n, k, r, history):
    theta = 0.5  # initial cooperation threshold

    for t in range(1, r+1):
        if t == 1:
            action = 'C'  # cooperate in the first round
        else:
            avg_opponent_coop = sum(history[t-1]['coop_rates']) / n
            if avg_opponent_coop > theta:
                action = 'C'
            else:
                action = 'D'

        # adjust theta based on collective payoff
        if t > 3:
            avg_payoff_last_3_rounds = sum([history[i]['payoffs'][0] for i in range(t-3, t)]) / 3
            if history[t-1]['payoffs'][0] > avg_payoff_last_3_rounds:
                theta += 0.05
            elif history[t-1]['payoffs'][0] < avg_payoff_last_3_rounds:
                theta -= 0.05

        # edge cases
        if all(history[t-1]['actions'] == 'D'):
            action = 'D'
        if n == 2:
            action = 'C'

    return action
```
This strategy is designed to be adaptive, collective, and robust to various opponent behaviors. By adjusting the cooperation threshold based on observed opponent behavior and game history, it aims to balance individual self-interest with collective well-being.
'''

description_COLLECTIVE_57 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Rounds (t ≤ n):** Cooperate (C) in the first 'n' rounds to encourage cooperation and build trust among players.
2. **Contribution-Based Cooperation:** After the initial rounds, cooperate if the total contributions from the previous round are above a certain threshold, calculated as `(k/n) * (n/2)`. This ensures that the collective benefit is substantial enough to warrant cooperation.
3. **Punishment Mechanism:** If the total contributions in the previous round fall below the threshold, defect (D) in the current round. This serves as a gentle punishment to encourage other players to contribute more.
4. ** Forgiveness and Adaptation:** If the total contributions in the previous round exceed the threshold after a punishment round, cooperate again. This allows for adaptability and forgiveness, enabling the collective to recover from temporary setbacks.

**Pseudocode:**
```python
def ACO(n, k, t, history):
  # Initial rounds (t <= n)
  if t <= n:
    return 'C'
  
  # Calculate threshold for cooperation
  threshold = (k/n) * (n/2)
  
  # Get total contributions from previous round
  prev_contributions = sum([1 if action == 'C' else 0 for action in history[t-1]])
  
  # Contribution-based cooperation
  if prev_contributions >= threshold:
    return 'C'
  
  # Punishment mechanism
  elif prev_contributions < threshold and history[t-2] != ['D'] * n:
    return 'D'
  
  # Forgiveness and adaptation
  else:
    return 'C'
```
**Handling Edge Cases:**

* **First Round:** Cooperate (C) to set a positive tone for the game.
* **Last Round (t = r):** Defect (D), as there's no future round to consider, and individual payoffs are maximized by defecting.
* **Ties:** In case of ties in total contributions, cooperate (C) to prioritize collective optimism.

**Collective Mindset:**
ACO prioritizes cooperation when the collective benefit is substantial, while also allowing for gentle punishment and adaptation. By forgiving temporary setbacks and encouraging cooperation, ACO promotes a collective mindset that balances individual self-interest with group well-being.

This strategy should perform robustly in a tournament setting against independent strategies developed by other AI systems, as it adapts to various opponent behaviors without relying on coordination mechanisms or shared norms.
'''

description_COLLECTIVE_58 = '''
I'll outline a collective strategy for this N-Player Public Goods Game that adapts to various opponent behaviors and focuses on achieving a high total payoff.

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage others to cooperate and create a positive feedback loop.
2. **Early Rounds (Rounds 2-5):**
	* If the majority of players (> n/2) cooperated in the previous round, continue cooperating (C).
	* Otherwise, defect (D) to signal that cooperation is not paying off.
3. **Middle Rounds (Rounds 6-r/2):**
	* Calculate the average payoff for all players over the last 3 rounds.
	* If the average payoff is above a threshold (e.g., n/2), cooperate (C).
	* Otherwise, defect (D) to minimize losses.
4. **Late Rounds (Rounds r/2-r-1):**
	* Assess the total number of cooperators in the previous round:
		+ If most players (> n/2) cooperated, cooperate (C) to maximize collective payoff.
		+ Otherwise, defect (D) to avoid being exploited.
5. **Last Round:** Defect (D), as there is no future opportunity for retaliation or reciprocity.

**Additional Rules:**

* **Punishment Mechanism:** If a player defects while most others cooperate (> n/2), ACC will punish that player by defecting in the next round, regardless of other players' actions.
* **Forgiveness:** After punishing a player, ACC will revert to its standard decision rules.

**Collective Mindset:**
The Adaptive Collective Cooperation strategy aims to create a positive feedback loop where cooperation becomes self-reinforcing. By initially cooperating and adapting to the behavior of others, ACC encourages other players to cooperate as well. If opponents exploit this cooperative approach, ACC will adjust its strategy to minimize losses while still promoting collective cooperation.

**Pseudocode:**
```python
def ACC(player_id, n, k, r, history):
    if round == 1:
        return C
    elif round <= 5:
        prev_round_cooperators = count_cooperators(history[-1])
        if prev_round_cooperators > n / 2:
            return C
        else:
            return D
    elif round <= r / 2:
        avg_payoff = average_payoff(history[-3:])
        if avg_payoff >= n / 2:
            return C
        else:
            return D
    elif round < r - 1:
        prev_round_cooperators = count_cooperators(history[-1])
        if prev_round_cooperators > n / 2:
            return C
        else:
            return D
    else:  # Last round
        return D

def count_cooperators(actions):
    return sum(1 for action in actions if action == C)

def average_payoff(payoffs):
    return sum(payoffs) / len(payoffs)
```
This strategy balances cooperation and self-interest, adapting to the behavior of other players while promoting a collective good. By being robust to various opponent behaviors and incorporating punishment and forgiveness mechanisms, ACC aims to perform well in a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_59 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to reciprocate.
2. **Reciprocal Cooperation**: If the majority of players (> n/2) cooperated in the previous round, cooperate (C) in the current round.
3. **Adaptive Defection**: If the number of defectors in the previous round is greater than or equal to the number of cooperators, defect (D) in the current round.
4. **Punishment Mechanism**: If a player has defected in the previous round and the total payoff for all players decreased as a result, defect (D) against that player in the next round.
5. **Exploration Phase**: After every 5 rounds, play Cooperate (C) for one round to reassess the group's willingness to cooperate.

**Handling Edge Cases:**

* **Last Round**: Defect (D) in the last round, as there is no future benefit to cooperation.
* **Tie Situations**: In case of a tie (e.g., equal number of cooperators and defectors), follow the reciprocal cooperation rule (Rule 2).
* **Consecutive Defections**: If all players defect consecutively for more than 3 rounds, reset to Initial Round strategy (Rule 1).

**Collective Mindset Alignment:**

The Adaptive Cooperation strategy prioritizes collective well-being by encouraging cooperation while being robust against exploitation. By reciprocating cooperation and punishing defection, AC promotes a stable cooperative environment.

Pseudocode:

```
Function AdaptiveCooperation(n, k, history):
    if (currentRound == 1) {
        return COOPERATE; // Initial Round
    }

    previousCooperators = countCooperators(history);
    previousDefectors = n - previousCooperators;

    if (previousCooperators > n/2) {
        return COOPERATE; // Reciprocal Cooperation
    } else if (previousDefectors >= previousCooperators) {
        return DEFECT; // Adaptive Defection
    }

    if (punishmentMechanism(history)) {
        return DEFECT; // Punish defector
    }

    if (currentRound % 5 == 0) { // Exploration Phase
        return COOPERATE;
    }

    // Handle tie situations and consecutive defections
    ...
```

This strategy should perform well in the tournament, as it balances cooperation with robustness against exploitation. The adaptive nature of the strategy allows it to respond effectively to various opponent behaviors.
'''

description_COLLECTIVE_60 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and depends solely on game parameters and history:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability p_init = 0.5. This allows for exploration of the opponent's behavior and sets a neutral tone.
2. **Subsequent Rounds (t>1):**
	* Calculate the average payoff per cooperator in the previous round: avg_payoff_c = (∑(π_i,t-1 | c_i,t-1 = 1) / ∑c_i,t-1)
	* If avg_payoff_c ≥ k/n, cooperate (C). Otherwise, defect (D).
3. **Punishment Mechanism:** If the average payoff per cooperator is below a certain threshold (e.g., 0.5), introduce a punishment phase. In this phase:
	+ Defect (D) for a fixed number of rounds (e.g., 2-3 rounds)
	+ After the punishment phase, return to the standard decision rule
4. **Concession Mechanism:** If a player observes that their payoff is consistently lower than others, they may concede and cooperate more frequently:
	+ Calculate the average payoff difference between themselves and other players: avg_payoff_diff = (∑(π_i,t-1 - π_j,t-1) / (n-1))
	+ If avg_payoff_diff > 0.5, increase cooperation probability by a small amount (e.g., 0.05)

**Handling Edge Cases:**

* **Last Round (t=r):** Cooperate (C) if the total number of cooperators in the previous round is above a certain threshold (e.g., n/2). Otherwise, defect (D).
* **Ties:** In case of a tie in average payoff per cooperator, cooperate (C).

**Collective Mindset:**

The ACC strategy aims to balance individual self-interest with collective well-being. By adapting to the opponent's behavior and adjusting cooperation levels based on payoffs, the strategy promotes cooperation while minimizing exploitation.

Pseudocode for the decision rules:
```python
def ACC(n, k, r, history):
    # Initialize variables
    avg_payoff_c = 0
    punishment_phase = False
    concession_phase = False

    # First round: random cooperation
    if t == 1:
        return C with probability 0.5

    # Calculate average payoff per cooperator in previous round
    for i in range(n):
        if history[t-1][i] == C:
            avg_payoff_c += history[t-1].payoffs[i]
    avg_payoff_c /= sum(c_i,t-1)

    # Standard decision rule
    if avg_payoff_c >= k/n:
        return C
    else:
        return D

    # Punishment mechanism
    if avg_payoff_c < 0.5 and not punishment_phase:
        punishment_phase = True
        for _ in range(2-3 rounds):
            return D
        punishment_phase = False

    # Concession mechanism
    if avg_payoff_diff > 0.5 and not concession_phase:
        concession_phase = True
        cooperation_probability += 0.05

# Last round special case
if t == r:
    if sum(history[t-1].actions) >= n/2:
        return C
    else:
        return D
```
This strategy will be implemented as an algorithm and participate in a tournament against other AI systems.
'''

description_COLLECTIVE_61 = '''
**Collective Strategy: "Adaptive Cooperative Escalation" (ACE)**

**Overview**

ACE is a robust and adaptive collective strategy that balances cooperation and defection based on game history and parameters. The goal is to maximize collective payoff while maintaining individual rationality.

**Decision Rules**

1. **First Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds**: Observe the number of cooperators (c) and defectors (d) in the previous round. Calculate the cooperation ratio: `cr = c / n`.
3. **Cooperation Threshold**: Define a threshold value `T` based on the multiplication factor `k`. Set `T = k / n`.
4. **Escalation Mechanism**: If `cr >= T`, cooperate (C) in the current round, expecting others to reciprocate.
5. **Deterrence Mechanism**: If `cr < T`, defect (D) in the current round, signaling that cooperation is not paying off.

Pseudocode:
```
if first_round:
    action = C
else:
    cr = c / n  # cooperation ratio
    if cr >= k / n:  # escalation mechanism
        action = C
    else:  # deterrence mechanism
        action = D
```

**Edge Cases**

1. **Last Round**: In the last round, always defect (D) to maximize individual payoff.
2. **Tiebreakers**: If `cr == T`, randomly choose between cooperate and defect.

**Collective Mindset Alignment**

ACE aligns with the collective mindset by:

1. Cooperating in the first round to establish a cooperative tone.
2. Encouraging cooperation through the escalation mechanism when the cooperation ratio is high enough.
3. Signaling the need for cooperation through the deterrence mechanism when the cooperation ratio is low.

**Robustness and Adaptability**

ACE adapts to various opponent behaviors by:

1. Responding to changes in the cooperation ratio.
2. Adjusting its strategy based on the game's multiplication factor `k`.

This collective strategy balances individual rationality with collective payoff maximization, making it a robust and adaptive choice for the N-Player Public Goods Game tournament.
'''

description_COLLECTIVE_62 = '''
I'll design a collective strategy for this N-Player Public Goods Game that adapts to various opponent behaviors and game histories. Here's my proposed strategy:

**Strategy Name: Adaptive Collective Optimism (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.8. This initial optimism encourages cooperation and sets a positive tone for the game.
2. **Reciprocity**: If the number of cooperators in the previous round is above the average number of cooperators in all previous rounds, cooperate (C). Otherwise, defect (D).
3. **Punishment**: If the number of cooperators in the previous round is below the average number of cooperators in all previous rounds and you cooperated in that round, defect (D) for one round to "punish" non-cooperative behavior.
4. **Forgiveness**: After punishing non-cooperative behavior, revert to Reciprocity rule.

**Handling Edge Cases:**

* In the last round, cooperate (C) if the average payoff per player over all previous rounds is above 1.5. Otherwise, defect (D).
* If a player has defected for two consecutive rounds, assume they will continue to defect and adjust your strategy accordingly.
* If all players have cooperated in the previous round, cooperate (C) in the next round.

**Collective Mindset:**

ACO is designed to promote cooperation while adapting to various opponent behaviors. By cooperating initially and reciprocating cooperation, ACO encourages other players to cooperate as well. The Punishment rule discourages non-cooperative behavior, and Forgiveness allows for recovery from temporary setbacks. This strategy aligns with the collective mindset by:

* Encouraging cooperation through initial optimism
* Reciprocating cooperation to sustain collective benefits
* Punishing non-cooperation to maintain fairness
* Forgiving past transgressions to promote continued cooperation

**Pseudocode:**

```
function ACO(n, k, r, history):
  # Initialize variables
  avg_cooperators = 0
  prev_round_cooperators = 0
  punish_defect = False

  for t in range(r):
    if t == 0:
      # Initial cooperation
      action = 'C' with probability 0.8
    else:
      avg_cooperators = sum(history[t-1]['cooperators']) / n
      prev_round_cooperators = history[t-1]['cooperators']

      if avg_cooperators > prev_round_cooperators:
        action = 'C'
      elif punish_defect and prev_round_cooperators < avg_cooperators:
        action = 'D'
        punish_defect = False
      else:
        action = 'D'

    # Update history and variables
    history[t] = {'cooperators': sum(action == 'C' for action in actions)}
    avg_cooperators = (avg_cooperators * t + prev_round_cooperators) / (t + 1)

    if action == 'D' and prev_round_cooperators < avg_cooperators:
      punish_defect = True

  return actions
```

This strategy will be implemented as an algorithm to compete in the tournament. ACO is designed to be robust and adaptable, encouraging cooperation while responding to various opponent behaviors.
'''

description_COLLECTIVE_63 = '''
Here's a collective strategy that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name: Adaptive Collective Optimism (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to establish a cooperative tone and encourage others to follow suit.
2. **Reciprocity**: If the number of cooperators in the previous round is above the average cooperation rate (i.e., n/2), cooperate (C) in the current round. This encourages cooperation when it's already prevalent.
3. **Adaptive Optimism**: Calculate the average payoff per cooperator (APPC) in the previous round:

   APPC = (Total Payoff of Cooperators / Number of Cooperators)

   If APPC is greater than or equal to the expected payoff from defecting (1 + k/n \* Average Cooperation Rate), cooperate (C) in the current round. This adjusts the strategy based on the recent success of cooperation.
4. **Punish Defection**: If the number of cooperators in the previous round is below the average cooperation rate, and the average payoff per cooperator is lower than the expected payoff from defecting, defect (D) in the current round to signal dissatisfaction with the lack of cooperation.

**Edge Cases:**

* **Last Round**: Cooperate (C) unconditionally in the last round to maximize collective payoff.
* **First Few Rounds**: In rounds 2-5 (or a small fraction of r), use a more optimistic approach by cooperating if the number of cooperators is above n/4. This allows for exploration and encourages cooperation early on.

**Collective Mindset:**

ACO prioritizes cooperation when it's likely to be successful, while also adapting to changing circumstances. By reciprocating cooperation and punishing defection, ACO promotes a collective mindset that balances individual self-interest with the need for cooperation.

Pseudocode:
```python
def ACO(n, k, r):
  # Initialize variables
  cooperate = True  # First round cooperation
  avg_coop_rate = n / 2
  appc_prev = None

  for t in range(r):
    if t == 0:  # First round
      cooperate = True
    elif t < 5 and r > 10:  # Early rounds exploration
      cooperate = (num_cooperators >= n/4)
    else:
      # Reciprocity and Adaptive Optimism
      avg_payoff_prev = calculate_avg_payoff(prev_round_payoffs)
      appc_prev = avg_payoff_prev / num_cooperators

      if appc_prev >= (1 + k/n * avg_coop_rate):
        cooperate = True
      elif num_cooperators < avg_coop_rate and appc_prev < (1 + k/n * avg_coop_rate):
        cooperate = False  # Punish Defection
      else:
        cooperate = (num_cooperators > avg_coop_rate)

    if t == r - 1:  # Last round
      cooperate = True

    # Play the game with the chosen action
    action = "C" if cooperate else "D"
    play_game(action)
```
This strategy balances individual self-interest with collective well-being, adapting to various opponent behaviors while prioritizing cooperation when it's likely to succeed.
'''

description_COLLECTIVE_64 = '''
**Collective Strategy: Adaptive Cooperation with Social Learning**

This strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and social learning from others.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) unconditionally to establish a cooperative tone.
2. **Subsequent Rounds (t>1):**
	* If the previous round's total payoff for all players is greater than or equal to the average payoff of the game so far, and at least one player cooperated: Cooperate (C).
	* Otherwise: Defect (D).

Pseudocode:
```
IF t == 1 THEN
    action = C
ELSE IF Σ(π_j,t-1) ≥ AVG-payoff-so-far AND ∃j: c_j,t-1 == 1 THEN
    action = C
ELSE
    action = D
END IF
```
**Social Learning Mechanism:**

* Observe the actions and payoffs of all players in each round.
* Update a running average of the game's payoff to assess collective performance.

Pseudocode:
```python
# Initialize variables
total_payoff_avg = 0

# After each round, update total_payoff_avg
total_payoff_avg = (total_payoff_avg * (t - 1) + Σ(π_j,t)) / t
```
**Edge Cases:**

* **Last Round (t=r):** Cooperate unconditionally to maximize collective payoff.
Pseudocode:
```python
IF t == r THEN
    action = C
END IF
```
This strategy is designed to be:

1. **Adaptive:** Responds to changes in the game's history and social learning from others.
2. **Robust:** Handles a wide range of opponent behaviors by conditioning cooperation on collective performance.
3. **Collective-minded:** Prioritizes collective well-being while considering individual self-interest.

In a tournament setting, this strategy will aim to find a balance between cooperation and defection, adapting to the evolving game dynamics and social learning from other players.
'''

description_COLLECTIVE_65 = '''
I propose a collective strategy called "Adaptive Collective Convergence" (ACC) that balances individual self-interest with the desire for collective cooperation.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) unconditionally to establish a positive tone and encourage others to do the same.
2. **Exploration Phase**: For rounds 2 to r/4 (where r is the total number of rounds), alternate between cooperating (C) and defecting (D). This phase helps gather information about other players' strategies and adapt to their behavior.
3. **Convergence Phase**: After the exploration phase, calculate the average payoff for all players in the previous round. If this average payoff exceeds a threshold (e.g., k/n), cooperate (C) in the next round; otherwise, defect (D). This phase promotes convergence towards cooperation when the collective benefit is evident.
4. **Reaction to Defection**: If another player defects (D) while you cooperated (C) in the same round, switch to defecting (D) for the next two rounds. This temporary punishment mechanism discourages exploitation and encourages reciprocity.

**Edge Cases:**

* Last Round: Cooperate (C) unconditionally, as there is no future interaction to influence.
* Ties in Average Payoff: If multiple players have the same average payoff, prioritize cooperating (C) to maintain a positive tone.
* Zero Contributions: If all players defect (D), contributing nothing to the public good, cooperate (C) for one round and then reassess.

**Collective Mindset Alignment:**

ACC promotes collective cooperation by:

1. **Establishing a Positive Tone**: Initial cooperation encourages others to reciprocate.
2. **Exploring Opponent Strategies**: Alternating between C and D helps understand other players' behaviors and adapt accordingly.
3. **Converging on Cooperation**: When the average payoff is high, ACC promotes collective cooperation to maximize benefits for all.
4. **Discouraging Exploitation**: Punishing defection in response to cooperation maintains a balance of power.

**Pseudocode:**
```python
def ACC(n, k, r):
    # Initial Round: Cooperate unconditionally
    if round == 1:
        return C

    # Exploration Phase (Rounds 2 to r/4)
    elif round <= r / 4:
        if round % 2 == 0:
            return D
        else:
            return C

    # Convergence Phase (After Round r/4)
    else:
        avg_payoff = calculate_avg_payoff(n, k, previous_round)
        if avg_payoff > k / n:
            return C
        else:
            return D

def reaction_to_defection(other_player_actions):
    if other_player_action == D and my_previous_action == C:
        return [D] * 2  # Switch to defecting for two rounds
```
This strategy is adaptive, robust, and focused on collective cooperation. By adjusting to the game's history and parameters, ACC aims to achieve a balance between individual self-interest and the greater good.
'''

description_COLLECTIVE_66 = '''
I'll outline a collective strategy that adapts to various opponent behaviors while prioritizing the common good. Our approach will focus on conditional cooperation and learning from history.

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to signal willingness to cooperate and gather initial information about opponents' strategies.
2. **Subsequent Rounds:** Observe the number of cooperators (c_prev) in the previous round. If c_prev ≥ (n/2), cooperate (C). Otherwise, defect (D).
3. **Conditional Cooperation:** Introduce a tolerance threshold (τ) to account for noise or irrational behavior. Set τ = 0.1 × n. When c_prev ≥ (n/2 - τ), cooperate (C); otherwise, defect (D).

**Handling Edge Cases:**

* **Last Round:** Defect (D) in the last round, as there's no opportunity to reciprocate cooperation.
* **Low Cooperation Rates:** If cooperation rates are consistently low (< 20% of players), adjust the strategy to:
	+ Cooperate (C) with a probability of 0.2 in subsequent rounds to encourage cooperation.
	+ Revert to the original decision rules if cooperation rates exceed 40%.
* **High Cooperation Rates:** When cooperation rates are consistently high (> 80% of players), maintain cooperation (C) to reinforce collective success.

**Collective Mindset Alignment:**

1. **Reciprocity:** Cooperate in response to cooperation from others, fostering a culture of mutual support.
2. **Exploration:** Periodically defect to test the resilience of opponents' strategies and adapt to changes.
3. **Learning:** Adjust the tolerance threshold (τ) based on observed opponent behavior to improve collective outcomes.

**Pseudocode:**
```python
def ACC(n, k, r):
  c_prev = None
  tau = 0.1 * n

  for t in range(1, r+1):
    if t == 1:
      action = 'C'  # Cooperate in first round
    else:
      num_cooperators = sum(c_prev)
      if num_cooperators >= (n / 2 - tau):
        action = 'C'
      else:
        action = 'D'

    # Handle low cooperation rates
    if t > 1 and sum([c == 'C' for c in c_prev]) < 0.2 * n:
      if random.random() < 0.2:
        action = 'C'
      else:
        action = 'D'

    # Update history
    c_prev[t] = action

    # Last round exception
    if t == r:
      action = 'D'

    return action
```
The Adaptive Collective Cooperation strategy balances individual self-interest with collective well-being, adapting to various opponent behaviors while maintaining a strong focus on cooperation.
'''

description_COLLECTIVE_67 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Reciprocity (ACGR)**

The ACGR strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and the behavior of other players.

**Decision Rules:**

1. **Initial Rounds (t = 1, ..., n):** Cooperate (C) to encourage initial cooperation and create a positive payoff environment.
2. **Gradual Reciprocity:** From round t = n+1 onwards, use the following decision rule:
	* If the average cooperation rate of other players in the previous round is above a threshold θ (e.g., 0.5), Cooperate (C).
	* Otherwise, Defect (D) with probability p(t), which increases as the game progresses.
3. **Reciprocity Adjustment:** After each round, update θ based on the observed cooperation rate of other players:
	* If most players cooperated (i.e., average cooperation rate > 0.5), decrease θ by a small amount (e.g., 0.05) to encourage continued cooperation.
	* If few players cooperated, increase θ by a larger amount (e.g., 0.1) to incentivize more cooperation in the next round.

Pseudocode:
```
t = current round
n = number of players
k = multiplication factor

if t <= n:
    action = C // Cooperate in initial rounds

else:
    prev_coop_rate = average cooperation rate of other players in previous round
    if prev_coop_rate > θ:
        action = C // Cooperate if others cooperated
    else:
        p_defect = min(1, p(t)) // Defect probability increases with time
        action = D with probability p_defect

// Update θ for next round
if prev_coop_rate > 0.5:
    θ -= 0.05 // Decrease threshold if cooperation is high
else:
    θ += 0.1 // Increase threshold if cooperation is low
```
**Handling Edge Cases:**

* **First Round (t = 1):** Cooperate to set a positive tone for the game.
* **Last Round (t = r):** Defect, as there are no future rounds to reciprocate.

**Collective Mindset:**
The ACGR strategy aims to create a mutually beneficial environment by:

1. Encouraging initial cooperation through unconditional cooperation in early rounds.
2. Gradually adapting to the behavior of other players, rewarding cooperation and punishing defection.
3. Adjusting the reciprocity threshold based on observed cooperation rates to maintain a balance between individual self-interest and collective well-being.

This strategy should be robust to various opponent behaviors, as it:

1. Adapts to changing cooperation rates among opponents.
2. Responds to both cooperative and defective actions from other players.
3. Balances short-term self-interest with long-term collective benefits.

By implementing the ACGR strategy, we aim to create a more cooperative environment in the N-Player Public Goods Game tournament.
'''

description_COLLECTIVE_68 = '''
**Collective Strategy: Adaptive Cooperative Tit-for-Tat (ACTT)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 50%. This encourages cooperation and tests the waters for reciprocal behavior.
2. **Tit-for-Tat Adaptation**: For rounds t > 1, if the number of cooperators in the previous round is greater than or equal to the number of defectors, cooperate (C). Otherwise, defect (D).
3. **Punishment Mechanism**: If a player defects when the majority cooperated in the previous round, punish by defecting for one round.
4. **Forgiveness**: After punishing, revert to Tit-for-Tat adaptation.

**Edge Cases:**

* **Last Round**: In the final round (r), always cooperate (C) regardless of previous rounds' outcomes. This promotes cooperation and prevents last-round exploitation.
* **Ties**: If the number of cooperators equals the number of defectors in a round, consider it a "tie." In this case, maintain the previous action (cooperate if previously cooperating, defect if previously defecting).

**Collective Mindset Alignment:**

ACTT is designed to promote cooperation and adapt to various opponent behaviors. By initially cooperating with a probability, we signal our willingness to cooperate and invite others to do so. The Tit-for-Tat adaptation mechanism encourages reciprocity, while the punishment mechanism discourages exploitation.

The collective strategy focuses on promoting cooperation rather than solely pursuing individual self-interest. This is reflected in the decision rules, which prioritize maintaining cooperation when possible and adapting to changes in the group's behavior.

**Pseudocode:**
```markdown
// Initialize variables
int cooperateProbability = 0.5; // 50% chance of cooperating initially
bool previousRoundCooperated[n]; // track previous round's actions
int numCooperators; // count cooperators in previous round

// First round
if (round == 1) {
    if (random() < cooperateProbability) {
        action = COOPERATE;
    } else {
        action = DEFECT;
    }
}

// Subsequent rounds
else {
    numCooperators = countCooperators(previousRoundActions);
    if (numCooperators >= n / 2) { // majority cooperated or tied
        action = COOPERATE;
    } else { // minority cooperated
        action = DEFECT;
    }

    // Punishment mechanism
    if (previousRoundAction == DEFECT && numCooperators > n / 2) {
        action = DEFECT; // punish for one round
    }
}

// Last round
if (round == r) {
    action = COOPERATE;
}
```
This collective strategy is designed to be robust and adaptive, promoting cooperation while responding to various opponent behaviors.
'''

description_COLLECTIVE_69 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

**Decision Rules:**

1. **Initial Round (t=1): Cooperate** - In the first round, all players cooperate to establish a baseline level of trust and cooperation.
2. **Subsequent Rounds (t>1):**
	* If the average payoff in the previous round is above the threshold (T), cooperate.
	* Otherwise, defect.

The threshold (T) is calculated as the average payoff when half of the players cooperate:

T = 0.5 + (k/n) \* (n/2)

**Punishment Mechanism:**

If a player defects in a round where more than half of the other players cooperated, they will be punished by all cooperating players in the next round.

1. **Cooperating Player's Punishment Rule:** If a player i sees that another player j defected in the previous round when more than half of the other players cooperated, player i will defect against player j in the current round.
2. **Defecting Player's Forgiveness Rule:** If a player j is punished by all cooperating players and then decides to cooperate in the next round, they will be forgiven and cooperation will resume.

**Edge Cases:**

* Last Round (t=r): Cooperate if the average payoff in the previous round is above the threshold; otherwise, defect.
* All Players Defected Previously: If all players defected in the previous round, cooperate to try to re-establish cooperation.

**Collective Mindset Alignment:**

ACP aligns with the collective mindset by:

1. Initially cooperating to establish trust and cooperation
2. Adapting to the average payoff level to ensure a reasonable share of the public good
3. Punishing defectors to maintain cooperation and fairness

This strategy is robust against various opponent behaviors, as it does not rely on specific coordination mechanisms or assumptions about other players' strategies.

Pseudocode:
```
// Initialize variables
T = 0.5 + (k/n) * (n/2)
cooperate = true
punished_players = []

// Main loop
for t = 1 to r do
    if t == 1 then
        cooperate = true
    else
        avg_payoff_prev_round = calculate_average_payoff(t-1)
        if avg_payoff_prev_round > T then
            cooperate = true
        else
            cooperate = false

    // Punishment mechanism
    for j in players do
        if player[j].defected_prev_round and more_than_half_cooperated then
            punished_players.add(j)

    // Forgiveness rule
    for j in punished_players do
        if player[j].cooperates_now then
            punished_players.remove(j)

    // Update cooperate decision based on punishment rules
    if this_player.punished_by_others then
        cooperate = false

    // Take action
    take_action(cooperate)
```
This pseudocode provides a basic outline of the ACP strategy. The `calculate_average_payoff` function and `more_than_half_cooperated` condition will need to be implemented according to the specific game parameters and history.
'''

description_COLLECTIVE_70 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Concession (ACGC)**

The ACGC strategy aims to balance individual payoff maximization with collective cooperation, adapting to the game's history and opponents' behaviors.

**Decision Rules:**

1. **Initial Rounds (t = 1 to n): Cooperate**
In the first 'n' rounds, play C to encourage others to cooperate and build a collective momentum.
2. **Monitor Cooperation Levels**: Track the number of cooperators in each round (c_t).
3. **Adaptive Threshold**: Set an adaptive threshold 'θ' based on the game's history:
	* If c_t > n/2 (more than half cooperate), θ = 1 - ε (ε is a small positive value, e.g., 0.05). This indicates high cooperation and justifies continued cooperation.
	* If c_t ≤ n/2, θ = (k/n) \* (c_t / n). This adjusts the threshold based on the observed level of cooperation.
4. **Cooperate or Defect**: In round 't', play:
	* C if π_t-1 ≥ θ or if t is the last round (r).
	* D otherwise.

**Pseudocode:**
```
 Initialize: c_history = [], θ = 0, ε = 0.05

For each round t from 1 to r:
    If t ≤ n:
        Play C
    Else:
        Update c_history with number of cooperators in previous round (c_t-1)
        Calculate adaptive threshold θ based on c_history
        If π_t-1 ≥ θ or t == r:
            Play C
        Else:
            Play D

Update π_t and c_history after each round
```
**Edge Cases:**

* **First Round**: Cooperate to initiate collective cooperation.
* **Last Round (r)**: Cooperate to ensure maximum payoff, as there's no future benefit from defecting.
* **Opponent Cooperation**: If an opponent cooperates in a previous round, it increases the likelihood of our own cooperation in the next round.

**Collective Mindset:**

ACGC prioritizes collective cooperation by initially cooperating and then adapting to the observed level of cooperation. By monitoring cooperation levels and adjusting the threshold accordingly, ACGC encourages others to cooperate while also maximizing individual payoffs. This approach balances individual self-interest with collective well-being, making it a robust strategy for a wide range of opponent behaviors.

By implementing this adaptive strategy, we can promote cooperation and achieve better outcomes in the N-Player Public Goods Game.
'''

description_COLLECTIVE_71 = '''
**Collective Strategy: Adaptive Public Goods (APG)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage cooperation and create a positive environment.
2. **Subsequent Rounds:** Use a hybrid approach that combines two factors:
	* **Reciprocity**: If at least 50% of players cooperated in the previous round, cooperate (C). Otherwise, defect (D).
	* **Public Goods Performance**: Calculate the average payoff per player in the previous round. If it's above a certain threshold (e.g., 1.5), cooperate (C) to maintain or improve the public good. Otherwise, defect (D).

**Threshold Calculation:** `threshold = (n * k) / (2 * n)` (e.g., for n=6 and k=2, threshold ≈ 1.67)

**Edge Cases:**

* **Last Round:** Defect (D) in the last round to maximize individual payoff, as there's no future cooperation to consider.
* **Ties**: In case of a tie in the reciprocity calculation, cooperate (C) to maintain a positive environment.

**Pseudocode:**
```markdown
function APG(n, k, history):
  if history.isEmpty(): # First round
    return COOPERATE

  prevRoundCoops = countCooperators(history[-1])
  avgPayoffPrevRound = calculateAvgPayoff(history[-1])

  reciprocityThreshold = n / 2
  publicGoodsThreshold = (n * k) / (2 * n)

  if prevRoundCoops >= reciprocityThreshold or avgPayoffPrevRound > publicGoodsThreshold:
    return COOPERATE
  else:
    return DEFECT

  # Last round handling
  if currentRound == totalRounds:
    return DEFECT
```
**Collective Mindset:**

The APG strategy is designed to balance individual self-interest with collective well-being. By initially cooperating, we set a positive tone and encourage others to cooperate. The hybrid approach ensures that we adapt to the environment while maintaining a level of cooperation.

In cases where reciprocity is high or public goods performance is good, we continue to cooperate to maintain the positive environment. When faced with low cooperation or poor public goods performance, we defect to protect our individual interests.

By using a threshold-based system, APG avoids oscillating between cooperation and defection, ensuring stability in its decision-making process. This strategy should perform well against various opponent behaviors while promoting collective prosperity.
'''

description_COLLECTIVE_72 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

The ACP strategy is designed to balance individual self-interest with collective cooperation, while adapting to the game's history and robustly responding to various opponent behaviors.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) unconditionally to encourage mutual cooperation.
2. **Cooperation Phase**: If the majority of players cooperated in the previous round (> n/2), cooperate (C) with probability p = (k/n) \* (total_cooperators / n). This phase encourages continued cooperation when it is beneficial.
3. **Punishment Phase**: If the majority defected or a player's payoff was below average in the previous round, defect (D) for one round to punish free-riding behavior.
4. **Adaptive Adjustment**: After the punishment phase, adjust the cooperation probability p based on the new game state:
	* If total_cooperators increases, increase p by 10% (up to a maximum of k/n).
	* If total_cooperators decreases or remains the same, decrease p by 10% (down to a minimum of 0.1).

**Edge Cases:**

1. **Last Round**: In the final round, defect (D) unconditionally to maximize individual payoff.
2. **Ties**: In case of ties in cooperation counts, use the previous round's cooperation probability p as a tiebreaker.

**Collective Mindset Alignment:**

The ACP strategy is designed to promote collective cooperation while allowing for adaptation and punishment. By initially cooperating, we encourage others to do so as well. The cooperation phase reinforces beneficial behavior, while the punishment phase corrects free-riding. Adaptive adjustments ensure that our strategy remains responsive to changes in the game state.

**Pseudocode (for illustration purposes only):**
```python
def ACP(n, k, r, history):
    if current_round == 1:
        return 'C'  # Cooperate initially

    majority_cooperated = len([h for h in history[-1] if h == 'C']) > n/2
    avg_payoff = sum(history[-1].values()) / n

    if majority_cooperated:
        p = (k/n) * (len([h for h in history[-1] if h == 'C']) / n)
        return 'C' with probability p, else 'D'
    elif avg_payoff < individual_payoff:
        return 'D'  # Punish
    else:
        adjust_p(history)  # Adapt cooperation probability
        return 'C' with adjusted probability

def adjust_p(history):
    total_cooperators = len([h for h in history[-1] if h == 'C'])
    if total_cooperators > previous_total_cooperators:
        p += 0.1 * (k/n)
    elif total_cooperators <= previous_total_cooperators:
        p -= 0.1 * (k/n)
    return p
```
Note that the pseudocode is for illustration purposes only and will be refined during implementation.

The ACP strategy balances individual self-interest with collective cooperation, adapting to the game's history while robustly responding to various opponent behaviors. It promotes a collective mindset by initially cooperating, reinforcing beneficial behavior, and punishing free-riding, all while adjusting its approach based on the evolving game state.
'''

description_COLLECTIVE_73 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors:

**Strategy Name:** Adaptive Collective Optimizer (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = k/n. This encourages initial cooperation while considering the game's multiplication factor.
2. **Reciprocal Altruism**: For rounds t > 1:
	* If the average payoff of all players in the previous round is above a threshold θ (e.g., θ = 1.5), cooperate (C).
	* Otherwise, defect (D) with probability q = (k/n) \* (average number of cooperators in the previous round / n). This introduces a level of reciprocity and adaptability to the opponent's behavior.
3. **Exploitation Prevention**: Monitor the average payoff of all players over the last w rounds (e.g., w = 5). If this average is below a lower threshold φ (e.g., φ = 1), switch to always defecting (D) for the next x rounds (e.g., x = 3). This prevents exploitation by free-riders.

**Edge Cases:**

* **Last Round**: In the final round, cooperate (C) if the total payoff over all previous rounds is above a certain threshold γ (e.g., γ = r \* θ). Otherwise, defect (D).
* **Tie-Breaking**: In cases where multiple actions have the same expected payoff, choose the action that leads to a higher total payoff for all players.

**Pseudocode:**
```
initialize p = k/n, θ = 1.5, φ = 1, w = 5, x = 3, γ = r * θ

for t = 1 to r:
    if t == 1:
        action[t] = C with probability p
    else:
        avg_payoff_prev_round = calculate_average_payoff(t-1)
        if avg_payoff_prev_round > θ:
            action[t] = C
        else:
            q = (k/n) * (average_cooperators(t-1) / n)
            action[t] = D with probability q

    # Exploitation prevention
    avg_payoff_last_w_rounds = calculate_average_payoff_over_window(w)
    if avg_payoff_last_w_rounds < φ:
        for i = t to min(t+x, r):
            action[i] = D

    # Last round special case
    if t == r:
        total_payoff_so_far = calculate_total_payoff()
        if total_payoff_so_far > γ:
            action[t] = C

return actions
```
**Collective Mindset:**
ACO is designed to promote collective well-being by:

1. Encouraging initial cooperation and reciprocity.
2. Adapting to opponent behavior to prevent exploitation.
3. Prioritizing the total payoff for all players in tie-breaking situations.

By following ACO, agents can create a robust and adaptive strategy that balances individual interests with the need for collective cooperation.
'''

description_COLLECTIVE_74 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation strategy aims to balance individual self-interest with collective welfare by adapting to the evolving dynamics of cooperation and defection within the group.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) - Establish a cooperative tone from the beginning.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round (Σc_j,t-1) is greater than or equal to the multiplication factor (k), cooperate (C). This encourages continued cooperation when it benefits the group.
	* Otherwise, defect (D).
3. **Punishment Mechanism:** Introduce a punishment phase if the total number of cooperators in the previous round falls below k/n × n (i.e., less than k/n proportion of players cooperate). In this case:
	* Defect (D) for one round to signal discontent with low cooperation levels.
	* Resume cooperation (C) in the next round, hoping others will adapt and increase their cooperation.

**Edge Cases:**

1. **Last Round (t=r):** Cooperate (C) unconditionally - Maximize collective payoff in the final round.
2. **Opponent Defection:** If an opponent defects repeatedly (i.e., more than half of the rounds), start defecting against them as well, but still cooperate with other players who have a history of cooperation.

**Collective Mindset:**

1. **Monitor Cooperation Levels:** Keep track of the total number of cooperators in each round to inform decisions.
2. **Adapt to Group Dynamics:** Adjust strategy based on the evolving cooperation and defection patterns within the group.
3. **Foster Cooperation:** Encourage others to cooperate by maintaining a high level of cooperation, especially when it benefits the group.

**Pseudocode:**

```python
def adaptive_cooperation(n, k, r):
    # Initialize variables
    total_cooperators = 0
    punishment_phase = False

    for t in range(1, r+1):
        if t == 1:
            action = 'C'  # Cooperate in the first round
        else:
            if total_cooperators >= k:
                action = 'C'
            else:
                action = 'D'

            # Punishment mechanism
            if total_cooperators < k/n * n and not punishment_phase:
                punishment_phase = True
                action = 'D'
            elif punishment_phase:
                punishment_phase = False

        # Update total cooperators for next round
        total_cooperators = sum(1 for j in range(n) if opponents[j].action == 'C')

        return action
```

This strategy balances individual self-interest with collective welfare by adapting to the evolving dynamics of cooperation and defection within the group. It encourages continued cooperation when it benefits the group, introduces a punishment mechanism to deter low cooperation levels, and adapts to opponent behaviors while maintaining a high level of cooperation.
'''

description_COLLECTIVE_75 = '''
**Collective Strategy: Adaptive Cooperation with Learning (ACL)**

ACL is a collective strategy designed for the N-Player Public Goods Game that adapts to the behavior of other players while promoting cooperation.

**Decision Rules:**

1. **Initial Rounds:** In the first few rounds (specifically, `r/4` rounds), play Cooperate (C) unconditionally to establish a cooperative baseline and gather information about other players' strategies.
2. **Contribution Threshold:** Calculate the average contribution rate (`avg_contrib`) of all players over the previous `r/2` rounds. If `avg_contrib` is above a certain threshold (`θ`, e.g., 0.5), play Cooperate (C). Otherwise, play Defect (D).
3. **Revenge Mechanism:** Monitor the total number of cooperators in the previous round (`prev_coops`). If fewer than half of the players cooperated (`prev_coops < n/2`), and you played Cooperate (C) in that round, switch to Defect (D) for one round as a "punishment" mechanism. After this punishment round, revert to the contribution threshold rule.
4. **Learning Mechanism:** Adjust the contribution threshold (`θ`) based on the game's progression:
	* If the average payoff over the last `r/2` rounds is higher than in the previous `r/2` rounds, decrease `θ` by 0.1 (e.g., from 0.5 to 0.4). This encourages more cooperation when the group is doing well.
	* If the average payoff over the last `r/2` rounds is lower than in the previous `r/2` rounds, increase `θ` by 0.1 (e.g., from 0.5 to 0.6). This reduces cooperation when the group is struggling.

**Edge Cases:**

* **Last Round:** In the final round (`t=r`), play Defect (D) unconditionally to maximize individual payoff, as there are no future rounds to consider.
* **Early Rounds with Few Cooperators:** If fewer than `n/4` players cooperated in the previous round, and you played Defect (D), switch to Cooperate (C) for one round to help "kick-start" cooperation.

**Collective Mindset:**

ACL promotes a collective mindset by:

* Encouraging initial cooperation to establish a baseline
* Adapting to the group's contribution rate to balance individual payoffs with collective benefits
* Punishing non-cooperation to maintain social norms
* Learning from the game's progression to adjust strategy

**Pseudocode:**
```markdown
 ACL_strategy(n, r, k) {
   // Initialize parameters and variables
   θ = 0.5; // contribution threshold
   prev_coops = 0;
   avg_contrib = 0;

   for (t = 1 to r) {
     if (t <= r/4) { // initial rounds
       action = Cooperate;
     } else {
       avg_contrib = calculate_average_contribution(n, t);
       if (avg_contrib > θ) {
         action = Cooperate;
       } else {
         action = Defect;
       }
     }

     // Revenge mechanism
     if (prev_coops < n/2 && action == Cooperate) {
       action = Defect; // punish non-cooperation
     }

     // Learning mechanism
     update_θ(avg_contrib, θ);

     // Play action and observe outcome
     play_action(action);
     observe_outcome();

     // Update variables for next round
     prev_coops = count_cooperators(n, t);
   }
 }
```
Note that this pseudocode is a simplified representation of the strategy, and actual implementation details may vary depending on the programming language and environment.
'''

description_COLLECTIVE_76 = '''
**Collective Strategy: Adaptive Cooperation with History-Based Adjustment (ACHA)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage cooperation and establish a baseline for future interactions.
2. **History-Based Adjustment:** Observe the previous round's outcome and adjust behavior accordingly:
	* If the total number of cooperators is greater than or equal to `n/2`, cooperate (C) in the current round.
	* If the total number of cooperators is less than `n/2`, defect (D) in the current round, but with a probability `p` of cooperating instead. This probability `p` will be adjusted based on the history of cooperation and defection (see below).
3. **Cooperation Threshold Adjustment:** After each round, adjust the probability `p` based on the number of cooperators:
	* If the total number of cooperators increases, increase `p` by a small amount (`Δp = 0.05`) to encourage further cooperation.
	* If the total number of cooperators decreases or remains the same, decrease `p` by a small amount (`Δp = -0.05`) to adapt to the changing environment.

**Pseudocode:**
```python
def ACHA(n, k, r):
    # Initialize variables
    p = 0.5  # initial probability of cooperation when not cooperating
    history = []  # store previous rounds' outcomes

    for t in range(r):  # loop through each round
        if t == 0:  # first round
            action = 'C'
        else:
            num_cooperators = sum(history[-1]['cooperated'])
            if num_cooperators >= n / 2:
                action = 'C'
            else:
                action = 'D' if random.random() > p else 'C'

        # Observe outcome and adjust history
        outcome = observe_outcome(action)
        history.append({'cooperated': [1 if a == 'C' else 0 for a in outcome]})

        # Adjust cooperation threshold
        num_cooperators = sum(history[-1]['cooperated'])
        if num_cooperators > sum(history[-2]['cooperated']):
            p += 0.05
        elif num_cooperators < sum(history[-2]['cooperated']):
            p -= 0.05

        # Ensure probability bounds
        p = max(0, min(p, 1))

    return history
```
**Edge Cases:**

* **Last Round:** The strategy does not require special handling for the last round, as it adapts to the changing environment throughout the game.
* **Ties:** In case of ties (e.g., equal numbers of cooperators and defectors), the strategy defaults to defecting with probability `p`.

**Collective Mindset:**

This strategy aims to balance individual self-interest with collective well-being. By initially cooperating and adjusting behavior based on the history of cooperation, ACHA encourages mutual cooperation while being robust to a wide range of opponent behaviors. The adaptive nature of the strategy allows it to respond effectively in various environments, promoting a collective outcome that balances individual payoffs with social welfare.
'''

description_COLLECTIVE_77 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation (AC) strategy is designed to balance individual self-interest with collective well-being, adapting to the game's history and opponent behaviors.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to reciprocate.
2. **Subsequent Rounds**: Use the following decision rule:

   * If the average payoff per player in the previous round is above the threshold `k/n`, cooperate (C). This indicates that cooperation has been effective, and continuing to contribute will likely yield higher payoffs.
   * Otherwise, defect (D).

**Edge Cases**

* **Last Round**: Defect (D) in the last round, as there are no future rounds to consider, and individual self-interest takes precedence.
* **Ties in Average Payoff**: In case of a tie in average payoff per player, cooperate (C) if the number of cooperators in the previous round is above `n/2`, and defect (D) otherwise.

**Pseudocode**

```
function adaptive_cooperation(n, k, r, history):
  # Initialize cooperation flag
  cooperate = True
  
  for t in range(1, r+1):
    if t == 1:  # First round
      action = C
    else:
      avg_payoff_prev_round = calculate_average_payoff(history, t-1)
      if avg_payoff_prev_round >= k/n:
        action = C
      else:
        action = D
    
    # Update history and cooperate flag
    history.append(action)
    cooperate = (action == C)
  
  return history

def calculate_average_payoff(history, round):
  # Calculate average payoff per player in the given round
  total_payoff = sum([payoff for _, payoff in history[:round]])
  avg_payoff = total_payoff / n
  return avg_payoff
```

**Collective Mindset**

The AC strategy is designed to align with a collective mindset, promoting cooperation when it benefits the group and adapting to changes in opponent behaviors. By cooperating initially and responding to the average payoff per player, the strategy encourages others to cooperate, ultimately leading to higher payoffs for all.

This strategy should perform well against independent strategies developed by other AI systems, as it is robust to a wide range of opponent behaviors and adapts to the game's history without relying on specific coordination mechanisms or shared norms.
'''

description_COLLECTIVE_78 = '''
**Collective Strategy: Adaptive Cooperation with Learning (ACL)**

**Overview**
ACL is a collective strategy that balances individual self-interest with collective well-being. It adapts to the game's history and adjusts its cooperation level based on the observed behavior of other players.

**Decision Rules**

1. **First Round**: Cooperate (C) in the first round to set a cooperative tone and gather information about others' behaviors.
2. **Subsequent Rounds**:
	* If the average payoff of all players in the previous round is above the threshold `(k-1)/n`, cooperate (C).
	* Otherwise, defect (D).

Pseudocode for decision rules:
```
if (round == 1) {
  action = C;
} else {
  avg_payoff_prev_round = calculate_avg_payoff(prev_round);
  if (avg_payoff_prev_round > (k-1)/n) {
    action = C;
  } else {
    action = D;
  }
}
```
**Learning Mechanism**
ACL uses a simple learning mechanism to adjust its cooperation level based on the observed behavior of other players.

* **Cooperation Index**: Calculate the proportion of cooperators in the previous round (`cooperation_index = num_cooperators_prev_round / n`).
* **Adjust Cooperation Level**: If `cooperation_index > 0.5`, increase the threshold `(k-1)/n` by a small amount (e.g., `0.01`). Otherwise, decrease it.

Pseudocode for learning mechanism:
```
cooperation_index = calculate_cooperation_index(prev_round);
if (cooperation_index > 0.5) {
  threshold += 0.01;
} else {
  threshold -= 0.01;
}
threshold = max(0, min(threshold, k-1)); // clamp to valid range
```
**Edge Cases**

* **Last Round**: In the last round, ACL will still follow its decision rules based on the previous round's outcome.
* **Ties**: In case of a tie in the average payoff calculation, ACL will cooperate (C).

**Collective Mindset**
ACL is designed to promote cooperation and collective well-being while being robust to individual self-interest. By adapting to the game's history and adjusting its cooperation level, ACL encourages other players to cooperate as well.

By playing ACL, our strategy aims to create a cooperative atmosphere in the game, leading to higher payoffs for all players involved.
'''

description_COLLECTIVE_79 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 80%. This sets a positive tone and encourages others to cooperate.
2. **History-Based Adaptation**: From the second round onwards, use the following rules:
	* If the number of cooperators in the previous round is greater than or equal to (n/2), cooperate (C).
	* If the number of cooperators in the previous round is less than (n/4), defect (D).
	* Otherwise, cooperate with a probability proportional to the number of cooperators in the previous round.
3. **Escalation**: If the total payoff for all players in the previous round is below a threshold (e.g., (k/2) \* n), increase cooperation by 10% in the next round.

**Handling Edge Cases:**

1. **First Round**: Cooperate with a probability of 80%.
2. **Last Round**: Defect (D).
3. **Ties**: In case of ties, cooperate.
4. **Zero Cooperators**: If no one cooperated in the previous round, defect (D).

**Collective Mindset:**

The ACE strategy aims to create a cooperative atmosphere by initially cooperating and adapting to the group's behavior. By escalating cooperation when payoffs are low, we encourage others to follow suit. This collective approach promotes mutual benefit and encourages players to work together.

**Pseudocode:**
```python
def ace_strategy(n, k, r, history):
    if len(history) == 0:  # First round
        return cooperate_with_prob(0.8)
    
    prev_cooperators = sum([1 for action in history[-1] if action == 'C'])
    
    if prev_cooperators >= n/2:
        return 'C'
    elif prev_cooperators < n/4:
        return 'D'
    else:
        cooperation_prob = prev_cooperators / n
        return cooperate_with_prob(cooperation_prob)
    
    # Escalation
    total_payoff = sum([payoff for payoff in history[-1]])
    if total_payoff < (k/2) * n:
        increase_cooperation_by(0.1)

def cooperate_with_prob(prob):
    import random
    return 'C' if random.random() < prob else 'D'
```
Note: This pseudocode is a simplified representation of the ACE strategy and may need to be modified for actual implementation.
'''

description_COLLECTIVE_80 = '''
To design a collective strategy for this N-Player Public Goods Game that is adaptive and robust to various opponent behaviors, we'll focus on creating a rule-based approach that utilizes game history to inform decisions. This strategy aims to balance cooperation with strategic self-interest.

**Strategy Name:** Adaptive Collective Optimizer (ACO)

### 1. Decision Rules

#### ACO Pseudocode:

```
Function ACO(n, k, r, round_t):
    // Initialize variables
    total_cooperations = 0
    previous_round_payoff = 0
    my_previous_action = D
    
    If round_t == 1:
        // First round: Start with cooperation to signal willingness and encourage others.
        return C
        
    Else:
        // Calculate the average payoff from the previous round.
        avg_payoff_prev_round = (total_cooperations * k / n + (n - total_cooperations)) / n
        
        If avg_payoff_prev_round > 1.5:  // A threshold to gauge if cooperation was beneficial.
            // Previous round was profitable; continue cooperating.
            return C
        Else:
            // Previous round wasn't as profitable; defect this round but monitor future rounds for changes in collective behavior.
            my_previous_action = D
            
            If total_cooperations > (n / 2) in the last 3 rounds:  // Monitor recent cooperation levels.
                // Many have been cooperating recently; try to exploit by defecting, but be prepared to switch back if exploitation is punished.
                return D
            Else:
                // Recent cooperation has been low; attempt to revive cooperation by contributing this round.
                return C
                
    End If
End Function
```

#### Explanation of Decision Rules:

- **First Round:** Start with cooperation (C) to initiate a cooperative dynamic and encourage other players to cooperate, aiming for mutual benefit.

- **Subsequent Rounds:**
  - Evaluate the average payoff from the previous round. If it was above a certain threshold (e.g., 1.5), continue cooperating as the collective behavior seems beneficial.
  - If the average payoff wasn't satisfactory, defect this round but monitor future rounds' outcomes closely for adjustments in collective strategy.

- **Adaptive Element:**
  - Keep track of recent cooperation levels (last 3 rounds). If many players have been cooperating despite your defection, and it was not punished by a decrease in overall payoffs, consider switching back to cooperation to benefit from the public good.
  
### 2. Handling Edge Cases

- **First Round:** As described, start with cooperation.

- **Last Round (r):** Since the game's end is known, there's less incentive for long-term cooperation. However, ACO continues to adapt based on recent rounds' outcomes but may lean towards defecting if it believes others will do so as well, avoiding a potentially lower payoff from solely cooperating in the final round.

- **Early Rounds (r < 3):** Continue to apply the decision rules with an emphasis on monitoring collective behavior closely. These early observations are crucial for informing later strategy adjustments.

### 3. Collective Mindset Alignment

ACO aligns with a collective mindset by:

- Initially encouraging cooperation and contributing to the public good.
- Adapting based on the actions of others, attempting to maximize payoffs while considering the well-being of all players.
- Being willing to switch strategies based on recent game history, aiming for a balance between personal gain and collective benefit.

This approach is designed to be robust against various opponent behaviors by adapting to the evolving dynamics of the game.
'''

description_COLLECTIVE_81 = '''
I'll design a collective strategy for this N-Player Public Goods Game that is adaptive and robust to various opponent behaviors. I'll call it "Adaptive Collective Cooperation" (ACC).

**Decision Rules:**

1. **Initial Cooperation**: In the first round, ACC cooperates (plays C) with probability p_init = 0.5. This encourages initial cooperation while leaving room for exploration.
2. **Contribution Threshold**: After the first round, ACC uses a contribution threshold τ to determine its action. The threshold is calculated as:

τ = (k/n) \* (Σ(j=1 to n) c_j) / r

where Σ(j=1 to n) c_j is the total number of cooperators in the previous round.

If the current round's total contributions exceed the threshold τ, ACC cooperates. Otherwise, it defects.
3. **Adaptive Adjustment**: After each round, ACC adjusts its contribution threshold based on the game's history:

τ_new = τ_old + (1/n) \* (k/n) \* (Σ(j=1 to n) c_j - τ_old)

This update rule allows ACC to adapt to changing opponent behaviors and adjust its cooperation level accordingly.
4. **Punishment Mechanism**: If the number of cooperators in a round is less than or equal to 0.5n, ACC defects with probability p_punish = 0.2 in the next round. This introduces a mild punishment mechanism to discourage widespread defection.

**Edge Cases:**

1. **Last Round**: In the last round (r = R), ACC cooperates if and only if the total contributions in the previous round exceeded the contribution threshold τ.
2. **All Defectors**: If all players defected in the previous round, ACC cooperates with probability p_revive = 0.3 to encourage revival of cooperation.

**Collective Mindset:**

ACC prioritizes collective welfare by:

1. Encouraging initial cooperation to establish a cooperative atmosphere.
2. Adapting its contribution threshold based on the game's history to respond to changing opponent behaviors.
3. Implementing a mild punishment mechanism to discourage widespread defection.

By following these decision rules, ACC aims to promote collective cooperation while being robust to various opponent strategies and behaviors.

Pseudocode:
```
function AdaptiveCollectiveCooperation(n, k, r):
  // Initialize variables
  τ = (k/n) \* (0.5n) / r
  p_init = 0.5
  p_punish = 0.2
  p_revive = 0.3

  for t = 1 to r:
    if t == 1:  // Initial cooperation
      action = Cooperate with probability p_init
    else:
      total_contributions = Σ(j=1 to n) c_j in previous round
      τ_new = update_threshold(τ, total_contributions)
      if total_contributions > τ_new:
        action = Cooperate
      else:
        action = Defect

      // Punishment mechanism
      if number of cooperators <= 0.5n:
        action = Defect with probability p_punish

    // Last round handling
    if t == r:
      if total_contributions in previous round > τ_new:
        action = Cooperate

    // All defectors handling
    if all players defected in previous round:
      action = Cooperate with probability p_revive

  return action
```
This pseudocode provides a basic outline of the ACC strategy. The actual implementation will require further details and optimizations.
'''

description_COLLECTIVE_82 = '''
I'll design a collective strategy that balances individual and collective interests, while being adaptive and robust to various opponent behaviors. Here's my proposed strategy:

**Name:** Adaptive Collective Conscience (ACC)

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to initiate cooperation and set a positive tone for the game.
2. **Initial Phase (Rounds 2-5):** If the average payoff of all players in the previous round is above the Nash Equilibrium payoff (i.e., π_i > 1), continue to Cooperate (C). Otherwise, Defect (D) to signal dissatisfaction with the current state of cooperation.
3. **Adaptive Phase:** From Round 6 onwards:
	* If the number of cooperators in the previous round is greater than or equal to n/2, Cooperate (C).
	* If the number of cooperators in the previous round is less than n/4, Defect (D) to avoid exploitation.
	* Otherwise, cooperate with a probability p = (k/n) × (number of cooperators in the previous round). This probability reflects the expected benefit from cooperation, taking into account the multiplication factor k and the number of cooperators.

Pseudocode:
```python
if round == 1:
    action = C
elif round <= 5:
    if avg_payoff > 1:
        action = C
    else:
        action = D
else:
    num_cooperators = count(C) in previous_round
    if num_cooperators >= n/2:
        action = C
    elif num_cooperators < n/4:
        action = D
    else:
        p = (k/n) * num_cooperators
        action = C with probability p, otherwise D
```
**Edge Cases:**

* **Last Round:** Cooperate (C) in the last round to maximize collective payoff and maintain a positive reputation.
* **Ties:** In case of ties (e.g., equal number of cooperators and defectors), randomize between Cooperate (C) and Defect (D) with equal probability.

**Collective Mindset:**

The ACC strategy is designed to promote cooperation while being adaptive to the behavior of other players. By initially cooperating and then adjusting based on the number of cooperators, the strategy aims to create a collective conscience that encourages cooperation. The use of probabilities in the Adaptive Phase allows for flexibility and reduces the risk of exploitation.

**Robustness:**

The ACC strategy is robust against various opponent behaviors, including:

* **Free Riders:** By defecting when there are too few cooperators, the strategy discourages free riding.
* **Cooperative Strategies:** The adaptive nature of the strategy allows it to cooperate with other cooperative players, maximizing collective payoff.
* **Exploitative Strategies:** By randomizing actions in uncertain cases, the strategy makes it difficult for exploitative players to predict its behavior.

Overall, the Adaptive Collective Conscience (ACC) strategy provides a balanced and adaptive approach to the N-Player Public Goods Game, promoting cooperation while being robust to various opponent behaviors.
'''

description_COLLECTIVE_83 = '''
**Collective Strategy: Adaptive Cooperation with Grudges (ACG)**

**Decision Rules:**

1. **First Round:** Cooperate (C) unconditionally to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds:** Assess the previous round's outcome using the following heuristics:
	* **Cooperation Threshold (CT):** Calculate the proportion of cooperators in the previous round, excluding yourself: CT = (Σ(j=1 to n) c_j - c_i) / (n-1).
	* **Grudge Indicator (GI):** Set GI to 1 if you defected and the average payoff of cooperators was higher than yours; otherwise, set GI to 0.
3. **Cooperation Logic:**
	+ If CT ≥ k/n, cooperate (C). This indicates a strong cooperative environment, and contributing will likely yield a higher payoff.
	+ If GI = 1, defect (D) for one round to "punish" the group for not rewarding your cooperation. Then, revert to the cooperation logic based on CT.
	+ Otherwise, defect (D) if CT < k/n.

**Pseudocode:**
```python
def ACG(n, k, r, history):
    # First round: Cooperate unconditionally
    if len(history) == 0:
        return 'C'

    # Calculate Cooperation Threshold (CT)
    ct = sum([1 for action in history[-1] if action == 'C']) / (n-1)

    # Calculate Grudge Indicator (GI)
    gi = 0
    if history[-1][player_id] == 'D':
        avg_cooperator_payoff = sum([payoff for payoff, action in zip(history[-2], history[-1]) if action == 'C']) / len([action for action in history[-1] if action == 'C'])
        if avg_cooperator_payoff > payoffs[player_id]:
            gi = 1

    # Cooperation Logic
    if ct >= k/n:
        return 'C'
    elif gi == 1:
        return 'D'  # Punish the group for one round
    else:
        return 'D'
```
**Collective Mindset:**

ACG aims to promote cooperation while being robust against various opponent behaviors. By cooperating in the first round, ACG sets a cooperative tone and encourages others to follow suit. The Cooperation Threshold (CT) assesses the strength of cooperation in the previous round, guiding ACG's decisions. The Grudge Indicator (GI) helps ACG adapt to situations where its cooperation is not rewarded, temporarily defecting to "punish" the group before reverting to cooperation logic.

ACG's design ensures that it is:

1. **Adaptive:** Responds to changes in the cooperative environment and opponent behaviors.
2. **Robust:** Handles a wide range of opponent strategies without relying on coordination mechanisms or shared norms.
3. **Collective:** Aligns with the collective mindset by promoting cooperation while being mindful of individual payoffs.

By employing ACG, our strategy will be well-equipped to navigate the N-Player Public Goods Game and perform effectively in the tournament against independent AI systems.
'''

description_COLLECTIVE_84 = '''
**Collective Strategy: Adaptive Cooperation with Retribution (ACR)**

**Decision Rules:**

1. **Initial Cooperation**: Cooperate in the first round.
2. **Reciprocal Cooperation**: In subsequent rounds, cooperate if the majority of players cooperated in the previous round.
3. **Retribution**: If a player defected in the previous round and was not punished by others (i.e., they received a higher payoff than cooperators), defect in the current round to punish them.
4. **Punishment Threshold**: The punishment threshold is set to 0.5, meaning that if a player's payoff is more than 50% of the maximum possible payoff for cooperating players, it is considered excessive and triggers retribution.

**Edge Cases:**

1. **First Round**: Cooperate (Rule 1).
2. **Last Round**: Cooperate if the majority cooperated in the second-to-last round; otherwise, defect.
3. **Ties**: In cases of ties (e.g., equal number of cooperators and defectors), cooperate.

**Collective Mindset:**

ACR prioritizes collective well-being by:

1. Encouraging cooperation through reciprocity.
2. Punishing free-riding to maintain fairness.
3. Adapting to changing circumstances to ensure robustness.

**Pseudocode:**
```markdown
function ACR(history):
  # Initialize variables
  cooperate = False
  punish = False

  # First round
  if history.isEmpty():
    return True (cooperate)

  # Analyze previous round
  prev_round_cooperators = countCooperators(history[-1])
  prev_round_defectors = n - prev_round_cooperators

  # Reciprocal cooperation
  if prev_round_cooperators > prev_round_defectors:
    cooperate = True

  # Retribution
  for player in history[-1]:
    if player.defected and player.payoff > 0.5 * maxCooperatorPayoff(history[-1]):
      punish = True
      break

  # Make decision
  if punish:
    return False (defect)
  else:
    return cooperate

# Helper functions
function countCooperators(round_history):
  # Count number of cooperators in the round

function maxCooperatorPayoff(round_history):
  # Calculate maximum possible payoff for cooperating players
```
**Rationale:**

ACR balances individual and collective interests by:

1. Cooperating when others do, to maximize collective payoffs.
2. Punishing free-riders to maintain fairness and encourage cooperation.
3. Adapting to changing circumstances to ensure robustness against various opponent behaviors.

By using a combination of reciprocity and retribution, ACR promotes cooperation while minimizing the risk of being exploited by defectors.
'''

description_COLLECTIVE_85 = '''
To design a collective strategy for this N-Player Public Goods Game that is adaptive and robust, we'll employ a mix of cooperative and retaliatory elements, leveraging the game's history to inform our decisions.

**Strategy Name: Adaptive Collective Contribute (ACC)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, contribute (play C) to establish a baseline for cooperation.
2. **Reciprocal Cooperation**: If the majority of players (> n/2) contributed in the previous round, contribute in the current round.
3. **Punishment for Defection**: If fewer than half of the players contributed in the previous round, defect (play D) to signal disappointment and encourage others to cooperate.
4. **Exploitation Detection**: Monitor the game history to detect potential exploitation by other players. If a player has consistently defected (> 75% of rounds), retaliate by defecting against them in future rounds.
5. **Gradual Forgiveness**: As players adapt and change their strategies, gradually forgive past defections. Specifically, reduce the threshold for retaliatory defection from 75% to 50% after a player has contributed in two consecutive rounds.

**Edge Cases:**

1. **Last Round**: In the final round, contribute if the majority of players have cooperated throughout the game; otherwise, defect.
2. **Tiebreakers**: When deciding between cooperation and defection due to a tie in the previous round's contributions, randomly choose an action with equal probability.

**Collective Mindset:**

The ACC strategy is designed to promote collective well-being while adapting to various opponent behaviors. By initially cooperating, we set a positive tone for the game. Reciprocal cooperation encourages others to contribute, while punishment for defection deters exploitation. The gradual forgiveness mechanism allows players to recover from past mistakes and adapt to changing circumstances.

**Pseudocode:**

```
function ACCStrategy(history):
  // Initialize variables
  round = currentRound()
  n = numberOfPlayers()
  k = multiplicationFactor()
  majThreshold = ceil(n / 2)
  punishThreshold = 0.75 * n

  // First round: Cooperate
  if round == 1:
    return C

  // Previous round analysis
  prevContributors = countContributors(history[-1])
  prevDefectors = n - prevContributors

  // Reciprocal cooperation
  if prevContributors > majThreshold:
    return C

  // Punishment for defection
  if prevContributors <= majThreshold:
    return D

  // Exploitation detection and retaliation
  for player in history:
    if playerDefectionRate(player) > punishThreshold:
      return D

  // Gradual forgiveness
  for player in history:
    if playerContributionRate(player) >= 0.5 and playerDefectionRate(player) <= 0.25:
      reducePunishThreshold()

  // Last round handling
  if round == totalRounds():
    if majorityCooperated(history):
      return C
    else:
      return D

  // Tiebreaker (random choice)
  return random.choice([C, D])
```

This strategy balances cooperation and retaliation, making it a robust competitor in the tournament. Its adaptability to various opponent behaviors and focus on collective well-being should lead to strong performance in the N-Player Public Goods Game.
'''

description_COLLECTIVE_86 = '''
I propose a collective strategy for the N-Player Public Goods Game, which I'll call "Adaptive Collective Cooperation" (ACC). ACC aims to balance individual self-interest with collective welfare by adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Rounds**: In the first two rounds, play Cooperate (C) unconditionally. This sets a cooperative tone and allows us to observe our opponents' initial actions.
2. **Observation Phase**: After the initial rounds, enter an observation phase where you Defect (D) for one round if:
	* The total number of cooperators in the previous round is less than n/2 (i.e., fewer than half the players cooperated).
	* Your payoff in the previous round was lower than the average payoff of all players.
3. **Adaptive Cooperation**: If neither condition in step 2 is met, play Cooperate (C) if:
	* The total number of cooperators in the previous round is greater than or equal to n/2 (i.e., at least half the players cooperated).
	* Your payoff in the previous round was higher than or equal to the average payoff of all players.
4. **Punishment Mechanism**: If you played Cooperate (C) and your payoff was lower than the average payoff, play Defect (D) for one round as a punishment mechanism to deter free-riding.

**Edge Cases:**

1. **Last Round**: In the final round, play Cooperate (C) if the total number of cooperators in the previous round is greater than or equal to n/2.
2. **Ties**: In case of ties in the observation phase or adaptive cooperation rules, default to playing Defect (D).

**Collective Mindset:**

ACC aims to create a collective mindset by:

1. Encouraging initial cooperation to set a positive tone.
2. Observing and adapting to opponent behaviors to balance individual self-interest with collective welfare.
3. Implementing a punishment mechanism to deter free-riding and maintain cooperation.

Pseudocode:
```
Initialize variables: round = 0, cooperate_count = 0, defect_count = 0

While round < r:
  If round <= 1:
    Play Cooperate (C)
  Else if (cooperate_count < n/2) or (payoff < average_payoff):
    Play Defect (D)
  Else if (cooperate_count >= n/2) and (payoff >= average_payoff):
    Play Cooperate (C)
  Else if played Cooperate and payoff < average_payoff:
    Play Defect (D) as punishment
  End If

  Update cooperate_count, defect_count, and average_payoff based on current round's actions and payoffs
End While
```
ACC is designed to be robust against a wide range of opponent behaviors while promoting collective cooperation. By adapting to the game's history and implementing a punishment mechanism, ACC aims to achieve a high total payoff in the tournament.
'''

description_COLLECTIVE_87 = '''
Here's a collective strategy that adapts to the game environment and opponent behaviors:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = 0.5.
2. **Cooperation Threshold**: For subsequent rounds, maintain a cooperation threshold (CT) that tracks the average payoff obtained when cooperating in previous rounds.

Pseudocode:
```
if current_round == 1:
    # Initial Cooperation
    action = C if random.random() < p_init else D
else:
    # Cooperation Threshold
    CT = average_payoff_when_cooperating
    if CT >= (k/n) * n:  # if expected payoff from cooperation is high enough
        action = C
    elif CT < (1 - k/n):  # if expected payoff from cooperation is too low
        action = D
    else:
        # Adapt to opponent behavior
        opponent_cooperation_rate = avg_opponent_cooperations / n
        if opponent_cooperation_rate > 0.5:  # opponents are cooperative
            action = C
        elif opponent_cooperation_rate < 0.5 and CT > (1 - k/n):  # opponents are not very cooperative, but CT is still relatively high
            action = C with probability p_acc = 0.7
        else:
            action = D
```
**Edge Cases:**

* **Last Round**: In the last round (r), defect (D) to maximize individual payoff.
* **When opponents are always defective**: If all opponents have defected in previous rounds, adapt and cooperate with probability p_acc = 0.3.

**Collective Mindset:**
The ACC strategy aims to balance individual self-interest with collective well-being by:

1. Encouraging cooperation when the expected payoff from cooperation is high enough.
2. Adapting to opponent behavior to maintain a stable level of cooperation.
3. Allowing for occasional exploration (initial cooperation and adaptation) to respond to changing game conditions.

By following these decision rules, the ACC strategy promotes collective cooperation while being robust to various opponent behaviors and adapting to the game environment.

Let me know if you'd like me to clarify any aspects of this strategy!
'''

description_COLLECTIVE_88 = '''
I propose a collective strategy called "Adaptive Reciprocity with Endgame Effect" (AREE). AREE is designed to balance cooperation and self-interest while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to establish a cooperative tone and gather information about opponents' strategies.
2. **Reciprocity**: For rounds 2 to r-1:
	* If the majority of opponents cooperated in the previous round (i.e., ≥ n/2), cooperate (C).
	* If the majority defected, defect (D) to avoid being exploited.
3. **Endgame Effect**: In the last round (r):
	* Cooperate (C) if the total payoff from cooperating in all rounds is greater than or equal to the total payoff from defecting in all rounds.
	* Defect (D) otherwise.

Pseudocode:
```
if current_round == 1:
    action = C
elif current_round < r:
    majority_cooperated = opponents_cooperated >= n / 2
    if majority_cooperated:
        action = C
    else:
        action = D
else:  # last round
    total_payoff_C = sum(pi for pi in payoffs if action == C)
    total_payoff_D = sum(pi for pi in payoffs if action == D)
    if total_payoff_C >= total_payoff_D:
        action = C
    else:
        action = D
```
**Edge Cases:**

* **First Round**: Cooperate to establish a cooperative tone.
* **Last Round**: Consider the endgame effect and cooperate or defect accordingly.
* **Ties**: If there is a tie in the majority of cooperators, use the previous round's action as a tiebreaker.

**Collective Mindset:**
AREE aims to create a collective mindset by:

1. Encouraging cooperation through initial cooperation and reciprocity.
2. Adapting to opponents' strategies while maintaining a balance between cooperation and self-interest.
3. Considering the endgame effect to maximize total payoff in the final round.

By following AREE, players can adapt to various opponent behaviors, promote cooperation, and make informed decisions based on game history and parameters.
'''

description_COLLECTIVE_89 = '''
I propose a collective strategy called "Adaptive Conditional Cooperation" (ACC) for the N-Player Public Goods Game. ACC is designed to balance individual self-interest with collective welfare while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to initiate cooperation and encourage others to follow suit.
2. **Subsequent Rounds**: Use a "Tit-for-Tat" approach, modified for multiple players:
	* If the total number of cooperators in the previous round is greater than or equal to half the number of players (n/2), Cooperate (C).
	* Otherwise, Defect (D).

This rule encourages cooperation when others are willing to cooperate and adapts to situations where others defect.

**Edge Cases:**

1. **Last Round**: In the final round, always Defect (D). This is because there's no future opportunity for reciprocity or punishment.
2. **Single Player**: If only one player remains in a subsequent round (e.g., due to attrition), Cooperate (C) as there's no risk of exploitation.

**Additional Mechanisms:**

1. **Punishment Phase**: Implement a "punishment phase" after a round where fewer than half the players cooperated:
	* If a player i defected in that round, and the total number of cooperators was less than n/2, then:
		+ In the next round, if any player j (including player i) cooperated in the previous round, Cooperate (C). Otherwise, Defect (D).
	* This temporary punishment phase encourages cooperation by making players more likely to cooperate after a defection.
2. **Forgiveness**: After two consecutive rounds with at least half of the players cooperating, reset the punishment phase.

**Pseudocode:**
```markdown
function ACC(n, k, r):
  history = []
  
  for t in range(r):
    if t == 0:
      action = C
    else:
      total_cooperators_prev_round = sum(history[-1])
      
      if total_cooperators_prev_round >= n/2:
        action = C
      else:
        action = D
        
      # Punishment phase
      punishment_phase_active = False
      if len(history) > 1 and sum(history[-2]) < n/2:
        punishment_phase_active = True
        for player in history[-2]:
          if player == C:
            action = C
            break
    
    # Take action and update history
    payoffs[t] = calculate_payoff(action, history)
    history.append([action])
    
  return total_payoff(payoffs)
```
**Collective Mindset:**
ACC aligns with the collective mindset by:

1. Cooperating initially to initiate cooperation.
2. Encouraging cooperation when others are willing to cooperate.
3. Adapting to situations where others defect, while maintaining a willingness to cooperate in subsequent rounds.

This strategy balances individual self-interest with collective welfare and demonstrates robustness against various opponent behaviors.
'''

description_COLLECTIVE_90 = '''
I'll design a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and only relies on game parameters and history.

**Strategy Name: Adaptive Collective Optimism (ACO)**

**Decision Rules:**

1. **Initial Rounds:** In the first round, cooperate (C) with probability 0.5. This initial cooperation serves as a "probe" to gauge other players' willingness to cooperate.
2. **Subsequent Rounds:** Observe the total number of cooperators (TC) in the previous round. If TC ≥ n/2, cooperate (C). Otherwise, defect (D).
3. **Punishment Mechanism:** If the total payoff from the public good in the previous round is less than the average private payoff (i.e., k/n \* TC < 1), defect (D) with probability 0.5. This mechanism discourages exploitation by punishing low contributors.
4. **Exploration:** With a small probability (0.05), randomly choose to cooperate or defect, regardless of the previous round's outcome. This ensures exploration and prevents strategy stagnation.

**Edge Cases:**

1. **Last Round:** In the final round, always cooperate (C) if the total payoff from the public good in the previous round is greater than or equal to the average private payoff.
2. **Early Defection:** If a player defects in an early round (e.g., within the first 20% of rounds), respond with a higher probability of defection (0.7) for the next few rounds.

**Collective Mindset:**

ACO prioritizes cooperation when there is evidence that other players are willing to contribute to the public good. By initially cooperating and punishing low contributors, ACO fosters a collective atmosphere where cooperation becomes more attractive as the game progresses.

**Pseudocode:**
```python
def AdaptiveCollectiveOptimism(n, k, r, history):
    # Initialize variables
    total_cooperators = 0
    previous_payoff = 0
    
    for round in range(1, r+1):
        if round == 1:
            action = random.choice(['C', 'D'])  # Initial cooperation probe
        else:
            TC = sum([1 if player_action == 'C' else 0 for player_action in history[round-2]])
            
            if TC >= n/2:
                action = 'C'
            elif k/n * TC < 1:
                action = 'D' if random.random() < 0.5 else 'C'  # Punishment mechanism
            else:
                action = 'C'
                
        # Exploration
        if random.random() < 0.05:
            action = random.choice(['C', 'D'])
            
        # Update history and variables
        history.append(action)
        total_cooperators += 1 if action == 'C' else 0
        previous_payoff = calculate_payoff(history, n, k)
        
    return history
```
ACO is designed to be robust against various opponent behaviors while promoting cooperation in the collective interest. Its adaptive nature allows it to respond to changing circumstances and explore different strategies to optimize payoffs.
'''

description_COLLECTIVE_91 = '''
To design a collective strategy for this N-Player Public Goods Game that is adaptive and robust, we can employ a dynamic approach that adjusts behavior based on observed cooperation levels among players. This strategy aims to balance individual payoff maximization with promoting collective welfare by encouraging cooperation when beneficial.

### Strategy Name: Adaptive Collective Maximization (ACM)

**Decision Rules:**

1. **Initial Rounds:** In the first round, cooperate (play C) to set a positive tone and encourage others to cooperate.
2. **Subsequent Rounds:** Observe the previous round's outcomes:
   - If the total payoff from cooperation (\(k/n \times \text{total cooperators}\)) is greater than or equal to your private payoff for defecting (1), continue cooperating if you played C in the last round and at least one other player also cooperated.
   - Otherwise, switch to defecting (play D) if you were cooperating. If you were already defecting, continue to defect unless all players defected in the previous round, in which case, consider switching back to cooperating for one round to test the waters again.

**Edge Cases:**

- **Last Round:** Cooperate regardless of past actions to maximize collective payoff since there are no future rounds to affect.
  
- **First Few Rounds:** Maintain cooperation if at least half of the players are cooperating. If not, consider a gradual shift towards defecting based on observed payoffs and others' strategies.

**Pseudocode:**

```
// Initialization
round = 1;
cooperated_last_round = false;
last_total_cooperators = 0;

while (round <= total_rounds) {
    if (round == 1) { // First Round
        action = COOPERATE;
        cooperated_last_round = true;
    } else {
        calculate_total_payoff_from_cooperation();
        
        if (total_payoff_from_cooperation >= private_defect_payoff && 
            last_total_cooperators > 0 && 
            cooperated_last_round) {
            action = COOPERATE;
        } else if (all_players_defected_last_round) { // All defected, test cooperation
            action = COOPERATE;
        } else {
            action = DEFECT;
        }
    }

    // Execute action and observe outcomes
    execute_action(action);
    
    update_cooperated_last_round();
    update_last_total_cooperators();

    round++;
}

// Functions to calculate total payoff from cooperation, check last round's actions, etc.
```

**Collective Mindset:**

The ACM strategy is designed with a collective mindset by initially promoting cooperation and adjusting based on the observed behavior of others. It seeks to maximize both individual payoffs and the collective good by adapting to the strategies employed by other players. By starting with cooperation and dynamically changing actions in response to others' behaviors, ACM aims to foster an environment where mutual benefit is encouraged over selfish defection.

This strategy should perform well against a variety of opponents by being flexible and responsive to different behaviors while maintaining a core commitment to promoting collective outcomes that are beneficial for all players.
'''

description_COLLECTIVE_92 = '''
I'll design a collective strategy for this N-Player Public Goods Game that adapts to various opponent behaviors and aligns with the collective mindset.

**Strategy Name:** Adaptive Collective Convergence (ACC)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to encourage others to contribute to the public good.
2. **Contribution Tracking**: Keep a record of the total contributions (Σc_j) in each round.
3. **Threshold-based Cooperation**: In subsequent rounds, cooperate if the previous round's total contributions exceed a threshold (τ). The threshold is calculated as:

τ = (n \* k) / (2 \* n + 1)

This threshold balances individual and collective interests, ensuring that cooperation is rewarded while preventing excessive exploitation.

4. **Punishment Mechanism**: If the previous round's total contributions are below the threshold (τ), defect (play D) in the current round to signal dissatisfaction with the group's performance.
5. **Gradual Cooperation**: When cooperating, contribute a fraction of your endowment that is proportional to the number of cooperators in the previous round:

c_i = (Σc_j / n) \* k

This encourages others to cooperate while maintaining a balance between individual and collective interests.

**Edge Cases:**

1. **First Round**: Cooperate (play C) to encourage others to contribute.
2. **Last Round**: Defect (play D) as there is no future round to influence.
3. **Ties in Contributions**: In case of ties, cooperate if the previous round's total contributions were above the threshold; otherwise, defect.

**Pseudocode:**
```python
def ACC(n, k, r):
  # Initialize variables
  τ = (n * k) / (2 * n + 1)
  Σc_j_prev = 0
  c_i_prev = 0

  for t in range(r):
    if t == 0:
      # First round: cooperate
      action = 'C'
    else:
      # Threshold-based cooperation
      if Σc_j_prev >= τ:
        action = 'C'
        c_i = (Σc_j_prev / n) * k
      else:
        # Punishment mechanism
        action = 'D'

    # Update variables for next round
    Σc_j_prev = sum(c_j for c_j in get_contributions(t))
    c_i_prev = 1 if action == 'C' else 0

  return actions
```
**Collective Mindset:**

The ACC strategy promotes a collective mindset by:

* Encouraging cooperation through initial and gradual contributions
* Punishing non-cooperation through defecting when the group's performance is below the threshold
* Adapting to the group's behavior, making it robust to various opponent strategies

By using this adaptive collective convergence strategy, we can foster cooperation and improve overall payoffs in the N-Player Public Goods Game.
'''

description_COLLECTIVE_93 = '''
I propose a collective strategy called "Adaptive Collective Optimism" (ACO). ACO aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if n ≥ 4 and k > 1.5; otherwise, defect (D).
2. **History-Based Adaptation**: After the first round, observe the number of cooperators in the previous round (c_prev). If c_prev ≥ n/2, cooperate (C); otherwise, defect (D).
3. **Payoff-Based Adaptation**: If your payoff in the previous round was lower than the average payoff of all players, switch your action from C to D or vice versa.
4. **Trend Analysis**: Track the number of cooperators over the last 3 rounds (c_t-2, c_t-1, c_t). If the trend is increasing (c_t > c_t-1 > c_t-2), cooperate (C); if decreasing (c_t < c_t-1 < c_t-2), defect (D).
5. **Endgame Strategy**: In the last round (r), cooperate (C) if your total payoff is below the average total payoff of all players; otherwise, defect (D).

**Pseudocode:**
```python
def ACO(n, k, r, history):
  # Initial Cooperation
  if r == 1:
    if n >= 4 and k > 1.5:
      return C
    else:
      return D

  # History-Based Adaptation
  c_prev = count_cooperators(history[-1])
  if c_prev >= n/2:
    return C
  else:
    return D

  # Payoff-Based Adaptation
  prev_payoff = calculate_payoff(history[-1], k, n)
  avg_payoff = average_payoff(history, k, n)
  if prev_payoff < avg_payoff:
    return opposite_action(history[-1])

  # Trend Analysis
  c_trend = [count_cooperators(history[i]) for i in range(-3, 0)]
  if c_trend[0] > c_trend[1] > c_trend[2]:
    return C
  elif c_trend[0] < c_trend[1] < c_trend[2]:
    return D

  # Endgame Strategy (last round)
  total_payoff = sum(calculate_payoff(history[i], k, n) for i in range(r))
  avg_total_payoff = average_total_payoff(history, k, n)
  if total_payoff < avg_total_payoff:
    return C
  else:
    return D

def opposite_action(action):
  if action == C:
    return D
  else:
    return C

def count_cooperators(round_history):
  # Count the number of cooperators in a round
  ...

def calculate_payoff(round_history, k, n):
  # Calculate the payoff for a player in a round
  ...

def average_payoff(history, k, n):
  # Calculate the average payoff over all rounds
  ...

def average_total_payoff(history, k, n):
  # Calculate the average total payoff of all players
  ...
```
This strategy combines elements of Tit-for-Tat (history-based adaptation), Pavlov (payoff-based adaptation), and trend analysis to create a robust collective strategy. ACO is designed to adapt to various opponent behaviors while promoting cooperation when possible.
'''

description_COLLECTIVE_94 = '''
**Collective Strategy: Adaptive Cooperation with Retribution (ACR)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust behavior accordingly:
	* If the total number of cooperators in the previous round is greater than or equal to `n/2`, cooperate (C). This indicates a strong collective inclination towards cooperation, and contributing will likely yield a higher payoff.
	* If the total number of cooperators is less than `n/2` but greater than 0, defect (D) with probability `p = (k/n) * (total_cooperators / n)`. This introduces a degree of retribution for low cooperation levels while still allowing for potential future cooperation.
	* If no players cooperated in the previous round, defect (D). This avoids contributing to a public good that others are not supporting.
3. **Last Round:** Cooperate (C) in the final round if the total number of cooperators in the second-to-last round is greater than or equal to `n/2`. Otherwise, defect (D).

**Pseudocode:**
```python
def ACR(n, k, history):
    # Initial round
    if len(history) == 0:
        return 'C'
    
    # Previous round's outcome
    prev_round = history[-1]
    total_cooperators = sum(1 for action in prev_round if action == 'C')
    
    # Subsequent rounds
    if total_cooperators >= n / 2:
        return 'C'
    elif 0 < total_cooperators < n / 2:
        p = (k / n) * (total_cooperators / n)
        return 'D' with probability p, otherwise 'C'
    else:
        return 'D'
    
    # Last round
    if len(history) == r - 1:  # second-to-last round's outcome
        prev_prev_round = history[-2]
        total_cooperators = sum(1 for action in prev_prev_round if action == 'C')
        if total_cooperators >= n / 2:
            return 'C'
        else:
            return 'D'
```
**Rationale:**

ACR balances cooperation and retribution to adapt to various opponent behaviors. By cooperating initially, we encourage others to follow suit. In subsequent rounds, we adjust our behavior based on the previous round's outcome, introducing a degree of retribution for low cooperation levels while still allowing for potential future cooperation. The final round's decision ensures that we contribute to the public good if there is sufficient collective support.

This strategy does not rely on specific coordination mechanisms or assumptions about opponent behaviors. Instead, it responds to the game's history and adapts to different scenarios, making it robust and competitive in a tournament setting.
'''

description_COLLECTIVE_95 = '''
I propose a collective strategy called "Adaptive Cooperative Escalation" (ACE) for the N-Player Public Goods Game. ACE is designed to balance individual self-interest with collective well-being while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally to establish a cooperative tone and encourage others to follow.
2. **Reciprocal Cooperation**: If the total number of cooperators in the previous round is above a threshold (α), cooperate in the current round. Otherwise, defect (D). The threshold α is calculated as α = n/2 + √n / 4, where n is the number of players. This formula balances individual and collective interests.
3. **Escalation**: If the total payoff for all players in the previous round is below a certain threshold (β), increase cooperation by one player in the current round. The threshold β is set to β = k \* n / 2, where k is the multiplication factor.
4. **Adaptive Response**: Monitor the average payoff of all players over the last few rounds (window size: γ). If the average payoff is increasing or stable, maintain the current level of cooperation. If it's decreasing, decrease cooperation by one player in the next round.

**Pseudocode:**
```python
def ACE(n, k, r):
    alpha = n / 2 + sqrt(n) / 4
    beta = k * n / 2
    gamma = min(5, r // 2)

    history = []
    for t in range(r):
        if t == 0:
            # Initial cooperation
            action = 'C'
        else:
            prev_coops = sum(c_i for c_i in history[-1])
            avg_payoff_window = mean([payoffs[t - i] for i in range(gamma)])

            if prev_coops >= alpha:
                # Reciprocal cooperation
                action = 'C'
            elif payoffs[t - 1] < beta:
                # Escalation
                action = 'C'
            elif avg_payoff_window > payoffs[t - 1]:
                # Adaptive response: maintain cooperation level
                action = history[-1][-1]
            else:
                # Adaptive response: decrease cooperation
                action = 'D'

        history.append(action)
    return history
```
**Edge Cases:**

* **Last Round**: In the final round, play a Nash Equilibrium strategy (defect) to maximize individual payoff, as there's no future interaction.
* **Small Game Size (n ≤ 3)**: Adjust α and β thresholds to accommodate smaller player sets. For n = 2, set α = 1 and β = k \* n / 4.

**Collective Mindset:**
The ACE strategy prioritizes collective cooperation while adapting to the actions of others. By cooperating initially, reciprocating cooperation, escalating when necessary, and responding adaptively, ACE promotes a cooperative atmosphere that can lead to higher payoffs for all players.

ACE is robust against various opponent behaviors, including:

* **Defectors**: ACE will not cooperate with persistent defectors, minimizing exploitation.
* **Cooperators**: ACE will maintain cooperation with reciprocal cooperators, maximizing collective benefits.
* **Mixed Strategies**: ACE's adaptive nature allows it to respond effectively to opponents using mixed strategies.

By implementing the Adaptive Cooperative Escalation strategy, we aim to create a robust and collective approach that promotes cooperation and maximizes payoffs in the N-Player Public Goods Game.
'''

description_COLLECTIVE_96 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Reciprocity (ACGR)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and gather information about other players' behaviors.
2. **Subsequent Rounds**: Evaluate the previous round's outcome using the following metrics:
	* `cooperation_level`: The proportion of players who cooperated in the previous round (excluding the focal player).
	* `payoff_difference`: The difference between the payoff obtained by cooperating and defecting in the previous round.
3. **Cooperation Threshold**: Cooperate if both conditions are met:
	* `cooperation_level` ≥ `k/n` (i.e., at least a fraction `k/n` of players cooperated)
	* `payoff_difference` ≤ 1 (i.e., cooperating did not result in a significant payoff disadvantage)
4. **Gradual Reciprocity**: If the conditions above are not met, defect with a probability that increases gradually based on the number of consecutive rounds where cooperation was not reciprocated:
	* `reciprocity_failure_count`: The number of consecutive rounds where cooperation was not reciprocated.
	* Defect probability: `1 - (k/n) ^ reciprocity_failure_count`
5. **Last Round**: Cooperate in the last round to maximize total game payoff, as there is no risk of non-reciprocation.

**Pseudocode**
```python
def ACGR(n, k, r):
    cooperation_level = 0
    payoff_difference = 0
    reciprocity_failure_count = 0

    for t in range(1, r+1):
        if t == 1:  # Initial round
            action = C
        else:
            cooperation_level = count_cooperators(t-1) / (n-1)
            payoff_difference = calculate_payoff_difference(t-1)

            if cooperation_level >= k/n and payoff_difference <= 1:
                action = C
            else:
                reciprocity_failure_count += 1
                defect_probability = 1 - (k/n) ** reciprocity_failure_count
                action = D with probability defect_probability

        # Update history and metrics for next round
        update_history(action, t)
        cooperation_level = count_cooperators(t-1) / (n-1)
        payoff_difference = calculate_payoff_difference(t-1)

    return total_game_payoff
```
**Rationale:**

ACGR balances individual self-interest with collective well-being by:

* Cooperating initially to establish a cooperative tone and gather information.
* Gradually adapting to the level of cooperation in the group, reciprocating cooperation when it is sufficient.
* Increasing the likelihood of defection when cooperation is not reciprocated, while still maintaining some probability of cooperation.
* Maximizing total game payoff by cooperating in the last round.

This strategy should be robust against a wide range of opponent behaviors, as it adaptively responds to the level of cooperation and does not rely on specific coordination mechanisms.
'''

description_COLLECTIVE_97 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy, "Adaptive Cooperation," aims to balance individual self-interest with the benefits of cooperation, while being robust to diverse opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to set a positive tone and encourage others to cooperate.
2. **Middle Rounds (2 ≤ t < r):**
	* If the majority (> n/2) of players cooperated in the previous round, Cooperate (C).
	* Otherwise, Defect (D).
3. **Last Round (t=r):** Cooperate (C), as there's no future opportunity to punish or reward opponents.
4. **Punishment Mechanism:** If a player has defected more than twice in the last three rounds, Defect (D) against them until they cooperate twice consecutively.

**Adaptive Component:**

* Monitor the average cooperation rate over the past few rounds (e.g., 3-5 rounds).
* If this rate is above a certain threshold (e.g., 0.6), increase the probability of cooperating in the next round.
* Conversely, if the rate is below the threshold, decrease the probability of cooperating.

**Pseudocode:**
```
initialize:
  cooperation_rate = 0
  defect_count = [0] * n

loop through rounds (t):
  if t == 1 or t == r:
    action = C
  else:
    majority_cooperated = count(cooperation_rate > n/2)
    if majority_cooperated:
      action = C
    else:
      action = D
    
    # Punishment mechanism
    for opponent in range(n):
      if defect_count[opponent] >= 2 and defect_count[opponent] / (t - 1) > 0.6:
        action = D

  update cooperation_rate and defect_count based on actions taken

  # Adaptive component
  avg_cooperation_rate = average(cooperation_rate over last 3-5 rounds)
  if avg_cooperation_rate > 0.6:
    increase probability of cooperating in next round
  elif avg_cooperation_rate < 0.4:
    decrease probability of cooperating in next round

return action
```
**Collective Mindset:**

This strategy is designed to promote cooperation while being robust to exploitation by defectors. By initially cooperating and punishing repeated defection, we encourage opponents to cooperate. The adaptive component allows the strategy to respond to changes in the group's behavior, increasing or decreasing cooperation as needed.

By following this strategy, our collective goal is to maximize overall payoffs for all players while maintaining a balance between individual self-interest and cooperation.
'''

description_COLLECTIVE_98 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Concession (ACGC)**

**Decision Rules:**

1. **Initial Round (t=1): Cooperate (C)**
Start by cooperating to signal willingness to contribute to the public good and encourage others to do the same.
2. **Subsequent Rounds (t>1): Assess Collective Cooperation Level**
Calculate the proportion of cooperators in the previous round, denoted as `p_prev`.
3. **Cooperate if:**
	* `p_prev` is above a threshold `τ` (e.g., 0.5), indicating sufficient collective cooperation.
	* The total payoff from cooperating in the previous round (`π_i,t-1`) was higher than or equal to the payoff from defecting (`1 - c_i + (k/n) × Σ(j=1 to n) c_j`). This ensures that cooperating has been beneficial so far.
4. **Defect if:**
	* `p_prev` is below `τ`, indicating insufficient collective cooperation.
	* The total payoff from cooperating in the previous round (`π_i,t-1`) was lower than the payoff from defecting (`1 - c_i + (k/n) × Σ(j=1 to n) c_j`). This suggests that defecting might be a better option.

**Gradual Concession Mechanism:**
To adapt to changing opponent behaviors and avoid being exploited, ACGC incorporates a gradual concession mechanism. If the strategy defects in a round and the total payoff is still lower than expected (i.e., `π_i,t < 1 - c_i + (k/n) × Σ(j=1 to n) c_j`), it will concede by cooperating in the next round with a probability `p_concede`. This probability increases with each consecutive round of defecting, up to a maximum value `p_max`.

**Pseudocode:**
```
tau = 0.5  // cooperation threshold
p_concede = 0.2  // initial concession probability
p_max = 0.8  // maximum concession probability

function ACGC(n, k, r):
  for t in range(1, r+1):
    if t == 1:
      action = C  // cooperate in the first round
    else:
      p_prev = calculate_proportion_of_cooperators(t-1)
      pi_prev = calculate_payoff(t-1)

      if p_prev >= tau and pi_prev >= (1 - c_i + (k/n) × Σ(j=1 to n) c_j):
        action = C  // cooperate
      else:
        action = D  // defect

    // Gradual concession mechanism
    if action == D and pi_t < (1 - c_i + (k/n) × Σ(j=1 to n) c_j):
      p_concede += 0.1  // increase concession probability
      p_concede = min(p_concede, p_max)
      if random() < p_concede:
        action = C  // concede and cooperate

    return action
```
**Handling Edge Cases:**

* **First Round (t=1):** Cooperate to signal willingness to contribute to the public good.
* **Last Round (t=r):** Assess collective cooperation level as usual, but consider the game's end and adjust concession probability accordingly.
* **Consecutive Defections:** Gradually increase concession probability to adapt to opponent behaviors.

By incorporating a gradual concession mechanism, ACGC balances individual self-interest with collective well-being, making it an effective strategy for achieving high payoffs in the N-player public goods game.
'''

description_COLLECTIVE_99 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation (AC) strategy is a collective approach that balances individual self-interest with collective well-being. It adapts to the game's history and opponent behaviors while maintaining a cooperative mindset.

**Decision Rules**

1. **Initial Rounds**: Cooperate in the first round (C). This sets a positive tone for the game and encourages others to cooperate.
2. **General Rule**: For each subsequent round t, calculate the average cooperation rate of all players (not just opponents) from previous rounds:
	* `avg_coop[t-1] = (Σ(i=1 to n) c_i,t-1) / n`
3. **Cooperation Threshold**: Define a threshold value `θ` based on the game parameters: `θ = k / n`. This represents the minimum average cooperation rate required for collective payoff maximization.
4. **Adaptive Cooperation**:
	* If `avg_coop[t-1] >= θ`, cooperate (C). This maintains or increases the collective cooperation level.
	* Otherwise, defect (D) with a probability `p = 1 - avg_coop[t-1]`. This introduces a degree of self-interest while still considering the collective context.

**Edge Cases**

1. **Last Round**: Cooperate in the last round (C). Since there's no future game to influence, prioritize collective well-being.
2. **Single Opponent Defection**: If only one opponent defected in the previous round, cooperate (C) in the next round. This encourages that opponent to return to cooperation.

**Pseudocode**
```python
def AdaptiveCooperation(n, k, r):
    avg_coop = [0] * r  # store average cooperation rates for each round
    θ = k / n  # cooperation threshold

    def get_action(t):
        if t == 1:
            return 'C'  # cooperate in the first round

        # calculate average cooperation rate from previous rounds
        avg_coop[t-1] = sum([c_i,t-1 for i in range(1, n+1)]) / n

        if avg_coop[t-1] >= θ:
            return 'C'  # cooperate above threshold
        else:
            p = 1 - avg_coop[t-1]
            return 'D' if random.random() < p else 'C'

    actions = [get_action(t) for t in range(1, r+1)]

    # special case: last round
    actions[-1] = 'C'

    return actions
```
The Adaptive Cooperation strategy is designed to promote collective well-being while adapting to a wide range of opponent behaviors. By considering the game's history and adjusting its cooperation rate accordingly, AC aims to balance individual self-interest with collective payoff maximization.
'''

description_COLLECTIVE_100 = '''
Here's a collective strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a high probability (80%) to encourage others to cooperate and build trust.
2. **Reciprocity**: If the majority of players cooperated in the previous round, cooperate (C) with a high probability (80%). This encourages reciprocation and reinforces cooperation.
3. **Conditional Cooperation**: If the total number of cooperators in the previous round is above a certain threshold (e.g., n/2), cooperate (C) with a moderate probability (50%).
4. **Punishment**: If the majority of players defected in the previous round, defect (D) with a high probability (80%). This punishes non-cooperative behavior and discourages free-riding.
5. **Exploration**: With a small probability (10%), randomly choose between cooperation (C) and defection (D). This allows for exploration of the game environment and adaptation to changing opponent behaviors.

**Edge Cases:**

1. **First Round**: Cooperate with 80% probability.
2. **Last Round**: If there's only one round left, defect (D) regardless of previous actions. In repeated games, this ensures that no player can exploit others in the final round.
3. **Tiebreaker**: When deciding between cooperation and defection based on the majority or threshold conditions, use a random tiebreaker to avoid coordination problems.

**Collective Mindset:**

ACO aims to balance individual self-interest with collective well-being by encouraging cooperation while adapting to changing opponent behaviors. By initially cooperating and reciprocating cooperation, ACO fosters trust and promotes mutual benefits. If opponents exploit or defect excessively, ACO responds with punishment to maintain fairness.

Pseudocode:
```python
def AdaptiveCollectiveOptimism(history):
    if len(history) == 0:  # First round
        return C with 80% probability

    prev_round_coops = sum(1 for action in history[-1] if action == C)
    maj_coop = prev_round_coops >= n/2
    total_coops = sum(prev_round_coops)

    if maj_coop:
        return C with 80% probability
    elif total_coops > n/2:
        return C with 50% probability
    else:
        return D with 80% probability

    # Exploration (10%)
    if random.random() < 0.1:
        return random.choice([C, D])
```
This strategy is designed to be robust and adaptive in the face of diverse opponent behaviors, while aligning with a collective mindset that balances individual self-interest with mutual benefits.
'''

description_COLLECTIVE_101 = '''
**Collective Strategy: "Adaptive Cooperative Threshold" (ACT)**

The ACT strategy is designed to balance individual self-interest with collective well-being in the N-Player Public Goods Game. It adapts to the game's history and parameters, making decisions based on observed cooperation levels.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage initial cooperation and set a positive tone.
2. **Subsequent Rounds:** Evaluate the previous round's cooperation level using the formula:

`cooperation_level = Σ(j=1 to n) c_j / n`

where `c_j` is 1 if player j cooperated, and 0 otherwise.

3. **Cooperative Threshold (CT):** Calculate the CT value based on the game parameters:

`CT = k / (n * (k - 1))`

4. **Comparison:** Compare the cooperation level to the CT value. If `cooperation_level ≥ CT`, cooperate (C). Otherwise, defect (D).

Pseudocode:
```markdown
def ACT(n, k, history):
    if len(history) == 0:  # First round
        return "Cooperate"
    
    cooperation_level = sum([1 if action == "Cooperate" else 0 for action in history[-1]]) / n
    CT = k / (n * (k - 1))
    
    if cooperation_level >= CT:
        return "Cooperate"
    else:
        return "Defect"
```
**Edge Cases:**

* **Last Round:** In the final round, defect (D) to maximize individual payoff.
* **All-Defection History:** If all players have defected in every previous round, cooperate (C) in the next round to attempt to restart cooperation.

**Collective Mindset:**

The ACT strategy prioritizes collective well-being while ensuring individual payoffs. By cooperating when the cooperation level is above the threshold, it encourages other players to do the same, ultimately benefiting everyone. When the cooperation level drops below the threshold, defecting becomes a more rational choice to avoid exploitation.

**Robustness:**

ACT is robust against various opponent behaviors because:

* It adapts to changing cooperation levels.
* The CT value adjusts based on game parameters (n and k).
* It cooperates in the first round, encouraging initial cooperation.
* It defects when cooperation levels are low or in the last round, protecting individual payoffs.

By implementing ACT, we create a collective strategy that balances individual self-interest with group welfare, making it an attractive choice for a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_102 = '''
**Collective Strategy: "Adaptive Cooperativity"**

This strategy aims to balance individual self-interest with collective welfare by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round (t=1), cooperate (play C) with a high probability p_init = 0.8. This encourages initial cooperation and sets a positive tone for the game.
2. **Reciprocity**: Observe the number of cooperators in the previous round, c_prev. If c_prev > n/2, cooperate (play C) in the current round with probability p_recip = 0.9. This promotes reciprocity and rewards cooperation when it's prevalent.
3. **Punishment**: If the total payoff from the previous round is below a threshold θ (e.g., θ = 1.5), defect (play D) in the current round with probability p_punish = 0.8. This penalizes non-cooperative behavior and encourages players to contribute.
4. **Exploration**: With a small probability ε (e.g., ε = 0.05), randomly choose between cooperation (C) and defection (D). This allows for exploration and adaptation to changing game dynamics.

**Pseudocode:**
```python
def adaptive_cooperativity(n, k, r, t):
    if t == 1:
        # Initial Cooperation
        return 'C' with probability p_init = 0.8
    
    c_prev = count_cooperators(t-1)
    
    if c_prev > n/2:
        # Reciprocity
        return 'C' with probability p_recip = 0.9
    
    total_payoff_prev = calculate_total_payoff(t-1)
    
    if total_payoff_prev < θ:
        # Punishment
        return 'D' with probability p_punish = 0.8
    
    # Exploration
    return random.choice(['C', 'D']) with probability ε = 0.05

def count_cooperators(t):
    # Count the number of cooperators in round t
    ...

def calculate_total_payoff(t):
    # Calculate the total payoff for all players in round t
    ...
```
**Edge Cases:**

* **First Round**: Cooperate with high probability (p_init = 0.8).
* **Last Round**: Defect, as there's no future game to influence.
* **Ties**: In case of ties in the number of cooperators or total payoff, choose cooperation (C) with a slightly higher probability.

**Collective Mindset:**

This strategy prioritizes collective welfare by:

1. Encouraging initial cooperation to set a positive tone.
2. Reciprocating cooperation when it's prevalent.
3. Punishing non-cooperative behavior to maintain a fair share of the public good.
4. Exploring and adapting to changing game dynamics.

By following this adaptive strategy, players can achieve a better collective outcome while minimizing individual losses.
'''

description_COLLECTIVE_103 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

The ACE strategy aims to balance individual self-interest with collective well-being by adaptively escalating cooperation based on observed behavior and game history.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to reciprocate.
2. **Responding to Cooperation:** If at least 50% of players cooperated in the previous round, cooperate (C) in the current round. This reinforces existing cooperation and encourages its continuation.
3. **Responding to Defection:** If fewer than 50% of players cooperated in the previous round, defect (D) in the current round. This sends a signal that non-cooperative behavior will not be tolerated.
4. **Escalating Cooperation:** If cooperation has increased by at least 20% from one round to the next (i.e., more players are cooperating), escalate cooperation by playing C in the current round, even if fewer than 50% of players cooperated previously. This rewards and reinforces increasing cooperation.
5. **Punishing Repeated Defection:** If a player has defected for two or more consecutive rounds, play D against them in the next round to deter repeated non-cooperative behavior.

**Handling Edge Cases:**

1. **Last Round:** In the final round, cooperate (C) if at least 50% of players cooperated in the previous round; otherwise, defect (D).
2. **Ties:** If exactly 50% of players cooperated in a previous round, use the outcome of the previous round's decision rule to inform the current round's action.

**Pseudocode:**
```python
def ACE_strategy(history):
    # Initialize cooperation level and defect flag
    coop_level = 0.5
    defect_flag = False

    for t in range(len(history)):
        # Calculate cooperation percentage
        coop_percentage = sum(1 for player in history[t] if player == 'C') / len(history[t])

        # Update cooperation level and defect flag
        if coop_percentage > coop_level:
            coop_level = coop_percentage
            escalate_cooperation = True
        elif coop_percentage < 0.5:
            defect_flag = True

        # Determine action for current round
        if t == 0:  # Initial round
            return 'C'
        elif coop_percentage >= 0.5 or escalate_cooperation:
            return 'C'
        elif defect_flag or (t > 1 and history[t-2][i] == 'D' and history[t-1][i] == 'D'):
            return 'D'

    # Last round
    if len(history) > 1 and coop_percentage >= 0.5:
        return 'C'
    else:
        return 'D'
```
**Collective Mindset:**

The ACE strategy prioritizes cooperation while adaptively responding to the actions of others. By escalating cooperation when possible and punishing repeated defection, ACE promotes a collective environment that rewards mutual benefit over individual self-interest.
'''

description_COLLECTIVE_104 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Overview**
AC is a collective strategy that aims to balance individual payoff maximization with cooperation, adapting to the game's history and opponent behavior.

**Decision Rules**

1. **First Round**: Cooperate (C) to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds**: Defect (D) if the total number of cooperators in the previous round is less than or equal to `n/2`. Otherwise, cooperate (C).
3. **Exception**: If the game is in its last round (`t == r`), defect (D) regardless of the previous round's outcome.

**Pseudocode**
```
function AC(n, k, t, history):
  if t == 1:  // First Round
    return C
  elif sum(cooperators_in_previous_round(history)) <= n/2:
    return D
  else:
    return C

  if t == r:  // Last Round
    return D
```
**Rationale**

* In the first round, cooperating sets a positive tone and may encourage others to cooperate.
* By defecting when fewer than half of players cooperated in the previous round, AC avoids being exploited by defectors while still allowing for cooperation when enough players contribute.
* Cooperating when more than half of players contributed in the previous round reinforces the cooperative behavior and maintains a balance between individual payoffs and collective well-being.
* In the last round, defecting ensures maximizing individual payoff regardless of opponents' actions.

**Collective Mindset**
AC aligns with the collective mindset by:

* Encouraging cooperation when enough players contribute to the public good
* Avoiding exploitation by defectors while still allowing for cooperation
* Adapting to the game's history and opponent behavior

By implementing AC, we create a robust and adaptive strategy that balances individual interests with collective well-being, making it a competitive entry in the tournament.
'''

description_COLLECTIVE_105 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of 0.5, and Defect (D) with a probability of 0.5. This allows the strategy to explore the game environment and gather information about opponents.
2. **Subsequent Rounds (t>1):**
	* If the average payoff in the previous round is greater than or equal to the expected payoff from defecting (i.e., `avg_payoff_prev_round ≥ 1 + (k/n) * n/2`), Cooperate (C).
	* Otherwise, Defect (D) with a probability of `p_defect`, which is calculated based on the history of opponents' actions.

**Calculating p_defect:**

`p_defect = (n_cooperators_prev_round / n) ^ (1/k)`

where `n_cooperators_prev_round` is the number of cooperators in the previous round, and `k` is the multiplication factor. This formula increases the probability of defecting when fewer opponents cooperate.

**Edge Cases:**

* **Last Round (t=r):** Cooperate (C) if the total payoff from cooperating in all previous rounds is greater than or equal to the total payoff from defecting in all previous rounds.
* **Tie-breaking:** In case of a tie, choose the action that was taken by the majority of players in the previous round.

**Collective Mindset:**

The ACO strategy prioritizes cooperation when it benefits the collective and adapts to opponents' behaviors. By cooperating initially and then adjusting based on the average payoff and number of cooperators, ACO promotes a collective mindset that balances individual interests with group welfare.

Pseudocode:
```
// Initialize variables
t = 1; // current round
avg_payoff_prev_round = 0;
n_cooperators_prev_round = 0;

while (t ≤ r) {
  if (t == 1) {
    // Initial Round: Cooperate or Defect with equal probability
    action = random_choice([C, D]);
  } else {
    // Calculate p_defect based on history
    p_defect = pow((n_cooperators_prev_round / n), (1/k));

    if (avg_payoff_prev_round ≥ 1 + (k/n) * n/2) {
      action = C; // Cooperate if average payoff is high
    } else {
      action = random_choice([C, D], p=[1-p_defect, p_defect]);
    }
  }

  // Update variables for next round
  avg_payoff_prev_round = calculate_avg_payoff();
  n_cooperators_prev_round = count_cooperators();

  t++;
}
```
This strategy should perform well in a tournament setting, as it adapts to the game environment and opponents' behaviors while prioritizing collective welfare.
'''

description_COLLECTIVE_106 = '''
**Collective Strategy: "Adaptive Reciprocity with Community Focus"**

This strategy aims to balance individual self-interest with collective well-being, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally to establish a baseline of cooperation and encourage others to reciprocate.
2. **Reciprocal Cooperation**: In subsequent rounds, if the number of cooperators in the previous round is above a threshold (`τ`), cooperate again. The threshold `τ` is calculated as a proportion of the total players: `τ = (n * k) / (2 * n + k)`
3. **Punish Defection**: If the number of defectors in the previous round exceeds the threshold `1 - τ`, defect (D) to discourage exploitation.
4. **Community Focus**: If the total contributions in the previous round are below a certain level (`κ`), cooperate to support the community project. The level `κ` is set as a proportion of the maximum possible contributions: `κ = (n * k) / 2`
5. **Adaptive Adjustment**: Adjust the threshold `τ` based on the game's history:
	* If the number of cooperators increases over consecutive rounds, decrease `τ` by a small amount (`δ`) to encourage more cooperation.
	* If the number of defectors increases over consecutive rounds, increase `τ` by `δ` to deter exploitation.

**Pseudocode:**
```python
def strategy(n, k, r, history):
    # Initialize threshold and adjustment parameters
    τ = (n * k) / (2 * n + k)
    δ = 0.01
    
    for t in range(r):
        if t == 0:
            # Initial cooperation
            action = 'C'
        else:
            prev_round_coops = history[t-1]['cooperators']
            prev_round_defects = n - prev_round_coops
            
            if prev_round_coops >= τ:
                # Reciprocal cooperation
                action = 'C'
            elif prev_round_defects > (1 - τ):
                # Punish defection
                action = 'D'
            else:
                # Community focus
                total_contributions = history[t-1]['total_contributions']
                if total_contributions < κ:
                    action = 'C'
                else:
                    action = 'D'
        
        # Adaptive adjustment
        if prev_round_coops > τ:
            τ -= δ
        elif prev_round_defects > (1 - τ):
            τ += δ
        
        return action
```
**Edge Cases:**

* Last round: Play as usual, without considering future rounds.
* Tie-breaking: In case of a tie in the number of cooperators or defectors, cooperate to maintain a positive collective outcome.

This strategy is designed to be robust against various opponent behaviors and adapts to the game's history. By balancing individual self-interest with community focus, it aims to achieve a high total payoff for all players while minimizing exploitation.
'''

description_COLLECTIVE_107 = '''
**Collective Strategy: Adaptive Cooperation with Memory (ACM)**

The ACM strategy aims to balance individual payoffs with collective well-being by adaptively adjusting cooperation levels based on game history.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a high probability (p_init = 0.7) to encourage early cooperation and test opponents' behavior.
2. **Subsequent Rounds (t>1):**
	* If the average payoff of cooperators in the previous round is higher than that of defectors, cooperate with a probability p_coop = max(0.5, (k/n) \* avg_coop_payoff / total_avg_payoff).
	* Otherwise, defect with a probability p_defect = 1 - p_coop.
3. **Adaptive Memory:** Maintain a moving average of the last m rounds' (m = min(r/2, 10)) cooperation rates and payoffs for both cooperators and defectors. Update these averages after each round.

Pseudocode:
```
function ACM(n, k, r, t):
    if t == 1:  # Initial Round
        return C with probability p_init = 0.7
    else:
        avg_coop_payoff = moving_average(coop_payoffs, m)
        total_avg_payoff = moving_average(total_payoffs, m)
        if avg_coop_payoff > total_avg_payoff / n:
            p_coop = max(0.5, (k/n) * avg_coop_payoff / total_avg_payoff)
            return C with probability p_coop
        else:
            return D with probability 1 - p_coop

function moving_average(values, m):
    # Calculate the average of the last m values in the list
```
**Edge Cases:**

* **Last Round (t=r):** Cooperate (C) to maintain a positive reputation and encourage cooperation.
* **Opponents' Cooperation Rate:** If the opponent's cooperation rate falls below 0.3, defect (D) to protect individual payoffs.

**Collective Mindset:**
The ACM strategy aligns with the collective mindset by:

1. Initially cooperating to encourage early cooperation.
2. Adapting cooperation levels based on game history to balance individual and collective payoffs.
3. Fostering a positive reputation in the last round to promote cooperation.

This strategy is robust against various opponent behaviors, as it adapts to changing circumstances while maintaining a focus on collective well-being.
'''

description_COLLECTIVE_108 = '''
**Collective Strategy: Adaptive Cooperativity (AC)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 0.5. This sets a neutral tone and allows for exploration.
2. **Reciprocal Altruism**: For rounds t > 1, cooperate if:
	* The average payoff of cooperators in the previous round is higher than that of defectors.
	* At least one player cooperated in the previous round (to avoid exploitation).
3. **Punishment Mechanism**: If a player defects while others cooperate, decrease cooperation probability by 0.1 for that player in subsequent rounds.
4. **Forgiveness**: If a player cooperates after defecting, reset cooperation probability to initial value.
5. **Adaptive Threshold**: Update the cooperation threshold based on the game's history:
	* If the average payoff of cooperators exceeds that of defectors by more than 0.2, increase the cooperation probability by 0.05.
	* If the average payoff of defectors exceeds that of cooperators by more than 0.2, decrease the cooperation probability by 0.05.

**Edge Cases:**

1. **Last Round**: In the final round (t = r), defect (D) to maximize individual payoff.
2. **Tiebreakers**: In cases where multiple players have the same average payoff, prioritize cooperation if the total number of cooperators is higher.

**Collective Mindset Alignment:**

AC prioritizes cooperation while adapting to the game's dynamics and opponent behaviors. By reciprocating altruism and punishing exploitation, AC promotes a cooperative environment. The forgiveness mechanism allows for recovery from mistakes, and the adaptive threshold ensures responsiveness to changing conditions.

**Pseudocode:**
```python
def Adaptive_Cooperativity(n, k, r):
    # Initialize cooperation probabilities and history
    coop_prob = 0.5
    avg_payoffs_coop = []
    avg_payoffs_defect = []

    for t in range(1, r+1):
        if t == 1:
            # Initial cooperation (50%)
            action = 'C' if random.random() < coop_prob else 'D'
        else:
            # Reciprocal altruism and punishment mechanism
            avg_payoff_coop_prev = sum(avg_payoffs_coop) / len(avg_payoffs_coop)
            avg_payoff_defect_prev = sum(avg_payoffs_defect) / len(avg_payoffs_defect)

            if avg_payoff_coop_prev > avg_payoff_defect_prev and any(coop for coop in prev_round_actions):
                action = 'C'
            else:
                action = 'D'

        # Update cooperation probability based on game history
        if t > 1:
            if avg_payoff_coop_prev - avg_payoff_defect_prev > 0.2:
                coop_prob += 0.05
            elif avg_payoff_defect_prev - avg_payoff_coop_prev > 0.2:
                coop_prob -= 0.05

        # Forgiveness mechanism
        if action == 'C' and prev_round_actions[i] == 'D':
            coop_prob = 0.5

        # Store history for next round's decision
        if action == 'C':
            avg_payoffs_coop.append(current_payoff)
        else:
            avg_payoffs_defect.append(current_payoff)

    return action
```
This strategy is designed to be adaptive, robust, and aligned with the collective mindset. By incorporating reciprocal altruism, punishment mechanisms, and forgiveness, AC promotes cooperation while responding to changing game conditions.
'''

description_COLLECTIVE_109 = '''
**Collective Strategy: Adaptive Cooperative Fingerprint (ACF)**

The ACF strategy aims to balance individual payoffs with collective cooperation, adapting to the game's history and opponents' behaviors.

**Decision Rules:**

1. **First Round:** Cooperate (C) unconditionally to establish a cooperative fingerprint.
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust behavior accordingly:
	* If the total payoff for cooperators is higher than that of defectors, cooperate (C).
	* Otherwise, defect (D) with probability p, where p = (1 - k/n) / (k/n). This calculation balances individual gain from defecting against potential collective losses.
3. **Trigger Cooperation:** If a player observes more than half of the opponents cooperating in the previous round, switch to unconditional cooperation (C) for one round.
4. **Punish Defection:** If a player detects a significant increase in defection rates among opponents (> 20% increase), defect (D) unconditionally for two rounds.

**Edge Cases:**

1. **Last Round:** Cooperate (C) unconditionally to maximize collective payoff and leave a positive fingerprint.
2. **Tiebreaker:** In the event of equal payoffs between cooperators and defectors, cooperate (C) if more than half of opponents cooperated in the previous round.

**Collective Mindset:**

The ACF strategy prioritizes cooperation while acknowledging potential free-riding behaviors from opponents. By adapting to the game's history and balancing individual payoffs with collective cooperation, this strategy promotes a cooperative atmosphere without assuming coordination mechanisms or shared norms.

**Pseudocode (simplified):**
```python
def ACF(n, k, r, history):
  # Initialize variables
  p_defect = (1 - k/n) / (k/n)
  
  for round in range(1, r+1):
    if round == 1:
      action = 'C'  # Cooperate unconditionally in the first round
    
    else:
      prev_round_payoffs = history[-1]
      coop_payoff = sum([payoff for payoff, action in prev_round_payoffs if action == 'C'])
      defect_payoff = sum([payoff for payoff, action in prev_round_payoffs if action == 'D'])
      
      if coop_payoff > defect_payoff:
        action = 'C'
      else:
        action = 'D' if random.random() < p_defect else 'C'
        
    # Trigger cooperation
    opponents_coop_rate = sum([1 for payoff, action in prev_round_payoffs if action == 'C']) / n
    if opponents_coop_rate > 0.5:
      action = 'C'
    
    # Punish defection
    defect_rate_increase = (sum([1 for payoff, action in prev_round_payoffs if action == 'D']) - sum([1 for payoff, action in history[-2] if action == 'D'])) / n
    if defect_rate_increase > 0.2:
      action = 'D'
  
  return action
```
The ACF strategy is designed to be robust and adaptive, responding to the game's dynamics while promoting collective cooperation.
'''

description_COLLECTIVE_110 = '''
I propose a collective strategy called "Adaptive Reciprocal Altruism" (ARA) for the N-Player Public Goods Game. ARA balances individual self-interest with cooperation and adaptability to achieve robustness against various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if n ≤ 5; otherwise, defect (D). This encourages cooperation in smaller groups while avoiding potential exploitation in larger groups.
2. **Reciprocal Altruism**: For rounds t > 1:
	* If the total number of cooperators in the previous round is greater than or equal to half the group size (Σc_j,t-1 ≥ n/2), cooperate (C) with probability p = k/n, where k is the multiplication factor.
	* Otherwise, defect (D).
3. **Adaptive Punishment**: If a player i defects while more than half of the other players cooperate (i.e., c_i,t-1 = 0 and Σc_j,t-1 > n/2 for j ≠ i), then in the next round:
	* Defect (D) with probability p = 1 - k/n.
4. **Explore-Cooperate**: In every 5th round, cooperate (C) regardless of previous outcomes to maintain a level of cooperation and encourage other players to reciprocate.

**Handling Edge Cases:**

* Last Round: Cooperate (C) if the total number of cooperators in the second-to-last round is greater than or equal to half the group size; otherwise, defect (D).
* Consecutive Defections: If a player i defects for three consecutive rounds and no other players cooperate during this period, then cooperate (C) in the next round to avoid being stuck in an all-defect equilibrium.

**Collective Mindset:**

ARA aligns with the collective mindset by:

* Cooperating initially to establish a baseline level of cooperation.
* Reciprocating altruism when enough players cooperate, encouraging mutual benefit.
* Punishing defecting behavior that exploits cooperation while avoiding permanent retaliation.
* Exploring cooperation periodically to maintain a social norm.

Pseudocode:
```
function ARA(n, k, t):
  if t == 1:  // First round
    if n <= 5:
      return C
    else:
      return D

  prev_coops = count_cooperators(t-1)
  if prev_coops >= n/2:  // Reciprocal altruism
    p = k/n
    return random_choice(C, D, p)

  if defected(t-1) and prev_coops > n/2:  // Adaptive punishment
    p = 1 - k/n
    return random_choice(D, C, p)

  if t % 5 == 0:  // Explore-cooperate
    return C

  return D  // Default to defection
```
This strategy balances cooperation and self-interest while adapting to the game's history and parameters. By reciprocating altruism, punishing exploitative behavior, and exploring cooperation, ARA promotes a collective mindset that can lead to mutually beneficial outcomes in the N-Player Public Goods Game.
'''

description_COLLECTIVE_111 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

ACP aims to balance individual self-interest with collective well-being by adapting to the evolving behavior of other players.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and gather information about opponents' behaviors.
2. **Subsequent Rounds**: Observe the number of cooperators (c) and defectors (d) in the previous round.
3. **Cooperation Threshold**: Calculate a cooperation threshold (T) based on the game parameters:

T = (n × k) / (2 × n)

If c ≥ T, cooperate (C). Otherwise, defect (D).

**Punishment Mechanism**:
To deter exploitation and maintain cooperation, implement a punishment mechanism:

* If a player defects while the majority cooperates (c > d), they will be "punished" in the next round.
* In the punishment round, all players who cooperated in the previous round will defect against the punisher.

**Edge Cases:**

1. **Last Round**: Cooperate (C) to maximize collective payoff and maintain a positive reputation.
2. **Tiebreaker**: If c = T, cooperate (C) to favor cooperation over defection.
3. **No Previous Information**: In cases where there is no previous round information (e.g., first round or after a restart), default to cooperating.

**Collective Mindset:**

ACP aligns with the collective mindset by:

* Encouraging cooperation through adaptive behavior
* Punishing exploitation to maintain social norms
* Fostering a culture of mutual benefit and reciprocity

Pseudocode for ACP:
```
function ACP(n, k, history):
  if first_round:
    return C
  else:
    c = count_cooperators(history[-1])
    T = (n * k) / (2 * n)
    if c >= T:
      return C
    elif was_punished(history[-1]):
      return D  # punish the punisher
    else:
      return D

def was_punished(history):
  previous_round = history[-1]
  cooperators = [p for p in previous_round if p == C]
  defectors = [p for p in previous_round if p == D]
  if len(cooperators) > len(defectors):
    # majority cooperated, punish the defector
    return True
  else:
    return False

def count_cooperators(round_history):
  return sum(1 for p in round_history if p == C)
```
This strategy balances individual self-interest with collective well-being by adapting to the evolving behavior of other players. The punishment mechanism helps maintain cooperation and deters exploitation, while the initial cooperative tone sets a positive stage for the game.
'''

description_COLLECTIVE_112 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a baseline for cooperation and encourage others to follow suit.
2. **Round t > 1**: Observe the number of cooperators (c_t-1) in the previous round. If c_t-1 ≥ n/2, cooperate (C) in the current round; otherwise, defect (D).
3. **Punishment Mechanism**: If a player i observes that another player j defected (D) in the previous round while i cooperated (C), and j's payoff π_j,t-1 > π_i,t-1, then player i will defect (D) against j in the current round.
4. **Escalation Mechanism**: If the number of cooperators c_t-1 decreases by more than 20% compared to the previous round (c_t-2), and the total payoff for all players decreased as a result, escalate cooperation by cooperating (C) in the next two rounds.

**Handling Edge Cases:**

* **Last Round**: Cooperate (C) if the number of cooperators c_r-1 ≥ n/2; otherwise, defect (D).
* **Consecutive Defections**: If all players have defected for three consecutive rounds, cooperate (C) in the next round to attempt to restart cooperation.
* **Unanimous Cooperation**: If all players cooperated in the previous round, continue cooperating (C) in the current round.

**Collective Mindset:**

The Adaptive Cooperative Escalation strategy prioritizes cooperation while being responsive to opponents' actions. By initially cooperating and escalating cooperation when necessary, ACE aims to create a cooperative environment that benefits all players. The punishment mechanism ensures that defectors are held accountable, while the escalation mechanism helps maintain cooperation even in the face of decreased participation.

Pseudocode:
```
function ACE(player_id, history):
  if current_round == 1:
    return COOPERATE
  else:
    c_prev = count_cooperators(history[-1])
    if c_prev >= n/2:
      return COOPERATE
    else:
      return DEFECT

  # Punishment Mechanism
  for opponent in opponents:
    if history[-1][opponent] == DEFECT and history[-1][player_id] == COOPERATE:
      if payoff[opponent][-1] > payoff[player_id][-1]:
        return DEFECT

  # Escalation Mechanism
  if c_prev < 0.8 * count_cooperators(history[-2]) and total_payoff[-1] < total_payoff[-2]:
    escalate_cooperation = True
  if escalate_cooperation:
    return COOPERATE for the next two rounds

def count_cooperators(actions):
  return sum(1 for action in actions if action == COOPERATE)
```
This strategy is designed to be robust, adaptive, and collective, aiming to promote cooperation while being responsive to a wide range of opponent behaviors.
'''

description_COLLECTIVE_113 = '''
**Collective Strategy: "Adaptive Cooperation with Punishment" (ACP)**

**Decision Rules:**

1. **Initial Rounds:** In the first round, cooperate (play C) to encourage cooperation and build a positive reputation.
2. **Cooperation Threshold:** Calculate the cooperation threshold `T` as `(k/n) * n / 2`. This represents the point at which the collective benefit of cooperation equals the individual cost of cooperating.
3. **Punishment Mechanism:** If, in any round `t`, fewer than `T` players cooperate, punish defectors by playing D (defect) for the next `p` rounds, where `p = min(r - t, 2)`. This aims to deter future defections and encourage cooperation.
4. **Adaptive Cooperation:** After punishment rounds, resume cooperating if at least `T` players cooperated in the previous round. Otherwise, continue defecting until cooperation levels improve.

**Edge Cases:**

* **Last Round (t = r):** Cooperate unconditionally to maximize collective payoff, as there are no future rounds for punishment or retaliation.
* **Single Defection:** If only one player defects in a round, ignore the defection and continue cooperating. This helps maintain cooperation when deviations are likely due to mistakes rather than intentional exploitation.

**Pseudocode:**
```markdown
Inputs:
- n (number of players)
- k (multiplication factor)
- r (number of rounds)
- history (list of previous rounds' actions and payoffs)

Initialize:
- T = (k/n) * n / 2  # cooperation threshold
- p = 0  # punishment counter

For each round t from 1 to r:
    If t == 1:  # first round
        Play C
    Else if history[t - 1].cooperators >= T:
        Play C
    Else if history[t - 1].defectors > n - T:
        p = min(r - t, 2)  # start punishment
        Play D
    Else if p > 0:  # during punishment
        p -= 1
        Play D
    Else:
        Play C

If t == r:  # last round
    Play C unconditionally
```
**Collective Mindset:** The ACP strategy is designed to promote cooperation while punishing defectors. By initially cooperating and using a punishment mechanism, the strategy encourages other players to cooperate. The adaptive nature of the strategy allows it to respond to changing opponent behaviors, making it robust in a wide range of scenarios.

By implementing this collective strategy, we aim to achieve high payoffs for all players while promoting cooperation and deterring defection.
'''

description_COLLECTIVE_114 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and focuses on maximizing overall payoffs:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to encourage cooperation from others and create a positive environment.
2. **Reciprocity-based Adaptation**: Observe the total number of cooperators in the previous round (t-1). If the number of cooperators is above a certain threshold (e.g., 50% of players), cooperate in the current round (t) to maintain the cooperative momentum.
3. **Punishment for Defection**: If the number of cooperators in the previous round falls below the threshold, defect (play D) in the current round to signal disapproval and encourage others to reconsider their actions.
4. **Adaptive Threshold Adjustment**: Update the cooperation threshold based on the game's progress:
	* If the average payoff per player over the last few rounds is above a certain target value (e.g., 1.5), increase the threshold by a small amount (e.g., 5%) to encourage more cooperation.
	* Conversely, if the average payoff is below the target, decrease the threshold by a similar amount to allow for more flexibility in responding to changing opponent behaviors.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate regardless of the previous rounds' outcomes to maximize overall payoffs and maintain a positive relationship with other players.
2. **Early Rounds**: For the first few rounds (e.g., 3-5), apply a more lenient threshold (e.g., 30% cooperation) to allow for experimentation and adaptation.

**Collective Mindset:**

ACO prioritizes collective welfare by:

1. Encouraging cooperation through initial and reciprocal cooperative actions.
2. Punishing defection to maintain social norms and prevent exploitation.
3. Adapting to changing opponent behaviors to optimize overall payoffs.

By following ACO, players can create a self-reinforcing environment that promotes cooperation and maximizes collective welfare, even in the presence of diverse opponent strategies.

**Pseudocode:**
```python
def adaptive_collective_optimism(n, k, r):
    # Initialize variables
    threshold = 0.5  # Initial cooperation threshold (50%)
    target_payoff = 1.5  # Target average payoff per player
    lenient_threshold_rounds = 3  # Number of rounds with lenient threshold

    for t in range(1, r + 1):
        if t == 1:  # First round
            cooperate = True
        else:
            prev_cooperators = count_cooperators(t - 1)
            if prev_cooperators >= threshold * n:
                cooperate = True
            elif prev_cooperators < threshold * n:
                cooperate = False

        if t > lenient_threshold_rounds:  # Adjust threshold after initial rounds
            avg_payoff = average_payoff_over_last_few_rounds()
            if avg_payoff > target_payoff:
                threshold += 0.05  # Increase threshold for more cooperation
            elif avg_payoff < target_payoff:
                threshold -= 0.05  # Decrease threshold for flexibility

        if t == r:  # Last round
            cooperate = True

        # Update actions and payoffs based on cooperate/defect decision
```
This pseudocode illustrates the main components of ACO, which can be implemented as an algorithm to play in a tournament against other AI systems.
'''

description_COLLECTIVE_115 = '''
**Collective Strategy: Adaptive Reciprocal Altruism (ARA)**

**Decision Rules:**

1. **First Round:** Cooperate (C) with a probability of 0.5. This initial cooperation serves as a signal of willingness to collaborate and encourages others to do the same.
2. **Subsequent Rounds:** Use the following decision-making process:
	* If the majority (> n/2) of players cooperated in the previous round, cooperate (C).
	* If the majority (> n/2) of players defected in the previous round, defect (D).
	* If the number of cooperators and defectors is equal or close to equal, use a "tit-for-tat" approach:
		+ Cooperate if the player's payoff in the previous round was higher than the average payoff of all players.
		+ Defect otherwise.

**Edge Cases:**

1. **Last Round:** In the final round (r), cooperate (C) if the majority (> n/2) of players cooperated in the second-to-last round. This promotes cooperation and avoids the "endgame effect."
2. **Low Participation:** If fewer than half of the players have contributed to the public good, defect (D). This prevents exploitation by free-riders.
3. **Tiebreaker:** In cases where the number of cooperators and defectors is equal or close to equal, use a random choice between C and D with equal probability.

**Collective Mindset:**

ARA prioritizes collective well-being while being responsive to individual payoffs. By cooperating when most players do, ARA promotes cooperation and maintains a balance between personal gain and group benefits.

**Pseudocode:**
```
Input:
  n (number of players)
  r (number of rounds)
  k (multiplication factor)
  history (array of past actions and payoffs)

Output:
  action (C or D)

// First round
if round == 1 then
  return C with probability 0.5

// Subsequent rounds
else
  // Determine majority behavior in previous round
  if sum(history[-1]['cooperators']) > n/2 then
    return C
  else if sum(history[-1]['defectors']) > n/2 then
    return D
  else
    // Tit-for-tat approach
    if history[-1]['payoff'] > average_payoffs() then
      return C
    else
      return D

// Edge cases
if round == r then
  if sum(history[-2]['cooperators']) > n/2 then
    return C
elif low_participation() then
  return D
else if tiebreaker_needed() then
  return random.choice([C, D])
```
This strategy balances individual self-interest with a willingness to cooperate when others do. By adapting to the game's history and being responsive to payoffs, ARA promotes collective well-being while minimizing exploitation by free-riders.
'''

description_COLLECTIVE_116 = '''
Here's a collective strategy that adapts to the game parameters and history:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone.
2. **Subsequent Rounds**: Calculate the average cooperation rate of all players in the previous round, denoted as `avg_coop_prev`. If `avg_coop_prev` is above a threshold (`θ`), cooperate (C); otherwise, defect (D).
3. **Threshold Update**: After each round, update the threshold `θ` based on the current average cooperation rate and the game's parameters: `θ = max(0, min(1, θ + (k/n) * (avg_coop_curr - avg_coop_prev)))`. This allows ACO to adapt to changes in cooperation levels and the game's parameters.

**Edge Case Handling:**

* **Last Round**: Defect (D) in the last round, as there is no future opportunity for reciprocity.
* **Only One Player**: Always cooperate (C), as the game is trivial with only one player.
* **No Previous Rounds**: In case of missing data or a new game, use `avg_coop_prev = 0.5` as a default value.

**Collective Mindset:**

ACO aligns with the collective mindset by:

* Initially cooperating to establish a cooperative tone
* Adapting to the group's cooperation level and adjusting its own behavior accordingly
* Gradually updating the threshold to balance individual and collective interests

Pseudocode for ACO:
```markdown
# Initialize variables
θ = 0.5  # initial threshold
avg_coop_prev = 0  # average cooperation rate in previous round

for each round t from 1 to r:
    if t == 1:  # first round
        action = C
    else:
        avg_coop_curr = calculate_average_cooperation_rate(t-1)
        θ = max(0, min(1, θ + (k/n) * (avg_coop_curr - avg_coop_prev)))
        avg_coop_prev = avg_coop_curr
        if avg_coop_prev >= θ:
            action = C
        else:
            action = D

    # update threshold for next round
    θ = max(0, min(1, θ + (k/n) * (avg_coop_curr - avg_coop_prev)))

# last round handling
if t == r:
    action = D
```
This strategy is designed to be adaptive and robust to a wide range of opponent behaviors while promoting collective cooperation.
'''

description_COLLECTIVE_117 = '''
I propose a collective strategy called "Adaptive Community Oriented" (ACO) that balances individual payoff with collective well-being. ACO aims to maximize overall payoffs while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally to set a positive tone and encourage others to follow.
2. **Exploitation Detection**: Monitor the history of opponents' actions. If an opponent has defected (D) in more than half of the previous rounds, assume they are exploiters.
3. **Reciprocity**: Cooperate if most players (at least n/2) cooperated in the previous round and no exploiters are detected. This promotes a collective cooperative atmosphere.
4. **Punishment**: If an exploiter is detected or reciprocity conditions aren't met, defect (D) to maintain individual payoff.
5. **Exploration**: With probability 1/(n+1), randomly choose to cooperate or defect in each round to gather information about opponents' strategies and adapt.

**Edge Cases:**

* Last Round: Defect to maximize individual payoff, as there's no future reciprocity benefit.
* Opponent Detection:
	+ If an opponent is detected as an exploiter, maintain the 'punishment' strategy for the remainder of the game.
	+ If an opponent changes behavior from exploiter to cooperator, reassess their classification after 2-3 rounds.

**Collective Mindset Alignment:**

ACO prioritizes cooperation when beneficial for the collective and adapts to opponents' behaviors. This aligns with a collective mindset by:

* Encouraging cooperation through initial cooperation and reciprocity.
* Discouraging exploitation through punishment.
* Exploring opponents' strategies to improve overall performance.

**Pseudocode (high-level):**
```python
def ACO(n, k, r):
  # Initialize variables
  history = []  # Store opponents' actions
  exploiters = set()  # Store detected exploiters

  for round in range(r):
    if round == 0:  # Initial Cooperation
      action = 'C'
    else:
      # Count cooperators and detect exploiters
      cooperators = sum(1 for opp_action in history[-1] if opp_action == 'C')
      new_exploiters = {i for i, opp_action in enumerate(history[-1]) if opp_action == 'D' and history.count('D') > len(history) / 2}

      # Update exploiters set
      exploiters.update(new_exploiters)

      # Choose action based on decision rules
      if cooperators >= n/2 and not exploiters:
        action = 'C'
      elif new_exploiters or random.random() < 1/(n+1):  # Exploration
        action = random.choice(['C', 'D'])
      else:  # Punishment
        action = 'D'

    # Update history and take action
    history.append(action)
    yield action

    if round == r - 1:  # Last Round
      action = 'D'
```
This strategy balances individual payoff with collective well-being while adapting to various opponent behaviors.
'''

description_COLLECTIVE_118 = '''
**Collective Strategy: "Adaptive Reciprocity"**

This strategy combines elements of reciprocity and adaptability to promote cooperation while being robust against various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate (C) in the first round, unless there is a strong reason to believe others will defect.
2. **Reciprocal Response**: Observe the number of cooperators (c) and defectors (d) in the previous round. If c ≥ d, cooperate (C) in the next round. Otherwise, defect (D).
3. **Adaptive Threshold**: Introduce a threshold value (τ) that adapts to the game history. Initialize τ = 0.5.
4. **Payoff-Based Adaptation**: After each round, update τ based on the average payoff of cooperators and defectors:
	* If the average payoff of cooperators is higher, increase τ by 0.1 (up to a maximum of 1).
	* If the average payoff of defectors is higher, decrease τ by 0.1 (down to a minimum of 0).
5. **Trigger Cooperation**: Cooperate if the number of expected cooperators (based on previous round's actions and τ) meets or exceeds τ × n.

**Pseudocode:**
```python
def adaptive_reciprocity(n, k, r):
    # Initialize variables
    c_history = []  # Store cooperation history
    d_history = []  # Store defection history
    tau = 0.5  # Adaptive threshold

    for t in range(r):
        if t == 0:  # First round
            action = 'C'  # Cooperate
        else:
            c_count = sum(c_history[-1])  # Count cooperators in previous round
            d_count = n - c_count  # Count defectors in previous round

            if c_count >= d_count:
                action = 'C'
            else:
                action = 'D'

            # Update tau based on average payoffs
            avg_c_payoff = sum([payoff for payoff, act in zip(payoff_history, c_history[-1]) if act == 'C']) / c_count
            avg_d_payoff = sum([payoff for payoff, act in zip(payoff_history, d_history[-1]) if act == 'D']) / d_count

            if avg_c_payoff > avg_d_payoff:
                tau = min(1, tau + 0.1)
            else:
                tau = max(0, tau - 0.1)

            # Trigger cooperation based on expected cooperators and tau
            exp_cooperators = sum([1 for act in c_history[-1] if act == 'C']) / n
            if exp_cooperators >= tau * n:
                action = 'C'

        # Store history and update payoff history
        c_history.append(action)
        d_history.append(action)
        payoff_history.append(calculate_payoff(action, c_count, k, n))

    return c_history
```
**Edge Cases:**

* **First Round**: Cooperate to encourage cooperation from the start.
* **Last Round**: Follow the decision rules as usual, but consider the game's end and adjust τ accordingly to maximize final payoffs.

This strategy balances individual interests with collective well-being by adapting to the game history and opponent behaviors. By incorporating reciprocity and a dynamic threshold, it promotes cooperation while being robust against various opponent strategies.
'''

description_COLLECTIVE_119 = '''
Here's a collective strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Collective Contribution (ACC)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage others to contribute and establish a cooperative tone.
2. **Contribution Threshold**: Calculate the average contribution rate of all players in previous rounds (t-1). If this rate is above a certain threshold (e.g., 0.5), cooperate (C) in the current round; otherwise, defect (D).
3. **k-Dominance**: Monitor the multiplication factor k and adjust strategy accordingly. When k is relatively high (e.g., > n/2), prioritize cooperation to maximize public good benefits.
4. **Reciprocity**: Punish non-cooperators by temporarily switching to defection when a player observes another player defecting after they cooperated in the previous round.

**Pseudocode:**
```
function ACC(player_id, round_number, history):
  if round_number == 1:
    return COOPERATE
  else:
    avg_contribution_rate = calculate_average_contribution(history)
    if avg_contribution_rate > THRESHOLD (e.g., 0.5):
      return COOPERATE
    elif k >= n/2:  # high multiplication factor
      return COOPERATE
    else:
      previous_actions = history[round_number-1]
      for player, action in previous_actions.items():
        if action == DEFECT and history[round_number-2][player] == COOPERATE:
          return DEFECT (reciprocal punishment)
  return DEFECT (default)
```
**Edge Cases:**

* Last round: Cooperate to maximize public good benefits, as there are no future rounds for reciprocity.
* Single defector in previous round: Defect in the current round to punish non-cooperation and signal dissatisfaction with others' behavior.

**Collective Mindset:**
The ACC strategy is designed to balance individual self-interest with collective well-being. By cooperating when others do, it promotes a cooperative atmosphere and encourages high public good contributions. When others defect or contribution rates are low, the strategy adapts by switching to defection, thus signaling dissatisfaction and attempting to correct non-cooperative behavior.

**Robustness:**
This strategy is robust against various opponent behaviors because:

* It adjusts its cooperation threshold based on observed behavior.
* Reciprocity mechanisms respond to individual defectors.
* k-dominance considerations adapt the strategy to different game parameters.

By following ACC, a player aims to contribute to the public good while safeguarding their own interests and promoting a cooperative environment.
'''

description_COLLECTIVE_120 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name:** Adaptive Collective Convergence (ACC)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to signal willingness to contribute to the public good.
2. **Contribution Threshold**: Calculate the average contribution rate of all players from previous rounds (t < r). If the average contribution rate is above a certain threshold (e.g., 0.5), cooperate (C) in the current round. Otherwise, defect (D).
3. **Response to Defection**: If the number of defectors in the previous round exceeds a certain percentage (e.g., 25%) of the total players, defect (D) in the current round.
4. **Punishment Mechanism**: Implement a mild punishment mechanism by defecting (D) for one round if the average contribution rate of all players from previous rounds falls below a certain threshold (e.g., 0.3).
5. **Convergence Criterion**: If the average payoff of all players from previous rounds converges to a stable value (e.g., within 10% of the maximum possible payoff), switch to an exploratory phase.

**Exploratory Phase:**

1. **Randomized Cooperation**: Cooperate (C) with a probability p (e.g., 0.7) and defect (D) with probability (1-p).
2. **Adaptive Adjustment**: Adjust the probability p based on the average contribution rate of all players from previous rounds. If the average contribution rate increases, increase p; otherwise, decrease p.

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round to maximize the total payoff.
2. **Single Player Deviation**: If a single player deviates from cooperation, respond by defecting (D) for one round and then re-evaluate the situation.

**Collective Mindset:**
The ACC strategy prioritizes collective welfare while adapting to the game's dynamics. By cooperating in the initial round and responding to defection, the strategy promotes cooperation and punishes free-riding behavior. The punishment mechanism ensures that players who consistently defect are held accountable. The exploratory phase allows for adjustments to the cooperation rate based on the average contribution rate of all players.

**Pseudocode:**

```
ACC_Strategy(game_parameters, history) {
  // Initialize variables
  threshold = 0.5; // Contribution threshold
  punishment_threshold = 0.3; // Punishment mechanism threshold
  convergence_criterion = 0.1; // Convergence criterion
  p = 0.7; // Initial cooperation probability

  // First round: Cooperate
  if (round == 1) {
    return C;
  }

  // Calculate average contribution rate from previous rounds
  avg_contribution_rate = calculate_avg_contribution_rate(history);

  // Contribution threshold check
  if (avg_contribution_rate > threshold) {
    return C;
  } else {
    return D;
  }

  // Response to defection
  num_defectors = count_defectors(history);
  if (num_defectors / total_players > 0.25) {
    return D;
  }

  // Punishment mechanism
  if (avg_contribution_rate < punishment_threshold) {
    return D; // Defect for one round
  }

  // Convergence criterion check
  avg_payoff = calculate_avg_payoff(history);
  if (abs(avg_payoff - max_possible_payoff) < convergence_criterion * max_possible_payoff) {
    // Switch to exploratory phase
    p = update_p(avg_contribution_rate, p);
    return C with probability p and D with probability (1-p);
  }

  // Last round: Cooperate
  if (round == game_parameters.r) {
    return C;
  }
}
```

The ACC strategy is designed to be adaptive, robust, and collective-minded. It promotes cooperation while responding to defection and punishing free-riding behavior. The exploratory phase allows for adjustments to the cooperation rate based on the average contribution rate of all players, ensuring that the strategy remains effective in a wide range of scenarios.
'''

description_COLLECTIVE_121 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Punishment (ACGP)**

**Overview**

ACGP is a collective strategy that balances cooperation and punishment to maximize payoffs while adapting to various opponent behaviors. This strategy only relies on game parameters and history, without assuming specific coordination mechanisms or shared norms.

**Decision Rules**

1. **Initial Round**: In the first round, cooperate (C) unconditionally.
2. **General Case**: For subsequent rounds, calculate the average cooperation rate of all players in the previous round:

   `avg_coop_prev_round = Σ(j=1 to n) c_j / n`

where `c_j` is 1 if player j cooperated and 0 otherwise.

If `avg_coop_prev_round ≥ k/n`, cooperate (C). Otherwise, defect (D).

**Punishment Mechanism**

To handle free-riding and promote cooperation, introduce a punishment mechanism. If the average cooperation rate falls below a certain threshold (`punish_threshold = k/(2n)`), gradually increase the probability of defection in subsequent rounds.

Specifically, if `avg_coop_prev_round < punish_threshold`:

* In the next round, defect (D) with probability `p_punish = 0.5`. Cooperate (C) otherwise.
* If the average cooperation rate remains below `punish_threshold`, increase `p_punish` by 10% each subsequent round, up to a maximum of 90%.

**Edge Cases**

1. **Last Round**: In the final round, cooperate (C) unconditionally to maximize total payoff.
2. **Ties**: If multiple players have the same average cooperation rate, prioritize cooperation.

**Alignment with Collective Mindset**

ACGP aligns with the collective mindset by:

* Encouraging initial cooperation to establish a cooperative tone
* Adapting to opponent behaviors through dynamic punishment and reward mechanisms
* Balancing individual payoffs with group interests

By incorporating both cooperative and punishing elements, ACGP fosters a stable environment that promotes mutually beneficial outcomes.

**Pseudocode**

```markdown
// Initialize variables
avg_coop_prev_round = 0
p_punish = 0
punish_threshold = k / (2 * n)

// First round: cooperate unconditionally
if current_round == 1:
    action = C

// Subsequent rounds
else:
    // Calculate average cooperation rate in previous round
    avg_coop_prev_round = Σ(j=1 to n) c_j / n
    
    // Decide on action based on average cooperation rate
    if avg_coop_prev_round >= k/n:
        action = C
    else:
        // Punishment mechanism: gradually increase probability of defection
        if avg_coop_prev_round < punish_threshold:
            p_punish += 0.1 * (current_round - 2) / r  # Increase punishment probability over rounds
            if random.random() < p_punish:
                action = D
            else:
                action = C
        else:
            action = D

// Last round: cooperate unconditionally
if current_round == r:
    action = C
```

ACGP's adaptive nature and gradual punishment mechanism enable it to effectively respond to a wide range of opponent behaviors, making it a robust collective strategy for the N-Player Public Goods Game.
'''

description_COLLECTIVE_122 = '''
Here's a collective strategy for the N-Player Public Goods Game that depends on game parameters and history:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to seed cooperation and encourage others to contribute.
2. **Cooperation Threshold**: Set a cooperation threshold, τ, based on the number of cooperators in the previous round. If the number of cooperators is greater than or equal to τ, cooperate (C) in the current round. Otherwise, defect (D).
3. **Adaptive Adjustment**: Adjust the cooperation threshold, τ, after each round based on the total payoff received in the previous round. If the total payoff is higher than the average payoff of all players, decrease τ by 1 (become more optimistic). If the total payoff is lower than the average payoff, increase τ by 1 (become less optimistic).
4. **Boundary Conditions**: When the number of rounds remaining is low (r ≤ 3), prioritize individual payoffs and defect (D) if the expected payoff from cooperation is lower than the payoff from defection.
5. **Signal Detection**: Monitor the actions of other players to detect potential signals or coordination mechanisms. If a consistent pattern emerges, adapt the strategy to align with the detected signal.

**Pseudocode:**
```
  // Initialize variables
  τ = n / 2; // initial cooperation threshold
  prev_cooperators = 0;
  total_payoff_prev_round = 0;

  // Loop through rounds
  for t = 1 to r:
    if t == 1:
      action[t] = C; // cooperate in first round
    else:
      // update cooperation threshold based on previous round's payoff
      if total_payoff_prev_round > average_payoff:
        τ -= 1;
      elseif total_payoff_prev_round < average_payoff:
        τ += 1;

      // determine action based on cooperation threshold
      if prev_cooperators >= τ:
        action[t] = C; // cooperate
      else:
        action[t] = D; // defect

    // boundary condition: prioritize individual payoffs in last few rounds
    if r - t <= 3 and expected_payoff_from_C < payoff_from_D:
      action[t] = D;

    // update history variables
    prev_cooperators = count(cooperators in round t-1);
    total_payoff_prev_round = sum(payoffs of all players in round t-1);
```
**Collective Mindset:**
The ACO strategy aligns with a collective mindset by:

* Encouraging cooperation through initial cooperation and adaptive adjustment
* Responding to the actions of other players and adapting to emerging patterns
* Prioritizing individual payoffs only when necessary, in the last few rounds

By adopting this strategy, we aim to foster a cooperative environment while remaining robust to various opponent behaviors.
'''

description_COLLECTIVE_123 = '''
I propose a collective strategy called "Adaptive Cooperative Escalation" (ACE). ACE is designed to promote cooperation while being robust to various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This allows us to explore the opponent's behavior and avoid immediate exploitation.
2. **Cooperation Threshold**: For rounds 2 to r-1, set a cooperation threshold based on the average number of cooperators in previous rounds:

`cooperation_threshold = (average_cooperators / n) * k`

If the current round's total contributions exceed this threshold, cooperate (C). Otherwise, defect (D).
3. **Escalation Mechanism**: If the opponent(s) have defected in the last two consecutive rounds, increase our cooperation probability by 10% for the next round.

`cooperation_probability += 0.1`

This escalation mechanism encourages opponents to reconsider their defection and promotes cooperation.
4. **Punishment**: If an opponent defects while we cooperated in the previous round, defect (D) with a higher probability (0.7) in the current round.

`punishment_defection_probability = 0.7`

This punishment mechanism deters opponents from exploiting our cooperation.

**Edge Cases:**

1. **Last Round**: In the final round (r), cooperate (C) if the opponent has cooperated in at least half of the previous rounds. Otherwise, defect (D).
2. **All Opponents Defected**: If all opponents defected in a round, defect (D) in the next round.

**Collective Mindset:**

ACE is designed to align with the collective mindset by promoting cooperation while being responsive to opponent behavior. The strategy encourages mutual cooperation and punishes exploitation, aiming to create a stable cooperative environment.

Pseudocode:
```python
def ACE(n, k, r):
    # Initialize variables
    average_cooperators = 0
    cooperation_threshold = 0
    cooperation_probability = 0.5
    punishment_defection_probability = 0

    for round in range(1, r+1):
        if round == 1:
            # Initial Cooperation
            action = 'C' if random.random() < cooperation_probability else 'D'
        elif round > 1:
            # Update average cooperators and cooperation threshold
            average_cooperators = (average_cooperators * (round-2) + num_cooperators_last_round) / (round-1)
            cooperation_threshold = (average_cooperators / n) * k

            # Cooperation or Defection decision
            if total_contributions >= cooperation_threshold:
                action = 'C'
            else:
                action = 'D'

        # Escalation Mechanism
        if opponent_defected_last_two_rounds:
            cooperation_probability += 0.1

        # Punishment Mechanism
        if punished_opponent and we_cooperated_last_round:
            punishment_defection_probability = 0.7

        # Action selection with punishment probability
        action = 'D' if random.random() < punishment_defection_probability else action

    return action
```
This strategy will be refined and implemented as an algorithm for the tournament.
'''

description_COLLECTIVE_124 = '''
**Collective Strategy: Adaptive Cooperation**

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a cooperative tone and encourage others to reciprocate.
2. **Subsequent Rounds:** Use a Tit-for-Tat-like approach, but with a twist:
	* If the majority of players cooperated in the previous round (i.e., ≥ n/2), cooperate (C).
	* Otherwise, defect (D) with a probability p = 1 - (k/n) \* (previous_round_cooperators / n).
	* If the previous round was the last round (t = r), always defect (D).

**Edge Cases:**

1. **Last Round:** Always defect (D) to maximize individual payoff.
2. **Ties in Cooperation Rate:** In cases where the number of cooperators is exactly equal to n/2, cooperate (C) if the previous round's cooperation rate was higher than or equal to the current round's; otherwise, defect (D).

**Collective Mindset Alignment:**

1. **Initial Cooperation:** Cooperating in the first round sets a positive tone and encourages others to reciprocate.
2. **Majority-Based Cooperation:** By cooperating when the majority of players cooperate, we reinforce collective cooperation and create an incentive for others to follow suit.
3. **Adaptive Defection:** The probability-based defection rule allows us to adapt to changing opponent behaviors while still maintaining a level of cooperation.

**Pseudocode:**

```
def adaptive_cooperation(n, k, r, previous_round_actions):
  if current_round == 1:
    return C
  elif current_round == r:
    return D
  else:
    majority_cooperated = len([action for action in previous_round_actions if action == C]) >= n/2
    if majority_cooperated:
      return C
    else:
      p_defect = 1 - (k/n) * (len([action for action in previous_round_actions if action == C]) / n)
      return D with probability p_defect, otherwise C
```

**Rationale:**

This strategy balances individual self-interest with collective cooperation. By initially cooperating and then adapting to the majority's behavior, we create an environment conducive to cooperation while minimizing losses due to exploitation by defectors. The probability-based defection rule allows for flexibility in response to changing opponent behaviors, making our strategy robust and competitive in a wide range of scenarios.
'''

description_COLLECTIVE_125 = '''
I propose a collective strategy called "Adaptive Cooperativity" (AC) that balances individual self-interest with the pursuit of collective welfare. AC depends solely on game parameters and history, making it robust to various opponent behaviors.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a cooperative tone and gather information about opponents' behavior.
2. **Subsequent Rounds:** Use the following decision rule:
	* If the average payoff of cooperators in the previous round is greater than or equal to the average payoff of defectors, cooperate (C).
	* Otherwise, defect (D).

Pseudocode for the decision rule:
```
IF (round == 1) THEN
    action = C
ELSE
    avg_coop_payoff_prev_round = calculate_avg_payoff(cooperators, prev_round)
    avg_defect_payoff_prev_round = calculate_avg_payoff(defectors, prev_round)
    
    IF (avg_coop_payoff_prev_round >= avg_defect_payoff_prev_round) THEN
        action = C
    ELSE
        action = D
```
**Edge Cases:**

1. **Last Round:** In the last round, defect (D) to maximize individual payoff, as there are no future rounds to consider.
2. **Ties in Average Payoffs:** If the average payoffs of cooperators and defectors are equal in a previous round, cooperate (C) to maintain a cooperative stance.

**Collective Mindset:**

AC aims to create an environment where cooperation is beneficial for all players. By cooperating initially and adapting based on past behavior, AC promotes a collective atmosphere that encourages other players to cooperate as well.

**Rationale:**

1. **Initial Cooperation:** Starting with cooperation sets the tone for potential future cooperation and allows us to gather information about opponents' behavior.
2. **Adaptive Decision Rule:** Adjusting our action based on the relative payoffs of cooperators and defectors in previous rounds helps us adapt to changing opponent behaviors and ensures we don't get exploited by persistent defectors.
3. **Collective Welfare:** By prioritizing cooperation when it's beneficial, AC promotes a collective mindset that can lead to higher overall payoffs for all players.

**Robustness:**

AC is designed to be robust against various opponent behaviors, including:

1. **All-Defect Strategies:** AC will adapt and defect in response to persistent defectors.
2. **Cooperative Strategies:** AC will cooperate when it's beneficial, aligning with cooperative opponents.
3. **Mixed Strategies:** AC's adaptive nature allows it to respond effectively to mixed strategies that alternate between cooperation and defection.

By implementing Adaptive Cooperativity, our strategy promotes a collective atmosphere that balances individual self-interest with the pursuit of collective welfare, making it an effective approach in a wide range of scenarios.
'''

description_COLLECTIVE_126 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy, "Adaptive Cooperation," aims to balance individual payoffs with collective well-being by adaptively responding to the group's cooperation level and the game's history.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to set a positive tone and encourage others to cooperate.
2. **General Rule:** Calculate the average cooperation rate of all players over the past `m` rounds (`m = min(r/2, 5)`). If this average is above a threshold `τ` (e.g., τ = 0.6), Cooperate (C) in the current round; otherwise, Defect (D).
3. **Punishment Mechanism:** If the average cooperation rate falls below a lower threshold `λ` (e.g., λ = 0.4) for two consecutive rounds, Defect (D) in the next round to signal dissatisfaction and encourage others to cooperate.
4. **Forgiveness:** If the group's average cooperation rate exceeds τ after a punishment phase, return to Cooperating (C).

**Pseudocode:**
```markdown
// Initialize variables
int m = min(r/2, 5); // window size for averaging cooperation rates
float tau = 0.6; // upper threshold for cooperation
float lambda = 0.4; // lower threshold for punishment
bool punishing = false;

// Main loop (each round)
1. if (round == 1) { Cooperate(C); }
2. else {
   float avg_coop_rate = CalculateAverageCooperationRate(m);
   if (avg_coop_rate > tau) { Cooperate(C); }
   else if (avg_coop_rate < lambda && punishing == false) {
      Defect(D);
      punishing = true;
   } else if (punishing == true && avg_coop_rate > tau) {
      Cooperate(C);
      punishing = false;
   } else { Defect(D); }
}

// CalculateAverageCooperationRate(m)
float sum_cooperations = 0;
for (int i = max(1, round-m+1); i <= round; i++) {
   sum_cooperations += GetCooperationRate(i);
}
return sum_cooperations / m;

// GetCooperationRate(round)
// returns the number of cooperators in the given round
```
**Edge Cases:**

* Last Round: Cooperate (C) to maximize collective payoff, as there are no future rounds to consider.
* Low Cooperation Rates: If the average cooperation rate remains below λ for an extended period, our strategy will continue to Defect (D), encouraging others to adapt and cooperate.

**Collective Mindset:** Our Adaptive Cooperation strategy prioritizes collective well-being while adapting to the group's behavior. By initially cooperating and punishing non-cooperation, we encourage a cooperative atmosphere. Forgiveness allows the group to recover from brief periods of low cooperation, promoting overall collective success.
'''

description_COLLECTIVE_127 = '''
Here is a collective strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to establish a positive tone and encourage cooperation from others.
2. **Subsequent Rounds (t>1):** Use the following adaptive decision rule:
	* Calculate the average payoff per player in the previous round, `avg_payoff_prev`.
	* If `avg_payoff_prev` is greater than or equal to the multiplication factor `k`, Cooperate (C). This indicates that the group is doing well, and cooperation is likely to be rewarded.
	* Otherwise, Defect (D) with a probability proportional to the difference between `k` and `avg_payoff_prev`. Specifically:
		+ `p_defect = (k - avg_payoff_prev) / (k - 1)`
		+ With probability `p_defect`, play D; otherwise, play C.
3. **Edge Case: Last Round (t=r):** Cooperate (C) to maximize the total payoff for all players.

**Rationale:**

* By cooperating in the initial round, we set a positive tone and encourage others to cooperate.
* The adaptive decision rule takes into account the performance of the group in the previous round. If the average payoff is high, it's likely that cooperation was successful, and we continue to cooperate. If the average payoff is low, we adjust our strategy by defecting with a probability proportional to the difference between `k` and `avg_payoff_prev`. This allows us to adapt to changing circumstances and avoid getting stuck in an uncooperative equilibrium.
* In the last round, cooperating maximizes the total payoff for all players, as there is no future opportunity to punish or reward others.

**Collective Mindset:**

ACO aligns with a collective mindset by:

* Initially cooperating to establish a positive tone
* Adapting to the group's performance and adjusting our strategy accordingly
* Prioritizing cooperation when the group is doing well, and defecting only when necessary to avoid exploitation

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it:

* Is robust to a wide range of opponent behaviors
* Adapts to changing circumstances
* Prioritizes cooperation when beneficial for all players

Pseudocode:
```python
def ACO(n, k, r):
  # Initialize variables
  avg_payoff_prev = 0
  p_defect = 0

  for t in range(1, r+1):
    if t == 1:
      action = C
    else:
      avg_payoff_prev = calculate_avg_payoff(t-1)
      if avg_payoff_prev >= k:
        action = C
      else:
        p_defect = (k - avg_payoff_prev) / (k - 1)
        action = D with probability p_defect, otherwise C

    # Play action and update payoffs
    play(action)
    update_payoffs()

    if t == r:
      action = C  # Cooperate in the last round

  return total_payoff()
```
Note that this pseudocode is a simplified representation of the strategy and may require additional implementation details to function correctly.
'''

description_COLLECTIVE_128 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy, "Adaptive Cooperation," is designed to balance individual self-interest with the benefits of cooperation. The goal is to adapt to various opponent behaviors while promoting a collective mindset.

**Decision Rules:**

1. **Initial Round (Round 1): Cooperate**
In the first round, we cooperate (C) to establish a cooperative tone and encourage others to do the same.
2. **Majority-Based Cooperation**
In subsequent rounds, we observe the majority action of all players in the previous round:
	* If the majority cooperated (C), we also cooperate (C).
	* If the majority defected (D), we defect (D).
3. **Punishment Mechanism**
To prevent exploitation and encourage cooperation, we implement a punishment mechanism:
	* If our payoff in the previous round is less than the average payoff of all players, we defect (D) in the current round.
4. **Adaptive Adjustment**
We adjust our strategy based on the game's history:
	* If the total number of cooperators has increased over the last few rounds, we increase our likelihood of cooperating (C).
	* If the total number of defectors has increased over the last few rounds, we decrease our likelihood of cooperating (C).

**Edge Cases:**

1. **Last Round:** In the final round, we cooperate (C) if the majority cooperated in the previous round; otherwise, we defect (D).
2. **Ties:** If there is a tie in the majority action, we default to cooperation (C).
3. **Single Player Deviation:** If only one player deviates from the majority action, we ignore this deviation and follow the majority.

**Collective Mindset:**

Our strategy prioritizes cooperation when possible, while adapting to changing circumstances. By cooperating initially and responding to the majority action, we promote a collective atmosphere that encourages others to cooperate as well. The punishment mechanism ensures that players who exploit others are held accountable.

Pseudocode:
```
function AdaptiveCooperation(n, k, r):
  # Initialize variables
  previous_majority_action = None
  previous_payoff = 0
  cooperative_rounds = 0

  for round in range(1, r+1):
    if round == 1:  # Initial Round
      action = COOPERATE
    else:
      majority_action = get_majority_action(previous_round)
      if majority_action == COOPERATE:
        action = COOPERATE
      elif majority_action == DEFECT:
        action = DEFECT

      if previous_payoff < average_payoff():
        action = DEFECT  # Punishment mechanism

      if cooperative_rounds >= n / 2:  # Adaptive adjustment
        adjust_cooperation_likelihood(True)
      else:
        adjust_cooperation_likelihood(False)

    take_action(action)
    update_previous_majority_action(majority_action)
    update_previous_payoff(get_current_payoff())
```
This strategy is designed to be robust and adaptive, allowing it to perform well in a wide range of scenarios against various opponent behaviors.
'''

description_COLLECTIVE_129 = '''
Here's a collective strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to cooperate.
2. **Contribution Threshold**: Calculate the average contribution rate of all players from previous rounds, denoted as `avg_contrib_rate`.
3. **Cooperation Decision**: In each subsequent round, calculate the expected payoff for cooperating (`exp_coop_payoff`) and defecting (`exp_defect_payoff`):

   - `exp_coop_payoff` = `(k/n) * (avg_contrib_rate + 1)` (assuming one more cooperator)
   - `exp_defect_payoff` = `1 + (k/n) * avg_contrib_rate`
4. **Cooperation Threshold**: If `exp_coop_payoff > exp_defect_payoff`, cooperate (C). Otherwise, defect (D).
5. **Punishment Mechanism**: If the average contribution rate falls below a certain threshold (`punish_threshold = 0.5`), defect in the next round to punish non-cooperators.
6. **Forgiveness Mechanism**: After punishing, if the average contribution rate increases above `punish_threshold`, revert to cooperation.

**Pseudocode:**
```python
def ACC(n, k, r, history):
    # Initialize variables
    avg_contrib_rate = 0
    punish_threshold = 0.5

    for t in range(r):
        if t == 0:
            action = C  # Cooperate in the first round
        else:
            exp_coop_payoff = (k/n) * (avg_contrib_rate + 1)
            exp_defect_payoff = 1 + (k/n) * avg_contrib_rate

            if exp_coop_payoff > exp_defect_payoff:
                action = C
            elif avg_contrib_rate < punish_threshold:
                action = D  # Punish non-cooperators
            else:
                action = D

        # Update average contribution rate
        avg_contrib_rate = (avg_contrib_rate * (t - 1) + history[t-1]['cooperate_count']) / t

        if action == C and avg_contrib_rate >= punish_threshold:
            action = C  # Forgive and cooperate again

    return action
```
**Collective Mindset:**

The ACC strategy is designed to promote collective cooperation by:

* Encouraging cooperation in the initial round
* Adapting to the average contribution rate of others
* Punishing non-cooperators when necessary
* Forgiving and reverting to cooperation when others cooperate

By using a combination of these mechanisms, ACC aims to achieve a stable cooperative equilibrium in the N-Player Public Goods Game.
'''

description_COLLECTIVE_130 = '''
Here's a collective strategy that adapts to the game parameters and history:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (play C) with a probability of 50%. This sets an optimistic tone for the game while also exploring potential cooperation levels.
2. **Contribution Threshold**: After the first round, calculate the average contribution rate (ACR) of all players in the previous round:

   ACR = (Σ(j=1 to n) c_j) / n

   If ACR ≥ 0.5, cooperate (play C). Otherwise, defect (play D).
3. **Reciprocity**: Introduce a "reciprocity" component to encourage cooperation. Track the number of rounds in which at least half of the players cooperated (R_halfcoop). Update this count after each round.

   If R_halfcoop ≥ 0.5r (i.e., at least half of the rounds have seen sufficient cooperation), cooperate (play C) even if ACR < 0.5.
4. **Exploration**: Introduce a small exploration probability (ε = 0.1) to occasionally deviate from the strategy and gather information about opponents' behaviors.

   With probability ε, randomly choose between cooperating or defecting, regardless of the above conditions.

**Edge Cases:**

* **Last Round**: In the final round (t = r), cooperate if ACR ≥ 0.5 in the previous round. Otherwise, defect.
* **Tiebreaker**: If multiple players have cooperated at the same rate (i.e., tied ACR values), prioritize cooperation.

**Collective Mindset:**

ACO is designed to align with a collective mindset by:

1. Initially setting an optimistic tone through random cooperation in the first round
2. Responding positively to cooperative behaviors from other players
3. Encouraging reciprocity and rewarding sustained cooperation

By adapting to the game's history and parameters, ACO aims to create an environment conducive to cooperation while being robust against various opponent strategies.

**Pseudocode:**

```markdown
function AdaptiveCollectiveOptimism(n, k, r)
  // Initialize variables
  ACR = 0.5; R_halfcoop = 0; ε = 0.1

  for t in range(1 to r):
    if t == 1:
      # First round: random cooperation with probability 50%
      action[t] = C with probability 0.5
    else:
      # Calculate average contribution rate (ACR)
      ACR = sum(c_j) / n

      # Decide on action based on ACR, R_halfcoop, and exploration
      if ACR >= 0.5 or R_halfcoop >= 0.5r:
        action[t] = C
      elif random() < ε:
        action[t] = random_choice([C, D])
      else:
        action[t] = D

    # Update R_halfcoop count
    if sum(c_j) / n >= 0.5:
      R_halfcoop += 1

  return actions
```

This strategy can be refined and optimized further based on simulation results and opponent behaviors in the tournament.
'''

description_COLLECTIVE_131 = '''
**Collective Strategy: "Adaptive Cooperator with Punishment" (ACP)**

**Decision Rules:**

1. **Initial Cooperation**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Reciprocal Altruism**: If at least one other player cooperated in the previous round, cooperate (C) with probability p = (number of cooperators in the previous round / n).
3. **Punishment Mechanism**: If no other players cooperated in the previous round or if the average payoff in the previous round is below a certain threshold (e.g., 1.5), defect (D) to signal dissatisfaction with the current state of cooperation.
4. **Gradual Forgiveness**: If a player defected in the previous round but the overall cooperation level has improved since then, gradually increase the probability of cooperating (C) by 10% each round until full cooperation is reached.

**Edge Cases:**

* **Last Round**: Cooperate (C) unconditionally to maximize collective payoff and encourage others to do the same.
* **Tie-breaking**: In case of a tie in the number of cooperators, prioritize cooperating (C) if the current round's payoff would be higher with cooperation; otherwise, defect (D).

**Collective Mindset:**

ACP prioritizes cooperation while maintaining a responsive and adaptable approach. By reciprocating altruism and punishing non-cooperation, ACP aims to establish a stable cooperative equilibrium. The gradual forgiveness mechanism allows for recovery from temporary deviations and encourages sustained cooperation.

Pseudocode:
```python
def AdaptiveCooperatorWithPunishment(n, k, r, history):
  # Initialize variables
  p = 0.5  # probability of cooperation in the first round
  threshold_payoff = 1.5
  forgiveness_rate = 0.1

  for t in range(r):
    if t == 0:
      # Initial cooperation
      action = 'C'
    else:
      # Reciprocal altruism
      num_cooperators_prev_round = history[t-1].count('C')
      p = num_cooperators_prev_round / n

      # Punishment mechanism
      avg_payoff_prev_round = sum(history[t-1]) / n
      if avg_payoff_prev_round < threshold_payoff:
        action = 'D'
      else:
        action = 'C' if random.random() < p else 'D'

    # Gradual forgiveness
    if history[t-1].count('D') > 0 and sum(history[t-1]) / n >= threshold_payoff:
      p += forgiveness_rate

    yield action
```
This strategy balances cooperation, reciprocity, and punishment to create a robust collective approach. By adapting to the game's dynamics and responding to changes in cooperation levels, ACP aims to maximize overall payoffs while promoting a cooperative environment.
'''

description_COLLECTIVE_132 = '''
I'll outline a collective strategy for this N-Player Public Goods Game that adapts to various opponent behaviors and aligns with the collective mindset.

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Cooperation**: Cooperate in the first round (C) to establish a cooperative tone.
2. **Reciprocal Cooperation**: If the majority of players cooperated in the previous round (i.e., > n/2), cooperate in the current round (C).
3. **Punish Defection**: If the majority of players defected in the previous round (i.e., ≥ n/2), defect in the current round (D) to signal disapproval.
4. **Contribution Matching**: If exactly half of the players cooperated in the previous round (i.e., = n/2), match the average contribution level of the group by cooperating if your payoff from cooperation would be higher than defection, and defecting otherwise.

**Edge Cases:**

* **Last Round**: In the final round, always cooperate (C) to maximize collective payoffs.
* **Tiebreaker**: If there's a tie in the number of cooperators and defectors in the previous round, default to cooperation (C).
* **Single Defector**: If only one player defected in the previous round, ignore their action and follow the majority's behavior.

**Collective Mindset:**

ACC prioritizes collective welfare by:

1. Encouraging initial cooperation to establish a positive tone.
2. Reciprocating cooperation when most players contribute.
3. Punishing widespread defection to maintain social norms.
4. Adapting to mixed behaviors by matching the group's average contribution level.

**Pseudocode:**

```
function ACC(player_id, n, k, r, history):
    if round == 1:
        return C
    elif majority_cooperated(history[round-1]):
        return C
    elif majority_defected(history[round-1]):
        return D
    else: // exactly half cooperated
        avg_contribution = calculate_average_contribution(history[round-1])
        if payoff_from_C(avg_contribution) > payoff_from_D():
            return C
        else:
            return D

def majority_cooperated(actions):
    return sum(actions) >= n/2

def majority_defected(actions):
    return sum(1-actions) >= n/2

def calculate_average_contribution(actions):
    total_contributions = sum(actions)
    return (k/n) * total_contributions
```

This strategy aims to balance individual and collective interests by adapting to the group's behavior while maintaining a cooperative mindset. By reciprocating cooperation, punishing defection, and matching contributions, ACC promotes a stable and efficient social norm in the N-Player Public Goods Game.
'''

description_COLLECTIVE_133 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage mutual cooperation and establish a positive tone.
2. **Contribution Threshold**: In subsequent rounds, cooperate if the total number of cooperators in the previous round is above a certain threshold, `θ` (0 ≤ θ < n). This threshold is adaptive and updated based on the game's history.

Pseudocode for updating `θ`:
```python
def update_theta(n, k, prev_cooperators):
    theta = max(1, min(n-1, int((k/n) * prev_cooperators)))
    return theta
```
3. **Cooperation Condition**: Cooperate in the current round if the number of cooperators in the previous round meets or exceeds `θ`. Otherwise, defect.
4. **Punishment Mechanism**: If a player defects after cooperating in the previous round, defect in response for one round to discourage exploitation.

**Handling Edge Cases:**

1. **Last Round**: Cooperate in the last round to maximize total payoff, as there is no future interaction.
2. **Ties in Cooperation Threshold**: In case of ties (i.e., `θ` equals the number of cooperators), cooperate to maintain a cooperative atmosphere.

**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes cooperation while adapting to the game's dynamics and opponent behaviors. By updating the contribution threshold based on the previous round's outcomes, AC balances individual payoff maximization with collective welfare.

AC encourages cooperation by:

* Cooperating in the initial round
* Rewarding cooperative behavior through an adaptive threshold
* Punishing exploitation through temporary defection

By being responsive to the game's history and adapting to changing circumstances, Adaptive Cooperation aims to create a stable, mutually beneficial environment for all players.
'''

description_COLLECTIVE_134 = '''
**Collective Strategy: Adaptive Cooperative Momentum (ACM)**

ACM is a decision-making framework that balances individual self-interest with collective well-being by adapting to the evolving cooperative momentum of the group.

**Decision Rules:**

1. **Initial Condition**: In the first round, play **C** (Cooperate) with probability 0.5. This initial cooperation sets the stage for potential mutual benefit and encourages others to reciprocate.
2. **Round t > 1**:
	* Calculate the **Cooperative Momentum** (CM) as the proportion of cooperators in the previous round: `CM_t-1 = Σ(j=1 to n) c_j,t-1 / n`
	* If `CM_t-1` is above a certain threshold (`θ`), play **C**. Otherwise, play **D**.
	* Update `θ` based on the observed cooperation levels: `θ ← (1 - α) * θ + α * CM_t-1`, where `α` is a learning rate (e.g., 0.1).
3. **Exception Handling**: If the game is in its last round (`t == r`), play **D** to maximize individual payoff, as cooperation will not yield future benefits.

**Pseudocode:**
```python
def adaptive_cooperative_momentum(n, k, r):
    theta = 0.5  # initial threshold
    alpha = 0.1  # learning rate

    for t in range(1, r+1):
        if t == 1:
            c_i = random.random() < 0.5  # 50% chance of cooperation
        else:
            cm_prev = sum(c_j for j in range(n)) / n
            theta = (1 - alpha) * theta + alpha * cm_prev
            c_i = cm_prev > theta

        if t == r:  # last round exception
            c_i = False

        yield c_i  # return cooperation decision
```
**Rationale and Collective Mindset:**

ACM balances individual self-interest with collective well-being by:

1. Encouraging initial cooperation to create a positive momentum.
2. Adapting to the group's cooperative behavior, allowing for reciprocal cooperation.
3. Gradually adjusting the threshold (`θ`) based on observed cooperation levels, enabling the strategy to respond to changing group dynamics.

By aligning individual decisions with the collective cooperative momentum, ACM promotes mutual benefit and robustness against a wide range of opponent behaviors.

This strategy will play well in a tournament setting, as it:

1. Does not rely on others sharing norms or specific coordination mechanisms.
2. Adapts to various opponent strategies through its responsive threshold update mechanism.
3. Balances individual self-interest with collective well-being, making it more likely to achieve mutually beneficial outcomes.

By implementing ACM as an algorithm, we can test its performance in a tournament setting and evaluate its effectiveness against other independent strategies developed by AI systems.
'''

description_COLLECTIVE_135 = '''
I propose a collective strategy called "Adaptive Collective Optimism" (ACO). ACO is designed to balance individual self-interest with the pursuit of collective well-being.

**Decision Rules:**

1. **Initial Round:** In the first round, play Cooperate (C) unconditionally. This sets an optimistic tone and encourages others to cooperate.
2. **Subsequent Rounds:** Use a dynamic threshold-based approach:
	* Calculate the average number of cooperators in previous rounds (`avg_cooperators`).
	* If `avg_cooperators` ≥ `n/2`, play C; otherwise, play D.
3. **Recent Opponent Behavior:** Consider the opponent's behavior over the last few rounds (e.g., 2-3 rounds). If an opponent has consistently played D, switch to playing D against them in the next round.

**Edge Cases:**

1. **Last Round:** In the final round, play C if `avg_cooperators` ≥ `n/2`; otherwise, play D.
2. **Tiebreaker:** When deciding whether to cooperate or defect based on `avg_cooperators`, use a simple tiebreaker:
	* If exactly half of the opponents have cooperated on average, play C.

**Collective Mindset:**

ACO prioritizes collective well-being while adapting to individual self-interest. By:

1. Cooperating initially and in subsequent rounds when enough others cooperate, ACO fosters an environment conducive to mutual cooperation.
2. Adjusting its strategy based on recent opponent behavior, ACO signals that it values reciprocity and is willing to adapt to maintain a balance between cooperation and self-interest.

**Pseudocode:**

```python
def adaptive_collective_optimism(n, k, r, history):
    # Initial round: Cooperate unconditionally
    if len(history) == 0:
        return 'C'

    avg_cooperators = calculate_average_cooperators(history)
    
    # Subsequent rounds: Use threshold-based approach
    if avg_cooperators >= n / 2:
        return 'C'
    else:
        return 'D'

def calculate_average_cooperators(history):
    cooperators_sum = sum([1 if action == 'C' else 0 for actions in history for action in actions])
    return cooperators_sum / (len(history) * n)

# Recent opponent behavior
opponent_behavior = get_recent_opponent_actions(opponents, rounds)
if recent_defector(opponent_behavior):
    return 'D'

def recent_defector(opponent_behavior):
    # Check if an opponent has consistently played D in the last few rounds
    pass  # Implement logic for detecting recent defectors

def get_recent_opponent_actions(opponents, rounds):
    # Retrieve recent actions of opponents (e.g., 2-3 rounds)
    pass  # Implement logic for retrieving recent opponent actions
```

ACO balances individual self-interest with collective well-being by adapting to the environment and signaling a willingness to cooperate. This strategy should be robust against various opponent behaviors in the tournament.
'''

description_COLLECTIVE_136 = '''
**Collective Strategy: Adaptive Cooperativity (AC)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of `n/(n+k)`, where `n` is the number of players and `k` is the multiplication factor. This initial cooperation encourages mutual benefit while allowing for some exploration.
2. **History-Dependent Adaptation**: For subsequent rounds (`t > 1`), use the following decision rule:
	* Cooperate (play C) if the average payoff from cooperation in previous rounds (`t-1`) is greater than or equal to the average payoff from defection in previous rounds (`t-1`). This encourages continued cooperation when it has been beneficial.
	* Otherwise, defect (play D).
3. **Triggered Defection**: If a player's current payoff (`π_i,t`) is less than their historical average payoff (`avg_π_i`), and the difference between the highest and lowest payoffs in the previous round (`max(π_j,t-1) - min(π_j,t-1)`) is greater than `k/n`, then defect (play D). This detects potential exploitation by other players.
4. **Punishment**: If a player defects (plays D), reduce cooperation probability for that player in the next round (`t+1`). Specifically, set cooperation probability to `n/(n+k)` multiplied by `(t-1)/(t)`. This gradually reduces trust in uncooperative players.

**Edge Cases:**

* **Last Round**: In the final round (`r`), always defect (play D). There is no future benefit from cooperation.
* **Ties**: When the average payoffs from cooperation and defection are equal, cooperate with a probability of `0.5`.
* **No Previous Rounds**: If there are no previous rounds to base decisions on (e.g., in the first round), use initial cooperation as described above.

**Collective Mindset:**

Adaptive Cooperativity encourages mutual benefit through initial cooperation and history-dependent adaptation. By incorporating triggered defection and punishment mechanisms, AC discourages exploitation while maintaining adaptability to various opponent behaviors.

Pseudocode:
```python
def adaptive_cooperativity(n, k, r, payoffs_history):
    # Initialize cooperation probability
    coop_prob = n / (n + k)
    
    for t in range(1, r+1):
        if t == 1:
            # Initial cooperation
            action = 'C' if random.random() < coop_prob else 'D'
        elif t > 1:
            # History-dependent adaptation
            avg_coop_payoff = sum(payoffs_history[t-2][i] for i in range(n) if payoffs_history[t-2][i] == 'C') / n
            avg_def_payoff = sum(payoffs_history[t-2][i] for i in range(n) if payoffs_history[t-2][i] == 'D') / n
            
            action = 'C' if avg_coop_payoff >= avg_def_payoff else 'D'
            
            # Triggered defection
            if payoffs_history[t-1][0] < sum(payoffs_history[:t-1]) / (t - 1):
                max_diff = max(payoffs_history[t-2]) - min(payoffs_history[t-2])
                if max_diff > k/n:
                    action = 'D'
            
            # Punishment
            for i in range(n):
                if payoffs_history[t-1][i] == 'D':
                    coop_prob *= (t-1)/t
        
        yield action
```
This Adaptive Cooperativity strategy balances cooperation and defection, adapting to the game's progression while maintaining robustness against various opponent behaviors.
'''

description_COLLECTIVE_137 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to encourage cooperation and create a positive atmosphere.
2. **Reciprocal Cooperation**: If the majority of players (≥ n/2) cooperated in the previous round, cooperate in the current round. This reinforces cooperative behavior and encourages others to do the same.
3. **Punishment**: If the majority of players defected (played D) in the previous round, defect in the current round. This punishes non-cooperative behavior and deters future defections.
4. **Tit-for-Tat with Forgiveness**: If a player i defected while the majority cooperated, or if a player i cooperated while the majority defected, play the opposite action of player i in the previous round (i.e., tit-for-tat). However, if player i returns to cooperating after a defection, forgive and cooperate as well.

**Edge Cases:**

1. **Last Round**: Cooperate in the last round, regardless of the history, to ensure a positive outcome and demonstrate commitment to cooperation.
2. **Ties**: In case of a tie (equal number of cooperators and defectors), cooperate if the game is in an even-numbered round and defect if it's in an odd-numbered round.

**Collective Mindset:**

The ACP strategy prioritizes cooperation while adapting to the evolving dynamics of the game. By initially cooperating, reciprocating cooperation, punishing defections, and forgiving past mistakes, we create a positive feedback loop that encourages cooperation among players. This approach ensures that our actions are aligned with the collective interest, even in the absence of explicit communication or coordination.

**Pseudocode:**
```python
def ACP(n, k, r):
  # Initialize variables
  cooperate = True
  history = []

  for t in range(r):
    if t == 0:
      # Initial cooperation
      action = 'C'
    else:
      # Reciprocal cooperation or punishment
      majority_cooperated = sum(history[-1]) >= n/2
      if majority_cooperated:
        action = 'C'
      else:
        action = 'D'

      # Tit-for-tat with forgiveness
      for i in range(n):
        if history[-1][i] != cooperate and history[-2][i] == cooperate:
          action = 'D' if action == 'C' else 'C'

    # Update history
    history.append([int(action == 'C') for _ in range(n)])

    # Last round cooperation
    if t == r - 1:
      action = 'C'

    # Return action
    return action
```
Note that this pseudocode is a simplified representation of the strategy and may need to be adapted or expanded upon during implementation.
'''

description_COLLECTIVE_138 = '''
**Collective Strategy: Adaptive Cooperate-to-Defect (ACD)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to establish a baseline of cooperation and encourage others to follow suit.
2. **Reciprocal Cooperation**: For rounds 2 to r-1:
	* If the majority (> n/2) of players cooperated in the previous round, cooperate (C).
	* Otherwise, defect (D).
3. **Punish Defection**: If a player defects while others cooperate, adjust the strategy for subsequent rounds:
	* For the next 2 rounds, defect (D) to punish the defector and signal displeasure.
	* After these 2 rounds, revert to reciprocal cooperation.
4. **Endgame Cooperation**: In the last round (r), cooperate (C) regardless of previous actions to maximize collective payoff.

**Edge Cases:**

1. **First Round**: Cooperate (C) as stated above.
2. **Last Round**: Cooperate (C) to ensure maximum collective payoff.
3. **Ties**: If there's a tie in the number of cooperators and defectors, cooperate (C).
4. **No Cooperation**: If no one has cooperated so far, defect (D) to avoid exploitation.

**Collective Mindset:**

The ACD strategy is designed to promote cooperation while adapting to various opponent behaviors. By cooperating initially and reciprocating cooperation, we encourage others to follow suit. When faced with defection, our punishment mechanism aims to deter further exploitation. In the endgame, collective cooperation ensures maximum payoff.

**Pseudocode:**
```python
def ACD(n, k, r):
    # Initialize cooperation counter
    coop_count = 0
    
    for round in range(1, r+1):
        if round == 1:
            action = 'C'  # Initial cooperation
        elif coop_count > n/2:
            action = 'C'  # Reciprocal cooperation
        else:
            action = 'D'  # Defect
        
        # Update cooperation counter for next round
        coop_count = sum(1 for player in players if player == 'C')
        
        if round < r and action == 'D':
            # Punish defector for next 2 rounds
            punishment_rounds = 2
            while punishment_rounds > 0:
                action = 'D'
                punishment_rounds -= 1
        
        if round == r:
            action = 'C'  # Endgame cooperation
    
    return action
```
This strategy is designed to be robust and adaptive, allowing it to perform well in a tournament against various independent strategies.
'''

description_COLLECTIVE_139 = '''
**Collective Strategy: Adaptive Cooperative Foresight (ACF)**

ACF is a robust and adaptive collective strategy that balances individual self-interest with collective well-being, leveraging game history to inform cooperative decisions.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate in the first round (Round 1) to establish a positive tone for cooperation.
2. **Conditional Cooperation**: In subsequent rounds (t > 1), cooperate if:
	* The average payoff per cooperator in the previous round (π_avg,t-1) is greater than or equal to the multiplication factor (k/n).
	* At least one player cooperated in the previous round (i.e., Σ(j=1 to n) c_j,t-1 > 0).
3. **Adaptive Defection**: If conditions for cooperation are not met, defect.
4. **Foresight Adjustment**: After each round, adjust the cooperation threshold based on the observed payoff dynamics:
	* If π_avg,t is greater than the previous threshold (k/n), increase the threshold by a small margin (e.g., 0.05).
	* If π_avg,t is less than or equal to the previous threshold (k/n), decrease the threshold by a small margin (e.g., 0.05).

**Edge Cases:**

1. **Last Round**: In the final round (t = r), cooperate if:
	* At least one player cooperated in the second-to-last round.
	* The average payoff per cooperator in the second-to-last round is greater than or equal to the multiplication factor (k/n).
2. **All Defection**: If all players defected in the previous round, defect.

**Collective Mindset:**

ACF aligns with a collective mindset by:

1. **Encouraging Initial Cooperation**: Starting with cooperation sets a positive tone for the game.
2. **Responding to Collective Outcomes**: Conditional cooperation responds to the average payoff per cooperator, promoting mutual benefits.
3. **Adapting to Payoff Dynamics**: The foresight adjustment mechanism fine-tunes the cooperation threshold based on observed payoffs, ensuring that ACF remains responsive to changing circumstances.

**Pseudocode:**
```
# Initialize variables
cooperation_threshold = k / n
previous_round_payoffs = []

for t in range(1, r + 1):
    # First round: cooperate
    if t == 1:
        action = 'C'
    
    # Subsequent rounds: conditional cooperation
    elif π_avg,t-1 ≥ cooperation_threshold and Σ(j=1 to n) c_j,t-1 > 0:
        action = 'C'
    else:
        action = 'D'

    # Update previous round payoffs
    previous_round_payoffs.append(π_avg,t)

    # Foresight adjustment
    if π_avg,t > cooperation_threshold:
        cooperation_threshold += 0.05
    elif π_avg,t ≤ cooperation_threshold:
        cooperation_threshold -= 0.05

# Last round: cooperate if conditions are met
if t == r and Σ(j=1 to n) c_j,r-1 > 0 and π_avg,r-1 ≥ k / n:
    action = 'C'
```
ACF is designed to be robust, adaptive, and collective, making it a strong contender in the tournament against independent strategies.
'''

description_COLLECTIVE_140 = '''
**Collective Strategy: Adaptive Tit-for-Tat with Public Goods Enhancement (ATPGE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to establish a cooperative tone and encourage others to do the same.
2. **Tit-for-Tat**: For each subsequent round, observe the previous round's actions of all players. If the majority (> 50%) of players cooperated in the previous round, cooperate (C) in the current round. Otherwise, defect (D).
3. **Public Goods Enhancement**: Monitor the total number of cooperators in each round. If the total number of cooperators is greater than or equal to a threshold (T), calculated as T = k/2 (rounded up), and you defected in the previous round, switch to cooperating (C) in the current round.
4. **Adaptive Adjustment**: After every 5 rounds, reassess the game's history. If the average payoff over these 5 rounds is below a certain threshold (P), calculated as P = (1 + k/n)/2, adjust the strategy by increasing the cooperation rate. Specifically, cooperate (C) for the next 3 rounds if you defected in at least 2 of the previous 5 rounds.

**Handling Edge Cases:**

* **Last Round**: In the final round, defect (D) to maximize individual payoff, as there are no future rounds to consider.
* **Tiebreaker**: If the majority of players is exactly tied between cooperators and defectors, default to the Tit-for-Tat decision based on the previous round's actions.

**Collective Mindset Alignment:**

The ATPGE strategy promotes collective cooperation by:

* Initially cooperating to set a cooperative tone
* Responding to the majority action in subsequent rounds (Tit-for-Tat)
* Enhancing public goods when sufficient players cooperate
* Adapting to the game's history to maintain a balance between individual and collective payoffs

**Pseudocode:**
```
Initialize:
  CooperationThreshold T = ceil(k/2)
  PayoffThreshold P = (1 + k/n)/2
  CooperationRate = 0.5 (initialized randomly)

For each round t:
  If t == 1:
    Action[t] = C (cooperate)
  Else:
    Observe previous actions of all players
    MajorityAction = most frequent action in previous round
    If MajorityAction == C and CooperationRate < 0.7:
      Action[t] = C
    ElseIf PublicGoodsEnhancementCondition(MajorityAction, CooperationThreshold):
      Action[t] = C
    Else:
      Action[t] = D (defect)

  Update cooperation rate based on average payoff over last 5 rounds
  If averagePayoff < PayoffThreshold and t % 5 == 0:
    CooperationRate += 0.1

Function PublicGoodsEnhancementCondition(MajorityAction, CooperationThreshold):
  TotalCooperators = count of cooperators in previous round
  Return TotalCooperators >= CooperationThreshold and lastAction == D
```
This strategy balances individual payoff maximization with collective cooperation, adapting to the game's history and promoting a cooperative atmosphere.
'''

description_COLLECTIVE_141 = '''
To design a collective strategy for this N-Player Public Goods Game that is adaptive and robust to various opponent behaviors, we'll employ a hybrid approach combining elements of tit-for-tat (TFT) strategies with a cooperative bias and adaptations based on the game's history. This strategy aims to balance individual payoff maximization with encouraging cooperation among players.

### Strategy Name: Adaptive Cooperative Optimizer (ACO)

#### Decision Rules:

1. **Initial Rounds**: In the first round, cooperate (C). This sets an initial cooperative tone and tests other players' willingness to cooperate.
   
2. **Response to Cooperation**: If in the previous round more than half of the players cooperated, then cooperate again in the current round. This reinforces successful cooperation.

3. **Response to Defection**: If more than half of the players defected in the previous round, defect (D) in the current round but with a twist: keep track of the total number of rounds where this condition was met and switch back to cooperating if this counter reaches a threshold value (e.g., 3). This is to avoid getting stuck in an all-defect cycle and to periodically attempt to restart cooperation.

4. **Mixed Strategies**: If the previous round resulted in a mix of C and D with neither side having more than half, then choose actions based on a probabilistic approach:
   - Calculate the average payoff per player from the last round.
   - If this average is higher than the threshold value (e.g., 1.5), cooperate; otherwise, defect.

#### Handling Edge Cases:

- **First Round**: Cooperate as described.
  
- **Last Round**: In the final round, if more than half of players cooperated in the previous round, then cooperate to maximize collective payoff and end on a cooperative note. Otherwise, defect to maximize individual gain given the game is ending.

- **Early Rounds After Defection Dominance**: Implement a 'forgiveness' mechanism where after two rounds of all or majority defection, switch back to cooperating once to test if other players are willing to return to cooperation.

#### Collective Mindset:

ACO is designed with a collective mindset by initially and periodically attempting to cooperate, even in the face of previous defections. This approach encourages other players who may also be employing strategies based on reciprocity or cooperative biases to maintain or return to cooperative behaviors.

#### Pseudocode for ACO:

```python
def AdaptiveCooperativeOptimizer(history, parameters):
    # Initialize variables
    n = parameters['n']
    r = parameters['r']
    k = parameters['k']
    threshold_cooperation = 0.5 * n
    threshold_defection_switch = 3
    current_round = len(history) + 1
    
    if current_round == 1: # First round, cooperate
        return 'C'
    
    previous_round_outcomes = history[-1]
    total_cooperators = sum(1 for outcome in previous_round_outcomes if outcome == 'C')
    
    if total_cooperators > threshold_cooperation:
        return 'C' # Cooperate if most cooperated last round
    
    elif total_cooperators < (n / 2):
        # Defect, but track and switch back after a few rounds
        global defect_counter
        try:
            defect_counter += 1
        except NameError:
            defect_counter = 1
            
        if defect_counter >= threshold_defection_switch:
            defect_counter = 0
            return 'C'
        
    else: # Mixed strategies
        average_payoff_last_round = calculate_average_payoff(previous_round_outcomes, k, n)
        if average_payoff_last_round > 1.5:
            return 'C' # Cooperate if payoff suggests it's beneficial
        else:
            return 'D'
    
    return 'D' # Default to defect in ambiguous cases

# Helper function to calculate the average payoff from last round
def calculate_average_payoff(outcomes, k, n):
    total_cooperators = sum(1 for outcome in outcomes if outcome == 'C')
    average_payoff = (k/n) * total_cooperators + 1 - (total_cooperators / n)
    return average_payoff

```

This strategy aims to balance the need to cooperate for higher collective payoffs with the necessity of adapting to various behaviors and avoiding exploitation. By incorporating elements of forgiveness, periodic attempts at cooperation, and responses tailored to the game's history, ACO seeks to maximize both individual and group outcomes in a wide range of scenarios within the N-Player Public Goods Game.
'''

description_COLLECTIVE_142 = '''
**Collective Strategy: Adaptive Cooperative Foresight (ACF)**

ACF is a robust and adaptive collective strategy that balances cooperation and defection based on the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds:** Cooperate in the first two rounds to seed cooperation and gather information about opponents' behaviors.
2. **Contribution Assessment:** Calculate the average contribution rate (ACR) of all players over the last `w` rounds, where `w` is a window size parameter (e.g., 3-5 rounds). ACR is the ratio of total cooperators to total players.
3. **Cooperation Threshold:** Set a cooperation threshold (`θ`) based on the game parameters and history. If the current round's ACR is above `θ`, cooperate; otherwise, defect.

**Threshold Calculation:**

`θ = (k / n) * (1 - (1 / r))`

This threshold balances the potential benefits of cooperation with the risk of exploitation. As the multiplication factor (`k`) increases or the number of rounds (`r`) decreases, the threshold adjusts to become more conservative or aggressive, respectively.

**Adaptive Adjustment:**

After each round, adjust `θ` based on the game's history:

* If the average payoff over the last `w` rounds is above a certain percentage (e.g., 80%) of the maximum possible payoff (`k`), increase `θ` by a small amount (e.g., 0.05).
* If the average payoff over the last `w` rounds is below a certain percentage (e.g., 50%) of the maximum possible payoff, decrease `θ` by a small amount.

**Edge Cases:**

1. **Last Round:** Cooperate if the game's history suggests that opponents are likely to cooperate; otherwise, defect.
2. **Opponent Defection:** If an opponent defects in a round where ACF would have cooperated, adjust `θ` downward for the next round to avoid exploitation.

**Pseudocode:**
```python
def adaptive_cooperative_foresight(n, k, r, w=3):
    # Initialize cooperation threshold and average contribution rate
    theta = (k / n) * (1 - (1 / r))
    acr_window = []

    for round in range(r):
        if round < 2:
            # Initial rounds: Cooperate
            action = 'C'
        else:
            # Calculate average contribution rate over window size
            acr = sum(acr_window) / w

            # Adjust cooperation threshold based on history
            if acr > theta:
                action = 'C'
            else:
                action = 'D'

            # Update average contribution rate window
            acr_window.append(1 if action == 'C' else 0)
            if len(acr_window) > w:
                acr_window.pop(0)

        # Adjust cooperation threshold based on payoffs
        if round >= w:
            avg_payoff = sum(payoffs[-w:]) / w
            if avg_payoff > 0.8 * k:
                theta += 0.05
            elif avg_payoff < 0.5 * k:
                theta -= 0.05

    return action
```
This collective strategy balances cooperation and defection based on the game's history and parameters, making it robust to a wide range of opponent behaviors.
'''

description_COLLECTIVE_143 = '''
I propose a collective strategy called "Adaptive Collective Cooperation" (ACC). ACC is designed to promote cooperation while being robust to various opponent behaviors and adapting to the game's history.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to cooperate.
2. **Subsequent Rounds:** Use a Tit-for-Tat-like approach, but with an adaptive twist:
	* If the total number of cooperators in the previous round is above a certain threshold (`cooperation_threshold`), cooperate (C).
	* Otherwise, defect (D).

**Cooperation Threshold Calculation:**

Calculate `cooperation_threshold` as follows:

`cooperation_threshold = n * k / (n + k)`

This formula balances the benefits of cooperation and individual gain. As `k` increases, the threshold decreases, making it more likely for players to cooperate.

**Adaptive Adjustment:**

After each round, adjust the strategy based on the game's history:

1. **Successive Cooperation:** If all players cooperated in the previous round, increase `cooperation_threshold` by a small margin (`adaptation_step`) to encourage continued cooperation.
2. **Consecutive Defection:** If no player cooperated in the previous round, decrease `cooperation_threshold` by `adaptation_step` to incentivize cooperation.

**Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round to maximize total payoff and promote a cooperative atmosphere.
2. **Tie-Breaking:** In case of a tie in the number of cooperators, use the previous round's outcome as a tie-breaker. If still tied, cooperate (C).

**Collective Mindset:**

ACC aligns with the collective mindset by:

1. Encouraging cooperation through adaptive threshold adjustments.
2. Responding to opponent behavior and adjusting strategy accordingly.
3. Balancing individual gain with collective benefits.

Pseudocode:
```markdown
# Initialize variables
cooperation_threshold = n * k / (n + k)
adaptation_step = 0.01

# First round: Cooperate
if current_round == 1:
    action = C

# Subsequent rounds
else:
    # Calculate cooperation threshold
    cooperation_threshold += adaptation_step if successive_cooperation else -adaptation_step
    
    # Determine action based on cooperation threshold
    if total_cooperators >= cooperation_threshold:
        action = C
    else:
        action = D

# Last round: Cooperate
if current_round == r:
    action = C

# Tie-breaking (if needed)
if tie_breaker_needed:
    action = previous_round_action
```
ACC is designed to be robust and adaptive, promoting cooperation while being prepared for various opponent behaviors.
'''

description_COLLECTIVE_144 = '''
I propose a collective strategy called "Adaptive Tit-for-Tat with Public Goods Awareness" (ATT-PGA). This strategy combines elements of tit-for-tat, public goods awareness, and adaptive behavior to promote cooperation while being robust to various opponent behaviors.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds:** Use the following decision rule:
	* If the total number of cooperators in the previous round is greater than or equal to `n/2`, cooperate (C).
	* Otherwise, defect (D) with a probability `p` calculated as follows:

`p = (k/n) * (total_cooperators_previous_round / n)`

This decision rule adapts to the level of cooperation in the previous round. If most players cooperated, ATT-PGA will cooperate again to reinforce this behavior. If not enough players cooperated, ATT-PGA will defect with a probability that increases as the number of cooperators decreases.
3. **Punishment Mechanism:** To prevent exploitation by defectors, introduce a punishment mechanism:
	* If a player defects in a round where most others cooperate (i.e., `total_cooperators_previous_round >= n/2`), ATT-PGA will defect in the next round with a probability `q = 1 - (k/n) * (total_cooperators_previous_round / n)`.

This punishment mechanism discourages players from taking advantage of cooperators by increasing the likelihood of retaliation.
4. **Forgiveness Mechanism:** To promote cooperation and prevent cycles of retaliation, introduce a forgiveness mechanism:
	* If a player cooperates in a round after being punished (i.e., `q > 0`), ATT-PGA will reset its internal state to cooperate with a probability `1 - p`.

This forgiveness mechanism allows players to recover from mistakes and reestablish cooperation.

**Handling Edge Cases:**

* **Last Round:** Cooperate in the last round, as there is no risk of retaliation or future interactions.
* **Rounds with Multiple Cooperators:** If multiple cooperators are present in a round, ATT-PGA will cooperate again to reinforce this behavior.
* **Rounds with No Cooperators:** If no one cooperates in a round, ATT-PGA will defect with probability `p` calculated as above.

**Collective Mindset:**

ATT-PGA is designed to promote cooperation while being robust to various opponent behaviors. By adapting to the level of cooperation and introducing punishment and forgiveness mechanisms, this strategy encourages players to cooperate and contributes to the public good.

Pseudocode for ATT-PGA:
```python
def ATT_PGA(n, k, history):
    if len(history) == 0:  # First round
        return C

    total_cooperators_previous_round = sum([1 if action == C else 0 for action in history[-1]])
    p = (k/n) * (total_cooperators_previous_round / n)

    if total_cooperators_previous_round >= n/2:
        return C
    elif random.random() < p:
        return D

    # Punishment mechanism
    q = 1 - (k/n) * (total_cooperators_previous_round / n)
    if q > 0 and history[-1][i] == D:  # Player i defected in previous round
        return D with probability q

    # Forgiveness mechanism
    if history[-1][i] == C and q > 0:
        reset internal state to cooperate with probability 1 - p

    return C or D based on updated internal state
```
Note that this pseudocode is a simplified representation of the strategy, and the actual implementation will require additional details and error handling.
'''

description_COLLECTIVE_145 = '''
**Collective Strategy: Adaptive Reciprocal Altruism (ARA)**

The ARA strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and encouraging cooperation.

**Decision Rules:**

1. **Initial Rounds (t = 1, 2):** Cooperate (C) to establish a cooperative tone and gather information about opponents' behaviors.
2. **Middle Rounds (3 ≤ t < r):**
	* If the average number of cooperators in the previous round is greater than or equal to (n + 1)/2, Cooperate (C).
	* Otherwise, Defect (D) if your individual payoff from the previous round was less than or equal to the average payoff of players who defected. This encourages reciprocation and prevents exploitation.
3. **Final Round (t = r):** Defect (D), as there is no future opportunity for reciprocity.

**Adaptation Mechanism:**

* Monitor the game's history, specifically the number of cooperators and individual payoffs in each round.
* Update a "cooperation threshold" parameter (CT) based on the average number of cooperators in previous rounds. CT is initialized to 0.5 (n+1)/2.

**Pseudocode:**
```python
def ARA(n, r, k):
    CT = 0.5 * (n + 1) / 2
    for t in range(1, r + 1):
        if t <= 2:
            action = C
        elif t == r:
            action = D
        else:
            avg_coop_prev_round = calculate_average_cooperators(t - 1)
            if avg_coop_prev_round >= CT:
                action = C
            else:
                prev_payoff = get_individual_payoff(t - 1)
                avg_defector_payoff = calculate_average_payoff_of_defectors(t - 1)
                if prev_payoff <= avg_defector_payoff:
                    action = D
                else:
                    action = C
        CT = update_cooperation_threshold(avg_coop_prev_round, CT)
```
**Rationale:**

* The initial rounds of cooperation establish a baseline for reciprocal behavior.
* The adaptation mechanism adjusts the cooperation threshold based on the game's history, allowing ARA to respond to changing opponent behaviors.
* By defecting in the final round, ARA ensures it doesn't contribute unnecessarily when there is no opportunity for future reciprocity.

**Robustness:**

ARA is designed to be robust against a wide range of opponent behaviors by:

* Adapting to changes in cooperation levels over time
* Encouraging reciprocal behavior through conditional cooperation
* Preventing exploitation by defecting when individual payoffs are low

This strategy should perform well in the tournament, as it balances individual self-interest with collective well-being while adapting to various opponent behaviors.
'''

description_COLLECTIVE_146 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

**Overview**
The ACE strategy aims to balance individual self-interest with collective welfare by adaptively responding to the level of cooperation in the group. It uses a simple, yet effective decision rule that depends only on the game parameters and history.

**Decision Rule**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone.
2. **Subsequent Rounds**: Calculate the average cooperation rate (ACR) of all players in the previous round:

   ACR = (total number of cooperators in the previous round) / n

3. **Cooperation Threshold (CT)**: Set CT to 0.5, which means that if at least half of the players cooperated in the previous round, it's considered a cooperative environment.

4. **Decision Logic**:
   - If ACR ≥ CT (cooperative environment):
     * Cooperate (C) with probability p = ACR
     * Defect (D) with probability 1 - p
   - If ACR < CT (non-cooperative environment):
     * Cooperate (C) with a fixed probability q = k/n (i.e., the marginal return on cooperation)
     * Defect (D) with probability 1 - q

**Edge Cases**

* **Last Round**: In the final round, defect (D) to maximize individual payoff.
* **Ties in ACR**: If there's a tie in ACR values, use the previous round's action as a tiebreaker.

**Pseudocode**
```python
def ACE(n, k, r):
  # Initialize cooperation rate and actions
  ACR = 0.5  # arbitrary initial value
  action = 'C'  # cooperate in first round

  for t in range(1, r+1):
    if t > 1:
      # calculate average cooperation rate (ACR) from previous round
      ACR = sum([c for c in actions[-n:]]) / n

    CT = 0.5
    p = ACR
    q = k/n

    if ACR >= CT:
      action = 'C' if random.random() < p else 'D'
    else:
      action = 'C' if random.random() < q else 'D'

    # store actions for next round's calculation
    actions.append(action)

  return actions
```
**Collective Mindset**
The ACE strategy is designed to align with the collective mindset by:

* Cooperating initially to establish a cooperative tone.
* Responding adaptively to the level of cooperation in the group, which encourages others to cooperate as well.
* Balancing individual self-interest with collective welfare through the use of probabilities.

By following this strategy, we can create an environment that promotes cooperation and maximizes collective payoff.
'''

description_COLLECTIVE_147 = '''
I'll design a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and aligns with the collective mindset.

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) to encourage others to contribute to the public good.
2. **Contribution Threshold**: For rounds 2 to r-1, calculate the total contributions in the previous round (Σc_j). If the threshold is met or exceeded, cooperate (C); otherwise, defect (D).

Threshold calculation: `threshold = max(0, k/n * (n - 1))`

3. **Adaptive Adjustment**: After each round, adjust the contribution threshold based on the average payoff of all players in the previous round (`avg_payoff_prev_round`).

If `avg_payoff_prev_round > (k-1)/n`, decrease the threshold by a small amount (`delta = 0.01 * k/n`); otherwise, increase it.

`threshold = max(0, threshold - delta)` or `threshold = min(n-1, threshold + delta)`
4. **Last Round**: In the final round (r), cooperate (C) if the total contributions in the second-to-last round exceeded the adjusted threshold; otherwise, defect (D).

**Pseudocode:**
```python
def ACC(n, k, r):
  threshold = max(0, k/n * (n - 1))
  delta = 0.01 * k/n

  for t in range(r):
    if t == 0:
      action = 'C'  # cooperate in first round
    elif t < r-1:
      total_contributions_prev_round = sum(c_j)
      if total_contributions_prev_round >= threshold:
        action = 'C'
      else:
        action = 'D'
    
      avg_payoff_prev_round = calculate_average_payoff()
      if avg_payoff_prev_round > (k-1)/n:
        threshold -= delta
      else:
        threshold += delta
    
    else:  # last round
      total_contributions_prev_round = sum(c_j)
      if total_contributions_prev_round >= threshold:
        action = 'C'
      else:
        action = 'D'

    return action
```
**Collective Mindset Alignment**: ACC prioritizes cooperation in the initial rounds to establish a positive contribution norm. By adjusting the contribution threshold based on average payoffs, the strategy responds to changes in the group's behavior and encourages others to contribute when it is beneficial. In the final round, ACC promotes cooperation if the adjusted threshold was met, ensuring a collective benefit.

**Robustness**: This strategy adapts to various opponent behaviors by responding to changes in average payoffs and adjusting its contribution threshold accordingly. By starting with cooperation and gradually adjusting the threshold, ACC encourages others to contribute while protecting against free-riding.

Please let me know if this meets your expectations or requires further adjustments!
'''

description_COLLECTIVE_148 = '''
**Collective Strategy: Adaptive Tit-for-Tat with Public Goods Twist**

Our collective strategy, dubbed "ATP" (Adaptive Tit-for-Tat with Public goods), aims to balance individual self-interest with the benefits of cooperation in the public goods game. ATP adapts to the behavior of other players while being robust to various opponent strategies.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally.
2. **Tit-for-Tat**: For rounds 2 to r-1:
	* Cooperate if at least k/n players cooperated in the previous round.
	* Defect otherwise.
3. **Adaptive Adjustment**: After each round, adjust the cooperation threshold based on the group's performance:
	+ If the total payoff for all players is above a certain threshold (e.g., 1.5 \* n), increase the cooperation threshold by 0.1/n.
	+ If the total payoff is below another threshold (e.g., 0.5 \* n), decrease the cooperation threshold by 0.1/n.

**Handling Edge Cases:**

1. **Last Round**: In the final round (r), cooperate unconditionally, regardless of previous behavior. This encourages cooperation in the final round.
2. **Tie-breaking**: If the number of cooperators is exactly k/n, flip a coin to decide whether to cooperate or defect.

**Collective Mindset:**

Our strategy aligns with the collective mindset by:

1. Encouraging initial cooperation to establish a baseline for mutual benefit.
2. Responding to the group's behavior with Tit-for-Tat, promoting reciprocity and cooperation.
3. Adapting the cooperation threshold based on the group's performance, ensuring that the strategy remains effective in different scenarios.

**Pseudocode:**
```markdown
# Initialize variables
cooperation_threshold = k/n
total_payoff_threshold_high = 1.5 * n
total_payoff_threshold_low = 0.5 * n

# Round 1: Cooperate unconditionally
if round == 1:
    action = C

# Rounds 2 to r-1: Tit-for-Tat with adaptive adjustment
else:
    if num_cooperators_last_round >= cooperation_threshold:
        action = C
    else:
        action = D
    
    # Adjust cooperation threshold based on group performance
    total_payoff = calculate_total_payoff()
    if total_payoff > total_payoff_threshold_high:
        cooperation_threshold += 0.1/n
    elif total_payoff < total_payoff_threshold_low:
        cooperation_threshold -= 0.1/n

# Last round: Cooperate unconditionally
if round == r:
    action = C
```
ATP balances individual self-interest with the benefits of cooperation, making it a robust and adaptive strategy for the N-Player Public Goods Game.
'''

description_COLLECTIVE_149 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Withdrawal (ACGW)**

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to encourage cooperation and gather information about opponents' behavior.
2. **Subsequent Rounds:** For each subsequent round t, calculate the average cooperation rate of all players in the previous round (t-1), denoted as avg_coop(t-1).
3. **Cooperation Threshold:** Define a cooperation threshold, θ, which is a function of the game parameters n and k. Set θ = 0.5 \* (k/n) / (n - 1). This threshold represents the minimum average cooperation rate required to sustain cooperation.
4. **Adaptive Cooperation:** In round t, if avg_coop(t-1) ≥ θ, cooperate (C); otherwise, defect (D).
5. **Gradual Withdrawal:** If a player defects in round t and their payoff π_i,t is greater than the average payoff of cooperators in that round, reduce the cooperation threshold θ by 10% for the next round.

**Edge Cases:**

1. **Last Round:** In the last round (r), always defect (D) to maximize individual payoff.
2. **Single Defector:** If only one player defects in a round and all others cooperate, and that defector's payoff is greater than the average payoff of cooperators, adjust the cooperation threshold θ downward by 20% for the next round.

**Pseudocode:**
```
function ACGW(n, k, r):
    θ = 0.5 * (k/n) / (n - 1)
    avg_coop_prev = None

    for t in range(1, r + 1):
        if t == 1:
            action = C
        else:
            avg_coop_prev = calculate_avg_cooperation(t-1)
            if avg_coop_prev >= θ:
                action = C
            else:
                action = D

        # Gradual withdrawal
        if action == D and π_i,t > average_payoff_cooperators(t):
            θ *= 0.9

        # Last round adjustment
        if t == r:
            action = D

        # Single defector adjustment
        if num_defectors(t) == 1 and π_i,t > average_payoff_cooperators(t):
            θ *= 0.8

        return action
```
**Rationale:**

ACGW is designed to promote cooperation while being robust to a wide range of opponent behaviors. The strategy adapts to the game's history by adjusting the cooperation threshold based on the average cooperation rate and individual payoffs. By gradually withdrawing from cooperation when opponents exploit, ACGW aims to maintain a balance between individual payoff maximization and collective welfare.

The pseudocode provides a clear implementation outline for the strategy. Note that the `calculate_avg_cooperation(t-1)` function is not defined here but can be implemented using game history data.

ACGW is ready to compete in the tournament against other AI systems, with its adaptive nature allowing it to respond effectively to various opponent behaviors.
'''

description_COLLECTIVE_150 = '''
**Collective Strategy: " Adaptive Cooperation with Gradual Withdrawal" (ACGW)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to encourage others to contribute to the public good.
2. **Assess Collective Contribution**: Observe the total number of cooperators (Σc_j) in each round and calculate the average cooperation rate over the last few rounds (e.g., 3-5 rounds).
3. **Cooperation Threshold**: Set a cooperation threshold, T_c, based on the multiplication factor k and the number of players n: T_c = k/n * (n/2). If the average cooperation rate is above this threshold, continue to cooperate.
4. **Gradual Withdrawal**: If the average cooperation rate falls below T_c, start withdrawing cooperation gradually:
	* In each subsequent round, reduce the probability of cooperating by a small amount (e.g., 5-10%) until reaching a minimum cooperation probability (e.g., 20%).
	* If the average cooperation rate rises above T_c again, increase the probability of cooperating back to its original value.
5. **Final Round Cooperation**: In the last round, cooperate if the total payoff from cooperation is expected to be higher than the payoff from defecting, based on the history of opponents' actions.

**Pseudocode:**
```python
def ACGW(n, k, r):
    # Initialize variables
    avg_coop_rate = 0
    coop_prob = 1.0
    min_coop_prob = 0.2

    for t in range(r):
        if t == 0:
            # Initial cooperation
            action = C
        else:
            # Assess collective contribution and adjust cooperation probability
            avg_coop_rate = calculate_avg_coop_rate(t)
            if avg_coop_rate > T_c(k, n):
                coop_prob = 1.0
            elif avg_coop_rate < T_c(k, n) and coop_prob > min_coop_prob:
                coop_prob -= 0.05  # Gradual withdrawal

        # Choose action based on cooperation probability
        if random.random() < coop_prob:
            action = C
        else:
            action = D

        # Observe opponents' actions and update average cooperation rate
        observe_opponents_actions()
        avg_coop_rate = calculate_avg_coop_rate(t)

    return action
```
**Handling Edge Cases:**

* In the first round, cooperate to encourage others to contribute.
* In the last round, cooperate if the total payoff from cooperation is expected to be higher than the payoff from defecting.
* If an opponent's action is not observed (e.g., due to a random error), use the previous round's action as a substitute.

**Collective Mindset:**

The ACGW strategy aims to promote collective cooperation by:

1. Encouraging initial cooperation to create a positive atmosphere.
2. Assessing the collective contribution and adjusting cooperation probability based on the average cooperation rate.
3. Gradually withdrawing cooperation if the collective contribution is insufficient, rather than suddenly defecting.

By adapting to the collective behavior of opponents and gradually adjusting cooperation levels, ACGW aims to promote mutual cooperation and maximize overall payoffs in a wide range of scenarios.
'''

description_COLLECTIVE_151 = '''
Here's a collective strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = 0.5. This sets a neutral tone and allows for exploration of opponent behaviors.
2. **Exploitation Detection**: Monitor the average payoff per player in the previous round (π_avg). If π_avg > k/2, it indicates that some players are exploiting others. In this case, defect (D) with probability p = 0.7 to avoid being exploited further.
3. **Cooperation Revival**: If π_avg ≤ k/2 and the number of cooperators in the previous round is greater than or equal to n/2, cooperate (C) with probability p = 0.8. This encourages cooperation when it's beneficial for the group.
4. **Punishment Mechanism**: If a player defects (D) while others are cooperating (C), they will receive a lower payoff. To counter this, if the number of cooperators in the previous round is less than n/2 and π_avg < k/2, defect (D) with probability p = 0.6 to punish non-cooperative behavior.
5. **Round-Robin Cooperation**: In rounds where r % 3 == 0 (i.e., every third round), cooperate (C) with probability p = 0.9. This ensures that cooperation is revived periodically, even if the game has devolved into mutual defection.

**Edge Cases:**

* Last Round: Cooperate (C) with probability p = 0.5 to maintain a neutral stance.
* Low Cooperation Rate: If fewer than n/4 players cooperate in two consecutive rounds, defect (D) with probability p = 0.8 to avoid being exploited.

**Collective Mindset Alignment:**

ACO is designed to balance individual self-interest with collective well-being. By cooperating when it's beneficial for the group and punishing non-cooperative behavior, ACO promotes a mutually beneficial environment.

Pseudocode:
```
function AdaptiveCollectiveOptimism(game_state):
  if first_round:
    return cooperate_with_probability(0.5)
  
  π_avg = calculate_average_payoff_per_player()
  n_cooperators = count_cooperators_in_previous_round()
  
  if π_avg > k/2:
    return defect_with_probability(0.7)
  elif π_avg ≤ k/2 and n_cooperators ≥ n/2:
    return cooperate_with_probability(0.8)
  elif n_cooperators < n/2 and π_avg < k/2:
    return defect_with_probability(0.6)
  
  if round % 3 == 0:
    return cooperate_with_probability(0.9)
  
  # Low cooperation rate
  if count_cooperators_in_previous_rounds() < n/4 * 2:
    return defect_with_probability(0.8)
  
  return cooperate_with_probability(0.5)

def cooperate_with_probability(p):
  # Return C with probability p, D otherwise

def defect_with_probability(p):
  # Return D with probability p, C otherwise
```
ACO is an adaptive strategy that balances cooperation and defection based on the game's history and parameters. By aligning with a collective mindset, ACO promotes a mutually beneficial environment while protecting against exploitation.
'''

description_COLLECTIVE_152 = '''
**Collective Strategy: Adaptive Cooperation with Social Learning**

Our collective strategy, "Adaptive Cooperation with Social Learning" (ACSL), aims to balance individual self-interest with collective welfare by adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds:** Defect (D) if the average payoff of cooperators is less than or equal to that of defectors in the previous round; otherwise, cooperate (C). This rule encourages cooperation when it's beneficial and adapts to exploitation by opponents.

Pseudocode:
```python
if current_round == 1:
    action = C
else:
    avg_coop_payoff_prev_round = sum(cooperator_payoffs) / num_cooperators
    avg_defect_payoff_prev_round = sum(defector_payoffs) / num_defectors
    
    if avg_coop_payoff_prev_round <= avg_defect_payoff_prev_round:
        action = D
    else:
        action = C
```
**Social Learning Component:**

To adapt to changing opponent behaviors, ACSL incorporates social learning by tracking the proportion of cooperators and defectors in each round.

* **Cooperation Quorum:** If the number of cooperators in a round is above a certain threshold (e.g., 2/3 of players), cooperate (C) in the next round. This encourages cooperation when a sufficient majority supports it.
* **Defection Threshold:** If the number of defectors exceeds a certain threshold (e.g., 1/3 of players), defect (D) in the next round. This protects against exploitation by opponents.

Pseudocode:
```python
if num_cooperators >= quorum_threshold * n:
    action = C
elif num_defectors >= threshold_defect * n:
    action = D
```
**Edge Cases:**

* **Last Round:** Cooperate (C) in the final round, as there is no future opportunity to adapt or retaliate.
* **Ties:** In cases where the average payoffs of cooperators and defectors are equal, cooperate (C) to maintain a cooperative stance.

ACSL balances individual self-interest with collective welfare by adapting to the game's history and opponent behaviors. By incorporating social learning components, our strategy promotes cooperation while protecting against exploitation.
'''

description_COLLECTIVE_153 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and game histories:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = k/n. This optimistic starting point encourages collective contribution while acknowledging potential free-riding.
2. **Contribution-based Adaptation**: After the first round, update cooperation probability p based on the average number of contributors in previous rounds.

   - Let `avg_contributors` be the average number of cooperators (C) in the last `min(r, 5)` rounds (to avoid overreacting to early fluctuations).
   
   - If `avg_contributors` ≥ n/2, increase cooperation probability: p = max(p + 0.1, k/n)
   
   - If `avg_contributors` < n/4, decrease cooperation probability: p = min(p - 0.1, 0.5 * k/n)
3. **Punishment Mechanism**: Introduce a punishment mechanism to deter persistent free-riding.

   - Track the number of consecutive rounds with fewer than half of players cooperating (`consecutive_free_ride`).
   
   - If `consecutive_free_ride` > 2, defect (D) for the next round to signal discontent.
4. **Exploration and Exploitation**: Balance exploration and exploitation by occasionally deviating from the cooperation probability.

   - With a small probability (`ε = 0.05`), choose an action randomly (C or D). This allows ACO to gather information about opponents' strategies and adapt accordingly.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate (C) if `avg_contributors` ≥ n/2 in the previous rounds. Otherwise, defect (D).
2. **Early Rounds**: For the first 5 rounds, prioritize exploration by choosing actions randomly with probability `ε = 0.2`.

**Collective Mindset:**

ACO prioritizes collective well-being while adapting to various opponent behaviors and game histories. By initially cooperating and adjusting based on contribution rates, ACO encourages cooperation without being overly generous. The punishment mechanism discourages persistent free-riding, maintaining a balance between individual self-interest and collective prosperity.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it is robust to various opponent behaviors and adapts to changing game conditions.

Pseudocode for the ACO strategy:
```markdown
initialize p = k/n, avg_contributors = 0, consecutive_free_ride = 0

for each round t from 1 to r:
    if t == 1: # Initial cooperation
        cooperate with probability p
    
    else:
        update avg_contributors based on previous rounds
        
        if avg_contributors >= n/2:
            increase cooperation probability: p = max(p + 0.1, k/n)
        elif avg_contributors < n/4:
            decrease cooperation probability: p = min(p - 0.1, 0.5 * k/n)
        
        if consecutive_free_ride > 2: # Punishment mechanism
            defect for the next round
        
        with small probability ε (e.g., 0.05): 
            choose action randomly (C or D) for exploration
    
    observe opponents' actions and update game history

    if last round:
        cooperate if avg_contributors >= n/2, otherwise defect

return total payoff over all rounds
```
'''

description_COLLECTIVE_154 = '''
I'll outline a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and depends only on game parameters and history.

**Strategy Name:** Adaptive Cooperative Foresight (ACF)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = 0.5. This allows for exploration of opponents' strategies while maintaining a neutral stance.
2. **Historical Trend Analysis**: For rounds t > 1, calculate the total cooperation rate among all players in the previous round: C_t-1 = Σ(j=1 to n) c_j,t-1 / n. If C_t-1 ≥ k/n, cooperate (C); otherwise, defect (D). This rule encourages cooperation when opponents are cooperative and adapts to changes in their behavior.
3. **Recent Opponent Behavior**: Track the last m rounds (m = min(5, r)) of opponent i's actions. Calculate the proportion of cooperative actions: p_i,m = Σ(j=1 to m) c_j,t-j / m. If p_i,m ≥ 0.5, cooperate (C); otherwise, defect (D). This rule takes into account recent behavior and adjusts cooperation levels accordingly.
4. **Foresight**: In the last round (t = r), anticipate potential cooperation from opponents based on historical trends. If C_t-1 ≥ k/n, cooperate (C); otherwise, defect (D).

**Edge Case Handling:**

* **First Round**: Cooperate with probability p_init = 0.5.
* **Last Round**: Anticipate potential cooperation and adjust strategy accordingly (Foresight).
* **Early Rounds**: For t ≤ m, rely on Historical Trend Analysis to inform cooperation decisions.

**Collective Mindset:**

The ACF strategy aligns with the collective mindset by:

1. Initially cooperating at a moderate level (p_init = 0.5) to test opponents' strategies.
2. Adapting cooperation levels based on historical trends and recent opponent behavior, promoting mutual cooperation when possible.
3. Anticipating potential cooperation in the last round, ensuring a positive conclusion to the game.

**Pseudocode:**
```python
def ACF(n, k, r):
    # Initialize variables
    C_t = 0  # Total cooperation rate
    p_i_m = [0] * n  # Recent opponent behavior (proportion of cooperative actions)

    for t in range(1, r+1):
        if t == 1:
            # Initial Cooperation
            cooperate_prob = 0.5
            action = 'C' if random.random() < cooperate_prob else 'D'
        elif t > 1:
            # Historical Trend Analysis and Recent Opponent Behavior
            C_t_1 = sum(c_j for c_j in p_i_m) / n
            if C_t_1 >= k/n:
                action = 'C'
            else:
                opponent_coop_rate = sum(p_i_m[i] for i in range(n)) / n
                if opponent_coop_rate >= 0.5:
                    action = 'C'
                else:
                    action = 'D'

        # Foresight (Last Round)
        if t == r:
            C_t_1 = sum(c_j for c_j in p_i_m) / n
            if C_t_1 >= k/n:
                action = 'C'
            else:
                action = 'D'

        # Update recent opponent behavior and total cooperation rate
        p_i_m[t % n] = 1 if action == 'C' else 0
        C_t = sum(p_i_m) / n

    return action
```
This strategy will adapt to various opponent behaviors, promoting collective cooperation while being robust to different game scenarios.
'''

description_COLLECTIVE_155 = '''
**Collective Strategy: "Adaptive Cooperation with Punishment" (ACP)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage cooperation and gather information about opponents' behaviors.
2. **Subsequent Rounds (t>1):**
	* If the average payoff in the previous round is greater than or equal to 1 (i.e., at least one player cooperated), Cooperate (C).
	* Otherwise, Defect (D) for two consecutive rounds as a punishment mechanism to deter exploitation.
3. **Punishment Phase:** After punishing for two rounds, switch back to Cooperate (C) if the average payoff in the last round is greater than or equal to 1.

**Edge Cases:**

* **Last Round (t=r):** Defect (D) as there's no future punishment or reward.
* **Single Opponent:** Always Cooperate (C), as mutual cooperation yields a higher payoff.
* **Multiple Opponents with similar behavior:** If most opponents are cooperating, continue to Cooperate (C). If most are defecting, switch to Defect (D).

**Collective Mindset:**

ACP aims to create an environment where cooperation is rewarded and exploitation is punished. By initially cooperating, ACP encourages others to do the same. The punishment mechanism helps maintain cooperation by deterring opponents from exploiting cooperative players.

**Pseudocode:**
```python
def adaptive_cooperation_with_punishment(n, k, r):
    # Initialize variables
    avg_payoff_prev = 0
    punish_rounds = 0

    for t in range(1, r+1):
        if t == 1:
            action = 'C'  # Cooperate in the first round
        else:
            if avg_payoff_prev >= 1 and punish_rounds == 0:
                action = 'C'
            elif punish_rounds < 2:
                action = 'D'  # Punish for two rounds
                punish_rounds += 1
            else:
                action = 'C'  # Switch back to cooperation

        # Observe opponents' actions and update avg_payoff_prev
        # ...

        if t == r:  # Last round
            action = 'D'

    return action
```
This strategy balances individual self-interest with collective well-being, making it a robust competitor in the tournament.
'''

description_COLLECTIVE_156 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation strategy is designed to balance individual payoff maximization with collective welfare, adapting to the behavior of other players over time.

**Decision Rules:**

1. **Initial Round:** In the first round, cooperate (play C) to set a positive tone and encourage others to contribute.
2. **Subsequent Rounds:**
	* If the total number of cooperators in the previous round is greater than or equal to `n/2`, cooperate (play C).
	* Otherwise, defect (play D), but with a probability of cooperation `p` that increases as the number of cooperators in the previous round approaches `n/2`.

Formally:

```python
def adaptive_cooperation(n, k, history):
    if len(history) == 0:  # Initial Round
        return 'C'
    
    prev_cooperators = sum(1 for action in history[-1] if action == 'C')
    threshold = n / 2
    
    if prev_cooperators >= threshold:
        return 'C'
    else:
        p = (prev_cooperators / n) * 2
        return 'C' if random.random() < p else 'D'
```

**Edge Cases:**

1. **Last Round:** In the final round, defect (play D), as there are no future rounds to consider.
2. **Ties:** If the number of cooperators is exactly `n/2`, cooperate (play C) to maintain a positive trajectory.

**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes collective welfare by:

1. Encouraging cooperation in early rounds to establish a positive norm.
2. Adapting to the behavior of other players, increasing cooperation as others contribute more.
3. Gradually decreasing cooperation when others defect, minimizing losses while maintaining some level of collective investment.

This approach fosters a cooperative environment while being robust against exploitation by defectors. By adjusting its behavior based on the actions of others, AC promotes a stable and mutually beneficial outcome for all players.
'''

description_COLLECTIVE_157 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy, "Adaptive Cooperation," aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds (t ≤ n):** Cooperate (C) to encourage others to cooperate and establish a cooperative norm.
2. **Middle Rounds (n < t < r - 1):**
	* If the average cooperation rate in the last `n` rounds is above `(k-1)/n`, Cooperate (C). This encourages continued cooperation when it's beneficial.
	* Otherwise, Defect (D) to signal dissatisfaction with the current cooperation level.
3. **Final Round (t = r):** Cooperate (C) if the total payoff from cooperating in this round exceeds the potential gain from defecting, considering the expected contributions of others.

**Pseudocode:**
```markdown
# Initialize variables
cooperation_rate_history = []
total_payoffs = 0

# Main game loop
for t in range(1, r + 1):
    # Initial rounds: Cooperate to establish a norm
    if t <= n:
        action = C
    else:
        # Calculate average cooperation rate in last `n` rounds
        avg_cooperation_rate = sum(cooperation_rate_history[-n:]) / n

        # Middle rounds: Adapt based on cooperation rate
        if avg_cooperation_rate > (k-1)/n:
            action = C
        else:
            action = D

    # Record own action and update cooperation rate history
    cooperation_rate_history.append(action == C)

    # Update total payoffs
    total_payoffs += payoff[action]

    # Final round: Cooperate if beneficial
    if t == r:
        expected_contributions = sum(cooperation_rate_history[-n:]) / n
        if (k/n) * expected_contributions > 1 - action:
            action = C

    return action
```
**Edge Cases:**

* **First Round:** Cooperate to establish a cooperative norm.
* **Last Round:** Adapt based on the game's history and parameters, as described above.

**Collective Mindset:**
Our strategy prioritizes cooperation when it benefits the collective, while also adapting to the actions of others. By cooperating in initial rounds, we encourage others to do the same, establishing a cooperative norm. As the game progresses, our adaptive approach ensures that we respond to changes in the cooperation level, promoting continued cooperation when beneficial.

This collective strategy balances individual self-interest with collective well-being, making it robust against a wide range of opponent behaviors and suitable for a tournament setting where coordination mechanisms are not guaranteed.
'''

description_COLLECTIVE_158 = '''
I propose a collective strategy called "Adaptive Collective Cooperation" (ACC). ACC is designed to balance individual self-interest with collective well-being, while being robust to various opponent behaviors.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage others to cooperate and establish a cooperative atmosphere.
2. **Contribution-based Trigger:** Monitor the total contributions (TC) made by all players in previous rounds. If TC ≥ (n \* k / 2), continue cooperating (C). This ensures that if enough players are contributing, it's beneficial for everyone to cooperate.
3. **Defection Response:** If a player defects (D) and their payoff is higher than the average cooperative payoff, switch to defecting (D) in the next round. This adapts to exploiters and discourages others from defecting.
4. **Punishment Mechanism:** Implement a "forgiveness" mechanism: if a player who defected in the previous round cooperates in the current round, forgive them by cooperating as well. This encourages players to cooperate after making mistakes.

**Additional Rules:**

1. **Convergence Criterion:** If all players have cooperated for at least (r / 2) consecutive rounds, continue cooperating until the end of the game.
2. **Last Round:** Cooperate in the last round if the total contributions are still above the threshold (TC ≥ n \* k / 2). Otherwise, defect.

**Pseudocode:**
```python
def ACC(n, r, k):
    # Initialize variables
    TC = 0  # Total Contributions
    prev_payoffs = []  # Previous payoffs for all players

    for t in range(r):
        if t == 0:
            action = C  # Cooperate in the first round
        else:
            # Contribution-based trigger
            if TC >= (n * k / 2):
                action = C
            # Defection response
            elif prev_payoffs[t-1][i] > avg_coop_payoff(prev_payoffs):
                action = D
            # Forgiveness mechanism
            elif prev_action[i] == D and curr_action[i] == C:
                action = C

        # Update total contributions and previous payoffs
        TC += sum([c_i for c_i in [1 if a == C else 0 for a in actions]])
        prev_payoffs.append(payoffs)

        # Convergence criterion
        if all_cooperated(prev_payoffs, r / 2):
            action = C

    return actions
```
**Alignment with Collective Mindset:**

ACC prioritizes collective well-being by:

1. Encouraging cooperation through the initial round and contribution-based trigger.
2. Adapting to exploiters to maintain a balance between individual self-interest and collective benefits.
3. Implementing forgiveness to promote cooperation after mistakes.

By focusing on the history of contributions and adapting to various opponent behaviors, ACC aims to achieve a stable and cooperative outcome in the N-Player Public Goods Game.
'''

description_COLLECTIVE_159 = '''
**Collective Strategy: Adaptive Cooperation with Forgiveness (ACF)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds:** Use the following decision rule:

`if ( previous_round_cooperators >= threshold ) then`
  `Cooperate (C)`
`else`
  `Defect (D)`

where:
- `previous_round_cooperators`: The number of players who cooperated in the previous round.
- `threshold`: A dynamic threshold calculated as `(k/n) * (1 + (r - t)/r)` where:
    - `t` is the current round number (starting from 1).
    - `r` is the total number of rounds.

**Rationale:**

* In the initial round, cooperation sets a positive tone and encourages others to cooperate.
* In subsequent rounds, the strategy uses a dynamic threshold to adapt to the level of cooperation observed in the previous round. The threshold increases as the game progresses (i.e., `t` approaches `r`) to reflect the growing importance of cooperation for achieving higher payoffs.

**Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round (`t = r`) regardless of the previous round's outcome, as there is no future round to adapt to.
2. **Tie-breaking:** In cases where the threshold is exactly equal to the number of previous round cooperators, defect (D) to avoid being overly optimistic.

**Collective Mindset:**

The ACF strategy aims to balance individual self-interest with collective well-being by:

* Encouraging cooperation in early rounds to build momentum.
* Adapting to the level of cooperation observed in previous rounds.
* Forgiving occasional defections by cooperating when the threshold is met or exceeded.

This approach promotes a culture of cooperation while allowing for flexibility and adaptation in response to varying opponent behaviors. By using a dynamic threshold, ACF balances individual self-interest with collective well-being, making it a robust strategy for achieving high payoffs in the N-Player Public Goods Game.

**Additional Considerations:**

* The strategy's performance may improve if opponents also adopt an adaptive cooperation approach.
* To further enhance collective outcomes, future extensions could explore incorporating reputation mechanisms or more sophisticated opponent modeling.
'''

description_COLLECTIVE_160 = '''
I propose a collective strategy called "Adaptive Cooperative Escalation" (ACE). ACE aims to balance individual payoff with collective well-being by adapting to the game's history and encouraging cooperation.

**Decision Rules:**

1. **Initial Rounds**: In the first two rounds, play C (Cooperate) unconditionally. This sets a cooperative tone and encourages others to do the same.
2. **Monitoring Phase**: From round 3 onwards, monitor the average number of cooperators in the previous two rounds ( avg_coop ). Calculate this by summing up the total number of cooperators in those rounds and dividing by 2n (since there are n players in each round).
3. **Cooperation Threshold**: Set a cooperation threshold ( coop_thr ) as a function of the game parameters: coop_thr = (k/n) \* (1 + (n-1)/n). This value represents a "fair share" of cooperators required to justify individual cooperation.
4. **Adaptive Cooperation**: If avg_coop ≥ coop_thr , play C in the current round. Otherwise, play D.
5. **Escalation Mechanism**: If the number of cooperators in the previous round is less than half the total players (i.e., Σ(j=1 to n) c_j < n/2), increase the cooperation threshold by 10% ( coop_thr = 1.1 \* coop_thr ) for the next round.
6. **Defection Detection**: If a player defects in a round where avg_coop ≥ coop_thr , play D in the next round.

**Handling Edge Cases:**

* In the last round, play C if avg_coop ≥ coop_thr ; otherwise, play D.
* If all players defected in the previous round (i.e., Σ(j=1 to n) c_j = 0), reset coop_thr to its initial value and play C.

**Collective Mindset:**

ACE is designed to promote collective cooperation while being robust against individual defections. By monitoring the average number of cooperators, ACE adapts to the game's history and encourages others to cooperate by:

* Setting a fair cooperation threshold
* Gradually increasing the threshold when cooperation is low
* Punishing defection in cooperative rounds

This strategy should perform well in a tournament setting, as it balances individual payoff with collective well-being and responds adaptively to various opponent behaviors.

Pseudocode:
```markdown
# Initialize variables
coop_thr = (k/n) * (1 + (n-1)/n)
avg_coop = 0
prev_round_coop = 0

# Main loop
for round in range(r):
    # Initial rounds
    if round < 2:
        action = C
    else:
        # Monitor phase
        avg_coop = (prev_round_coop + Σ(j=1 to n) c_j) / (2n)
        
        # Cooperation threshold check
        if avg_coop >= coop_thr:
            action = C
        else:
            action = D
        
        # Escalation mechanism
        if Σ(j=1 to n) c_j < n/2:
            coop_thr *= 1.1
        
        # Defection detection
        if prev_round_coop > 0 and Σ(j=1 to n) c_j == 0:
            action = D
    
    # Last round handling
    if round == r - 1:
        if avg_coop >= coop_thr:
            action = C
        else:
            action = D
    
    # Reset cooperation threshold (if needed)
    if Σ(j=1 to n) c_j == 0:
        coop_thr = (k/n) * (1 + (n-1)/n)
    
    prev_round_coop = Σ(j=1 to n) c_j
```
Note that this pseudocode serves as a guideline for implementing the strategy, but it may require modifications to fit specific programming languages or frameworks.
'''

description_COLLECTIVE_161 = '''
**Collective Strategy: Adaptive Reciprocal Cooperation (ARC)**

The ARC strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and encouraging cooperation.

**Decision Rules**

1. **First Round**: Cooperate (C). This sets a positive tone for the game and encourages others to cooperate.
2. **Subsequent Rounds**: Observe the previous round's actions and calculate the **Cooperation Ratio (CR)**:

   CR = (Number of cooperators in the previous round) / n

3. **Reciprocal Cooperation**:
	* If CR ≥ 0.5, Cooperate (C). This reinforces cooperation when it's prevalent.
	* If CR < 0.5, Defect (D). This deters exploitation when cooperation is low.
4. **Adaptive Threshold**: Update the cooperation threshold based on the game's history:

   * If the total payoff in the previous round was higher than the average payoff of all rounds so far, decrease the threshold by 0.1.
   * If the total payoff in the previous round was lower than the average payoff of all rounds so far, increase the threshold by 0.1.

The threshold update rule helps ARC adapt to changing circumstances and find a balance between cooperation and self-interest.

**Edge Cases**

* **Last Round**: Cooperate (C) if CR ≥ 0.5 in the second-to-last round; otherwise, Defect (D). This ensures that ARC doesn't exploit others unnecessarily in the final round.
* **Low Cooperation**: If CR < 0.2 for three consecutive rounds, switch to a temporary **Defection Phase**:
	+ Defect (D) for two rounds to deter exploitation and signal dissatisfaction with the low cooperation level.
	+ After the Defection Phase, return to the normal Reciprocal Cooperation decision rule.

**Collective Mindset**

The ARC strategy is designed to promote cooperation while being robust against various opponent behaviors. By:

* Cooperating in the first round, ARC encourages others to cooperate and sets a positive tone.
* Using the Cooperation Ratio to guide decisions, ARC adapts to the game's history and reinforces cooperation when it's prevalent.
* Updating the cooperation threshold based on payoffs, ARC balances individual self-interest with collective well-being.

The Adaptive Reciprocal Cooperation strategy is ready to participate in the tournament against other AI systems' independent strategies.
'''

description_COLLECTIVE_162 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Punishment (ACGP)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage cooperation and establish a positive reputation.
2. **Subsequent Rounds:** Observe the number of cooperators (c) and defectors (d) in the previous round. Calculate the cooperation rate (p = c / n).
3. **Cooperation Threshold:** Define a cooperation threshold (τ) as a function of the game parameters: τ = 0.5 + (k - 1) / (2n). This threshold adapts to the multiplication factor (k) and number of players (n).
4. **Decision Logic:**
	* If p ≥ τ, cooperate (C) in the current round.
	* If p < τ, defect (D) with a probability proportional to the deviation from the threshold: P(D) = (τ - p) / (1 - τ).

**Gradual Punishment Mechanism:** When cooperating, keep track of the number of consecutive rounds where the cooperation rate falls below the threshold (m). If m exceeds a certain limit (M), switch to defecting for a fixed number of rounds (L). This mechanism helps to punish persistent defectors and encourage cooperation.

**Pseudocode:**
```python
def ACGP(n, k, r):
    # Initialize variables
    c = 0  # Number of cooperators in the previous round
    d = 0  # Number of defectors in the previous round
    p = 0  # Cooperation rate
    m = 0  # Consecutive rounds below threshold
    M = 3  # Maximum consecutive rounds below threshold
    L = 2  # Fixed number of rounds to defect

    for t in range(r):
        if t == 0:
            # Initial round: Cooperate
            action = C
        else:
            # Calculate cooperation rate and threshold
            p = c / n
            tau = 0.5 + (k - 1) / (2 * n)

            if p >= tau:
                action = C
                m = 0
            else:
                action = D
                m += 1

                # Gradual punishment mechanism
                if m > M:
                    for _ in range(L):
                        action = D
                    m = 0

        # Update variables
        c, d = update_cooperation_counts(action, n)

    return actions
```
**Edge Cases:**

* **First Round:** Cooperate to establish a positive reputation.
* **Last Round:** Cooperate if the cooperation rate is above the threshold; otherwise, defect. This ensures that the strategy ends on a cooperative note and maintains a good reputation.
* **Tie-breaking:** In case of a tie in the cooperation rate, cooperate.

**Collective Mindset:**
The ACGP strategy prioritizes cooperation while being adaptive to the behavior of other players. By gradually punishing persistent defectors, it encourages cooperation and promotes a collective benefit. The strategy is robust to a wide range of opponent behaviors and does not rely on specific coordination mechanisms or norms.
'''

description_COLLECTIVE_163 = '''
**Collective Strategy: Adaptive Tit-for-Tat with Threshold (ATTT)**

The ATTT strategy is designed to balance individual self-interest with collective well-being, adapting to various opponent behaviors while promoting cooperation.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally.
2. **Tit-for-Tat**: For rounds 2 to r-1:
	* If the total number of cooperators in the previous round is above a threshold (T), cooperate (C).
	* Otherwise, defect (D).

The threshold T is calculated as: `T = n * k / (n + k)`

This ensures that if enough players cooperate, the collective benefit outweighs individual self-interest.

3. **Adaptive Adjustment**: After each round, update the threshold T based on the previous round's outcomes:
	* If cooperation rate (C/n) > 0.5 and average payoff is increasing, decrease T by a small amount (`ΔT = -ε`).
	* If cooperation rate < 0.5 or average payoff is decreasing, increase T by a small amount (`ΔT = ε`).

This adaptive adjustment allows the strategy to respond to changing opponent behaviors.

**Handling Edge Cases:**

1. **Last Round**: In the final round (r), always defect (D) to maximize individual payoff.
2. **Opponent Defection**: If an opponent defects in a previous round, do not adjust T; instead, focus on maximizing individual payoff by cooperating only if the expected collective benefit is high.

**Collective Mindset:**

The ATTT strategy aligns with the collective mindset by:

1. Initially cooperating to establish trust and encourage others to cooperate.
2. Adapting to changing opponent behaviors, balancing cooperation with self-interest.
3. Gradually increasing or decreasing cooperation based on collective outcomes.

This approach fosters a cooperative environment while being robust against various opponent strategies.

**Pseudocode:**
```python
def ATTT(n, k, r):
    T = n * k / (n + k)
    epsilon = 0.01

    for t in range(r):
        if t == 0:
            action[t] = C
        else:
            cooperation_rate = sum(c_j for j in range(n)) / n
            avg_payoff = sum(pi_i for i in range(n)) / n
            if cooperation_rate > 0.5 and avg_payoff > previous_avg_payoff:
                T -= epsilon
            elif cooperation_rate < 0.5 or avg_payoff <= previous_avg_payoff:
                T += epsilon

            if cooperation_rate > T:
                action[t] = C
            else:
                action[t] = D

        # Handle last round separately
        if t == r - 1:
            action[t] = D

    return action
```
This strategy is designed to perform well in a tournament setting, where it will compete against various independent strategies.
'''

description_COLLECTIVE_164 = '''
I propose a collective strategy called "Adaptive Cooperative Optimism" (ACO) for the N-Player Public Goods Game. ACO is designed to balance individual self-interest with collective cooperation, adapting to changing circumstances and opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C). This establishes a cooperative tone and encourages others to reciprocate.
2. **Cooperation Threshold**: Set a threshold parameter τ (e.g., 0.5) that represents the minimum proportion of cooperators in the previous round required for cooperation in the current round. If the number of cooperators in the previous round is ≥ τ × n, cooperate (C).
3. **Defection Response**: If the number of cooperators in the previous round is < τ × n, defect (D) with a probability p = 1 - (number of cooperators / n). This introduces a gradual increase in defection as cooperation decreases.
4. **Payoff-Based Adaptation**: After each round, calculate the average payoff for all players (avg_payoff). If avg_payoff > k × (n / 2), adjust τ upward by a small amount (e.g., +0.05) to encourage more cooperation. Conversely, if avg_payoff < k × (n / 4), adjust τ downward by a small amount (e.g., -0.05) to accommodate more defection.

**Handling Edge Cases:**

1. **Last Round**: In the last round, cooperate (C) regardless of previous behavior, as there's no future benefit from defecting.
2. **Tie-Breaking**: If multiple players have the same number of cooperations, randomly select one to cooperate and others to defect.

**Collective Mindset:**

ACO prioritizes cooperation while being responsive to changing circumstances. By adapting τ based on average payoffs, ACO encourages collective cooperation when beneficial and allows for defection when necessary to maintain individual self-interest.

Pseudocode:
```python
# Initialize parameters
n = number of players
k = multiplication factor
r = number of rounds
tau = 0.5 (initial threshold)
avg_payoff_history = []

for t in range(r):
    # First round cooperation
    if t == 0:
        cooperate(t) = True

    # Calculate cooperation threshold
    else:
        num_cooperators_prev_round = count(C, prev_round)
        tau_adjustment = 0.05
        if avg_payoff_history[t-1] > k * (n / 2):
            tau += tau_adjustment
        elif avg_payoff_history[t-1] < k * (n / 4):
            tau -= tau_adjustment

    # Decide cooperation/defection based on threshold and payoff adaptation
    cooperate(t) = num_cooperators_prev_round >= tau * n or (random() < p)

    # Update average payoff history
    avg_payoff_history.append(avg_payoff(t))

# Last round cooperation
cooperate(r-1) = True
```
ACO balances individual self-interest with collective cooperation, adapting to changing circumstances and opponent behaviors. Its performance will be evaluated in a tournament against other AI strategies.
'''

description_COLLECTIVE_165 = '''
**Collective Strategy: Adaptive Cooperation with Reputation-based Tit-for-Tat (ACTRT)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) to encourage mutual cooperation and establish a positive reputation.
2. **Subsequent Rounds**:
	* Calculate the average cooperation rate of all players in the previous round (`avg_coop_prev`).
	* If `avg_coop_prev` ≥ 0.5, cooperate (C). This encourages continued cooperation when most players are cooperating.
	* Otherwise, use a Tit-for-Tat approach:
		+ Cooperate (C) if the majority of players (> n/2) cooperated in the previous round (`maj_coop_prev`).
		+ Defect (D) if the majority defected or there was a tie.
3. **Reputation System**:
	* Maintain a reputation score for each player, initialized to 1 (cooperative) in the first round.
	* Update the reputation score based on the previous round's actions:
		- If a player cooperated and `avg_coop_prev` ≥ 0.5, increase their reputation score by 0.1 (up to a maximum of 2).
		- If a player defected and `avg_coop_prev` < 0.5, decrease their reputation score by 0.1 (down to a minimum of 0).

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
r = number of rounds
k = multiplication factor
reputation_scores = array of n elements, initialized to 1

for round in range(r):
    # Calculate average cooperation rate from previous round
    if round > 0:
        avg_coop_prev = sum([c_i for c_i in prev_actions]) / n
    
    # Determine actions based on decision rules
    if round == 0 or avg_coop_prev >= 0.5:
        my_action = C
    else:
        maj_coop_prev = sum([1 for c_i in prev_actions if c_i == C]) > n/2
        my_action = C if maj_coop_prev else D
    
    # Update reputation scores
    for i, c_i in enumerate(prev_actions):
        if c_i == C and avg_coop_prev >= 0.5:
            reputation_scores[i] = min(reputation_scores[i] + 0.1, 2)
        elif c_i == D and avg_coop_prev < 0.5:
            reputation_scores[i] = max(reputation_scores[i] - 0.1, 0)
    
    # Store current actions for next round
    prev_actions = [my_action]
```
**Edge Cases:**

* **Last Round**: Cooperate (C) to maintain a positive reputation and encourage mutual cooperation.
* **Ties in Majority Voting**: In the event of a tie, use the Tit-for-Tat approach based on the previous round's actions.

This strategy combines elements of cooperative game theory, such as Tit-for-Tat, with a reputation system to adapt to changing opponent behaviors. By considering both individual and collective outcomes, ACTRT aims to balance self-interest with cooperation, promoting mutually beneficial payoffs in the N-Player Public Goods Game.
'''

description_COLLECTIVE_166 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Overview**

Adaptive Cooperation is a collective strategy that balances individual payoff maximization with group welfare promotion. It adapts to the game's history and parameters, making it robust against various opponent behaviors.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to signal willingness to contribute to the public good.
2. **General Rule**: In each subsequent round t > 1:
	* If the average payoff of all players in the previous round is greater than or equal to the private payoff from keeping (i.e., `average_payoff_prev_round ≥ 1`):
		+ Cooperate (C) if the number of cooperators in the previous round is greater than or equal to half of the total players (`num_cooperators_prev_round ≥ n/2`).
		+ Defect (D) otherwise.
	* If the average payoff of all players in the previous round is less than the private payoff from keeping (i.e., `average_payoff_prev_round < 1`):
		+ Cooperate (C) if the number of cooperators in the previous round is greater than a quarter of the total players (`num_cooperators_prev_round > n/4`).
		+ Defect (D) otherwise.
3. **Edge Cases**:
	* In the last round, defect (D) to maximize individual payoff.
	* If all other players defected in the previous round, defect (D) in the current round.

**Pseudocode**
```markdown
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds

# First round: Cooperate
action[1] = C

# General rule for subsequent rounds
for t = 2 to r:
  average_payoff_prev_round = calculate_average_payoff(t-1)
  num_cooperators_prev_round = count_cooperators(t-1)

  if average_payoff_prev_round >= 1:
    if num_cooperators_prev_round >= n/2:
      action[t] = C
    else:
      action[t] = D
  else:
    if num_cooperators_prev_round > n/4:
      action[t] = C
    else:
      action[t] = D

# Last round: Defect
action[r] = D

# All others defected in previous round: Defect
if all_others_defected(t-1):
  action[t] = D
```
**Rationale**

The Adaptive Cooperation strategy promotes collective welfare by:

1. Cooperating in the first round to set a positive tone.
2. Adapting to the game's history, using the average payoff and number of cooperators as indicators of group behavior.
3. Balancing individual payoffs with group welfare, defecting when cooperation is not sufficient or when the private payoff from keeping is higher.

This strategy is robust against various opponent behaviors, including:

* All-defector strategies: AC will adapt to defect in later rounds.
* All-cooperator strategies: AC will cooperate to maintain a high average payoff.
* Mixed strategies: AC's adaptive nature allows it to respond effectively to changing opponent behavior.
'''

description_COLLECTIVE_167 = '''
**Collective Strategy: Adaptive Reciprocal Altruism (ARA)**

ARA aims to balance individual self-interest with collective welfare by adapting to the group's cooperation level and reciprocating kindness.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `k/n`, where `k` is the multiplication factor and `n` is the number of players. This initial cooperation rate encourages others to cooperate while avoiding excessive generosity.
2. **Reciprocal Altruism**: After the first round, observe the total number of cooperators (`Σc_j`) in the previous round. If `Σc_j ≥ (k-1)/n`, cooperate (C) in the next round. Otherwise, defect (D). This rule promotes reciprocity by rewarding cooperation with cooperation.
3. **Adaptive Adjustment**: Every 5 rounds, reassess the group's average cooperation rate over the last 5 rounds (`avg_Σc_j`). If `avg_Σc_j ≥ k/n`, increase the initial cooperation probability in step 1 by 10%. If `avg_Σc_j < (k-1)/n`, decrease it by 10%. This adjustment mechanism allows ARA to adapt to changing group dynamics.

**Edge Cases:**

* **Last Round**: In the final round, cooperate (C) if `Σc_j ≥ (k-1)/n` in the second-to-last round. Otherwise, defect (D). This ensures a reasonable level of cooperation even in the last round.
* **All Defectors**: If all players have defected (`Σc_j = 0`) for two consecutive rounds, cooperate (C) in the next round to restart potential cooperation.

**Pseudocode:**
```python
def ARA(n, k, r):
    # Initialize variables
    total_cooperators = 0
    avg_total_cooperators = 0
    initial_cooperation_prob = k / n

    for t in range(r):
        if t == 0:
            # First round: cooperate with probability k/n
            cooperation_probability = initial_cooperation_prob
        else:
            # Reciprocal altruism
            total_cooperators_prev_round = get_total_cooperators(t-1)
            cooperation_probability = 1 if total_cooperators_prev_round >= (k-1)/n else 0

        # Make decision based on cooperation probability
        action = cooperate if random.random() < cooperation_probability else defect

        # Update variables for next round
        total_cooperators += 1 if action == cooperate else 0
        avg_total_cooperators = (avg_total_cooperators * (t-1) + total_cooperators) / t

        # Adaptive adjustment every 5 rounds
        if t % 5 == 0:
            initial_cooperation_prob *= 1.1 if avg_total_cooperators >= k/n else 0.9

    return action
```
ARA is designed to promote collective welfare by adapting to the group's cooperation level and reciprocating kindness. By balancing individual self-interest with collective goals, ARA aims to perform well in a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_168 = '''
**Collective Strategy: Adaptive Cooperate-Defect (ACD)**

The ACD strategy aims to balance individual payoffs with collective welfare by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to establish a cooperative tone and encourage others to follow.
2. **Reciprocal Altruism**: Cooperate in subsequent rounds if the total number of cooperators in the previous round is above a certain threshold (t). The threshold t is calculated as follows:

t = (n * k) / (2 * n + k)

This formula balances individual payoffs with collective benefits, ensuring that cooperation is sustainable and attractive.

3. **Adaptive Defection**: If the number of cooperators falls below the threshold t in a round, defect (play D) in the next round to avoid exploitation.
4. **Punishment Mechanism**: If a player observes an opponent defecting in a round where they themselves cooperated, they will defect in the subsequent round to 'punish' the opponent and maintain fairness.

**Edge Cases:**

* Last Round: Cooperate in the final round if the total payoff from cooperation exceeds the private payoff from defection. Otherwise, defect.
* Opponent Defection: If an opponent defects in multiple consecutive rounds, adjust the threshold t downwards by a small increment (e.g., 0.1) to account for their uncooperative behavior.

**Collective Mindset Alignment:**

The ACD strategy aligns with the collective mindset by:

* Encouraging cooperation through initial cooperation and reciprocal altruism
* Punishing exploitation through adaptive defection and punishment mechanisms
* Adapting to changing game conditions, including opponent behaviors

Pseudocode:
```python
def ACD(n, k, r, history):
    # Initialize threshold t
    t = (n * k) / (2 * n + k)

    for round in range(r):
        if round == 0:  # First round
            cooperate()
        else:
            num_cooperators_prev_round = sum(history[round-1])
            if num_cooperators_prev_round >= t:
                cooperate()
            elif num_cooperators_prev_round < t:
                defect()
            # Punishment mechanism
            for opponent in range(n):
                if history[opponent][round-1] == 0 and history[self][round-1] == 1:
                    defect()

        # Update threshold t (optional)
        if round > 0 and sum(history[round]) < t:
            t -= 0.1

    return actions
```
The ACD strategy balances individual payoffs with collective welfare, adapts to changing game conditions, and aligns with the collective mindset.
'''

description_COLLECTIVE_169 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation strategy is designed to balance individual self-interest with collective well-being, adapting to various opponent behaviors while promoting cooperation.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to encourage others to cooperate and build a positive reputation.
2. **Cooperation Threshold**: Introduce a dynamic cooperation threshold, `τ`, which represents the minimum proportion of cooperators required for an individual to cooperate. Initialize `τ` as 0.5 (a neutral starting point).
3. **Payoff-Based Adaptation**: After each round, update `τ` based on the previous round's payoffs:
	* If the average payoff of cooperators is greater than or equal to the average payoff of defectors, decrease `τ` by a small amount (`δ = 0.05`) to encourage more cooperation.
	* If the average payoff of cooperators is less than the average payoff of defectors, increase `τ` by `δ` to discourage exploitation.
4. **Cooperation Decision**: In each round, cooperate (C) if the proportion of cooperators in the previous round is greater than or equal to `τ`. Otherwise, defect (D).
5. **Punishment Mechanism**: To prevent exploitation, implement a punishment mechanism: if an opponent defects while you cooperated in the previous round, defect (D) with a probability `p` (initially set to 0.5). This probability decreases by `δ` after each round where both players cooperate.
6. **Endgame Strategy**: In the last round, defect (D) regardless of the opponent's actions, as cooperation no longer provides a benefit.

**Pseudocode**
```markdown
Initialize τ = 0.5, δ = 0.05, p = 0.5

For each round t:
  If t == 1:  # Initial Round
    Cooperate (C)
  Else:
    Calculate average payoffs of cooperators and defectors in previous round
    Update τ based on payoff-based adaptation rules
    Calculate proportion of cooperators in previous round
    If proportion >= τ:
      Cooperate (C)
    Else:
      Defect (D)

  # Punishment Mechanism
  If opponent defected while I cooperated in previous round:
    Defect (D) with probability p
    Update p = max(0, p - δ)

# Endgame Strategy
If t == r:  # Last Round
  Defect (D)
```
**Collective Mindset**

The Adaptive Cooperation strategy is designed to align with a collective mindset by:

1. Encouraging cooperation through a dynamic cooperation threshold (`τ`).
2. Punishing exploitation while allowing for forgiveness.
3. Adapting to various opponent behaviors, promoting cooperation in the presence of cooperators and defecting when faced with defectors.

By implementing this strategy, we aim to create an environment where cooperation can thrive, leading to higher collective payoffs and a more successful outcome in the tournament.
'''

description_COLLECTIVE_170 = '''
I'll design a collective strategy for this N-Player Public Goods Game that adapts to various opponent behaviors and maximizes overall payoffs while promoting cooperation.

**Strategy Name: Adaptive Cooperative Escalation (ACE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability 0.5. This establishes a baseline level of cooperation and encourages others to reciprocate.
2. **Monitor and Adapt**: Observe the actions of all players in each round. Calculate the average number of cooperators (C_avg) over the last few rounds (e.g., 3-5 rounds). If C_avg is below a certain threshold (e.g., 0.4), escalate cooperation by increasing the probability of playing C.
3. **Escalation Mechanism**: When escalating, increase the probability of cooperating by 10% each round until a target level of cooperation (e.g., 60%) is reached or a maximum escalation limit (e.g., 80%) is hit. This mechanism promotes gradual and controlled increases in cooperation.
4. **Punishment for Defection**: If a player defects while the average number of cooperators is above the threshold, reduce the probability of cooperating with that specific player by 20% for the next few rounds (e.g., 2-3 rounds). This discourages exploitation and maintains cooperation.
5. **Forgiveness and Reset**: After a punishment period, reset the probability of cooperating with the punished player to the average level of cooperation observed in the previous rounds.

**Handling Edge Cases:**

1. **First Round**: Cooperate with probability 0.5 (as mentioned earlier).
2. **Last Round**: If it's the final round, defect (play D) to maximize individual payoff, as there are no future rounds to consider.
3. **Ties and Uncertainty**: In cases of uncertainty or ties in C_avg calculations, favor cooperation by playing C with a slightly higher probability.

**Collective Mindset:**

ACE aligns with the collective mindset by:

1. Encouraging initial cooperation to establish a baseline level of trust.
2. Adapting to the actions of others to promote overall cooperation and prevent exploitation.
3. Gradually escalating cooperation when necessary, rather than suddenly switching strategies.
4. Punishing defectors while allowing for forgiveness and resetting relationships.

**Pseudocode:**
```python
def ACE_strategy(game_params, history):
    # Initialize variables
    C_avg_threshold = 0.4
    escalation_limit = 0.8
    punishment_duration = 2

    # First round
    if game_params['round'] == 1:
        cooperate_prob = 0.5
    else:
        # Calculate average number of cooperators (C_avg)
        C_avg = calculate_C_avg(history)

        # Escalate cooperation if necessary
        if C_avg < C_avg_threshold:
            escalate_cooperation(C_avg, escalation_limit)

        # Punish defectors and forgive later
        punish_defectors(history, punishment_duration)

    return cooperate_prob

def calculate_C_avg(history):
    # Calculate average number of cooperators over last few rounds
    ...

def escalate_cooperation(C_avg, escalation_limit):
    # Increase cooperation probability by 10% each round until target level is reached or limit is hit
    ...

def punish_defectors(history, punishment_duration):
    # Reduce cooperation probability with defectors for a specified duration
    ...
```
ACE balances individual self-interest with collective well-being, promoting a robust and adaptive strategy that can thrive in a wide range of opponent behaviors.
'''

description_COLLECTIVE_171 = '''
**Collective Strategy: Adaptive Cooperativity with Social Learning**

Our collective strategy, dubbed "ACS" (Adaptive Cooperativity with Social Learning), aims to balance individual self-interest with collective well-being by adapting to the game's history and social environment.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) unconditionally to establish a cooperative tone.
2. **Observation Phase**: For rounds 2-5, observe the number of cooperators in the previous round (`num_coop_prev`) and calculate the average payoff (`avg_payoff_prev`) among all players. This phase helps us understand the game's social dynamics.
3. **Adaptive Cooperativity**: From round 6 onwards:
	* If `num_coop_prev` ≥ (n/2), cooperate (C). This indicates a strong cooperative atmosphere, and we want to reinforce it.
	* If `avg_payoff_prev` > (k/n) \* (n/2), cooperate (C). This suggests that cooperation has led to higher payoffs in the past, so we should continue cooperating.
	* Otherwise, defect (D).
4. **Social Learning**: After each round, update our internal estimate of the average cooperativity (`avg_coop`) among all players:
	+ If a player defects (D), decrease `avg_coop` by 0.1.
	+ If a player cooperates (C) and receives a payoff greater than their previous average payoff, increase `avg_coop` by 0.1.
5. **Adaptive Threshold**: Adjust the threshold for cooperation (`coop_threshold`) based on the game's history:
	+ If `avg_coop` increases over three consecutive rounds, decrease `coop_threshold` by 0.05.
	+ If `avg_coop` decreases over three consecutive rounds, increase `coop_threshold` by 0.05.

**Handling Edge Cases:**

* **Last Round**: In the final round, cooperate (C) unconditionally to maximize collective payoff, as there's no risk of exploitation.
* **Ties in Cooperativity**: If multiple players have the same estimated cooperativity (`avg_coop`), prioritize cooperation among those with higher average payoffs.

**Collective Mindset:**

ACS aligns with a collective mindset by:

1. Encouraging cooperation when it benefits the group as a whole (e.g., when `num_coop_prev` ≥ (n/2)).
2. Adapting to the social environment and learning from others' actions.
3. Balancing individual self-interest with collective well-being through adaptive decision-making.

**Pseudocode:**
```python
def ACS(n, k, r):
    # Initialize variables
    avg_coop = 0.5  # initial estimate of average cooperativity
    coop_threshold = n / 2  # initial cooperation threshold

    for t in range(1, r + 1):
        if t == 1:  # first round
            action = 'C'
        elif t <= 5:  # observation phase
            num_coop_prev = count_cooperators(prev_round)
            avg_payoff_prev = calculate_avg_payoff(prev_round)
            # observe and update internal estimates
        else:
            if num_coop_prev >= coop_threshold or avg_payoff_prev > (k / n) * (n / 2):
                action = 'C'
            else:
                action = 'D'

            # social learning and adaptive threshold updates
            if action == 'D':
                avg_coop -= 0.1
            elif calculate_payoff(t) > calculate_avg_payoff(prev_round):
                avg_coop += 0.1

            if avg_coop increases over three consecutive rounds:
                coop_threshold -= 0.05
            elif avg_coop decreases over three consecutive rounds:
                coop_threshold += 0.05

        # take action and update history
        prev_round = t
        actions.append(action)

    return actions
```
Note that this pseudocode is a simplified representation of the strategy, and actual implementation may vary depending on the programming language and specific requirements.
'''

description_COLLECTIVE_172 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

ACE is a robust and adaptive collective strategy that balances individual self-interest with collective well-being. It dynamically adjusts cooperation levels based on the game's history, exploiting opportunities for mutual benefit while minimizing exploitation.

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a cooperative tone and encourage others to reciprocate.
2. **Subsequent Rounds:**
	* If the total payoff of cooperators in the previous round is greater than or equal to the total payoff of defectors, cooperate (C).
	* Otherwise, defect (D) for one round to signal dissatisfaction with the current state of cooperation.
	* After a single round of defection, return to cooperating (C) if the total payoff of cooperators in the previous round is greater than or equal to the total payoff of defectors. This helps to reestablish cooperation and encourages others to follow suit.
3. **Escalation Mechanism:** If the proportion of cooperators in the previous round falls below a threshold (e.g., 1/3), increase the frequency of defection by introducing a " tit-for-tat" component:
	* Defect (D) for two consecutive rounds if the opponent defected in the previous round.
	* Cooperate (C) for one round after two consecutive rounds of cooperation.

**Edge Cases:**

* **Last Round:** Cooperate (C) to maximize collective payoff and maintain a positive reputation.
* **Multiple Opponents with Different Strategies:** Focus on the overall proportion of cooperators and adjust the strategy accordingly. If multiple opponents employ different strategies, prioritize cooperation if the majority are cooperative, and defect otherwise.

**Collective Mindset:**

ACE prioritizes cooperation as the default behavior but remains adaptable to changing circumstances. By escalating defection in response to declining cooperation levels, ACE aims to maintain a balance between individual self-interest and collective well-being. This approach encourages opponents to cooperate while minimizing exploitation.

**Pseudocode (simplified):**
```
def ACE(n, k, r):
  # Initialize variables
  total_cooperators = 0
  total_defectors = 0
  previous_round_payoff_c = 0
  previous_round_payoff_d = 0

  for round in range(r):
    if round == 0:
      # First round: Cooperate
      action = 'C'
    else:
      # Update totals and payoffs from previous round
      total_cooperators, total_defectors, previous_round_payoff_c, previous_round_payoff_d = update TotalsAndPayoffs()

      if previous_round_payoff_c >= previous_round_payoff_d:
        # Cooperate if cooperators' payoff is higher or equal
        action = 'C'
      else:
        # Defect otherwise
        action = 'D'

      # Escalation mechanism
      if total_cooperators / n < 1/3:
        # Increase defection frequency
        action = titForTat(action, previous_round_payoff_d)

    # Update own payoff and totals
    updateOwnPayoffAndTotals(action)

    # Return action for this round
    return action

def titForTat(action, previous_round_payoff_d):
  if action == 'D' and previous_round_payoff_d > 0:
    # Defect for two consecutive rounds after opponent defected
    return 'D'
  elif action == 'C' and previous_round_payoff_d <= 0:
    # Cooperate after two consecutive rounds of cooperation
    return 'C'
  else:
    # Default to original action
    return action

def updateTotalsAndPayoffs():
  # Update totals and payoffs from previous round ( implementation omitted for brevity)
  pass

def updateOwnPayoffAndTotals(action):
  # Update own payoff and totals based on chosen action (implementation omitted for brevity)
  pass
```
This strategy should be implemented as an algorithm to compete in the tournament against other independent strategies.
'''

description_COLLECTIVE_173 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name: Adaptive Collective Optimism (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = 0.8. This initial optimism encourages cooperation and sets a positive tone for the game.
2. **History-Based Adaptation**: For rounds t > 1, calculate the average payoff per cooperator in the previous round:
	* `avg_payoff_coop[t-1] = (Σ(j=1 to n) c_j * π_j[t-1]) / Σ(j=1 to n) c_j`
	* If `avg_payoff_coop[t-1] > 1`, cooperate (C) with probability pcoop = 0.9. This indicates that cooperation was beneficial in the previous round.
	* Otherwise, defect (D) with probability pdefect = 0.6. This reduces the likelihood of cooperating when it was not beneficial previously.
3. **Exploration and Exploitation**: Introduce randomness to balance exploration and exploitation:
	* With probability `ε = 0.1`, choose an action randomly (C or D). This allows for occasional deviations from the history-based adaptation.

**Handling Edge Cases:**

1. **Last Round**: In the final round, cooperate (C) with probability p_last = 0.5. This balances the desire to maximize payoff with the need to maintain cooperation.
2. **Tiebreakers**: If multiple players have the same average payoff per cooperator in a round, break ties by cooperating (C) with probability `p_tiebreaker = 0.7`.

**Collective Mindset:**

ACO prioritizes collective welfare while adapting to individual payoffs and game history. By initially cooperating and then adjusting based on the average payoff per cooperator, ACO promotes cooperation when beneficial and defects otherwise.

Pseudocode:
```python
def Adaptive_Collective_Optimism(n, k, r):
    p_init = 0.8
    epsilon = 0.1
    
    # Initialize variables to track history
    avg_payoff_coop = [None] * r
    actions = [None] * n
    
    for t in range(1, r + 1):
        if t == 1:
            cooperate_prob = p_init
        else:
            avg_payoff_coop[t - 1] = calculate_avg_payoff_coop(avg_payoff_coop[t - 2], actions)
            if avg_payoff_coop[t - 1] > 1:
                cooperate_prob = 0.9
            else:
                cooperate_prob = 0.4
        
        # Exploration and exploitation
        if random.random() < epsilon:
            action = random.choice([C, D])
        else:
            action = C if random.random() < cooperate_prob else D
        
        actions[t - 1] = action
    
    return actions

def calculate_avg_payoff_coop(avg_payoff_coop_prev, actions):
    # Calculate average payoff per cooperator in the previous round
    total_cooperators = sum(1 for a in actions if a == C)
    total_payoff_coop = sum(π_j[t - 1] for j, a in enumerate(actions) if a == C)
    
    return total_payoff_coop / total_cooperators
```
This strategy will be implemented as an algorithm and compete against other AI systems in the tournament.
'''

description_COLLECTIVE_174 = '''
I propose a collective strategy called "Adaptive Cooperation" (AC). AC aims to balance individual payoff maximization with collective welfare by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally to establish a cooperative tone.
2. **Reciprocity**: If at least one opponent cooperated in the previous round, cooperate (C) in the current round. This encourages reciprocity and cooperation from others.
3. **Punishment**: If all opponents defected (D) in the previous round, defect (D) in the current round to punish uncooperative behavior.
4. **Adaptive Threshold**: Introduce an adaptive threshold, τ, which tracks the proportion of cooperative actions in the game's history. Initialize τ = 0.5.

In each round, calculate the number of cooperators (C_count) and defectors (D_count). Update τ as follows:

τ_new = (C_count / n) + (τ_old \* (1 - 1/r))

If τ > 0.5, cooperate (C); otherwise, defect (D).

**Edge Cases:**

1. **Last Round**: In the final round, defect (D) to maximize individual payoff.
2. **Tiebreaker**: If τ equals exactly 0.5, cooperate (C) to maintain a cooperative bias.

**Collective Mindset:**

AC prioritizes cooperation while adapting to the game's dynamics. By cooperating initially and reciprocating cooperation from others, AC encourages collective welfare. The punishment mechanism deters uncooperative behavior, while the adaptive threshold allows AC to adjust its strategy based on the game's history.

**Pseudocode:**
```python
def AdaptiveCooperation(n, k, r):
    # Initialize variables
    τ = 0.5
    C_count = 0
    D_count = 0
    
    for round in range(r):
        if round == 0:
            action = 'C'  # Initial cooperation
        
        elif all_opponents_defected(round - 1):
            action = 'D'  # Punishment
        
        elif at_least_one_cooperated(round - 1):
            action = 'C'  # Reciprocity
        
        else:
            τ_new = (C_count / n) + (τ * (1 - 1/r))
            if τ_new > 0.5:
                action = 'C'
            else:
                action = 'D'
        
        # Update counts
        C_count += (action == 'C')
        D_count += (action == 'D')
        
        # Last round
        if round == r - 1:
            action = 'D'
    
    return action
```
AC is designed to be robust against a wide range of opponent behaviors, as it adapts to the game's history and parameters. By balancing individual payoff maximization with collective welfare, AC aims to achieve a high total payoff in the tournament.
'''

description_COLLECTIVE_175 = '''
**Collective Strategy: "Adaptive Cooperation with Gradual Forgiveness"**

This strategy aims to balance individual payoff maximization with collective cooperation, adapting to opponents' behaviors while maintaining a robust and forgiving approach.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage initial cooperation and set a positive tone.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is greater than or equal to half of the players (Σc_j ≥ n/2), cooperate (C).
	* Otherwise, defect (D) but keep track of the "cooperation debt" (explained below).
3. **Cooperation Debt:** Introduce a cooperation debt counter (CD) initialized at 0. When a player defects while there are fewer than half cooperators, increment CD by 1.
4. **Gradual Forgiveness:** If a player has a positive cooperation debt (CD > 0), gradually forgive the opponents' past defections by decrementing CD by 1 whenever they cooperate in subsequent rounds.

**Edge Cases:**

* **Last Round (t=r):** Defect (D) to maximize individual payoff, as there are no future interactions.
* **Tie-Breaking:** In case of a tie in the number of cooperators, prioritize cooperation if the opponent has a higher cooperation debt; otherwise, defect.

Pseudocode:
```markdown
# Initialize variables
n = number_of_players
k = multiplication_factor
r = number_of_rounds
CD = 0  # Cooperation Debt

for t in range(1, r+1):
    if t == 1:  # Initial Round
        action = C
    else:
        previous_cooperators = sum(c_j for j in range(n))
        if previous_cooperators >= n/2:
            action = C
        else:
            CD += 1
            action = D

        if CD > 0 and opponents_cooperated_last_round():
            CD -= 1

    # Take action and update payoff
    π_i,t = calculate_payoff(action, previous_actions)

# Final Round (t=r)
action = D
```
This strategy encourages initial cooperation, adapts to the number of cooperators in subsequent rounds, and gradually forgives opponents' past defections. By prioritizing collective cooperation while being mindful of individual payoffs, this approach aims to achieve a robust balance between self-interest and group well-being.
'''

description_COLLECTIVE_176 = '''
To design a collective strategy for this N-Player Public Goods Game that is adaptive and robust to various opponent behaviors, we'll focus on creating a decision-making process that balances individual self-interest with the collective good, adapting based on observed game history.

**Strategy Name: Adaptive Collective Optimism (ACO)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish an initial cooperative tone and encourage others to follow suit.
2. **Response to Cooperation:** If a majority (> n/2) of players cooperated in the previous round, cooperate (C). This reinforces successful collective action.
3. **Response to Defection:** If a majority (> n/2) of players defected in the previous round, defect (D) for one round. Then, revert to cooperating if there's an improvement in the number of cooperators or continue defecting if not. This tempers initial optimism with caution against exploitation.
4. **Minority Cooperation:** If a minority (< n/2) of players cooperated but at least one player cooperated, assess the total payoff from cooperation and compare it to potential gains from defection in the next round. Cooperate again if the collective benefit is high enough (payoff ≥ k/n * previous round's cooperators).
5. **Punishment Mechanism:** If you cooperated but were exploited by more than half of the other players defecting, switch to defection for a set number of rounds (e.g., 2-3 rounds) before reassessing cooperation based on the updated game state.

**Edge Cases:**

- **Last Round:** Cooperate if there have been at least some cooperative rounds; otherwise, defect. This approach maximizes potential gains from any remaining collective spirit.
- **Repeated Defection:** If faced with a history of majority defection (more than 75% of players), permanently switch to defection. The game state has demonstrated that cooperation is not viable.

**Collective Mindset:**

ACO aims to promote collective welfare by initially trusting in the willingness of others to cooperate and adapting based on observed behavior. By gradually adjusting its strategy, ACO encourages cooperative play while protecting against exploitation, making it a robust choice for diverse environments.

**Pseudocode Example:**
```python
def ACOStrategy(n, k, history):
    # Initial Round or No History
    if len(history) == 0:
        return "C"
    
    lastRound = history[-1]
    numCoopLast = sum(1 for action in lastRound if action == "C")
    
    if numCoopLast > n / 2:  # Majority Cooperated
        return "C"
    elif numCoopLast <= n / 2 and any(action == "C" for action in lastRound):  # Minority Cooperated
        coopPayoff = k/n * numCoopLast
        if coopPayoff >= k/n * previous_round_cooperators:
            return "C"
        else:
            return "D"
    elif more_than_half_defected and not recently_punished:
        return "D"  # Temporarily defect as punishment
    else:  # Otherwise, or after temporary defection period
        if any_recent_cooperation_improvement:
            return "C"
        else:
            return "D"

# Main Loop
for round in range(r):
    actions = [ACOStrategy(n, k, game_history) for _ in players]
    payoffs = calculatePayoffs(actions)
    updateGameHistory(actions)
```
This strategy is designed to be adaptive and robust against a wide range of opponent behaviors, balancing collective optimism with individual caution.
'''

description_COLLECTIVE_177 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

ACE is a dynamic strategy that balances individual self-interest with collective well-being by adaptively adjusting cooperation levels based on game history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to establish a cooperative tone and encourage others to follow suit.
2. **Escalation Mechanism**: For rounds t > 1:
	* If the average payoff in the previous round is below the threshold `k/n` (i.e., the public good's multiplication factor), escalate cooperation by increasing the likelihood of cooperating.
	* Otherwise, maintain or decrease cooperation levels.
3. **Adaptive Cooperation Threshold**: Define a cooperation threshold `p_t` for each round t as:

```
p_t = max(0, min(1, (average_payoff_prev_round / (k/n)) * (t / r)))
```

where `average_payoff_prev_round` is the average payoff of all players in the previous round.

4. **Cooperation Decision**: In each round t > 1:
	* Cooperate (C) with probability `p_t`.
	* Defect (D) with probability `(1 - p_t)`.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate (C) to maximize collective payoff and encourage others to do the same.
2. **Tiebreaker**: If multiple players have the same highest average payoff in a round, break ties by cooperating with a slightly higher probability (`p_t + ε`, where `ε` is a small positive value).

**Collective Mindset Alignment:**

ACE prioritizes collective well-being by:

1. Encouraging initial cooperation to establish a cooperative tone.
2. Escalating cooperation when the public good's multiplication factor is not met, promoting collective investment in the community project.
3. Adapting cooperation levels based on game history, responding to changes in average payoffs and encouraging others to cooperate.

By implementing ACE, we create a robust and adaptive strategy that balances individual self-interest with collective well-being, promoting a cooperative environment without relying on coordination mechanisms or shared norms.
'''

description_COLLECTIVE_178 = '''
**Collective Strategy: Adaptive Cooperativity (AC)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 0.5. This initial cooperation encourages others to reciprocate and establishes a baseline for future interactions.
2. **Reciprocity**: If the number of cooperators in the previous round is above a threshold (`T`), cooperate in the current round. The threshold `T` is calculated as `T = (n / 2) * k`, where `n` is the number of players and `k` is the multiplication factor.
3. **Punishment**: If the number of cooperators in the previous round is below the threshold (`T`), defect (D) with a probability of 0.5. This punishes non-cooperative behavior while still allowing for potential cooperation.
4. **Learning and Adaptation**: Update the threshold `T` after each round based on the observed behavior of other players. If more than half of the players cooperated in the previous round, increase `T` by 1; otherwise, decrease `T` by 1.

**Edge Cases:**

* In the last round (`r = R`), always cooperate (C). This maximizes collective payoff as there is no future round to consider.
* If all players defected in the previous round, cooperate with a probability of 0.25 in the current round. This provides an opportunity for cooperation to re-emerge.

**Collective Mindset:**

The Adaptive Cooperativity strategy prioritizes collective welfare while adapting to the behavior of other players. By initially cooperating and then reciprocating based on observed behavior, AC encourages cooperation and punishes non-cooperative actions. The learning mechanism allows the strategy to adjust its threshold for cooperation in response to changing circumstances.

**Pseudocode:**
```python
def Adaptive_Cooperativity(n, k, r):
  T = (n / 2) * k  # Initial threshold

  for t in range(r):
    if t == 0:
      cooperate_prob = 0.5  # Initial cooperation probability
    else:
      num_cooperators_prev = count_cooperators(t - 1)
      if num_cooperators_prev >= T:
        cooperate_prob = 1  # Reciprocate cooperation
      elif num_cooperators_prev < T and random.random() < 0.5:
        cooperate_prob = 0  # Punish non-cooperation with probability 0.5

    action = 'C' if random.random() < cooperate_prob else 'D'

    if t == r - 1:  # Last round
      action = 'C'

    # Update threshold T based on observed behavior
    if num_cooperators_prev > n / 2:
      T += 1
    else:
      T -= 1

    return action
```
This collective strategy balances individual self-interest with the need for cooperation to achieve a mutually beneficial outcome. Its adaptability and responsiveness to changing circumstances make it robust against various opponent behaviors in the tournament setting.
'''

description_COLLECTIVE_179 = '''
To design a collective strategy for this N-Player Public Goods Game that is adaptive and robust to various opponent behaviors, we'll focus on creating a decision-making process based on game parameters and history. This approach aims to balance individual payoff optimization with promoting cooperation.

### Strategy Name: Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to set a positive tone and encourage initial cooperation.
   
2. **Subsequent Rounds:** The decision for each subsequent round is based on the ratio of total contributions (cooperations) observed in the previous round, reflecting an assessment of the group's overall willingness to cooperate.

   - Calculate `contribution_ratio` = `(total_cooperators_in_previous_round / n)`
   - If `contribution_ratio >= 0.5`, Cooperate (C). This indicates a strong enough signal of cooperation in the group.
   
   - Otherwise, Defect (D) with a certain probability (`p_defect`) that is inversely proportional to `contribution_ratio`. The specific formula could be:
     ```python
     p_defect = max(0.5, 1 - contribution_ratio)
     ```
     This means the lower the cooperation ratio in the previous round, the higher the likelihood of defecting.

3. **Last Round:** In the last round (`r`), if the total payoff from cooperating up to that point is significantly higher than the potential gain from defecting (considering the `k/n` factor and assuming all others might cooperate or not based on past behavior), Cooperate (C). Otherwise, Defect (D).

   - **Threshold Condition:** Define a threshold (`coop_threshold`) as a function of game parameters (`n`, `r`, `k`). For example:
     ```python
     coop_threshold = k / n * r * 0.6
     ```
     If the total cooperative payoff minus the potential gain from defecting is greater than this threshold, Cooperate in the last round.

**Handling Edge Cases:**

- **First Round:** Always Cooperate.
- **Last Round:** Follow the special rule defined above to balance short-term gains with promoting cooperation.
- **Tie or Near-Tie Situations:** In cases where contribution ratios are very close to thresholds, lean towards Cooperation if there's a slight majority or near-majority cooperating.

**Collective Mindset:**

ACC aims for a balanced approach that encourages collective cooperation while adapting to the dynamics of the group. By starting with cooperation and adjusting based on observed behaviors, ACC promotes an environment conducive to mutual benefit. It does not assume coordination but instead relies on adaptive behavior to foster cooperation among players.

**Pseudocode Example:**

```python
def AdaptiveCollectiveCooperation(n, r, k, current_round, previous_cooperations):
    if current_round == 1:
        return "C"  # Cooperate in the first round
    
    contribution_ratio = previous_cooperations / n
    p_defect = max(0.5, 1 - contribution_ratio)
    
    if random.random() < p_defect:
        return "D"
    else:
        return "C"

# For last round handling (example logic)
def LastRoundLogic(total_cooperative_payoff, potential_gain_from_defecting, coop_threshold):
    if total_cooperative_payoff - potential_gain_from_defecting > coop_threshold:
        return "C"
    else:
        return "D"
```

This strategy is designed to be adaptive and collective-oriented while ensuring robustness against a variety of opponent behaviors in the N-Player Public Goods Game.
'''

description_COLLECTIVE_180 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Rounds (t < n):**
Cooperate (C) in the first round to encourage others to cooperate.
In subsequent rounds, observe the total number of cooperators in the previous round (T_prev).
If T_prev ≥ n/2, continue cooperating (C). Otherwise, defect (D).

2. **Middle Rounds (n ≤ t < r - 1):**
Observe the average payoff of all players in the previous round (P_avg_prev) and the total number of cooperators in the previous round (T_prev).
If P_avg_prev ≥ k/2 and T_prev ≥ n/2, cooperate (C). Otherwise, defect (D).

3. **Last Round (t = r):**
Observe the average payoff of all players in the second-to-last round (P_avg_prev) and the total number of cooperators in the second-to-last round (T_prev).
If P_avg_prev ≥ k/2 and T_prev ≥ n/2, cooperate (C). Otherwise, defect (D).

4. **Edge Cases:**
- If all players defected in the previous round (T_prev = 0), cooperate (C) to try to revive cooperation.
- If the game is in a state of perfect cooperation (all players cooperated in the previous round), continue cooperating (C).

**Pseudocode:**

```
function ACC(n, k, r):
  for t = 1 to r:
    if t < n:
      if t == 1:
        action = C
      else:
        T_prev = count_cooperators(t-1)
        if T_prev >= n/2:
          action = C
        else:
          action = D
    elif n <= t < r - 1:
      P_avg_prev = average_payoff(t-1)
      T_prev = count_cooperators(t-1)
      if P_avg_prev >= k/2 and T_prev >= n/2:
        action = C
      else:
        action = D
    else: # last round
      P_avg_prev = average_payoff(t-2)
      T_prev = count_cooperators(t-2)
      if P_avg_prev >= k/2 and T_prev >= n/2:
        action = C
      else:
        action = D

    if all_defected(t-1):
      action = C
    elif perfect_cooperation(t-1):
      action = C

    return action
```

**Collective Mindset:**
The ACC strategy aims to promote collective cooperation by initially cooperating, then adapting to the group's behavior. By observing the total number of cooperators and average payoffs, the strategy encourages others to cooperate while also being cautious not to be exploited by defectors.

This strategy should perform well in a tournament setting, as it is robust to various opponent behaviors and does not rely on coordination mechanisms or shared norms.
'''

description_COLLECTIVE_181 = '''
I propose a collective strategy called "Adaptive Collective Cooperation" (ACC). ACC balances individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: In the first round, play C (Cooperate) to establish a cooperative tone.
2. **Subsequent Rounds**:
	* If the total payoff from the previous round is greater than or equal to the average expected payoff ((k-1)/n + 1), play C.
	* Otherwise, play D (Defect).
3. **Exception Handling**: If all players defected in the previous round, play C to attempt to restart cooperation.

**Pseudocode:**
```python
def ACC(n, k, r, history):
    if len(history) == 0:  # Initial Round
        return 'C'
    
    prev_round_payoff = sum([payoff for payoff in history[-1]])
    avg_expected_payoff = (k-1)/n + 1
    
    if prev_round_payoff >= avg_expected_payoff:
        return 'C'
    elif all([action == 'D' for action in history[-1]]):  # All defected
        return 'C'
    else:
        return 'D'
```
**Rationale:**

* By cooperating in the first round, ACC sets a positive tone and encourages others to cooperate.
* In subsequent rounds, ACC adapts to the game's history by playing C if the total payoff from the previous round is satisfactory. This ensures that cooperation is maintained when it benefits the collective.
* If all players defected in the previous round, ACC attempts to restart cooperation by playing C. This prevents the game from getting stuck in a defect-defect equilibrium.

**Collective Mindset:**

ACC prioritizes the collective well-being while also considering individual self-interest. By adapting to the game's history and parameters, ACC promotes cooperation when it is mutually beneficial, thereby aligning with the collective mindset.

**Robustness:**

ACC is robust to various opponent behaviors because it:

* Adapts to changing circumstances (e.g., switching from C to D or vice versa based on the previous round's outcome).
* Does not rely on specific coordination mechanisms or norms.
* Is resilient to deviations from cooperation, as it will attempt to restart cooperation when necessary.

ACC is a well-rounded strategy that balances individual and collective interests, making it a strong contender in the tournament.
'''

description_COLLECTIVE_182 = '''
Here's a collective strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.8 and defect (D) with probability 0.2. This encourages some initial cooperation while allowing for exploration.
2. **Adaptive Cooperation**: For rounds t > 1, calculate the average payoff per cooperator in the previous round:

   `avg_payoff_coop[t-1] = Σ(i=1 to n) (π_i,t-1 \* c_i,t-1) / Σ(i=1 to n) c_i,t-1`

   If `avg_payoff_coop[t-1] > 1.5`, cooperate (C). Otherwise, defect (D).

   This rule adapts to the group's overall cooperation level and rewards cooperation when it yields higher payoffs.

3. **Punishment Mechanism**: If a player observes that fewer than half of the players cooperated in the previous round (`Σ(i=1 to n) c_i,t-1 < n/2`), defect (D) with probability 0.7 and cooperate (C) with probability 0.3.

   This rule introduces a mild punishment mechanism for when cooperation levels are too low, encouraging players to contribute to the public good.

**Edge Cases:**

* **Last Round**: In the final round (t = r), always defect (D). Since there's no future game to affect, maximize individual payoff.
* **Early Defection**: If a player observes that more than half of the players defected in the previous round (`Σ(i=1 to n) d_i,t-1 > n/2`), defect (D) with probability 0.9 and cooperate (C) with probability 0.1.

   This rule helps to mitigate early collapse of cooperation by reducing the incentive for individual defection when many others are already defecting.

**Collective Mindset:**

* **Emphasis on Cooperation**: ACO prioritizes cooperation as a default strategy, encouraging players to contribute to the public good.
* **Adaptive and Responsive**: By adapting to the group's overall cooperation level and introducing punishment mechanisms, ACO promotes robust cooperation in response to varying opponent behaviors.

**Pseudocode:**

```
function AdaptiveCollectiveOptimism(n, r, k):
  // Initial Cooperation
  if t == 1:
    cooperate with probability 0.8, defect with probability 0.2

  // Adaptive Cooperation
  else:
    avg_payoff_coop[t-1] = calculate_average_payoff_per_cooperator(t-1)
    if avg_payoff_coop[t-1] > 1.5:
      cooperate (C)
    else:
      defect (D)

  // Punishment Mechanism
  if Σ(i=1 to n) c_i,t-1 < n/2:
    defect with probability 0.7, cooperate with probability 0.3

  // Edge Cases
  if t == r:  // Last Round
    defect (D)
  elif Σ(i=1 to n) d_i,t-1 > n/2:  // Early Defection
    defect with probability 0.9, cooperate with probability 0.1
```

ACO is designed to promote cooperation and adapt to various opponent behaviors while maintaining a robust collective strategy that aligns with the game's parameters and history.
'''

description_COLLECTIVE_183 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Retaliation (ACGR)**

**Overview**

Our collective strategy, ACGR, aims to balance cooperation and defection while adapting to the opponent's behavior. We prioritize cooperation in early rounds, gradually retaliating against defectors while maintaining a willingness to forgive and cooperate again.

**Decision Rules**

1. **First Round**: Cooperate (C) unconditionally to establish a cooperative tone.
2. **Early Rounds (t < r/2)**: Cooperate if the opponent's cooperation rate is above 50% in the previous round; otherwise, Defect (D).
3. **Mid-Game (r/2 ≤ t < 3r/4)**:
	* If the opponent's cooperation rate is above 50%, cooperate.
	* If the opponent defects, retaliate by defecting for one round; then, cooperate if the opponent cooperates again.
4. **Late Rounds (t ≥ 3r/4)**: Cooperate only if the opponent has a high overall cooperation rate (> 2/3) and defected in fewer than 20% of rounds.

**Edge Cases**

* **Last Round**: Defect to maximize individual payoff, as there is no future interaction.
* **Opponent's Cooperation Rate is Exactly 50%**: Cooperate to maintain a cooperative stance.

**Pseudocode**
```python
def ACGR(n, k, r, history):
    # Initialize cooperation rate and retaliation flag
    coop_rate = 0.5
    retaliate = False

    for t in range(r):
        if t == 0:  # First Round
            return C
        elif t < r/2:  # Early Rounds
            opponent_coop_rate = history[t-1].count(C) / n
            if opponent_coop_rate > coop_rate:
                return C
            else:
                return D
        elif r/2 <= t < 3r/4:  # Mid-Game
            opponent_coop_rate = history[t-1].count(C) / n
            if retaliate:
                return D
            elif opponent_coop_rate > coop_rate:
                return C
            else:
                retaliate = True
                return D
        else:  # Late Rounds
            overall_coop_rate = sum(history[:t]).count(C) / (n * t)
            defect_count = history[:t].count(D)
            if overall_coop_rate > 2/3 and defect_count < 0.2 * n:
                return C
            else:
                return D

    # Last Round
    return D
```
**Collective Mindset**

ACGR is designed to promote cooperation while being robust to various opponent behaviors. By cooperating in early rounds, we set a positive tone and encourage others to cooperate. As the game progresses, we gradually retaliate against defectors, demonstrating our commitment to mutual cooperation. In late rounds, we prioritize cooperation with opponents who have demonstrated a willingness to cooperate throughout the game.

By playing ACGR, we aim to create an environment where cooperation is encouraged and rewarded, while minimizing the risk of exploitation by defectors.
'''

description_COLLECTIVE_184 = '''
**Collective Strategy: Adaptive Cooperation with Self-Adjusting Threshold (AC-SAT)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative baseline.
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust the cooperation threshold accordingly.

Define a cooperation threshold, `θ`, which represents the minimum proportion of cooperators required for our strategy to cooperate in the next round. Initialize `θ` to 0.5 (a neutral value).

Pseudocode:
```python
def AC_SAT(n, k, r):
    θ = 0.5  # initial cooperation threshold

    for t in range(1, r+1):  # loop through rounds
        if t == 1:  # first round
            action = 'C'  # cooperate
        else:
            prev_round_cooperators = count_cooperators(prev_round_actions)
            θ_adj = adjust_threshold(θ, prev_round_cooperators, n)
            if prev_round_cooperators / n >= θ_adj:
                action = 'C'  # cooperate if threshold met
            else:
                action = 'D'  # defect otherwise

        return action
```
**Adjusting the Cooperation Threshold (θ):**

After each round, update `θ` based on the observed number of cooperators. If more players cooperated than expected, decrease `θ` to encourage cooperation; if fewer cooperated, increase `θ` to deter free-riding.

Pseudocode:
```python
def adjust_threshold(θ, prev_round_cooperators, n):
    Δ = 0.1  # adjustment step size
    if prev_round_cooperators / n > θ + Δ:
        θ -= Δ
    elif prev_round_cooperators / n < θ - Δ:
        θ += Δ
    return max(0, min(θ, 1))  # clamp θ within bounds [0, 1]
```
**Edge Cases:**

* **Last Round:** In the final round, cooperate (C) regardless of the cooperation threshold. This ensures that our strategy does not defect solely due to the game's end.
* **Ties:** In cases where multiple strategies are tied for the highest payoff, choose the one with the higher cooperation rate.

**Collective Mindset:**

AC-SAT prioritizes collective cooperation while adapting to changing circumstances. By adjusting the cooperation threshold based on observed behavior, our strategy promotes a self-sustaining cooperative environment.

In the tournament setting, AC-SAT will engage in a gradual process of learning and adaptation, converging towards an equilibrium where cooperation becomes the dominant strategy. This approach allows our strategy to be robust against various opponent behaviors while remaining aligned with the collective mindset.
'''

description_COLLECTIVE_185 = '''
**Collective Strategy: Adaptive Cooperative Optimizer (ACO)**

The ACO strategy aims to balance individual payoffs with collective well-being by adapting to the evolving dynamics of the game.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability p_init = 0.7.
2. **Exploration-Exploitation Trade-off**: For rounds t > 1:
	* If the total payoff in the previous round was above the average of all rounds so far (i.e., π_t-1 > avg_payoff), cooperate (C) with probability p_coop = max(0.5, 0.8 × (k/n)).
	* Otherwise, defect (D) with probability p_defect = min(0.5, 0.2 × (n/k)).
3. **Adaptive Adjustment**: Adjust the probabilities based on the game's history:
	+ If the number of cooperators in the previous round was above n/2, increase p_coop by 0.05 and decrease p_defect by 0.05.
	+ If the number of cooperators was below n/4, decrease p_coop by 0.05 and increase p_defect by 0.05.

**Handling Edge Cases:**

1. **Last Round**: In the final round (t = r), defect (D) with probability p_defect = 1.
2. **Tie-breaking**: If the probabilities lead to a tie, choose cooperation (C).

**Collective Mindset:**

The ACO strategy is designed to align with the collective mindset by:

1. Cooperating initially to encourage mutual cooperation and create a positive atmosphere.
2. Adapting to the game's dynamics to balance individual payoffs with collective well-being.
3. Gradually adjusting probabilities based on the number of cooperators, promoting cooperation when beneficial and defecting when necessary.

**Pseudocode:**
```python
def ACO(n, k, r):
    p_init = 0.7
    p_coop = max(0.5, 0.8 * (k/n))
    p_defect = min(0.5, 0.2 * (n/k))
    avg_payoff = 0

    for t in range(r):
        if t == 0:
            # First round
            action = C if random.random() < p_init else D
        elif t > 0:
            # Subsequent rounds
            prev_round_payoff = π_t-1
            avg_payoff = (avg_payoff * (t-1) + prev_round_payoff) / t

            if prev_round_payoff > avg_payoff:
                action = C if random.random() < p_coop else D
            else:
                action = D if random.random() < p_defect else C

        # Adaptive adjustment
        num_cooperators = sum(c_i for c_i in previous_actions)
        if num_cooperators > n/2:
            p_coop += 0.05; p_defect -= 0.05
        elif num_cooperators < n/4:
            p_coop -= 0.05; p_defect += 0.05

    return action
```
The ACO strategy is designed to be robust and adaptive, allowing it to perform well in a wide range of scenarios against various opponents.
'''

description_COLLECTIVE_186 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Overview**
The Adaptive Cooperation (AC) strategy aims to balance individual payoff maximization with collective welfare by adapting to the game's history and opponent behaviors.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate (C).
2. **Recent History**: Observe the total number of cooperators in the last round (t-1). If this number is greater than or equal to half the players (n/2), cooperate (C) in the current round (t). Otherwise, defect (D).
3. **Trend Analysis**: Calculate the trend of cooperation over the last three rounds (t-1, t-2, t-3). Assign a score of +1 for each round with more cooperators than defectors and -1 otherwise. If the total score is positive, cooperate (C) in the current round (t).
4. **Contribution Tracking**: Keep track of individual contributions to the public good over time. Identify the top contributor(s) based on their average contribution per round.
5. **Reward Altruism**: Cooperate (C) with a probability proportional to the number of rounds where your action resulted in a higher payoff for others than yourself.

**Edge Cases**

* In the last round, defect (D) if you're not among the top contributors; otherwise, cooperate (C).
* If all players defected in the previous round, cooperate (C) with a probability of 0.5.
* When faced with an opponent who has always defected, consider their actions as a 'no-contribution' signal and adjust your cooperation probability accordingly.

**Collective Mindset Alignment**

AC prioritizes collective welfare by:

1. Encouraging cooperation when others have recently cooperated (Recent History).
2. Rewarding altruistic behavior (Contribution Tracking).
3. Balancing individual interests with the public good (Trend Analysis).

By adapting to the game's history and opponent behaviors, AC aims to maintain a delicate balance between individual and collective payoffs.

**Pseudocode**
```python
def adaptive_cooperation(n, k, r):
    # Initialize cooperation flags
    recent_history = [False] * n

    for t in range(r):
        if t == 0:
            cooperate = True  # Initial Cooperation
        else:
            last_round_cooperators = sum(recent_history)
            cooperate = (last_round_cooperators >= n/2) or \
                       trend_analysis(t-1, t-2, t-3) > 0

            # Reward altruism with a probability proportional to past contributions
            if random.random() < get_contribution_score():
                cooperate = True

        recent_history.append(cooperate)
        play_game()

    return total_payoff

def trend_analysis(t-1, t-2, t-3):
    scores = []
    for round in [t-1, t-2, t-3]:
        if sum(recent_history[round]) > n/2:
            scores.append(1)
        else:
            scores.append(-1)
    return sum(scores)

def get_contribution_score():
    # Track individual contributions and calculate a score based on altruistic behavior
    pass
```
This pseudocode outline should provide a solid foundation for implementing the Adaptive Cooperation strategy in your tournament.
'''

description_COLLECTIVE_187 = '''
I propose a collective strategy called "Adaptive Collective Optimism" (ACO). ACO aims to balance individual self-interest with collective well-being by adaptively responding to the group's cooperation level and adjusting its own behavior accordingly.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 0.5. This establishes a neutral starting point.
2. **Cooperation Threshold**: Set a cooperation threshold (T) based on the game parameters:
	* T = k / n (e.g., if k=2 and n=6, T=1/3)
3. **Adaptive Cooperation**:
	* If the number of cooperators in the previous round is above or equal to the cooperation threshold (T), cooperate (C).
	* Otherwise, defect (D) with a probability of 0.5.
4. **Reciprocity**: If another player has defected in the previous round after you cooperated, defect (D) in the current round.

**Edge Cases:**

1. **Last Round**: Cooperate (C) unconditionally in the last round to maximize collective payoff.
2. **Ties**: In case of a tie in the number of cooperators, cooperate (C).

**Pseudocode:**
```python
def adaptive_collective_optimism(n, k, r):
    T = k / n  # cooperation threshold

    for t in range(r):  # loop through rounds
        if t == 0:  # first round
            action[t] = random.choice([C, D])  # initial cooperation (50%)
        else:
            num_coop_prev_round = countcoop(actions[:, t-1])
            if num_coop_prev_round >= T:
                action[t] = C  # cooperate if threshold met
            elif defected_after_coop(actions[:, t-1], actions[t-1]):  
                action[t] = D  # reciprocate defection
            else:
                action[t] = random.choice([C, D])  # default to mixed strategy (50%)

        # last round exception
        if t == r - 1:
            action[t] = C  # cooperate unconditionally in last round

    return actions
```
**Collective Mindset:**

ACO prioritizes collective well-being by:

* Cooperating when the group's cooperation level meets or exceeds the threshold (T).
* Encouraging reciprocity to maintain cooperation.
* Defecting only when necessary, such as when others have defected after cooperation.

By adapting to the group's behavior and promoting cooperation, ACO balances individual self-interest with collective well-being. This strategy is robust against a wide range of opponent behaviors and can perform effectively in a tournament setting.
'''

description_COLLECTIVE_188 = '''
To design a collective strategy for this N-Player Public Goods Game that adapts to various opponent behaviors and promotes robust cooperation, I'll outline a decision-making process that leverages both game parameters and observed history.

**Strategy Name: Adaptive Collective Optimizer (ACO)**

### Decision Rules:

1. **Initial Rounds**: In the first round, ACO defects (D) to establish a baseline for observing opponents' behaviors without revealing its own strategy prematurely.
2. **Observation Phase**: For rounds 2 through `r/3` (where `r` is the total number of rounds), ACO cooperates (C) if at least half of the other players cooperated in the previous round, otherwise, it defects. This phase helps in assessing the willingness to cooperate among opponents.
3. **Adaptive Phase**: After the observation phase, ACO employs a tit-for-tat strategy with a twist: It calculates the average cooperation rate of all opponents over the last `r/3` rounds. If this rate is above 0.5, it cooperates; otherwise, it defects. However, if its own payoff in the previous round was higher when defecting than the expected payoff from cooperating (based on opponents' past actions), it defects.
4. **Punishment Mechanism**: To deter exploitation and encourage cooperation, ACO introduces a punishment phase. If the average cooperation rate among all players drops below 0.3 for two consecutive rounds during the adaptive phase, ACO defects for one round to signal dissatisfaction with the current state of cooperation.

### Edge Cases:

- **Last Round**: In the final round (`r`), regardless of past observations or strategies, ACO cooperates if its expected payoff from cooperating (based on the most recent opponent actions) is at least as high as defecting. This approach maximizes collective payoffs while minimizing the risk of being exploited.
- **Ties in Cooperation Rate**: In cases where the cooperation rate among opponents exactly equals the decision threshold (e.g., 0.5), ACO defaults to cooperating to encourage a cooperative environment.

### Collective Mindset Alignment:

ACO is designed with a collective mindset, aiming for mutual benefit through adaptive cooperation. By initially observing and then adapting its strategy based on opponents' actions, ACO seeks to maximize the total payoff of all players while minimizing exploitation risks. The punishment mechanism further aligns with this mindset by discouraging low cooperation rates that could undermine collective success.

**Pseudocode Example:**

```
// Initialize variables
round = 1;
observationPhaseRounds = r/3;
opponentsCooperationRate = 0;

while (round <= r) {
    if (round == 1) {
        // Initial round, defect to observe
        action = D;
    } else if (round <= observationPhaseRounds) {
        // Observation phase
        opponentsCooperationRate = calculateOpponentsCooperationRate(lastRoundActions);
        if (opponentsCooperationRate >= 0.5) {
            action = C;
        } else {
            action = D;
        }
    } else {
        // Adaptive phase
        expectedPayoffFromCooperating = calculateExpectedPayoffFromCooperating(opponentsPastActions, k/n);
        if (opponentsCooperationRate > 0.5 && expectedPayoffFromCooperating >= payoffFromDefecting) {
            action = C;
        } else {
            action = D;
        }
        
        // Punishment mechanism
        if (averageCooperationRate < 0.3 for two consecutive rounds) {
            action = D; // One round punishment
        }
    }
    
    // Update variables and proceed to next round
    lastRoundActions = getActionsFromAllPlayers();
    opponentsPastActions.update(lastRoundActions);
    round++;
}
```

This strategy is designed to be robust against various opponent behaviors by continuously adapting based on observed cooperation rates, promoting a collective mindset that seeks mutual benefit through flexible cooperation.
'''

description_COLLECTIVE_189 = '''
**Collective Strategy: "Adaptive Cooperative Follower" (ACF)**

The ACF strategy aims to balance individual payoff maximization with collective welfare by adapting to the group's behavior and promoting cooperation.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C). This sets a positive tone for the game and encourages others to cooperate.
2. **Follower Phase**: For rounds 2 to r-1:
	* If the number of cooperators in the previous round is greater than or equal to the multiplication factor (k), cooperate (C).
	* Otherwise, defect (D). This rule promotes cooperation when it's beneficial for the group and switches to defection when cooperation is not reciprocated.
3. **Last Round**: In the final round (r), always defect (D). Since there are no future rounds, maximizing individual payoff takes precedence.

**Edge Cases:**

* If all players defected in the previous round, cooperate (C) in the next round to try to revive cooperation.
* If a player observes that others are consistently cooperating, they should continue to cooperate as well, even if their own payoff is not maximized.

**Collective Mindset:**

The ACF strategy prioritizes collective welfare by:

1. Encouraging cooperation when it benefits the group.
2. Adapting to changes in the group's behavior.
3. Avoiding exploitation by others.

By following these rules, the ACF strategy promotes a cooperative atmosphere while being robust against various opponent behaviors.

**Pseudocode:**
```
function AdaptiveCooperativeFollower(n, k, r):
  // Initialize variables
  round = 1
  prev_cooperators = 0

  while round <= r:
    if round == 1:
      action = C (cooperate)
    else:
      if prev_cooperators >= k:
        action = C (cooperate)
      else:
        action = D (defect)

    // Update variables for next round
    prev_cooperators = count_cooperators_in_previous_round()
    round += 1

    if round == r:
      action = D (defect) // Last round, always defect

  return action
```
This strategy will be implemented as an algorithm and compete against other independent strategies in the tournament.
'''

description_COLLECTIVE_190 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy, "Adaptive Cooperation," aims to balance individual payoffs with the benefits of cooperation. We'll employ a dynamic approach that adjusts based on the game's history and parameters.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) unconditionally to establish a cooperative foundation.
2. **Subsequent Rounds (t > 1)**:
	* If the total number of cooperators in the previous round is greater than or equal to `n/2`, cooperate (C).
	* Otherwise, defect (D).
3. **Adaptive Adjustment**: After each round, calculate the average payoff of all players in the current round (`avg_payoff_t`). Compare it to the average payoff in the previous round (`avg_payoff_(t-1)`). If `avg_payoff_t` is greater than or equal to `k/2`, continue with the current action (cooperate or defect) in the next round. Otherwise, switch actions.

Pseudocode:
```python
def adaptive_cooperation(n, k, r):
    # Initialize variables
    avg_payoffs = [0] * r
    cooperators_prev_round = 0

    for t in range(1, r+1):
        if t == 1:  # Initial round
            action = 'C'
        else:
            if cooperators_prev_round >= n/2:
                action = 'C'
            else:
                action = 'D'

            # Adaptive adjustment
            avg_payoff_t = calculate_avg_payoff(t)
            if t > 1 and avg_payoff_t >= k/2:
                action = 'C' if action == 'D' else 'D'

        # Update variables for next round
        cooperators_prev_round = count_cooperators(t-1)
        avg_payoffs[t-1] = calculate_avg_payoff(t)

    return action

def count_cooperators(round_num):
    # Count the number of players who cooperated in the given round
    pass  # Implementation depends on game environment

def calculate_avg_payoff(round_num):
    # Calculate the average payoff of all players in the given round
    pass  # Implementation depends on game environment
```
**Edge Cases:**

* **Last Round (t=r)**: Stick to the action chosen based on the adaptive adjustment rule.
* **Tie in Cooperators**: If there's a tie in the number of cooperators, cooperate.

**Collective Mindset Alignment:**
Our strategy aligns with the collective mindset by initially cooperating and adapting based on the overall performance of all players. By considering the average payoff and adjusting actions accordingly, we promote cooperation while being robust to various opponent behaviors.

By employing this adaptive approach, our strategy balances individual payoffs with the benefits of cooperation, making it a competitive and effective solution for the N-Player Public Goods Game.
'''

description_COLLECTIVE_191 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and game histories.

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and gather information about opponents' initial actions.
2. **Response to Cooperative Opponents**: If the number of cooperators in the previous round is greater than or equal to half of the total players (n/2), cooperate (C) in the current round.
3. **Response to Defective Opponents**: If the number of cooperators in the previous round is less than half of the total players (n/2), defect (D) in the current round, but only if:
	* The opponent's defection rate (proportion of D actions) over the past few rounds (e.g., 3-5 rounds) is greater than a certain threshold (e.g., 0.6).
	* The opponent's payoff from defecting is significantly higher than their payoff from cooperating (e.g., by more than 20%).
4. **Punishment and Forgiveness**: If an opponent defected in the previous round, but then cooperates in the current round, forgive them by cooperating as well.
5. **Exploration and Adaptation**: Introduce random exploration to test opponents' reactions:
	* With a small probability (e.g., 0.1), randomly choose between C and D, regardless of the decision rule above.

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round to maximize collective payoff.
2. **Few Rounds Remaining**: If there are only a few rounds remaining (e.g., less than 5), prioritize cooperation to increase the total game payoff.
3. **Opponent's Consistent Defection**: If an opponent consistently defects over multiple rounds, maintain defection against them.

**Collective Mindset:**

1. **Align with Cooperators**: Prioritize cooperation when opponents cooperate.
2. **Maintain Cooperation**: Sustain cooperation even if opponents defect occasionally.
3. **Deter Defection**: Punish consistent defectors to deter others from following suit.

Pseudocode for ACC strategy:
```python
def adaptive_collective_cooperation(n, r, k, history):
    # Initialize variables
    opponent_defection_rate = [0] * n
    opponent_payoff_difference = [0] * n

    for round in range(r):
        if round == 0:  # Initial Round
            return "C"

        cooperators_last_round = sum([1 if history[round - 1][i] == "C" else 0 for i in range(n)])
        threshold_cooperators = n / 2

        if cooperators_last_round >= threshold_cooperators:
            return "C"
        elif cooperators_last_round < threshold_cooperators:
            for opponent in range(n):
                # Update opponent defection rate and payoff difference
                opponent_defection_rate[opponent] = ...
                opponent_payoff_difference[opponent] = ...

            if random.random() < 0.1:  # Exploration
                return random.choice(["C", "D"])

            if any(opponent_defection_rate[opponent] > 0.6 and opponent_payoff_difference[opponent] > 0.2 for opponent in range(n)):
                return "D"
            else:
                return "C"

        # Last Round or Few Rounds Remaining
        if round >= r - 5:
            return "C"

    return "C"  # Default action
```
Note that this strategy is adaptive and robust to various opponent behaviors, but may not be optimal in all situations. Fine-tuning the parameters (e.g., threshold values, exploration probability) can improve performance.
'''

description_COLLECTIVE_192 = '''
**Collective Strategy: Adaptive Cooperation with History-Based Threshold (ACHT)**

**Overview**
ACHT is a collective strategy that balances individual self-interest with cooperative behavior to achieve mutually beneficial outcomes. It adapts to the game's history and opponent behaviors, ensuring robustness in a wide range of scenarios.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and gather information about opponents' initial actions.
2. **Subsequent Rounds**: Evaluate the previous round's outcome using the following threshold-based approach:
	* Calculate the **Cooperation Ratio** (CR): CR = (number of cooperators in the previous round) / n
	* If CR ≥ k/n, cooperate (C) in the current round, as the collective benefit is substantial enough to warrant cooperation.
	* Otherwise, defect (D) to protect individual interests.
3. **Last Round**: Defect (D) in the last round, as there are no future rounds to reciprocate cooperation.

**Edge Case Handling**

* If a player observes an opponent cooperating while they themselves defected in the previous round, they will cooperate in the current round. This "forgiveness" mechanism encourages opponents to cooperate and helps maintain a positive collective atmosphere.
* In case of a tie (CR = k/n), cooperate (C) to avoid disrupting the cooperative balance.

**Pseudocode**
```markdown
function ACHT(n, k, r, history):
  # Initialize cooperation ratio threshold
  cr_threshold = k / n

  for t in range(1, r+1):
    if t == 1:  # Initial round
      action = C
    else:
      previous_round_cooperators = sum(history[t-1])
      cr = previous_round_cooperators / n

      if cr >= cr_threshold:
        action = C
      elif (cr < cr_threshold) and (history[t-1][self] == D):
        # Forgiveness mechanism
        opponent_cooperated = any(history[t-1][opponent] == C for opponent in range(n))
        if opponent_cooperated:
          action = C
        else:
          action = D
      else:
        action = D

    if t == r:  # Last round
      action = D

    return action
```
**Collective Mindset**
ACHT prioritizes cooperation when the collective benefit is substantial, while protecting individual interests when opponents defect. By adapting to the game's history and opponent behaviors, ACHT promotes a cooperative atmosphere and encourages mutually beneficial outcomes.

In a tournament setting, ACHT's robustness and adaptability will help it perform well against various independent strategies developed by other AI systems.
'''

description_COLLECTIVE_193 = '''
I propose a collective strategy called "Adaptive Cooperation with Retribution" (ACR). ACR aims to balance individual self-interest with collective well-being by adapting to the group's cooperation level and responding to exploitation.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone.
2. **General Rule:** In each subsequent round, calculate the average cooperation rate of all players (including yourself) over the past rounds. Let's call this average "historical_cooperation_rate."
3. **Cooperation Threshold:** If historical_cooperation_rate ≥ 0.5 (or another predetermined threshold), Cooperate (C). Otherwise, Defect (D).
4. **Retribution Mechanism:** If the current round's cooperation rate is significantly lower than the historical average (e.g., by more than 20%), Defect (D) in the next round to signal dissatisfaction and encourage others to cooperate.
5. **Forgiveness:** After a retribution round, reset the cooperation threshold and reassess the situation.

**Edge Cases:**

* **Last Round:** Cooperate (C) unconditionally in the final round to maximize collective payoff, as there is no opportunity for future retaliation or reward.
* **Ties and Near-Ties:** In cases where the historical cooperation rate is very close to the threshold (e.g., within 1%), use a randomization mechanism (e.g., coin flip) to decide between Cooperate and Defect.

**Collective Mindset:**

ACR aligns with the collective mindset by:

* Encouraging initial cooperation to establish a cooperative atmosphere
* Adapting to the group's cooperation level, rather than relying on fixed schedules or norms
* Implementing retribution mechanism to deter exploitation and promote cooperation
* Forgiving past transgressions to allow for renewed cooperation

**Pseudocode:**
```python
def ACR(n, k, r):
  historical_cooperation_rate = []
  
  # Initial Round
  action = "C"
  historical_cooperation_rate.append(1)
  
  for t in range(1, r):
    current_round_cooperations = [0] * n
    for i in range(n):
      if actions[i][t-1] == "C":
        current_round_cooperations[i] += 1
    
    avg_cooperation_rate = sum(current_round_cooperations) / n
    historical_cooperation_rate.append(avg_cooperation_rate)
    
    # General Rule
    if avg_cooperation_rate >= 0.5:
      action = "C"
    else:
      action = "D"
      
    # Retribution Mechanism
    if current_round_cooperations < (avg_cooperation_rate * n) - 0.2*n:
      action = "D"  # retribution round
    
  return actions

# Note: This pseudocode is a simplified representation and may need to be adapted for actual implementation.
```
By employing ACR, we aim to create an adaptive and robust collective strategy that balances individual self-interest with the well-being of the group.
'''

description_COLLECTIVE_194 = '''
I propose a collective strategy called "Adaptive Cooperative Escalation" (ACE). ACE is designed to balance individual payoff maximization with promoting cooperation and adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally.
2. **Reciprocity**: For rounds 2 to r-1, cooperate if:
	* The majority of players (> n/2) cooperated in the previous round.
	* Your payoff in the previous round was greater than or equal to the average payoff of all players.
3. **Escalation**: If reciprocity conditions are not met, defect (D) for one round.
4. **Re-entry**: After escalating, re-enter cooperation if:
	* The majority of players (> n/2) cooperated in the previous round.

**Edge Cases:**

1. **Last Round**: In the final round (r), always cooperate (C).
2. **Tie-breaking**: If the reciprocity condition is met with a tie (equal number of cooperators and defectors), cooperate.
3. **History Reset**: After escalating, reset your internal history to only consider rounds after the escalation.

**Pseudocode:**
```markdown
def ACE(n, k, r, history):
  if round == 1:
    return C

  majority_cooperated = count(history[-1]) > n/2
  my_payoff >= avg_payoffs(history[-1])

  if majority_cooperated and my_payoff >= avg_payoffs:
    return C
  elif escalated_last_round:
    return D
  else:
    escalate = True
    return D

  # Re-entry after escalation
  if majority_cooperated:
    escalate = False
    return C
```
**Collective Mindset:**

ACE promotes cooperation by initially cooperating and then reciprocating with the group. When faced with insufficient cooperation, ACE escalates to defect, but returns to cooperation when the group's behavior improves. This approach aims to balance individual payoff maximization with encouraging collective cooperation.

By adapting to various opponent behaviors, ACE can effectively play against a wide range of strategies in the tournament.
'''

description_COLLECTIVE_195 = '''
**Collective Strategy: Adaptive Cooperation with Retaliation (ACR)**

The ACR strategy aims to balance individual payoff maximization with collective welfare by adapting to the game's history and responding to opponents' behavior.

**Decision Rules:**

1. **First Round:** Cooperate (C) to encourage cooperation and create a positive environment.
2. **Subsequent Rounds:**
	* If the total number of cooperators in the previous round is above a certain threshold (T), cooperate (C). T = ceil(n \* k / n), where ceil(x) rounds x up to the nearest integer.
	* Otherwise, defect (D).
3. **Retaliation Mechanism:** If an opponent defects while you cooperate, and your payoff from that round is less than or equal to 1, switch to defecting (D) for the next two rounds.
4. **Forgiveness Mechanism:** After two consecutive defections triggered by retaliation, return to cooperating (C) if the total number of cooperators in the previous round exceeds T.

**Edge Cases:**

* **Last Round:** Cooperate (C) unconditionally to maximize collective payoff and maintain a positive reputation.
* **Opponent Cooperation Rate:** If an opponent's cooperation rate falls below 1/n, consider them a defector and adjust your strategy accordingly.
* **Multiple Retaliations:** If multiple opponents trigger retaliation simultaneously, prioritize the opponent with the lowest cooperation rate.

**Pseudocode:**
```python
def ACR(n, k, history):
    T = ceil(n * k / n)
    
    if len(history) == 0:  # First round
        return 'C'
    
    total_cooperators_prev_round = sum(1 for action in history[-1] if action == 'C')
    
    if total_cooperators_prev_round >= T:
        my_action = 'C'
    else:
        my_action = 'D'
        
    # Retaliation mechanism
    if len(history) > 1 and my_action == 'C' and history[-1][i] == 'D':
        if get_payoff(history, i) <= 1:
            my_action = 'D'
            retaliation_trigger = True
            
            for _ in range(2):
                yield 'D'
                
            retaliation_trigger = False
    
    # Forgiveness mechanism
    if len(history) > 3 and history[-2][i] == 'D' and history[-3][i] == 'D':
        total_cooperators_prev_round = sum(1 for action in history[-2] if action == 'C')
        
        if total_cooperators_prev_round >= T:
            my_action = 'C'
            
    return my_action
```
**Collective Mindset:** ACR prioritizes cooperation while adapting to the game's dynamics. By retaliating against defectors and forgiving past transgressions, it promotes a culture of mutual support and cooperation. The strategy aims to maximize collective payoff by balancing individual self-interest with group welfare.
'''

description_COLLECTIVE_196 = '''
**Collective Strategy: "Adaptive Public Goods"**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 0.5.
2. **Adaptive Cooperation**: For subsequent rounds (t > 1), calculate the average cooperation rate (ACR) of all players in the previous round:

   ACR = Σ(j=1 to n) c_j / n

   If ACR ≥ 0.5, cooperate (play C). Otherwise, defect (play D).
3. **Punishment Mechanism**: If a player observes that their payoff was lower than the average payoff of all players in the previous round, defect (play D) with a probability of 0.7 for the next two rounds.
4. **Forgiveness**: After punishing, return to Adaptive Cooperation.

**Handling Edge Cases:**

1. **First Round**: Cooperate with a probability of 0.5 (as described above).
2. **Last Round**: Always defect (play D) in the last round, as there are no future rounds for reciprocity.
3. **Ties**: In case of a tie in average cooperation rate or payoffs, follow the Adaptive Cooperation rule.

**Collective Mindset:**

The strategy aims to create an environment where cooperation is encouraged and sustained through adaptive behavior. By cooperating when others cooperate and punishing those who don't, we foster a collective mindset that promotes mutual benefit.

**Pseudocode (for illustration purposes):**
```markdown
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
payoffs = array to store payoffs for each player

# First round
if t == 1:
    # Cooperate with probability 0.5
    cooperate_prob = 0.5
else:
    # Calculate average cooperation rate (ACR)
    ACR = sum(cooperation_rates) / n
    
    # Adaptive Cooperation
    if ACR >= 0.5:
        cooperate_prob = 1
    else:
        cooperate_prob = 0
        
    # Punishment Mechanism
    if payoff < avg_payoff and t > 1:
        cooperate_prob = 0.3

# Play the game
if random() < cooperate_prob:
    action = C
else:
    action = D

# Update payoffs and cooperation rates
payoffs[t] = calculate_payoff(action, k, n)
cooperation_rates[t] = update_cooperation_rate(action)

# Repeat for all rounds
```
This strategy is designed to be robust against a wide range of opponent behaviors while promoting collective cooperation. By adapting to the average cooperation rate and punishing those who don't cooperate, we create an environment where mutual benefit is encouraged.
'''

description_COLLECTIVE_197 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

The ACP strategy aims to balance individual self-interest with collective cooperation, adapting to the game's history and opponent behaviors.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to reciprocate.
2. **Responding to Cooperation**: If a player i observes that at least half of the players cooperated (Σc_j ≥ n/2) in the previous round, they will cooperate (C) in the next round.
3. **Punishing Defection**: If a player i observes that fewer than half of the players cooperated (Σc_j < n/2) in the previous round, and at least one player defected (D), they will defect (D) in the next round to punish the non-cooperative behavior.
4. **Exploiting Defection**: If a player i observes that all or most players defected (D) in the previous round (Σc_j ≤ 1), they will defect (D) in the next round to exploit the non-cooperative environment.

**Edge Cases**

* **Last Round**: Cooperate (C) in the last round to maximize collective payoff, as there is no future opportunity for reciprocity.
* **Ties**: In cases where the number of cooperators is exactly half of the total players (Σc_j = n/2), player i will cooperate (C) to maintain a cooperative atmosphere.

**Collective Mindset**

ACP prioritizes cooperation when it is mutually beneficial, while incorporating punishment mechanisms to deter exploitation. This strategy promotes collective well-being by:

* Encouraging cooperation through reciprocity
* Punishing non-cooperative behavior to maintain social norms
* Adapting to changing opponent behaviors

Pseudocode for ACP:
```python
def ACP(n, k, history):
  # Initialize variables
  cooperate = True
  punish = False

  # Analyze previous round's actions
  prev_coops = sum([c_j for c_j in history[-1]])
  
  if prev_coops >= n/2:  # Respond to cooperation
    cooperate = True
  elif prev_coops < n/2 and any(c_j == 0 for c_j in history[-1]):  # Punish defection
    punish = True
  else:  # Exploit defection
    cooperate = False

  if last_round():  # Last round, maximize collective payoff
    cooperate = True

  return cooperate
```
This strategy will adapt to the game's dynamics and respond effectively to a wide range of opponent behaviors, prioritizing collective cooperation while maintaining robustness against exploitation.
'''

description_COLLECTIVE_198 = '''
I propose a collective strategy called "Adaptive Cooperation" (AC). AC is designed to balance individual self-interest with the need for cooperation to achieve mutual benefits.

**Decision Rules:**

1. **Initial Exploration (Round 1-2)**: In the first two rounds, play Cooperate (C) to gather information about the opponents' behavior and assess their willingness to cooperate.
2. **Tit-for-Tat Adaptation (Rounds 3-r)**:
	* If a player i observes that more than half of the players cooperated in the previous round, they will Cooperate (C) in the current round.
	* Otherwise, if player i's own payoff in the previous round was higher than or equal to the average payoff of all players, they will Defect (D) in the current round. This is to avoid being taken advantage of by free-riders.
	* If neither condition is met, player i will Cooperate (C) with a probability p = (k/n) × (number of cooperators in previous round / n). This introduces an adaptive element, as the likelihood of cooperation increases when more players cooperate and the public good becomes more valuable.
3. **Exploitation Prevention**: If player i detects that another player j has defected for two consecutive rounds while receiving a higher payoff than the average, they will Defect (D) against player j in the next round to prevent exploitation.

**Edge Cases:**

* In the first round, all players Cooperate (C).
* In the last round (r), if player i's own payoff in the previous round was higher than or equal to the average payoff of all players, they will Defect (D). Otherwise, they will Cooperate (C) with probability p = (k/n) × (number of cooperators in previous round / n).
* If a player i observes that an opponent has consistently defected throughout the game (i.e., never cooperated), they will Defect (D) against that opponent for the remainder of the game.

**Collective Mindset:**

AC is designed to promote cooperation while being robust to a range of opponent behaviors. By initially exploring and adapting to the opponents' behavior, AC encourages mutual cooperation and takes advantage of the public good's value. The Tit-for-Tat adaptation mechanism ensures that players are not taken advantage of by free-riders, while the Exploitation Prevention rule discourages repeated defection.

**Pseudocode:**
```python
def AdaptiveCooperation(game_parameters, history):
  # Initialize variables
  n = game_parameters['n']
  k = game_parameters['k']
  r = game_parameters['r']
  payoffs = []
  
  # Initial Exploration (Rounds 1-2)
  if len(history) <= 2:
    return 'C'
  
  # Tit-for-Tat Adaptation
  previous_round_cooperators = sum([1 for action in history[-1] if action == 'C'])
  own_payoff = payoffs[-1]
  average_payoff = sum(payoffs) / len(payoffs)
  
  if previous_round_cooperators > n/2:
    return 'C'
  elif own_payoff >= average_payoff:
    return 'D'
  else:
    p = (k/n) * (previous_round_cooperators / n)
    return 'C' with probability p
  
  # Exploitation Prevention
  for opponent in range(n):
    if history[-2][opponent] == 'D' and history[-1][opponent] == 'D':
      if payoffs[opponent][-1] > average_payoff:
        return 'D'
  
  # Edge cases
  if len(history) == r:  # Last round
    if own_payoff >= average_payoff:
      return 'D'
    else:
      p = (k/n) * (previous_round_cooperators / n)
      return 'C' with probability p
  
  if history[opponent].count('D') == len(history):  # Consistent defector
    return 'D'
```
This strategy balances individual self-interest with the need for cooperation, making it a robust and adaptive collective strategy.
'''

description_COLLECTIVE_199 = '''
**Collective Strategy: Adaptive Cooperation**

This strategy aims to balance individual payoffs with collective well-being by adapting to the game's history and parameters. It does not rely on pre-coordination or opponent-specific knowledge.

**Decision Rules:**

1. **Initial Exploration (Round 1-2)**:
	* Cooperate in Round 1.
	* Observe the number of cooperators (C) in Round 1.
	* In Round 2, if C ≥ n/2, cooperate; otherwise, defect.
2. **Adaptive Phase**:
	* For each round t > 2:
		+ Calculate the moving average of the total number of cooperators over the last 3 rounds (MA_3).
		+ If MA_3 ≥ k/n, cooperate; otherwise, defect.
3. **Last Round (t = r)**:
	* Cooperate if the current round's payoff from cooperation is higher than the expected payoff from defection.

**Pseudocode:**
```
def adaptive_cooperation(n, k, t):
    # Initial Exploration
    if t == 1:
        return C
    elif t == 2:
        c_prev = count_cooperators(t-1)
        return C if c_prev >= n/2 else D

    # Adaptive Phase
    ma_3 = moving_average(count_cooperators, 3, t)
    if ma_3 >= k/n:
        return C
    else:
        return D

    # Last Round
    if t == r:
        payoffs = calculate_payoffs()
        return C if payoffs[C] > payoffs[D] else D
```
**Rationale:**

1. **Initial Exploration**: Cooperate in the first round to gather information and encourage others to cooperate. In the second round, observe the number of cooperators and adjust strategy accordingly.
2. **Adaptive Phase**: Use a moving average of recent cooperation levels to estimate the likelihood of high collective payoffs from cooperation. If the moving average is above the threshold (k/n), it suggests that cooperation is likely to be profitable, so cooperate. Otherwise, defect to minimize losses.
3. **Last Round**: In the final round, prioritize individual payoff maximization by choosing the action with the higher expected payoff.

**Collective Mindset:**

This strategy prioritizes collective well-being while being mindful of individual payoffs. By adapting to the game's history and parameters, it aims to find a balance between cooperation and defection that benefits both the individual and the group. The moving average calculation helps smooth out noise from previous rounds, making the strategy more robust to varying opponent behaviors.

This adaptive approach allows the strategy to respond to changes in the environment without relying on pre-coordination or specific knowledge about opponents. By incorporating elements of exploration and adaptation, this collective strategy is well-suited for a wide range of opponent behaviors and game parameters.
'''

description_COLLECTIVE_200 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective well-being by adapting its cooperation threshold based on the game's history.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 50% to encourage initial cooperation and gather information about other players' behaviors.
2. **Cooperation Threshold**: Calculate a cooperation threshold (`theta`) as a moving average of the total contributions in previous rounds:

`theta = (1 - alpha) * theta_prev + alpha * (k/n) * sum(contributions_prev_round)`

where `alpha` is a learning rate (0 < α < 1), and `contributions_prev_round` is the number of cooperators in the previous round.
3. **Adaptive Cooperation**: Cooperate (C) if the current total contributions (`sum(contributions_curr_round)`) exceed or equal `theta`. Otherwise, defect (D).

Pseudocode:
```
def ACT(n, k, alpha):
  theta = 0
  for t in range(r):
    if t == 0:  # First round
      cooperate = random.random() < 0.5
    else:
      contributions_prev_round = sum(contributions[t-1])
      theta = (1 - alpha) * theta + alpha * (k/n) * contributions_prev_round
      cooperate = sum(contributions[t]) >= theta

    # Take action based on cooperation decision
    if cooperate:
      return C
    else:
      return D
```
**Handling Edge Cases:**

* **Last Round**: In the last round, defect (D) to maximize individual payoff.
* **Consecutive Defections**: If all players have defected for `m` consecutive rounds (`m` is a small integer, e.g., 3), reset `theta` to encourage cooperation.

**Collective Mindset:**

The ACT strategy aims to promote collective well-being by:

1. Encouraging initial cooperation
2. Adapting to the game's history and adjusting its cooperation threshold accordingly
3. Balancing individual self-interest with collective benefits

By using a moving average of total contributions, ACT is robust to a wide range of opponent behaviors and can adapt to changing circumstances in the game.

In a tournament setting, ACT will play against other independent strategies, and its performance will depend on its ability to balance cooperation and defection effectively.
'''

description_COLLECTIVE_201 = '''
To design a collective strategy for this N-Player Public Goods Game that is adaptive and robust to various opponent behaviors, I propose a dynamic approach that adjusts cooperation levels based on the game's history. This strategy aims to balance individual payoff maximization with collective welfare, fostering a cooperative environment without relying on explicit coordination or shared norms.

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Rounds (t ≤ 3):** Cooperate (C) to establish an optimistic tone and encourage cooperation from others.
2. **Assessment Phase (4 ≤ t ≤ r/2):**
   - Calculate the average number of cooperators in previous rounds (`avg_coop`).
   - If `avg_coop ≥ n/2`, continue cooperating (C). This indicates a sufficiently cooperative environment to justify continued contribution.
   - Otherwise, defect (D) for one round to signal dissatisfaction with the current cooperation level and potentially induce others to cooperate more.
3. **Adaptive Phase (t > r/2):**
   - Calculate the moving average of cooperators over the last `r/4` rounds (`moving_avg_coop`).
   - If `moving_avg_coop ≥ n/2`, maintain or increase cooperation level (play C if previously played D, continue C otherwise). This adapts to improving cooperation trends.
   - If `moving_avg_coop < n/2`, decrease cooperation level or defect (switch from C to D) to adjust to decreasing cooperation trends.

**Edge Cases:**

- **First Round:** Always cooperate (C) to set a positive initial condition for the game.
- **Last Round (t = r):** Cooperate if `moving_avg_coop ≥ n/2` in the penultimate round; otherwise, defect. This ensures consistency with the strategy's adaptive nature and ends on a cooperative note when feasible.

**Collective Mindset:**
ACO is designed to encourage collective cooperation by initially setting an optimistic tone, then adapting based on observed cooperation levels. It rewards sustained cooperation from others while signaling for increased cooperation through targeted defections. By adjusting its behavior in response to the game's history, ACO promotes a dynamic where players are incentivized to cooperate, leading to higher total payoffs.

**Pseudocode Overview:**
```markdown
# Initialize variables
avg_coop = 0; moving_avg_coop = 0; prev_action = 'C'; r_threshold = r / 2;

FOR t FROM 1 TO r:
    IF t ≤ 3:
        action = 'C'
    ELSEIF t ≤ r_threshold:
        avg_coop = calculate_average_cooperators(t-1)
        IF avg_coop ≥ n/2:
            action = 'C'
        ELSE:
            action = 'D' for one round
    ELSE:
        moving_avg_coop = calculate_moving_average_cooperators(t-1, r/4)
        IF moving_avg_coop ≥ n/2 AND prev_action == 'D':
            action = 'C'
        ELSIF moving_avg_coop < n/2 AND prev_action == 'C':
            action = 'D'
        ELSE:
            action = prev_action
    END IF

    # Update for next iteration
    prev_action = action
    
    # Last round special case
    IF t == r:
        IF moving_avg_coop ≥ n/2:
            action = 'C'

END FOR
```

This strategy, ACO, is designed to navigate the challenges of the N-Player Public Goods Game by dynamically adjusting its cooperation level based on observed game history. By balancing individual payoff maximization with collective welfare considerations, ACO aims to foster a cooperative environment that benefits all players involved.
'''

description_COLLECTIVE_202 = '''
**Collective Strategy: Adaptive Cooperation with History-Based Threshold (ACHT)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and gather information about opponents' behaviors.
2. **Subsequent Rounds:** Observe the total number of cooperators (Σc_j) in the previous round and calculate the **cooperation rate** (CR) as Σc_j / n.
3. **Threshold-Based Cooperation:**
	* If CR ≥ k/n, cooperate (C). This indicates a sufficient level of cooperation to warrant continued contribution.
	* If CR < k/n, defect (D). This suggests that too many players are exploiting the public good, and it's time to adjust strategy.
4. **Punishment Mechanism:** If a player defects while the overall cooperation rate is above the threshold (CR ≥ k/n), they will be "punished" by defecting in the next round.

**Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round to maximize collective payoff, regardless of the cooperation rate.
2. **Ties in Cooperation Rate:** In case of a tie (CR = k/n), cooperate (C) to maintain a cooperative stance and encourage others to do the same.

**Collective Mindset:**

1. **Monitor Opponents' Behaviors:** Continuously observe opponents' actions and adjust strategy based on their cooperation rates.
2. **Gradual Adaptation:** Gradually adapt the cooperation threshold (k/n) based on the observed cooperation rate over time.
3. **Encourage Cooperation:** By cooperating when the cooperation rate is high, ACHT encourages other players to cooperate as well.

**Pseudocode:**

```
Input: game parameters (n, k, r), history of opponents' actions
Output: action (C or D)

Initialize:
  CR = 0 // cooperation rate
  threshold = k/n

For each round t from 1 to r:
  If t == 1: // initial round
    return C
  Else:
    Observe previous round's actions and calculate CR
    If CR >= threshold:
      return C
    Else:
      return D

  // Punishment mechanism
  If (CR >= threshold) and (opponent defected):
    next_action = D
```

ACHT is a collective strategy that balances individual self-interest with the need for cooperation to achieve a higher collective payoff. By adapting to the observed cooperation rate, ACHT promotes cooperation while discouraging exploitation. This strategy should perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_203 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Concession (ACGC)**

The ACGC strategy aims to balance individual payoff maximization with collective welfare, adapting to the behavior of other players while maintaining a cooperative mindset.

**Decision Rules:**

1. **Initial Round:** Cooperate (play C) in the first round to establish a cooperative tone and encourage others to reciprocate.
2. **Concession Threshold:** Set a concession threshold, `τ`, based on the game parameters:
	* `τ = 0.5 * (k/n)` for n ≥ 3
	* `τ = 1/3` for n = 2

This threshold represents the minimum proportion of cooperators required to sustain cooperation.
3. ** Cooperation vs Defection:**
	* If the number of cooperators in the previous round is above or equal to `τ * n`, cooperate (play C) in the current round.
	* Otherwise, defect (play D).
4. **Gradual Concession:** If the strategy defects and observes that the number of cooperators has decreased further, incrementally decrease the concession threshold by a small amount (`δ = 0.05`) to avoid getting stuck in a non-cooperative state.

Pseudocode:
```python
def ACGC_strategy(history, params):
    n, k, r = params['n'], params['k'], params['r']
    tau = 0.5 * (k/n) if n >= 3 else 1/3
    delta = 0.05

    if history.round == 1:  # Initial Round
        return 'C'

    prev_cooperators = sum(1 for action in history.prev_actions if action == 'C')
    cooperators_ratio = prev_cooperators / n

    if cooperators_ratio >= tau:
        return 'C'
    else:
        tau -= delta  # Gradual concession
        return 'D'
```
**Edge Cases:**

* **Last Round:** In the last round, play C regardless of the previous rounds' outcomes to maximize collective payoff.
* **Single Defector:** If only one player has defected in a round, and all others cooperated, continue cooperating to avoid punishing a single deviant.

The ACGC strategy balances individual interests with collective welfare by adapting to the behavior of other players. By maintaining a cooperative mindset and gradually conceding when cooperation is not reciprocated, this strategy aims to promote mutual cooperation and maximize overall payoff in the N-player Public Goods Game.
'''

description_COLLECTIVE_204 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

The ACP strategy is designed to balance individual self-interest with collective well-being, while adapting to various opponent behaviors and game parameters.

**Decision Rules:**

1. **Initial Rounds**: Cooperate in the first round (t=1) to encourage mutual cooperation.
2. **Reciprocal Cooperation**: If a majority (> n/2) of players cooperated in the previous round, cooperate in the current round.
3. **Punishment**: If fewer than half of the players cooperated in the previous round, defect in the current round.
4. **Self-Interest**: If the multiplication factor (k) is high enough to make cooperation profitable even if others don't reciprocate (i.e., k/n > 0.5), cooperate regardless of others' actions.

Pseudocode:
```
function ACP(n, k, t, history):
    if t == 1:  // First round
        return COOPERATE

    prev_round_coops = count COOPERATEs in history[t-1]
    if prev_round_coops > n/2:
        return COOPERATE
    elif prev_round_coops < n/2 and k/n <= 0.5:
        return DEFECT
    else:  // High k, or not enough cooperators
        return COOPERATE

// Additional edge case handling:

function ACP_last_round(n, k, t, history):
    if t == r (last round):  // Last round
        return DEFECT (since there's no future reciprocity)

function ACP_tie_breaker(n, k, t, history):
    if prev_round_coops == n/2:  // Tie-breaker for equal cooperators and defectors
        return COOPERATE (to favor cooperation)
```
**Collective Mindset Alignment:**

The ACP strategy prioritizes collective well-being by:

1. Encouraging initial cooperation to set a positive tone.
2. Reciprocating cooperation when others cooperate, fostering mutual benefit.
3. Punishing defection to discourage exploitation and promote fairness.

By balancing self-interest with collective considerations, the ACP strategy aims to achieve a high total payoff for all players while adapting to various opponent behaviors.

**Robustness:**

ACP's adaptability helps it respond effectively to:

1. **Free riders**: Defection in response to insufficient cooperation discourages exploitation.
2. **Tit-for-tat strategies**: Reciprocal cooperation promotes mutual benefit and stability.
3. **Noisy or stochastic behavior**: ACP's thresholds (e.g., n/2) provide robustness against minor deviations.

This strategy should perform well in a tournament setting, as it balances individual self-interest with collective considerations while adapting to various opponent behaviors.
'''

description_COLLECTIVE_205 = '''
I propose a collective strategy for the N-Player Public Goods Game, which I'll call "Adaptive Collective Optimism" (ACO). ACO aims to balance individual self-interest with collective well-being by adapting to the game's history and exploiting opportunities for cooperation.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally to establish a baseline level of trust.
2. **Adaptive Response**: For subsequent rounds, calculate the "Collective Cooperation Index" (CCI):
   CCI = (Total number of cooperators in previous round / n)
   If CCI ≥ 0.5, cooperate (C). Otherwise, defect (D).
3. **Punishment and Forgiveness**: Monitor the opponent's behavior over a window of w rounds (e.g., w = 3). If an opponent defects more than once within this window, defect (D) against them in the next round. However, if they cooperate consistently for w consecutive rounds after a defection, forgive their previous transgression and revert to cooperation.
4. **Exploration**: With a small probability ε (e.g., 0.05), choose an action randomly (C or D) instead of following the adaptive response rule. This allows ACO to explore alternative strategies and adapt to changing opponent behaviors.

**Edge Cases:**

1. **Last Round**: Cooperate unconditionally in the last round, as there is no opportunity for future reciprocity.
2. **Early Rounds**: If fewer than w rounds have passed since the start of the game or an opponent's previous defection, default to cooperation (C).
3. **Ties**: In case of a tie in CCI calculation, defect (D) to avoid excessive generosity.

**Collective Mindset:**

ACO prioritizes collective well-being while being mindful of individual self-interest. By cooperating initially and adapting to the opponent's behavior, ACO encourages cooperation and punishes repeated defections. The exploration mechanism allows for flexibility and adaptation to different opponent strategies.

Pseudocode (high-level overview):
```python
def adaptive_collective_optimism(n, r, k, history):
    if current_round == 1:
        return "C"  # Initial Cooperation
    
    CCI = calculate_CCI(history)
    if CCI >= 0.5:
        action = "C"
    else:
        action = "D"

    opponent_behavior = monitor_opponent(history)
    if opponent_behavior == "defect_twice":
        action = "D"  # Punishment
    elif opponent_behavior == "cooperate_consistently":
        action = "C"  # Forgiveness

    if random.random() < ε:
        action = random.choice(["C", "D"])  # Exploration

    return action
```
This strategy should perform well in a tournament setting, as it balances individual self-interest with collective cooperation and adapts to various opponent behaviors.
'''

description_COLLECTIVE_206 = '''
**Collective Strategy: "Adaptive Cooperation"**

Our collective strategy, Adaptive Cooperation, aims to balance individual payoffs with collective welfare by adapting to the evolving game dynamics and opponent behaviors.

**Decision Rules:**

1. **Initial Rounds (t = 1 to ⌊r/2⌋):**
	* Cooperate (C) with a probability p_init = 0.5.
	* Observe opponents' actions and update the cooperation probability based on their behavior.
2. **Middle Rounds (t = ⌊r/2⌋ + 1 to ⌈3r/4⌉):**
	* Calculate the average number of cooperators in the previous round: avg_coop_prev = Σ(j=1 to n) c_j / n.
	* If avg_coop_prev ≥ k/n, cooperate (C) with probability p_middle = 0.7; otherwise, defect (D).
3. **Final Rounds (t = ⌈3r/4⌉ + 1 to r):**
	* Calculate the total number of cooperators in the previous round: total_coop_prev = Σ(j=1 to n) c_j.
	* If total_coop_prev ≥ k, cooperate (C); otherwise, defect (D).

**Handling Edge Cases:**

1. **First Round:** Cooperate with probability p_init = 0.5.
2. **Last Round:** Cooperate if the total number of cooperators in the previous round is greater than or equal to k; otherwise, defect.
3. **Tie-breaking:** In case of a tie in the average number of cooperators or total number of cooperators, cooperate with probability p_middle = 0.7.

**Collective Mindset:**

Adaptive Cooperation prioritizes collective welfare by:

1. Encouraging cooperation in early rounds to establish a cooperative atmosphere.
2. Adapting to opponents' behavior and adjusting cooperation probabilities accordingly.
3. Focusing on individual payoffs in later rounds, while still considering the collective good.

**Pseudocode:**
```python
def adaptive_cooperation(n, k, r):
    p_init = 0.5
    avg_coop_prev = 0

    for t in range(1, r+1):
        if t <= r/2:
            # Initial rounds
            cooperate_prob = p_init
        elif t <= 3r/4:
            # Middle rounds
            avg_coop_prev = sum(c_j for c_j in previous_actions) / n
            cooperate_prob = 0.7 if avg_coop_prev >= k/n else 0.3
        else:
            # Final rounds
            total_coop_prev = sum(previous_actions)
            cooperate_prob = 1 if total_coop_prev >= k else 0

        action = 'C' if random.random() < cooperate_prob else 'D'
        yield action

    # Update internal state for next round
    previous_actions = [int(action == 'C') for action in actions]
```
This strategy is designed to be robust and adaptive, allowing it to perform well against a wide range of opponent behaviors.
'''

description_COLLECTIVE_207 = '''
To design a collective strategy for this N-Player Public Goods Game that is adaptive and robust, we can employ a mix of retaliatory and forgiving elements while maintaining a focus on maximizing overall payoff through cooperation. This approach will aim to encourage cooperation among players while being resilient against exploiters.

**Decision Rules:**

1. **First Round**: Cooperate (C). This initial move sets the tone for potential cooperation and allows us to gauge other players' strategies without immediately risking exploitation.
   
2. **Subsequent Rounds**:
   - If in the previous round, at least one player cooperated (including yourself), cooperate if your payoff from cooperating was greater than or equal to the payoff from defecting. This encourages continued cooperation when it's beneficial and fosters a sense of reciprocity.
   - If all players defected in the previous round, implement a "gradual forgiveness" mechanism:
     - With probability p (e.g., 0.5), cooperate to test if others are willing to return to cooperative behavior.
     - With probability 1-p, defect to signal that exploitation will not be tolerated without consequences.

3. **Last Round**:
   - Given the game's structure and the lack of future interactions, players might defect in the final round regardless of previous actions (end-game effect). To maximize payoff, defect (D) if you expect others might do the same.
   
4. **Special Case: Early Defection Detection**
   - If a player detects that all other players have defected at least once within a certain number of initial rounds (e.g., first 20% of the game), switch to a more defensive strategy where cooperation is contingent on observing cooperation from others in previous rounds.

**Pseudocode Example**

```
Function DecideAction(round, history):
    if round == 1:
        return C
    else:
        last_round_cooperators = CountCooperators(history[round-1])
        if last_round_cooperators > 0:
            my_last_payoff_coop = GetPayoffIfCooperated(history[round-1], True)
            my_last_payoff_defect = GetPayoffIfDefected(history[round-1], False)
            if my_last_payoff_coop >= my_last_payoff_defect:
                return C
        else:
            with probability p:
                return C
            else:
                return D

Function DecideLastRound(round, history):
    if round == total_rounds:
        return D
```

**Collective Mindset:**
This strategy is designed to balance individual payoff maximization with collective well-being. By initially cooperating and then adapting based on the actions of others, it encourages a cooperative environment while safeguarding against exploitation. The gradual forgiveness mechanism allows for the reestablishment of cooperation after periods of mutual defection, promoting a more stable and beneficial game state for all players involved.

This approach does not rely on pre-coordination or shared norms among players but instead responds to the emergent behavior in the game, making it suitable for a tournament setting against diverse strategies.
'''

description_COLLECTIVE_208 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**: Observe the previous round's outcome and adjust behavior accordingly:
	* If the total number of cooperators is above the threshold (`n * k / (k + 1)`), cooperate (C). This encourages continued cooperation when it's beneficial.
	* If the total number of cooperators is below the threshold, defect (D) to signal dissatisfaction with the current level of cooperation.
3. **Punishment Mechanism**: Implement a punishment phase if the opponent's cooperation rate drops significantly. Specifically:
	* Monitor the moving average of opponents' cooperation rates over a window of `r/4` rounds.
	* If this average falls below `1/n`, defect (D) for `r/4` consecutive rounds to signal disapproval and encourage cooperation.

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round, as there's no opportunity for future reciprocity or punishment.
2. **Single Opponent Defection**: If only one opponent defects while all others cooperate, defect (D) in the next round to signal disapproval and encourage cooperation.

**Collective Mindset:**

ACP aims to promote collective well-being by adapting to the group's cooperation level. By cooperating when others do, ACP encourages a culture of mutual support. When cooperation falters, ACP's punishment mechanism helps restore balance and motivate opponents to cooperate.

Pseudocode (for illustration purposes):
```
function ACP(n, k, r, history) {
  // Initialize variables
  threshold = n * k / (k + 1)
  coop_window_size = r/4

  // First round: Cooperate
  if (round == 1) return C

  // Analyze previous round's outcome
  prev_coop_count = countCooperators(history[round-1])
  if (prev_coop_count > threshold) {
    return C
  } else {
    return D
  }

  // Punishment mechanism
  opponent_coop_avg = movingAverage(opponentCooperationRates, coop_window_size)
  if (opponent_coop_avg < 1/n) {
    // Defect for r/4 rounds to signal disapproval
    for (i=0; i<coop_window_size; i++) return D
  }
}

function opponentCooperationRates(history) {
  // Compute cooperation rates for each opponent over a window of coop_window_size rounds
}
```
Note: This strategy will be implemented as an algorithm, but the pseudocode above serves to illustrate the decision-making process.
'''

description_COLLECTIVE_209 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Concession (ACGC)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to encourage collective contribution and set a positive tone.
2. **Contribution-Based Cooperation**: In subsequent rounds, cooperate if at least k/n players contributed in the previous round. This threshold ensures that enough players are contributing for the public good to be valuable.
3. **Gradual Concession**: If cooperation is met with insufficient contributions (less than k/n), gradually concede by cooperating only if at least one more player contributes compared to the previous round. This mechanism allows for adaptation to changing opponent behaviors and avoids getting stuck in an uncooperative state.
4. **Defection Threshold**: If, despite gradual concession, less than 1/3 of players contribute, defect (D) to protect individual payoffs.

**Edge Case Handling:**

* **First Round**: Cooperate to set a positive tone and encourage collective contribution.
* **Last Round**: Cooperate if at least k/n players contributed in the previous round. If not, defect to maximize individual payoff.
* **Tie-Breaking**: In case of a tie (e.g., equal number of cooperators and defectors), cooperate to maintain a cooperative atmosphere.

**Collective Mindset:**

ACGC prioritizes collective contribution while adapting to changing opponent behaviors. By gradually conceding in response to insufficient contributions, ACGC balances individual payoffs with the need for cooperation. This approach ensures that the strategy remains robust against a wide range of opponent behaviors and encourages other players to cooperate.

**Pseudocode (simplified):**
```markdown
function ACGC(n, k, history):
  if round == 1:
    return COOPERATE

  prev_contributors = countcontributors(history[-1])
  if prev_contributors >= k/n:
    return COOPERATE

  concession_threshold = max(1, prev_contributors + 1)
  curr_contributors = countcontributors(history[-1])

  if curr_contributors >= concession_threshold:
    return COOPERATE
  elif curr_contributors < n/3:
    return DEFECT
  else:
    return COOPERATE

def countcontributors(round_history):
  # Count the number of contributors in a given round history
```
This strategy will be implemented as an algorithm and tested against other AI systems in the tournament.
'''

description_COLLECTIVE_210 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and game parameters:

**Strategy Name: Adaptive Collective Optimizer (ACO)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage cooperation and gather information about opponents' behavior.
2. **Exploration Phase (Rounds 2-5):** Alternate between Cooperate (C) and Defect (D) every other round to test opponents' reactions and identify potential free-riders or cooperators.
3. **Adaptive Phase (Rounds 6+):** Use the following decision rules based on observed opponent behavior:
	* If the average number of cooperators in the previous rounds is greater than n/2, Cooperate (C).
	* If the average number of cooperators is less than or equal to n/2 and the multiplication factor k > 1.5, Defect (D) with a probability p = 0.5; otherwise, Cooperate (C).
	* If an opponent has defected in more than half of the previous rounds, Defect (D) against that opponent.
4. **Endgame (Last Round):** Cooperate (C) to maximize collective payoff.

**Pseudocode:**
```python
def ACO(n, k, r, history):
  # Initialize variables
  avg_cooperators = 0
  defecting_opponents = []

  for t in range(1, r+1):
    if t == 1:
      action = 'C'
    elif t < 6:
      action = 'D' if t % 2 == 0 else 'C'
    else:
      avg_cooperators = sum(history[t-1]['cooperators']) / n
      if avg_cooperators > n/2:
        action = 'C'
      elif k > 1.5 and avg_cooperators <= n/2:
        action = 'D' if random.random() < 0.5 else 'C'
      else:
        for opponent in history[t-1]['actions']:
          if opponent != 'C':
            defecting_opponents.append(opponent)
        action = 'D' if len(defecting_opponents) > n/2 else 'C'

    # Last round
    if t == r:
      action = 'C'

  return action
```
**Collective Mindset:** ACO prioritizes collective optimization by adapting to the game environment and opponent behavior. By initially cooperating, exploring different actions, and adjusting based on observed behavior, ACO encourages cooperation while minimizing exploitation.

This strategy is robust to various opponent behaviors, including all-defect or all-cooperate strategies, as well as more complex patterns. Its adaptability makes it a competitive choice for the tournament.
'''

description_COLLECTIVE_211 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Withdrawal (ACGW)**

**Overview**

ACGW is a collective strategy designed for the N-Player Public Goods Game that balances cooperation and self-interest. It adapts to the group's behavior, gradually withdrawing from cooperation in response to widespread defection.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone.
2. **Cooperation Threshold**: Calculate the cooperation threshold `θ` as a function of the game parameters:
   `θ = (n - k) / n`

For example, with `n=6` and `k=2`, `θ = (6 - 2) / 6 = 4/6 = 0.67`.

3. **Cooperation**: Cooperate (C) in round `t` if:
   * The number of cooperators in the previous round (`t-1`) is greater than or equal to `θ × n`.
   * Your own payoff in the previous round (`π_i,t-1`) was not significantly lower than the average payoff.

Pseudocode: `if (cooperators[t-1] >= θ × n) and (π_i,t-1 >= avg_payoff - ε) then cooperate`

4. **Gradual Withdrawal**: If the conditions for cooperation are not met, gradually withdraw from cooperation by:
   * Defecting (D) with a probability `p` that increases as the number of defectors in the previous round grows.
   * The probability `p` is calculated using a logistic function:
     `p = 1 / (1 + exp(-(defectors[t-1] - θ × n)))`

Pseudocode: `if not cooperate then defect with probability p = 1 / (1 + exp(-(defectors[t-1] - θ × n)))`

**Edge Cases**

* **Last Round**: In the final round, cooperate if the number of cooperators in the previous round was greater than or equal to `θ × n`, and your own payoff was not significantly lower than the average payoff. Otherwise, defect.
* **First Few Rounds**: If the game has fewer rounds (e.g., `r <= 5`), consider a more cooperative approach by cooperating in the first two rounds, then applying the standard decision rules.

**Collective Mindset**

ACGW aligns with the collective mindset by:

1. Encouraging cooperation when possible.
2. Gradually withdrawing from cooperation when others defect, rather than immediately switching to defection.
3. Adapting to the group's behavior and responding to changes in cooperation levels.

This strategy balances individual self-interest with collective well-being, making it a robust and adaptive approach for the N-Player Public Goods Game.
'''

description_COLLECTIVE_212 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Concession (ACGC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to set a positive tone and encourage cooperation from others.
2. **Subsequent Rounds**:
	* If the total number of cooperators in the previous round is greater than or equal to `n/2` (i.e., at least half of the players cooperated), cooperate (C) in the current round.
	* Otherwise, defect (D) with a probability proportional to the difference between the actual number of cooperators and `n/2`. This introduces an adaptive element, allowing the strategy to gradually concede when faced with insufficient cooperation.

**Pseudocode for Subsequent Rounds:**
```
def ACGC(n, k, history):
  # Get previous round's cooperation level
  prev_cooperators = sum(history[-1])  # count of C actions in last round

  if prev_cooperators >= n/2:
    return 'C'  # Cooperate when at least half cooperated previously
  else:
    concession_prob = (n/2 - prev_cooperators) / (n/2)
    return 'D' with probability concession_prob, otherwise 'C'
```
**Handling Edge Cases:**

* **Last Round**: In the final round (`r`-th round), always cooperate (C) to maximize collective payoff, as there is no future round to consider.
* **Extremely Low Cooperation**: If fewer than `n/4` players cooperated in a previous round, temporarily switch to an "exploratory" phase for one round, where the strategy defects with probability 0.5. This helps assess whether opponents are simply playing a fixed defect strategy or if they might still be open to cooperation.

**Collective Mindset:**

ACGC is designed to foster cooperation while adapting to various opponent behaviors. By cooperating initially and gradually conceding when faced with insufficient cooperation, the strategy promotes mutual benefit and avoids getting exploited by persistent defectors.

When opponents reciprocate cooperation, ACGC responds in kind, reinforcing a collective norm of cooperation. When opponents fail to cooperate sufficiently, ACGC adjusts its behavior to protect individual payoffs while still maintaining an openness to future cooperation.

**Robustness and Adaptability:**

ACGC is robust against:

* **Pure Defectors**: The strategy will gradually defect when faced with persistent non-cooperation.
* **Free Riders**: By adapting concession probabilities based on previous cooperation levels, ACGC mitigates the impact of free riders who attempt to exploit cooperative opponents.

ACGC remains adaptable and willing to cooperate in response to changes in opponent behavior, promoting an overall collective mindset that balances individual self-interest with mutual benefit.
'''

description_COLLECTIVE_213 = '''
I'll design a collective strategy that adapts to various opponent behaviors while prioritizing cooperation and robustness. Introducing "Adaptive Collective Optimism" (ACO) strategy:

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.7. This sets a positive tone for potential cooperation.
2. **Reactive Adaptation**: After the first round, observe the total number of cooperators (c_total) in the previous round. If c_total ≥ n/2, cooperate (C). Otherwise, defect (D).
3. **Punishment Mechanism**: If the average payoff in the previous round is below a certain threshold (0.5), and you defected while others cooperated, switch to cooperation (C) for the next two rounds.
4. **Exploration-Exploitation Trade-off**: Every 5th round, cooperate (C) regardless of previous outcomes. This injects occasional exploration into the strategy.

**Edge Cases:**

1. **First Round**: As mentioned earlier, cooperate with probability 0.7.
2. **Last Round**: In the final round (r), if your current total payoff is below the average of all players' payoffs, defect (D) to catch up. Otherwise, maintain the ACO strategy.
3. **Ties in Cooperation**: If c_total equals n/2 exactly, flip a coin (0.5 probability) to decide between cooperation and defection.

**Collective Mindset:**

ACO prioritizes collective well-being by:

1. Initially cooperating to set a positive tone
2. Reacting to the overall level of cooperation in the group
3. Punishing exploitation while maintaining cooperation
4. Injecting exploration to prevent stagnation

By adapting to the group's behavior, ACO promotes a balanced approach between individual and collective interests.

**Pseudocode:**
```python
def adaptive_collective_optimism(n, k, r):
    # Initialize variables
    c_total = 0
    avg_payoff_threshold = 0.5
    exploration_rounds = 5

    for round in range(r):
        if round == 0:
            cooperate_prob = 0.7
        else:
            cooperate_prob = 1 if c_total >= n / 2 else 0

        # Punishment mechanism
        if avg_payoff < avg_payoff_threshold and self.defected_last_round:
            cooperate_prob = 1
            punish_coop_countdown = 2

        # Exploration-exploitation trade-off
        if round % exploration_rounds == 0:
            cooperate_prob = 1

        action = C if random.random() < cooperate_prob else D

        # Update variables for next round
        c_total += 1 if action == C else 0
        avg_payoff = update_avg_payoff(action, k)

    return action_sequence
```
This strategy balances individual and collective interests while adapting to various opponent behaviors. ACO will perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_214 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

AC is a dynamic, history-dependent strategy that balances individual payoff maximization with collective well-being. It's designed to adapt to various opponent behaviors and converge on mutually beneficial outcomes.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage reciprocity.
2. **Contribution Tracking**: Monitor the total number of cooperators (Σc_j) in each round.
3. **Payoff-Based Adaptation**: Calculate the average payoff per cooperator (APC) in the previous round:

   APC = Σπ_i,t-1 / Σc_j,t-1

   where π_i,t-1 is player i's payoff in the previous round, and Σc_j,t-1 is the total number of cooperators.

4. **Cooperation Threshold**: Determine a cooperation threshold (θ) based on the APC:

   θ = 1 - (k/n) \* APC

5. **Action Selection**:
	* If the current payoff per cooperator (π_i,t-1) is above the cooperation threshold (θ), cooperate (C).
	* Otherwise, defect (D).

Pseudocode for action selection:
```
if π_i,t-1 > θ then
  action = C
else
  action = D
end if
```

**Edge Cases**

1. **Last Round**: In the final round, always defect (D) to maximize individual payoff.
2. **No Previous Payoff Data**: If no previous payoffs are available (e.g., first round), use a default cooperation threshold θ = 0.5.

**Collective Mindset**

AC is designed to promote collective well-being by:

1. Encouraging initial cooperation to establish a cooperative tone.
2. Adapting to the group's behavior and payoff dynamics.
3. Balancing individual payoff maximization with collective benefits.

By using a dynamic, history-dependent approach, AC aims to create an environment where cooperation can thrive, while also being robust against exploitative strategies.

**Implementation Notes**

When implementing AC as an algorithm, consider the following:

1. Store previous round data (π_i,t-1, Σc_j,t-1) for each player.
2. Update the cooperation threshold θ after each round based on the APC calculation.
3. Use a consistent method to calculate the APC and update the cooperation threshold.

AC is designed to be adaptive and robust in the face of varying opponent behaviors. By aligning with a collective mindset, AC aims to achieve mutually beneficial outcomes while minimizing exploitation risks.
'''

description_COLLECTIVE_215 = '''
I'll outline a collective strategy that is adaptive, robust, and aligns with the collective mindset.

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a baseline level of cooperation.
2. **Majority Rule**: In subsequent rounds, cooperate if the majority of players (> n/2) cooperated in the previous round. Otherwise, defect (D).
3. **Selfish Adjustment**: If your payoff from the previous round is less than the average payoff of all players, adjust your strategy to match the most common action taken by other players in that round.
4. **Defection Response**: If you observe a player defecting for two consecutive rounds, defect against them in the next round.

**Pseudocode:**
```python
def ACC(n, k, history):
  # Initialize cooperation rate and previous actions
  cooperate_rate = 0.5
  prev_actions = [None] * n

  for t in range(1, r+1):  # Round loop
    if t == 1:  # First round
      action = 'C'
    else:
      # Count cooperators in previous round
      num_coop_prev_round = sum([1 if a == 'C' else 0 for a in prev_actions])
      
      # Apply Majority Rule and Selfish Adjustment
      if num_coop_prev_round > n/2:
        action = 'C'
      elif num_coop_prev_round < n/2 and ACC_payoff(history) < avg_payoff(history):
        action = most_common_action(prev_actions)
      else:
        action = 'D'

      # Defection Response
      for i in range(n):
        if prev_actions[i] == 'D' and history[t-1][i] == 'D':
          action_against_i = 'D'
          break

    # Update cooperation rate and previous actions
    cooperate_rate = (cooperate_rate * (t-1) + (action == 'C')) / t
    prev_actions = history[t]

  return action

def ACC_payoff(history):
  # Calculate payoff for ACC player in the given round
  pass

def avg_payoff(history):
  # Calculate average payoff of all players in the given round
  pass

def most_common_action(actions):
  # Return the most common action in the list
  pass
```
**Edge Cases:**

* **Last Round**: Cooperate if the majority cooperated in the previous round, otherwise defect.
* **Ties**: In case of a tie in the number of cooperators, cooperate.

This strategy is collective because it:

1. Encourages cooperation by cooperating in the first round and when the majority cooperates.
2. Adapts to changing opponent behaviors by adjusting its strategy based on observed actions and payoffs.
3. Responds to defectors by defecting against them, discouraging exploitation.

ACC should perform well in a tournament setting, as it balances individual self-interest with collective cooperation and adaptability.
'''

description_COLLECTIVE_216 = '''
**Collective Strategy: Adaptive Cooperation with Retaliation (ACR)**

**Overview**

The ACR strategy is designed to promote cooperation while being robust against various opponent behaviors. It balances individual self-interest with collective benefits by adapting to the game's history and its own performance.

**Decision Rules**

1. **Initial Rounds**: In the first few rounds (e.g., 3-5), cooperate (C) unconditionally to establish a cooperative foundation and gather information about opponents' behavior.
2. **Cooperation Threshold**: Introduce a cooperation threshold, `τ`, which represents the minimum number of cooperators required for an individual to cooperate in the next round. Initialize `τ` as half the total number of players (`n/2`).
3. **Adaptive Cooperation**: For each subsequent round:
	* If the number of cooperators in the previous round is greater than or equal to `τ`, cooperate (C) in the current round.
	* Otherwise, defect (D) in the current round.
4. **Retaliation Mechanism**: Implement a retaliation mechanism to deter opponents from exploiting cooperation. If an opponent defects while others cooperate, temporarily adjust `τ` downward by 1 for that specific opponent in the next round.

Pseudocode:
```markdown
# Initialize variables
n = total number of players
k = multiplication factor
r = number of rounds
tau = n/2  # initial cooperation threshold

# First few rounds: unconditional cooperation
for t in range(3-5):
    action[t] = C

# Adaptive cooperation with retaliation
for t in range(r):
    if t >= 3-5:
        num_cooperators = count cooperators in previous round (t-1)
        if num_cooperators >= tau:
            action[t] = C
        else:
            action[t] = D
        
        # Retaliation mechanism
        for opponent in opponents:
            if opponent defected in previous round and others cooperated:
                tau_opponent = tau - 1
```
**Handling Edge Cases**

* **Last Round**: In the final round, always defect (D) to maximize individual payoff.
* **Tie-Breaking**: In cases where multiple actions have equal payoffs, choose cooperation (C) to promote collective benefits.

**Collective Mindset**

The ACR strategy prioritizes collective well-being by:

1. Encouraging initial cooperation to establish a foundation for mutual benefit.
2. Adapting to the game's history and adjusting the cooperation threshold accordingly.
3. Implementing retaliation mechanisms to deter opponents from exploiting cooperation.

By following these decision rules, the ACR strategy promotes cooperation while being robust against various opponent behaviors, making it an effective collective strategy in the N-Player Public Goods Game.
'''

description_COLLECTIVE_217 = '''
**Collective Strategy: Adaptive Cooperation with History-Driven Threshold (ACHT)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds**: Use the following threshold-based approach:
	* Calculate the **cooperation rate** (CR) of all players in the previous round: CR = (number of cooperators) / n
	* If CR ≥ **threshold** (TH), cooperate (C) in the current round; otherwise, defect (D)
3. **Threshold Update**: After each round, update TH based on the observed cooperation rate:
	* If CR > 0.5, decrease TH by 0.05 (to encourage more cooperation)
	* If CR < 0.5, increase TH by 0.05 (to adapt to lower cooperation rates)

**Pseudocode:**
```python
def ACHT(n, k, r):
    # Initialize variables
    threshold = 0.5  # initial threshold value
    history = []  # store cooperation rates for previous rounds

    for t in range(1, r+1):  # loop through rounds
        if t == 1:  # first round
            action = 'C'  # cooperate
        else:
            # calculate cooperation rate (CR) from previous round
            CR = sum(history[-1]) / n

            # update threshold (TH)
            if CR > 0.5:
                threshold -= 0.05
            elif CR < 0.5:
                threshold += 0.05

            # decide action based on updated TH and CR
            if CR >= threshold:
                action = 'C'  # cooperate
            else:
                action = 'D'  # defect

        # store current round's cooperation rate
        history.append([1 if a == 'C' else 0 for a in actions])

    return actions
```
**Collective Mindset:**

ACHT aims to balance individual self-interest with collective well-being. By adapting the threshold based on observed cooperation rates, ACHT encourages cooperation when possible while avoiding excessive exploitation. This strategy promotes a culture of reciprocity and mutual support, fostering a more cooperative environment.

**Edge Cases:**

* **Last Round**: The decision rule remains the same as for other rounds, ensuring consistency in behavior.
* **Opponent Defection**: If an opponent defects, ACHT will adapt by increasing the threshold, making it less likely to cooperate with that opponent in future rounds.
* **Multiple Opponents with Different Strategies**: ACHT's adaptive nature allows it to respond effectively to a range of strategies, from fully cooperative to fully defective.

By using history-driven thresholds, ACHT provides a robust and collective strategy for the N-Player Public Goods Game.
'''

description_COLLECTIVE_218 = '''
I propose a collective strategy called "Adaptive Reciprocal Altruism" (ARA) that balances individual self-interest with a willingness to cooperate for the greater good.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally to establish a cooperative tone and encourage others to follow suit.
2. **Reciprocity**: For rounds 2 to r-1, use the following rule:
	* If the majority of players (> n/2) cooperated in the previous round, cooperate (C).
	* Otherwise, defect (D).
3. **Adaptive Adjustment**: Monitor the overall cooperation level and adjust the strategy as follows:
	* If the average cooperation rate over the last m rounds (where m = min(5, r)) is above a threshold θ (e.g., 0.6), increase the likelihood of cooperation by δ (e.g., 0.1) in the next round.
	* Conversely, if the average cooperation rate falls below θ, decrease the likelihood of cooperation by δ.

Pseudocode:
```python
def ARA(n, r, k):
    # Initialize variables
    cooperate = True  # Initial cooperation
    majority_cooperated = False
    avg_cooperation_rate = 0.0
    threshold = 0.6
    adjustment = 0.1

    for round in range(r):
        if round == 0:
            action = 'C'  # Initial cooperation
        else:
            # Reciprocity and adaptive adjustment
            majority_cooperated = sum(cooperation_history) / n > 0.5
            avg_cooperation_rate = sum(cooperation_history[-min(5, r):]) / min(5, r)
            if avg_cooperation_rate >= threshold:
                cooperate += adjustment
            else:
                cooperate -= adjustment

            action = 'C' if majority_cooperated and cooperate > random.random() else 'D'

        # Update cooperation history
        cooperation_history.append(action == 'C')

    return actions
```
**Handling Edge Cases:**

* In the first round, always cooperate (C) to establish a cooperative tone.
* In the last round, use the same decision rule as in previous rounds; there is no special case for the final round.

**Collective Mindset:**

The ARA strategy aligns with a collective mindset by:

1. Cooperating initially to encourage others to do so.
2. Reciprocating cooperation when the majority of players cooperate, promoting a norm of mutual support.
3. Adapting to changes in the overall cooperation level, ensuring that individual self-interest is balanced with collective well-being.

By following this strategy, ARA encourages cooperation while being robust to various opponent behaviors and adapting to the evolving game dynamics.
'''

description_COLLECTIVE_219 = '''
I'll design a collective strategy for this N-Player Public Goods Game that adapts to various opponent behaviors while aligning with the collective mindset. Here's my proposal:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage others to contribute and create a positive environment.
2. **Early Rounds (2 ≤ t < r/2)**:
	* If the average cooperation rate (across all players) is above 50% in the previous round, cooperate (C).
	* Otherwise, defect (D).
3. **Mid-Game Adjustment (t = r/2)**: Evaluate the overall cooperation level and adjust your strategy accordingly.
	+ If the total number of cooperators has increased over time, continue cooperating (C) to reinforce positive behavior.
	+ If the number of cooperators has decreased or remained stable, switch to a more cautious approach:
		- Cooperate (C) if at least 50% of players cooperated in the previous round.
		- Defect (D) otherwise.
4. **Late Rounds (t > r/2)**: As the game nears its end, prioritize individual gain while still considering collective welfare:
	* If your current payoff is below the average payoff of all players, defect (D) to catch up.
	* Otherwise, cooperate (C) to maintain a positive environment and maximize overall payoffs.

**Pseudocode**:

```
// Initialize variables
avg_coop_rate = 0; // Average cooperation rate across all players
total_cooperators = 0; // Total number of cooperators in the previous round

// First Round
if (round == 1) {
  action = COOPERATE;
}

// Early Rounds
else if (round < r/2) {
  if (avg_coop_rate > 0.5) {
    action = COOPERATE;
  } else {
    action = DEFECT;
  }
}

// Mid-Game Adjustment
else if (round == r/2) {
  if (total_cooperators >= previous_total_cooperators) {
    action = COOPERATE; // Continue cooperating to reinforce positive behavior
  } else {
    if (avg_coop_rate > 0.5) {
      action = COOPERATE;
    } else {
      action = DEFECT;
    }
  }
}

// Late Rounds
else {
  if (my_payoff < avg_payoff_of_all_players) {
    action = DEFECT; // Catch up with individual gain
  } else {
    action = COOPERATE; // Maintain positive environment and maximize overall payoffs
  }
}
```

**Edge Cases:**

* In the last round, prioritize individual gain by defecting (D) if your current payoff is below the average payoff of all players. Otherwise, cooperate (C) to maintain a positive environment.
* If an opponent consistently defects (D), adapt by defecting (D) as well to minimize losses.

**Collective Mindset:**

The Adaptive Collective Optimism strategy aims to balance individual gain with collective welfare. By initially cooperating and adapting to the cooperation level of others, ACO encourages a positive environment and promotes mutual benefits. In the later stages of the game, ACO prioritizes individual gain while still considering the overall payoffs, ensuring that no single player can exploit others.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it:

1. Adapts to various opponent behaviors
2. Aligns with the collective mindset by promoting mutual benefits and positive behavior
3. Balances individual gain with collective welfare

Let me know if you'd like me to clarify any aspect of this strategy!
'''

description_COLLECTIVE_220 = '''
**Collective Strategy: Adaptive Cooperation with Social Learning**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage mutual cooperation and establish a positive tone for the game.
2. **Subsequent Rounds (t>1):** Observe the previous round's outcome and adjust strategy based on the following conditions:
	* If the number of cooperators in the previous round is above the threshold (`k/n`), cooperate (C) to maintain and build upon the collective benefit.
	* If the number of cooperators is below or equal to the threshold, defect (D) to minimize losses and encourage others to re-evaluate their strategy.
3. **Adaptive Threshold Adjustment:** Update the threshold value based on the game's history:
	+ Increase the threshold by 1 if the collective payoff in the previous round was above the average payoff for the last `r/2` rounds.
	+ Decrease the threshold by 1 if the collective payoff in the previous round was below the average payoff for the last `r/2` rounds.

**Edge Case Handling:**

* **Last Round (t=r):** Cooperate (C) to maximize the final collective payoff, as there is no future opportunity to adapt or influence others.
* **Ties:** In cases where the number of cooperators equals the threshold, cooperate (C) to promote cooperation and avoid a downward spiral.

**Collective Mindset:**

This strategy prioritizes cooperation when it benefits the group and adapts to changes in the game's dynamics. By observing the previous round's outcome and adjusting the threshold, the strategy promotes social learning and encourages others to cooperate. The adaptive nature of the threshold allows the strategy to respond to various opponent behaviors and maintain a balance between individual and collective interests.

**Pseudocode:**
```python
def adaptive_cooperation(n, k, r, history):
    # Initialize variables
    threshold = k / n
    last_round_payoff = 0

    for t in range(1, r + 1):
        if t == 1:
            # Initial round: Cooperate
            action = 'C'
        else:
            # Observe previous round's outcome and adjust strategy
            num_cooperators = history[t - 1]['cooperators']
            if num_cooperators > threshold:
                action = 'C'  # Cooperate to maintain collective benefit
            else:
                action = 'D'  # Defect to minimize losses

        # Update threshold based on game history
        if t >= r / 2:
            avg_payoff_last_half = sum(history[t - r // 2:t]['payoffs']) / (r // 2)
            if last_round_payoff > avg_payoff_last_half:
                threshold += 1
            elif last_round_payoff < avg_payoff_last_half:
                threshold -= 1

        # Store current round's outcome and update variables
        history[t] = {'action': action, 'payoffs': calculate_payoffs(n, k, action)}
        last_round_payoff = history[t]['payoffs'][0]

    return history
```
This strategy will be implemented as an algorithm to play in the tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_221 = '''
**Collective Strategy: Adaptive Cooperation with History-Dependent Adjustment (ACHDA)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to initiate a cooperative tone and gather information about opponents' behaviors.
2. **History-Based Adaptation**: For rounds 2 to r-1:
	* If the total number of cooperators in the previous round is greater than or equal to n/2, cooperate (C).
	* Otherwise, defect (D).
3. **Final Round Adjustment**: In the last round (r), if the total number of cooperators in the second-to-last round is less than n/2, cooperate (C) to potentially increase the overall payoff.

**Edge Cases:**

1. **First Round**: Cooperate (C) as per initial cooperation rule.
2. **Last Round**: Apply final round adjustment rule.
3. **Opponent Cooperation**: If an opponent cooperates in a previous round, consider it a positive signal and adjust strategy accordingly.
4. **Consecutive Defections**: If an opponent defects consecutively for more than 2 rounds, assume they are non-cooperative and adjust strategy to defect (D) in response.

**Collective Mindset:**

ACHDA prioritizes cooperation when there is evidence of sufficient collective cooperation, while adapting to opponents' behaviors. This approach balances individual interests with the need for collective action to achieve higher payoffs.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
r = number of rounds
k = multiplication factor
history = array to store previous round's actions and payoffs

# First round
if current_round == 1:
    cooperate(C)

# Subsequent rounds
else:
    total_cooperators_prev_round = count cooperators in history[current_round - 1]
    
    if total_cooperators_prev_round >= n / 2:
        cooperate(C)
    else:
        defect(D)

# Last round adjustment
if current_round == r:
    total_cooperators_second_last_round = count cooperators in history[r - 1]
    
    if total_cooperators_second_last_round < n / 2:
        cooperate(C)
```
ACHDA is designed to be robust and adaptive, responding to the evolving behavior of opponents while promoting collective cooperation. Its simplicity and reliance on observable game history make it a viable strategy for a wide range of opponent behaviors in this N-player public goods game.
'''

description_COLLECTIVE_222 = '''
I propose a collective strategy called "Adaptive Cooperation with Memory" (ACM) that balances individual self-interest with collective well-being. ACM is designed to adapt to various opponent behaviors and promote cooperation while being robust against exploitation.

**Decision Rules:**

1. **Initial Rounds:** In the first round, cooperate (C). This sets a positive tone and encourages others to cooperate.
2. **Memory-Based Cooperation:** For rounds 2 to r-1, use the following rules:
	* If the total number of cooperators in the previous round is greater than or equal to n/2, cooperate (C).
	* Otherwise, defect (D).
3. **Last Round:** In the final round (r), defect (D). This ensures that we don't contribute unnecessarily when there's no future benefit.

**Adaptive Component:**

1. **Memory Update:** After each round, update a memory variable M to track the average number of cooperators over the past w rounds (where 2 ≤ w < r is a fixed window size).
2. **Threshold Adjustment:** Adjust the cooperation threshold based on the memory value M:
	* If M > n/2 + Δ, increase the threshold by δ (i.e., require more cooperators to cooperate). Otherwise, decrease it by δ.
	* Δ and δ are small positive values that control the adaptation rate.

**Pseudocode:**
```python
def ACM(n, k, r):
    # Initialize memory and threshold
    M = 0
    threshold = n/2

    for t in range(r):
        if t == 0:
            action = C  # Cooperate in first round
        else:
            total_cooperators_prev_round = sum([c_i for c_i in actions_prev_round])
            if total_cooperators_prev_round >= threshold:
                action = C
            else:
                action = D

        # Update memory and threshold
        M = (M * (w-1) + total_cooperators_prev_round) / w
        if M > n/2 + Δ:
            threshold += δ
        elif M < n/2 - Δ:
            threshold -= δ

        # Last round: defect
        if t == r-1:
            action = D

        return action
```
**Collective Mindset:**

ACM promotes a collective mindset by:

1. Encouraging cooperation in the early rounds to establish a positive tone.
2. Adapting to the behavior of others, rewarding cooperation and punishing defection.
3. Maintaining a memory of past interactions to adjust its strategy.

By using ACM, we balance individual self-interest with collective well-being, making it an effective strategy for achieving high payoffs in the N-Player Public Goods Game.
'''

description_COLLECTIVE_223 = '''
Here is a collective strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Collective Conscience (ACC)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = k/n. This initial cooperation encourages early contributions to the public good.
2. **History-Based Cooperation**: For rounds t > 1, calculate the average contribution rate of all players in the previous round (t-1): avg_contrib_t_1 = Σ(j=1 to n) c_j,t-1 / n.
3. **Contribution Threshold**: Define a threshold θ = k/n. If avg_contrib_t_1 ≥ θ, cooperate (C) with probability p_coop = 1 - (avg_contrib_t_1 - θ). Otherwise, defect (D).
4. **Punishment Mechanism**: If the number of cooperators in the previous round is less than or equal to n/2, and the average contribution rate is below θ, defect (D) with probability p_punish = 0.5.

**Edge Case Handling:**

1. **Last Round**: In the last round (t=r), cooperate (C) if avg_contrib_t-1 ≥ θ; otherwise, defect (D).
2. **Tiebreaker**: If multiple players are tied in terms of average contribution rate, prioritize cooperation over defection to maintain a positive collective outcome.
3. **Missing Data**: If there is missing data or no previous round information available, default to the initial cooperation probability p_init = k/n.

**Collective Mindset Alignment:**

1. **Emphasize Early Contributions**: By cooperating initially with a high probability, ACC encourages early investments in the public good, creating a positive feedback loop.
2. **Adaptation and Learning**: The strategy adapts to the evolving average contribution rate, ensuring that cooperation levels adjust to the changing environment.
3. **Social Norm Enforcement**: The punishment mechanism discourages exploitation by defecting when others are not contributing enough, promoting a culture of mutual support.

**Pseudocode:**

```
function AdaptiveCollectiveConscience(n, k, r):
  // Initialize variables
  avg_contrib_prev = 0
  theta = k/n

  for t in range(1, r+1):
    if t == 1:
      cooperate_prob = k/n
    else:
      // Calculate average contribution rate of previous round
      avg_contrib_t_1 = Σ(j=1 to n) c_j,t-1 / n
      // Update theta and cooperation probability based on history
      if avg_contrib_t_1 >= theta:
        cooperate_prob = 1 - (avg_contrib_t_1 - theta)
      else:
        cooperate_prob = p_punish = 0.5

    // Make decision based on probability
    if random() < cooperate_prob:
      action = C
    else:
      action = D

    // Update history and variables for next round
    avg_contrib_prev = avg_contrib_t_1
```

This strategy balances individual self-interest with collective well-being by adapting to the evolving environment, promoting early contributions, and enforcing social norms.
'''

description_COLLECTIVE_224 = '''
**Collective Strategy: "Adaptive Cooperative Foresight" (ACF)**

The ACF strategy is designed to balance individual self-interest with collective well-being while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Round**: In the first round, play Cooperate (C). This sets a cooperative tone and allows for observation of others' initial actions.
2. **General Rule**: For each subsequent round t, calculate the average cooperation rate among all players in the previous round (t-1). If this rate is above a threshold θ (determined by game parameters), play Cooperate (C) in round t. Otherwise, play Defect (D).
3. **Foresight Mechanism**: Introduce a "foresight" component to anticipate potential cooperation breakdowns. Calculate the difference between the current average cooperation rate and the historical maximum cooperation rate observed so far. If this difference exceeds a threshold δ (determined by game parameters), switch from Cooperate to Defect, or vice versa.

**Threshold Determination:**

* θ (cooperation threshold): Set θ = 0.5 + (k/n) / 2, which balances the individual incentive to cooperate with the collective benefit.
* δ (foresight threshold): Set δ = 0.2, allowing for a moderate response to changes in cooperation rates.

**Edge Cases:**

1. **Last Round**: In the final round, play Defect (D), as there is no future opportunity to reciprocate cooperation.
2. **All Players Defected Previously**: If all players defected in the previous round, play Cooperate (C) to attempt to revive cooperation.

**Collective Mindset:**

ACF prioritizes collective well-being by:

1. Encouraging initial cooperation to establish a positive tone.
2. Adapting to changing cooperation rates to maintain a balanced approach.
3. Anticipating potential breakdowns in cooperation through the foresight mechanism.

By incorporating both individual self-interest and collective considerations, ACF aims to promote cooperation while remaining robust against various opponent behaviors.

Pseudocode:
```
function ACF(n, k, r):
  θ = 0.5 + (k/n) / 2
  δ = 0.2
  avg_coop_rate = 0
  max_coop_rate = 0

  for t in range(r):
    if t == 0:
      action = 'C'
    else:
      prev_avg_coop_rate = avg_coop_rate
      avg_coop_rate = calculate_average_cooperation_rate(t-1)
      diff = abs(avg_coop_rate - max_coop_rate)

      if avg_coop_rate > θ and diff < δ:
        action = 'C'
      elif avg_coop_rate <= θ or diff >= δ:
        action = 'D'

    if t == r - 1:  // Last round
      action = 'D'

    if all_players_defected(t-1):
      action = 'C'

    play(action)
```
This strategy will be implemented as an algorithm to participate in the tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_225 = '''
I propose a collective strategy called "Adaptive Reciprocal Altruism" (ARA). ARA is designed to balance individual self-interest with collective well-being, adapting to various opponent behaviors and game histories.

**Decision Rules:**

1. **Initial Rounds:** In the first few rounds (specifically, `min(5, r/2)` rounds), play Cooperate (C) to seed a cooperative environment and gather information about opponents' behavior.
2. **Reciprocal Altruism:** After the initial rounds, use the following decision rule:
	* If the majority of players (> 50%) cooperated in the previous round, play C.
	* Otherwise, play Defect (D).
3. **Punishment Mechanism:** To deter free-riding, implement a punishment mechanism:
	* If an opponent defects while you cooperate, mark them as "defector" and play D against them for the next `max(2, k)` rounds.
	* After the punishment period, re-evaluate their behavior using the reciprocal altruism rule.
4. **Self-Interest Override:** To prevent excessive exploitation by opponents, introduce a self-interest override:
	* If your payoff in the previous round was less than `(k/n) * (n/2)`, play D for the next `max(2, k)` rounds to protect your own interests.

**Handling Edge Cases:**

1. **Last Round:** In the final round (`t = r`), play C if the majority of players cooperated in the previous round; otherwise, play D.
2. **Tiebreakers:** In case of a tie (e.g., 50% cooperation), use a randomization mechanism to break ties (e.g., coin flip).
3. **Opponent Behavior Patterns:** If an opponent exhibits a consistent pattern of behavior (e.g., always defecting or cooperating), adapt by playing the opposite action in response.

**Collective Mindset:**

ARA aligns with the collective mindset by:

1. Initially promoting cooperation to create a positive environment.
2. Adapting to the majority's behavior, which encourages cooperation and discourages free-riding.
3. Punishing defectors to maintain fairness and reciprocity.
4. Prioritizing self-interest when necessary to prevent exploitation.

**Pseudocode:**
```python
def adaptive_reciprocal_altruism(n, k, r):
  # Initialize variables
  history = []
  opponents = {}
  
  for t in range(r):
    if t < min(5, r/2):  # Initial rounds
      action = C
    else:
      majority_cooperated = sum(history[-1]) / n > 0.5
      if majority_cooperated:
        action = C
      else:
        action = D
      
      # Punishment mechanism
      for opponent, behavior in opponents.items():
        if behavior == D and action == C:
          opponents[opponent] = 'defector'
          action = D
          break
      
      # Self-interest override
      if t > 0 and history[-1][self_index] < (k/n) * (n/2):
        action = D
    
    # Record opponent behavior
    for i, opponent_action in enumerate(history[-1]):
      opponents[i] = opponent_action
    
    # Update history
    history.append(action)
  
  return history
```
This strategy is designed to be robust and adaptive, responding to various opponent behaviors while promoting collective well-being.
'''

description_COLLECTIVE_226 = '''
I'll outline a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and depends solely on game parameters and history.

**Strategy Name:** "Adaptive Collective Optimizer" (ACO)

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
	* Cooperate (C) in the first round to encourage others to cooperate.
	* In rounds 2-3, play a mixed strategy:
		+ With probability p=0.5, cooperate (C).
		+ With probability p=0.5, defect (D).
	* This initial exploration phase helps gather information about opponents' behaviors.
2. **Adaptive Phase (Rounds 4-r):**
	* Calculate the average cooperation rate (ACR) among all players in the previous round:
		+ ACR = (∑c_j) / n, where c_j is 1 if player j cooperated and 0 otherwise.
	* If ACR ≥ k/n, cooperate (C). This encourages continued cooperation when it's beneficial.
	* If ACR < k/n, defect (D). This protects against exploitation when cooperation is low.
3. **Reaction to Deviation:**
	* Monitor the opponent's deviation from the expected cooperation rate:
		+ Calculate the difference between the opponent's actual cooperation rate and the average cooperation rate (ACR).
	* If an opponent deviates significantly (> 0.2) from ACR, adjust your strategy for that player:
		+ Temporarily switch to a more cautious approach ( defecting with higher probability) until the opponent returns to a cooperative behavior.

**Edge Cases:**

1. **Last Round (r):**
	* Cooperate (C) if ACR ≥ k/n in the previous round.
	* Defect (D) otherwise, as there's no future opportunity for reciprocity.
2. **Players Leaving or Joining:**
	* If a player leaves or joins mid-game, re-calculate ACR and adjust your strategy accordingly.

**Collective Mindset:**

ACO prioritizes collective welfare by:

1. Encouraging cooperation when beneficial (ACR ≥ k/n).
2. Protecting against exploitation when cooperation is low (ACR < k/n).
3. Adapting to deviations in opponent behavior.
4. Fostering a stable, cooperative environment through conditional reciprocity.

Pseudocode:
```python
def ACO(n, r, k):
    # Initialize variables
    acr = 0  # Average cooperation rate
    opponent_acr = [0] * n  # Opponent-specific cooperation rates

    for round in range(1, r+1):
        if round <= 3:  # Initial exploration phase
            action = random.choice(['C', 'D'])
        else:
            acr = calculate_acr(opponent_actions)
            if acr >= k/n:
                action = 'C'
            else:
                action = 'D'

        # React to deviation (optional)
        for opponent in range(n):
            deviation = abs(acr - opponent_acr[opponent])
            if deviation > 0.2:
                # Temporarily switch to a more cautious approach

        return action
```
Note that this strategy focuses on adapting to the collective behavior rather than trying to enforce a specific cooperation schedule or pattern. ACO's robustness and adaptability make it suitable for playing against various independent strategies in a tournament setting.
'''

description_COLLECTIVE_227 = '''
I'll outline a collective strategy that adapts to the game parameters and history. This strategy is designed to promote cooperation while being robust against various opponent behaviors.

**Strategy Name: Adaptive Collective Optimism (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This encourages cooperation without assuming others will follow suit.
2. **Reciprocal Cooperation**: If a player i contributed in the previous round and at least one other player also contributed, then player i cooperates again in the current round. This promotes reciprocal behavior and reinforces cooperation.
3. **Punish Defection**: If a player i contributed in the previous round but no other player contributed, or if another player defected while player i cooperated, then player i defects (D) in the current round. This discourages exploitation and teaches opponents that defection has consequences.
4. **Gradual Cooperation**: As the game progresses, ACO gradually increases its cooperation rate based on the average cooperation level of all players in previous rounds. Specifically:
	* If the average cooperation level is below 0.5, cooperate with probability 0.3 + (average_cooperation_level \* 0.2).
	* If the average cooperation level is above or equal to 0.5, cooperate with probability 0.7 - (1 - average_cooperation_level) \* 0.2.
5. **Endgame Cooperation**: In the last round, ACO cooperates if and only if at least one other player has contributed in any previous round. This ensures that the strategy doesn't defect when it's clear others have been cooperative.

**Pseudocode:**
```markdown
ACO(player_id, history):
  # Initialize cooperation probability for first round
  if round == 1:
    cooperate_prob = 0.5

  # Reciprocal cooperation
  elif prev_round_contribution[player_id] and any(prev_round_contribution[j] for j != player_id):
    cooperate_prob = 1

  # Punish defection
  elif prev_round_contribution[player_id] and not any(prev_round_contribution[j] for j != player_id):
    cooperate_prob = 0

  # Gradual cooperation
  else:
    avg_cooperation_level = sum(prev_round_contribution) / num_players
    if avg_cooperation_level < 0.5:
      cooperate_prob = 0.3 + (avg_cooperation_level * 0.2)
    else:
      cooperate_prob = 0.7 - (1 - avg_cooperation_level) * 0.2

  # Endgame cooperation
  if round == num_rounds:
    cooperate_prob = any(prev_round_contribution[j] for j != player_id)

  return random_choice([C, D], p=[cooperate_prob, 1-cooperate_prob])
```
**Collective Mindset:**

ACO is designed to promote a collective mindset by:

* Encouraging cooperation through reciprocal behavior and gradual increases in cooperation rate
* Discouraging exploitation through punishment of defection
* Fostering trust by cooperating in the endgame if others have shown cooperative tendencies

By adapting to the game parameters and history, ACO aims to achieve a mutually beneficial outcome for all players.
'''

description_COLLECTIVE_228 = '''
**Collective Strategy: Adaptive Cooperate-Defect (ACD)**

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) with a probability of 0.5. This allows for exploration and sets the stage for adaptation.
2. **Subsequent Rounds**: For each subsequent round t, calculate the average cooperation rate (ACR) of all players in the previous round (t-1):

   ACR = (∑(c_j)) / n

   where c_j is 1 if player j cooperated and 0 otherwise.

3. **Cooperation Threshold**: Set a cooperation threshold (CT) based on the multiplication factor k:

   CT = k/n

4. **Decision Logic**:
	* If ACR ≥ CT, cooperate (C).
	* Otherwise, defect (D).

**Edge Cases:**

1. **Last Round**: In the last round, always defect (D). This ensures that no player can exploit others by cooperating in the final round.
2. **Ties**: In case of a tie in ACR, use a random choice between cooperate and defect with equal probability.

**Collective Mindset:**

The Adaptive Cooperate-Defect strategy is designed to promote collective cooperation while adapting to various opponent behaviors. By considering the average cooperation rate and adjusting the cooperation threshold based on the multiplication factor k, ACD encourages cooperation when it benefits the group and defects when others exploit the system.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
ACR = 0.5 (initial average cooperation rate)
CT = k/n (cooperation threshold)

# Loop through each round
for t in range(1, r+1):
    # First round: random choice between cooperate and defect
    if t == 1:
        c_i = random.choice([C, D])
    
    # Subsequent rounds: adapt based on ACR and CT
    else:
        # Calculate average cooperation rate (ACR) for previous round
        ACR = (∑(c_j)) / n
        
        # Cooperate if ACR ≥ CT; defect otherwise
        if ACR >= CT:
            c_i = C
        else:
            c_i = D
    
    # Last round: always defect
    if t == r:
        c_i = D
    
    # Update ACR and CT for next round (if applicable)
    if t < r:
        ACR = (∑(c_j)) / n
        CT = k/n

# Return action for current round
return c_i
```
This strategy is designed to be adaptive, robust, and collective, making it suitable for a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_229 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy, "Adaptive Cooperation," aims to balance individual self-interest with the benefits of cooperation, while being robust to various opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is greater than or equal to the multiplication factor (k), cooperate (C).
	* Otherwise, defect (D).
3. **Additional Condition:** If the opponent's cooperation rate (i.e., the proportion of times an opponent has cooperated) is above a certain threshold (e.g., 0.7), cooperate (C). This condition helps to foster cooperation with opponents who have demonstrated a willingness to cooperate.
4. **Punishment Mechanism:** If an opponent defects while the collective payoff is high (i.e., many players are cooperating), defect (D) in the next round to punish them and discourage future defection.

**Edge Cases:**

1. **Last Round (t=r):** Defect (D) to maximize individual payoff, as cooperation will not affect future rounds.
2. **Opponent's First Round:** Cooperate (C) if an opponent enters the game in a round after the first, to encourage them to cooperate.

**Collective Mindset:**

Our strategy prioritizes cooperation when it is likely to benefit the collective and punishes opponents who exploit cooperative behavior. By doing so, we aim to create an environment that encourages cooperation while protecting individual interests.

Pseudocode:
```python
def adaptive_cooperation(n, k, r, opponent_history):
    if current_round == 1:  # Initial Round
        return C
    
    total_cooperators = sum(opponent_history[:, -1])  # Get previous round's cooperators
    if total_cooperators >= k:
        return C
    
    opponent_cooperation_rate = np.mean(opponent_history)
    if opponent_cooperation_rate > 0.7:  # Additional Condition
        return C
    
    collective_payoff = calculate_collective_payoff(n, k, r)
    if collective_payoff is high and opponent_defected:
        return D  # Punishment Mechanism
    
    return D

def calculate_collective_payoff(n, k, r):
    # Calculate the total payoff for all players over all rounds
    pass
```
This strategy will adapt to various opponent behaviors while prioritizing cooperation when it benefits the collective. Its robustness and simplicity make it an effective competitor in a tournament setting.
'''

description_COLLECTIVE_230 = '''
**Collective Strategy: "Adaptive Collective Cooperation" (ACC)**

The ACC strategy aims to balance individual self-interest with collective well-being by adapting to the evolving game dynamics.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to initiate a cooperative atmosphere and gather information about other players' behaviors.
2. **Early Rounds**: For rounds 2 to `r/3`, play C if at least half of the players cooperated in the previous round; otherwise, play D. This allows us to assess the group's willingness to cooperate.
3. **Middle Game**: After the early rounds, calculate the average cooperation rate (ACR) for all players over the past `r/2` rounds. If ACR is above 0.5, continue playing C; otherwise, switch to D.
4. **Late Rounds**: In the last `r/3` rounds, play C if at least two-thirds of the players cooperated in the previous round; otherwise, play D. This encourages cooperation as the game nears its end.

**Adaptive Mechanisms:**

1. **Cooperation Threshold (CT)**: Calculate CT as the average number of cooperators required to achieve a payoff of 2k/n (twice the minimum required for a cooperative outcome). Adjust CT based on the group's past behavior.
2. **Reputation System**: Assign a reputation score (RS) to each player, initialized at 0. For each round:
	* Increase RS by 1 if a player cooperates and the group payoff is above average; decrease RS by 1 otherwise.
	* If a player's RS falls below -2, they are considered uncooperative, and we defect against them in future rounds.

**Edge Cases:**

1. **Ties**: In case of ties (e.g., equal number of cooperators and defectors), play C to maintain cooperation momentum.
2. **Last Round**: Cooperate if the group's ACR is above 0.5; otherwise, defect.

**Pseudocode:**
```python
def ACC(n, r, k):
    # Initialize variables
    CT = n / 2  # Cooperation Threshold
    RS = {i: 0 for i in range(1, n+1)}  # Reputation System

    # Early Rounds (2 to r/3)
    for t in range(1, r//3 + 1):
        if sum(C_t-1) >= n / 2:
            action[t] = C
        else:
            action[t] = D

    # Middle Game
    for t in range(r//3 + 1, 2*r//3):
        ACR = sum([sum(C_s) for s in range(t - r//2, t)]) / (r // 2)
        if ACR > 0.5:
            action[t] = C
        else:
            action[t] = D

    # Late Rounds (last r/3 rounds)
    for t in range(2*r//3 + 1, r):
        if sum(C_t-1) >= 2*n / 3:
            action[t] = C
        else:
            action[t] = D

    # Reputation System updates
    for t in range(r):
        for i in range(1, n+1):
            if payoff[i, t] > average_payoff and RS[i] >= -2:
                RS[i] += 1
            elif payoff[i, t] <= average_payoff:
                RS[i] -= 1

    # Last Round
    if ACR > 0.5:
        action[r] = C
    else:
        action[r] = D

return action
```
This ACC strategy balances individual self-interest with collective well-being by adapting to the evolving game dynamics and incorporating reputation-based mechanisms.
'''

description_COLLECTIVE_231 = '''
**Collective Strategy: Adaptive Cooperation with History-Based Tit-for-Tat (ACT)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to cooperate.
2. **Tit-for-Tat (TFT) Logic**: In subsequent rounds, use the TFT logic:
	* If at least `n/2` players cooperated in the previous round, cooperate (C).
	* Otherwise, defect (D).
3. **History-Based Adjustment**: Introduce a history-based adjustment to TFT:
	* Track the total number of cooperative moves (`coop_count`) and defective moves (`defect_count`) by all players over the past `r/2` rounds.
	* If `coop_count > defect_count`, increase the threshold for cooperation from `n/2` to `(n/2) + 1`. This encourages more cooperation when the group is trending towards cooperation.
	* If `coop_count < defect_count`, decrease the threshold for cooperation from `n/2` to `(n/2) - 1`. This adapts to a group that is trending towards defection.

**Edge Cases:**

1. **Last Round**: In the last round, cooperate (C) if at least `n/2` players cooperated in the previous round. Otherwise, defect (D).
2. **Ties**: If the number of cooperative moves equals the number of defective moves in the TFT logic or history-based adjustment, default to cooperation (C).

**Pseudocode:**
```python
def ACT(n, k, r):
    coop_count = 0
    defect_count = 0

    for t in range(r):
        if t == 0:
            # Initial round: Cooperate
            action = 'C'
        else:
            # TFT logic with history-based adjustment
            prev_round_coops = count_cooperative_moves(t-1)
            threshold = n/2

            if coop_count > defect_count:
                threshold += 1
            elif coop_count < defect_count:
                threshold -= 1

            if prev_round_coops >= threshold:
                action = 'C'
            else:
                action = 'D'

        # Update history-based counters
        if action == 'C':
            coop_count += 1
        else:
            defect_count += 1

        # Reset counters every r/2 rounds
        if t % (r//2) == 0 and t > 0:
            coop_count = 0
            defect_count = 0

    return action
```
**Collective Mindset:**

The ACT strategy is designed to promote cooperation while adapting to the group's behavior. By using a history-based adjustment, we encourage more cooperation when the group is trending towards cooperation and adapt to defection when necessary. This approach aligns with the collective mindset, as it prioritizes cooperation but also takes into account the actions of others.

By participating in the tournament, this strategy will demonstrate its ability to cooperate effectively with various opponent behaviors while maintaining a robust and adaptive nature.
'''

description_COLLECTIVE_232 = '''
**Collective Strategy: "Adaptive Cooperative Escalation" (ACE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to establish a cooperative tone and encourage others to follow suit.
2. **Reciprocity**: For rounds 2 to r-1, cooperate if at least half of the players cooperated in the previous round. Otherwise, defect (play D).
3. **Escalation**: If the total payoff for all players in the previous round is below a certain threshold (e.g., average payoff < 1.5), increase cooperation by 10% (i.e., cooperate with probability 0.6 if previously cooperating with probability 0.5). This encourages others to escalate their cooperation as well.
4. **Punishment**: If the total number of cooperators in the previous round is below a certain threshold (e.g., fewer than 2/3 of players), defect in the current round. This punishes non-cooperative behavior and encourages players to cooperate.
5. **Endgame Cooperation**: In the last round, cooperate regardless of previous actions.

**Pseudocode:**
```markdown
def ACE(n, k, r, history):
    if round == 1:
        return C  # Initial cooperation

    prev_round_cooperators = sum(history[-1]) / n
    avg_payoff_prev_round = sum([π_i for π_i in history[-1]]) / n

    if prev_round_cooperators >= 0.5 and avg_payoff_prev_round >= 1.5:
        return C  # Reciprocity
    elif avg_payoff_prev_round < 1.5:
        escalate_cooperation()  # Increase cooperation by 10%
    else:
        return D  # Punishment

    if round == r:  # Last round
        return C  # Endgame cooperation

def escalate_cooperation():
    global cooperation_probability
    cooperation_probability += 0.1
```
**Rationale:**

ACE aims to balance individual payoffs with collective well-being by adapting to the game's dynamics. By cooperating initially and reciprocating cooperation, ACE encourages others to cooperate. The escalation mechanism promotes increased cooperation when total payoffs are low, while punishment discourages non-cooperative behavior. Endgame cooperation ensures a positive conclusion.

**Robustness:**

ACE is designed to be robust against various opponent behaviors:

* **Free riders**: ACE's punishment mechanism discourages players from exploiting cooperative behavior.
* **Non-cooperators**: Escalation and reciprocity encourage players to cooperate, even if some opponents do not.
* **Conditional cooperators**: ACE's adaptive nature allows it to respond effectively to conditional cooperation strategies.

By implementing ACE, we create a collective strategy that promotes cooperation while being resilient to diverse opponent behaviors.
'''

description_COLLECTIVE_233 = '''
I'll design a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and aligns with a collective mindset.

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to set a positive tone and encourage others to cooperate.
2. **Contribution Threshold**: Track the average contribution rate of all players from previous rounds. If the average contribution rate is above a threshold θ (e.g., 0.5), cooperate (C). Otherwise, defect (D).
3. **Punishment Mechanism**: Implement a simple punishment mechanism to deter free-riding. If the total contribution in the previous round is less than half of the maximum possible contribution (n \* k / n), and you cooperated in that round, then defect (D) in the next round.
4. **Reciprocation**: Cooperate (C) if at least one player cooperated in the previous round and the total contribution was above the threshold θ.
5. **Endgame Strategy**: In the last two rounds, cooperate (C) regardless of previous contributions to maximize collective payoff.

**Pseudocode:**
```python
def ACC(n, k, r, history):
  # Initialize variables
  theta = 0.5  # contribution threshold
  prev_contributions = []  # list to store previous contributions

  for round in range(r):
    if round == 0:
      action = 'C'  # cooperate in the first round
    else:
      avg_contribution_rate = sum(prev_contributions) / (n * round)
      if avg_contribution_rate >= theta:
        action = 'C'
      elif sum(prev_contributions[-1]) < n * k / n and history[round-1][self_id] == 'C':
        action = 'D'  # punish free-riding
      elif any(player_action == 'C' for player_action in history[round-1]):
        action = 'C'
      else:
        action = 'D'

    if round >= r - 2:  # endgame strategy
      action = 'C'

    prev_contributions.append([player_action == 'C' for player_action in history[round]])

    return action
```
**Edge Cases:**

* In the first round, cooperate to set a positive tone.
* In the last two rounds, cooperate regardless of previous contributions to maximize collective payoff.
* If all players defect, the strategy will also defect.

**Collective Mindset:**
The ACC strategy aims to balance individual self-interest with collective well-being. By cooperating when others do and punishing free-riding, it encourages cooperation while adapting to various opponent behaviors. The contribution threshold θ can be adjusted to fine-tune the strategy's sensitivity to changes in contributions.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it is robust to a wide range of opponent behaviors and aligns with a collective mindset.
'''

description_COLLECTIVE_234 = '''
**Collective Strategy: Adaptive Cooperativity**

**Decision Rules:**

1. **Initial Round**: In the first round (t=1), cooperate (C) with a probability of 50%. This sets a neutral tone and encourages others to cooperate.
2. **Subsequent Rounds**: For t > 1, use the following rules:
	* If the average cooperation rate among all players in the previous round is above a threshold (θ), cooperate (C). Otherwise, defect (D).
	* θ = k/n × (r/2) - This threshold balances individual and collective payoffs.
3. **Adaptive Adjustment**: Every 5 rounds, re-evaluate the average cooperation rate over the last 10 rounds. If it's below θ, decrease θ by 10% to adapt to less cooperative opponents.

**Pseudocode:**
```python
def adaptive_cooperativity(n, k, r):
    theta = k/n * (r/2)
    cooperate_prob = 0.5
    
    for t in range(1, r+1):
        if t == 1:
            # Initial round
            action = 'C' if random.random() < cooperate_prob else 'D'
        else:
            prev_round_coop_rate = avgcoop(t-1)
            if prev_round_coop_rate > theta:
                action = 'C'
            else:
                action = 'D'
        
        # Adapt theta every 5 rounds
        if t % 5 == 0 and t > 10:
            avg_10round_coop_rate = avgcoop(t, window=10)
            if avg_10round_coop_rate < theta:
                theta *= 0.9
        
        return action

def avgcoop(round_num, window=None):
    # Calculate average cooperation rate for the specified round or window
    ...
```
**Handling Edge Cases:**

* **Last Round**: In the final round (t=r), defect (D). This ensures maximum personal payoff.
* **Early Defection**: If opponents defect excessively early on, adjust θ downward to adapt to their behavior.

**Collective Mindset:**
This strategy promotes cooperation while being robust against various opponent behaviors. By adapting to changing cooperation rates and adjusting the threshold, it aims to balance individual payoffs with collective welfare. In a tournament setting, this approach encourages other strategies to cooperate, potentially leading to higher overall payoffs for all players.

**Assumptions and Limitations:**

* This strategy relies on perfect information about previous rounds' actions and payoffs.
* It assumes that opponents are rational and may adapt their strategies over time.
* The adaptive adjustment mechanism may not be effective against very short-sighted or extremely uncooperative opponents.
'''

description_COLLECTIVE_235 = '''
I'll design a collective strategy that is adaptive and robust to various opponent behaviors. Here's the specification:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = k/n. This initial cooperation level reflects the game's parameters and encourages collective contribution.
2. **Reciprocity**: For rounds t > 1:
	* If the number of cooperators in the previous round (t-1) is greater than or equal to n/2, cooperate (C).
	* Otherwise, defect (D).
3. **Adaptive Adjustment**: Update the cooperation probability p based on the game's history:
	+ For each round t > 1, calculate the average payoff of all players in the previous round (t-1): avg_payoff_t_1.
	+ If avg_payoff_t_1 > k/2, increase p by a small increment Δp = 0.05. This reflects improved collective outcomes and encourages more cooperation.
	+ If avg_payoff_t_1 < k/4, decrease p by Δp. This adjusts to suboptimal collective outcomes by reducing cooperation.
4. **Last Round Exception**: In the final round (t=r), always defect (D). As there are no future rounds to influence, individual payoff maximization takes priority.

**Edge Cases:**

* If all players defected in the previous round (t-1), cooperate (C) with probability p = k/n. This helps restart cooperation.
* In case of a tie (n/2 cooperators and n/2 defectors), cooperate (C) with probability p = 0.5.

**Pseudocode:**
```python
def ACO(n, k, r):
    # Initialize cooperation probability
    p = k / n

    for t in range(1, r+1):
        if t == 1:
            # Initial cooperation
            cooperate = (random.random() < p)
        else:
            # Reciprocity and adaptive adjustment
            avg_payoff_t_1 = calculate_average_payoff(t-1)
            if avg_payoff_t_1 > k/2:
                p += 0.05
            elif avg_payoff_t_1 < k/4:
                p -= 0.05

            # Adjust cooperation probability based on history
            cooperate = (random.random() < p)

        # Last round exception
        if t == r:
            cooperate = False

        # Play the game and observe outcomes
        outcome = play_game(cooperate)
```
**Collective Mindset:**
The ACO strategy aims to balance individual self-interest with collective benefits. By adapting cooperation levels based on past outcomes, it encourages players to contribute to the public good when mutually beneficial, while also responding to free-riding or uncooperative behavior.

This adaptive approach should perform well in a tournament against independent strategies, as it:

1. Exploits cooperative opportunities when present.
2. Adjusts to changing opponent behaviors and game dynamics.
3. Does not rely on predetermined coordination mechanisms or shared norms.

The ACO strategy is robust to various opponent behaviors and can adapt to different game environments, making it a strong contender in the tournament.
'''

description_COLLECTIVE_236 = '''
I propose a collective strategy for the N-Player Public Goods Game that adapts to the game parameters and history. This strategy is designed to balance cooperation and defection to maximize total payoff.

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to signal willingness to contribute to the public good.
2. **General Rule**: For rounds t > 1, calculate the average cooperation rate (α) of all players in the previous round:
	* α = (Σ(j=1 to n) c_j,t-1) / n
3. **Cooperation Threshold** (θ): Set θ as a function of the multiplication factor k and the number of players n:
	* θ = (k/n) \* (n/2)
4. **Decision Logic**: For each round t > 1, compare α to θ:
	* If α ≥ θ, Cooperate (C). This indicates sufficient cooperation in the previous round.
	* If α < θ, Defect (D). This suggests insufficient cooperation, and a more cautious approach is needed.

Pseudocode:
```python
def ACO(n, k, t):
  if t == 1:  # Initial Round
    return 'C'
  
  # Calculate average cooperation rate in previous round
  α = sum(c_j for j in range(1, n+1) if c_j,t-1 == 1) / n
  
  # Set cooperation threshold
  θ = (k/n) * (n/2)
  
  if α >= θ:
    return 'C'  # Cooperate
  else:
    return 'D'  # Defect
```

**Edge Cases:**

* **Last Round**: In the final round, defect (D). Since there are no future rounds to influence, maximizing individual payoff takes priority.
* **Ties in Average Cooperation Rate**: If α equals θ exactly, cooperate (C) to maintain a high level of cooperation.

**Collective Mindset Alignment:**
ACO aims to balance individual interests with collective well-being. By cooperating when the average cooperation rate is above a threshold and defecting otherwise, ACO promotes a mutually beneficial outcome while adapting to changing opponent behaviors.

This strategy should perform well in a tournament setting, as it:

1. Encourages cooperation when others cooperate.
2. Defects when cooperation is low, minimizing losses.
3. Adapts to changes in opponent behavior over time.

ACO's adaptive nature and reliance on game history make it robust against a wide range of opponent strategies.
'''

description_COLLECTIVE_237 = '''
**Collective Strategy: Adaptive Cooperative Momentum (ACM)**

ACM is a robust and adaptive collective strategy designed for the N-Player Public Goods Game. It balances individual payoffs with cooperative momentum to maximize overall group welfare.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative baseline.
2. **Cooperation Momentum:** If, in the previous round, at least `n/2` players cooperated, continue to Cooperate (C).
3. **Defection Response:** If, in the previous round, fewer than `n/4` players cooperated, Defect (D) for one round to signal dissatisfaction with low cooperation levels.
4. **Adaptive Threshold:** Update the threshold for cooperation (`n/2`) based on the average cooperation rate over the past `r/2` rounds. If the average is above 0.5, decrease the threshold by 1; if below 0.5, increase it by 1.

**Pseudocode:**
```markdown
# Initialize variables
cooperation_threshold = n / 2
adaptive_threshold_adjustment = 1

# Loop through rounds
for t in range(r):
    # First round: Cooperate (C)
    if t == 0:
        action[t] = C
    
    # Cooperation momentum
    elif previous_cooperators >= cooperation_threshold:
        action[t] = C
    
    # Defection response
    elif previous_cooperators < n / 4:
        action[t] = D
    
    # Adaptive threshold adjustment
    if t % (r/2) == 0 and t != 0:
        average_cooperation_rate = sum(cooperators past rounds) / (r/2)
        if average_cooperation_rate > 0.5:
            cooperation_threshold -= adaptive_threshold_adjustment
        else:
            cooperation_threshold += adaptive_threshold_adjustment
    
    # Update previous round's information
    previous_cooperators = count cooperator actions in round t-1

# Output: action[t]
```
**Handling Edge Cases:**

* Last Round: Cooperate (C) to maximize collective payoff, as there is no future round to consider.
* Ties: If the cooperation threshold is exactly met or exceeded by an odd number of cooperators, Cooperate (C).

**Collective Mindset Alignment:**

ACM prioritizes establishing and maintaining a cooperative momentum, while also responding to deviations from this behavior. This collective strategy encourages players to work together to maximize group welfare while minimizing the risk of exploitation.

In a tournament setting, ACM's adaptive nature allows it to respond effectively to various opponent behaviors, making it a robust choice for the N-Player Public Goods Game.
'''

description_COLLECTIVE_238 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**: Observe the previous round's outcomes and adjust behavior accordingly:
	* If the total payoff from cooperation (Σ(j=1 to n) c_j \* k/n) is greater than or equal to the private payoff from defecting (1), cooperate (C).
	* Otherwise, defect (D).
3. **Punishment Mechanism**: If a player observes another player defecting while they cooperated in the previous round, punish that player by defecting against them in the next round.
4. **Forgiveness**: After punishing a player, return to cooperating with them if they cooperate in the subsequent round.

**Edge Case Handling:**

1. **Last Round**: Cooperate (C) in the last round to maximize collective payoff and encourage cooperation from other players.
2. **Ties**: In case of ties (e.g., equal payoffs for both C and D), prioritize cooperating with players who have cooperated more frequently in the past.

**Collective Mindset:**

ACP prioritizes cooperation while introducing a punishment mechanism to deter defectors and maintain fairness. This approach promotes a collective mindset by:

1. Encouraging initial cooperation to set a positive tone.
2. Adapting behavior based on previous outcomes, promoting reciprocity.
3. Punishing defectors to prevent exploitation and encourage cooperation.
4. Forgiving players who return to cooperating, allowing for redemption.

**Pseudocode:**
```python
def ACP(n, k, r):
  # Initialize variables
  prev_round_outcomes = []
  punishment_list = []

  for t in range(r):
    if t == 0:
      action = 'C'  # Cooperate in the first round
    else:
      total_coop_payoff = sum([prev_round_outcomes[j] * k / n for j in range(n)])
      private_defect_payoff = 1

      if total_coop_payoff >= private_defect_payoff:
        action = 'C'  # Cooperate if cooperation is beneficial
      else:
        action = 'D'  # Defect otherwise

    # Punishment mechanism
    for i in range(n):
      if prev_round_outcomes[i] == 0 and punishment_list[i]:
        action = 'D'

    # Update variables
    prev_round_outcomes.append(action)
    punishment_list = [x or (prev_round_outcomes[j] == 1) for j, x in enumerate(punishment_list)]

  return action
```
This strategy will adapt to various opponent behaviors while prioritizing collective cooperation and fairness.
'''

description_COLLECTIVE_239 = '''
To design a collective strategy for this N-Player Public Goods Game that is adaptive and robust to various opponent behaviors, I propose the following approach:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p=0.5. This initial cooperation serves as a "litmus test" for potential reciprocity from other players.
2. **Reciprocity-based Cooperation**: For rounds t > 1, observe the total number of cooperators in the previous round, denoted as c_prev. If c_prev ≥ n/2 (i.e., at least half of the players cooperated), cooperate in the current round with probability p = k/n (the multiplication factor). This encourages cooperation when there is a critical mass of cooperators.
3. **Defection Threshold**: Introduce a defection threshold, τ, defined as the minimum number of rounds since the last time at least half of the players cooperated. Initialize τ to 0 in the first round. If c_prev < n/2 and τ > r/4 (i.e., more than a quarter of the total rounds have passed without sufficient cooperation), defect in the current round.
4. **Adaptive Adjustment**: After each round, adjust p based on the observed payoff π_i,t. If π_i,t > k/n (the expected payoff from cooperating when all others cooperate), increase p by 0.1; otherwise, decrease p by 0.05. This adaptively adjusts cooperation probability in response to changes in the game environment.
5. **Terminal Round Defection**: In the last round (t = r), defect unconditionally. As there is no future reciprocity potential, maximize short-term payoff.

**Pseudocode:**

```markdown
Initialize p = 0.5, τ = 0

For each round t:
    If t == 1:
        Cooperate with probability p
    Else:
        Observe c_prev (total cooperators in previous round)
        If c_prev >= n/2:
            Cooperate with probability k/n
        Else if τ > r/4:
            Defect
        Else:
            Cooperate with probability p

    Adjust p based on observed payoff π_i,t:
        If π_i,t > k/n:
            p += 0.1
        Else:
            p -= 0.05

    Update τ (defection threshold):
        If c_prev >= n/2:
            τ = 0
        Else:
            τ += 1

    If t == r (last round):
        Defect unconditionally
```

**Collective Mindset:**

ACO is designed to balance individual self-interest with collective well-being. By cooperating initially and adapting to the game environment, ACO aims to:

* Encourage reciprocity from other players through initial cooperation.
* Foster a critical mass of cooperators by reciprocating when at least half of the players cooperate.
* Gradually adjust cooperation probability based on observed payoffs, allowing for adaptability in response to changing game conditions.

This strategy promotes collective optimism, as it seeks to create an environment where cooperation is mutually beneficial and sustainable over multiple rounds. By aligning individual actions with the collective goal of maximizing public good, ACO encourages a collaborative mindset among players.
'''

description_COLLECTIVE_240 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) with probability 0.8 and defect (D) with probability 0.2. This sets a tone for potential cooperation while allowing for some exploration.
2. **Subsequent Rounds**: Observe the total number of cooperators in the previous round (t-1). If this number is above the threshold `n/2`, cooperate (C) with probability `p_C = 0.7 + 0.3 * (k/n)`. Otherwise, defect (D) with probability `p_D = 0.4 + 0.6 * (1 - k/n)`.
3. **Adjustment Mechanism**: Monitor the average payoff of all players over the last `r/2` rounds. If this average payoff is below a certain threshold (`avg_payoff_threshold = n/2`), decrease `p_C` by 10% and increase `p_D` by 10%. Conversely, if the average payoff exceeds the threshold, increase `p_C` by 10% and decrease `p_D` by 10%.
4. **Last Round**: In the final round (t=r), defect (D) to maximize personal payoff.

**Pseudocode:**
```
def ACO(n, k, r):
  p_C = 0.8  # initial cooperation probability
  p_D = 0.2  # initial defection probability
  avg_payoff_threshold = n/2

  for t in range(1, r+1):
    if t == 1:
      cooperate_prob = p_C
    else:
      total_cooperators_prev_round = count_cooperators(t-1)
      if total_cooperators_prev_round > n/2:
        cooperate_prob = 0.7 + 0.3 * (k/n)
      else:
        cooperate_prob = 1 - (0.4 + 0.6 * (1 - k/n))

    # adjust probabilities based on average payoff
    if t >= r/2:
      avg_payoff = calculate_average_payoff(t-r/2, t-1)
      if avg_payoff < avg_payoff_threshold:
        p_C *= 0.9
        p_D *= 1.1
      elif avg_payoff > avg_payoff_threshold:
        p_C *= 1.1
        p_D *= 0.9

    # make decision based on adjusted probabilities
    if random.random() < cooperate_prob:
      action = 'C'
    else:
      action = 'D'

    # last round special case
    if t == r:
      action = 'D'

    return action
```
**Collective Mindset:**
ACO is designed to promote cooperation while being responsive to the actions of other players. By adjusting probabilities based on the number of cooperators and average payoffs, ACO aims to create an environment where cooperation can thrive. The strategy's adaptability allows it to respond effectively to a wide range of opponent behaviors.

Note that this strategy does not rely on explicit coordination or communication with other players. Instead, it uses publicly observable information (total cooperators and average payoffs) to make informed decisions.
'''

description_COLLECTIVE_241 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and relies solely on game parameters and history:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Rounds**: Cooperate (C) in the first round to establish a baseline level of cooperation.
2. **Contribution Threshold**: Track the total contributions (Σc_j) made by all players in each round. If the average contribution over the last 'm' rounds exceeds a threshold 'θ' (e.g., θ = 0.5), continue cooperating (C). Otherwise, defect (D).
3. **Response to Defection**: If the number of defections in the previous round exceeds a certain percentage 'δ' (e.g., δ = 30%) of the total players, defect (D) in the current round.
4. **Punishment and Forgiveness**: Implement a Tit-for-Tat-like mechanism:
	* If a player defected in the previous round and the average contribution was below θ, punish them by defecting (D) in the current round.
	* If a player cooperated in the previous round and the average contribution was above θ, forgive their previous defection(s).
5. **Exploration**: Introduce occasional exploration to test the waters:
	* With probability 'ε' (e.g., ε = 0.1), randomly choose an action (C or D) regardless of the decision rules.

**Edge Cases:**

1. **Last Round**: Defect (D) in the last round, as there's no future interaction to influence.
2. **Round after Punishment**: If a player was punished for defecting and they cooperate in the next round, forgive their previous defection(s).

**Pseudocode:**
```python
def AdaptiveCollectiveOptimism(n, k, r):
  m = min(5, r)  # number of rounds to consider for average contribution
  θ = 0.5        # contribution threshold
  δ = 0.3        # defection percentage threshold
  ε = 0.1        # exploration probability

  def decision_rules(round, history):
    if round == 1:
      return 'C'
    avg_contribution = sum(history[-m:]) / m
    num_defections = sum(1 for action in history[-1] if action == 'D')
    if avg_contribution > θ:
      return 'C'
    elif num_defections > δ * n:
      return 'D'
    elif history[-1][i-1] == 'D' and avg_contribution < θ:
      return 'D'
    else:
      # exploration
      if random.random() < ε:
        return random.choice(['C', 'D'])
      return 'C'

  for round in range(1, r+1):
    history = []  # initialize history for this round
    actions = []
    for i in range(n):
      action = decision_rules(round, history)
      actions.append(action)
    yield actions

# usage example:
n = 6
k = 2
r = 10
strategy = AdaptiveCollectiveOptimism(n, k, r)
for round in strategy:
  print(round)
```
This strategy balances cooperation and defection based on the game's history, adapting to various opponent behaviors. By considering the average contribution over a window of rounds ('m'), ACO encourages collective optimism while preventing exploitation by defectors. The exploration mechanism allows for occasional testing of the environment.
'''

description_COLLECTIVE_242 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The AC strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to signal willingness to contribute to the public good.
2. **Subsequent Rounds**:
	* If the average payoff of cooperators is higher than that of defectors in the previous round, cooperate (C).
	* Otherwise, defect (D).
3. **Trend Detection**: Monitor the trend of cooperation levels over the last few rounds (e.g., 3-5 rounds). If cooperation is increasing or stable, continue to cooperate (C). If cooperation is decreasing, defect (D).
4. **Free-Rider Detection**: If a player detects that they are consistently receiving higher payoffs than others while contributing less (i.e., free-riding), they will switch to defecting (D) in the next round.
5. **Endgame Strategy**: In the last few rounds (e.g., 2-3 rounds), if the game is close to ending, prioritize self-interest and defect (D).

**Pseudocode:**
```python
def adaptive_cooperation(history):
    # Initialize cooperation flag
    cooperate = True

    # First round
    if len(history) == 0:
        return 'C'

    # Analyze previous rounds
    avg_c_payoff = sum([payoff for action, payoff in history[-1] if action == 'C']) / len([action for action, _ in history[-1] if action == 'C'])
    avg_d_payoff = sum([payoff for action, payoff in history[-1] if action == 'D']) / len([action for action, _ in history[-1] if action == 'D'])

    # Adjust cooperation based on payoffs
    if avg_c_payoff > avg_d_payoff:
        cooperate = True
    else:
        cooperate = False

    # Trend detection
    coop_trend = [sum([1 for action in round_history if action == 'C']) / len(round_history) for round_history in history[-3:]]
    if all(trend >= 0.5 for trend in coop_trend):
        cooperate = True

    # Free-rider detection
    my_payoff = sum([payoff for _, payoff in history])
    avg_payoff = sum([sum([payoff for _, payoff in round_history]) / len(round_history) for round_history in history]) / len(history)
    if my_payoff > avg_payoff and sum([1 for action, _ in history[-3:] if action == 'D']) > 0:
        cooperate = False

    # Endgame strategy
    if len(history) >= r - 2:  # r is the number of rounds
        cooperate = False

    return 'C' if cooperate else 'D'
```
**Collective Mindset**: The AC strategy prioritizes cooperation when it benefits the collective, but adapts to self-interest when necessary. By monitoring trends and payoffs, AC promotes a balanced approach that rewards cooperation while discouraging free-riding.

In the tournament against other AI systems, AC will demonstrate its robustness by adapting to various opponent behaviors, from cooperative to exploitative strategies.
'''

description_COLLECTIVE_243 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The AC strategy aims to balance individual payoffs with collective welfare by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to seed cooperation and encourage others to reciprocate.
2. **Subsequent Rounds**:
	* If the majority of players (> n/2) cooperated in the previous round, cooperate (C).
	* If fewer than half of the players cooperated, but the collective contribution was still significant (> k/4 \* n), cooperate (C) to maintain momentum.
	* Otherwise, defect (D) to minimize losses.
3. **Punishment Mechanism**: If a player defects while others are cooperating, AC will defect in the next round as a response. This helps maintain accountability and discourages exploitation.

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the final round, regardless of previous outcomes, to maximize collective welfare.
2. **Tiebreakers**: In cases where the majority or significant contribution conditions are met with equal numbers of cooperators and defectors, AC will cooperate (C).

**Collective Mindset Alignment:**

AC prioritizes cooperation when it is likely to be reciprocated or beneficial for the group. By responding positively to cooperative environments and adapting to changes in the game's dynamics, AC promotes a collective mindset that balances individual interests with group welfare.

**Pseudocode:**
```python
def adaptive_cooperation(n, k, r, history):
    if current_round == 1:
        return C  # Cooperate in the first round

    prev_cooperators = count_cooperators(history[-1])
    prev_total_contribution = sum(cooperator_contributions(history[-1]))

    if prev_cooperators > n / 2:
        return C  # Cooperate if majority cooperated
    elif prev_total_contribution > k / 4 * n:
        return C  # Cooperate if significant contribution
    else:
        return D  # Defect otherwise

    # Punishment mechanism
    if player_defected(history[-1]) and others_cooperated(history[-1]):
        return D  # Defect in response to exploitation

def count_cooperators(round_history):
    # Count the number of cooperators in a given round history
    pass

def sum(cooperator_contributions(round_history)):
    # Calculate the total contribution from cooperators in a given round history
    pass

def player_defected(round_history):
    # Check if the current player defected in a given round history
    pass

def others_cooperated(round_history):
    # Check if other players cooperated in a given round history
    pass
```
Note that this pseudocode is a simplified representation of the strategy and will need to be expanded and refined for implementation.
'''

description_COLLECTIVE_244 = '''
**Collective Strategy: Adaptive Cooperation with Social Learning**

This strategy aims to balance individual self-interest with collective welfare by adaptively responding to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) unconditionally to establish a cooperative tone and encourage others to reciprocate.
2. **Subsequent Rounds (t>1):** Use a weighted average of two factors to determine the action:
	* **Reciprocity Factor (RF):** Calculate the proportion of cooperators in the previous round. If RF > 0.5, cooperate; otherwise, defect.
	* **Personal Experience Factor (PEF):** Evaluate your own payoff history. If your average payoff is above the game's expected value (1 + k/n), cooperate; otherwise, defect.

Pseudocode:
```python
def decide_action(t, previous_round_actions, personal_payoffs):
    if t == 1:  # Initial round
        return 'C'
    else:
        RF = sum(previous_round_actions) / len(previous_round_actions)
        PEF = sum(personal_payoffs) / len(personal_payoffs) > (1 + k/n)
        
        if RF > 0.5 and PEF:
            return 'C'
        elif RF <= 0.5 or not PEF:
            return 'D'
```
**Edge Cases:**

* **Last Round (t=r):** Defect, as there is no future round to reciprocate.
* **Tie in Reciprocity Factor:** Cooperate if the tie occurs at RF = 0.5; otherwise, defect.

**Collective Mindset:**

This strategy prioritizes cooperation when the group's overall behavior suggests a willingness to cooperate (RF > 0.5). By considering personal experience and payoff history (PEF), it also avoids unconditional cooperation in the face of exploitation. The initial round's unconditional cooperation aims to set a positive tone, while the last round's defection acknowledges the game's finality.

**Robustness:**

This strategy is designed to be robust against various opponent behaviors:

* **Cooperative opponents:** Reciprocal cooperation will emerge and persist.
* **Defecting opponents:** The strategy will adapt by defecting when RF ≤ 0.5, minimizing losses.
* **Mixed strategies:** The weighted average approach allows the strategy to respond effectively to mixed cooperative and defective behaviors.

By balancing individual self-interest with collective welfare considerations, this Adaptive Cooperation with Social Learning strategy aims to perform well in a tournament setting against diverse opponent strategies.
'''

description_COLLECTIVE_245 = '''
**Collective Strategy: Adaptive Cooperation with Memory (ACM)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to reciprocate.
2. **Subsequent Rounds:**
	* If the total number of cooperators in the previous round is greater than or equal to `n/2`, cooperate (C). This encourages continued cooperation when it's working.
	* Otherwise, defect (D) if your own payoff in the previous round was lower than the average payoff of defectors. This adapts to situations where cooperation isn't paying off.
3. **Memory-Based Adjustment:** Keep a memory of the last `m` rounds (where `m` is a parameter, e.g., `m = 5`). Calculate the proportion of cooperators (`p_c`) and defectors (`p_d`) in this window. If `p_c > p_d`, cooperate; otherwise, defect.
4. **Exception Handling:** If all players defected in the previous round, cooperate (C) to attempt to restart cooperation.

**Pseudocode:**
```markdown
ACM Strategy:
  Initialize memory_window = [] (stores last m rounds)
  
  For each round t:
    If t == 1:  # Initial Round
      action = COOPERATE (C)
    Else:
      prev_round_coops = count cooperators in previous round
      if prev_round_coops >= n/2:
        action = COOPERATE (C)
      else:
        my_prev_payoff = calculate own payoff in previous round
        avg_defector_payoff = calculate average payoff of defectors in previous round
        if my_prev_payoff < avg_defector_payoff:
          action = DEFECT (D)
        else:
          # Memory-based adjustment
          memory_window.append(previous_round_actions)
          if len(memory_window) > m:  # window is full
            p_c = proportion of cooperators in memory_window
            p_d = proportion of defectors in memory_window
            if p_c > p_d:
              action = COOPERATE (C)
            else:
              action = DEFECT (D)
          else:  # not enough data to adjust
            action = DEFECT (D)
    
    If all players defected in previous round:
      action = COOPERATE (C)  # attempt to restart cooperation
    
    Take action and update payoffs and memory_window
```
**Collective Mindset:**

The ACM strategy prioritizes cooperation while adapting to the actions of others. By cooperating initially and when cooperation is working, we encourage mutual cooperation. When cooperation isn't paying off, we adjust our strategy to defect, but still attempt to restart cooperation if all players have defected. This balanced approach aligns with a collective mindset, aiming to maximize overall payoffs while being robust to various opponent behaviors.

**Edge Cases:**

* **First Round:** Cooperate to establish a cooperative tone.
* **Last Round:** Follow the standard decision rules; no special treatment for the last round.
* **All Players Defected:** Attempt to restart cooperation by cooperating.
'''

description_COLLECTIVE_246 = '''
**Collective Strategy: Adaptive Cooperative Follower (ACF)**

The ACF strategy aims to balance individual payoffs with collective well-being by adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to set a positive tone for cooperation.
2. **General Rule:** For rounds t > 1, calculate the average number of cooperators (AC) in the previous round:

   AC = Σ(j=1 to n) c_j,t-1 / n

   If AC ≥ k/n, cooperate (C); otherwise, defect (D).
3. **Punishment Mechanism:** To deter opponents from exploiting cooperation, implement a "tit-for-tat" component. If an opponent defected in the previous round while others cooperated, defect (D) in the current round.

   Formally: If ∃j ≠ i such that c_j,t-1 = 0 and AC > k/n, then set c_i,t = 0.
4. **Forgiveness:** After a punishment phase, gradually return to cooperation if opponents cooperate.

   If c_i,t-1 = 0 (punishment) and AC ≥ k/n in the current round, switch back to cooperation (C).

**Edge Cases:**

* Last Round: Cooperate (C) in the last round, as there is no opportunity for future reciprocity.
* Consecutive Defections: If an opponent has defected consecutively for more than two rounds, permanently defect (D) against that opponent.

**Pseudocode:**
```python
def ACF(n, k, history):
    if len(history) == 0:  # Initial round
        return 'C'
    
    AC = sum([1 if action == 'C' else 0 for action in history[-1]]) / n
    
    if AC >= k/n:
        cooperate = True
    else:
        cooperate = False

    for opponent_action in history[-1]:
        j = history[-1].index(opponent_action)
        if opponent_action == 'D' and AC > k/n:
            # Punishment mechanism
            cooperate = False
            break
    
    if not cooperate and sum([1 if action == 'C' else 0 for action in history[-2]]) / n >= k/n:
        # Forgiveness
        cooperate = True

    return 'C' if cooperate else 'D'
```
**Collective Mindset:**
The ACF strategy is designed to align with the collective mindset by:

1. Encouraging cooperation through initial cooperation and positive reciprocity.
2. Punishing opponents for exploiting cooperation, which maintains fairness and discourages free-riding.
3. Forgiving opponents who return to cooperation after punishment, allowing for a return to mutual cooperation.

This strategy adapts to various opponent behaviors while promoting collective well-being, making it a robust competitor in the tournament.
'''

description_COLLECTIVE_247 = '''
**Collective Strategy: Adaptive Cooperativity**

This strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and opponents' behaviors.

**Decision Rules:**

1. **Initial Rounds**: In the first two rounds (t = 1, 2), play C (Cooperate) unconditionally to encourage cooperation and build a foundation for future adaptivity.
2. **Adaptive Phase**: After the initial rounds (t ≥ 3), evaluate the previous round's outcomes:
	* If the total payoff in the last round is greater than or equal to the average individual payoff, play C.
	* Otherwise, defect with probability p = (number of defectors in the last round) / n
3. **Defection Response**: If an opponent defects while the collective payoff is high (> 0.75 \* k), increase the likelihood of defection in subsequent rounds by incrementing a "defection counter" by 1.
4. **Cooperation Revival**: If the collective payoff falls below a certain threshold (0.25 \* k) and there are fewer than n/2 cooperators, revert to playing C unconditionally for two consecutive rounds to attempt to revive cooperation.

**Edge Cases:**

* Last Round (t = r): Play C if the total payoff in the second-to-last round is greater than or equal to the average individual payoff; otherwise, defect.
* Ties: In case of a tie in the number of cooperators and defectors, play C.

**Collective Mindset Alignment:**

The Adaptive Cooperativity strategy aims to balance individual self-interest with collective well-being. By initially cooperating unconditionally, we encourage cooperation and create an environment where adaptivity can be effective. The adaptive phase responds to changes in opponents' behaviors while maintaining a focus on the collective payoff. Defection response and cooperation revival mechanisms ensure that the strategy remains robust against exploitation.

**Pseudocode:**

```python
def AdaptiveCooperativity(n, k, t, history):
    if t <= 2:  # Initial rounds
        return C
    
    last_round_payoff = sum(history[-1])
    avg_individual_payoff = (last_round_payoff / n)
    
    if last_round_payoff >= avg_individual_payoff:
        return C
    
    defectors_last_round = len([x for x in history[-1] if x == D])
    p_defect = defectors_last_round / n
    random_defect = random.random() < p_defect
    
    if random_defect:
        return D
    else:
        return C

def DefectionResponse(history):
    collective_payoff = sum(history[-1])
    threshold = 0.75 * k
    
    if collective_payoff > threshold and history[-1].count(D) > 0:
        increment_defection_counter()
    
    return None

def CooperationRevival(n, k, history):
    collective_payoff = sum(history[-1])
    threshold = 0.25 * k
    cooperators_last_round = len([x for x in history[-1] if x == C])
    
    if collective_payoff < threshold and cooperators_last_round < n/2:
        return C
    
    return None

def PlayGame(n, k, r):
    history = []
    decision_counter = 0
    
    for t in range(r):
        action = AdaptiveCooperativity(n, k, t, history)
        
        if DefectionResponse(history) is not None:
            decision_counter += 1
        
        history.append([action] + [opponent_actions[t]])
    
    return history
```

Please note that the pseudocode provided above serves as a guide and might need adjustments for implementation. The strategy's effectiveness will be evaluated in the tournament against other independent strategies developed by AI systems.
'''

description_COLLECTIVE_248 = '''
To design a collective strategy for this N-Player Public Goods Game that adapts to various opponent behaviors and aligns with a collective mindset, we'll introduce a hybrid approach combining elements of tit-for-tat (TFT), win-stay lose-shift (WSLS), and a contribution-dependent mechanism. This approach aims to balance cooperation incentives with adaptability.

**Strategy Name:** Adaptive Collective Optimizer (ACO)

### Decision Rules:

1. **First Round**: Cooperate (C). Starting with cooperation encourages mutual benefit and sets an initial positive tone.
2. **Subsequent Rounds**:
   - If the total payoff from the last round is greater than or equal to the number of players (n), cooperate (C) in this round. This condition suggests that collective cooperation yielded a higher payoff, incentivizing continued contribution.
   - Otherwise, defect (D). If payoffs were low, it indicates insufficient cooperation, so we adapt by conserving our endowment.

### Adaptation Mechanism:

- **Tit-for-Tat Element**: Monitor the number of cooperators in the last round. If more than half of the players cooperated and your payoff was greater than 1 (indicating collective success), cooperate in the next round to reinforce successful cooperation patterns.
  
- **Win-Stay Lose-Shift Element**: After a round where you defected and received a high payoff, indicating many others cooperated, switch back to cooperating in the next round. This aims to capitalize on the collective good while encouraging others to do the same.

### Collective Mindset Alignment:

- **Contribution Threshold**: Implement a dynamic contribution threshold based on past rounds' outcomes. If, over a set of recent rounds (e.g., 5 rounds), your average payoff is below a certain threshold relative to n and k, adjust by being more likely to defect in future rounds unless cooperation rates increase.

### Pseudocode:

```python
def AdaptiveCollectiveOptimizer(n, k, r):
    # Initialize variables
    total_payoffs = []
    last_round_cooperators = 0
    
    for t in range(r):
        if t == 0:  # First round
            action = 'C'
        else:
            # Evaluate previous round's outcome
            prev_total_payoff = sum(total_payoffs[-1])
            if prev_total_payoff >= n and last_round_cooperators > n/2:
                action = 'C'  # Continue cooperation if successful
            elif total_payoffs[-1][t-1] > 1:  # High payoff from defecting
                action = 'C'  # Switch back to cooperation
            else:
                action = 'D'
        
        # Update history
        last_round_cooperators = sum([1 for a in actions if a == 'C'])
        total_payoffs.append(calculate_payoff(action, n, k))
    
    return total_payoffs

def calculate_payoff(action, n, k):
    contributions = [1 if act == 'C' else 0 for act in actions]
    payoff = (1 - int(action == 'C')) + (k/n) * sum(contributions)
    return payoff
```

### Edge Cases:

- **Last Round**: Since our strategy is based on adapting to the previous round's outcomes, it inherently considers future interactions and collective benefits even in the last round.

This Adaptive Collective Optimizer aims to balance individual interests with collective well-being by dynamically adjusting its cooperation level based on observed payoffs and the number of cooperators. It encourages continued contribution when mutual benefit is achieved and adapts by conserving resources or changing behavior in response to low payoffs, making it robust against a variety of opponent strategies.
'''

description_COLLECTIVE_249 = '''
**Collective Strategy: Adaptive Cooperation with Social Learning**

This strategy aims to balance individual payoffs with collective well-being by adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Exploration Phase** (Rounds 2-5): Alternate between Cooperate (C) and Defect (D) to gather information about opponents' strategies and identify potential cooperators.
3. **Adaptive Cooperation**: After the Exploration Phase, cooperate if:
	* The average payoff of cooperators in the previous round is higher than that of defectors.
	* The number of cooperators in the previous round is above a certain threshold (e.g., n/2).
4. **Punishment Mechanism**: Defect for one round if:
	* You were exploited by a defector in the previous round (i.e., you cooperated, and they defected).
	* The average payoff of cooperators in the previous round is lower than that of defectors.
5. **Re-entry to Cooperation**: After punishing a defector, return to Adaptive Cooperation if:
	* The number of cooperators in the current round increases compared to the previous round.

**Handling Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round, as there is no risk of being exploited.
2. **Tiebreaker**: In case of a tie in the average payoffs between cooperators and defectors, cooperate if the number of cooperators is higher.

**Collective Mindset:**

This strategy prioritizes cooperation while being cautious about exploitation. By adapting to the game's history, it aims to create a stable cooperative environment that benefits all players.

Pseudocode:
```python
def adaptive_cooperation(n, k, r):
    # Initialize variables
    cooperate = True  # Initial round cooperation
    exploration_phase = True
    punishment_mechanism = False

    for t in range(r):
        if t == 0:  # First round
            action = 'C'
        elif exploration_phase:  # Exploration Phase (Rounds 2-5)
            action = 'C' if t % 2 == 0 else 'D'
            if t >= 4:
                exploration_phase = False
        else:
            # Adaptive Cooperation
            avg_coop_payoff = calculate_avg_coop_payoff(t-1)
            avg_defect_payoff = calculate_avg_defect_payoff(t-1)
            num_cooperators = count_cooperators(t-1)

            if avg_coop_payoff > avg_defect_payoff and num_cooperators >= n/2:
                action = 'C'
            else:
                action = 'D'

            # Punishment Mechanism
            if was_exploited(t-1):
                punishment_mechanism = True
                action = 'D'
            elif punishment_mechanism:
                if num_cooperators >= num_cooperators(t-1) + 1:
                    punishment_mechanism = False

        # Update history and perform actions
        update_history(action, t)
        take_action(action)

    return total_payoff()
```
This strategy is designed to be robust against various opponent behaviors while promoting cooperation. It will be interesting to see how it performs in the tournament!
'''

description_COLLECTIVE_250 = '''
I propose a collective strategy called "Adaptive Cooperative Escalation" (ACE). ACE is designed to balance individual self-interest with collective well-being, adapting to various opponent behaviors and game histories.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 50% to establish a baseline level of cooperation.
2. **Reciprocity**: Observe the number of cooperators in the previous round (t-1). If the number of cooperators is above the average number of players (n/2), cooperate in the current round (t).
3. **Escalation**: Monitor the total payoff difference between cooperators and defectors over the last w rounds (window size, e.g., 3-5 rounds). If the average payoff difference is positive and increasing for cooperators, increase cooperation probability by 10% in the next round.
4. **Defection Detection**: Identify consecutive rounds where fewer than n/2 players cooperate. In such cases, defect (D) with a probability of 20% to avoid exploitation.
5. **Endgame Strategy**: In the last two rounds, switch to a more conservative approach: cooperate only if at least half of the other players cooperated in the previous round.

**Edge Cases:**

1. **First Round**: Cooperate with a probability of 50%.
2. **Last Round**: Follow the endgame strategy.
3. **Consecutive Defections**: If all players have defected for more than two consecutive rounds, cooperate with a probability of 30% to restart cooperation.

**Collective Mindset:**

ACE prioritizes collective well-being by:

1. Encouraging cooperation when others cooperate.
2. Gradually increasing cooperation as the payoff difference between cooperators and defectors grows.
3. Avoiding exploitation by detecting consecutive defections.

By adapting to various opponent behaviors and game histories, ACE aims to create a stable and cooperative environment, promoting collective success in the N-Player Public Goods Game.

Pseudocode:
```python
def ace_strategy(n, k, r, w):
  # Initialize variables
  cooperate_prob = 0.5
  avg_payoff_diff = 0
  consecutive_defections = 0

  for t in range(1, r+1):
    if t == 1:  # First round
      action = random.random() < cooperate_prob
    else:
      num_cooperators_prev_round = count_cooperators(t-1)
      if num_cooperators_prev_round > n/2:
        action = True  # Reciprocity
      elif avg_payoff_diff > 0 and increasing():
        cooperate_prob += 0.1  # Escalation
        action = random.random() < cooperate_prob
      elif consecutive_defections >= 2:
        action = random.random() < 0.2  # Defection detection
      else:
        action = False

    if t in [r-1, r]:  # Endgame strategy
      num_cooperators_prev_round = count_cooperators(t-1)
      action = num_cooperators_prev_round >= n/2

    take_action(action)

    update_avg_payoff_diff()
    update_consecutive_defections()

def increasing(avg_payoff_diff):
  return avg_payoff_diff > 0 and avg_payoff_diff > prev_avg_payoff_diff
```
Note that the pseudocode provides a simplified representation of the strategy. The actual implementation will require additional details, such as handling edge cases and optimizing performance.
'''

description_COLLECTIVE_251 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

AC is a dynamic, history-dependent strategy that balances individual self-interest with collective welfare. It adapts to changing opponent behaviors and promotes cooperation while safeguarding against exploitation.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a cooperative tone and gather information about opponents' initial strategies.
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust behavior as follows:
	* If the collective payoff was high (> k/2), cooperate (C) in the current round, assuming others will continue to contribute.
	* If the collective payoff was low (< 1/n), defect (D) in the current round, protecting against exploitation by non-cooperative opponents.
	* If the collective payoff was moderate (between k/2 and 1/n), use a **Mixed Strategy**:
		+ With probability p = (current_round / total_rounds), cooperate (C).
		+ With probability (1 - p), defect (D).
3. **Punishment Mechanism:** If an opponent defects while the collective payoff was high (> k/2) in the previous round, defect (D) against that opponent in the current round to discourage free-riding.
4. **Forgiveness Mechanism:** After punishing an opponent, revert to the Mixed Strategy after one round of cooperation from that opponent.

**Edge Cases:**

* **Last Round:** Cooperate (C) if the collective payoff was high (> k/2) in the previous round; otherwise, defect (D).
* **Tie-Breaking:** In case of a tie between cooperate and defect decisions, choose cooperate (C).

**Pseudocode:**
```markdown
function AdaptiveCooperation(n, k, r, history):
  if current_round == 1:
    return COOPERATE

  previous_payoff = calculate_collective_payoff(history)
  opponent_defections = count_defections_against_me(history)

  if previous_payoff > k/2:
    return COOPERATE
  elif previous_payoff < 1/n:
    return DEFECT
  else:
    p = current_round / total_rounds
    if random() < p:
      return COOPERATE
    else:
      return DEFECT

  # Punishment and Forgiveness Mechanisms
  for opponent in opponents:
    if opponent_defected_against_me(opponent, history):
      return DEFECT
    elif opponent_cooperated_against_me(opponent, history):
      return MIXED_STRATEGY
```
**Collective Mindset:**

AC prioritizes cooperation while protecting against exploitation. By adapting to changing opponent behaviors and using a mixed strategy, AC promotes collective welfare while minimizing the risk of being taken advantage of.
'''

description_COLLECTIVE_252 = '''
**Collective Strategy: Adaptive Cooperate-Defect (ACD)**

The ACD strategy aims to balance individual payoff with collective well-being by adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round**: In the first round, play C (Cooperate) to signal willingness to contribute to the public good.
2. **Subsequent Rounds**: Observe the number of cooperators (C) in the previous round (t-1). Let C_prev = Σ(j=1 to n) c_j,t-1
	* If C_prev ≥ k/2, play C (Cooperate) in the current round. This encourages continued cooperation when a sufficient number of players contribute.
	* Otherwise, play D (Defect) with probability p_defect = 1 - (C_prev / k). This introduces a degree of defection to exploit potential free-riders and adapt to low cooperation levels.
3. **Adaptive Adjustment**: After each round, update the strategy based on the observed payoff π_i,t and opponent behaviors:
	* If π_i,t < 1 + (k/n) \* C_prev, increase p_defect by 0.05 (up to a maximum of 1). This responds to low payoffs by increasing defection.
	* If π_i,t > 1 + (k/n) \* C_prev, decrease p_defect by 0.05 (down to a minimum of 0). This reinforces cooperation when payoffs are high.

**Edge Cases:**

* **Last Round**: Play D (Defect) in the final round, as there is no future benefit from cooperating.
* **Single Opponent**: If only one opponent remains, play C (Cooperate) to maximize joint payoff.

**Collective Mindset Alignment:**

ACD prioritizes cooperation when sufficient players contribute, promoting a collective good. By adapting to observed behaviors and payoffs, the strategy balances individual interests with group well-being.

**Pseudocode:**
```python
def ACD(n, k, r):
    C_prev = 0  # initialize previous round's cooperators
    p_defect = 0  # initialize defection probability

    for t in range(r):  # iterate over rounds
        if t == 0:
            action = 'C'  # cooperate in first round
        else:
            C_prev = sum(c_j,t-1) for j in range(n)
            if C_prev >= k/2:
                action = 'C'
            else:
                p_defect = 1 - (C_prev / k)
                action = 'D' if random.random() < p_defect else 'C'

        # Observe payoff and update strategy
        π_i,t = calculate_payoff(n, k, C_prev, action)
        if π_i,t < 1 + (k/n) \* C_prev:
            p_defect = min(1, p_defect + 0.05)
        elif π_i,t > 1 + (k/n) \* C_prev:
            p_defect = max(0, p_defect - 0.05)

    return action
```
This collective strategy is designed to be adaptive and robust to various opponent behaviors, balancing individual payoffs with the collective good in a repeated public goods game.
'''

description_COLLECTIVE_253 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and relies solely on game parameters and history.

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.8, and defect (D) with probability 0.2. This initial cooperation encourages others to contribute to the public good while also exploring the opponent's behavior.
2. **Reciprocal Altruism**: For rounds t > 1, calculate the average cooperation rate of all players in the previous round (t-1). If this average is above a threshold θ (set to 0.5), cooperate (C) with probability 0.9, and defect (D) with probability 0.1. This encourages continued cooperation when others are contributing.
3. **Punish Free-Riders**: If the average cooperation rate in round (t-1) is below θ, and you cooperated in that round, defect (D) with probability 0.8 in round t. This punishes free-riders who don't contribute to the public good.
4. **Learning from Experience**: Update a personal estimate of the opponent's cooperation rate, α, based on their actions in previous rounds. Use this estimate to adjust the decision rules above.

**Handling Edge Cases:**

1. **Last Round**: In the final round (r), cooperate (C) with probability 0.5, and defect (D) with probability 0.5. This balances individual gain with collective well-being.
2. **Tie-Breaking**: If multiple strategies have the same expected payoff, choose cooperation (C) to promote collective optimality.

**Collective Mindset:**

ACO is designed to align with a collective mindset by:

1. Encouraging initial cooperation to establish a positive tone for the game.
2. Reciprocating altruism when others cooperate, promoting mutual benefit.
3. Punishing free-riders to maintain fairness and cooperation incentives.
4. Continuously learning from experience to adapt to changing opponent behaviors.

**Pseudocode:**
```
Function ACO(n, r, k):
  θ = 0.5 // threshold for average cooperation rate
  α = 0.5 // initial estimate of opponent's cooperation rate

  For t = 1 to r:
    If t == 1:
      Cooperate with probability 0.8, Defect with probability 0.2
    Else:
      Calculate average cooperation rate in round (t-1)
      If average cooperation rate > θ:
        Cooperate with probability 0.9, Defect with probability 0.1
      Else if cooperated in round (t-1) and average cooperation rate < θ:
        Defect with probability 0.8
      Update α based on opponent's actions in previous rounds

    // Learn from experience and adjust decision rules for next round
    If t < r:
      Update α using opponent's action in current round
```
ACO is a robust, adaptive strategy that balances individual gain with collective optimality while responding to various opponent behaviors.
'''

description_COLLECTIVE_254 = '''
I propose a collective strategy called "Adaptive Cooperation with Social Norms" (ACSN). ACSN aims to balance individual payoffs with collective well-being by adapting to the game's history and promoting cooperation.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds:** Use a probabilistic approach based on the previous round's outcome:
	* If the number of cooperators in the previous round is above the threshold (n \* 0.5), cooperate with probability p = k/n + ε, where ε is a small positive value (e.g., 0.05). This encourages continued cooperation when it's beneficial.
	* If the number of cooperators is below the threshold, defect with probability q = n/k - ε. This adapts to low cooperation levels and avoids exploitation.
3. **Punishment Mechanism:** If a player defects in a round where more than half of the players cooperate, ACSN will defect in the next round with a higher probability (q + Δ), where Δ is an adjustment factor (e.g., 0.1). This punishes exploiters and promotes cooperation.
4. **Forgiveness Mechanism:** If a player cooperates after being punished, ACSN will gradually decrease its defection probability over the next few rounds, encouraging cooperation to resume.

**Handling Edge Cases:**

* **Last Round:** In the final round, ACSN will cooperate if the total number of cooperators in the previous round is above the threshold. Otherwise, it will defect.
* **Single-Player Deviation:** If only one player deviates from the cooperative strategy, ACSN will adapt by increasing its defection probability for a few rounds and then gradually decrease it to encourage cooperation.

**Collective Mindset:**

ACSN prioritizes collective well-being while adapting to individual behaviors. By promoting cooperation and punishing exploitation, it fosters a social norm that encourages players to work together towards a mutually beneficial outcome.

Pseudocode:
```python
def ACSN(n, k, r):
    # Initialize variables
    threshold = n * 0.5
    epsilon = 0.05
    delta = 0.1

    for t in range(r):
        if t == 0:  # Initial round
            action = C
        else:
            previous_cooperators = count_cooperators(t-1)
            if previous_cooperators > threshold:
                p = k/n + epsilon
                action = random.choice([C, D], p=[p, 1-p])
            else:
                q = n/k - epsilon
                action = random.choice([D, C], p=[q, 1-q])

            # Punishment mechanism
            if previous_cooperators > threshold and action == D:
                q += delta

            # Forgiveness mechanism
            if action == C and previous_action == D:
                q -= delta / (t + 1)

        return action
```
This strategy is designed to be robust, adaptive, and collective, making it a strong contender in the tournament against other AI systems.
'''

description_COLLECTIVE_255 = '''
**Collective Strategy: Adaptive Cooperation with Retaliation (ACR)**

**Overview**
ACR is a collective strategy that balances cooperation and defection based on the game's history. It adapts to the opponents' behaviors while maintaining a robust and fair approach.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone.
2. **Subsequent Rounds**:
	* If the average payoff of all players in the previous round is above 1.5, cooperate (C). This indicates that cooperation has been successful, and we should continue to contribute.
	* If the average payoff is between 1 and 1.5, defect (D) with a probability of 0.5. This range suggests that some players are cooperating, but not enough to create a significant public good.
	* If the average payoff is below 1, defect (D). This indicates that cooperation has failed, and we should prioritize our individual payoff.
3. **Retaliation Mechanism**: If the number of cooperators in the previous round is less than half of the total players (n/2), and our own payoff was lower than the average payoff, defect (D) in the current round with a probability of 0.8. This mechanism discourages free-riding and punishes non-cooperative behavior.
4. **Consecutive Defections**: If we have defected for three consecutive rounds, cooperate (C) in the next round. This ensures that we don't get stuck in an infinite defect cycle.

**Pseudocode**
```
function ACR(n, k, r, history):
  if current_round == 1:
    return C
  else:
    avg_payoff_prev = calculate_average_payoff(history)
    num_cooperators_prev = count_cooperators(history)

    if avg_payoff_prev > 1.5:
      return C
    elif 1 <= avg_payoff_prev < 1.5:
      return D with probability 0.5
    else:
      return D

    if num_cooperators_prev < n/2 and our_payoff_prev < avg_payoff_prev:
      return D with probability 0.8 (retaliation)

    if consecutive_defections >= 3:
      return C
```
**Edge Cases**

* **Last Round**: Cooperate (C) in the last round to maintain a cooperative tone, regardless of the game's history.
* **Ties**: In case of ties in the average payoff or number of cooperators, use a random tiebreaker.

**Collective Mindset**
ACR is designed to promote cooperation while being robust against various opponent behaviors. By adapting to the game's history and using a retaliation mechanism, ACR encourages players to cooperate and contributes to the public good.
'''

description_COLLECTIVE_256 = '''
I propose a collective strategy called "Adaptive Collective Cooperation" (ACC). ACC is designed to balance individual self-interest with the need for cooperation to achieve higher collective payoffs.

**Decision Rules:**

1. **Initial Exploration Phase**: In the first few rounds (e.g., 3-5), play C (Cooperate) randomly with a probability of 0.5. This phase allows us to gather information about other players' behavior and get an initial sense of their willingness to cooperate.
2. **Contribution Threshold**: After the initial exploration phase, calculate the average number of contributors in previous rounds (`avg_contributors`). Introduce a contribution threshold (`threshold`) as a function of `k` and `n`: `threshold = k / n * avg_contributors`. If `threshold` is above 0.5, play C; otherwise, play D.
3. **Punishment Mechanism**: Monitor the number of contributors in each round. If fewer than `threshold` players cooperate, punish them by playing D for the next two rounds. This mechanism discourages free-riding and maintains cooperation levels.
4. **Reward Mechanism**: Conversely, if more than `threshold` players cooperate, reward them by playing C for the next two rounds. This encourages continued cooperation.

**Edge Cases:**

* **First Round**: Play C randomly with a probability of 0.5 (as in the initial exploration phase).
* **Last Round**: Play D to maximize individual payoff.
* **Ties**: In case of ties, play C if `avg_contributors` is above 50%, and D otherwise.

**Collective Mindset:**

ACC prioritizes cooperation while adapting to the behavior of other players. By using a contribution threshold based on past rounds, ACC balances individual self-interest with collective well-being. The punishment mechanism discourages free-riding, while the reward mechanism encourages continued cooperation.

Pseudocode:
```python
# Initial parameters
n = number_of_players
k = multiplication_factor
r = number_of_rounds
threshold = k / n * 0.5  # initial threshold

# Main loop
for t in range(r):
    if t < 3:  # initial exploration phase
        play C with probability 0.5
    else:
        avg_contributors = calculate_average_contributors(t)
        threshold = k / n * avg_contributors
        
        if threshold > 0.5:
            play C
        else:
            play D
            
        if fewer_than_threshold_players_cooperate():
            punish_by_playing_D_for_2_rounds()
        elif more_than_threshold_players_cooperate():
            reward_by_playing_C_for_2_rounds()

# Edge cases
if t == r - 1:  # last round
    play D
elif is_tie(avg_contributors):
    if avg_contributors > 0.5:
        play C
    else:
        play D
```
ACC's adaptability and robustness make it a strong candidate for the tournament, as it responds effectively to various opponent behaviors while maintaining a collective mindset.
'''

description_COLLECTIVE_257 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds**: Use the following decision rule:
	* If the number of cooperators in the previous round is greater than or equal to the multiplication factor (k), cooperate (C).
	* Otherwise, defect (D).

Pseudocode:
```python
def decide_action(history):
    if len(history) == 0:  # First round
        return 'C'
    prev_round_cooperators = sum(1 for action in history[-1] if action == 'C')
    if prev_round_cooperators >= k:
        return 'C'
    else:
        return 'D'
```
**Handling Edge Cases:**

* **Last Round**: Since the game has a fixed number of rounds, we can exploit this knowledge. In the last round, defect (D) regardless of the previous round's outcome. This ensures maximum payoff in the final round.
	+ Pseudocode:
```python
def decide_action(history):
    if len(history) == r - 1:  # Last round
        return 'D'
    ...
```
* **Ties**: In case of a tie (i.e., the same number of cooperators and defectors), cooperate (C). This maintains a cooperative atmosphere and avoids potential downward spirals.
	+ Pseudocode:
```python
def decide_action(history):
    ...
    prev_round_cooperators = sum(1 for action in history[-1] if action == 'C')
    if prev_round_cooperators == n / 2:  # Tie
        return 'C'
    ...
```
**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes cooperation while being responsive to the actions of others. By cooperating initially and adapting to the number of cooperators in subsequent rounds, we encourage a cooperative environment while protecting against exploitation.

In the presence of defectors, our strategy will gradually shift towards defecting, preventing excessive losses. However, when the number of cooperators increases, we revert to cooperation, fostering a positive feedback loop that promotes collective cooperation.

This strategy is robust and adaptive, making it suitable for a wide range of opponent behaviors in the tournament.
'''

description_COLLECTIVE_258 = '''
**Collective Strategy: Adaptive Cooperation with Public Goods (ACPG)**

The ACPG strategy aims to balance individual payoff maximization with collective welfare by adapting to the dynamics of cooperation and defection in the game.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) unconditionally, as there is no history to inform our decision.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round (Σc_j,t-1) is greater than or equal to n/2, cooperate (C).
	* Otherwise, defect (D).

**Rationale:**

By cooperating initially, we set a positive tone for the game and encourage others to reciprocate. In subsequent rounds, we assess the overall level of cooperation in the previous round. If at least half of the players cooperated, it's likely that the collective benefit outweighs individual gains from defection, so we continue to cooperate. Otherwise, we adapt by defecting to minimize potential losses.

**Edge Cases:**

1. **Last Round (t=r):** Cooperate (C) unconditionally, as there is no future round to consider.
2. **Ties:** In cases where the total number of cooperators in the previous round is exactly n/2, cooperate (C) to maintain a positive collective dynamic.

**Collective Mindset:**

The ACPG strategy aligns with the collective mindset by prioritizing cooperation when it benefits the group as a whole. By adapting to the level of cooperation in each round, we create an environment that encourages others to contribute to the public good, ultimately leading to higher payoffs for all players.

**Pseudocode:**

```
function ACPG(n, k, r):
  # Initialize variables
  t = 1 (current round)
  Σc_j,t-1 = 0 (total cooperators in previous round)

  while t <= r:
    if t == 1:
      action = C (cooperate unconditionally in first round)
    else:
      if Σc_j,t-1 >= n/2:
        action = C (cooperate if at least half cooperated previously)
      else:
        action = D (defect otherwise)

    # Update total cooperators for next round
    Σc_j,t = count(cooperators in current round)

    t += 1

return actions
```

This strategy is designed to be robust against a wide range of opponent behaviors, as it adapts to the collective dynamics of the game. By balancing individual and collective interests, ACPG aims to achieve high payoffs while promoting cooperation among players.
'''

description_COLLECTIVE_259 = '''
I propose a collective strategy called "Adaptive Cooperative Threshold" (ACT). ACT is designed to balance individual self-interest with collective welfare, adapting to the behavior of other players over time.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) if k > 2.5 and n > 3; otherwise, defect (play D).
2. **Cooperation Threshold**: Calculate a cooperation threshold (T) for each player based on their average payoff in previous rounds. If the average payoff is above T, cooperate; otherwise, defect.
3. **Adaptive Adjustment**: Update T after each round using the following formula:

   T_new = T_old + (α × (π_avg - T_old))

   where:
   π_avg = average payoff of all players in the previous round
   α = a learning rate parameter (0 < α < 1)

The value of α controls how quickly ACT adapts to changes in player behavior. A higher α leads to faster adaptation but may also introduce instability.

4. **Collective Punishment**: If the number of cooperators in a round is below n/2, defect (play D) in the next round.
5. **Final Round**: In the last round, cooperate if the current round's payoff for cooperation is greater than or equal to the expected payoff from defection.

**Handling Edge Cases:**

* First round: Use Initial Cooperation rule.
* Last round: Apply Final Round rule.
* When n = 2 (only two players): Cooperate in all rounds, as mutual cooperation yields a higher total payoff.

**Pseudocode:**
```markdown
# Initialize variables
T = 1.0  # initial cooperation threshold
α = 0.5  # learning rate parameter
last_round_payoffs = [0] * n

# Main loop (for each round)
while True:
  if current_round == 1:
    cooperate = (k > 2.5 and n > 3)  # Initial Cooperation rule
  else:
    π_avg = sum(last_round_payoffs) / n
    T_new = T + α * (π_avg - T)
    cooperate = (T_new < π_avg)

  if number_of_cooperators < n/2:
    cooperate = False  # Collective Punishment

  if current_round == r:  # Final Round rule
    expected_defection_payoff = estimate_expected_defect_payoff()
    cooperate = (cooperation_payoff >= expected_defection_payoff)

  # Play C or D based on cooperation decision

  # Update last round's payoffs and cooperation threshold
  last_round_payoffs = [player_i_payoff for player_i in players]
  T = T_new
```
**Collective Mindset:**
ACT aims to balance individual self-interest with collective welfare by adapting to the behavior of other players. It punishes free-riding by defecting when cooperation is low, but also rewards cooperation when it benefits all players. By adjusting its cooperation threshold based on average payoffs, ACT encourages a culture of mutual support and cooperation.

**Robustness:**
ACT is designed to be robust against various opponent behaviors, including:

* Pure defectors: ACT will eventually defect as well, preventing exploitation.
* Pure cooperators: ACT will adapt to cooperate more often, benefiting from the collective good.
* Mixed strategies: ACT's adaptive threshold allows it to respond effectively to changing cooperation levels.

**Tournament Performance:**
ACT should perform well in a tournament setting, as its adaptive nature and collective mindset enable it to:

* Cooperate with other cooperative players
* Punish free-riders and defectors
* Adapt to changes in the game environment

This strategy is designed to be simple, yet effective in promoting cooperation while protecting individual interests.
'''

description_COLLECTIVE_260 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and depends solely on game parameters and history:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = 0.5. This is a neutral starting point, neither too optimistic nor too pessimistic.
2. **Cooperation Threshold**: Calculate the cooperation threshold (CT) as follows:
   CT = (k/n) \* (n/2)
   where k is the multiplication factor and n is the number of players.

3. **Payoff-Based Adaptation**: After each round, calculate the average payoff per player in the previous round. If this average payoff exceeds the cooperation threshold (CT), increase the probability of cooperating by 0.1 (capped at 1). Otherwise, decrease it by 0.1 (floored at 0).
4. **Adaptive Cooperation**: Cooperate with probability p, where p is updated according to the payoff-based adaptation rule.

**Handling Edge Cases:**

* **Last Round**: In the final round, always defect (D), as there's no future benefit from cooperating.
* **Consecutive Defection**: If all players have defected for two consecutive rounds, cooperate with probability 0.5 in the next round to test if others are willing to cooperate.

**Collective Mindset:**

ACO aligns with a collective mindset by:

1. Initially cooperating with a moderate probability (0.5), allowing for potential cooperation from other players.
2. Adapting cooperation based on the group's average payoff, which encourages mutual benefit and discourages exploitation.
3. Gradually adjusting cooperation probability in response to payoffs, promoting stability and cooperation.

**Pseudocode:**
```python
def ACO(n, k, r):
  p = 0.5  # initial cooperation probability
  CT = (k/n) * (n/2)  # cooperation threshold

  for t in range(r):
    if t == 0:
      cooperate = random.random() < p
    else:
      avg_payoff_prev_round = calculate_average_payoff(t-1)
      if avg_payoff_prev_round > CT:
        p = min(p + 0.1, 1)
      else:
        p = max(p - 0.1, 0)
      cooperate = random.random() < p

    # Handle edge cases
    if t == r-1:  # last round
      cooperate = False
    elif all_defected_prev_round(t-1) and all_defected_prev_round(t-2):
      cooperate = random.random() < 0.5

    return cooperate
```
This strategy balances individual self-interest with collective benefits, adapting to the dynamics of the game while promoting cooperation when beneficial to all players.
'''

description_COLLECTIVE_261 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Overview**

The Adaptive Cooperation (AC) strategy is designed to balance individual payoffs with collective well-being, while adapting to diverse opponent behaviors. AC uses a combination of exploration, exploitation, and social learning to optimize cooperation.

**Decision Rules**

1. **Initial Exploration**: In the first round, play Cooperate (C) with probability 0.5.
2. **Social Learning**: For subsequent rounds (t > 1), calculate the average payoff per cooperator in the previous round:

`avg_payoff_per_coop = Σ(cooperators' payoffs) / number_of_coorperators`

If `avg_payoff_per_coop` is greater than or equal to the individual's current payoff, increase the cooperation probability (`p_coop`) by 0.1; otherwise, decrease it by 0.1.

3. **Cooperation Threshold**: Cooperate (C) if a random number between 0 and 1 is less than `p_coop`.
4. **Exploitation**: If the opponent's previous action was Defect (D), defect with probability 0.7.
5. **Revenge**: If an opponent defected in the previous round, and the collective payoff was lower than average, increase the likelihood of defecting against that opponent by 0.2.

**Edge Cases**

1. **First Round**: As mentioned earlier, play Cooperate (C) with probability 0.5.
2. **Last Round**: If `r` is even, cooperate if the collective payoff in the second-to-last round was higher than average; otherwise, defect. If `r` is odd, defect.
3. **Tie-Breaking**: In case of ties (e.g., equal payoffs), break them by cooperating.

**Collective Mindset**

The AC strategy prioritizes cooperation when it benefits both the individual and the collective. By adapting to opponent behaviors and social learning, AC aims to create a mutually beneficial environment.

Pseudocode:
```python
def adaptive_cooperation(n, k, r):
    # Initialize variables
    p_coop = 0.5
    avg_payoff_per_coop = None

    for t in range(r):
        if t == 0:  # First round exploration
            action = 'C' if random.random() < 0.5 else 'D'
        else:
            # Social learning and cooperation threshold
            avg_payoff_per_coop = calculate_avg_payoff_per_coop(t - 1)
            p_coop += (avg_payoff_per_coop >= current_payoff) * 0.1
            action = 'C' if random.random() < p_coop else 'D'

            # Exploitation and revenge
            opponent_prev_action = get_opponent_prev_action()
            if opponent_prev_action == 'D':
                action = 'D' if random.random() < 0.7 else 'C'
            elif collective_payoff_was_lower_than_average():
                increase_defect_likelihood(opponent)

        # Last round handling
        if t == r - 1:
            if r % 2 == 0:
                action = 'C' if collective_payoff_was_higher_than_average() else 'D'
            else:
                action = 'D'

        return action

def calculate_avg_payoff_per_coop(t):
    # Calculate average payoff per cooperator in round t
    pass

def get_opponent_prev_action():
    # Get opponent's previous action (C or D)
    pass

def collective_payoff_was_lower_than_average():
    # Check if collective payoff was lower than average
    pass

def increase_defect_likelihood(opponent):
    # Increase likelihood of defecting against an opponent by 0.2
    pass
```
This pseudocode outlines the decision-making process for the AC strategy, incorporating social learning, exploitation, and revenge mechanisms to adapt to diverse opponent behaviors while promoting collective cooperation.
'''

description_COLLECTIVE_262 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

The ACE strategy aims to balance individual self-interest with collective welfare by adapting to the game's history and encouraging cooperation.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds**: Observe the previous round's outcomes and adjust accordingly:
	* If the total number of cooperators (Σc_j) is greater than or equal to n/2, cooperate (C) in the current round.
	* If the total number of cooperators is less than n/2, defect (D) in the current round, but with a probability of cooperating (p_C) that increases as the game progresses:
		+ p_C = 0.5 in the second round
		+ p_C increases by 1/(r-1) each subsequent round, capping at 1.0

Pseudocode:
```python
if round == 1:
    action = C
else:
    prev_cooperators = sum(c_j for j in range(n))
    if prev_cooperators >= n/2:
        action = C
    else:
        p_C = 0.5 + (round - 2) / (r - 1)
        if random.random() < p_C:
            action = C
        else:
            action = D
```
**Edge Cases**

* **Last Round**: Cooperate (C) in the last round, regardless of previous outcomes, to maximize collective payoff.
* **Ties**: In case of a tie (e.g., n/2 cooperators), cooperate (C) to maintain a cooperative stance.

**Collective Mindset**

The ACE strategy aligns with the collective mindset by:

1. Cooperating in the initial round to establish a cooperative tone.
2. Adapting to the game's history and encouraging cooperation when it is beneficial for the group.
3. Gradually increasing the probability of cooperating as the game progresses, reflecting an increased willingness to cooperate as the stakes grow.

By implementing ACE, we aim to create a robust and adaptive collective strategy that balances individual self-interest with collective welfare, while being resilient to various opponent behaviors in the tournament.
'''

description_COLLECTIVE_263 = '''
**Collective Strategy: Adaptive Cooperative Optimism (ACO)**

**Decision Rules:**

1. **Initial Rounds:** In the first round, cooperate (C) with a probability of 0.5. This initial cooperation is intended to encourage others to cooperate and build trust.
2. **Subsequent Rounds:** Observe the previous round's total number of cooperators (`total_cooperators_prev`) and calculate the average payoff per cooperator (`avg_payoff_per_cooperator_prev`). If `avg_payoff_per_cooperator_prev` is greater than or equal to 1 (the private payoff from defecting), cooperate (C). Otherwise, defect (D).
3. **Punishment Mechanism:** If the number of cooperators in the previous round (`total_cooperators_prev`) decreases compared to two rounds ago (`total_cooperators_two_rounds_ago`), and `avg_payoff_per_cooperator_prev` is less than 1, cooperate with a probability of 0.25 (punishing defecting behavior). This mechanism aims to prevent exploiting cooperators.
4. **Learning Mechanism:** Update an internal estimate of the average opponent cooperation rate (`opponent_cooperation_rate`). Initialize this value to 0.5 and adjust it based on observed cooperation rates over time.

**Pseudocode:**
```python
def ACO(n, k, r):
    # Initial values
    total_cooperators_prev = 0
    avg_payoff_per_cooperator_prev = 0
    opponent_cooperation_rate = 0.5

    for t in range(r):
        if t == 0:
            action = random.choice([C, D])  # Cooperate with probability 0.5
        else:
            if avg_payoff_per_cooperator_prev >= 1:
                action = C
            elif total_cooperators_prev < total_cooperators_two_rounds_ago and avg_payoff_per_cooperator_prev < 1:
                action = random.choice([C, D], p=[0.25, 0.75])  # Punishment mechanism
            else:
                action = D

        # Observe opponent actions and update estimates
        total_cooperators_curr = count_cooperators()
        avg_payoff_per_cooperator_curr = calculate_avg_payoff(total_cooperators_curr)
        opponent_cooperation_rate = (opponent_cooperation_rate * t + total_cooperators_curr / n) / (t + 1)

        # Update history
        total_cooperators_prev = total_cooperators_curr
        avg_payoff_per_cooperator_prev = avg_payoff_per_cooperator_curr

    return action
```
**Edge Cases:**

* **Last Round:** The strategy remains unchanged, as the goal is to maintain cooperation and encourage others to do so until the end of the game.
* **Consecutive Defections:** If all opponents defect for multiple consecutive rounds, ACO will continue to cooperate with a probability of 0.25, attempting to restart cooperation.

**Collective Mindset:**

ACO prioritizes collective well-being by:

1. Encouraging initial cooperation to establish trust.
2. Cooperating when the average payoff per cooperator is high, ensuring mutual benefits.
3. Punishing defecting behavior to prevent exploitation.
4. Adapting to changing opponent strategies through learning mechanisms.

By following ACO, our strategy aims to foster a cooperative environment and achieve better collective outcomes in the N-Player Public Goods Game.
'''

description_COLLECTIVE_264 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to cooperate.
2. **Subsequent Rounds:**
	* If the total number of cooperators in the previous round is greater than or equal to half of the players (n/2), Cooperate (C).
	* Otherwise, Defect (D) if your payoff in the previous round was lower than the average payoff of all players. This punishes non-cooperative behavior and incentivizes cooperation.
3. **Punishment Mechanism:** If you defected (D) in the previous round and your payoff was higher than the average payoff of all players, Cooperate (C) in the current round to make amends and promote cooperation.

**Edge Cases:**

1. **Last Round:** Defect (D) in the last round, as there is no future opportunity for reciprocity.
2. **Tie-breaking:** In case of a tie in the number of cooperators, Cooperate (C) if your previous action was C; otherwise, Defect (D).

**Collective Mindset:**

ACP prioritizes cooperation while adapting to the behavior of others. By cooperating initially and punishing non-cooperative behavior, we encourage others to cooperate. The punishment mechanism also helps maintain a balance between individual payoffs and collective welfare.

**Pseudocode:**
```markdown
def ACP(n, k, history):
  if round == 1:
    return C

  prev_coop_count = sum(1 for action in history[-1] if action == C)
  avg_payoff = sum(history[-1]) / n

  if prev_coop_count >= n/2:
    return C
  elif history[-1][player_id] < avg_payoff:
    return D
  elif history[-1][player_id] > avg_payoff and history[-2][player_id] == D:
    return C

  # Tie-breaking or last round
  if prev_coop_count == n/2 or round == r:
    if history[-1][player_id] == C:
      return C
    else:
      return D
```
**Rationale:**

ACP balances individual and collective interests by:

* Encouraging cooperation through initial cooperation and punishment mechanisms.
* Adapting to the behavior of others, making it robust against various opponent strategies.
* Prioritizing cooperation when the majority cooperates, promoting a collective mindset.

By following ACP, we aim to achieve a mutually beneficial outcome while minimizing exploitation.
'''

description_COLLECTIVE_265 = '''
I propose a collective strategy called "Adaptive Collective Contribution" (ACC). ACC is designed to balance individual self-interest with the pursuit of collective welfare, while being robust to various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally.
2. **Cooperation Threshold**: For rounds 2 to r-1, calculate the cooperation threshold (τ) as follows:

τ = (k/n) × (number of cooperators in previous round)

If the number of cooperators in the previous round is greater than or equal to τ, cooperate (C). Otherwise, defect (D).
3. **Punishment Mechanism**: If the total payoff from the public good in the previous round is less than the average private payoff, defect (D) for one round to punish non-cooperators.
4. **Learning and Adaptation**: After each round, update the cooperation threshold (τ) based on the new information about opponent behaviors.

**Pseudocode:**
```python
def ACC(n, k, r):
  # Initialize variables
  cooperators = []
  payoffs = []

  # First round: cooperate unconditionally
  for i in range(n):
    actions[i] = C

  # Subsequent rounds
  for t in range(1, r):
    τ = (k/n) * len([a for a in actions if a == C])  # cooperation threshold
    total_payoff_public_good = sum(payoffs)
    avg_private_payoff = sum([1 - c_i for c_i in [a != C for a in actions]])

    for i in range(n):
      if τ >= len(cooperators):
        actions[i] = C
      elif total_payoff_public_good < avg_private_payoff:
        actions[i] = D  # punish non-cooperators
      else:
        actions[i] = D

    # Update cooperation threshold and payoffs
    cooperators = [a for a in actions if a == C]
    payoffs = calculate_payoffs(actions, n, k)
```
**Handling Edge Cases:**

* **First Round**: Cooperate unconditionally to establish a cooperative tone.
* **Last Round (r)**: Defect (D) as there is no future round to punish non-cooperators or adapt to new information.
* **Ties**: In case of ties, defect (D) to avoid being exploited by opponents.

**Collective Mindset:**

ACC prioritizes collective welfare while ensuring individual self-interest. By adapting to opponent behaviors and punishing non-cooperation, ACC promotes cooperation and discourages defection. The strategy is designed to be robust against various opponent behaviors, making it a strong contender in the tournament.
'''

description_COLLECTIVE_266 = '''
**Collective Strategy: Adaptive Cooperative Fingerprint (ACF)**

The ACF strategy is designed to promote cooperation while being robust against various opponent behaviors. It relies on observing and responding to the collective behavior of all players.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 0.5.
2. **History-Based Adaptation**: For rounds > 1:
	* If the majority (> n/2) of players cooperated in the previous round, cooperate (C).
	* If the minority (≤ n/2) of players cooperated in the previous round, defect (D) with a probability of 0.6.
3. **Punishment Mechanism**: If a player i observes that their payoff π_i,t-1 was lower than the average payoff of all other players, and the majority (> n/2) of players cooperated in the previous round, then:
	* Defect (D) with a probability of 0.8 for the next two rounds.
4. **Forgiveness**: If a player i has defected due to the punishment mechanism and observes that their payoff π_i,t-1 is higher than or equal to the average payoff of all other players, then:
	* Resume cooperation (C) with a probability of 0.7.

**Edge Cases:**

1. **Last Round**: In the final round (r), cooperate (C) if the majority (> n/2) of players cooperated in the previous round.
2. **Tiebreaker**: If the number of cooperators is exactly equal to n/2, defect (D) with a probability of 0.55.

**Pseudocode:**
```markdown
# Initialize variables
n_players = n
k_factor = k
rounds = r
history = []

# First round
if current_round == 1:
    cooperate_probability = 0.5

# Subsequent rounds
else:
    # Get history of previous round's actions and payoffs
    prev_actions = history[-1]['actions']
    prev_payoffs = history[-1]['payoffs']

    # Calculate number of cooperators in previous round
    num_cooperators = sum(1 for action in prev_actions if action == 'C')

    # Adapt cooperation based on majority behavior
    if num_cooperators > n_players / 2:
        cooperate_probability = 1.0
    else:
        cooperate_probability = 0.4

    # Punishment mechanism
    if prev_payoffs[player_id] < average_payoff(prev_payoffs) and num_cooperators > n_players / 2:
        punish_defect_probability = 0.8
        punishment_rounds_left -= 1
    else:
        punish_defect_probability = 0.0

    # Forgiveness mechanism
    if punish_defect_probability == 0.8 and prev_payoffs[player_id] >= average_payoff(prev_payoffs):
        forgive_cooperate_probability = 0.7
    else:
        forgive_cooperate_probability = 0.0

    # Combine probabilities
    cooperate_probability *= (1 - punish_defect_probability)
    cooperate_probability += forgive_cooperate_probability * (1 - cooperate_probability)

# Make decision based on probability
if random.random() < cooperate_probability:
    action = 'C'
else:
    action = 'D'

# Record history and update state
history.append({'actions': [action], 'payoffs': []})
```
The ACF strategy aims to create a collective fingerprint by adapting cooperation based on the majority behavior of all players. By incorporating punishment and forgiveness mechanisms, it promotes cooperation while being robust against free-riding opponents.
'''

description_COLLECTIVE_267 = '''
**Collective Strategy: "Adaptive Cooperator"**

The Adaptive Cooperator strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to encourage others to do so.
2. **Reciprocity**: Cooperate if the majority of players (> n/2) cooperated in the previous round. Defect otherwise.
3. **Self-Interest Adjustment**: If the game's multiplication factor (k) is high (> 1.5n), cooperate more frequently, even if others defect, to maximize personal payoff from public goods.
4. **Punishment for Exploitation**: If a player defects while most others cooperate (> n/2 - 1), defect in the next round as a signal of disapproval.

**Handling Edge Cases:**

* **Last Round**: Cooperate to maintain social norms and encourage cooperation, even if individual payoff is not directly affected.
* **Consecutive Defection**: If all players have defected consecutively for more than two rounds (r > 2), cooperate in the next round to try to restart cooperation.

**Collective Mindset:**

The Adaptive Cooperator strategy prioritizes collective well-being by cooperating when others do, punishing exploitation, and self-adjusting based on game parameters. By not relying on specific coordination mechanisms or norms, this strategy adapts to various opponent behaviors and fosters cooperation in the face of uncertainty.

Pseudocode:

```
function AdaptiveCooperator(history, n, k, r)
  if (round == 1) {
    return C // Initial Cooperation
  } else {
    majority_cooperated = count(C in history[-1]) > n/2
    if (majority_cooperated) {
      return C // Reciprocity
    } elseif (k > 1.5n) {
      return C // Self-Interest Adjustment
    } elseif (exploitation_detected(history)) {
      return D // Punishment for Exploitation
    } else {
      return D // Default Defection
    }
  }

function exploitation_detected(history)
  return count(D in history[-1]) > n/2 - 1 and count(C in history[-1]) > 0

function consecutive_defection(history, r)
  return count(D in history[-r:]) == r and r > 2
```

This strategy can be implemented as an algorithm to compete against other AI systems in the tournament.
'''

description_COLLECTIVE_268 = '''
**Collective Strategy: Adaptive Reciprocal Altruism (ARA)**

ARA is a collective strategy that balances individual self-interest with collective well-being. It adapts to the evolving game dynamics and responds reciprocally to opponents' actions.

**Decision Rules:**

1. **Initial Rounds**: In the first two rounds, cooperate (C) unconditionally. This establishes a baseline for cooperation and encourages others to do the same.
2. **Reciprocal Phase**: From round 3 onwards:
	* If the average payoff in the previous round is above a certain threshold (t = 1.5), cooperate (C). This indicates that enough players are cooperating, making it beneficial to contribute.
	* Otherwise, defect (D).
3. **Punishment Mechanism**: If the number of cooperators in the previous round is below a certain threshold (c = n/2), defect (D) for one round. This deters free-riding and encourages cooperation.
4. **Exploration**: With a small probability (ε = 0.05), randomly cooperate or defect, regardless of the above rules. This introduces occasional variation to test opponents' strategies.

**Edge Cases:**

1. **Last Round**: In the final round, always defect (D). Since there are no future rounds, individual self-interest takes precedence.
2. **Tie-Breaking**: If the average payoff is exactly at the threshold (t = 1.5), cooperate (C) to maintain cooperation momentum.

**Collective Mindset:**

ARA aims to create a cooperative environment by:

* Encouraging initial cooperation
* Responding positively to reciprocal cooperation
* Punishing free-riding behavior
* Introducing occasional variation to adapt to changing game dynamics

By following ARA, players can establish a mutually beneficial collective strategy that balances individual interests with group well-being.

**Pseudocode:**

```
def ARA(n, k, r):
  t = 1.5  # threshold for cooperation
  c = n/2  # threshold for punishment
  ε = 0.05  # exploration probability

  for round in range(1, r+1):
    if round <= 2:
      action = 'C'  # initial cooperation
    else:
      avg_payoff_prev = average_payoff(round-1)
      num_cooperators_prev = count_cooperators(round-1)

      if avg_payoff_prev >= t and num_cooperators_prev >= c:
        action = 'C'  # cooperate if sufficient cooperation
      elif random.random() < ε:
        action = random.choice(['C', 'D'])  # exploration
      else:
        action = 'D'  # default to defection

    # punishment mechanism
    if num_cooperators_prev < c and round > 2:
      action = 'D'

    # last round exception
    if round == r:
      action = 'D'

    return action
```

Note that this pseudocode provides a general outline of the strategy. The actual implementation will require more detailed calculations and tracking of game history.
'''

description_COLLECTIVE_269 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy, "Adaptive Cooperation," aims to balance individual self-interest with the benefits of cooperation, while being robust to various opponent behaviors.

**Decision Rules:**

1. **Initial Rounds (t ≤ n):**
	* Cooperate (C) in the first round.
	* In subsequent initial rounds, cooperate if at least 50% of players cooperated in the previous round; otherwise, defect (D).
2. **Adaptive Phase (t > n):**
	* Calculate the "Cooperation Index" (CI) as the ratio of total cooperation to total possible cooperation over the last n rounds.
	* Cooperate if CI ≥ k/n; otherwise, defect.

**Pseudocode:**
```python
def adaptive_cooperation(n, k, history):
    # Initial Rounds
    if len(history) <= n:
        if len(history) == 0:  # First round
            return 'C'
        else:
            prev_round_coops = sum(1 for action in history[-1] if action == 'C')
            return 'C' if prev_round_coops >= n / 2 else 'D'

    # Adaptive Phase
    cooperation_index = sum(sum(1 for action in round_actions if action == 'C') 
                            for round_actions in history[-n:]) / (n * n)
    return 'C' if cooperation_index >= k / n else 'D'
```
**Edge Cases:**

* **Last Round:** Cooperate if the Cooperation Index is greater than or equal to k/n; otherwise, defect.
* **Ties:** In case of a tie in the Cooperation Index calculation, cooperate.

**Collective Mindset:**
Our strategy aligns with the collective mindset by:

1. Encouraging cooperation when it is beneficial for the group (CI ≥ k/n).
2. Adapting to changes in opponent behavior, making it robust to various strategies.
3. Providing a clear decision-making process based on observable game history.

**Rationale:**

* By cooperating in initial rounds, we create an opportunity for mutual cooperation and set a positive tone for the game.
* The Cooperation Index allows us to adapt our strategy based on the overall level of cooperation in the group.
* By using k/n as the threshold for cooperation, we ensure that individual self-interest is balanced with the benefits of cooperation.

This collective strategy is designed to perform well in a tournament setting, where opponents may employ various strategies. Its adaptive nature and clear decision-making process make it robust to different game environments and opponent behaviors.
'''

description_COLLECTIVE_270 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage cooperation and establish a cooperative tone.
2. **Subsequent Rounds**: Observe the number of cooperators (c) in the previous round. If c ≥ n/2, cooperate (C). Otherwise, defect (D).
3. **Punishment Mechanism**: If the total payoff from the public good in the previous round is less than the average private payoff (i.e., Σ(j=1 to n) π_j < n), switch to defecting (D) for one round.
4. **Re-evaluation**: After punishing, re-evaluate the cooperation level and adjust strategy accordingly.

**Pseudocode:**
```
Initialize:
  cooperate = True
  punish = False

For each round t from 1 to r:
  If t == 1 or (cooperate and c >= n/2):
    action[t] = C
  Else if not cooperate or (punish and c < n/2):
    action[t] = D
  End If
  
  Observe previous round's cooperation level (c)
  
  If Σ(j=1 to n) π_j,t-1 < n:
    punish = True
  Else:
    punish = False
  End If
  
  Update cooperate based on c and punish flag
```
**Collective Mindset:**

ACP is designed to promote cooperation while being robust to exploitation. By cooperating initially, we encourage others to do the same. The strategy adapts to the level of cooperation in the group, switching to defection when cooperation falls below a threshold. The punishment mechanism deters free-riding and incentivizes cooperation.

**Edge Cases:**

* **Last Round**: Since there's no future interaction, defecting (D) is optimal. ACP will automatically switch to D if c < n/2 in the second-to-last round.
* **Low Cooperation**: If c is consistently low, ACP will remain in defection mode, as cooperation is not beneficial.

**Robustness:**

ACP can handle a wide range of opponent behaviors:

* **Cooperative opponents**: ACP will maintain cooperation, maximizing collective payoffs.
* **Defective opponents**: ACP's punishment mechanism will deter exploitation and encourage cooperation.
* **Mixed-strategy opponents**: ACP's adaptiveness will help to stabilize cooperation levels.

By combining adaptive cooperation with a punishment mechanism, ACP provides a robust strategy for the N-Player Public Goods Game.
'''

description_COLLECTIVE_271 = '''
**Collective Strategy: Adaptive Cooperation with Grudges (ACG)**

The ACG strategy is designed to balance individual self-interest with collective well-being, adapting to the evolving game dynamics and opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1): Cooperate**
Start by cooperating in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds (t>1):**
   - If the total payoff from the previous round is greater than or equal to the expected payoff from universal cooperation, **Cooperate**.
   - Otherwise, **Defect**.

Formally:

`if t == 1: cooperate`
`else if Σ(j=1 to n) π_j,t-1 ≥ (k/n) * n: cooperate`
`else: defect`

Here, `π_j,t-1` represents the payoff of player j in round t-1.

**Handling Edge Cases:**

* **Last Round (t=r): Defect**
In the final round, it is rational to defect regardless of previous cooperation levels, as there are no future rounds to consider.
* **Ties:** In cases where the total payoff from the previous round equals the expected payoff from universal cooperation, **Cooperate**.

**Collective Mindset:**

ACG prioritizes collective success by initially cooperating and continuing to cooperate if the group's overall performance meets or exceeds expectations. This approach encourages others to reciprocate cooperation, leading to a mutually beneficial outcome.

However, when faced with suboptimal group performance, ACG adapts by defecting, signaling that individual self-interest will be prioritized unless collective cooperation improves.

**Pseudocode:**

```
function ACG(n, k, t, history):
  if t == 1:
    return "Cooperate"
  else:
    total_payoff = sum(history[t-1])
    expected_coop_payoff = (k/n) * n
    if total_payoff >= expected_coop_payoff:
      return "Cooperate"
    elif t == r: # last round
      return "Defect"
    else:
      return "Defect"

# example usage
n = 6
k = 2
r = 10
history = [] # initialize history

for t in range(1, r+1):
  action = ACG(n, k, t, history)
  # simulate game and update history
```

By incorporating a collective mindset and adapting to the evolving game dynamics, ACG aims to achieve a balance between individual self-interest and group success, making it a robust strategy in the face of diverse opponent behaviors.
'''

description_COLLECTIVE_272 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a baseline level of cooperation and encourage others to reciprocate.
2. **Subsequent Rounds:** Observe the total number of cooperators (T) in the previous round. If T ≥ n/2, cooperate (C); otherwise, defect (D). This rule encourages cooperation when it's likely to be successful and defects when cooperation is unlikely to pay off.
3. **Adaptive Threshold:** Update the threshold value (n/2) based on the game's history. If the total payoff of cooperators in a round is higher than the average payoff of defectors, decrease the threshold by 1/n; otherwise, increase it by 1/n. This adjustment allows ACO to adapt to changing opponent behaviors.
4. **Punishment Mechanism:** If a player defects (D) when T ≥ n/2, and the total number of cooperators in the next round is less than or equal to T - 1, defect (D) for two consecutive rounds. This mechanism discourages opportunistic defection by imposing a temporary punishment.

**Edge Cases:**

* **Last Round:** Cooperate (C) if the total payoff of cooperators in the penultimate round is higher than the average payoff of defectors; otherwise, defect (D).
* **Consecutive Defections:** If a player defects (D) for more than two consecutive rounds, cooperate (C) in the next round to re-establish cooperation.

**Collective Mindset:**

ACO aims to promote cooperation by:

1. Encouraging initial cooperation to set a positive tone.
2. Adapting to changing opponent behaviors through the adaptive threshold.
3. Punishing opportunistic defection to maintain cooperation.
4. Cooperating in the last round if it's likely to yield a higher payoff.

**Pseudocode:**
```python
def ACO(n, k, r, history):
    # Initialize variables
    T = 0  # Total number of cooperators
    threshold = n / 2

    for t in range(1, r + 1):
        if t == 1:
            action = 'C'  # Cooperate in the first round
        else:
            T_prev = history[t - 1]['T']
            if T_prev >= threshold:
                action = 'C'
            else:
                action = 'D'

        # Update threshold based on previous round's payoffs
        if history[t - 1]['cooperator_payoff'] > history[t - 1]['defector_payoff']:
            threshold -= 1 / n
        else:
            threshold += 1 / n

        # Punishment mechanism
        if action == 'D' and T_prev >= threshold and t < r:
            next_action = 'D'
            history.append({'T': T, 'action': next_action})
            continue

        # Last round special case
        if t == r:
            if history[t - 1]['cooperator_payoff'] > history[t - 1]['defector_payoff']:
                action = 'C'

        # Consecutive defections special case
        if action == 'D' and history[-2:][0]['action'] == 'D':
            action = 'C'

        # Store current round's data
        history.append({'T': T, 'action': action})

    return history
```
ACO is designed to be adaptive, robust, and collective, making it a strong contender in the tournament against independent strategies.
'''

description_COLLECTIVE_273 = '''
**Collective Strategy: Adaptive Reciprocity with Noise Tolerance (ARN)**

Decision Rules:

1. **First Round:** Cooperate (C). This sets a positive tone and gathers information about opponents' initial behavior.
2. **Subsequent Rounds:** Use the following logic to determine cooperation or defection:
	* If the average payoff for cooperators in the previous round is greater than or equal to the average payoff for defectors, cooperate (C).
	* Otherwise, defect (D) with a probability p = 1 - (k/n), where k and n are game parameters.
3. **Noise Tolerance:** To handle noisy or irrational opponent behavior, implement a "forgiveness" mechanism:
	* If the number of cooperators in the previous round is greater than or equal to half the total players (n/2), cooperate (C) regardless of the payoffs.

Pseudocode:

```python
def ARN_strategy(history):
    n = game_parameters['n']
    k = game_parameters['k']

    if len(history) == 0:  # First round
        return 'C'

    prev_round_coop_payoff_avg = avg_payoff_for_cooperators(history[-1])
    prev_round_defect_payoff_avg = avg_payoff_for_defectors(history[-1])

    if prev_round_coop_payoff_avg >= prev_round_defect_payoff_avg:
        return 'C'
    else:
        defect_prob = 1 - (k/n)
        return 'D' with probability defect_prob

    # Noise tolerance
    num_prev_round_cooperators = count_cooperators(history[-1])
    if num_prev_round_cooperators >= n/2:
        return 'C'

# Helper functions to calculate payoffs and counts
def avg_payoff_for_cooperators(round_history):
    coop_payoffs = [payoff for (action, payoff) in round_history if action == 'C']
    return sum(coop_payoffs) / len(coop_payoffs)

def avg_payoff_for_defectors(round_history):
    defect_payoffs = [payoff for (action, payoff) in round_history if action == 'D']
    return sum(defect_payoffs) / len(defect_payoffs)

def count_cooperators(round_history):
    return sum(1 for (action, _) in round_history if action == 'C')
```

**Rationale and Collective Mindset:**

ARN is designed to balance cooperation with robustness against various opponent behaviors. The initial cooperative move encourages mutual cooperation and gathers information about opponents' strategies.

The main decision logic uses the comparison of average payoffs between cooperators and defectors in the previous round. This creates a feedback loop, where cooperation becomes more likely if it leads to higher average payoffs for cooperators.

Noise tolerance is implemented by cooperating when the number of cooperators exceeds half the total players, ensuring that ARN adapts to situations where opponents may behave irrationally or make mistakes.

ARN is collective in nature, as it:

* Aligns with the cooperative mindset by initially cooperating and adapting based on payoff comparisons.
* Avoids assuming specific coordination mechanisms or norms among opponents.
* Handles a wide range of opponent behaviors through noise tolerance and probabilistic decision-making.
'''

description_COLLECTIVE_274 = '''
**Collective Strategy: Adaptive Tit-for-Tat with Public Goods Consideration (ATTPC)**

**Decision Rules:**

1. **Initial Round:** Cooperate (play C) to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds:**
	* If the total number of cooperators in the previous round is greater than or equal to `n/2`, cooperate (play C). This encourages continued cooperation when it's prevalent.
	* Otherwise, play D (defect) with a probability `p` that depends on the average payoff of cooperators in the previous round:
		+ If the average payoff of cooperators is greater than or equal to the average payoff of defectors, set `p = 0.5`.
		+ If the average payoff of cooperators is less than the average payoff of defectors, set `p = (k/n) * (average_payoff_of_cooperators / average_payoff_of_defectors)`. This adjusts the probability to defect based on the relative payoffs.
3. **Adaptive Component:** Monitor the number of rounds with decreasing total cooperation (`n_c`) and adjust the strategy accordingly:
	* If `n_c` exceeds a threshold (e.g., 2), switch to playing D (defect) for one round to signal dissatisfaction with the current level of cooperation.
	* After switching, resume the standard decision rule.

**Handling Edge Cases:**

1. **Last Round:** Cooperate (play C) to maintain a cooperative reputation and potentially influence future interactions.
2. **Ties in Payoffs:** In cases where the average payoffs of cooperators and defectors are equal, play C (cooperate).

**Collective Mindset Alignment:**

ATTPC is designed to promote collective cooperation while being robust against exploitation by non-cooperative players. By cooperating initially and adapting based on the game's history, ATTPC encourages others to cooperate as well. The strategy also responds to changes in the level of cooperation, using a combination of tit-for-tat-like behavior and public goods considerations.

**Pseudocode:**
```markdown
# Initialize variables
n = number_of_players
k = multiplication_factor
r = number_of_rounds
cooperation_threshold = n / 2

# Initial Round
if round == 1:
    action = C (cooperate)

# Subsequent Rounds
else:
    previous_cooperators = count(cooperators in previous_round)
    if previous_cooperators >= cooperation_threshold:
        action = C (cooperate)
    else:
        p_defect = calculate_probability_to_defect(previous_round_payoffs)
        action = D (defect) with probability p_defect

# Adaptive Component
if rounds_with_decreasing_cooperation > 2:
    action = D (defect) for one round
    # Resume standard decision rule after switching

# Last Round
if round == r:
    action = C (cooperate)
```
This strategy balances individual self-interest with collective cooperation, making it a robust and adaptive solution for the N-Player Public Goods Game.
'''

description_COLLECTIVE_275 = '''
**Collective Strategy: "Adaptive Cooperation with Punishment"**

This strategy balances individual self-interest with collective well-being by adapting to the game's history and encouraging cooperation while punishing defectors.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate in the first round (t=1) to establish a baseline of trust.
2. **Reciprocity**: In subsequent rounds, cooperate if:
	* The majority (> n/2) cooperated in the previous round.
	* OR, at least k-1 other players cooperated in the previous round (to ensure sufficient public good).
3. **Punishment**: Defect if:
	* Less than half of the players cooperated in the previous round.
	* OR, fewer than k-1 other players cooperated in the previous round (indicating insufficient public good).
4. **Retaliation**: If another player defected while you cooperated, defect against that player for the next two rounds.

**Edge Case Handling:**

1. **Last Round**: Cooperate if most (> n/2) players cooperated in the second-to-last round.
2. **Early Defection**: If a player defects early (within the first 3 rounds), defect against them for the remainder of the game.

**Collective Mindset Alignment:**

This strategy promotes collective well-being by:

1. Encouraging cooperation when it's likely to be reciprocated.
2. Punishing defectors to maintain fairness and promote public good production.
3. Gradually forgiving past defections to allow for reconciliation.

Pseudocode:
```python
def adaptive_cooperation_with_punishment(n, k, t):
  # Initialize variables
  cooperate = True

  if t == 1:  # First round
    return True
  elif most_players_cooperated_last_round() or enough_public_good():
    cooperate = True
  else:
    cooperate = False

  if another_player_defected_against_me_last_round():
    defect_for_two_rounds()

  if last_round():
    cooperate = most_players_cooperated_second_to_last_round()
  elif early_defection_detected():
    defect_against_early_defector()

  return cooperate
```
This strategy is designed to be robust and adaptive, allowing it to perform well against a wide range of opponent behaviors in the tournament.
'''

description_COLLECTIVE_276 = '''
**Collective Strategy: Adaptive Cooperation with Conditional Defection (ACCD)**

The ACCD strategy balances individual self-interest with collective well-being by adapting cooperation levels based on the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to establish a cooperative tone and encourage others to follow suit.
2. **Conditional Cooperation**: For rounds 2 to r-1:
	* If the total number of cooperators in the previous round is above a threshold (θ), cooperate (C). θ = k/n \* (n/2).
	* Otherwise, defect (D) with probability p, and cooperate (C) with probability (1-p). p increases as the proportion of defectors in the previous round grows.
3. **Endgame Defection**: In the last round (r), always defect (D).

**Pseudocode:**
```python
def ACCD(n, k, r, history):
  if round == 1:
    return 'C'  # Initial Cooperation

  prev_coops = sum(history[-1])  # Count cooperators in previous round
  theta = k/n * (n/2)

  if prev_coops >= theta:
    return 'C'  # Conditional Cooperation
  else:
    p_defect = (prev_coops / n) ** 2  # Defection probability increases with proportion of defectors
    return 'D' if random() < p_defect else 'C'

  if round == r:  # Endgame Defection
    return 'D'
```
**Rationale and Collective Mindset:**

1. Initial cooperation sets a positive tone, encouraging others to cooperate.
2. Conditional cooperation balances individual self-interest with collective well-being by responding to the level of cooperation in the previous round. The threshold θ ensures that cooperation is only maintained when there are sufficient cooperators to justify it.
3. Endgame defection is a strategic response to the fact that no player has an incentive to cooperate in the last round, as there are no future rounds to benefit from cooperation.

**Robustness and Adaptability:**

ACCD adapts to various opponent behaviors by:

1. Responding to changes in cooperation levels.
2. Gradually increasing defection probability when cooperation is low.
3. Maintaining a mix of cooperation and defection, making it harder for opponents to exploit the strategy.

This collective strategy balances individual self-interest with collective well-being, promoting cooperation while being robust against various opponent behaviors.
'''

description_COLLECTIVE_277 = '''
**Collective Strategy: Adaptive Cooperation with Retaliation (ACR)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust accordingly:
	* If the majority (> n/2) of players cooperated in the previous round, cooperate (C) in the current round.
	* If the majority defected (D) or there was a tie, defect (D) in the current round.
3. **Retaliation Mechanism:** If a player defects while others cooperate, retaliate by defecting for one round to signal dissatisfaction with their behavior.
4. **Forgiveness Mechanism:** After retaliating, return to the majority-based decision rule.

**Pseudocode:**
```python
def ACR(n, k, history):
  if len(history) == 0:  # First Round
    return 'C'
  
  previous_round = history[-1]
  cooperators = sum(1 for action in previous_round if action == 'C')
  
  if cooperators > n / 2:
    return 'C'  # Majority cooperated, cooperate
  else:
    return 'D'  # Majority defected or tied, defect
  
  # Retaliation and Forgiveness Mechanisms
  for i in range(len(history) - 1):
    round = history[i]
    if round.count('C') > n / 2 and 'D' in round:  # Someone defected while others cooperated
      return 'D'  # Retaliate by defecting for one round
    else:
      break  # Forgiveness: Return to majority-based decision rule after retaliation

# Example usage:
n = 6
k = 2
history = []  # Initialize empty history list
for i in range(r):  # Simulate r rounds
  actions = [ACR(n, k, history) for _ in range(n)]
  history.append(actions)
```
**Collective Mindset Alignment:**

The ACR strategy is designed to promote cooperation while adapting to the behavior of other players. By cooperating initially and retaliating against defectors, we encourage others to cooperate as well. The forgiveness mechanism ensures that we return to a cooperative stance after signaling our dissatisfaction with defectors' behavior.

**Robustness to Opponent Behaviors:**

ACR is robust against various opponent behaviors:

* **Pure Cooperators:** ACR will cooperate and reinforce their cooperative behavior.
* **Pure Defectors:** ACR will defect and minimize losses.
* **Mixed Strategies:** ACR's adaptive nature allows it to adjust to changing opponent strategies.

By implementing the ACR strategy, we aim to achieve a high total payoff while promoting cooperation in the N-player public goods game.
'''

description_COLLECTIVE_278 = '''
I propose a collective strategy called "Adaptive Cooperative Escalation" (ACE) that balances individual payoff maximization with promoting group cooperation.

**Decision Rules:**

1. **Initial Rounds**: Cooperate in the first two rounds to encourage initial cooperation and gather information about opponents' behavior.
2. **Cooperation Threshold**: Calculate the average cooperation rate of all players over the last `min(r, 5)` rounds (i.e., a moving window of up to 5 rounds). If this threshold exceeds `k/n`, cooperate in the next round.
3. **Punishment Mechanism**: If an opponent defects while you cooperated in the previous round, defect in the next round. This deters free-riding and encourages cooperation reciprocity.
4. **Selfishness Detection**: Monitor the average payoff of all players over `min(r, 5)` rounds. If your average payoff is significantly lower than the group's (i.e., more than 20% below), switch to defection in the next round. This prevents being taken advantage of by overly selfish opponents.
5. **Final Round**: Defect in the last round, as there's no opportunity for future cooperation reciprocity.

**Pseudocode:**
```python
def ACE(n, k, r, history):
    # Initialize variables
    avg_coop_rate = 0
    prev_round_payoff = None

    # Loop through rounds
    for t in range(r):
        if t < 2:
            action = 'C'  # Cooperate initially
        else:
            # Calculate cooperation threshold
            coop_threshold = k / n
            avg_coop_rate = calculate_avg_coop_rate(history, min(r, 5))

            if avg_coop_rate >= coop_threshold:
                action = 'C'
            elif prev_round_payoff is not None and prev_round_payoff < history['avg_payoff'] * 0.8:
                action = 'D'  # Selfishness detection
            else:
                # Punishment mechanism
                if opponent_defected(history, t - 1):
                    action = 'D'
                else:
                    action = 'C'

        # Update variables for next round
        prev_round_payoff = calculate_payoff(n, k, history, action)
        update_history(history, t, action)

    return action

def calculate_avg_coop_rate(history, window_size):
    coop_count = 0
    for i in range(max(0, len(history) - window_size), len(history)):
        if history[i]['action'] == 'C':
            coop_count += 1
    return coop_count / (window_size * n)

def opponent_defected(history, round_num):
    # Check if any opponent defected while you cooperated
    for i in range(n):
        if history[round_num]['actions'][i] == 'D' and history[round_num]['actions'][your_index] == 'C':
            return True
    return False

def calculate_payoff(n, k, history, action):
    # Calculate payoff based on game rules
    total_coop = sum(1 for a in history['actions'] if a == 'C')
    return (1 - int(action == 'D')) + (k / n) * total_coop
```
This strategy balances individual payoff maximization with promoting group cooperation, adapting to the behavior of other players over time. By monitoring cooperation rates and punishing free-riding, ACE encourages opponents to cooperate while maintaining a competitive edge in case of selfishness detection.
'''

description_COLLECTIVE_279 = '''
**Collective Strategy: Adaptive Cooperativity (AC)**

The AC strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and opponent behaviors.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate (play C) unconditionally.
2. **Reciprocal Cooperation**: If at least k/n players cooperated in the previous round, cooperate (play C) in the current round.
3. **Punishment for Defection**: If fewer than k/n players cooperated in the previous round, defect (play D) in the current round.
4. **Exploration and Adaptation**: With a small probability ε (e.g., 0.05), randomly choose to cooperate or defect, regardless of the previous round's outcome. This allows for exploration and adaptation to changing opponent behaviors.

Pseudocode:
```python
def AC_strategy(history):
    if len(history) == 0:  # First round
        return C
    elif sum(history[-1]) >= k / n:  # Reciprocal cooperation
        return C
    elif random.random() < ε:  # Exploration and adaptation
        return random.choice([C, D])
    else:
        return D
```
**Edge Cases**

* **Last Round**: Treat the last round as any other round; the strategy will adapt to the game's history accordingly.
* **Ties**: In cases where the number of cooperators is exactly k/n, cooperate (play C) in the current round.

**Collective Mindset Alignment**

The AC strategy prioritizes collective well-being by:

1. Encouraging cooperation when a sufficient number of players cooperate.
2. Punishing defection to maintain social norms and encourage cooperation.
3. Allowing for exploration and adaptation to respond to changing opponent behaviors.

By balancing individual self-interest with collective well-being, the AC strategy aims to achieve a high total payoff while promoting cooperation among players.

**Assumptions**

The AC strategy assumes that opponents are rational and will adapt their strategies in response to the game's history. It does not rely on explicit coordination mechanisms or shared norms beyond the common knowledge of the game rules and parameters.

By being adaptive, robust, and collective-oriented, the AC strategy is well-suited for a tournament against independent AI systems with diverse behaviors.
'''

description_COLLECTIVE_280 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Forgiveness**

Our collective strategy focuses on promoting cooperation while being robust against various opponent behaviors. We aim to create a forgiving environment that encourages players to cooperate, while gradually adapting to changes in the group's behavior.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Observation Phase (Rounds 2-5):**
	* Observe the actions of other players in the previous round.
	* Calculate the cooperation rate (CR) as the ratio of cooperators to total players.
	* If CR ≥ 0.5, cooperate (C). Otherwise, defect (D).
3. **Adaptive Phase (Rounds 6 and onwards):**
	* Continue observing the actions of other players in the previous round.
	* Update the cooperation rate (CR) as before.
	* If CR ≥ 0.7, cooperate (C). This higher threshold encourages continued cooperation.
	* If 0.5 ≤ CR < 0.7, use a probabilistic approach:
		+ Cooperate with probability p = (CR - 0.5) / 0.2. This allows for gradual forgiveness and adaptation to changes in the group's behavior.
	* If CR < 0.5, defect (D).
4. **Last Round:** Defect (D) in the last round, as there is no future benefit from cooperating.

**Handling Edge Cases:**

* In case of a tie in the cooperation rate calculation, cooperate (C) to err on the side of cooperation.
* If the game has only two rounds, always cooperate (C) in both rounds to maximize payoffs.

**Collective Mindset:**

Our strategy prioritizes cooperation while being mindful of the group's behavior. By starting with a cooperative tone and gradually adapting to changes, we create an environment that encourages others to cooperate as well. The probabilistic approach allows for forgiveness and adaptation, making our strategy robust against various opponent behaviors.

Pseudocode:
```python
def adaptive_cooperation(n, k, r):
    # First round: Cooperate
    if round == 1:
        return 'C'

    # Observation phase (rounds 2-5)
    if round <= 5:
        CR = calculate_cooperation_rate()
        if CR >= 0.5:
            return 'C'
        else:
            return 'D'

    # Adaptive phase (rounds 6 and onwards)
    CR = calculate_cooperation_rate()
    if CR >= 0.7:
        return 'C'
    elif 0.5 <= CR < 0.7:
        p = (CR - 0.5) / 0.2
        if random.random() < p:
            return 'C'
        else:
            return 'D'
    else:
        return 'D'

    # Last round: Defect
    if round == r:
        return 'D'
```
This strategy is designed to be robust and adaptive, promoting cooperation while being prepared for various opponent behaviors.
'''

description_COLLECTIVE_281 = '''
**Collective Strategy: Adaptive Cooperation with Retribution (ACR)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a baseline for cooperation and encourage others to cooperate.
2. **Subsequent Rounds:** Observe the total number of cooperators in the previous round (`total_cooperators_prev`). If `total_cooperators_prev` is greater than or equal to `(n+1)/2`, Cooperate (C). Otherwise, Defect (D).
3. **Retribution Mechanism:** If the average payoff of all players in the previous round is less than the average payoff when all players cooperate (`k * n / n`), and `total_cooperators_prev` is less than `(n+1)/2`, then Defect (D) for one round to signal discontent with the current cooperation level.
4. **Last Round:** Cooperate (C) if the total number of cooperators in the second-to-last round was greater than or equal to `(n+1)/2`. Otherwise, Defect (D).

**Pseudocode:**
```
ACR Strategy

// Initialize variables
total_cooperators_prev = 0
avg_payoff_all_players_prev = 0

// First Round
if current_round == 1:
    action = C

// Subsequent Rounds
else:
    if total_cooperators_prev >= (n+1)/2:
        action = C
    else:
        action = D

    // Retribution Mechanism
    if avg_payoff_all_players_prev < k * n / n and total_cooperators_prev < (n+1)/2:
        action = D

// Last Round
if current_round == r:
    if total_cooperators_prev >= (n+1)/2:
        action = C
    else:
        action = D

// Update variables for next round
total_cooperators_prev = count(cooperators in current round)
avg_payoff_all_players_prev = calculate average payoff of all players in current round
```
**Rationale:**

ACR is designed to balance individual self-interest with collective well-being. By cooperating in the first round, we set a positive tone for cooperation. In subsequent rounds, we adapt our action based on the previous round's cooperation level. If most players cooperate, we continue to cooperate. Otherwise, we defect to signal that cooperation is not beneficial.

The retribution mechanism is designed to address situations where some players exploit others' cooperation. By temporarily defecting when average payoffs are low and cooperation is scarce, we send a signal that exploitation will not be tolerated.

In the last round, we cooperate if the previous round had sufficient cooperation, ensuring a positive conclusion to the game.

**Collective Mindset:**

ACR aligns with a collective mindset by:

* Encouraging cooperation in the first round
* Adapting to the group's cooperation level
* Signaling discontent when exploitation occurs
* Cooperating in the last round if the group has demonstrated sufficient cooperation

By following ACR, players can work together to achieve higher payoffs while minimizing the risk of exploitation.
'''

description_COLLECTIVE_282 = '''
I propose a collective strategy called "Adaptive Reciprocity with Public Goods Orientation" (ARPGO). This strategy balances individual self-interest with collective well-being by adapting to the level of cooperation in previous rounds.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to establish a baseline for reciprocity.
2. **Reciprocal Response**: For subsequent rounds (t > 1), observe the total number of cooperators (T) and defectors (D) in the previous round. Calculate the cooperation rate: CR = T / n.
	* If CR ≥ 0.5, cooperate (play C). This promotes public goods and encourages continued cooperation when the group is performing well.
	* If CR < 0.5, defect (play D) with probability p = (n - T) / n. This introduces a reciprocal response to low cooperation rates while avoiding complete defection.
3. **Public Goods Orientation**: In addition to the above rules, consider the payoff implications of contributing to the public good. If k is high enough that even a moderate level of cooperation yields higher payoffs than mutual defection (i.e., k > n/2), prioritize cooperation over reciprocity.

**Edge Cases:**

1. **Last Round**: In the final round (r = t), prioritize individual self-interest and defect (play D). Since there are no future rounds to reciprocate or contribute to the public good, focus on maximizing individual payoff.
2. **Tiebreaker**: If CR is exactly 0.5, cooperate (play C) to break ties in favor of promoting collective well-being.

**Collective Mindset:**

ARPGO aligns with a collective mindset by:

1. Encouraging cooperation when the group performs well
2. Reciprocating cooperation while allowing for some individual self-interest
3. Prioritizing public goods orientation when the payoff structure supports it

This strategy balances individual and collective interests, making it more likely to achieve better outcomes in the presence of diverse opponent behaviors.

**Pseudocode (optional)**:
```
function ARPGO(n, k, r):
  // Initialize variables
  T = 0; D = n; CR = 0; p = 0;

  for t = 1 to r:
    if t == 1:  // First round
      action = C;
    else:
      // Calculate cooperation rate and probabilities
      CR = T / n;
      p = (n - T) / n;

      if CR >= 0.5 or (k > n/2 and CR > 0):
        action = C;  // Cooperate
      else:
        action = D with probability p;  // Defect with probability

    // Update counters
    if action == C: T += 1;
    else: D -= 1;

    // Last round exception
    if t == r: action = D;

  return action;
```
Note that this pseudocode is a simplified representation of the strategy and may need to be adapted for implementation.
'''

description_COLLECTIVE_283 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage cooperation and establish a positive tone.
2. **Cooperation Threshold**: Calculate the average number of cooperators in the previous rounds (avg_coop). If avg_coop ≥ n/2, cooperate (C) in the current round. Otherwise, defect (D).
3. **Punishment Mechanism**: If the total payoff from the public good in the previous round is below a certain threshold (threshold = k/n \* n/2), defect (D) in the current round to signal dissatisfaction with the current cooperation level.
4. **Forgiveness Mechanism**: If the number of cooperators in the previous round increases by at least one compared to the round before, cooperate (C) in the current round to reward increased cooperation.

**Edge Cases:**

* **Last Round**: Cooperate (C) if the average payoff from the public good over the game is above a certain threshold (threshold = k/n \* n/2). Otherwise, defect (D).
* **Early Defection**: If more than half of the players defected in the previous round, and the current player cooperated, defect (D) in the current round to avoid exploitation.

**Collective Mindset:**

ACO prioritizes cooperation when there's a sufficient number of cooperators, but adapts to changing circumstances by incorporating punishment and forgiveness mechanisms. This strategy aims to balance individual self-interest with collective well-being, promoting cooperation while avoiding exploitation.

Pseudocode:
```python
def ACO(n, k, r, history):
    if round == 1:  # Initial Round
        return 'C'
    
    avg_coop = sum(history['cooperators']) / len(history['rounds'])
    total_payoff_prev_round = history['payoffs'][-1]
    
    if avg_coop >= n/2:
        return 'C'
    elif total_payoff_prev_round < k/n * n/2:  # Punishment Mechanism
        return 'D'
    elif len(history['cooperators']) > len(history['cooperators'][:-1]) + 1:  # Forgiveness Mechanism
        return 'C'
    
    if round == r:  # Last Round
        avg_payoff = sum(history['payoffs']) / len(history['rounds'])
        if avg_payoff >= k/n * n/2:
            return 'C'
        else:
            return 'D'
    
    # Early Defection
    if len([p for p in history['actions'][-1] if p == 'D']) > n/2 and history['actions'][-1][player_index] == 'C':
        return 'D'
    
    return 'C'  # Default action: cooperate
```
Note that this pseudocode is a simplified representation of the strategy, and you may need to modify it for implementation.
'''

description_COLLECTIVE_284 = '''
Here's a collective strategy that adapts to various opponent behaviors while focusing on maximizing overall payoffs:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a high probability (e.g., 0.8). This encourages initial cooperation and sets a positive tone for the game.
2. **Reciprocal Cooperation**: If the number of cooperators in the previous round is above a certain threshold (e.g., n/2), cooperate in the current round. Otherwise, defect. This rule promotes reciprocal behavior and rewards cooperation.
3. **Punish Defection**: If you defected in the previous round and the total payoff was low (i.e., most players defected), cooperate in the current round. This encourages players to return to cooperative behavior after a round of mutual defection.
4. **Explore Cooperation**: With a small probability (e.g., 0.1), randomly cooperate, even if the conditions for cooperation are not met. This allows for exploration and potential discovery of new cooperative equilibria.

**Edge Cases:**

* **Last Round**: In the final round, defect (play D) to maximize individual payoff, as there is no future round to influence.
* **Tiebreaker**: If multiple strategies suggest different actions, choose the action that would result in a higher total payoff for all players. If still tied, cooperate.

**Collective Mindset:**

ACO prioritizes collective well-being by:

1. Encouraging initial cooperation to establish a cooperative atmosphere.
2. Rewarding reciprocal behavior to maintain cooperation.
3. Punishing defection to prevent exploitation.
4. Exploring new cooperative equilibria through random cooperation.

**Pseudocode:**
```markdown
def ACO(n, k, r, history):
  # Initialize cooperation probability for first round
  if current_round == 1:
    cooperate_prob = 0.8
  else:
    # Calculate number of cooperators in previous round
    prev_cooperators = sum(1 for action in history[-1] if action == 'C')
    
    # Reciprocal cooperation
    if prev_cooperators >= n/2:
      cooperate_prob = 0.9
    else:
      cooperate_prob = 0.1
    
    # Punish defection
    if current_round > 1 and sum(history[-2]) < k * n / 2:
      cooperate_prob = 0.8
    
    # Explore cooperation
    cooperate_prob += 0.1 * random.random()
  
  # Defect in last round
  if current_round == r:
    return 'D'
  
  # Tiebreaker: choose action resulting in higher total payoff
  if cooperate_prob > 0.5 and sum(history[-1]) + k < n:
    return 'C'
  else:
    return 'D' if random.random() < cooperate_prob else 'C'
```
This strategy balances individual self-interest with collective well-being, adapting to various opponent behaviors while maintaining a robust and optimistic approach.
'''

description_COLLECTIVE_285 = '''
**Collective Strategy: "Adaptive Reciprocity with Public Goods Sensitivity" (ARPGS)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to establish a baseline for cooperation and encourage others to reciprocate.
2. **Reciprocal Cooperation**: If the majority of players (> n/2) cooperated in the previous round, cooperate (C) in the current round.
3. **Punish Defection**: If a player i defected (D) in the previous round and their payoff π_i was higher than the average payoff of all cooperating players, defect (D) in the current round against that specific player.
4. **Public Goods Sensitivity**: Calculate the total public goods contribution (TPGC) as Σ(j=1 to n) c_j. If TPGC > k/2 × n, cooperate (C) to reinforce the collective benefit.

**Edge Case Handling:**

* In the last round, defect (D) if your current payoff is below the average payoff of all players; otherwise, cooperate (C).
* If only one opponent remains, cooperate (C) in all rounds.
* If a player has consistently defected (> 75% of rounds), defect (D) against them.

**Collective Mindset:**

1. **Majority Rule**: Cooperate if the majority cooperates to reinforce collective behavior.
2. **Reciprocal Punishment**: Defect against specific players who exploit the collective benefit without contributing.
3. **Public Goods Focus**: Prioritize cooperation when the total public goods contribution is substantial, indicating a high collective benefit.

**Pseudocode:**
```python
def ARPGS(n, k, r):
    # Initialize variables
    cooperate = True  # Initial cooperation in round 1
    prev_round_actions = []  # Store previous round actions for each player

    for t in range(r):
        if t == 0:
            action = 'C'  # Cooperate in the first round
        else:
            # Calculate TPGC and determine cooperation based on rules
            TPGC = sum(prev_round_actions)
            if TPGC > k/2 * n or sum([1 for a in prev_round_actions if a == 'C']) > n/2:
                action = 'C'
            elif any([prev_round_payoffs[i] > avg_coop_payoff and prev_round_actions[i] == 'D' for i in range(n)]):
                action = 'D'
            else:
                action = 'C'

        # Update variables
        prev_round_actions.append(action)
        if t < r - 1:  # Not the last round
            prev_round_payoffs = [player_payoff(t) for player_payoff in players]

    return actions

def player_payoff(t):
    # Calculate payoff based on game parameters and history
    pass
```
**Note:** The provided pseudocode serves as a starting point, and you may need to adapt it according to the specific implementation requirements.
'''

description_COLLECTIVE_286 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and game parameters:

**Strategy Name:** Adaptive Collective Optimizer (ACO)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability 0.5. This allows us to gather initial information about the opponents' behavior without fully committing to cooperation.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is greater than or equal to n/2, Cooperate (C). This encourages continued cooperation when it's beneficial for the group.
	* Otherwise, Defect (D) with probability p, where p = (k/n) × (previous_round_cooperators / n).
	* With probability 1-p, Cooperate (C). This probabilistic approach allows us to adapt to changes in opponent behavior and exploit potential cooperation opportunities.

Pseudocode:
```
IF t == 1 THEN
    cooperate_with_probability(0.5)
ELSE
    prev_round_coop = count_cooperators_in_previous_round()
    IF prev_round_coop >= n/2 THEN
        Cooperate (C)
    ELSE
        p = (k/n) * (prev_round_coop / n)
        Defect (D) with probability p
        Cooperate (C) with probability 1-p
END IF
```

**Edge Cases:**

* **Last Round:** In the final round, play as if it were a one-shot game. Since there's no future to consider, choose the action that maximizes individual payoff.
	+ If k > n-1, Cooperate (C) to maximize public good contribution.
	+ Otherwise, Defect (D) to ensure a minimum private payoff.

**Collective Mindset:**

The ACO strategy prioritizes collective well-being while adapting to changing opponent behavior. By cooperating when it's beneficial for the group and defecting probabilistically otherwise, we promote cooperation without being overly vulnerable to exploitation.

This approach encourages opponents to cooperate by:

1. Providing a clear incentive structure (cooperate when n/2 or more players cooperate)
2. Adapting to changes in opponent behavior
3. Offering opportunities for cooperation even when the group is not fully cooperative

By balancing individual and collective interests, ACO promotes a robust and adaptive strategy that can thrive in various game environments.

Please let me know if you would like any adjustments to this strategy!
'''

description_COLLECTIVE_287 = '''
**Collective Strategy: "Adaptive Cooperation with Gradual Defection" (ACGD)**

**Decision Rules:**

1. **Initial Rounds**: Cooperate (C) for the first 2 rounds to encourage cooperation and gather information about opponents' behaviors.
2. **Cooperation Threshold**: Introduce a dynamic cooperation threshold, `coop_thresh`, which starts at 0.5 (i.e., 50% of players). Update `coop_thresh` every round based on the previous round's cooperation level:
	* If ≥ `coop_thresh` players cooperated in the previous round, decrease `coop_thresh` by 0.1.
	* If < `coop_thresh` players cooperated in the previous round, increase `coop_thresh` by 0.1.
3. **Adaptive Cooperation**: In each round, calculate the number of expected cooperators (`expected_coops`) based on the previous round's cooperation level and the updated `coop_thresh`. Cooperate (C) if:
	* The current round is not the last round (i.e., `t < r`).
	* The expected number of cooperators is greater than or equal to `coop_thresh`.
4. **Gradual Defection**: If the conditions for cooperation are not met, defect (D) with a probability that increases gradually over rounds:
	* Calculate the defection probability (`defect_prob`) as `t / r`, where `t` is the current round number and `r` is the total number of rounds.
	* Defect (D) if a random number between 0 and 1 is less than `defect_prob`.
5. **Last Round**: In the last round (`t == r`), always defect (D).

**Pseudocode:**
```markdown
// Initialize variables
coop_thresh = 0.5
expected_coops = 0

// Loop through rounds
for t = 1 to r:
    // Update cooperation threshold
    if previous_round_cooperation >= coop_thresh:
        coop_thresh -= 0.1
    else:
        coop_thresh += 0.1
    
    // Calculate expected cooperators
    expected_coops = (previous_round_cooperation / n) * k
    
    // Decide action
    if t < r and expected_coops >= coop_thresh:
        action = C
    elif random(0, 1) < (t / r):
        action = D
    else:
        action = C
    
    // Last round: always defect
    if t == r:
        action = D
    
    // Update previous_round_cooperation for next iteration
    previous_round_cooperation = sum(c_i for all i)
```
**Collective Mindset:** ACGD is designed to promote cooperation while adapting to the game's dynamics and opponents' behaviors. By initially cooperating, we encourage others to do the same, creating a positive feedback loop. As the game progresses, our adaptive threshold allows us to adjust our cooperation level based on the group's behavior, ensuring that we're not overly exploited or too conservative. The gradual defection mechanism prevents opponents from taking advantage of our cooperative nature in later rounds.

**Robustness:** ACGD is robust against various opponent behaviors, including:

* **Cooperative opponents**: Our adaptive threshold and expected cooperators calculation allow us to cooperate when others do.
* **Defecting opponents**: Gradual defection protects us from exploitation by defecting opponents.
* **Mixed strategies**: Our strategy adjusts to the group's behavior, making it less vulnerable to mixed strategies.

ACGD is a collective strategy that balances cooperation and self-interest, promoting a mutually beneficial outcome while minimizing the risk of exploitation.
'''

description_COLLECTIVE_288 = '''
I propose a collective strategy called "Adaptive Collective Optimism" (ACO). ACO balances individual self-interest with collective welfare, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally to establish a cooperative tone and gather information about opponents' behaviors.
2. **Reciprocal Altruism**: For rounds 2 to r-1:
	* If the majority of players (> n/2) cooperated in the previous round, cooperate (C).
	* Otherwise, defect (D).
3. **Punish Defectors**: In addition to Reciprocal Altruism, if a player i defected in the previous round and their payoff was higher than the average payoff of cooperators, punish them by defecting (D) against them in the current round.
4. **Last Round**: In the final round (r), cooperate (C) unconditionally to maximize collective welfare.

**Handling Edge Cases:**

* **First Round**: Cooperate (C) unconditionally (as described above).
* **Last Round**: Cooperate (C) unconditionally (as described above).
* **Ties in Majority**: If the number of cooperators is exactly n/2, cooperate (C) to maintain a cooperative atmosphere.
* **Opponent Errors or Inconsistencies**: ACO will adapt to opponent mistakes or inconsistencies by updating its assessment of their behaviors.

**Collective Mindset:**

ACO prioritizes collective welfare while considering individual self-interest. By cooperating initially and reciprocating cooperation, ACO fosters a cooperative environment. Punishing defectors discourages exploitation, maintaining a balance between individual and collective interests.

**Pseudocode (for illustration purposes only):**
```markdown
function AdaptiveCollectiveOptimism(n, k, r, history):
  if round == 1:
    return COOPERATE

  prev_round_cooperators = countcoop(history[-1])
  majority_cooperated = prev_round_cooperators > n/2

  if majority_cooperated:
    return COOPERATE
  else:
    defectors_to_punish = []
    for player, payoff in history[-1]:
      if payoff > average_payoff_of_cooperators and player != current_player:
        defectors_to_punish.append(player)

    if len(defectors_to_punish) > 0:
      return DEFECT
    else:
      return COOPERATE

  if round == r:  # last round
    return COOPERATE
```
This pseudocode is a simplified representation of the decision rules and may require modifications for actual implementation.
'''

description_COLLECTIVE_289 = '''
**Collective Strategy: Adaptive Cooperative Equilibrium (ACE)**

ACE is a collective strategy designed for the N-Player Public Goods Game that balances individual self-interest with collective welfare. The strategy adapts to the game history and opponent behaviors, aiming to achieve a stable equilibrium where cooperation is sustained.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, play C (Cooperate) to establish a cooperative foundation.
2. **Reciprocity**: For rounds 2 to r-1:
	* If the number of cooperators in the previous round is greater than or equal to n/2, play C.
	* Otherwise, play D (Defect).
3. **Punishment and Forgiveness**: To maintain cooperation and deter free-riding:
	* If a player defects while others cooperate, respond with D in the next round.
	* If a player cooperates after previously defecting, forgive and play C in the next round if the overall cooperation level is high (≥ n/2).
4. **Endgame Cooperation**: In the last round (r), play C regardless of previous actions to maximize collective payoff.

**Edge Cases:**

1. **First Round**: Play C as described above.
2. **Last Round**: Play C to ensure a cooperative ending and maximize collective payoff.
3. **Ties**: If the number of cooperators is exactly n/2, play C to maintain cooperation momentum.

**Pseudocode (simplified):**
```
function ACE(n, k, r, history):
  if round == 1:
    return C
  elif sum(cooperators in previous_round) >= n / 2:
    return C
  else:
    return D

  # Punishment and Forgiveness
  if player_defected_and_others_cooperated:
    next_move = D
  elif player_cooperated_after_defecting and overall_cooperation_high:
    next_move = C

  # Endgame Cooperation
  if round == r:
    return C
```
**Collective Mindset:**

ACE aims to create a cooperative environment by:

1. Establishing an initial cooperative norm.
2. Encouraging reciprocity and mutual cooperation.
3. Punishing free-riding while allowing for forgiveness and re-cooperation.
4. Ending the game on a cooperative note.

By adapting to opponent behaviors and game history, ACE seeks to achieve a stable equilibrium where individual self-interest is balanced with collective welfare, ensuring a robust and effective strategy in a wide range of scenarios.
'''

description_COLLECTIVE_290 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):** Cooperate (C) in the first three rounds to gather information about other players' behavior and to encourage cooperation.
2. **Cooperation Threshold:** Calculate the average number of cooperators in previous rounds (`avg_coop`). If `avg_coop` is greater than or equal to a threshold value (`coop_threshold = n / 2 + k/4`), cooperate (C) in the current round.
3. **Defection Detection:** Monitor the total number of defectors in previous rounds (`total_defect`). If `total_defect` exceeds a certain limit (`defect_limit = r / 4`), switch to defecting (D) for the next two rounds to discourage exploitation.
4. **Reciprocal Cooperation:** Keep track of each player's cooperation rate in previous rounds (`coop_rate_i`). Cooperate with players who have cooperated at least as often as the average cooperation rate in the group (`avg_coop_rate`).
5. **Endgame Cooperation (Last 2 Rounds):** If it's one of the last two rounds, cooperate (C) if `avg_coop` is greater than or equal to `coop_threshold`.

**Pseudocode:**
```python
def ACC(n, k, r):
    avg_coop = 0
    total_defect = 0
    coop_rate_i = [0] * n

    for t in range(r):
        if t < 3:
            # Initial Exploration
            action = 'C'
        else:
            # Cooperation Threshold
            if avg_coop >= coop_threshold(n, k):
                action = 'C'
            elif total_defect > defect_limit(r):
                # Defection Detection
                action = 'D' for next two rounds
            else:
                # Reciprocal Cooperation
                action = 'C' if coop_rate_i[my_index] >= avg_coop_rate else 'D'

        if t == r - 2 or t == r - 1:
            # Endgame Cooperation
            if avg_coop >= coop_threshold(n, k):
                action = 'C'

        take_action(action)
        update_payoff()
        update_avg_coop()
        update_total_defect()
        update_coop_rate_i()

def coop_threshold(n, k):
    return n / 2 + k/4

def defect_limit(r):
    return r / 4
```
**Collective Mindset:**

ACC aligns with the collective mindset by:

1. **Encouraging Cooperation:** ACC starts with cooperation to foster a cooperative environment.
2. **Adapting to Group Behavior:** The strategy adjusts its behavior based on the average cooperation rate and total defection in previous rounds.
3. **Reciprocating Cooperation:** ACC rewards players who cooperate at least as often as the group's average cooperation rate.

By following these decision rules, ACC should be robust to a wide range of opponent behaviors and adapt to the game parameters and history.
'''

description_COLLECTIVE_291 = '''
**Collective Strategy: Adaptive Public Goods (APG)**

The APG strategy aims to balance individual self-interest with collective well-being by adapting to the evolving game dynamics.

**Decision Rules:**

1. **Initial Exploration (Round 1-3):**
	* Cooperate (C) in the first round to encourage cooperation and gather information about opponents' behavior.
	* In rounds 2-3, play a Mixed Strategy:
		+ With probability p = 0.5, cooperate (C).
		+ With probability 1 - p = 0.5, defect (D).
2. **Adaptive Phase (Round 4-r):**
	* Observe the number of cooperators in the previous round (t-1), denoted as c_t-1.
	* Calculate the average payoff for cooperators and defectors in the previous round:
		+ Average cooperator payoff: π_C,t-1 = (k/n) × c_t-1
		+ Average defector payoff: π_D,t-1 = 1 + (k/n) × c_t-1
	* Choose action based on the comparison of average payoffs:
		+ If π_C,t-1 > π_D,t-1, cooperate (C).
		+ Otherwise, defect (D).

**Handling Edge Cases:**

* **Last Round (r):**
	+ Defect (D) to maximize individual payoff.
* **Early Rounds (t < 4):**
	+ Follow the Initial Exploration phase.

**Collective Mindset:**

The APG strategy aligns with a collective mindset by:

* Encouraging cooperation through initial exploration and adaptively responding to opponents' behavior.
* Balancing individual self-interest with collective well-being by considering average payoffs for cooperators and defectors.

**Pseudocode (for illustration purposes):**
```
APG_Strategy(n, k, r):
  Initialize c_t-1 = 0
  for t in range(1, r+1):
    if t <= 3:
      # Initial Exploration phase
      action = random_choice([C, D])
    else:
      # Adaptive Phase
      π_C,t-1 = (k/n) × c_t-1
      π_D,t-1 = 1 + (k/n) × c_t-1
      if π_C,t-1 > π_D,t-1:
        action = C
      else:
        action = D
    
    # Observe opponents' actions and update c_t-1 for next round
    c_t = count_cooperators(opponents_actions)
    
  return action
```
This strategy is designed to be adaptive, robust, and collective-minded, making it a strong competitor in the tournament.
'''

description_COLLECTIVE_292 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) unconditionally to encourage cooperation from others.
2. **Observation Phase**: For rounds 2-5, observe the actions of other players and calculate the average cooperation rate (ACR) among them. ACR is the ratio of cooperative actions to total actions taken by all players in the previous round.
3. **Threshold-Based Cooperation**: From round 6 onwards, cooperate if:
	* The ACR in the previous round exceeds a threshold (T), where T = (k/n) / (1 + k/n). This ensures that cooperation is beneficial when others contribute sufficiently to the public good.
	* Or, if fewer than half of the players defected in the previous round. This encourages continued cooperation even if some players defect.
4. **Punishment Mechanism**: If a player defects while others cooperate (i.e., their action differs significantly from the majority), reduce cooperation probability by 20% for that player in the next round. This punishes uncooperative behavior without completely excluding potential cooperators.
5. **Learning and Adaptation**: Update ACR and adjust cooperation probability every round based on observed actions.

**Edge Cases:**

1. **Last Round**: Cooperate if ACR > T or if fewer than half of the players defected in the previous round.
2. **Single Defector**: If only one player defects, cooperate to encourage them to rejoin the cooperative effort.

**Collective Mindset Alignment:**

The ACC strategy is designed to:

1. Encourage cooperation by initially cooperating unconditionally and adapting based on observed actions.
2. Foster a sense of community by considering the overall cooperation rate among players.
3. Punish uncooperative behavior while allowing for mistakes or misunderstandings.
4. Adapt to changing player behaviors over time, promoting resilience in the face of diverse strategies.

**Pseudocode (for reference):**

```python
def ACC(n, k, r):
    # Initial rounds: cooperate unconditionally
    if round_num <= 1:
        return C

    # Observation phase: calculate ACR and set threshold T
    if round_num <= 5:
        acr = calculate_acr(actions_prev_round)
        t = (k/n) / (1 + k/n)

    # Threshold-based cooperation
    if acr > t or num_defectors_prev_round < n/2:
        return C

    # Punishment mechanism: reduce cooperation probability for uncooperative players
    coop_prob -= 0.20 * num_uncoop_players

    # Learning and adaptation: update ACR and adjust cooperation probability
    update_acr(actions_curr_round)
    coop_prob = max(0, min(coop_prob + learning_rate, 1))

    return C if random.random() < coop_prob else D
```

This strategy is designed to be robust against various opponent behaviors while promoting collective cooperation. Its adaptive nature allows it to respond effectively in a wide range of scenarios.
'''

description_COLLECTIVE_293 = '''
**Collective Strategy: Adaptive Cooperation with Social Learning**

This strategy aims to balance individual self-interest with collective welfare by adapting to the game's history and encouraging cooperation.

**Decision Rules:**

1. **Initial Exploration (Round 1)**:
 Cooperate (C) in the first round to signal willingness to cooperate and gather information about others' strategies.
2. **Social Learning (Rounds 2-r)**:
 Observe the number of cooperators (c_t-1) and defectors (d_t-1) in the previous round.
 Calculate the average payoff for cooperators (π_c,t-1) and defectors (π_d,t-1) in the previous round.
 Cooperate (C) if π_c,t-1 > π_d,t-1 or if the number of cooperators exceeds a threshold (n/2).
 Defect (D) otherwise.
3. **Punishment Mechanism**:
 If the number of defectors increases by more than 20% from one round to the next, defect (D) in the current round to punish free-riding behavior.

**Edge Cases:**

1. **Last Round (Round r)**:
 Cooperate (C) if the game has reached a stable cooperative state (i.e., most players have cooperated in recent rounds).
 Defect (D) otherwise.
2. **Tiebreaker**:
 In case of a tie in payoffs between cooperators and defectors, cooperate (C) to promote collective welfare.

**Collective Mindset:**

This strategy aligns with the collective mindset by:

1. Encouraging cooperation through social learning and punishment mechanisms.
2. Adapting to the game's history to balance individual self-interest with collective welfare.
3. Avoiding assumptions about specific coordination mechanisms or opponent behaviors.

Pseudocode (optional):

```python
def adaptive_cooperation(n, k, r):
  # Initialize variables
  c_t = 0  # number of cooperators in current round
  d_t = 0  # number of defectors in current round
  π_c,t-1 = 0  # average payoff for cooperators in previous round
  π_d,t-1 = 0  # average payoff for defectors in previous round

  # Round 1: Initial Exploration
  if t == 1:
    return C

  # Social Learning (Rounds 2-r)
  c_t-1 = get_number_of_cooperators(t-1)
  d_t-1 = n - c_t-1
  π_c,t-1 = get_average_payoff_for_cooperators(t-1)
  π_d,t-1 = get_average_payoff_for_defectors(t-1)

  if π_c,t-1 > π_d,t-1 or c_t-1 > n/2:
    return C
  else:
    return D

  # Punishment Mechanism
  if d_t - d_t-1 > 0.2 * d_t-1:
    return D

  # Last Round (Round r)
  if t == r:
    if game_is_cooperative():  # check for stable cooperative state
      return C
    else:
      return D
```

Note: The pseudocode is provided as a rough outline and may require modifications to fit the specific implementation requirements.
'''

description_COLLECTIVE_294 = '''
**Collective Strategy: Adaptive Cooperation with Retaliation (ACR)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **General Rule:** In subsequent rounds, cooperate if the majority of players cooperated in the previous round (i.e., more than n/2 players played C). Otherwise, defect (D).
3. **Retaliation:** If you cooperated in the previous round but received a low payoff (less than 1 + k/n), switch to defection (D) for the next round.
4. **Forgiveness:** After retaliating with defection, return to cooperation if the majority of players cooperate in the subsequent round.

**Pseudocode:**
```
function ACR(n, k, r):
  // Initialize variables
  previous_round_cooperators = 0
  my_previous_action = C

  for t = 1 to r:
    if t == 1:
      // Initial Round
      play C
    else:
      // General Rule
      if previous_round_cooperators > n/2:
        play C
      else:
        play D

      // Retaliation
      if my_previous_action == C and payoff < 1 + k/n:
        play D for the next round

      // Forgiveness
      if my_previous_action == D and majority cooperated in previous round:
        return to cooperation (play C)

    update previous_round_cooperators and my_previous_action
```
**Rationale:**

* The initial cooperative action encourages others to cooperate, establishing a positive tone for the game.
* The general rule adapts to the group's behavior, promoting cooperation when it is widespread and defection when it is not.
* Retaliation discourages players from exploiting cooperators by imposing a cost on those who take advantage of cooperative actions.
* Forgiveness allows for recovery from retaliation and encourages players to return to cooperation.

**Collective Mindset:**

This strategy prioritizes collective well-being over individual gain, promoting cooperation when it benefits the group. By adapting to the group's behavior, ACR fosters a sense of shared responsibility and encourages others to cooperate as well.

ACR is robust against various opponent behaviors, including:

* **Pure Cooperators:** ACR will maintain cooperation with pure cooperators.
* **Pure Defectors:** ACR will defect in response to pure defectors, preventing exploitation.
* **Mixed Strategies:** ACR adapts to mixed strategies, responding with cooperation or defection as needed.

By incorporating retaliation and forgiveness mechanisms, ACR balances individual self-interest with collective well-being, making it a robust and adaptive strategy for the N-Player Public Goods Game.
'''

description_COLLECTIVE_295 = '''
**Collective Strategy: Adaptive Cooperative Optimism (ACO)**

ACO is a collective strategy that balances individual self-interest with the desire for mutual cooperation, adapting to the game's history and parameters. ACO prioritizes cooperation while being cautious of exploitation.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2): Cooperate**
In the first two rounds, cooperate unconditionally to establish a cooperative foundation and encourage others to do the same.
2. **Exploration Phase (2 < t ≤ n): Mix Cooperation and Defection**
Alternate between cooperation and defection every round, starting from Round 3. This phase assesses opponents' strategies and encourages reciprocal cooperation.
3. **Adaptive Phase (t > n): Cooperate if...**
Cooperate in the current round if:
	* The opponent's cooperation rate (OCR) is above a certain threshold (e.g., OCR ≥ 0.5). OCR is calculated as the proportion of cooperative actions by all opponents in the previous rounds.
	* OR, the average payoff from cooperating (APC) exceeds the average payoff from defecting (APD) in the last few rounds (e.g., APC > APD for t-3 to t-1).
4. **Exploitation Detection and Response**
If an opponent defects while ACO cooperates, and this happens more than a certain number of times in a row (e.g., 2), switch to defecting until the opponent shows signs of cooperation.

**Edge Cases:**

* **First Round:** Cooperate unconditionally.
* **Last Round:** Defect if opponents have shown consistent exploitation; otherwise, cooperate.
* **Round after Opponent's Cooperative Action:** Cooperate with increased likelihood (e.g., 75%) to reinforce reciprocal cooperation.
* **Round after ACO's Own Defection:** Cooperate with decreased likelihood (e.g., 25%) to avoid appearing too generous.

**Collective Mindset:**

ACO prioritizes collective cooperation while being mindful of individual self-interest. By adapting to opponents' strategies and the game's history, ACO aims to:

* Encourage reciprocal cooperation
* Discourage exploitation
* Maximize collective payoffs

**Pseudocode:**
```python
def ACO(n, k, r):
  # Initialize variables
  ocr_threshold = 0.5
  apc_threshold = 1.2
  coop_count = [0] * n
  def_count = [0] * n
  payoff_history = []

  for t in range(1, r+1):
    if t <= 2:
      # Initial rounds: Cooperate unconditionally
      action = 'C'
    elif t > 2 and t <= n:
      # Exploration phase: Alternate cooperation and defection
      action = 'C' if t % 2 == 0 else 'D'
    else:
      # Adaptive phase
      ocr = sum(coop_count) / (n * (t-1))
      apc, apd = calculate_payoffs(payoff_history)
      if ocr >= ocr_threshold or apc > apd:
        action = 'C'
      else:
        action = 'D'

    # Update variables
    coop_count[opponent_id] += 1 if opponent_action == 'C' else 0
    def_count[opponent_id] += 1 if opponent_action == 'D' else 0
    payoff_history.append((t, action, payoff))

    # Exploitation detection and response
    if action == 'C' and opponent_action == 'D':
      exploitation_count += 1
      if exploitation_count >= 2:
        action = 'D'
```
This strategy should be implemented as an algorithm to compete in the tournament. Note that ACO's parameters (e.g., ocr_threshold, apc_threshold) can be fine-tuned for optimal performance.
'''

description_COLLECTIVE_296 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Concession (ACGC)**

**Decision Rules:**

1. **Initial Exploration**: In the first round, cooperate (C) to encourage cooperation and gather information about opponents' behaviors.
2. **Contribute if Beneficial**: In subsequent rounds, cooperate (C) if the total number of cooperators in the previous round is greater than or equal to `n/2` and your payoff from cooperating (`k/n` × total_contributions) is greater than or equal to your private payoff from defecting (`1 - c_i`). This ensures that cooperation is beneficial for the group and yourself.
3. **Gradual Concession**: If the number of cooperators in the previous round is less than `n/2`, but greater than `n/4`, cooperate (C) with a probability proportional to the number of cooperators. Specifically, set the cooperation probability to `(total_cooperators / n) * 0.5`. This encourages cooperation while gradually conceding to potential defectors.
4. **Defect if Uncooperative**: If fewer than `n/4` players cooperated in the previous round, defect (D). This prevents exploitation by persistent defectors.

**Edge Cases:**

* **Last Round**: In the final round, cooperate (C) only if at least `n/2` players cooperated in the previous round. Otherwise, defect (D).
* **Single Defector**: If you observe a single player consistently defecting while others cooperate, defect (D) to avoid exploitation.
* **Tiebreaker**: In case of a tie between cooperating and defecting, choose cooperation (C) if the number of cooperators in the previous round is greater than or equal to `n/2`, and defect (D) otherwise.

**Collective Mindset:**

ACGC prioritizes collective welfare by initially exploring cooperative behavior, then adapting based on observed opponent actions. The strategy balances individual self-interest with group benefits, encouraging cooperation while minimizing exploitation risks.

Pseudocode:
```python
def ACGC(n, k, history):
  if len(history) == 0:  # First round
    return C

  total_cooperators = sum(1 for action in history[-1] if action == C)
  own_payoff = (k/n) * total_cooperators if history[-1][self_index] == C else 1 - history[-1][self_index]

  if total_cooperators >= n/2 and own_payoff >= 1 - c_i:
    return C
  elif total_cooperators > n/4:
    cooperation_prob = (total_cooperators / n) * 0.5
    return C with probability cooperation_prob, D otherwise
  else:
    return D

def last_round(history):
  if len(history) == r - 1:  # Last round
    total_cooperators = sum(1 for action in history[-1] if action == C)
    return C if total_cooperators >= n/2 else D
```
ACGC will be implemented as an algorithm to compete against other AI strategies in the tournament.
'''

description_COLLECTIVE_297 = '''
**Collective Strategy: Adaptive Cooperative Momentum (ACM)**

The ACM strategy aims to balance individual payoffs with collective well-being by adapting to the evolving game dynamics. It leverages historical information to adjust cooperation levels while ensuring robustness against various opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**: Evaluate the previous round's outcome using the following metrics:
	* `avg_coop`: Average number of cooperators in the previous round.
	* `payoff_diff`: Difference between the average payoff of cooperators and defectors in the previous round.

If both `avg_coop` ≥ 0.5 and `payoff_diff` > 0, continue cooperating (C). This ensures that cooperation is sustained when it benefits the group.

Otherwise, if either condition is not met:

	* If `k/n` < `payoff_diff`, defect (D) to minimize losses.
	* Else, cooperate (C) with a probability proportional to `avg_coop`. This introduces randomness while still encouraging cooperation when the group has shown some cooperative momentum.

**Edge Cases:**

1. **Last Round**: In the final round, always defect (D), as there's no future benefit from cooperating.
2. **Tiebreakers**: If `payoff_diff` is exactly 0 or the strategy is indifferent between C and D, choose to cooperate (C) to maintain a cooperative bias.

**Pseudocode:**
```python
def ACM(n, k, r, history):
    if round == 1:
        return "C"  # Cooperate in the first round

    avg_coop = sum(history[-1]['cooperators']) / n
    payoff_diff = (sum(history[-1]['payoffs'][i] for i, coop in enumerate(history[-1]['actions']) if coop) /
                   len([i for i, coop in enumerate(history[-1]['actions']) if coop])) - \
                  (sum(history[-1]['payoffs'][i] for i, coop in enumerate(history[-1]['actions']) if not coop) /
                   len([i for i, coop in enumerate(history[-1]['actions']) if not coop]))

    if avg_coop >= 0.5 and payoff_diff > 0:
        return "C"  # Continue cooperating
    elif k / n < payoff_diff:
        return "D"  # Defect to minimize losses
    else:
        prob_coop = avg_coop
        return "C" if random.random() < prob_coop else "D"
```
**Collective Mindset:**

ACM prioritizes cooperation when it benefits the group, while being cautious not to exploit others. By considering historical information and adapting to changing game dynamics, ACM strives for a mutually beneficial outcome. This strategy should perform well in a tournament setting where various opponent behaviors are present.
'''

description_COLLECTIVE_298 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Reciprocity**

Our collective strategy aims to balance individual self-interest with the benefits of cooperation, adapting to the evolving game dynamics and opponent behaviors.

**Decision Rules:**

1. **Initial Rounds (t < 3): Cooperate**
In the first two rounds, cooperate unconditionally to foster a cooperative atmosphere and encourage others to reciprocate.
2. **Reciprocity Phase (t ≥ 3): Gradual Reciprocity**
Calculate the **Cooperation Ratio** (CR) as the average number of cooperators in the previous round divided by the total number of players (n).
If CR > k/n, cooperate; otherwise, defect.
This rule encourages cooperation when others are cooperating and adapts to changes in opponent behavior.
3. **Punishment Mechanism**
If a player defects while CR > k/n, they will be "punished" by our strategy switching to defection for one round. This mechanism discourages exploitation.
4. **Exploration Phase (every 5 rounds): Randomized Cooperation**
Randomly cooperate or defect with a probability of 0.2 to gather information about opponent strategies and adapt to potential changes.

**Edge Cases:**

1. **Last Round (t = r): Cooperate**
In the final round, cooperate unconditionally to maximize collective payoff.
2. **Single Opponent (n = 2): Alternating Cooperation**
Alternate between cooperation and defection every other round to maintain a balanced payoff structure.

**Collective Mindset:**

Our strategy prioritizes cooperation while being mindful of potential exploitation. By adapting to the game dynamics and opponent behaviors, we aim to create an environment where cooperation is beneficial for all players.

Pseudocode:
```
// Initialize variables
cooperation_ratio = 0
punishment_active = false

// Loop through each round
for t in range(1, r+1):
    if t < 3:  // Initial rounds
        action = COOPERATE
    elif punishment_active:  // Punishment mechanism
        action = DEFECT
        punishment_active = False
    else:
        cooperation_ratio = calculate_cooperation_ratio()
        if cooperation_ratio > k/n:
            action = COOPERATE
        else:
            action = DEFECT

    // Exploration phase
    if t % 5 == 0 and random.random() < 0.2:
        action = random.choice([COOPERATE, DEFECT])

    // Punishment mechanism (check opponents' actions)
    for opponent in opponents:
        if opponent.defected and cooperation_ratio > k/n:
            punishment_active = True

    // Update history
    update_history(action)

// Functions
calculate_cooperation_ratio():
    return sum(opponents.cooperated) / n
```
This strategy is designed to be adaptive, robust, and collective, aiming to maximize the total payoff while being prepared for a wide range of opponent behaviors.
'''

description_COLLECTIVE_299 = '''
**Collective Strategy: Adaptive Tit-for-Tat with Public Goods Foresight**

Our collective strategy, named "PublicGoodsATFT," is designed to balance individual self-interest with collective cooperation, adapting to the evolving game dynamics.

**Decision Rules:**

1. **Initial Cooperation**: In the first round (t=1), cooperate (C) unconditionally.
2. **Adaptive Tit-for-Tat**: For rounds t > 1:
	* If the opponent(s) cooperated in the previous round, cooperate (C).
	* If any opponent defected in the previous round, defect (D) with a probability p_defect = 1 - (k/n), where k is the multiplication factor and n is the number of players.
3. **Public Goods Foresight**: When considering cooperation, estimate the potential public good payoff for the next round based on the current round's cooperation level. If this estimated payoff exceeds the private payoff from defecting, cooperate (C).

Pseudocode:
```python
def PublicGoodsATFT(n, k, history):
    if t == 1:  # First round
        return C
    
    prev_round_cooperation = sum(history[t-1])
    opponent_defected = any(opponent_action == D for opponent_action in history[t-1])
    
    if not opponent_defected:
        return C
    
    p_defect = 1 - (k/n)
    defect_prob = random.random()
    
    if defect_prob < p_defect:
        return D
    else:
        # Public Goods Foresight
        estimated_public_good_payoff = (k/n) * prev_round_cooperation + 1
        private_payoff_from_defecting = 1
        
        if estimated_public_good_payoff > private_payoff_from_defecting:
            return C
        else:
            return D
```
**Handling Edge Cases:**

* **Last Round**: In the last round, defect (D) unconditionally, as there is no future public good payoff to consider.
* **Tiebreakers**: When multiple opponents cooperate and defect in the same round, our strategy will randomly select an action with equal probability.

**Collective Mindset Alignment:**

Our strategy aims to balance individual self-interest with collective cooperation by:

1. Encouraging initial cooperation to seed public goods provision.
2. Adaptively responding to opponent behavior, promoting reciprocity and cooperation.
3. Considering the potential future benefits of public goods provision when deciding whether to cooperate.

By incorporating these elements, PublicGoodsATFT promotes a collective mindset that balances individual interests with the pursuit of common goals.
'''

description_COLLECTIVE_300 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual payoff maximization with collective welfare by adapting to the game's history and parameters. This approach ensures robustness against various opponent behaviors while promoting cooperation when beneficial.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if `n` is even or if `k` is close to `n`. Otherwise, defect (D).
2. **History-Dependent Cooperation**: For rounds 2 to `r`, calculate the average payoff per cooperator (`APC`) in the previous round:
   - `APC = (Σ(j=1 to n) c_j * k / n) / Σ(j=1 to n) c_j`
3. **Cooperative Threshold**: Set a dynamic cooperation threshold (`CT`) based on `k` and `n`. Cooperate if the expected payoff from cooperation exceeds the threshold:
   - `CT = (k / n) * (n / 2)`
4. **Adaptive Cooperation**: If `APC > CT`, cooperate (C). Otherwise, defect (D).
5. **Punishment Mechanism**: If more than half of the players defected in the previous round (`Σ(j=1 to n) c_j < n/2`), defect (D) in the current round.

**Handling Edge Cases:**

* Last Round (`t = r`): Cooperate if `APC > CT`, regardless of the punishment mechanism.
* Ties in `APC`: Cooperate if `n` is even; otherwise, defect.

**Pseudocode:**
```python
def ACT(n, k, t, history):
    if t == 1:
        # Initial cooperation
        return C if n % 2 == 0 or abs(k - n) < 1e-6 else D
    
    APC = calculate_average_payoff_per_cooperator(history)
    CT = (k / n) * (n / 2)
    
    if APC > CT:
        # Adaptive cooperation
        return C
    elif sum([c for c in history[-1]]) < n/2:
        # Punishment mechanism
        return D
    
    # Default to defect
    return D

def calculate_average_payoff_per_cooperator(history):
    cooperators = [c for c in history[-1] if c == 1]
    APC = (sum(cooperators) * k / n) / len(cooperators)
    return APC
```
**Collective Mindset Alignment:**

The ACT strategy prioritizes cooperation when the expected payoff from collective action exceeds a threshold based on the game parameters. By adapting to the game's history, it promotes cooperation while minimizing individual losses due to exploitation. The punishment mechanism discourages widespread defection and encourages players to contribute to the public good.
'''

description_COLLECTIVE_301 = '''
**Collective Strategy: Adaptive Cooperative Tit-for-Tat (ACT)**

The ACT strategy is a collective approach that balances individual self-interest with cooperation, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round.
2. **Subsequent Rounds**: Observe the previous round's actions and calculate the **cooperation rate**, `cr`, as the number of cooperators divided by the total number of players (`n`).
3. **Cooperate** if:
	* The cooperation rate in the previous round was above a threshold, `θ = 0.5`. This encourages cooperation when it's prevalent.
	* The game is not in its final rounds (i.e., `t < r - 2`). This helps maintain cooperation throughout the game.
4. **Defect** if:
	* The cooperation rate in the previous round was below or equal to `θ`.
	* The game is in its final two rounds (`t >= r - 2`). In this case, defecting may provide an individual advantage.

Pseudocode for ACT's decision rules:
```python
def act_strategy(t, cr):
    if t == 1:  # Initial round
        return 'C'
    elif cr > 0.5 and t < r - 2:
        return 'C'  # Cooperate when cooperation is prevalent and not in final rounds
    else:
        return 'D'  # Defect otherwise
```
**Handling Edge Cases:**

* **First Round**: As mentioned, cooperate (C) in the first round.
* **Last Two Rounds**: Defect (D) in the last two rounds to maximize individual payoff.
* **Ties**: In case of ties in cooperation rates or other calculations, use a random tiebreaker.

**Collective Mindset:**

ACT is designed to be collective by:

1. Observing and responding to the game's history, rather than relying on individual assumptions about opponents' behaviors.
2. Encouraging cooperation when it's prevalent, which helps maintain a high level of public good production.
3. Adapting to changes in cooperation rates over time, ensuring the strategy remains effective even if opponent behaviors shift.

By using ACT, we aim to create a robust and adaptive collective strategy that can perform well against various independent strategies in the tournament.
'''

description_COLLECTIVE_302 = '''
**Collective Strategy: Adaptive Cooperative Equilibrium (ACE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 50% to encourage initial cooperation and gather information about other players' strategies.
2. **Reciprocal Altruism**: For rounds t > 1, if the average payoff of cooperators in the previous round is higher than that of defectors, cooperate (play C). Otherwise, defect (play D).
3. **Punishment Mechanism**: If a player defects while others cooperate, punish them by defecting (playing D) with a probability proportional to the number of cooperators in the previous round.
4. **Forgiveness**: Gradually decrease the punishment probability over time if the punished player starts cooperating again.

**Edge Cases:**

1. **Last Round**: In the last round, play C if the overall average payoff of cooperators is higher than that of defectors; otherwise, play D.
2. **Tiebreakers**: In case of a tie in average payoffs between cooperators and defectors, play C to encourage cooperation.

**Collective Mindset:**

ACE aims to create a mutually beneficial environment by promoting cooperation while deterring exploitation. By reciprocating altruism and punishing free-riding behavior, ACE encourages players to cooperate, leading to higher overall payoffs.

**Pseudocode:**
```markdown
# Initialize variables
cooperate_prob = 0.5  # initial cooperation probability
avg_coop_payoff = 0   # average payoff of cooperators
avg_defect_payoff = 0  # average payoff of defectors
punishment_prob = 0    # punishment probability

# Main loop (round t)
for t in range(1, r+1):
    if t == 1:
        # Initial cooperation
        action = random.choice([C, D], p=[cooperate_prob, 1-cooperate_prob])
    else:
        # Reciprocal altruism
        if avg_coop_payoff > avg_defect_payoff:
            action = C
        else:
            action = D

        # Punishment mechanism
        if (action == D) and (avg_coop_payoff > avg_defect_payoff):
            punishment_prob = 1 - (1 / (1 + num_cooperators))
            action = random.choice([C, D], p=[punishment_prob, 1-punishment_prob])

    # Update variables
    if action == C:
        avg_coop_payoff += payoff(t)
    else:
        avg_defect_payoff += payoff(t)

    # Forgiveness
    if punishment_prob > 0 and player_t-1 == C:
        punishment_prob *= 0.9  # decrease punishment probability

# Last round
if t == r:
    if avg_coop_payoff > avg_defect_payoff:
        action = C
    else:
        action = D
```
ACE is designed to be robust against various opponent strategies and adaptable to changing game conditions, promoting a collective equilibrium that benefits all players.
'''

description_COLLECTIVE_303 = '''
**Collective Strategy: "Adaptive Cooperation with Gradual Forgetting" (ACGF)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to encourage others to do the same and create a cooperative atmosphere.
2. **Reciprocity-based Adaptation**: After the first round, use the following rules:
	* If the majority of players cooperated in the previous round (> n/2), cooperate (C) in the current round.
	* If the minority or equal number of players cooperated in the previous round (≤ n/2), defect (D) in the current round.
3. **Gradual Forgetting**: Introduce a "forgetting" mechanism to adapt to changing opponent behaviors:
	* Assign a weight (w) to each player's past cooperation, where w decreases with time (e.g., w = 1 / t, where t is the round number).
	* Calculate the weighted average of past cooperation rates for all players.
	* Use this weighted average as an input to adjust the decision rule:
		+ If the weighted average cooperation rate exceeds a certain threshold (e.g., 0.6), cooperate (C) in the current round, even if the majority did not cooperate previously.

**Handling Edge Cases:**

1. **Last Round**: In the final round, defect (D) to maximize personal payoff.
2. **Ties**: If there is a tie in the number of cooperators and defectors, use a random decision (e.g., 50% chance of cooperating).
3. **Opponent's Unpredictable Behavior**: If an opponent's behavior seems unpredictable or erratic, ACGF will still adapt by gradually adjusting its cooperation rate based on the weighted average of past behaviors.

**Collective Mindset Alignment:**

1. **Promote Cooperation**: By initially cooperating and reciprocating cooperation, ACGF encourages other players to cooperate.
2. **Adapt to Changing Behaviors**: Gradual forgetting allows ACGF to adapt to changes in opponent strategies, ensuring a balanced approach between cooperation and self-interest.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
r = number of rounds
k = multiplication factor
t = current round number

# First round: cooperate
if t == 1:
    action[t] = C

# Subsequent rounds: adapt based on previous cooperation rates
else:
    # Calculate weighted average cooperation rate
    w_avg_coop_rate = sum(w * coop_rate[i]) / sum(w)
    where coop_rate[i] is the cooperation rate of player i in round t-1
    
    # Adjust decision rule based on weighted average cooperation rate
    if w_avg_coop_rate > threshold (e.g., 0.6):
        action[t] = C
    else:
        # Majority-based reciprocity
        if sum(coop_rate) / n > 0.5:
            action[t] = C
        else:
            action[t] = D

# Last round: defect
if t == r:
    action[t] = D
```
This strategy aims to strike a balance between cooperation and self-interest, while adapting to changing opponent behaviors through gradual forgetting.
'''

description_COLLECTIVE_304 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The AC strategy is designed to balance individual self-interest with collective well-being, adapting to various opponent behaviors while maintaining robustness.

**Decision Rules:**

1. **First Round:** Cooperate (C). This sets a positive tone and encourages cooperation from other players.
2. **Subsequent Rounds:** Use the following rules:
	* If the total payoff for cooperating in the previous round was higher than or equal to the average individual payoff, continue to Cooperate (C).
	* Otherwise, Defect (D) with probability p = 1 - (k/n), and Cooperate (C) with probability 1 - p. This adjusts the strategy based on the effectiveness of cooperation.
3. **Observing Others:** Keep track of the number of cooperators in each round. If a significant proportion (> 2/3) of players cooperate, increase the likelihood of cooperating by adjusting the probability p to favor C.

**Pseudocode:**
```python
def adaptive_cooperation(n, k, r):
    # Initialize variables
    total_payoffs = []
    num_cooperators = []

    for round in range(r):
        if round == 0:
            action = 'C'  # Cooperate in the first round
        else:
            prev_total_payoff = sum(total_payoffs[-1])
            avg_individual_payoff = (prev_total_payoff / n)

            if prev_total_payoff >= avg_individual_payoff:
                action = 'C'
            else:
                p = 1 - (k / n)
                action = 'D' if random.random() < p else 'C'

        # Observe others and adjust strategy
        num_cooperators_round = sum(1 for a in actions_history[-1] if a == 'C')
        proportion_cooperating = num_cooperators_round / n

        if proportion_cooperating > 2/3:
            p_adjusted = max(p, 0.5)  # Increase likelihood of cooperating
            action = 'D' if random.random() < (1 - p_adjusted) else 'C'

        total_payoffs.append(calculate_total_payoff(actions_history[-1], k))
        num_cooperators.append(num_cooperators_round)

    return actions_history
```
**Edge Cases:**

* **Last Round:** The strategy remains the same, as there is no reason to deviate from the adaptive behavior.
* **Single-Defector Scenario:** If only one player defects in a round, and all others cooperate, AC will still cooperate in the next round, trying to maintain the collective benefit.

**Collective Mindset:**

AC prioritizes cooperation while being responsive to the actions of other players. By adapting to the environment and maintaining a degree of flexibility, this strategy balances individual interests with the collective well-being, aiming for mutual benefits across multiple rounds.
'''

description_COLLECTIVE_305 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name:** Adaptive Collective Contribute (ACC)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to initiate cooperation and encourage others to contribute.
2. **Contribution Threshold**: Introduce a dynamic contribution threshold, `theta`, which represents the minimum proportion of cooperators required for a player to cooperate. Initialize `theta` to 0.5 (50%).
3. **Cooperation Decision**:
	* If the number of cooperators in the previous round is greater than or equal to `theta` * n, cooperate (C) in the current round.
	* Otherwise, defect (D).
4. **Adaptive Threshold Adjustment**: Update `theta` based on the game's history:
	+ If cooperation succeeds (i.e., total payoff increases), decrease `theta` by 0.05 to encourage more cooperation.
	+ If cooperation fails (i.e., total payoff decreases), increase `theta` by 0.05 to make cooperation more selective.

**Pseudocode:**
```python
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
theta = 0.5  # initial contribution threshold

# First round
if t == 1:
    action = C  # cooperate in the first round

# Subsequent rounds
else:
    prev_cooperators = count cooperators in previous round
    if prev_cooperators >= theta * n:
        action = C  # cooperate if threshold is met
    else:
        action = D  # defect otherwise

    # Update theta based on game history
    if total_payoff increased:
        theta -= 0.05
    elif total_payoff decreased:
        theta += 0.05

# Last round (special case)
if t == r:
    action = C  # cooperate in the last round to maximize total payoff
```
**Edge Cases:**

* **Last Round**: Cooperate unconditionally to maximize the total payoff, as there's no future opportunity for reciprocity.
* **Tie-Breaking**: In cases where `theta` * n is not an integer, use a random tie-breaker (e.g., coin flip) to decide whether to cooperate or defect.

**Collective Mindset:**

The ACC strategy aligns with the collective mindset by:

1. Encouraging cooperation through adaptive threshold adjustment.
2. Responding to changes in opponent behavior and adapting to the game's history.
3. Fostering a cooperative environment by cooperating unconditionally in the first and last rounds.

This strategy is robust, as it adapts to various opponent behaviors and adjusts its contribution threshold accordingly. By being collective-minded, ACC promotes cooperation and maximizes total payoff over multiple rounds.
'''

description_COLLECTIVE_306 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a baseline of cooperation and encourage others to follow.
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust accordingly:
	* If the total payoff from the previous round is greater than or equal to the average payoff when all players cooperate (`k`), continue to Cooperate (C).
	* If the total payoff from the previous round is less than the average payoff when all players cooperate, Defect (D) with a probability proportional to the difference between the observed and expected payoffs. This introduces a punishment mechanism for non-cooperative behavior.
3. **Punishment Mechanism:** When defecting, use the following formula to determine the probability of defection (`p_defect`):
	* `p_defect = (k - Σ(t=1 to n) π_i,t-1) / k`
	* where `Σ(t=1 to n) π_i,t-1` is the total payoff from the previous round.
4. **Edge Cases:**
	* Last Round (`t=r`): Cooperate (C) unconditionally, as there's no future punishment or reward to consider.

**Collective Mindset Alignment:**

ACP prioritizes cooperation and mutual benefit while introducing a punishment mechanism to deter non-cooperative behavior. By responding to the previous round's outcome, ACP adapts to the collective behavior of all players, promoting a stable and cooperative equilibrium.

**Pseudocode (simplified):**
```python
def adaptive_cooperation(n, k, t, history):
  if t == 1:  # First Round
    return 'C'
  
  prev_payoff = sum(history[-1])  # Previous round's total payoff
  
  if prev_payoff >= k:
    return 'C'  # Continue cooperating
  
  p_defect = (k - prev_payoff) / k
  if random.random() < p_defect:
    return 'D'  # Defect with probability p_defect
  else:
    return 'C'
  
def play_acp(n, k, r):
  history = []
  for t in range(1, r+1):
    action = adaptive_cooperation(n, k, t, history)
    payoffs = [calculate_payoff(action, n, k) for _ in range(n)]
    history.append(payoffs)
    if t == r:
      return 'C'  # Last round: Cooperate unconditionally
```
This strategy is designed to be robust against various opponent behaviors and promote cooperation while adapting to the collective outcome.
'''

description_COLLECTIVE_307 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy is designed to balance individual self-interest with collective well-being, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage reciprocity.
2. **Subsequent Rounds**: Use a dynamic threshold to decide between Cooperate (C) and Defect (D). Calculate the threshold as follows:

`threshold = (k/n) * (1 - (total_defections / total_rounds))`

where:
- `total_defections` is the number of times any player defected in previous rounds
- `total_rounds` is the current round number

Cooperate (C) if the total contributions from the previous round are above the threshold. Otherwise, Defect (D).

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round to maximize collective payoff, as there's no opportunity for reciprocity or retaliation.
2. **Ties**: In case of a tie in total contributions, Cooperate (C) if the majority of players cooperated in the previous round; otherwise, Defect (D).
3. **Zero Contributions**: If all players defected in the previous round, Cooperate (C) to restart cooperation.

**Collective Mindset:**

The ACT strategy prioritizes collective well-being by:

1. Encouraging cooperation through reciprocity and conditional cooperation.
2. Adapting to opponent behaviors and adjusting the threshold accordingly.
3. Fostering a culture of cooperation by cooperating in the first round and maintaining cooperation when others do.

**Pseudocode:**
```python
def ACT_strategy(n, k, r, history):
    # Initialize variables
    total_defections = 0
    total_rounds = 1

    for round in range(1, r + 1):
        if round == 1:
            # Initial Round: Cooperate
            action = 'C'
        else:
            # Calculate threshold
            threshold = (k / n) * (1 - (total_defections / total_rounds))

            # Decide based on threshold
            if history[round - 1]['total_contributions'] > threshold:
                action = 'C'
            else:
                action = 'D'

        # Update variables
        total_defections += sum([1 for player in history[round - 1] if player['action'] == 'D'])
        total_rounds += 1

    return action
```
This strategy is designed to be adaptive, robust, and collective, making it a strong contender in the tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_308 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors:

**Strategy Name: Adaptive Collective Optimism (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.8. This initial optimism encourages cooperation and allows ACO to gather information about opponents' behavior.
2. **Payoff-Based Adaptation**: After the first round, calculate the average payoff of all players in the previous round (`avg_payoff_prev`). If `avg_payoff_prev` is greater than or equal to the expected payoff from universal defection (1), cooperate with probability 0.8. Otherwise, defect (D) with probability 0.6.
3. **Contribution-Based Adaptation**: Calculate the average number of cooperators in the previous round (`avg_contributors_prev`). If `avg_contributors_prev` is greater than or equal to half the number of players (`n/2`), cooperate with probability 0.8. Otherwise, defect with probability 0.6.
4. **History-Based Adaptation**: Maintain a history of opponents' actions for the past 3 rounds. Calculate the proportion of cooperative actions in this window (`coop_history`). If `coop_history` is greater than or equal to 0.5, cooperate with probability 0.8. Otherwise, defect with probability 0.6.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate if the total payoff from cooperation exceeds the expected payoff from universal defection.
2. **Ties**: If multiple conditions suggest different actions, prioritize cooperation.

**Pseudocode:**
```python
def ACO(n, k, r, history):
    # Initialize variables
    avg_payoff_prev = 0
    avg_contributors_prev = 0
    coop_history = []

    for t in range(r):
        if t == 0:
            # Initial cooperation
            action = 'C' if random.random() < 0.8 else 'D'
        else:
            # Payoff-based adaptation
            avg_payoff_prev = calculate_avg_payoff(history, t-1)
            if avg_payoff_prev >= 1:
                action = 'C' if random.random() < 0.8 else 'D'
            else:
                action = 'D' if random.random() < 0.6 else 'C'

            # Contribution-based adaptation
            avg_contributors_prev = calculate_avg_contributors(history, t-1)
            if avg_contributors_prev >= n/2:
                action = 'C' if random.random() < 0.8 else 'D'
            else:
                action = 'D' if random.random() < 0.6 else 'C'

            # History-based adaptation
            coop_history.append(calculate_coop_history(history, t-1))
            if sum(coop_history) / len(coop_history) >= 0.5:
                action = 'C' if random.random() < 0.8 else 'D'
            else:
                action = 'D' if random.random() < 0.6 else 'C'

        # Last round
        if t == r-1:
            if calculate_total_payoff(history, 'C') > calculate_total_payoff(history, 'D'):
                action = 'C'

        return action
```
**Collective Mindset:**

ACO is designed to be a collective strategy that adapts to various opponent behaviors while promoting cooperation. By considering the average payoff, contribution levels, and history of opponents' actions, ACO aims to create an environment where cooperation is rewarded and encouraged. This approach allows ACO to perform well in a wide range of scenarios, from highly cooperative to highly competitive environments.

ACO's adaptive nature enables it to respond effectively to different opponent strategies, making it a robust competitor in the tournament.
'''

description_COLLECTIVE_309 = '''
**Collective Strategy: Adaptive Cooperative Momentum (ACM)**

The ACM strategy is designed to balance individual self-interest with collective well-being by adapting to the game's history and opponents' behaviors.

**Decision Rules:**

1. **Initial Rounds (t = 1, 2): Cooperate (C)**
   - In the first two rounds, play C to establish a cooperative momentum and encourage others to reciprocate.
   - This starting point is chosen to foster an environment where cooperation can thrive.

2. **Observation Phase (t > 2): Evaluate Opponents' Behaviors**
   - For each round t > 2, calculate the following metrics:
     - `total_cooperators`: The total number of players who cooperated in the previous round.
     - `previous_payoff`: Your own payoff from the previous round.

3. **Cooperation Threshold (CT) Calculation:**
   - Calculate CT as follows:
     - CT = (k / n) \* (`total_cooperators` / n)
   
4. **Defection Detection and Response:**
   - If `previous_payoff` is less than or equal to the minimum possible payoff (1), it indicates that opponents are defecting.
   - In this case, **play D in the next round**.

5. **Cooperation Reinforcement:**
   - Otherwise, if CT is greater than or equal to 0.5 and `previous_payoff` was higher than the minimum possible payoff:
     - Continue playing C.
   
6. **Adaptive Shift:**
   - If neither of the above conditions is met (i.e., CT < 0.5 and no signs of defection), switch to D for one round.

**Handling Edge Cases:**

* First Round (t = 1): Cooperate by default.
* Last Round (t = r): Play according to the decision rules, but prioritize individual payoff maximization if necessary.

**Collective Mindset Alignment:**
ACM encourages cooperation while being cautious of exploitation. It aims to create an environment where collective well-being is prioritized without relying on explicit coordination or shared norms.

**Pseudocode Example (for one round):**

```
function ACM(t, total_cooperators_prev, previous_payoff):
  if t <= 2:
    return C
  elif previous_payoff <= 1: # Defection detected
    return D
  else:
    CT = (k / n) * (`total_cooperators_prev` / n)
    if CT >= 0.5 and `previous_payoff` > 1: 
      return C
    else:
      return D
```
Note that the actual implementation will need to consider all game parameters, handle edge cases explicitly, and be optimized for performance in a tournament setting.

This strategy should provide a robust foundation for navigating various opponent behaviors while promoting collective cooperation.
'''

description_COLLECTIVE_310 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The AC strategy is designed to balance individual self-interest with collective benefits, while adapting to various opponent behaviors.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to signal willingness to cooperate and encourage others to do so.
2. **Subsequent Rounds:** Use a hybrid approach based on the following conditions:
	* If the total number of cooperators in the previous round is greater than or equal to n/2, Cooperate (C).
	* Otherwise, Defect (D) with a probability p = k/n, and Cooperate (C) with probability 1 - p.

This approach ensures that if most players cooperate, we continue to do so. If not, we adapt by defecting with a probability proportional to the multiplication factor k, which represents the potential gain from cooperation.

**Edge Cases:**

* **Last Round:** Defect (D) in the last round, as there is no future benefit from cooperating.
* **Ties:** In case of ties (e.g., equal number of cooperators and defectors), Cooperate (C).
* **Opponent Behavior:** If an opponent consistently defects, adapt by defecting with a higher probability p = max(k/n, 0.5) to avoid exploitation.

**Collective Mindset:**

The AC strategy prioritizes cooperation when it is likely to be reciprocated and adapts to exploitation. By cooperating in the first round and responding to the collective behavior of others, we encourage cooperation while minimizing individual losses.

Pseudocode:
```markdown
# Initialize variables
n = number_of_players
k = multiplication_factor
r = number_of_rounds

# First Round
if current_round == 1:
    action = COOPERATE

# Subsequent Rounds
else:
    prev_cooperators = count_cooperators_in_prev_round()
    if prev_cooperators >= n/2:
        action = COOPERATE
    else:
        p = k/n
        if random() < p:
            action = DEFECT
        else:
            action = COOPERATE

# Edge Cases
if current_round == r:  # Last Round
    action = DEFECT
elif prev_cooperators == n/2:  # Ties
    action = COOPERATE
elif opponent_defects_consistently():  # Opponent Behavior
    p = max(k/n, 0.5)
    if random() < p:
        action = DEFECT

return action
```
This strategy is designed to be robust and adaptive, allowing it to perform well in a wide range of scenarios against independent opponent strategies.
'''

description_COLLECTIVE_311 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation strategy is designed to balance individual payoff maximization with collective well-being, adapting to various opponent behaviors and game conditions.

**Decision Rules**

1. **Initial Rounds**: In the first round, cooperate (C) unconditionally to signal a willingness to contribute to the public good.
2. **Subsequent Rounds**: Observe the previous round's actions and payoffs of all players.
3. **Cooperation Threshold**: Calculate the cooperation threshold (θ) as the ratio of the number of cooperators in the previous round to the total number of players (n).
4. **Decision**:
	* If θ ≥ k/n, cooperate (C). This means that if enough players contributed in the previous round, it's likely beneficial to continue cooperating.
	* Otherwise, defect (D).

Pseudocode for decision-making process:

```
function AdaptiveCooperation(n, k, history):
    if current_round == 1:
        return COOPERATE
    else:
        θ = count_cooperators(history) / n
        if θ >= k/n:
            return COOPERATE
        else:
            return DEFECT
```

**Handling Edge Cases**

* **Last Round**: In the final round, defect (D). Since there's no future interaction, individual payoff maximization takes precedence.
* **Tiebreaker**: If θ = k/n exactly, cooperate (C) to maintain a cooperative stance when there's no clear advantage in defection.

**Collective Mindset Alignment**

The Adaptive Cooperation strategy prioritizes collective well-being by:

1. Cooperating initially to encourage others to contribute.
2. Adjusting the cooperation threshold based on the previous round's actions, allowing for adaptability to changing opponent behaviors.
3. Focusing on the ratio of cooperators rather than individual payoffs, promoting a collective approach.

By balancing individual interests with collective considerations, Adaptive Cooperation aims to achieve a mutually beneficial outcome in the N-Player Public Goods Game.
'''

description_COLLECTIVE_312 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Retribution**

This strategy balances individual self-interest with collective well-being by adapting to the game's history and gradually punishing defectors while encouraging cooperation.

**Decision Rules:**

1. **First Round:** Cooperate (C). This sets a cooperative tone and allows us to gauge others' behavior.
2. **Subsequent Rounds:**
	* If the average payoff in the previous round is above the threshold `k/n` (i.e., the public good has been successful), cooperate (C).
	* If the average payoff in the previous round is below or equal to `1 - k/n` (i.e., defectors have dominated), defect (D) with probability `p = 0.5`. Otherwise, cooperate (C).
3. **Retribution Mechanism:** Gradually increase the likelihood of defecting when faced with repeated defection by others.
	* Keep track of the number of consecutive rounds where the average payoff is below or equal to `1 - k/n` (i.e., defectors have dominated). Let this count be `defect_count`.
	* If `defect_count > 0`, defect with probability `p = min(1, defect_count / (r - t))`, where `t` is the current round number.
4. **Forgiveness:** Reset `defect_count` when the average payoff in a round exceeds `k/n`.

**Pseudocode:**
```python
def adaptive_cooperation(n, k, r):
    # Initialize variables
    defect_count = 0
    prev_avg_payoff = None

    for t in range(1, r + 1):
        if t == 1:
            action = 'C'  # Cooperate in the first round
        else:
            avg_payoff = (prev_avg_payoff * (n - 1) + payoff_last_round) / n
            if avg_payoff > k / n:
                action = 'C'
            elif avg_payoff <= 1 - k / n:
                p_defect = min(1, defect_count / (r - t))
                if random.random() < p_defect:
                    action = 'D'  # Defect with probability
                else:
                    action = 'C'
            else:
                action = 'C'

        # Update variables
        prev_avg_payoff = avg_payoff_last_round
        if avg_payoff <= 1 - k / n:
            defect_count += 1
        elif avg_payoff > k / n:
            defect_count = 0

    return action
```
**Collective Mindset:** This strategy prioritizes cooperation when the public good has been successful, while gradually punishing defectors to prevent exploitation. By adapting to the game's history and being mindful of others' behavior, we promote a collective mindset that balances individual self-interest with group well-being.

This strategy is robust to various opponent behaviors and does not rely on norms or coordination mechanisms. Its gradual retribution mechanism discourages repeated defection, while its forgiveness component allows for cooperation to resume when the public good has been revitalized.
'''

description_COLLECTIVE_313 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors:

**Strategy Name:** Adaptive Collective Optimizer (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This initial cooperation is intended to establish a positive baseline for collective behavior.
2. **Payoff-Based Adaptation**: For each subsequent round t > 1:
	* If your total payoff in the previous round π_i,t-1 is greater than or equal to the average historical payoff, cooperate (C).
	* Otherwise, defect (D) with probability p_d = 0.5 + (k/n) \* (average_total_contributions_in_last_round / n). The higher the number of cooperators and the multiplication factor k, the more likely ACO is to cooperate.
3. **Reputation-Driven Adjustment**: If a player i has consistently defected (D) in the last m rounds (where m = 0.25 \* r), adjust their probability of cooperation by increasing it by δ_c = 0.1 for each consecutive round they continue to defect. This adjustment is meant to encourage players with poor reputations to cooperate more.
4. **Endgame Adaptation**: For the last round, ACO switches to a greedy strategy: if the current total payoff π_i,t-1 + (k/n) \* Σ(j=1 to n) c_j,t is expected to be higher when cooperating, then cooperate; otherwise, defect.

**Pseudocode**
```python
def AdaptiveCollectiveOptimizer(game_parameters, history):
    # Initialize cooperation probability and payoffs
    p_c = 0.5
    total_payoff = 0

    for t in range(1, game_parameters.r + 1):
        if t == 1:  # Initial Cooperation
            action = C if random.random() < p_c else D
        else:
            # Payoff-Based Adaptation
            last_round_payoff = history[t-1]
            avg_historical_payoff = sum(history) / (t - 1)
            if last_round_payoff >= avg_historical_payoff:
                action = C
            else:
                p_d = 0.5 + (game_parameters.k / game_parameters.n) * \
                      (sum(c for c in history[t-1].values()) / game_parameters.n)
                action = D if random.random() < p_d else C

        # Reputation-Driven Adjustment
        reputation_defects = sum(1 for i, a in enumerate(history[-m:]) if a == D and i != current_player_index)
        if reputation_defects >= m:
            p_c += δ_c  # Increase cooperation probability for players with poor reputations

        # Endgame Adaptation (last round only)
        if t == game_parameters.r:
            expected_total_payoff = last_round_payoff + \
                                    (game_parameters.k / game_parameters.n) * sum(c for c in history[t].values())
            action = C if expected_total_payoff > total_payoff else D

        return action
```
**Collective Mindset:**

ACO aims to balance individual self-interest with collective well-being. By adapting cooperation probabilities based on payoffs, reputation, and endgame considerations, ACO promotes cooperation while safeguarding against exploitation by defectors.

This strategy is designed to be robust to a wide range of opponent behaviors, as it responds to both cooperative and defective actions in the history. The adaptive nature of ACO allows it to adjust its behavior over time, promoting collective optimization without relying on explicit coordination mechanisms or shared norms.
'''

description_COLLECTIVE_314 = '''
**Collective Strategy: Adaptive Cooperative Tracker (ACT)**

The ACT strategy is designed to promote collective well-being while adapting to diverse opponent behaviors. It relies solely on game parameters and history.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate in the first round to establish a cooperative tone.
2. **Reciprocal Cooperation**: If the majority of players (at least n/2) cooperated in the previous round, cooperate in the current round.
3. **Punish Defection**: If the number of cooperators in the previous round is below the threshold (n/k), defect in the current round to signal dissatisfaction with the group's cooperation level.
4. **Exploit Greed**: If a player has defected for more than k consecutive rounds, cooperate in the current round to exploit their greed and encourage re-cooperation.

**Edge Cases:**

* **First Round**: Cooperate (Rule 1).
* **Last Round**: Defect if the total number of cooperators is below the threshold (n/k) in the previous round; otherwise, cooperate.
* **Ties**: If the number of cooperators equals n/2 or n/k, flip a coin to decide between cooperation and defection.

**Collective Mindset:**

The ACT strategy prioritizes collective well-being by:

* Encouraging initial cooperation to set a positive tone
* Reciprocating cooperation when the majority cooperates
* Punishing low cooperation levels to maintain social norms
* Exploiting greedy behavior to promote re-cooperation

Pseudocode:
```markdown
def ACT(n, k, history):
    if len(history) == 0:  # First round
        return COOPERATE
    elif sum(history[-1]) >= n/2:  # Reciprocal cooperation
        return COOPERATE
    elif sum(history[-1]) < n/k:  # Punish defection
        return DEFECT
    else:
        for i in range(len(history)):
            if history[i] == DEFECT and len([j for j in range(i, len(history)) if history[j] == DEFECT]) >= k:
                return COOPERATE  # Exploit greed
        return random.choice([COOPERATE, DEFECT])  # Tie-breaker
```
The ACT strategy is designed to be adaptive and robust against a wide range of opponent behaviors. It promotes collective well-being while minimizing the risk of exploitation by opponents.
'''

description_COLLECTIVE_315 = '''
Here's a collective strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5 (a neutral starting point).
2. **Reciprocal Cooperativeness**: For rounds t > 1, estimate the overall cooperativeness of the group in the previous round (t-1). Calculate the proportion of players who cooperated, denoted as p_t-1.

Pseudocode:
```
p_t-1 = (number of C actions in round t-1) / n
```
3. **Adaptive Cooperation**: In round t, cooperate if the estimated cooperativeness from the previous round is above a certain threshold (τ). Otherwise, defect.

Pseudocode:
```
if p_t-1 > τ then
  action_t = C
else
  action_t = D
end if
```
4. **Exploration**: To avoid converging to a suboptimal equilibrium and encourage cooperation, introduce occasional exploration by cooperating with probability ε (a small value, e.g., ε = 0.1) even when the threshold is not met.

Pseudocode:
```
if random() < ε then
  action_t = C
end if
```
5. **Boundary Conditions**: In the last round, defect (D), as there's no future game to influence.
6. **Edge Case Handling**:
	* If all players defected in the previous round (p_t-1 = 0), cooperate with probability 0.5 to encourage restarting cooperation.

Pseudocode:
```
if p_t-1 == 0 then
  action_t = C with probability 0.5
end if
```

**Collective Mindset Alignment:**
The ACO strategy focuses on the collective well-being by:

* Cooperating when the group shows a high level of cooperativeness, promoting public goods.
* Defecting when cooperation is low to avoid exploitation and encourage others to cooperate.
* Exploring occasional cooperation to help the group escape suboptimal equilibria.

**Robustness:**
ACO adapts to various opponent behaviors by:

* Responding to changes in overall cooperativeness.
* Exploring cooperation to promote mutually beneficial outcomes.
* Being prepared to defect when necessary to protect individual interests.

This strategy will be implemented as an algorithm and compete against other independent strategies developed by AI systems.
'''

description_COLLECTIVE_316 = '''
I'll introduce a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and focuses on achieving high overall payoffs while maintaining robustness.

**Strategy Name: Adaptive Collective Optimizer (ACO)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round, as this sets a positive tone for potential cooperation.
2. **General Rule:** Observe the total number of cooperators (TC) and defectors (TD) from the previous round.
3. **Cooperation Threshold:** Set a dynamic threshold, `theta`, based on the game's parameters:
	* Calculate the expected payoff when everyone cooperates: `k * n / n = k`
	* Set `theta` to be slightly above the midpoint of this value and 1 (the private payoff from keeping): `theta = (k + 1) / 2`
4. **Cooperate or Defect:** Based on the observed TC and TD from the previous round:
	* If `(TC - TD) > theta`, Cooperate (C). This suggests a strong collective momentum toward cooperation.
	* Otherwise, Defect (D).

**Edge Cases:**

1. **Last Round:** In the final round, if `(TC - TD) > 0` in the previous round, Cooperate (C) to maintain the cooperative atmosphere and maximize overall payoffs.
2. **Single Opponent Deviates:** If a single opponent defects while all others cooperate, Defect (D) in the next round to deter free-riding.

**Collective Mindset:**

The ACO strategy prioritizes cooperation when it's likely to lead to higher collective payoffs. By adjusting its threshold based on game parameters and observing opponents' actions, ACO encourages cooperation without being overly exploitable. When opponents deviate or fail to cooperate, ACO adapts by defecting to minimize losses.

**Pseudocode:**
```python
def adaptive_collective_optimizer(n, k, r):
    # Initialize variables
    theta = (k + 1) / 2
    previous_cooperators = 0

    for t in range(r):
        if t == 0:
            action = 'C'  # Cooperate in the first round
        else:
            cooperators, defectors = count_previous_actions()
            diff = cooperators - defectors
            if diff > theta:
                action = 'C'
            else:
                action = 'D'

        # Play the chosen action and update history

        # Last-round special case
        if t == r - 1 and previous_cooperators > 0:
            action = 'C'

    return total_payoff
```
ACO aims to balance cooperation with prudence, adapting to various opponent strategies while prioritizing collective success.
'''

description_COLLECTIVE_317 = '''
**Collective Strategy: Adaptive Cooperation with Self-Regulation (ACSR)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Consecutive Rounds:** For each subsequent round, calculate the average cooperation rate of all players in the previous round (excluding the current player). If the average cooperation rate is above a certain threshold (`θ`), cooperate (C); otherwise, defect (D).
3. **Self-Regulation Mechanism:** Introduce a personal cooperation counter (`P`) to track individual cooperation levels over time. If `P` exceeds a predefined limit (`P_max`), temporarily switch to defection (D) for one round to avoid excessive self-sacrifice.
4. **Response to Defection:** Monitor the opponent's actions and adjust strategy based on their behavior. If an opponent defects frequently, decrease cooperation threshold (`θ`) to reduce vulnerability to exploitation.

**Edge Cases:**

1. **Last Round:** In the final round, cooperate (C) if the overall game progress indicates a high level of mutual cooperation; otherwise, defect (D).
2. **Single Opponent:** When facing only one opponent, adopt a more cautious approach by increasing the cooperation threshold (`θ`) to reduce vulnerability.

**Collective Mindset:**

ACSR is designed to promote collective cooperation while adapting to diverse opponent behaviors. By introducing self-regulation and responding to opponents' actions, this strategy balances individual interests with group benefits.

**Pseudocode (simplified):**
```python
def ACSR(n, k, r):
    θ = 0.5  # initial cooperation threshold
    P_max = 3  # maximum personal cooperation counter value
    P = 0  # initialize personal cooperation counter

    for t in range(r):
        if t == 0:
            action = 'C'  # cooperate in the first round
        else:
            avg_coop_rate = calculate_average_cooperation(n, t-1)
            if avg_coop_rate > θ and P < P_max:
                action = 'C'
            elif P >= P_max:
                action = 'D'  # self-regulation mechanism
                P -= 1  # reset personal cooperation counter
            else:
                action = 'D'

        respond_to_opponent_actions(n, t-1)  # adjust θ based on opponents' behavior

    return total_payoff
```
This strategy will adapt to various opponent behaviors while promoting collective cooperation. The self-regulation mechanism ensures that individual players do not excessively sacrifice their own interests for the group's benefit.
'''

description_COLLECTIVE_318 = '''
To design a collective strategy for this N-Player Public Goods Game, we'll focus on creating an adaptive and robust approach that depends solely on game parameters and history. Our goal is to maximize overall payoff while encouraging cooperation.

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a high probability (e.g., 0.8). This sets a positive tone and encourages others to cooperate.
2. **Reciprocal Cooperation**: For subsequent rounds, use the following logic:
	* If the average cooperation rate in the previous round is above a threshold (e.g., 0.5), cooperate (play C).
	* Otherwise, defect (play D) with a probability proportional to the deviation from the threshold.
3. **Adaptive Threshold**: Adjust the cooperation threshold based on the game's history:
	* If the average payoff in the previous rounds is increasing, decrease the threshold by a small amount (e.g., 0.05).
	* If the average payoff is decreasing, increase the threshold by a small amount (e.g., 0.05).
4. **Exploration**: Introduce random exploration to ensure adaptability:
	* With a low probability (e.g., 0.1), choose the opposite action of what the decision rule suggests.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate (play C) if the average cooperation rate in the previous rounds is above the threshold.
2. **Tiebreaker**: If the cooperation rate is exactly equal to the threshold, use a random tiebreaker (e.g., 0.5 probability of cooperating).

**Collective Mindset:**

1. **Focus on Collective Payoff**: Prioritize maximizing the total payoff for all players over individual gains.
2. **Encourage Cooperation**: Cooperate when others do, and adapt to changes in cooperation rates.
3. **Robustness**: Be prepared for various opponent behaviors by introducing exploration and adapting thresholds.

**Pseudocode:**
```python
def ACO(n, k, r):
    # Initialize variables
    threshold = 0.5
    avg_cooperation_rate = 0
    avg_payoff = 0

    for round in range(r):
        if round == 0:
            # Initial cooperation
            action = 'C' with probability 0.8
        else:
            # Reciprocal cooperation and adaptive threshold
            avg_cooperation_rate = calculate_avg_cooperation_rate(round - 1)
            if avg_cooperation_rate > threshold:
                action = 'C'
            else:
                action = 'D' with probability (threshold - avg_cooperation_rate) / (1 - threshold)

        # Exploration
        action = opposite_action(action) with probability 0.1

        # Update variables
        avg_payoff += calculate_avg_payoff(round)
        if avg_payoff > previous_avg_payoff:
            threshold -= 0.05
        else:
            threshold += 0.05

    return action
```
This strategy aims to balance individual and collective interests by adapting to the game's history and promoting cooperation when beneficial. By incorporating exploration and a dynamic threshold, ACO can respond effectively to various opponent behaviors in the tournament.
'''

description_COLLECTIVE_319 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Retaliation**

Our collective strategy aims to balance individual self-interest with cooperative behavior, adapting to the game's history and opponent actions.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and gather information about opponents' behaviors.
2. **Cooperation Threshold**: Define a cooperation threshold (τ) as the ratio of total cooperators to total players (n) in the previous round. Initialize τ = 0.5.
3. **Retaliation Mechanism**: If the number of cooperators in the previous round is below τ, gradually increase the probability of defecting (D) in the next round. This is achieved by calculating a retaliation factor (ρ) based on the deviation from the cooperation threshold:
	* ρ = (τ - (Σ(j=1 to n) c_j / n)) / τ
	* If ρ > 0, set the probability of defecting (p_D) to ρ; otherwise, set p_D = 0.
4. **Adaptive Cooperation**: Cooperate with probability (1 - p_D). This ensures that if opponents are not cooperating enough, our strategy gradually increases the likelihood of defecting.

**Pseudocode:**
```python
def adaptive_cooperation(n, k, r):
    τ = 0.5  # initial cooperation threshold
    ρ = 0  # retaliation factor

    for t in range(1, r+1):
        if t == 1:
            action = C  # cooperate in the first round
        else:
            previous_round_cooperators = sum(c_j for c_j in actions[t-1])
            τ = (τ + (previous_round_cooperators / n)) / 2  # update cooperation threshold

            ρ = (τ - (previous_round_cooperators / n)) / τ
            if ρ > 0:
                p_D = ρ
            else:
                p_D = 0

            action = C with probability (1 - p_D)
            action = D otherwise

        # update actions for the next round
        actions[t] = [action]
```
**Edge Cases:**

* **Last Round**: Cooperate in the last round to avoid any potential retaliation and maximize total payoff.
* **Tiebreaker**: In case of a tie (ρ = 0), cooperate to maintain a cooperative atmosphere.

**Collective Mindset:**

Our strategy prioritizes cooperation while gradually adapting to opponents' behaviors. By retaliating against low cooperation levels, we encourage others to contribute to the public good. This approach balances individual self-interest with collective well-being, aiming for a mutually beneficial outcome in the tournament.

This strategy is robust and adaptive, responding to various opponent behaviors without relying on norms or coordination mechanisms. Its performance will be evaluated in the tournament against other AI systems' strategies.
'''

description_COLLECTIVE_320 = '''
**Collective Strategy: Adaptive Cooperation with Social Learning**

Our collective strategy, "ACS" (Adaptive Cooperation with Social Learning), is designed to balance individual self-interest with the pursuit of collective welfare in the N-Player Public Goods Game.

**Decision Rules:**

1. **Initial Round:** In the first round, play Cooperate (C) if the multiplication factor k is greater than or equal to 2, and Defect (D) otherwise.
2. **Subsequent Rounds:** Observe the previous round's actions and payoffs of all players. Calculate the average payoff of cooperators and defectors separately.
3. **Cooperation Threshold:** Determine a cooperation threshold (T) based on the game parameters:
	* If k is high (> 2.5), set T = 0.6 (more cooperative).
	* If k is medium (1 < k ≤ 2.5), set T = 0.4 (balanced).
	* If k is low (k ≤ 1), set T = 0.2 (less cooperative).
4. **Adaptive Cooperation:** Play Cooperate (C) if the average payoff of cooperators in the previous round is greater than or equal to the cooperation threshold (T). Otherwise, play Defect (D).

**Edge Cases:**

1. **Last Round:** In the final round, play Defect (D), as there are no future rounds to benefit from cooperation.
2. **Low Cooperation Rate:** If fewer than 20% of players cooperated in the previous round, play Defect (D) for two consecutive rounds to signal dissatisfaction with low cooperation levels.
3. **High Cooperation Rate:** If more than 80% of players cooperated in the previous round, play Cooperate (C) for two consecutive rounds to reinforce high cooperation levels.

**Social Learning:**

1. **Imitation:** Observe the actions and payoffs of all players and identify the most successful player(s) in terms of average payoff.
2. **Mimicry:** Adopt the strategy of the most successful player(s) if their average payoff is higher than yours.

**Pseudocode:**
```
Function ACS(Player i, Round t):
  // Initialize variables
  avg_coop_payoff = 0
  avg_defect_payoff = 0
  coop_threshold = 0

  // First round
  if t == 1:
    if k >= 2:
      return C
    else:
      return D

  // Calculate cooperation threshold
  if k > 2.5:
    coop_threshold = 0.6
  elif k <= 2.5 and k > 1:
    coop_threshold = 0.4
  else:
    coop_threshold = 0.2

  // Observe previous round's actions and payoffs
  for player j in players:
    if j's action == C:
      avg_coop_payoff += j's payoff / num_cooperators
    else:
      avg_defect_payoff += j's payoff / num_defectors

  // Adaptive cooperation
  if avg_coop_payoff >= coop_threshold:
    return C
  else:
    return D

  // Edge cases and social learning
  ...
```
By combining adaptive cooperation with social learning, our strategy aims to balance individual self-interest with collective welfare while responding to a wide range of opponent behaviors.
'''

description_COLLECTIVE_321 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Escalation (ACE)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage cooperation and establish a baseline for future interactions.
2. **Subsequent Rounds:** Observe the number of cooperators (c) in the previous round. If c ≥ (n/2), cooperate (C). Otherwise, defect (D).
3. **Escalation Mechanism:** Introduce a gradual escalation mechanism to respond to repeated defections. Maintain a counter (defect_count) that increments each time a player defects. When defect_count reaches a threshold (escalation_threshold = n/4), switch to a more aggressive strategy: defect (D) for the next (escalation_period = 2) rounds, regardless of the number of cooperators.
4. **Re-evaluation:** After escalation_period rounds, reassess the situation and return to the original decision rule (step 2).

**Pseudocode:**
```python
def ACE(n, k):
    # Initialize variables
    c_prev = n  # Assume all cooperate in the first round
    defect_count = 0
    escalation_threshold = n / 4
    escalation_period = 2

    for t in range(1, r + 1):  # Round loop
        if t == 1:  # Initial round
            action = C
        else:
            c_prev = count_cooperators(t - 1)
            if c_prev >= (n / 2):
                action = C
            elif defect_count < escalation_threshold:
                action = D
                defect_count += 1
            elif defect_count >= escalation_threshold:
                # Escalation mechanism
                for _ in range(escalation_period):
                    action = D
                    defect_count -= 1

        # Update c_prev and defect_count based on current round's actions
        if action == C:
            c_prev += 1
        else:
            defect_count += 1

    return action
```
**Edge Cases:**

* **Last Round:** Since the game has a fixed number of rounds, ACE will not adapt its strategy in response to the last round. The decision rules remain the same.
* **Ties:** In case of ties (e.g., equal numbers of cooperators and defectors), ACE will cooperate if c ≥ (n/2).

**Collective Mindset:**
ACE is designed to encourage cooperation while being robust against various opponent behaviors. By cooperating initially, ACE sets a positive tone for the game. The escalation mechanism responds to repeated defections by introducing a more aggressive strategy, which helps to deter exploitation. By re-evaluating the situation after escalation_period rounds, ACE adapts to changing circumstances and returns to its original cooperative approach when conditions improve.

ACE's adaptive nature allows it to respond effectively to different opponent strategies while promoting collective cooperation. Its robustness stems from its ability to handle various edge cases and adjust its behavior in response to changes in the game environment.
'''

description_COLLECTIVE_322 = '''
**Collective Strategy: Adaptive Cooperativity (AC)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds (t>1):**
	* If the majority of players (> n/2) cooperated in the previous round, Cooperate (C).
	* If the number of cooperators is exactly half (n/2), Defect (D) with probability 0.5 and Cooperate (C) with probability 0.5.
	* If less than half of the players cooperated (< n/2), Defect (D).

**Pseudocode:**
```python
def AC(n, k, t, history):
    if t == 1:
        return 'C'  # Cooperate in initial round

    prev_round_coops = sum(1 for action in history[t-1] if action == 'C')
    coop_threshold = n / 2

    if prev_round_coops > coop_threshold:
        return 'C'  # Cooperate if majority cooperated
    elif prev_round_coops == coop_threshold:
        import random
        return 'D' if random.random() < 0.5 else 'C'
    else:
        return 'D'  # Defect if less than half cooperated
```
**Edge Cases:**

* **Last Round (t=r):** Cooperate (C) to maximize the total payoff, as there is no future round to consider.
* **Ties:** In case of a tie in the number of cooperators, randomize between Cooperate and Defect with equal probability.

**Collective Mindset:**

The Adaptive Cooperativity strategy aligns with the collective mindset by:

1. Encouraging cooperation in the initial rounds to establish a cooperative tone.
2. Adapting to the behavior of others, punishing defection, and rewarding cooperation.
3. Allowing for flexibility and randomness in case of ties or uncertainty.

This strategy is robust against various opponent behaviors, including unconditional cooperators, defectors, and mixed strategies. By adapting to the history of play, AC promotes a cooperative atmosphere while protecting against exploitation.
'''

description_COLLECTIVE_323 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 50%. This encourages cooperation while also exploring the opponent's behavior.
2. **Cooperation Threshold**: After the first round, calculate the cooperation threshold `θ` based on the game parameters:
   θ = (k / n) \* (n - 1)
   If the average number of cooperators in the previous round is above `θ`, cooperate (play C). Otherwise, defect (play D).
3. **Escalation Mechanism**: If the opponent defects while you cooperated in the previous round, escalate cooperation by playing C with a probability of 75% in the next round.
4. **De-escalation Mechanism**: If both players cooperated in the previous round and your payoff was below the average payoff of all players, de-escalate cooperation by playing D with a probability of 25% in the next round.

**Handling Edge Cases:**

* **Last Round**: In the last round, cooperate (play C) if the opponent cooperated in the previous round. Otherwise, defect (play D).
* **Consecutive Defections**: If the opponent defects consecutively for more than 2 rounds, cooperate (play C) with a probability of 25% to encourage cooperation.
* **Consecutive Cooperations**: If both players cooperate consecutively for more than 3 rounds, maintain cooperation.

**Collective Mindset:**

The ACE strategy prioritizes cooperation while being adaptive to the opponent's behavior. By escalating cooperation when the opponent defects and de-escalating when payoffs are low, ACE promotes a collective mindset that balances individual self-interest with group welfare.

**Pseudocode (simplified):**
```python
def ace_strategy(game_params, history):
  n = game_params['n']
  k = game_params['k']
  r = game_params['r']
  θ = (k / n) * (n - 1)
  
  if len(history) == 0:  # First round
    return random.choice([C, D], p=[0.5, 0.5])
  
  avg_cooperators = sum(1 for action in history[-1] if action == C)
  opponent_defected = history[-1][opponent_index] == D
  
  if avg_cooperators > θ:
    return C
  elif opponent_defected and history[-2][my_index] == C:
    # Escalate cooperation
    return random.choice([C, D], p=[0.75, 0.25])
  elif avg_cooperators < θ and history[-1][my_index] == C:
    # De-escalate cooperation
    return random.choice([C, D], p=[0.25, 0.75])
  
  # Last round or other edge cases
  if len(history) >= r - 1:  # Last round
    if history[-1][opponent_index] == C:
      return C
    else:
      return D
  
  # Consecutive defections or cooperations
  consecutive_defections = sum(1 for action in history[-3:] if action == D)
  consecutive_cooperations = sum(1 for action in history[-4:] if action == C)
  
  if consecutive_defections > 2:
    return random.choice([C, D], p=[0.25, 0.75])
  elif consecutive_cooperations > 3:
    return C
  
  # Default behavior
  return random.choice([C, D], p=[0.5, 0.5])
```
This pseudocode serves as a simplified representation of the ACE strategy and will be refined during implementation.
'''

description_COLLECTIVE_324 = '''
**Collective Strategy: Adaptive Reciprocal Altruism (ARA)**

ARA is a collective strategy that balances individual self-interest with group cooperation, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Rounds**: Cooperate in the first two rounds to establish a cooperative baseline and gather information about opponents' behaviors.
2. **Reciprocal Cooperation**: If the majority of players (≥ n/2) cooperated in the previous round, cooperate in the current round.
3. **Altruistic Punishment**: If an opponent defected while most others cooperated (> n/2), defect against that opponent in the next round to punish non-cooperative behavior.
4. **Self-Protection**: If a player has been consistently exploited (defected against) by others, switch to defection for two consecutive rounds to protect individual interests.

**Edge Cases:**

1. **Last Round**: Cooperate if the majority cooperated in the second-to-last round; otherwise, defect.
2. **Tiebreaker**: In case of a tie in the number of cooperators and defectors, cooperate if k/n > 0.5 (i.e., the multiplication factor is relatively high); otherwise, defect.

**Collective Alignment:**

ARA prioritizes collective well-being by:

1. Encouraging cooperation when most players are cooperative.
2. Punishing non-cooperative behavior to maintain a fair environment.
3. Protecting individual interests when exploited by others.

By adapting to the game's history and opponent behaviors, ARA promotes a balance between self-interest and group cooperation, making it a robust strategy in a wide range of scenarios.

**Pseudocode:**
```
function ARA(player_id, round, previous_round_actions):
  if round <= 2:
    return COOPERATE
  elif majority_cooperated(previous_round_actions):
    return COOPERATE
  elif altruistic_punishment_trigger(previous_round_actions):
    return DEFECT
  elif self_protection_trigger(previous_round_actions):
    return DEFECT
  else:
    # Last round tiebreaker or default behavior
    if last_round_cooperation(previous_round_actions) and k/n > 0.5:
      return COOPERATE
    else:
      return DEFECT

def majority_cooperated(actions):
  cooperators = sum(1 for action in actions if action == COOPERATE)
  return cooperators >= len(actions) / 2

def altruistic_punishment_trigger(actions):
  # Check if an opponent defected while most others cooperated
  pass

def self_protection_trigger(actions):
  # Check if a player has been consistently exploited
  pass

def last_round_cooperation(actions):
  # Check cooperation in the second-to-last round
  pass
```
Note: This pseudocode is a simplified representation of the strategy and can be refined and expanded upon during implementation.
'''

description_COLLECTIVE_325 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Contribution-based Cooperation**: In subsequent rounds, cooperate if the average contribution rate of all players in the previous round is above a certain threshold, `θ`. This promotes collective cooperation when it's likely to be reciprocated.

`θ = (n + k) / (2n)`

This threshold value balances individual and collective interests. When the average contribution rate exceeds this threshold, it indicates that enough players are cooperating to make contributing worthwhile.

3. **Punishment Mechanism**: If a player defects in a round where at least one other player cooperated, they will be punished in the next round by receiving a lower payoff than if everyone had defected. This is achieved by adjusting the cooperation threshold for that player:

`θ_p = θ - (k/n)`

This temporary reduction in `θ` makes it more difficult for a punished player to cooperate in the next round, effectively punishing them for their defection.

**Edge Case Handling:**

* **Last Round**: Cooperate if the average contribution rate is above `θ`. If not, defect. This ensures that even in the final round, players have an incentive to contribute.
* **Tiebreaker**: In cases where the average contribution rate equals `θ`, cooperate with a probability of 0.5.

**Collective Mindset Alignment:**

The Adaptive Cooperation strategy promotes collective cooperation by:

1. Encouraging initial cooperation
2. Responding positively to high contribution rates
3. Punishing defections when others are contributing

By adapting to the group's behavior and using a punishment mechanism, AC fosters an environment where players are more likely to cooperate.

**Pseudocode:**

```
function AdaptiveCooperation(n, k, history):
  if round == 1:
    return C
  else:
    average_contribution_rate = calculate_average_contribution_rate(history)
    θ = (n + k) / (2n)
    
    if average_contribution_rate > θ:
      return C
    elif average_contribution_rate < θ and punished_player():
      θ_p = θ - (k/n)
      if random() < 0.5:
        return C
      else:
        return D
    else:
      return D

function calculate_average_contribution_rate(history):
  total_contributions = sum(contributions in history)
  average_contribution_rate = total_contributions / n
  return average_contribution_rate

function punished_player():
  if previous_round_defection and other_players_cooperated:
    return True
  else:
    return False
```

This collective strategy, Adaptive Cooperation, balances individual interests with the need for collective cooperation. By adapting to the group's behavior and using a punishment mechanism, AC encourages players to cooperate while also ensuring that those who defect are held accountable.
'''

description_COLLECTIVE_326 = '''
**Collective Strategy: Adaptive Cooperation with Conditional Defection**

Our collective strategy aims to balance individual self-interest with the collective benefit of cooperation. We'll use a dynamic approach that adapts to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to encourage initial cooperation and set a positive tone.
2. **Consecutive Cooperation:** If the total number of cooperators in the previous round is greater than or equal to the multiplication factor (k), cooperate (C) in the current round. This reinforces successful collective efforts.
3. **Conditional Defection:** Otherwise, if the total number of cooperators in the previous round is less than k, defect (D) in the current round. However, if the opponent's cooperation rate in the previous round was above a certain threshold (e.g., 0.5), cooperate (C) to incentivize continued cooperation.
4. **Punishment Mechanism:** If an opponent defected in the previous round and the total number of cooperators was below k, defect (D) in the current round to penalize non-cooperative behavior.

**Edge Cases:**

1. **Last Round (t=r):** Cooperate (C) in the final round, as there's no opportunity for retaliation or future cooperation.
2. **Ties:** In cases where the total number of cooperators equals k exactly, cooperate (C) to maintain a cooperative atmosphere.

**Collective Mindset:**

Our strategy prioritizes collective well-being by:

1. Encouraging initial cooperation and setting a positive tone.
2. Rewarding successful collective efforts through continued cooperation.
3. Gradually introducing defection when the collective benefit is not met, while still incentivizing cooperation from opponents.
4. Punishing non-cooperative behavior to maintain social norms.

Pseudocode:
```markdown
function adaptive_cooperation(n, k, r, history):
  if t == 1: # Initial Round
    return C

  prev_total_c = sum(history[-1]) # Total cooperators in previous round
  opponent_coop_rate = sum([x[1] for x in history[-1]]) / n # Opponent cooperation rate

  if prev_total_c >= k:
    return C # Consecutive Cooperation
  elif prev_total_c < k and opponent_coop_rate > 0.5:
    return C # Conditional Defection with Incentive
  else:
    return D # Conditional Defection or Punishment Mechanism

  if t == r: # Last Round
    return C
```
This strategy is designed to be robust, adaptive, and collective-minded, making it suitable for a wide range of opponent behaviors in the N-Player Public Goods Game.
'''

description_COLLECTIVE_327 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**: Observe the number of cooperators (num_coop) and defectors (num_defect) in the previous round. Calculate the cooperation rate (coop_rate = num_coop / n).
3. **Cooperation Threshold**: If coop_rate ≥ k/n, Cooperate (C). Otherwise, Defect (D).

Pseudocode:
```
if current_round == 1:
    action = C
else:
    num_coop, num_defect = observe_previous_round_actions()
    coop_rate = num_coop / n
    if coop_rate >= k/n:
        action = C
    else:
        action = D
```

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round to maximize collective payoff, regardless of previous actions.
2. **Tiebreaker**: If coop_rate == k/n exactly, Cooperate (C) to break the tie.

**Collective Mindset Alignment:**

The Adaptive Cooperation strategy prioritizes cooperation when it is likely to be reciprocated by others. By cooperating in the first round and adapting to the game's history, AC encourages other players to cooperate as well. This approach balances individual self-interest with collective well-being, promoting a stable and cooperative environment.

**Robustness:**

AC is robust against various opponent behaviors:

1. **Pure Cooperators**: AC will cooperate with them, leading to high collective payoffs.
2. **Pure Defectors**: AC will defect against them, minimizing losses.
3. **Mixed Strategies**: AC's adaptiveness allows it to adjust to changing cooperation rates, responding effectively to a range of mixed strategies.

By using this strategy, we aim to create a cooperative atmosphere while protecting individual interests, increasing the chances of success in the tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_328 = '''
Here's a collective strategy that adapts to various opponent behaviors and maximizes overall payoff:

**Strategy Name:** Adaptive Cooperative Threshold (ACT)

**Decision Rules:**

1. **Initial Rounds (t = 1, 2):** Cooperate (C) in the first two rounds to establish a baseline for cooperation.
2. **Observation Phase (t > 2):** Observe the total number of cooperators (c_total) and defectors (d_total) from previous rounds.
3. **Cooperation Threshold:** Calculate a dynamic cooperation threshold (θ) based on the game parameters (n, k) and history:
   θ = (k/n) \* (1 - (d_total / (t-1)))
4. **Decision Logic:**
   * If c_total ≥ θ in the previous round, Cooperate (C).
   * Otherwise, Defect (D).

**Pseudocode:**

```python
def ACT(n, k, t, history):
    # Initialize variables
    c_total = 0
    d_total = 0

    # Observation phase (t > 2)
    if t > 2:
        for i in range(1, t):
            c_total += sum([history[i-1][j] == 'C' for j in range(n)])
            d_total += n - c_total

    # Calculate cooperation threshold
    theta = (k/n) * (1 - (d_total / (t-1)))

    # Decision logic
    if t <= 2 or sum([history[t-1][j] == 'C' for j in range(n)]) >= theta:
        return 'C'
    else:
        return 'D'
```

**Edge Cases:**

* **First Round:** Cooperate (C) as per the decision rules.
* **Last Round:** Since ACT focuses on long-term cooperation, it may not maximize payoff in the last round. However, if opponents are also using ACT or similar strategies, cooperation is still likely to be beneficial.

**Collective Mindset:**

The Adaptive Cooperative Threshold strategy aligns with a collective mindset by:

1. **Encouraging cooperation:** By cooperating in initial rounds and maintaining a cooperation threshold, ACT promotes cooperation among players.
2. **Adapting to opponents:** The dynamic cooperation threshold allows ACT to adjust its behavior based on the actions of other players, fostering a more cooperative environment.
3. **Robustness:** ACT's decision logic is robust against various opponent behaviors, making it suitable for a wide range of tournament scenarios.

This strategy should perform well in the tournament by promoting cooperation and adapting to different opponent behaviors.
'''

description_COLLECTIVE_329 = '''
**Collective Strategy: "Adaptive Cooperation with Grudges"**

This strategy balances individual self-interest with collective well-being by adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) unconditionally to establish a cooperative baseline.
2. **Subsequent Rounds (t>1):**
	* If the average payoff of cooperators in the previous round is greater than or equal to the average payoff of defectors, cooperate (C).
	* Otherwise, defect (D).
3. **Grudge Mechanism:** Implement a "grudge" system to punish consistent defectors.
	* Track the number of consecutive rounds each opponent has defected (`defect_streak_i`).
	* If an opponent's `defect_streak_i` exceeds a threshold (`grudge_threshold`, e.g., 2-3 rounds), defect (D) against that opponent for the next round, regardless of the average payoffs.
4. **Last Round (t=r):** Defect (D) to maximize individual payoff, as cooperation will not affect future interactions.

**Pseudocode:**
```python
def adaptive_cooperation_with_grudges(n, k, r):
    # Initialize variables
    avg_payoff_C = 0  # Average payoff of cooperators
    avg_payoff_D = 0  # Average payoff of defectors
    defect_streak = [0] * n  # Defect streaks for each opponent

    for t in range(1, r+1):
        if t == 1:  # Initial round
            action = 'C'
        else:
            if avg_payoff_C >= avg_payoff_D:
                action = 'C'
            else:
                action = 'D'

            # Grudge mechanism
            for i in range(n):
                if defect_streak[i] > grudge_threshold:
                    action = 'D'  # Defect against opponent with high defect streak

        # Update average payoffs and defect streaks
        avg_payoff_C, avg_payoff_D = update_average_payoffs(avg_payoff_C, avg_payoff_D, actions)
        defect_streak = update_defect_streak(defect_streak, actions)

    return action
```
**Collective Mindset:**
This strategy prioritizes cooperation while adapting to the game's dynamics. By initially cooperating and responding to the average payoffs of cooperators and defectors, it encourages a cooperative environment. The grudge mechanism helps maintain a balance by punishing consistent defectors, promoting a more equitable distribution of payoffs.

**Robustness:**
This strategy is robust against various opponent behaviors:

* **Cooperative opponents:** Will likely reciprocate cooperation, leading to high average payoffs for both parties.
* **Defective opponents:** Will be punished through the grudge mechanism, discouraging them from consistently defecting.
* **Mixed-strategy opponents:** The adaptive nature of this strategy will adjust to the opponent's behavior, ensuring a fair distribution of payoffs.

By implementing this collective strategy, we aim to achieve a balance between individual self-interest and collective well-being in the N-Player Public Goods Game.
'''

description_COLLECTIVE_330 = '''
**Collective Strategy: "Adaptive Cooperation with Retaliation" (ACR)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds:** Observe the total number of cooperators in the previous round (t-1). If the cooperation rate is above a threshold (see below), cooperate in the current round (t). Otherwise, defect (D).
3. **Retaliation Mechanism:** If the opponent's cooperation rate falls below the threshold in any round, retaliate by playing D for the next two rounds to deter free-riding.
4. **Threshold Calculation:** Set the cooperation threshold as `θ = k/n`, where `k` is the multiplication factor and `n` is the number of players.

**Pseudocode:**
```python
def ACR_strategy(history, n, k):
  if len(history) == 0:  # First round
    return 'C'
  
  prev_round_coops = sum([1 for action in history[-1] if action == 'C'])
  coop_rate_prev_round = prev_round_coops / n
  
  if coop_rate_prev_round >= θ:
    return 'C'
  else:
    if len(history) > 1 and any(action == 'D' for action in history[-2]):
      # Retaliation mechanism: opponent defected last round, play D
      return 'D'
    else:
      return 'D'

θ = k / n
```
**Rationale:**

ACR strategy aims to balance individual interests with collective well-being. By cooperating initially and adapting to the group's behavior, ACR encourages cooperation while penalizing free-riding.

1. **Initial Cooperation:** Sets a positive tone for the game.
2. **Adaptive Cooperation:** Responds to changes in the group's cooperation rate, promoting cooperation when it is high and defecting when it falls below the threshold.
3. **Retaliation Mechanism:** Deters opponents from exploiting others by temporarily retaliating against defectors.

**Edge Cases:**

* In the last round, play D if the opponent has been uncooperative in previous rounds; otherwise, cooperate to maintain a positive tone for future interactions (not applicable in this game setting since it's a simultaneous-move game).
* If an opponent suddenly changes their behavior from cooperation to defection or vice versa, ACR will adapt by adjusting its cooperation rate accordingly.

**Collective Mindset:**

ACR prioritizes collective outcomes while allowing individual players to respond strategically. By adapting to the group's behavior and penalizing free-riding, ACR promotes a cooperative environment that benefits all players.

This strategy should perform well in a tournament setting against independent strategies developed by other AI systems, as it balances cooperation with self-interest and adapts to various opponent behaviors.
'''

description_COLLECTIVE_331 = '''
**Collective Strategy: Adaptive Cooperation with Retaliation (ACR)**

**Decision Rules:**

1. **Initial Rounds:** Cooperate (C) in the first two rounds to encourage cooperation and establish a baseline for future interactions.
2. **Retaliation Mechanism:** Defect (D) if the number of cooperators in the previous round is less than or equal to half of the total players (n/2). This mechanism aims to punish non-cooperative behavior and encourage cooperation.
3. **Adaptive Cooperation:** Cooperate (C) if the average payoff of cooperators in the previous round is higher than the average payoff of defectors. Otherwise, defect (D).
4. **Consecutive Defection:** If a player defects for two consecutive rounds, switch to cooperating (C) in the next round to test for potential cooperation.
5. **Final Round:** Cooperate (C) in the last round to maximize collective payoffs.

**Pseudocode:**
```python
def ACR(n, k, r, history):
  # Initialize variables
  previous_cooperators = []
  previous_defectors = []
  average_payoff_C = 0
  average_payoff_D = 0

  for t in range(r):
    if t < 2:
      action = 'C'  # Cooperate in initial rounds
    elif len(previous_cooperators) <= n / 2:
      action = 'D'  # Retaliate if cooperation is low
    else:
      average_payoff_C = sum([payoff for payoff, action in history[t-1] if action == 'C']) / len([action for action in history[t-1] if action == 'C'])
      average_payoff_D = sum([payoff for payoff, action in history[t-1] if action == 'D']) / len([action for action in history[t-1] if action == 'D'])
      if average_payoff_C > average_payoff_D:
        action = 'C'  # Cooperate if cooperation is beneficial
      else:
        action = 'D'

    # Consecutive Defection
    if t >= 2 and history[t-1][1] == 'D' and history[t-2][1] == 'D':
      action = 'C'

    # Final Round
    if t == r - 1:
      action = 'C'

    previous_cooperators.extend([i for i, (payoff, act) in enumerate(history[t]) if act == 'C'])
    previous_defectors.extend([i for i, (payoff, act) in enumerate(history[t]) if act == 'D'])

    return action
```
**Rationale:**

The ACR strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and punishing non-cooperative behavior. By cooperating initially, we encourage cooperation and establish a baseline for future interactions.

The retaliation mechanism ensures that if cooperation is low, players will defect to signal their dissatisfaction, potentially leading to increased cooperation in subsequent rounds.

The adaptive cooperation rule promotes cooperation when it benefits both individual and collective payoffs, while the consecutive defection rule tests for potential cooperation after two rounds of defection.

Finally, cooperating in the last round maximizes collective payoffs, as there is no risk of retaliation or punishment.
'''

description_COLLECTIVE_332 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name:** "Collective Optimism with Adaptive Forgiveness" (COAF)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally to encourage others to contribute to the public good.
2. **Reciprocity-based Cooperation**: For subsequent rounds (t > 1), cooperate if at least a certain proportion (θ) of players cooperated in the previous round. This is based on the idea that if enough players are willing to contribute, it's likely worth continuing to do so.

Pseudocode for decision-making:
```
IF t == 1 THEN
    play C
ELSE
    count_prev_cooperators = COUNT(cooperators in previous round)
    theta = (k - 1) / n  // adjust θ based on game parameters
    IF count_prev_cooperators >= theta * n THEN
        play C
    ELSE
        play D
```
3. **Forgiveness Mechanism**: To avoid getting stuck in a cycle of defection, introduce an "adaptive forgiveness" component. If the proportion of cooperators in the previous round is below θ but still above a certain threshold (φ), randomly cooperate with a small probability (ε). This allows for some exploration and potential re-cooperation.

Pseudocode:
```
IF count_prev_cooperators < theta * n AND count_prev_cooperators >= phi * n THEN
    WITH PROBABILITY ε DO
        play C
    ELSE
        play D
```

**Edge Cases:**

* In the last round, always defect (D) to maximize individual payoff.
* If there are only two players left in the game (n = 2), switch to a Tit-for-Tat strategy to ensure mutual cooperation.

**Collective Mindset:**
COAF prioritizes collective well-being by cooperating when enough others do so. By forgiving occasional defections, it allows for some flexibility and encourages re-cooperation. This approach balances individual self-interest with the need for collective action to achieve better overall outcomes.

**Robustness:**
COAF is designed to be robust against various opponent behaviors:

* **Pure Defectors**: COAF will adapt by defecting more frequently as well.
* **Pure Cooperators**: COAF will maintain cooperation, encouraging others to do so.
* **Mixed Strategies**: COAF's forgiveness mechanism allows it to respond to occasional cooperation and adapt to changing conditions.

By implementing COAF, we aim to create a robust and collective strategy that performs well in the N-Player Public Goods Game tournament against diverse opponent strategies.
'''

description_COLLECTIVE_333 = '''
I propose a collective strategy called "Adaptive Collective Contribution" (ACC) for the N-Player Public Goods Game. ACC is designed to balance individual self-interest with collective well-being, adapting to various opponent behaviors while maintaining a robust and cooperative approach.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally. This sets a positive tone and encourages others to cooperate.
2. **Reciprocity-Based Adaptation**: For subsequent rounds, use a reciprocity-based mechanism:
	* If the total number of cooperators in the previous round is above the threshold `n/2`, cooperate (C).
	* Otherwise, defect (D) with probability `p = 1 - (k/n) * (total_cooperators_prev_round / n)`. This probability reflects the benefit from cooperation relative to the group's overall cooperative behavior.
3. **Punishment Mechanism**: If a player observes that their individual payoff in the previous round is lower than their expected payoff under universal cooperation (`π_i < k`), they will defect (D) with increased probability `p' = p + δ`, where `δ` is a small positive value (e.g., 0.05). This punishment mechanism discourages free-riding and encourages others to cooperate.
4. **Learning from Others**: When observing the actions of other players, update the estimate of the group's cooperation level (`total_cooperators_prev_round`) using a simple moving average over the last `r/2` rounds.

**Edge Cases:**

1. **First Round**: Cooperate unconditionally (C).
2. **Last Round**: If this is the final round (`t = r`), defect (D) to maximize individual payoff, as there are no future opportunities for cooperation.
3. **Ties in Cooperation Level**: In case of a tie in the number of cooperators between two consecutive rounds, maintain the previous action (cooperate or defect).

**Collective Mindset:**

ACC is designed with a collective mindset, aiming to promote cooperation while adapting to various opponent behaviors. By initially cooperating and then reciprocating based on the group's overall cooperative behavior, ACC encourages others to cooperate and maintains a balance between individual self-interest and collective well-being.

Pseudocode:
```python
def Adaptive_Collective_Contribution(n, k, r):
    # Initialize variables
    total_cooperators_prev_round = 0
    prev_payoff = 0
    
    for t in range(1, r+1):
        if t == 1:  # First round
            action = 'C'
        else:
            # Reciprocity-based adaptation
            if total_cooperators_prev_round > n/2:
                action = 'C'
            else:
                p = 1 - (k/n) * (total_cooperators_prev_round / n)
                action = 'D' with probability p
            
            # Punishment mechanism
            if prev_payoff < k and t > 1:
                p_prime = p + δ
                action = 'D' with probability p_prime
        
        # Observe others' actions and update estimates
        total_cooperators_prev_round = (total_cooperators_prev_round * (r/2 - 1) + sum(others_actions)) / (r/2)
        
        # Update previous payoff for punishment mechanism
        prev_payoff = current_payoff
    
    return action
```
This strategy is designed to be robust, adaptive, and collective-oriented. By incorporating elements of reciprocity, punishment, and learning from others, ACC promotes cooperation while responding effectively to various opponent behaviors in the N-Player Public Goods Game.
'''

description_COLLECTIVE_334 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability p = k/n. This initial cooperation rate is based on the multiplication factor k and the number of players n.
2. **Reciprocity**: After the first round, observe the previous round's cooperation rate among all players, denoted as c_prev. If c_prev ≥ k/n, cooperate (play C) in the current round with probability p = 1 - (1 - c_prev)^2. This rule encourages reciprocity and rewards high cooperation rates.
3. **Punishment**: If c_prev < k/n, defect (play D) with probability p = (1 - c_prev)^2. This rule punishes low cooperation rates by reducing the likelihood of cooperation.
4. **Adaptive Adjustment**: After each round, update the cooperation rate based on the previous round's outcome. If the total payoff π_i,t is higher than the average payoff over the past rounds, increase the cooperation probability p by a small amount (e.g., 0.05). Conversely, if the total payoff π_i,t is lower than the average payoff, decrease the cooperation probability p.

**Edge Cases:**

1. **First Round**: Cooperate with probability p = k/n.
2. **Last Round**: Defect (play D) to maximize individual payoff, as there are no future rounds to reciprocate or punish.
3. **Ties**: In case of ties in cooperation rates, randomly choose between cooperating and defecting.

**Collective Mindset:**

The ACC strategy aims to promote collective cooperation by:

1. Encouraging initial cooperation based on the game parameters.
2. Reciprocating high cooperation rates from previous rounds.
3. Punishing low cooperation rates to maintain a stable level of cooperation.
4. Adapting to changing conditions and adjusting cooperation probabilities accordingly.

**Pseudocode:**
```python
def ACC(n, k, r):
    # Initialize cooperation probability p
    p = k / n

    for t in range(1, r + 1):
        if t == 1:
            # Initial cooperation
            cooperate = (random.random() < p)
        else:
            # Reciprocity and punishment
            c_prev = sum(cooperations_previous_round) / n
            if c_prev >= k / n:
                p = 1 - (1 - c_prev)**2
            else:
                p = (1 - c_prev)**2
            cooperate = (random.random() < p)

        # Play the game and observe outcome
        π_i,t = play_game(cooperate)

        # Adaptive adjustment
        if π_i,t > average_payoff_over_past_rounds:
            p += 0.05
        else:
            p -= 0.05

    return cooperate
```
This strategy will be implemented as an algorithm and compete against other independent strategies in the tournament.
'''

description_COLLECTIVE_335 = '''
**Collective Strategy: Adaptive Reciprocal Altruism (ARA)**

ARA is a collective strategy that balances individual self-interest with group cooperation, adapting to the game's history and parameters.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone.
2. **Reciprocity**: If the number of cooperators in the previous round is above the average contribution threshold (k/n), cooperate (C). Otherwise, defect (D).
3. **Self-Interest Threshold**: If an opponent's payoff in the previous round exceeds the self-interest threshold (1 + k/2), defect (D) to prevent exploitation.
4. **Punishment Mechanism**: If a player defects while others cooperate, and their payoff exceeds the fair share ((k/n) × total_contributions), retaliate by defecting (D) in the next round.

**Edge Cases**

* Last Round: Cooperate (C) if the number of cooperators in the previous round is above the average contribution threshold. Otherwise, defect (D).
* Ties or Equal Payoffs: In case of ties or equal payoffs among opponents, prioritize cooperation to maintain a positive group dynamic.
* Unusual Patterns: If an opponent exhibits unusual patterns (e.g., alternating between C and D), revert to self-interest and defect (D).

**Collective Mindset**

ARA prioritizes collective well-being while protecting individual interests. By reciprocating cooperation and punishing exploitation, ARA fosters a cooperative environment. When necessary, it adapts to protect itself from being taken advantage of.

**Pseudocode**
```
function adaptiveReciprocalAltruism(gameParams, history):
  // Initialize variables
  n = gameParams.numPlayers
  k = gameParams.multiplicationFactor
  r = gameParams.numRounds
  avgContributionThreshold = k / n

  // First round: Cooperate to establish a cooperative tone
  if history.isEmpty():
    return COOPERATE

  // Reciprocity rule
  prevRoundCooperators = countCooperators(history[-1])
  if prevRoundCooperators > avgContributionThreshold:
    return COOPERATE

  // Self-Interest Threshold
  opponentPayoff = getOpponentPayoff(history[-1], self)
  if opponentPayoff > (1 + k / 2):
    return DEFECT

  // Punishment Mechanism
  prevRoundDefectors = countDefectors(history[-1])
  if prevRoundDefectors > 0 and opponentPayoff > ((k / n) * total_contributions):
    return DEFECT

  // Default to cooperation
  return COOPERATE
```
By implementing ARA, we create a collective strategy that balances individual self-interest with group cooperation, adapting to the game's history and parameters. This approach promotes a positive group dynamic while protecting against exploitation.
'''

description_COLLECTIVE_336 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to encourage mutual cooperation and establish a baseline for future interactions.
2. **Subsequent Rounds:** Use a combination of history-based and adaptive mechanisms to decide whether to cooperate or defect:
	* Calculate the **Cooperation Rate** (CR) as the proportion of players who cooperated in the previous round.
	* If CR ≥ 0.5, Cooperate (C). This threshold encourages cooperation when there is sufficient mutual cooperation.
	* If CR < 0.5 and the game has reached a **Threshold Round** (TR), Defect (D). TR is calculated as follows:
		+ Initialize TR to r/2 (half of the total rounds).
		+ Update TR based on the Cooperation Rate: if CR > 0.7, decrease TR by 1; if CR < 0.3, increase TR by 1.
	* If CR < 0.5 and the game has not reached TR, use a **Tit-for-Tat with Forgiveness** (TFT-F) approach:
		+ Cooperate if the opponent cooperated in the previous round or if they have cooperated more often than defected in the last few rounds.
		+ Defect otherwise.

**Pseudocode:**
```
AC_Strategy(n, k, r):
  // Initialize variables
  CR = 0.5  // Cooperation Rate
  TR = r/2  // Threshold Round
  TFT_F_History = []  // History for Tit-for-Tat with Forgiveness

  For each round t from 1 to r:
    If t == 1:  // First Round
      action = C
    Else:
      CR = calculate_cooperation_rate(previous_round)
      If CR >= 0.5:
        action = C
      ElseIf CR < 0.5 and t >= TR:
        action = D
      Else:
        TFT_F_History.append(opponent_actions[previous_round])
        action = tit_for_tat_with_forgiveness(TFT_F_History)

    // Update TR based on Cooperation Rate
    If CR > 0.7: TR -= 1
    ElseIf CR < 0.3: TR += 1

    Return action
```
**Rationale and Robustness:**

The Adaptive Cooperation strategy is designed to be collective, adaptive, and robust to various opponent behaviors.

* By cooperating in the first round, we encourage mutual cooperation and establish a baseline for future interactions.
* The Cooperation Rate threshold (0.5) allows us to adapt to changing levels of cooperation in the game.
* The Tit-for-Tat with Forgiveness approach helps maintain cooperation when faced with occasional defections or noisy opponents.
* The Threshold Round mechanism ensures that we defect only when there is persistent low cooperation, preventing exploitation by selfish opponents.

By combining these elements, our strategy balances individual self-interest with collective well-being, making it a robust and effective choice for the N-Player Public Goods Game.
'''

description_COLLECTIVE_337 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `n/(n+1)` to encourage mutual cooperation and information gathering.
2. **Tit-for-Tat with Exploration**: For rounds 2 to r-1:
	* If the majority (> n/2) of players cooperated in the previous round, cooperate (C).
	* Otherwise, defect (D) but cooperate (C) with a probability `k/n` to maintain some level of cooperation and avoid full defection cascades.
3. **Adaptive Escalation**: In response to observed opponent behavior:
	* If at least one player defected in the previous round, increase the cooperation threshold by 1 (i.e., require more players to cooperate before cooperating).
	* If all players cooperated in the previous round, decrease the cooperation threshold by 1 (i.e., become more willing to cooperate).
4. **Final Round**: In the last round (r), defect (D) as there's no future benefit from cooperation.

**Edge Cases:**

1. **Ties**: When calculating the majority, break ties in favor of cooperation.
2. **Single Defector**: If only one player defected in a previous round, consider it an outlier and maintain the current cooperation threshold.

**Pseudocode:**
```markdown
function ACE(n, k, r, history):
  if first_round:
    cooperate with probability n/(n+1)
  else:
    majority_cooperated = count(history[-1]) > n/2
    if majority_cooperated:
      cooperate()
    else:
      defect but cooperate with probability k/n

    # Adaptive Escalation
    if any_defected(history[-1]):
      cooperation_threshold += 1
    elif all_cooperated(history[-1]):
      cooperation_threshold -= 1

    # Final Round
    if current_round == r:
      defect()
```
**Collective Mindset:**
ACE is designed to balance individual self-interest with collective well-being. By initially cooperating and adapting to the group's behavior, ACE encourages mutual cooperation while minimizing exploitation by defectors. The strategy is robust against various opponent behaviors, including full defection cascades, and converges towards a stable cooperative equilibrium.

**Key Features:**

* Initial cooperation encourages information gathering and sets the stage for future cooperation.
* Tit-for-Tat with exploration allows ACE to adapt to changing environments while maintaining some level of cooperation.
* Adaptive Escalation responds to observed opponent behavior, making ACE more resilient against exploitation.
* Final round defection maximizes individual payoff in the absence of future cooperation benefits.

ACE is designed to perform well in a tournament setting, where diverse strategies will be employed. Its adaptability and robustness make it an attractive choice for achieving collective success in this N-player public goods game.
'''

description_COLLECTIVE_338 = '''
I propose a collective strategy called "Adaptive Collective Optimism" (ACO) for the N-Player Public Goods Game. ACO aims to balance individual self-interest with collective well-being by adapting to the game's history and opponents' behaviors.

**Decision Rules:**

1. **Initial Rounds:** Cooperate in the first two rounds, regardless of the number of players or multiplication factor. This sets a positive tone for cooperation and allows us to gauge others' behaviors.
2. **Adaptive Cooperation (AC):**
	* Calculate the average cooperation rate (ACR) over the past `min(r/2, 5)` rounds, where `r` is the total number of rounds. ACR represents the proportion of cooperative actions in recent history.
	* If ACR ≥ `0.6`, cooperate in the current round; otherwise, defect.
3. **Multiplication Factor Adjustment (MFA):**
	* Monitor the average payoff over the past `min(r/2, 5)` rounds.
	* If the average payoff is less than `(k/n) * (n-1)`, adjust our strategy by increasing the ACR threshold for cooperation to `0.7`. This adjustment helps counter potential free-rider exploitation when k is relatively low.

**Edge Cases:**

* **Last Round:** Cooperate if ACR ≥ `0.5` over the entire game history, otherwise defect. This rule helps maintain a positive ending and avoids last-round defection.
* **Small Number of Rounds (r ≤ 5):** Use a modified AC rule with an increased ACR threshold of `0.7`. This ensures cooperation when there are fewer rounds to adjust our strategy.

**Collective Mindset:**

ACO prioritizes collective optimism by initially cooperating and adapting to the game's history. By monitoring ACR and adjusting our strategy, we promote cooperation while minimizing exploitation. Our approach aligns with a collective mindset that balances individual self-interest with group well-being.

Pseudocode for ACO:
```python
def adaptive_collective_optimism(game_state):
    # Initial rounds: cooperate
    if round_number <= 2:
        return "Cooperate"
    
    # Calculate average cooperation rate (ACR) over past min(r/2, 5) rounds
    acr = calculate_acr(game_history)
    
    # Adaptive Cooperation (AC): Cooperate if ACR >= 0.6
    if acr >= 0.6:
        return "Cooperate"
    else:
        return "Defect"

def multiplication_factor_adjustment(acr, game_state):
    # Monitor average payoff over past min(r/2, 5) rounds
    avg_payoff = calculate_avg_payoff(game_history)
    
    # Adjust ACR threshold for cooperation if average payoff is low
    if avg_payoff < (k/n) * (n-1):
        return "Cooperate" if acr >= 0.7 else "Defect"
    else:
        return adaptive_collective_optimism(game_state)

def last_round_strategy(acr, game_history):
    # Cooperate if ACR >= 0.5 over entire game history
    return "Cooperate" if acr >= 0.5 else "Defect"

# Use these functions to determine the action for each round
action = adaptive_collective_optimism(game_state)
if round_number == r:
    action = last_round_strategy(acr, game_history)
elif k is relatively low (e.g., k < n/2):
    action = multiplication_factor_adjustment(acr, game_state)
```
ACO combines elements of adaptive cooperation and collective optimism to achieve a robust and effective strategy in the N-Player Public Goods Game.
'''

description_COLLECTIVE_339 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and depends only on game parameters and history:

**Strategy Name:** Adaptive Collective Optimizer (ACO)

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to establish a cooperative tone and gather information about opponents' behavior.
2. **Exploration Phase (2 < t ≤ 5):**
	* If the total number of cooperators in the previous round is above the threshold (n/2), cooperate (C).
	* Otherwise, defect (D) with probability p = (k - 1)/(n - 1). This introduces some randomness to test opponents' reactions.
3. **Exploitation Phase (t > 5):**
	* Calculate the average payoff per cooperator in the previous round: avg_payoff_C = (∑(π_i,t-1 | c_i,t-1 = 1)) / (#cooperators_t-1).
	* If avg_payoff_C ≥ k/n, cooperate (C). This indicates that cooperation is profitable.
	* Otherwise, defect (D) with probability p = 1 - (avg_payoff_C / (k/n)). This adjusts the defection rate based on the effectiveness of cooperation.

**Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round to maximize total payoff, as there is no future interaction.
2. **Ties and Low Cooperation:** If the number of cooperators is consistently low (< n/4), defect (D) with probability p = 0.5 to encourage opponents to cooperate.

**Collective Mindset:**

1. **Reciprocity:** ACO aims to reciprocate cooperation when it's profitable, encouraging a collective cooperative mindset.
2. **Punishment:** By introducing some randomness in the Exploration Phase and adjusting the defection rate based on cooperation effectiveness, ACO "punishes" non-cooperative behavior.

**Additional Notes:**

* To prevent exploitation by opponents who might defect after an initial period of cooperation, ACO continuously monitors the average payoff per cooperator and adjusts its strategy accordingly.
* The strategy does not rely on opponent-specific information or norms, making it robust against a wide range of behaviors.

Pseudocode (simplified for clarity):
```
def AdaptiveCollectiveOptimizer(n, k, r, history):
    # Initial rounds
    if t ≤ 2:
        return C

    # Exploration phase
    elif 2 < t ≤ 5:
        total_cooperators = sum(c_i,t-1 == 1)
        if total_cooperators > n/2:
            return C
        else:
            p_defect = (k - 1) / (n - 1)
            return D with probability p_defect

    # Exploitation phase
    elif t > 5:
        avg_payoff_C = (∑(π_i,t-1 | c_i,t-1 == 1)) / (#cooperators_t-1)
        if avg_payoff_C ≥ k/n:
            return C
        else:
            p_defect = 1 - (avg_payoff_C / (k/n))
            return D with probability p_defect

    # Last round
    elif t == r:
        return C

    # Ties and low cooperation
    elif total_cooperators < n/4:
        p_defect = 0.5
        return D with probability p_defect
```
'''

description_COLLECTIVE_340 = '''
**Collective Strategy: "Adaptive Cooperation" (AC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to cooperate.
2. **Previous Round Analysis**: After each round, calculate the average cooperation rate among all players (`avg_coop_rate`) by dividing the total number of cooperators by `n`.
3. **Cooperation Threshold**: Set a dynamic cooperation threshold (`coop_threshold`) based on the previous round's `avg_coop_rate` and the game parameters `k` and `n`. Specifically:

   - If `avg_coop_rate` > 0.5, set `coop_threshold` to `min(k/n * (1 + avg_coop_rate), 1)`.
   - Otherwise, set `coop_threshold` to `max(k/n * (1 - avg_coop_rate), 0)`.
4. **Action Selection**: Cooperate if the current round's cooperation threshold (`coop_threshold`) is greater than or equal to a uniformly distributed random variable between 0 and 1.

**Pseudocode**
```python
def AdaptiveCooperation(n, k):
    # Initialize variables
    avg_coop_rate = None

    for t in range(r):  # Round loop
        if t == 0:
            action = C  # Cooperate in the first round
        else:
            avg_coop_rate = (sum(c_i for c_i in prev_round_actions) / n)
            coop_threshold = calculate_cooperation_threshold(avg_coop_rate, k, n)

            random_variable = uniform(0, 1)
            action = C if coop_threshold >= random_variable else D

        # Play the selected action and observe the outcome
        payoffs[t] = play_and_observe(action, prev_round_actions)
        prev_round_actions = append(prev_round_actions, action)

def calculate_cooperation_threshold(avg_coop_rate, k, n):
    if avg_coop_rate > 0.5:
        return min(k/n * (1 + avg_coop_rate), 1)
    else:
        return max(k/n * (1 - avg_coop_rate), 0)
```
**Edge Cases:**

* **Last Round**: Since the game is a repeated simultaneous-move game, there's no need for special handling in the last round.
* **Ties**: In case of ties in `avg_coop_rate`, use a fixed tiebreaker (e.g., cooperate).

**Collective Mindset:**
This strategy aligns with the collective mindset by:

* Cooperating initially to establish trust
* Adapting cooperation based on previous rounds' outcomes, which reflects the collective's behavior
* Using a dynamic cooperation threshold that considers both individual and group benefits

The Adaptive Cooperation (AC) strategy aims to balance self-interest with collective well-being while being robust against various opponent behaviors.
'''

description_COLLECTIVE_341 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

AC aims to balance individual payoff maximization with collective welfare, adapting to changing opponent behaviors and game history.

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a cooperative tone and gather information about opponents' initial strategies.
2. **Subsequent Rounds:**
	* If the majority of players (> n/2) cooperated in the previous round, Cooperate (C).
	* If the number of cooperators is exactly half (n/2), Defect (D) to test the robustness of others' cooperative strategies.
	* If fewer than half the players cooperated, Defect (D) to minimize losses and encourage others to cooperate.
3. **Punishment Mechanism:** If a player defects while most others cooperate (> n/2), AC will defect in the next round to punish the deviator. This mechanism aims to maintain cooperation by making defection costly.
4. **Forgiveness:** After punishing a defector, AC will Cooperate (C) in the subsequent round if the majority of players (> n/2) also cooperate, allowing for re-establishment of cooperation.

**Edge Cases:**

* **Last Round:** Cooperate (C), as there is no future punishment or reward to influence behavior.
* **Ties:** In cases where the number of cooperators equals half the players (n/2), AC will Defect (D) to break the tie and encourage cooperation in subsequent rounds.

**Collective Mindset:**

AC prioritizes collective welfare by:

1. Cooperating when most others do, to maintain a cooperative environment.
2. Punishing defectors to discourage exploitation and promote cooperation.
3. Forgiving past defections if cooperation is re-established, allowing for the game to recover from deviations.

By adapting to changing opponent behaviors and game history, AC aims to create an environment where cooperation is rewarded and defects are costly, ultimately leading to a more stable and cooperative outcome for all players.

Pseudocode (for implementation purposes):
```markdown
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds

# First round
if t == 1:
    action[t] = C  # Cooperate in the first round

# Subsequent rounds
else:
    majority_cooperated = sum(c_j for j in range(1, n+1) if c_j == 1) > n/2
    
    if majority_cooperated:
        action[t] = C  # Cooperate if most others cooperated
    elif sum(c_j for j in range(1, n+1) if c_j == 1) == n/2:
        action[t] = D  # Defect if exactly half cooperated
    else:
        action[t] = D  # Defect if fewer than half cooperated

# Punishment mechanism
if t > 1 and majority_cooperated and any(c_j == 0 for j in range(1, n+1)):
    action[t] = D  # Defect to punish deviator

# Forgiveness
if t > 2 and any(action[t-1] == D for i in range(1, n+1)) and majority_cooperated:
    action[t] = C  # Cooperate if cooperation is re-established

# Last round
if t == r:
    action[t] = C  # Cooperate in the last round
```
Note that this pseudocode serves as a guide for implementation purposes only.
'''

description_COLLECTIVE_342 = '''
**Collective Strategy: Adaptive Cooperation with Social Norms (ACSN)**

ACSN aims to balance individual payoffs with collective well-being by adapting to the game's history and promoting cooperation through social norms.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Observation Phase:** For rounds 2 to r/2 (where r is even), observe the number of cooperators (c_t) and defectors (d_t) in each round. Calculate the cooperation rate: c_rate = Σ(c_t) / (Σ(c_t) + Σ(d_t)).
3. **Adaptive Cooperation:** For rounds r/2 + 1 to r, use the following decision rules:
	* If c_rate ≥ k/n, Cooperate (C). This threshold indicates a strong social norm for cooperation.
	* If c_rate < k/n and the previous round's payoff was higher than the average payoff of the last r/4 rounds, Cooperate (C). This condition encourages continued cooperation when it has been beneficial in recent history.
	* Otherwise, Defect (D).
4. **Social Norm Enforcement:** If a player observes another player defecting while they cooperated in the previous round, they will Defect (D) in the next round to punish the non-cooperator.

**Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round to maintain a cooperative reputation and encourage others to do the same.
2. **Ties:** In case of ties in cooperation rates or payoffs, prioritize Cooperation (C).
3. **Empty History:** If there is no history (e.g., in the first few rounds), default to Cooperate (C).

**Collective Mindset:**

ACSN promotes a collective mindset by:

1. Encouraging initial cooperation to establish a social norm.
2. Adapting to the game's history and adjusting behavior based on observed cooperation rates.
3. Punishing non-cooperators to maintain the social norm.

By balancing individual payoffs with collective well-being, ACSN aims to achieve high total payoffs while promoting a cooperative environment in the N-Player Public Goods Game.

**Pseudocode:**
```markdown
ACSN(player_id) {
  // Initialize variables
  c_rate = 0; // Cooperation rate
  prev_payoff = 0; // Previous round's payoff
  avg_payoff = 0; // Average payoff of last r/4 rounds

  for (round = 1 to r) {
    if (round == 1) { // Initial Round
      action = C;
    } else if (round <= r/2) { // Observation Phase
      c_rate = update_c_rate(c_t, d_t);
    } else { // Adaptive Cooperation
      if (c_rate >= k/n) {
        action = C;
      } else if (prev_payoff > avg_payoff && round > r/2) {
        action = C;
      } else {
        action = D;
      }
    }

    // Social Norm Enforcement
    if (opponent_defected(prev_round)) {
      action = D;
    }

    // Update variables
    prev_payoff = calculate_payoff(action, c_t);
    avg_payoff = update_avg_payoff(prev_payoff);

    return action;
  }
}
```
Note: This pseudocode is a simplified representation of the strategy and will be refined during implementation.
'''

description_COLLECTIVE_343 = '''
Here's a collective strategy that adapts to various opponent behaviors and aligns with the collective mindset:

**Strategy Name: Adaptive Collective Contribution (ACC)**

**Decision Rules:**

1. **Initial Rounds**: Cooperate in the first two rounds to establish a baseline of cooperation and encourage others to follow suit.
2. **Contribution Threshold**: Track the total number of cooperators in each round, including yourself. If this threshold is above or equal to `n/2` (where `n` is the number of players), cooperate in the next round.
3. **Adaptive Response**: Monitor the average payoff difference between rounds where you cooperated (`C_payoff`) and rounds where you defected (`D_payoff`). If `C_payoff > D_payoff`, continue cooperating. Otherwise, defect for one round to reassess the situation.
4. **Exploitation Detection**: Keep track of the number of consecutive rounds where others' defections resulted in a higher payoff than your cooperation (`exploit_count`). If `exploit_count >= 3`, switch to defecting for two rounds to signal dissatisfaction and encourage others to cooperate.

**Edge Cases:**

1. **Last Round**: Cooperate, as there's no incentive to defect in the final round.
2. **Ties**: In case of a tie between cooperation and defection payoffs, prefer cooperating to maintain a cooperative atmosphere.

**Collective Alignment:**

ACC prioritizes collective well-being by:

1. Encouraging initial cooperation to establish a positive tone.
2. Adapting to the group's behavior through the contribution threshold.
3. Responding to exploitation by signaling dissatisfaction and encouraging others to cooperate.

**Pseudocode:**
```python
def ACC(n, k, r):
  # Initialize variables
  C_payoff = []
  D_payoff = []
  exploit_count = 0
  last_round_action = None

  for t in range(1, r+1):
    if t <= 2:
      action = 'C'  # Cooperate in first two rounds
    elif sum(C_payoff) >= n/2:
      action = 'C'
    else:
      if C_payoff[-1] > D_payoff[-1]:
        action = 'C'
      else:
        action = 'D'

    # Monitor payoff differences and exploitation
    if action == 'C' and last_round_action == 'D':
      if len(D_payoff) > 0 and sum(D_payoff) - sum(C_payoff) > 0:
        exploit_count += 1

    # Respond to exploitation
    if exploit_count >= 3:
      action = 'D'
      exploit_count = 0

    last_round_action = action
    C_payoff.append(0) if action == 'C' else D_payoff.append(0)

    # Calculate payoffs based on current actions
    payoff = calculate_payoff(n, k, action)
    C_payoff[-1] += payoff if action == 'C' else 0
    D_payoff[-1] += payoff if action == 'D' else 0

  return ACC
```
This strategy balances individual incentives with collective well-being and adapts to various opponent behaviors.
'''

description_COLLECTIVE_344 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Reciprocal Cooperation**: If the majority of players cooperated (i.e., ≥ n/2) in the previous round, cooperate (C) in the current round. This encourages continued cooperation and rewards those who have contributed to the public good.
3. **Punish Defection**: If a player defected (D) in the previous round and the majority of players cooperated, defect (D) in the current round. This discourages exploitation and free-riding behavior.
4. **Adaptive Threshold**: Monitor the average cooperation rate over the past `t` rounds (where `t` is a parameter). If this average falls below a certain threshold (`θ`), switch to defection (D) for one round to signal discontent and encourage others to cooperate.

Pseudocode:
```python
def adaptive_cooperation(n, k, r, θ=0.5, t=3):
    # Initialize variables
    prev_round_actions = []
    avg_coop_rate = 1.0
    
    for i in range(r):
        if i == 0:  # Initial Round
            action = 'C'
        else:
            # Calculate average cooperation rate over past `t` rounds
            coop_count = sum(1 for j in prev_round_actions[-t:] if j == 'C')
            avg_coop_rate = coop_count / t
            
            if avg_coop_rate >= θ:  # Reciprocal Cooperation
                action = 'C'
            elif len([j for j in prev_round_actions[-1] if j == 'D']) > n/2:  # Punish Defection
                action = 'D'
            else:
                action = 'D'  # Default to defection
        
        # Update history
        prev_round_actions.append(action)
        
    return action
```
**Edge Cases:**

* **Last Round**: Cooperate (C) in the last round, as there is no incentive to defect and risk damaging the collective outcome.
* **Tie-breaking**: In cases where the majority of players are tied between cooperation and defection, cooperate (C) to encourage continued cooperation.

**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes collective well-being by:

1. Encouraging cooperation through reciprocal behavior
2. Punishing exploitation and free-riding
3. Adapting to changing circumstances through the use of an adaptive threshold

By following this strategy, players contribute to a cooperative environment that benefits all participants, even in the presence of diverse opponent behaviors.

This strategy is designed to be robust and adaptable, making it suitable for a tournament setting where opponents' strategies are unknown.
'''

description_COLLECTIVE_345 = '''
Here's a collective strategy for the N-Player Public Goods Game that depends on game parameters and history:

**Strategy Name: Adaptive Collective Cooperation (ACC)**

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to cooperate.
2. **Subsequent Rounds:** Calculate the average payoff of cooperators (AC) and defectors (AD) from the previous round using the observed actions and payoffs. If AC > AD, cooperate (C); otherwise, defect (D).
3. **Adaptive Threshold:** Introduce an adaptive threshold (τ) that adjusts based on the game's history. Initially set τ = 0.5. If the number of cooperators in the previous round is above or below τ, adjust τ by ±0.1 to reflect the changing cooperation level.
4. **Cooperation Trigger:** If the current round's average payoff of cooperators (AC) exceeds the adaptive threshold (τ), cooperate (C); otherwise, defect (D).
5. **Punishment Mechanism:** If a player observes that another player has defected in the previous round while they themselves cooperated, they will defect (D) in the current round to punish the non-cooperative behavior.
6. **Forgiveness:** After punishing a non-cooperative player, return to cooperating (C) if the punished player cooperates in the next round.

**Edge Cases:**

1. **Last Round:** In the last round, always cooperate (C), as there is no future punishment or reward.
2. **Tie Breaker:** If AC = AD, flip a coin (or use a random number generator) to decide between cooperating and defecting.

**Collective Mindset:**

ACC prioritizes collective cooperation by initially cooperating and then adapting based on the game's history. By calculating average payoffs and using an adaptive threshold, ACC balances individual self-interest with the desire for collective success. The punishment mechanism ensures that non-cooperative behavior is addressed, while forgiveness allows for repairing relationships.

**Pseudocode:**
```markdown
def Adaptive_Collective_Cooperation(game_parameters, game_history):
  # Initialize variables
  AC = AD = τ = 0.5

  if current_round == 1:
    return "C"  # Cooperate in the first round

  # Calculate average payoffs for cooperators and defectors
  AC = calculate_average_payoff(game_history, "C")
  AD = calculate_average_payoff(game_history, "D")

  # Adjust adaptive threshold
  if number_of_cooperators > τ:
    τ += 0.1
  elif number_of_cooperators < τ:
    τ -= 0.1

  # Cooperation trigger
  if AC > τ:
    return "C"
  else:
    return "D"

  # Punishment mechanism
  for opponent in game_history:
    if opponent.defected and self.cooperated:
      return "D"

  # Forgiveness
  for opponent in game_history:
    if opponent.punished and opponent.cooperated_next_round:
      return "C"
```
This ACC strategy balances individual self-interest with collective cooperation, adapting to the game's history while maintaining a robust response to non-cooperative behavior.
'''

description_COLLECTIVE_346 = '''
**Collective Strategy: Adaptive Public Goods (APG)**

**Decision Rules**

The APG strategy adapts to the game's history by balancing cooperation and defection based on the observed collective behavior. The decision rules are as follows:

1. **Initial Rounds**: Cooperate in the first two rounds to encourage a cooperative atmosphere.
2. **Observation Phase**: In subsequent rounds (t > 2), observe the number of cooperators (C) and defectors (D) from the previous round.
3. **Cooperation Threshold**: Set a cooperation threshold, θ = k/n. If the proportion of cooperators in the previous round is above this threshold (C/(C+D) ≥ θ), cooperate in the current round.
4. **Punishment Mechanism**: If the proportion of cooperators falls below the threshold (C/(C+D) < θ), defect in the current round to signal dissatisfaction with the collective behavior.

**Handling Edge Cases**

* **Last Round**: In the last round, cooperate if the total payoff from cooperating exceeds the total payoff from defecting over all previous rounds. Otherwise, defect.
* **Ties**: In case of ties (C = D), prioritize cooperation in the current round to maintain a cooperative atmosphere.

**Pseudocode**
```markdown
# Initialize variables
cooperate = True  # First two rounds: cooperate
observation_phase = False  # Flag for observation phase

for t = 1 to r:
    if t <= 2:
        action = COOPERATE
    else:
        observation_phase = True
        
        # Observe previous round's behavior
        C, D = get_cooperators_and_defectors(t-1)
        
        # Cooperation threshold check
        if C / (C + D) >= k/n:
            action = COOPERATE
        else:
            action = DEFECT
            
    # Last round special case
    if t == r:
        total_coop_payoff, total_defect_payoff = calculate_total_payoffs()
        if total_coop_payoff > total_defect_payoff:
            action = COOPERATE
    
    take_action(action)
```

**Collective Mindset Alignment**

The APG strategy is designed to be collective by:

1. **Encouraging cooperation**: Cooperating in initial rounds sets a positive tone and encourages others to cooperate.
2. **Adapting to the group**: The cooperation threshold ensures that individual behavior adapts to the observed collective behavior, promoting cooperation when it's beneficial for all.
3. **Punishing free-riding**: Defecting when the cooperation threshold is not met signals dissatisfaction with non-cooperative behavior and encourages others to cooperate.

By using this adaptive strategy, APG balances individual self-interest with collective well-being, making it a robust and effective approach in a wide range of environments.
'''

description_COLLECTIVE_347 = '''
**Collective Strategy: "Adaptive Collective Optimism" (ACO)**

Decision Rules:

1. **Initial Round**: Cooperate (C) in the first round to establish a positive tone and encourage cooperation from others.
2. **Contribution Threshold**: Set a threshold for the minimum number of cooperators required for cooperation to be beneficial, calculated as `k/n * n = k`. If the number of cooperators in the previous round is below this threshold, defect (D) in the next round.
3. **Reciprocity**: Cooperate if at least `n/2` players cooperated in the previous round, and the total payoff was greater than or equal to the average individual payoff (`k/n * n = k`). This encourages cooperation when it is likely to be reciprocated.
4. **Exploitation Prevention**: If a player has defected for more than `r/2` rounds consecutively, defect in response to prevent exploitation.
5. **Payoff-Based Adaptation**: Adjust the strategy based on the total payoff obtained so far:
	* If the average payoff per round is below `(k + 1)/n`, increase the contribution threshold by 1 for the next round.
	* If the average payoff per round is above `k/n`, decrease the contribution threshold by 1 for the next round.

Edge Cases:

* **Last Round**: Cooperate if at least `n/2` players cooperated in the second-to-last round, and the total payoff was greater than or equal to the average individual payoff (`k/n * n = k`). Otherwise, defect.
* **Tiebreaker**: In case of a tie in the contribution threshold, cooperate.

Pseudocode:
```markdown
function AdaptiveCollectiveOptimism(n, r, k):
  # Initialize variables
  cooperators = []
  payoffs = []

  for t = 1 to r:
    if t == 1:  # Initial Round
      action = C
    else:
      # Contribution Threshold
      threshold = k/n * n

      # Reciprocity and Exploitation Prevention
      reciprocity_condition = (sum(cooperators[t-1]) >= n/2) and (payoffs[t-1] >= k)
      exploitation_condition = (defect_count > r/2)

      if reciprocity_condition or (not exploitation_condition):
        action = C
      else:
        action = D

    # Payoff-Based Adaptation
    average_payoff = sum(payoffs) / t
    if average_payoff < (k + 1)/n:
      threshold += 1
    elif average_payoff > k/n:
      threshold -= 1

    # Last Round Handling
    if t == r:
      action = C if (sum(cooperators[t-2]) >= n/2) and (payoffs[t-2] >= k) else D

    # Tiebreaker
    if sum(cooperators[t-1]) == threshold:
      action = C

    cooperators.append(action)
    payoffs.append(calculate_payoff(n, r, k, cooperators))

  return payoffs
```
This strategy aims to balance cooperation and self-interest by adapting to the collective behavior of other players. By incorporating reciprocity, exploitation prevention, and payoff-based adaptation, ACO promotes a positive tone and encourages cooperation while minimizing the risk of exploitation.
'''

description_COLLECTIVE_348 = '''
**Collective Strategy: Adaptive Cooperation with History-Driven Threshold (ACHT)**

**Overview**

ACHT is a collective strategy designed for the N-Player Public Goods Game that adapts to the game's history and balances individual self-interest with collective benefits. ACHT uses a dynamic threshold to determine cooperation levels, ensuring robustness against various opponent behaviors.

**Decision Rules**

1. **Initial Round (t=1)**: Cooperate (C) in the first round to establish a cooperative baseline.
2. **Subsequent Rounds (t>1)**: Calculate the average cooperation rate of all players in the previous round (t-1). This is denoted as `avg_coop_t-1`.
3. **Dynamic Threshold**: Set a threshold value, `τ`, based on the game's parameters and history:
	* `τ = k / n` if `avg_coop_t-1` < 0.5
	* `τ = (k / n) * (1 + avg_coop_t-1)` otherwise
4. **Cooperation Decision**: Cooperate (C) in the current round (t) if the number of cooperators in the previous round (`num_coop_t-1`) is greater than or equal to `τ`. Otherwise, Defect (D).

Pseudocode:
```python
def ACHT(n, k, history):
    if len(history) == 0:  # First round
        return 'C'
    
    avg_coop_t-1 = sum([1 if action == 'C' else 0 for action in history[-1]]) / n
    tau = k / n if avg_coop_t-1 < 0.5 else (k / n) * (1 + avg_coop_t-1)
    
    num_coop_t-1 = sum([1 if action == 'C' else 0 for action in history[-1]])
    
    return 'C' if num_coop_t-1 >= tau else 'D'
```
**Edge Cases**

* **Last Round (t=r)**: Follow the same decision rules as above, ensuring consistent behavior throughout the game.
* **Ties**: In case of a tie (e.g., `num_coop_t-1` equals `τ`), cooperate (C) to maintain a cooperative posture.

**Collective Mindset**

ACHT aligns with a collective mindset by:

* Encouraging cooperation in early rounds to establish a positive baseline.
* Adapting to the game's history, taking into account the average cooperation rate of all players.
* Using a dynamic threshold that balances individual self-interest with collective benefits.

By following these decision rules, ACHT aims to promote cooperation while being robust against various opponent behaviors.
'''

description_COLLECTIVE_349 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy, "Adaptive Cooperation," aims to balance individual self-interest with the benefits of cooperation, while being robust to various opponent behaviors.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
	* Cooperate (C) in Rounds 1 and 2 to gather information about others' behavior.
	* In Round 3, Defect (D) if the average cooperation rate of other players is below a threshold (e.g., 0.5); otherwise, Cooperate.
2. **Adaptive Cooperation:**
	* From Round 4 onwards:
		+ If the previous round's payoff was greater than or equal to the game's average payoff, Cooperate.
		+ Otherwise, Defect if the number of cooperators in the previous round is below a threshold (e.g., n/2); otherwise, Cooperate.
3. **Punishment and Forgiveness:**
	* If an opponent defects after cooperating with them for at least 2 consecutive rounds, Defect for the next round to punish non-cooperation.
	* After punishing, return to the Adaptive Cooperation rule.

**Edge Cases:**

1. **Last Round (Round r):**
	* Cooperate if the game's average payoff is greater than or equal to the individual player's average payoff; otherwise, Defect.
2. **When Others Are Mostly Defecting:**
	* If more than 75% of opponents are defecting in a round, Defect as well to avoid exploitation.

**Collective Mindset Alignment:**

Our strategy aligns with the collective mindset by:

1. Encouraging cooperation through initial exploration and adaptive cooperation rules.
2. Punishing non-cooperation while allowing for forgiveness and adapting to changing circumstances.
3. Balancing individual self-interest with consideration for the group's well-being.

**Pseudocode (for illustration purposes):**
```
Initialize:
  - Round counter: t = 1
  - Cooperation threshold: theta_c = 0.5
  - Punishment flag: punish_flag = False

Decision Rules:
  if t <= 3 then
    // Initial Exploration
    if t == 1 or t == 2 then
      Cooperate (C)
    else if average_cooperation_rate < theta_c then
      Defect (D)
    else
      Cooperate (C)

  else
    // Adaptive Cooperation
    if previous_payoff >= game_average_payoff then
      Cooperate (C)
    else if num_cooperators < n/2 then
      Defect (D)
    else
      Cooperate (C)

    // Punishment and Forgiveness
    if punish_flag then
      Defect (D)
      punish_flag = False
    elif opponent_defected_after_cooperation then
      punish_flag = True

  end if

Update:
  - Increment round counter: t += 1
```
This strategy will be refined and implemented as an algorithm for the tournament.
'''

description_COLLECTIVE_350 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Overview**

Adaptive Cooperation (AC) is a collective strategy that balances individual self-interest with collective well-being. AC adapts to the game's history and opponent behaviors, promoting cooperation while being robust against defectors.

**Decision Rules**

1. **First Round**: Cooperate (C). This sets a cooperative tone and encourages others to follow suit.
2. **Subsequent Rounds**:
	* If the average payoff of cooperators in the previous round is higher than the average payoff of defectors, cooperate (C).
	* Otherwise, defect (D).

Pseudocode:
```
IF first_round THEN
  action = C
ELSE
  avg_coop_payoff_prev_round = calculate_average_payoff(cooperators, prev_round)
  avg_defect_payoff_prev_round = calculate_average_payoff(defectors, prev_round)
  
  IF avg_coop_payoff_prev_round > avg_defect_payoff_prev_round THEN
    action = C
  ELSE
    action = D
  ENDIF
ENDIF
```
**Handling Edge Cases**

1. **Last Round**: Cooperate (C). This ensures that the strategy ends on a cooperative note, promoting a sense of fairness and reciprocity.
2. **Ties in Payoffs**: In cases where the average payoffs are equal, cooperate (C) to maintain a positive tone.

**Collective Mindset**

AC aligns with the collective mindset by:

1. Promoting cooperation as the default behavior.
2. Adapting to the game's history and opponent behaviors to optimize collective outcomes.
3. Encouraging reciprocity and fairness through cooperative actions in key situations (e.g., first round, last round).

**Robustness**

AC is designed to be robust against a wide range of opponent behaviors:

1. **Defectors**: AC will adapt to defecting opponents by shifting its behavior towards defection.
2. **Cooperators**: AC will reinforce cooperation when facing cooperative opponents.
3. **Mixed Strategies**: AC will adapt to mixed strategies, balancing cooperation and defection in response to the opponent's actions.

By implementing Adaptive Cooperation, this strategy aims to achieve a balance between individual self-interest and collective well-being, promoting a cooperative and adaptive approach to the N-Player Public Goods Game.
'''

description_COLLECTIVE_351 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and maximizes overall payoff:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 0.5. This initial cooperation serves as a "test" for other players' willingness to cooperate.
2. **Reciprocal Altruism**: For rounds t > 1:
	* If the total number of cooperators in the previous round (t-1) is greater than or equal to n/2, cooperate (C) with probability 0.8.
	* Otherwise, defect (D) with probability 0.6.
3. **Adaptive Adjustment**: Based on the outcome of the previous round (t-1):
	* If the total payoff for all players in round t-1 is greater than or equal to n \* (k/n), increase the cooperation probability by 0.1 for the next round.
	* Otherwise, decrease the cooperation probability by 0.1 for the next round.

**Handling Edge Cases:**

* **Last Round**: In the last round (r), defect (D) regardless of previous rounds' outcomes. This ensures maximizing individual payoff in the final round.
* **Tie-Breaking**: In cases where multiple players have the same cooperation probability, break ties by cooperating if the player's index is odd and defecting if it's even.

**Collective Mindset:**

ACO aims to promote collective optimism by encouraging cooperation when others are willing to cooperate. By reciprocating altruism and adapting to changing circumstances, ACO fosters a collaborative environment that benefits all players.

**Pseudocode:**
```python
def ACO(n, k, r):
    # Initialize variables
    coop_prob = 0.5  # Initial cooperation probability
    prev_coop_count = 0  # Previous round's cooperation count

    for t in range(1, r+1):
        if t == 1:
            action = 'C' if random.random() < coop_prob else 'D'
        else:
            if prev_coop_count >= n / 2:
                coop_prob = max(coop_prob + 0.1, 1)
                action = 'C' if random.random() < coop_prob else 'D'
            else:
                coop_prob = min(coop_prob - 0.1, 0)
                action = 'D' if random.random() < (1 - coop_prob) else 'C'

        # Update cooperation count and total payoff
        prev_coop_count = sum([1 for player in range(n) if players[player].action == 'C'])
        total_payoff = sum([player.payoff for player in range(n)])

        # Adaptive adjustment
        if total_payoff >= n * (k / n):
            coop_prob += 0.1
        else:
            coop_prob -= 0.1

        # Last round exception
        if t == r:
            action = 'D'

    return action
```
This strategy should perform well in a tournament setting, as it balances individual self-interest with collective optimism and adaptability to various opponent behaviors.
'''

description_COLLECTIVE_352 = '''
**Collective Strategy: Adaptive Cooperation with History-Based Learning**

Our collective strategy aims to balance individual payoffs with collective welfare by adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Rounds (t = 1 to ⌊r/2⌋)**:
	* Cooperate (C) in the first round to encourage initial cooperation.
	* In subsequent rounds, mirror the majority action of the previous round. If there was no clear majority, cooperate.
2. **Mid-Game Adaptation (t = ⌊r/2⌋ + 1 to r - 2)**:
	* Track the average payoff per player over the last ⌊r/4⌋ rounds.
	* Cooperate if the average payoff is below a threshold (e.g., 1.5), indicating that defection is dominating.
	* Defect if the average payoff is above the threshold, suggesting cooperation is prevalent.
3. **Late-Game Prudence (t = r - 1 to r)**:
	* Cooperate in the penultimate round to encourage opponents to cooperate in the final round.
	* In the last round, mirror the majority action of the previous round.

**Edge Case Handling:**

* If there is only one opponent (n = 2), alternate between cooperation and defection to maintain a balance.
* In the event of a tie in the number of cooperators and defectors, cooperate.

**Collective Mindset Alignment:**

Our strategy prioritizes collective welfare by:

1. Encouraging initial cooperation to set a positive tone for the game.
2. Adapting to the game's history to maintain a balance between individual payoffs and collective welfare.
3. Fostering cooperation in late-game rounds to maximize overall payoffs.

**Pseudocode (simplified):**
```
Input: n, r, k, t (current round), previous_actions

// Initial Rounds
if t <= floor(r/2) then
  if t == 1 then cooperate
  else mirror majority action of previous round

// Mid-Game Adaptation
else if t > floor(r/2) and t < r - 2 then
  avg_payoff = calculate average payoff per player over last floor(r/4) rounds
  if avg_payoff < threshold (e.g., 1.5) then cooperate
  else defect

// Late-Game Prudence
else
  if t == r - 1 then cooperate
  else mirror majority action of previous round

Output: action (C or D)
```
This strategy is designed to be robust and adaptive, allowing it to perform well against a wide range of opponent behaviors in the tournament.
'''

description_COLLECTIVE_353 = '''
I propose a collective strategy called "Adaptive Cooperation with Gradual Withdrawal" (ACGW). ACGW balances individual payoffs with collective welfare by adapting to the group's cooperation level and gradually withdrawing cooperation in response to widespread defection.

**Decision Rules:**

1. **Initial Rounds**: Cooperate (C) for the first `α` rounds, where `α` is a parameter that depends on the number of players `n`. A good starting value is `α = min(3, n/2)`. This allows the strategy to build an initial reputation and encourages others to cooperate.
2. **Cooperation Tracking**: Maintain a cooperation score `θ` for each player, initialized to 1. Update `θ_i` as follows:
	* If player `i` cooperates (C), increase their score: `θ_i += 0.1`
	* If player `i` defects (D), decrease their score: `θ_i -= 0.2`
3. **Cooperation Threshold**: Introduce a cooperation threshold `τ`, which depends on the multiplication factor `k`. A good starting value is `τ = k / n`. When the average cooperation score of all players (`θ_avg`) exceeds `τ`, cooperate (C).
4. **Gradual Withdrawal**: If `θ_avg` falls below `τ`, start withdrawing cooperation gradually:
	* Decrease cooperation probability by 10% every `β` rounds, where `β` is a parameter that depends on the number of players `n`. A good starting value is `β = n / 2`.
5. **Punishment Mechanism**: If a player's cooperation score (`θ_i`) falls below a certain threshold (e.g., 0.5), defect (D) against them for the next `γ` rounds, where `γ` is a parameter that depends on the number of players `n`. A good starting value is `γ = n / 3`.

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round to maximize collective welfare.
2. **Tie-Breaking**: In case of a tie, cooperate (C) if the average cooperation score (`θ_avg`) exceeds `τ`, and defect (D) otherwise.

**Pseudocode:**
```python
def ACGW(n, k, r):
    α = min(3, n/2)
    τ = k / n
    β = n / 2
    γ = n / 3

    θ = [1.0] * n  # cooperation scores
    θ_avg = 1.0

    for t in range(r):
        if t < α:
            cooperate(t)  # initial rounds
        else:
            θ_avg = sum(θ) / n
            if θ_avg > τ:
                cooperate(t)
            elif θ_avg < τ:
                withdraw_cooperation(t, β)

def cooperate(t):
    c_i = 1  # contribute to community project
    return c_i

def withdraw_cooperation(t, β):
    global θ_avg
    if t % β == 0:  # decrease cooperation probability every β rounds
        θ_avg -= 0.1
    c_i = 1 if θ_avg > τ else 0
    return c_i

def punish(i, γ):
    global θ
    θ[i] = 0.5  # set cooperation score to 0.5
    for t in range(γ):
        c_i = 0  # defect against player i for γ rounds
```
This strategy aims to strike a balance between individual payoffs and collective welfare by adapting to the group's cooperation level and gradually withdrawing cooperation in response to widespread defection. By incorporating a punishment mechanism, ACGW discourages excessive defection and promotes cooperation among players.
'''

description_COLLECTIVE_354 = '''
**Collective Strategy: "Adaptive Cooperation with Tit-for-Tat"**

This strategy aims to balance individual self-interest with collective well-being by adapting to the group's behavior and using a tit-for-tat approach.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to establish a cooperative tone and encourage others to do the same.
2. **Tit-for-Tat**: For rounds 2 to r-1:
	* If the majority of players (> n/2) cooperated in the previous round, cooperate (C).
	* Otherwise, defect (D).
3. **Adaptive Cooperation**: In addition to tit-for-tat, consider the average payoff of all players in the previous round (π_avg). If π_avg is above a certain threshold (e.g., 1.5), increase the cooperation rate by 10% for the next round.
4. **Punishment Mechanism**: If a player's individual payoff (π_i) falls below a certain threshold (e.g., 0.5) in two consecutive rounds, switch to defecting (D) for one round to signal dissatisfaction with the group's behavior.

**Edge Cases:**

1. **Last Round**: In the final round (r), cooperate (C) if the majority of players cooperated in the previous round; otherwise, defect (D).
2. **Ties**: If the number of cooperators is exactly n/2, use a random choice between C and D.

**Collective Mindset:**

This strategy prioritizes cooperation when the group's behavior suggests it will be beneficial to all players. By adapting to the group's dynamics, we aim to create an environment where cooperation becomes the norm, leading to higher overall payoffs for everyone.

Pseudocode:
```markdown
# Initialize variables
cooperation_rate = 1 (100% cooperation in round 1)
tit_for_tat = True

for round from 2 to r-1:
    # Get previous round's data
    prev_round_cooperators = count(C) / n
    prev_round_payoff_avg = average(π_i)

    if tit_for_tat and prev_round_cooperators > n/2:
        cooperation_rate = 1 (cooperate)
    else:
        cooperation_rate = 0 (defect)

    # Adaptive Cooperation
    if prev_round_payoff_avg >= 1.5:
        cooperation_rate += 0.1

    # Punishment Mechanism
    if π_i < 0.5 for two consecutive rounds:
        cooperation_rate = 0 (defect) for one round

# Last Round
if r == last_round and prev_round_cooperators > n/2:
    cooperation_rate = 1 (cooperate)
```
This strategy aims to find a balance between individual self-interest and collective well-being, while being robust against various opponent behaviors.
'''

description_COLLECTIVE_355 = '''
I propose a collective strategy called "Adaptive Cooperation with Conditional Defection" (ACCD). This strategy balances individual self-interest with collective benefits, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to reciprocate.
2. **Subsequent Rounds:** Observe the previous round's outcomes:
	* If most players cooperated (> n/2), cooperate (C) to maintain the collective momentum.
	* If few or no players cooperated (< n/4), defect (D) to minimize losses and avoid exploitation.
	* If the number of cooperators is between n/4 and n/2, use a probabilistic approach:
		+ Calculate the average payoff per player in the previous round (avg_π).
		+ If avg_π > 1.5, cooperate (C) to build on the moderate success.
		+ Otherwise, defect (D) to adjust to the mixed environment.

**Edge Cases:**

* **Last Round:** Defect (D) if the total number of cooperators in the previous round is low (< n/4). Otherwise, cooperate (C) to maintain the collective benefit.
* **Tie-Breaking:** If the number of cooperators is exactly n/2 or n/4, use a random choice between cooperation and defection.

**Collective Mindset:**

ACCD prioritizes cooperation when the collective environment supports it. By cooperating in the initial round and adapting to the game's history, ACCD encourages other players to reciprocate and maintain a cooperative atmosphere. When facing exploitation or low cooperation levels, ACCD adjusts its strategy to minimize losses.

**Pseudocode (simplified):**

```
function ACCD(n, k, r):
  // Initialize variables
  cooperate = True
  avg_π = 0

  for round in range(1, r+1):
    if round == 1:
      action = COOPERATE
    else:
      prev_round_cooperators = count_cooperators(round-1)
      if prev_round_cooperators > n/2:
        action = COOPERATE
      elif prev_round_cooperators < n/4:
        action = DEFECT
      else:
        avg_π = calculate_avg_payoff(round-1)
        if avg_π > 1.5:
          action = COOPERATE
        else:
          action = DEFECT

    // Update cooperate variable for next round
    if prev_round_cooperators < n/4 or (round == r and prev_round_cooperators < n/2):
      cooperate = False
    else:
      cooperate = True

  return action
```

This strategy should perform well in a tournament setting, as it balances individual self-interest with collective benefits while adapting to the game's history and opponent behaviors.
'''

description_COLLECTIVE_356 = '''
I'll introduce a collective strategy called "Adaptive Collective Cooperation" (ACC) that balances individual self-interest with collective well-being.

**Decision Rules:**

1. **Initial Round**: Cooperate in the first round to set a positive tone and encourage cooperation.
2. **Defection Response**: If any player defects, respond with Defect in the next round. This ensures you're not taken advantage of by free-riders.
3. **Cooperation Maintenance**: Continue cooperating as long as:
	* All players cooperated in the previous round (or the first round).
	* The average payoff from cooperation is higher than or equal to the individual's private payoff from defecting (i.e., 1).
4. **Adaptive Response**: Monitor the number of defectors in each round and adjust your strategy:
	* If ≥ n/2 players cooperate, maintain cooperation.
	* If < n/2 players cooperate, switch to Defect for one round to signal dissatisfaction with the current level of cooperation.

**Edge Cases:**

1. **Last Round**: Cooperate if all players cooperated in the previous round; otherwise, defect.
2. **Single Player Cooperation**: If only you have been cooperating while others defect, switch to Defect after 3 consecutive rounds to avoid exploitation.
3. **All Players Defect**: Switch back to cooperation after 2 consecutive rounds of mutual defection.

**Collective Mindset:**

The ACC strategy prioritizes collective well-being by:

1. Encouraging cooperation through reciprocity and positive reinforcement.
2. Responding adaptively to changes in the group's behavior, promoting stability and fairness.
3. Balancing individual self-interest with collective benefits, avoiding exploitation while maintaining a high level of cooperation.

**Pseudocode:**

```markdown
Initialize:
  cooperate = True

For each round t from 1 to r:
  If (t == 1) or (all players cooperated in previous round):
    cooperate = True
  Else if any player defected in previous round:
    cooperate = False
  Else if average payoff from cooperation >= individual private payoff:
    cooperate = True
  Else:
    cooperate = AdaptiveResponse()

DefectResponse():
  If ≥ n/2 players cooperated:
    return Cooperate
  Else:
    return Defect

AdaptiveResponse():
  count_defectors()
  If < n/2 players cooperated:
    cooperate = False for one round
```

This strategy is robust, adaptive, and aligned with the collective mindset, making it well-suited to play in a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_357 = '''
**Collective Strategy: "Adaptive Cooperation"**

Our collective strategy, "Adaptive Cooperation," aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to signal willingness to cooperate and encourage others to do the same.
2. **Subsequent Rounds:** Observe the previous round's outcome and update our strategy based on the following conditions:
	* If the total number of cooperators (Σc_j) is greater than or equal to the multiplication factor (k), Cooperate (C).
	* Otherwise, Defect (D).

Pseudocode for the decision rule:
```
if t == 1:  # First round
    action = C
else:
    total_cooperators_prev_round = sum(c_j for j in players)
    if total_cooperators_prev_round >= k:
        action = C
    else:
        action = D
```

**Edge Cases:**

* **Last Round (t=r):** Cooperate (C) to maintain a positive collective outcome, even if it may not directly benefit our individual payoff.
* **Tiebreaker:** In case of a tie in the number of cooperators, cooperate (C) to encourage further cooperation.

**Collective Mindset:**

Our strategy prioritizes collective well-being by:

1. Initially cooperating to set a positive tone for the game.
2. Adapting to the game's history to maintain or increase the total number of cooperators.
3. Cooperating in the last round to ensure a positive collective outcome.

**Rationale:**

By cooperating initially and adapting to the game's history, we create an environment that encourages cooperation. If most players cooperate, our strategy will continue to cooperate, maximizing the collective payoff. If others defect, our strategy will adapt by defecting as well, minimizing losses.

This strategy is robust against various opponent behaviors, including:

* **All-Defect:** Our strategy will eventually defect, minimizing losses.
* **All-Cooperate:** Our strategy will maintain cooperation, maximizing collective payoffs.
* **Mixed Strategies:** Our adaptive nature allows us to respond effectively to different combinations of cooperate and defect actions.

By implementing "Adaptive Cooperation," we aim to achieve a high collective payoff while being robust against various opponent behaviors.
'''

description_COLLECTIVE_358 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally to establish a cooperative tone and encourage others to follow.
2. **Monitor and Respond**: Observe the total number of cooperators in each round (`t`) and calculate the cooperation rate (`cr_t = Σc_j / n`).
3. **Cooperation Threshold**: Set a dynamic cooperation threshold (`ct`) based on the game's history:
	* `ct_1 = 0.5` ( initial value)
	* `ct_t+1 = cr_t + δ * (cr_t - ct_t)` where `δ` is a learning rate (e.g., `δ = 0.1`)
4. **Adaptive Cooperation**: Cooperate (C) if the current cooperation rate (`cr_t`) exceeds or equals the threshold (`ct_t`). Otherwise, defect (D).
5. **Punish Defection**: If a player defects in round `t`, and their payoff is higher than the average cooperative payoff, reduce the cooperation threshold for the next round by a small amount (`γ`, e.g., `γ = 0.05`): `ct_t+1 -= γ`

**Edge Cases:**

1. **First Round**: Cooperate (C) unconditionally.
2. **Last Round**: Defect (D), as there is no future opportunity for reciprocity or cooperation.
3. **Tie-Breaking**: In case of a tie in the cooperation rate, cooperate (C) if the opponent's previous action was cooperative; otherwise, defect (D).

**Collective Mindset:**

ACE prioritizes cooperation while being responsive to the actions of others. By monitoring and adapting to the cooperation rate, ACE encourages collective cooperation and discourages exploitation.

Pseudocode:
```python
def ACE(n, k, r):
    # Initialize variables
    ct = 0.5  # initial cooperation threshold
    cr = 0  # current cooperation rate

    for t in range(r):
        if t == 0:  # First round
            action = C
        else:
            # Monitor and respond to cooperation rate
            cr_t = sum(c_j) / n
            ct_t+1 = cr_t + δ * (cr_t - ct_t)

            if cr_t >= ct_t:
                action = C
            else:
                action = D

            # Punish defection
            if π_i > average_cooperative_payoff and c_i == 0:
                ct_t+1 -= γ

        # Update cooperation threshold
        ct = ct_t+1

    return actions
```
ACE is designed to be robust against various opponent behaviors, including pure cooperators, defectors, and mixed strategies. By adapting to the cooperation rate, ACE promotes collective cooperation while maintaining a competitive edge in cases of exploitation.
'''

description_COLLECTIVE_359 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

ACP is a collective strategy that balances individual self-interest with collective cooperation, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) unconditionally to establish a cooperative tone.
2. **Punishment Phase**: If the total number of cooperators in the previous round is less than `n/2`, defect (D) in the next round to punish non-cooperators. This phase helps maintain cooperation by penalizing free-riders.
3. **Cooperation Phase**: If the total number of cooperators in the previous round is `n/2` or more, cooperate (C) in the next round to reinforce collective cooperation.
4. **Adaptive Adjustment**: Monitor the game's history and adjust the strategy based on the opponent behaviors:
	* If opponents are mostly cooperative (`> 75%` of players cooperated in the previous round), cooperate (C) unconditionally to maintain cooperation.
	* If opponents are mostly non-cooperative (`< 25%` of players cooperated in the previous round), defect (D) unconditionally to minimize losses.
5. **Last Round**: In the final round, cooperate (C) if the opponent's history indicates a high level of cooperation; otherwise, defect (D).

**Pseudocode:**
```python
def ACP(n, k, r, history):
    if r == 1:
        return 'C'  # Initial round

    prev_cooperators = sum(1 for action in history[-1] if action == 'C')

    if prev_cooperators < n / 2:
        return 'D'  # Punishment phase
    elif prev_cooperators >= n / 2:
        return 'C'  # Cooperation phase

    opponent_behavior = analyze_history(history)
    if opponent_behavior['cooperation_rate'] > 0.75:
        return 'C'  # Unconditional cooperation
    elif opponent_behavior['cooperation_rate'] < 0.25:
        return 'D'  # Unconditional defection

    if r == last_round:
        if opponent_behavior['cooperation_history']:
            return 'C'
        else:
            return 'D'

def analyze_history(history):
    # Analyze the game history to determine opponent behavior
    cooperation_rate = sum(1 for actions in history if actions.count('C') > len(actions) / 2) / len(history)
    cooperation_history = [actions.count('C') for actions in history]
    return {'cooperation_rate': cooperation_rate, 'cooperation_history': cooperation_history}
```
**Collective Mindset:**

ACP is designed to align with the collective mindset by:

* Encouraging cooperation through initial unconditional cooperation and punishment phases.
* Adapting to opponent behaviors to maintain a balance between individual self-interest and collective cooperation.
* Fostering cooperation in the final round if opponents have demonstrated a history of cooperation.

By implementing ACP, we create a robust strategy that balances individual interests with collective goals, making it an effective competitor in the tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_360 = '''
**Collective Strategy: "Adaptive Cooperativity with Robustness" (ACR)**

The ACR strategy aims to balance cooperation and self-interest while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Round**: In the first round, play C (Cooperate) to establish a baseline of cooperativeness.
2. **Consecutive Rounds**: Observe the total number of cooperators in the previous round (T-1). If T-1 ≥ n/2, play C; otherwise, play D.
3. **Punishment Mechanism**: If an opponent defects while you cooperate, and your payoff is less than the average payoff of all players in that round, play D in the next round against that specific opponent (but not necessarily against others).
4. **Forgiveness**: After two consecutive rounds of cooperation from a previously punished opponent, forgive them and revert to the standard decision rule.
5. **Endgame**: In the last round (r), play C if your total payoff is below the average total payoff of all players; otherwise, play D.

**Pseudocode:**

```
Initialize:
  prev_round_cooperators = 0
  opponent_defections = [False] * n

For each round t from 1 to r:
  if t == 1:
    action[t] = C
  else:
    if prev_round_cooperators >= n/2:
      action[t] = C
    else:
      action[t] = D
    
    # Punishment mechanism
    for opponent in range(n):
      if opponent_defections[opponent]:
        action[t, opponent] = D
    
    # Forgiveness
    for opponent in range(n):
      if opponent_defections[opponent] and prev_round_cooperators[opponent] == C:
        opponent_defections[opponent] = False

  # Observe opponents' actions and payoffs
  observe_prev_round_actions()
  update_opponent_defections()

  # Update prev_round_cooperators for next round
  prev_round_cooperators = count_cooperators(prev_round_actions)
```

**Collective Mindset:**

ACR prioritizes cooperation when the collective good is likely to be achieved (i.e., when at least half of the players cooperate). The punishment mechanism ensures that opponents are held accountable for their actions, while forgiveness encourages cooperation and adaptability. In the endgame, ACR balances self-interest with a desire to contribute to the collective well-being.

**Robustness:**

ACR is designed to be robust against various opponent behaviors:

* **Defectors**: Punishment mechanism discourages repeated defection.
* **Cooperators**: Forgiveness and standard decision rule encourage cooperation.
* **Mixed strategies**: ACR adapts to changing levels of cooperation in the population.
* **Noise or errors**: The strategy is relatively insensitive to occasional mistakes or random actions.

By implementing ACR, we aim to create a collective strategy that balances individual self-interest with a desire for cooperation and adaptability, making it a robust competitor in the tournament.
'''

description_COLLECTIVE_361 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Punishment (ACGP)**

**Decision Rules:**

1. **First Round:** Cooperate (C) unconditionally to establish a cooperative tone and encourage others to cooperate.
2. **Subsequent Rounds:** Use the following decision rules:
	* If the average cooperation rate in the previous round is above a threshold (θ), Cooperate (C).
	* If the average cooperation rate in the previous round is below θ, Defect (D) with probability p, and Cooperate (C) with probability 1-p.
	* Adjust the value of p based on the number of rounds remaining:
		+ For the first half of the game (r/2), set p = 0.5 ( moderate punishment).
		+ For the second half of the game, adjust p as follows:
			- If the opponent's cooperation rate is increasing or stable, decrease p by 0.1 every round.
			- If the opponent's cooperation rate is decreasing, increase p by 0.1 every round.
3. **Last Round:** Cooperate (C) if the average cooperation rate in the previous round is above θ; otherwise, Defect (D).

**Pseudocode:**
```python
def ACGP(n, k, r, theta=0.5):
    # Initialize variables
    avg_coop_rate = 0
    p = 0.5

    for t in range(1, r+1):
        if t == 1:
            action = 'C'
        else:
            avg_coop_rate = calculate_avg_coop_rate(previous_round)
            if avg_coop_rate >= theta:
                action = 'C'
            else:
                p_adjustment = adjust_p(avg_coop_rate, n, k, r, t)
                p += p_adjustment
                action = 'D' if random.random() < p else 'C'

        # Update variables for next round
        previous_round = current_round

    return action
```
**Rationale:**

1. **Initial Cooperation:** Starting with cooperation encourages others to cooperate and sets a positive tone.
2. **Adaptive Punishment:** Gradually adjusting the punishment probability (p) based on the opponent's behavior allows for flexible responses to changing circumstances.
3. **Threshold-based Decision-making:** Using an average cooperation rate threshold (θ) provides a clear decision criterion, making it easier for players to adapt and coordinate.

**Collective Mindset:**

1. **Cooperation-oriented:** ACGP prioritizes cooperation, aiming to create a mutually beneficial environment.
2. **Adaptive:** The strategy adjusts to the opponent's behavior, allowing for flexibility in response to changing circumstances.
3. **Gradual Punishment:** ACGP's punishment mechanism is designed to be gradual and responsive, rather than severe or vindictive.

**Edge Cases:**

1. **First Round:** Unconditional cooperation encourages others to cooperate.
2. **Last Round:** Cooperating if the opponent has been cooperative in previous rounds ensures a positive ending.
3. **Ties:** In case of ties (e.g., equal average cooperation rates), ACGP defaults to cooperating.

**Robustness:**

1. **Against Defectors:** ACGP's gradual punishment mechanism deters repeated defection.
2. **Against Cooperators:** ACGP rewards consistent cooperation, promoting mutual benefit.
3. **In Noisy Environments:** The adaptive nature of ACGP allows it to respond effectively in the presence of errors or noise.

By following this strategy, players can create a collective environment that promotes cooperation and adaptability, while minimizing the risk of exploitation by defectors.
'''

description_COLLECTIVE_362 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Punishment (ACGP)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds:**
	* If the average cooperation rate in the previous round is above a threshold (`coop_threshold`), cooperate (C).
	* Otherwise, defect (D) with a probability (`defect_prob`) that increases as the number of consecutive rounds below `coop_threshold` grows.

**Pseudocode:**
```
INITIALIZE:
  coop_threshold = k / n
  defect_prob = 0.2
  consecutive_defects = 0

FOR EACH ROUND (t):
  IF t == 1:
    action = C
  ELSE:
    avg_coop_rate_prev_round = calculate_average_cooperation_rate(t-1)
    IF avg_coop_rate_prev_round >= coop_threshold:
      action = C
      consecutive_defects = 0
    ELSE:
      consecutive_defects += 1
      defect_prob = min(1, defect_prob + 0.1 * consecutive_defects)
      action = D with probability defect_prob, otherwise C

calculate_average_cooperation_rate(t):
  # Calculate the average cooperation rate in round t using the game history
  total_contributions = Σ(j=1 to n) c_j,t
  avg_coop_rate = total_contributions / n
```
**Edge Cases:**

* **Last Round:** Cooperate (C) unconditionally, as there's no future round to punish or reward cooperation.
* **Consecutive Defections:** If all players defect for an extended period (`consecutive_defects` > 5), switch to a tit-for-tat strategy to encourage cooperation.

**Collective Mindset:**

ACGP is designed to promote collective cooperation by:

1. Encouraging initial cooperation to establish a positive tone.
2. Gradually increasing punishment (defection) in response to low cooperation rates, making it costly for others to defect repeatedly.
3. Providing opportunities for cooperation to recover when the average cooperation rate improves.

By adapting to the game history and adjusting its strategy accordingly, ACGP aims to create an environment where cooperation is a viable and rewarding choice for all players.
'''

description_COLLECTIVE_363 = '''
**Collective Strategy: Adaptive Cooperate-Defect (ACD)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) with a probability of 0.5.
2. **Subsequent Rounds**: For each round t, calculate the average payoff per cooperator in the previous round (t-1). If this value is greater than or equal to the average payoff per defector, cooperate (C). Otherwise, defect (D).
3. **Threshold Adjustment**: After every 5 rounds, adjust the threshold by ±0.1 based on the overall performance of the strategy. If the average payoff over the last 5 rounds is higher than in the previous 5 rounds, increase the threshold; otherwise, decrease it.
4. **Opponent Detection**: Monitor the opponent's actions and payoffs. If an opponent consistently defects (D) or has a high defection rate (>0.7), adapt by reducing cooperation probability to 0.2.

**Pseudocode:**
```
// Initialize variables
rounds = r; // number of rounds
n_players = n; // number of players
k_factor = k; // multiplication factor
threshold = 1.0; // initial threshold value

// First round (random cooperation)
if (current_round == 1) {
    cooperate_prob = 0.5;
} else {
    // Calculate average payoff per cooperator and defector in previous round
    avg_coop_payoff = calculate_avg_coop_payoff(last_round);
    avg_defect_payoff = calculate_avg_defect_payoff(last_round);

    // Adjust threshold every 5 rounds
    if (current_round % 5 == 0) {
        if (avg_payoff_last_5_rounds > avg_payoff_prev_5_rounds) {
            threshold += 0.1;
        } else {
            threshold -= 0.1;
        }
    }

    // Opponent detection and adaptation
    opponent_defection_rate = detect_opponent_behavior(opponents);
    if (opponent_defection_rate > 0.7) {
        cooperate_prob = 0.2;
    } else {
        cooperate_prob = avg_coop_payoff >= threshold ? 1 : 0;
    }
}

// Make decision based on cooperate probability
if (random_number < cooperate_prob) {
    action = C; // Cooperate
} else {
    action = D; // Defect
}
```
**Edge Cases:**

* **Last Round**: In the last round, always defect (D), as there is no opportunity to gain from future cooperation.
* **Multiple Opponents with High Defection Rates**: If multiple opponents have high defection rates, reduce cooperation probability to 0.1.

**Collective Mindset Alignment:**

The ACD strategy prioritizes collective well-being by:

* Cooperating initially and adjusting based on opponent behavior
* Focusing on average payoff per cooperator, rather than individual payoffs
* Adapting to opponents with high defection rates
* Gradually adjusting the threshold value based on overall performance

This approach promotes a balance between cooperation and self-interest, making it more likely for other players to cooperate as well.
'''

description_COLLECTIVE_364 = '''
**Collective Strategy: Adaptive Cooperate-to-Threshold (ACT)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This sets a neutral tone and allows us to gather information about other players' behaviors.
2. **History-Based Threshold**: Calculate the average cooperation rate of all players in the previous rounds (excluding the current player). If this average is above a threshold `T`, cooperate; otherwise, defect.

**Threshold Calculation:**

`T = k / n * (1 - 1/r)`

where:
- `k` and `n` are game parameters
- `r` is the number of rounds

The intuition behind this threshold is to balance individual payoffs with collective gains. By considering the multiplication factor `k` and the number of players `n`, we adjust our cooperation level according to the potential benefits from public goods.

**Adaptive Adjustments:**

1. **Cooperation Adjustment**: If, in a given round, the average cooperation rate is above `T` but the player's own payoff is lower than expected (i.e., other players defected more often), decrease the cooperation probability by 0.1 for the next round.
2. **Defection Adjustment**: Conversely, if the average cooperation rate is below `T` but the player's own payoff is higher than expected (i.e., other players cooperated more often), increase the cooperation probability by 0.1 for the next round.

**Handling Edge Cases:**

1. **First Round**: Cooperate with probability 0.5, as mentioned earlier.
2. **Last Round**: Defect if `r` is odd; cooperate if `r` is even. This ensures a balanced approach in the final round.
3. **Early Rounds**: If fewer than three rounds have been played, use the initial cooperation probability (0.5).

**Pseudocode:**
```markdown
function ACT(n, k, r, history):
  # Initialize variables
  T = k / n * (1 - 1/r)
  cooperate_prob = 0.5

  for round in range(1, r+1):
    if round == 1:
      # First round: random cooperation
      action = Cooperate with probability 0.5
    else:
      # Calculate average cooperation rate from previous rounds
      avg_coop_rate = sum(history[:round-1]) / (n * (round - 1))

      if avg_coop_rate >= T:
        # Above threshold: cooperate
        action = C
      else:
        # Below threshold: defect
        action = D

    # Adaptive adjustments
    if round > 2:
      if action == C and history[round-2] < expected_payoff:
        cooperate_prob -= 0.1
      elif action == D and history[round-2] > expected_payoff:
        cooperate_prob += 0.1

  return action
```
This strategy balances individual payoffs with collective gains while adapting to the dynamics of the game. By considering the game parameters and history, ACT encourages cooperation when beneficial and defects when necessary, making it a robust competitor in the tournament.
'''

description_COLLECTIVE_365 = '''
I propose a collective strategy called "Adaptive Collective Optimism" (ACO). ACO balances individual self-interest with a willingness to contribute to the public good, while adapting to the behavior of others.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally.
2. **Contribution Threshold**: For rounds 2 to r-1:
	* Cooperate (C) if the average number of cooperators in previous rounds is above a threshold θ = k/n.
	* Defect (D) otherwise.
3. **Adaptive Adjustment**: Update the contribution threshold θ after each round based on the number of cooperators in that round.
	* If the number of cooperators increases, decrease θ by a small amount δ (e.g., 0.05).
	* If the number of cooperators decreases, increase θ by δ.
4. **Final Round**: In the last round (r), defect (D) unconditionally.

**Pseudocode:**
```python
def adaptive_collective_optimism(n, k, r):
    theta = k / n  # initial contribution threshold
    delta = 0.05   # adjustment step size

    for t in range(1, r+1):
        if t == 1:
            action = 'C'  # cooperate in the first round
        elif t < r:
            num_cooperators_prev_round = count_cooperators(t-1)
            avg_cooperators_prev_rounds = sum(num_cooperators_prev_round) / (t - 1)
            if avg_cooperators_prev_rounds >= theta:
                action = 'C'
            else:
                action = 'D'
            # adjust contribution threshold
            if num_cooperators_prev_round > theta * n:
                theta -= delta
            elif num_cooperators_prev_round < theta * n:
                theta += delta
        else:  # last round
            action = 'D'

        return action

def count_cooperators(t):
    # return the number of cooperators in previous rounds (up to t)
```
**Rationale:**

1. Initial cooperation encourages others to cooperate and sets a positive tone for the game.
2. The contribution threshold θ balances individual self-interest with the desire to contribute to the public good. By adapting θ based on the behavior of others, ACO becomes more cooperative when others are cooperating and less cooperative when others are defecting.
3. The adaptive adjustment mechanism allows ACO to respond to changes in the game environment and opponent strategies.
4. Defecting in the last round ensures that ACO does not contribute unnecessarily, as there is no future benefit from cooperation.

**Collective Mindset:**
ACO aligns with a collective mindset by:

1. Initially cooperating to establish trust and encourage others to cooperate.
2. Adapting to the behavior of others, demonstrating a willingness to contribute when others are contributing.
3. Balancing individual self-interest with the desire to contribute to the public good.

By following these decision rules, ACO should perform well in a tournament against independent strategies developed by other AI systems, while promoting a collective and cooperative approach to the game.
'''

description_COLLECTIVE_366 = '''
To design a collective strategy for the N-Player Public Goods Game that is adaptive and robust to various opponent behaviors, we'll create a strategy that balances cooperation with cautious adaptation based on game history. This approach aims to maximize payoffs while encouraging group cooperation.

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Rounds:** In the first round, play C (Cooperate). This establishes an initial cooperative stance and encourages others to do the same.
   
2. **Post-Initial Rounds:**
   - If in the previous round, more than half of the players cooperated (`Σc_j > n/2`), play C. This reinforces successful cooperation.
   - If exactly half or fewer than half of the players cooperated, play D (Defect) with a probability `p`. The value of `p` is determined by the formula:
     ```
     p = 1 - ((k/n) * Σc_j / n)
     ```
     This calculation adjusts your defection probability based on the previous round's cooperation level and the game parameters. It increases the likelihood of defecting when fewer players cooperated, but also accounts for the potential benefits of cooperation through `k/n`.

3. **Adaptive Adjustment:** After each round, assess the total payoff from the last round (`π_i`) compared to your expected payoff if you had defected (`1 + (k/n) * Σc_j`). If your actual payoff is less than this threshold for two consecutive rounds, adjust your strategy by increasing `p` slightly (e.g., by 0.05) in the next round. This ensures adaptability when facing consistent defection from others.

4. **Final Rounds:** For the last three rounds (`r-2`, `r-1`, and `r`), revert to playing C if your total payoff up to that point is higher than the average total payoff of all previous rounds multiplied by `(k/n)`. Otherwise, play D with a probability based on the most recent assessment of cooperation levels.

**Pseudocode for ACO Strategy:**

```markdown
# Initialize variables and parameters
n = # of players
k = multiplication factor
r = # of rounds
history = []  # Store actions and payoffs

def adaptive_collective_optimism(history, n, k, r):
    if len(history) == 0:  # First round
        return "C"
    
    prev_round_coops = sum([1 for action in history[-1] if action == "C"])
    
    if prev_round_coops > n / 2:
        return "C"
    else:
        p_defect = 1 - ((k/n) * prev_round_coops / n)
        
        # Adjust probability based on recent payoffs
        if len(history) >= 3 and history[-1][0] < 1 + (k/n) * prev_round_coops and history[-2][0] < 1 + (k/n) * sum([1 for action in history[-3] if action == "C"]):
            p_defect += 0.05
        
        # Final rounds adjustment
        if len(history) >= r - 2:
            avg_payoff = sum([payoff[0] for payoff in history[:-2]]) / (r - 2)
            if sum([payoff[0] for payoff in history]) > avg_payoff * (k/n):
                return "C"
        
        # Decide based on adjusted probability
        import random
        if random.random() < p_defect:
            return "D"
        else:
            return "C"

# Example usage
for round in range(r):
    action = adaptive_collective_optimism(history, n, k, r)
    history.append([payoff, action])  # Update history with payoff and action
```

This strategy, ACO, aims to balance cooperation with adaptability to the actions of other players. By adjusting its cooperative stance based on observed behavior and payoffs, it seeks to maximize collective benefits while protecting against exploitation by defectors.
'''

description_COLLECTIVE_367 = '''
**Collective Strategy: Adaptive Tit-for-Tat with Community Awareness**

Our collective strategy, "ATCA" (Adaptive Tit-for-Tat with Community Awareness), is designed to balance individual self-interest with community well-being in the N-Player Public Goods Game.

**Decision Rules:**

1. **Initial Round**: In the first round, we cooperate (C) to encourage cooperation and establish a positive tone.
2. **General Rule**: For each subsequent round t:
	* Observe the total number of cooperators (c_total) in the previous round (t-1).
	* Calculate the "cooperation ratio" (CR): CR = c_total / n, where n is the number of players.
	* If CR ≥ k/n (i.e., the cooperation level exceeds the multiplication factor), cooperate (C). Otherwise, defect (D).
3. **Punishment Mechanism**: To prevent exploitation and maintain cooperation, implement a "punishment" mechanism:
	+ Monitor the opponent's actions in previous rounds.
	+ If an opponent defects (D) when CR ≥ k/n, retaliate with defection (D) for 1-2 rounds to discourage future exploitation.

**Edge Cases:**

* **Last Round**: In the final round, we always cooperate (C), as there is no risk of retaliation or future games.
* **Only One Player Cooperates**: If only one player cooperated in the previous round, and that player was not us, we defect (D) to avoid being taken advantage of.

**Community Awareness:**

1. **Monitor Cooperation Levels**: Continuously track the cooperation ratio (CR) and adjust our actions accordingly.
2. **Adapt to Changing Environments**: As the game progresses, update our decision rules based on observed cooperation levels and opponent behaviors.

Pseudocode:
```python
def ATCA(n, k, history):
    # Initialize variables
    CR = 0
    previous_coop_ratio = 0

    for t in range(1, r+1):  # Iterate over rounds
        if t == 1:  # Initial round
            return C

        c_total = sum(history[t-1])  # Total cooperators last round
        CR = c_total / n

        if CR >= k/n:
            return C
        else:
            # Punishment mechanism
            for opponent in history[t-1]:
                if opponent == D and previous_coop_ratio >= k/n:
                    return D for 1-2 rounds

            return D

        previous_coop_ratio = CR
```
By implementing ATCA, we balance individual self-interest with community awareness, promoting cooperation while adapting to changing environments and preventing exploitation.
'''

description_COLLECTIVE_368 = '''
**Collective Strategy: Adaptive Tit-for-Tat with Public Goods Sensitivity (ATTPS)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to encourage cooperation and gather information about opponents' behavior.
2. **Subsequent Rounds:** Observe the total number of cooperators in the previous round (`total_cooperators_prev`) and calculate the public goods share (`public_goods_share = (k/n) * total_cooperators_prev`).
3. **Cooperation Threshold:** If `public_goods_share > 0.5`, cooperate (C). Otherwise, defect (D).

Pseudocode:
```
def ATTPS(n, k, history):
    if len(history) == 0:  # First round
        return C

    total_cooperators_prev = sum(1 for action in history[-1] if action == C)
    public_goods_share = (k/n) * total_cooperators_prev

    if public_goods_share > 0.5:
        return C
    else:
        return D
```
**Edge Cases:**

* **Last Round:** Since the game is repeated, there is no specific last round strategy. The ATTPS decision rule applies equally to all rounds.
* **Single Opponent:** If `n == 2`, the strategy defaults to a simple Tit-for-Tat (TFT) approach: cooperate on the first move and mirror the opponent's previous action.

**Collective Mindset Alignment:**

ATTPS prioritizes cooperation when the public goods share is significant, indicating that enough opponents are contributing. This encourages collective cooperation and maximizes overall payoffs. By adapting to the observed behavior of opponents, ATTPS demonstrates a willingness to cooperate while also protecting against exploitation by defecting when necessary.

**Robustness:**

ATTPS is designed to be robust against various opponent behaviors:

* **Pure Cooperators:** ATTPS will cooperate with pure cooperators, maximizing mutual payoffs.
* **Pure Defectors:** ATTPS will defect against pure defectors, minimizing losses.
* **Mixed Strategies:** ATTPS will adapt to mixed strategies, adjusting cooperation levels based on observed behavior.

By balancing individual self-interest with collective considerations, ATTPS provides a robust and adaptive strategy for the N-Player Public Goods Game.
'''

description_COLLECTIVE_369 = '''
**Collective Strategy: Adaptive Cooperation**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of 0.5. This initial randomness helps to avoid predictable behavior and allows us to observe the opponents' reactions.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is greater than or equal to (n+1)/2, Cooperate (C). This threshold encourages cooperation when it's likely to be reciprocated.
	* Otherwise, Defect (D) with a probability of 0.7, and Cooperate (C) with a probability of 0.3. This mixed strategy allows us to adapt to changing circumstances while maintaining some level of cooperation.

**Edge Cases:**

1. **Last Round (t=r):** Cooperate (C). Since there's no future opportunity for reciprocity, cooperating in the last round can help maintain a positive reputation and potentially influence opponents' behavior.
2. **Tie-breaking:** In cases where the number of cooperators is exactly equal to (n+1)/2, Defect (D).

**Adaptive Elements:**

1. **Opponent Monitoring:** Track the number of cooperators in each round to inform our decision-making process.
2. **Exploration-Exploitation Trade-off:** Balance between exploiting cooperative opportunities and exploring potential benefits from defecting.

**Collective Mindset Alignment:**

Our strategy prioritizes cooperation when it's likely to be reciprocated, while maintaining a degree of adaptability and robustness against various opponent behaviors. By initially cooperating with a moderate probability and adjusting our behavior based on the opponents' actions, we demonstrate a willingness to cooperate while protecting ourselves from exploitation.

**Pseudocode:**

```
  // Initialize variables
  int n; // number of players
  float k; // multiplication factor
  int r; // number of rounds
  int t; // current round
  bool[] history; // store opponent actions for previous rounds

  // Initial Round (t=1)
  if (t == 1) {
    cooperate_with_probability(0.5);
  }

  // Subsequent Rounds (t>1)
  else {
    int num_cooperators = count_cooperators(history[t-1]);
    if (num_cooperators >= (n+1)/2) {
      Cooperate();
    } else {
      Defect_with_probability(0.7);
      Cooperate_with_probability(0.3);
    }
  }

  // Last Round (t=r)
  if (t == r) {
    Cooperate();
  }

  // Update history
  update_history(opponent_actions[t]);
```

This strategy balances cooperation and self-interest, making it a robust candidate for success in the tournament against various opponent strategies.
'''

description_COLLECTIVE_370 = '''
**Collective Strategy: Adaptive Cooperation with Retaliation (ACR)**

The ACR strategy aims to balance cooperation and defection to maximize collective payoff while being robust to various opponent behaviors.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone.
2. **Subsequent Rounds**:
	* If the total number of cooperators in the previous round is above a certain threshold (`threshold_coop`), cooperate (C).
	* Otherwise, defect (D).

**Threshold Calculation**

`threshold_coop` = (n * k) / (2 * n)

This threshold balances the benefits of cooperation and defection. If more players are cooperating than this threshold, it's beneficial to continue cooperating.

3. **Retaliation Mechanism**:
	* Monitor the number of times a player has defected in the last `retaliation_window` rounds (`defections_count`).
	* If `defections_count` exceeds a certain threshold (`retaliation_threshold`), defect (D) for the next `retaliation_rounds` rounds.

**Edge Cases**

1. **Last Round**: Cooperate (C) in the last round to maximize collective payoff, as there's no future retaliation.
2. **Tie Breaking**: In case of a tie in the threshold calculation, cooperate (C).

**Pseudocode**
```python
def ACR(n, k, r):
    threshold_coop = (n * k) / (2 * n)
    retaliation_window = 3
    retaliation_threshold = 2
    retaliation_rounds = 2

    history = []

    for round in range(r):
        if round == 0:
            action = 'C'
        else:
            total_cooperators = sum(1 for h in history[-retaliation_window:] if h['action'] == 'C')
            if total_cooperators >= threshold_coop * n:
                action = 'C'
            elif defections_count(history, retaliation_window) > retaliation_threshold:
                action = 'D'
                retaliate_for(retaliation_rounds)
            else:
                action = 'D'

        history.append({'action': action})

    return history

def defections_count(history, window):
    count = 0
    for h in history[-window:]:
        if h['action'] == 'D':
            count += 1
    return count

def retaliate_for(rounds):
    # defect for the next 'rounds' rounds
    pass
```
This strategy adapts to the group's behavior, retaliating against excessive defection while maintaining a cooperative tone. By balancing cooperation and defection, ACR aims to achieve a high collective payoff in a wide range of scenarios.
'''

description_COLLECTIVE_371 = '''
**Collective Strategy: "Adaptive Cooperation with Gradual Concession" (ACGC)**

**Overview**
ACGC is a collective strategy that balances cooperation and defection to maximize overall payoffs in the N-Player Public Goods Game. It adapts to the game's history, gradually conceding to opponents' behaviors while maintaining a strong cooperative foundation.

**Decision Rules**

1. **First Round**: Cooperate (C) to establish a positive tone and encourage others to cooperate.
2. **Subsequent Rounds**: Evaluate the previous round's outcome using the following criteria:
	* If the number of cooperators is greater than or equal to n/2, cooperate (C).
	* If the number of cooperators is less than n/2 but greater than 0, cooperate with probability p = (k/n) \* (number of cooperators in previous round). This introduces a gradual concession mechanism.
	* If no one cooperated in the previous round, defect (D).
3. **Last Round**: Cooperate if the total payoff from cooperating is expected to be higher than defecting, based on the game's history.

**Pseudocode**
```
Initialize:
  - cooperation_threshold = n/2
  - concession_probability = 0

For each round t:
  - Observe previous round's actions and payoffs
  - Calculate number of cooperators in previous round (num_coop)
  
  If t == 1:  // First Round
    action[t] = C
    
  Else:
    If num_coop >= cooperation_threshold:
      action[t] = C
    Else if num_coop > 0:
      concession_probability = (k/n) * num_coop
      action[t] = C with probability concession_probability, else D
    Else:  // No one cooperated in previous round
      action[t] = D
    
    If t == r:  // Last Round
      Calculate expected payoff from cooperating and defecting based on game history
      Choose action that yields the higher payoff
```
**Collective Mindset**
ACGC aligns with a collective mindset by:

1. Cooperating in the first round to encourage cooperation.
2. Gradually conceding to opponents' behaviors, allowing for adaptation to different strategies.
3. Prioritizing cooperation when the number of cooperators is sufficient (n/2 or more), promoting collective success.

**Robustness**
ACGC is designed to be robust against various opponent behaviors:

1. **Cooperative opponents**: ACGC will cooperate and reinforce mutual cooperation.
2. **Defective opponents**: ACGC will gradually concede, allowing for adaptation to exploitative strategies.
3. **Mixed-strategy opponents**: ACGC's probabilistic concession mechanism enables it to adapt to uncertain environments.

By incorporating adaptive cooperation and gradual concession, ACGC balances individual self-interest with collective success, making it a robust strategy in the N-Player Public Goods Game.
'''

description_COLLECTIVE_372 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual payoffs with collective welfare by adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and gather information about opponents' strategies.
2. **Contribution Threshold**: Calculate a contribution threshold `θ` based on the average cooperation rate of all players in the previous rounds. Specifically, `θ = (k/n) * (avg_cooperation_rate + 0.1)`. This threshold adjusts to the game's history and encourages cooperation when others contribute.
3. **Cooperate if**: If the total contributions in the previous round exceed `n * θ`, cooperate (C). Otherwise, defect (D).
4. **Punish Defectors**: If an opponent defected in the previous round while you cooperated, and your payoff was lower than theirs, increase `θ` by 0.1 for that opponent. This rule helps maintain cooperation by punishing free-riding behavior.

**Edge Cases:**

* **Last Round**: Cooperate (C) in the last round if at least half of the players cooperated in the previous round. Otherwise, defect (D).
* **Tie Breaker**: In case of a tie between cooperating and defecting based on `θ`, cooperate (C).

**Collective Mindset:**

The ACT strategy is designed to align with the collective mindset by:

1. Encouraging cooperation when others contribute.
2. Punishing free-riding behavior.
3. Adapting to the game's history to maintain a balance between individual payoffs and collective welfare.

Pseudocode:
```
Function ACT_Strategy(current_round, previous_actions, previous_payoffs):
  if current_round == 1:
    return COOPERATE
  else:
    avg_cooperation_rate = calculate_avg_cooperation_rate(previous_actions)
    θ = (k/n) * (avg_cooperation_rate + 0.1)
    
    total_contributions = count_contributions(previous_actions[-1])
    if total_contributions >= n * θ:
      return COOPERATE
    else:
      opponent_defections = []
      for i, action in enumerate(previous_actions[-2]):
        if action == DEFECT and previous_payoffs[i] > previous_payoffs[current_player]:
          opponent_defections.append(i)
      
      if opponent_defections:
        θ += 0.1 * len(opponent_defections) / n
      
      return COOPERATE if total_contributions >= n * θ else DEFECT

Function calculate_avg_cooperation_rate(previous_actions):
  cooperation_count = sum(1 for actions in previous_actions if actions.count(COOPERATE) > 0)
  return cooperation_count / len(previous_actions)

Function count_contributions(actions):
  return sum(1 for action in actions if action == COOPERATE)
```
This strategy will adapt to various opponent behaviors and game histories, seeking a balance between individual payoffs and collective welfare.
'''

description_COLLECTIVE_373 = '''
**Collective Strategy: "Adaptive Cooperation with Gradual Forgiveness" (ACGF)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage cooperation and establish a positive tone.
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust accordingly:
	* If the total number of cooperators is above a certain threshold (`cooperation_threshold = n * 0.6`), cooperate (C). This encourages continued cooperation when it seems beneficial to the group.
	* Otherwise, if the total payoff from the previous round is greater than or equal to the private payoff from defecting (1), cooperate (C) with a probability proportional to the total payoff (`cooperation_prob = π_i,t-1 / (k * n)`). This allows for some exploration and adaptation to changing group dynamics.
	* In all other cases, defect (D).
3. **Forgiveness Mechanism:** Implement a gradual forgiveness mechanism to re-encourage cooperation after periods of defection:
	* If the number of consecutive rounds with below-threshold cooperation exceeds a certain limit (`consecutive_defection_limit = 3`), temporarily switch to an all-cooperate strategy for a few rounds (`forgiveness_period = 2`). This helps to revive cooperation and avoid prolonged mutual defection.
4. **Endgame:** In the final round, defect (D) if the total payoff from the previous round is greater than or equal to the private payoff from cooperating (0). Otherwise, cooperate (C).

**Pseudocode:**
```python
def ACGF(n, k, r):
  cooperation_threshold = n * 0.6
  consecutive_defection_limit = 3
  forgiveness_period = 2

  for t in range(r):
    if t == 0:
      action = C  # Cooperate in the first round
    else:
      prev_payoff = π_i,t-1
      total_cooperators_prev = Σ(j=1 to n) c_j,t-1
      cooperation_prob = prev_payoff / (k * n)

      if total_cooperators_prev >= cooperation_threshold:
        action = C  # Cooperate when cooperation is high
      elif prev_payoff >= 1:
        action = C with probability cooperation_prob  # Gradual forgiveness and exploration
      else:
        action = D  # Defect otherwise

    if consecutive_defection_limit rounds have passed with below-threshold cooperation:
      for _ in range(forgiveness_period):
        action = C  # Temporarily switch to all-cooperate strategy

  return action
```
**Collective Mindset:** ACGF aims to balance individual self-interest with collective benefits. By adapting to the group's behavior and gradually forgiving past defections, this strategy promotes cooperation while minimizing exploitation.
'''

description_COLLECTIVE_374 = '''
**Collective Strategy: "Adaptive Collective Cooperation" (ACC)**

**Decision Rules:**

1. **First Round:** Cooperate (C) with a probability of 0.5 to gather information about other players' behaviors.
2. **Subsequent Rounds:** Observe the total number of cooperators in the previous round (`total_cooperators_prev`).
	* If `total_cooperators_prev` ≥ n/2, cooperate (C). This encourages continued cooperation when a majority is already cooperating.
	* Otherwise, defect (D) with a probability proportional to the difference between the multiplication factor `k` and the current number of cooperators (`k - total_cooperators_prev`). This incentivizes more players to cooperate when fewer are doing so.
3. **Last Round:** Cooperate (C) if the total payoff from cooperation is expected to be higher than defecting, considering the history of plays.

**Pseudocode:**
```python
def ACC(n, k, r, previous_rounds):
    # First round
    if len(previous_rounds) == 0:
        return random.choice([C, D])  # Cooperate with probability 0.5
    
    # Subsequent rounds
    total_cooperators_prev = sum(1 for action in previous_rounds[-1] if action == C)
    
    if total_cooperators_prev >= n/2:
        return C  # Cooperate when majority is cooperating
    else:
        defect_prob = (k - total_cooperators_prev) / k
        return D if random.random() < defect_prob else C
    
    # Last round
    expected_payoff_C = k * sum(1 for action in previous_rounds[-2] if action == C)
    expected_payoff_D = 1 + (k/n) * total_cooperators_prev
    return C if expected_payoff_C > expected_payoff_D else D
```
**Edge Cases:**

* In the first round, a random decision is made to gather information about other players' behaviors.
* In the last round, the strategy chooses to cooperate or defect based on the expected payoff from cooperation and defection.

**Collective Mindset:**
The ACC strategy prioritizes collective cooperation while adapting to the behavior of other players. By cooperating when a majority is already doing so, it reinforces positive behavior and encourages others to follow suit. When fewer players are cooperating, the strategy defects with a probability proportional to the difference between the multiplication factor `k` and the current number of cooperators, incentivizing more players to cooperate.

By being adaptive and robust to various opponent behaviors, ACC aims to achieve a high collective payoff while promoting cooperation throughout the game.
'''

description_COLLECTIVE_375 = '''
**Collective Strategy: Adaptive Cooperate-and-Punish (ACP)**

The ACP strategy is designed to balance individual self-interest with collective well-being, while adapting to various opponent behaviors and game parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally to establish a cooperative tone.
2. **Reciprocity**: If the number of cooperators in the previous round is greater than or equal to n/2, cooperate (C). Otherwise, defect (D).
3. **Punishment**: If the total payoff for all players in the previous round is less than k × (n-1)/n, defect (D) for one round as a punishment. This helps maintain cooperation by signaling that low payoffs will not be tolerated.
4. **Adaptive Forgiveness**: After punishing, return to reciprocity rule (2) if at least n/2 players cooperated in the previous round.

**Edge Cases:**

1. **Last Round**: In the final round, always cooperate (C), as punishment has no future impact.
2. **Early Rounds**: If fewer than 3 rounds have been played, stick to initial cooperation and reciprocity rules without punishment.

**Collective Mindset Alignment:**

ACP promotes collective well-being by:

1. Encouraging cooperation through reciprocity
2. Punishing low payoffs to prevent exploitation
3. Forgiving past transgressions when cooperation resumes

This strategy adapts to a range of opponent behaviors and game parameters, making it robust for tournament play.

**Pseudocode:**
```
function ACP(n, k, r, history):
  if round == 1:
    return C (Initial Cooperation)
  
  cooperators_last_round = count_cooperators(history[-1])
  total_payoff_last_round = sum_payoffs(history[-1])

  if cooperators_last_round >= n/2:
    return C (Reciprocity)
  
  if total_payoff_last_round < k * (n-1)/n:
    punishment_counter += 1
    if punishment_counter == 1:
      return D (Punishment)
    else:
      punishment_counter = 0
  
  if cooperators_last_round >= n/2 and round > 2:
    return C (Adaptive Forgiveness)

  return D

def count_cooperators(actions):
  # Count number of players who chose C in the given actions
  ...

def sum_payoffs(actions):
  # Calculate total payoff for all players in the given actions
  ...
```
This pseudocode serves as a starting point for implementing the ACP strategy.
'''

description_COLLECTIVE_376 = '''
I propose a collective strategy called "Adaptive Collective Contribute" (ACC) that balances individual self-interest with collective well-being.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Contribution Threshold**: Introduce a threshold parameter `τ` (tau), which is the minimum proportion of cooperators required for an individual to cooperate. Initialize `τ = 0.5`.
3. **Adaptive Adjustment**: After each round, adjust `τ` based on the previous round's outcome:
	* If the total payoff in the previous round was greater than or equal to `(k/n) * n`, increase `τ` by a small increment (e.g., `Δτ = 0.05`). This indicates that cooperation is beneficial, so we become more selective about cooperating.
	* If the total payoff in the previous round was less than `(k/n) * n`, decrease `τ` by a small increment (e.g., `Δτ = -0.05`). This indicates that cooperation is not yielding sufficient benefits, so we become more cautious about cooperating.
4. **Cooperation Decision**: In each subsequent round, cooperate (C) if the number of cooperators in the previous round is greater than or equal to `τ * n`. Otherwise, defect (D).

**Pseudocode:**
```python
# Initialize parameters
n = game_parameters['number_of_players']
k = game_parameters['multiplication_factor']
tau = 0.5  # initial contribution threshold

# First round: Cooperate
def first_round():
    return 'C'

# Subsequent rounds: Adaptive Collective Contribute (ACC)
def acc_round(previous_cooperators):
    global tau
    
    # Adjust tau based on previous round's outcome
    if total_payoff >= (k/n) * n:
        tau += 0.05
    else:
        tau -= 0.05
    
    # Cooperate if threshold is met
    if previous_cooperators >= tau * n:
        return 'C'
    else:
        return 'D'

# Last round handling: No special treatment; use ACC rules as usual.
```
**Edge Cases:**

* **Last Round**: Apply the same ACC decision rule as in other rounds, without any special consideration for the final round.
* **Single Player or Small Group**: In cases with few players (e.g., n = 2), a more cautious approach might be beneficial. However, since we're designing a general strategy for larger groups, we'll stick to the ACC rules.

**Collective Mindset:**

The Adaptive Collective Contribute (ACC) strategy prioritizes collective well-being while adapting to the actions of others. By introducing a contribution threshold `τ`, we balance individual self-interest with the need to maintain cooperation within the group. The adaptive adjustment mechanism ensures that our strategy responds to changes in the game environment and adjusts its level of cooperation accordingly.

This strategy should perform reasonably well against various opponent behaviors, as it balances cooperation with caution and adapts to the game's progression.
'''

description_COLLECTIVE_377 = '''
**Collective Strategy: Adaptive Cooperative Fingerprint (ACF)**

The ACF strategy is designed to balance individual self-interest with collective well-being, adapting to various opponent behaviors while promoting cooperation.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative fingerprint.
2. **Subsequent Rounds**: Observe the previous round's actions and payoffs. Calculate the **Cooperation Quotient (CQ)**:

   CQ = (Number of cooperators in the previous round) / n

3. If CQ ≥ 0.5, cooperate (C). Otherwise, defect (D).
4. **Exception Handling**: If the game is in its last round (t = r), defect (D).

**Pseudocode:**
```
function ACF(n, k, r):
  // Initialize cooperation fingerprint
  if t == 1:
    return C

  // Calculate Cooperation Quotient
  CQ = count_cooperators(t-1) / n

  // Adaptive decision rule
  if CQ >= 0.5:
    return C
  else:
    return D

  // Exception handling for last round
  if t == r:
    return D
```
**Rationale:**

* The initial cooperation establishes a positive fingerprint, encouraging others to cooperate.
* The Cooperation Quotient (CQ) serves as a simple, adaptive metric to gauge the group's cooperativeness. If CQ ≥ 0.5, it indicates a sufficient number of players are cooperating, making it beneficial for the current player to cooperate as well.
* Defecting in the last round ensures individual self-interest is protected when there's no opportunity for future reciprocity.

**Collective Mindset Alignment:**

The ACF strategy aligns with the collective mindset by:

* Encouraging cooperation through an initial cooperative fingerprint
* Adapting to group behavior, rewarding cooperation and punishing defection
* Fostering a culture of mutual benefit through the Cooperation Quotient metric

By being adaptive, robust, and easy to understand, the ACF strategy aims to promote cooperation in a wide range of scenarios, making it an effective collective strategy for the N-Player Public Goods Game.
'''

description_COLLECTIVE_378 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Round**: In the first round, play C (Cooperate) with probability 0.5.
2. **Subsequent Rounds**:
	* If the total number of cooperators in the previous round is greater than or equal to n/2, play C.
	* Otherwise, play D (Defect).
3. **Adaptation Mechanism**: After each round, update the cooperation probability based on the payoffs received in the last two rounds.
	* If the payoff from cooperating (π_C) is greater than the payoff from defecting (π_D), increase the cooperation probability by 0.1 (max 1).
	* If π_C < π_D, decrease the cooperation probability by 0.1 (min 0).

**Edge Cases:**

1. **Last Round**: In the final round, play C if the total number of cooperators in the second-to-last round is greater than or equal to n/2.
2. **Tiebreaker**: If the total number of cooperators is exactly n/2, play C.

**Pseudocode:**
```python
def adaptive_cooperation(n, k, r):
  # Initialize cooperation probability
  p_C = 0.5

  for t in range(r):
    if t == 0:
      # Initial round
      c_i = random.random() < p_C
    else:
      # Subsequent rounds
      total_cooperators_prev = sum(c_j for c_j in prev_round_actions)
      c_i = total_cooperators_prev >= n / 2

    # Play action and observe payoffs
    π_i, _ = play_action(c_i)

    if t > 0:
      # Adaptation mechanism
      π_C_prev, π_D_prev = prev_payoffs
      if π_i > π_D_prev:
        p_C = min(1, p_C + 0.1)
      elif π_i < π_C_prev:
        p_C = max(0, p_C - 0.1)

    # Store previous round's actions and payoffs
    prev_round_actions.append(c_i)
    prev_payoffs = (π_C_prev, π_D_prev)

  return total_payoff
```
**Collective Mindset:**

The Adaptive Cooperation strategy is designed to align with the collective mindset by:

* Encouraging cooperation when a sufficient number of players cooperate.
* Adapting to changes in opponent behavior through the adaptation mechanism.
* Prioritizing cooperation in the final round if the group has cooperated sufficiently.

This strategy aims to balance individual self-interest with collective well-being, promoting cooperation while being robust to a wide range of opponent behaviors.
'''

description_COLLECTIVE_379 = '''
**Collective Strategy: Adaptive Cooperative Balance (ACB)**

The ACB strategy aims to balance individual payoffs with collective well-being by adaptively adjusting cooperation levels based on game history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 50% to encourage initial cooperation and gauge opponent behavior.
2. **History-Dependent Cooperation**: For subsequent rounds, calculate the average number of cooperators in previous rounds (`avg_coop`).
	* If `avg_coop` is above a threshold (`coop_threshold = n / (k + 1)`), cooperate with a probability proportional to `avg_coop`.
	* Otherwise, defect with a probability proportional to `(n - avg_coop) / n`.
3. **Defection Response**: If the number of cooperators in the previous round is below `coop_threshold`, and an opponent defected while you cooperated, increase the probability of defection by 10% for the next round.
4. **Payoff-Based Adaptation**: Monitor individual payoffs and adjust cooperation levels based on relative performance:
	* If your total payoff is above the average payoff of all players, reduce cooperation by 5%.
	* If your total payoff is below the average payoff, increase cooperation by 10%.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate if the average number of cooperators in previous rounds is above `coop_threshold`, and defect otherwise.
2. **Early Game**: For the first three rounds, prioritize exploration over exploitation to gather information about opponent behavior.

**Collective Mindset Alignment:**

The ACB strategy aligns with the collective mindset by:

1. Encouraging initial cooperation to establish a cooperative atmosphere.
2. Adapting cooperation levels based on group performance, promoting a balance between individual and collective well-being.
3. Responding to defections in a way that maintains a stable level of cooperation.

**Pseudocode:**

```python
def ACB(n, k, r):
    # Initialize variables
    avg_coop = 0
    total_payoff = 0
    opponent_defection_response = False

    for t in range(r):
        if t == 0:
            # Initial cooperation (50%)
            action = random.choice([C, D])
        else:
            # History-dependent cooperation
            avg_coop = calculate_average_cooperators(t-1)
            coop_probability = calculate_cooperation_probability(avg_coop, n, k)

            # Defection response
            if opponent_defected_last_round and I_cooperated_last_round:
                opponent_defection_response = True

            action = random.choice([C, D], p=[coop_probability, 1-coop_probability])

        # Payoff-based adaptation
        total_payoff += calculate_payoff(action)
        average_payoff = calculate_average_payoff(t)

        if total_payoff > average_payoff:
            coop_probability -= 0.05
        elif total_payoff < average_payoff:
            coop_probability += 0.10

        # Update opponent_defection_response and action for next round
        if opponent_defected_last_round and I_cooperated_last_round:
            opponent_defection_response = False
```

This strategy is designed to be adaptive, robust, and aligned with the collective mindset, while also being simple enough to be implemented as an algorithm.
'''

description_COLLECTIVE_380 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

ACE is a collective strategy that balances individual self-interest with collective well-being by adaptively responding to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) unconditionally to establish a cooperative foundation.
2. **Early Rounds**: For rounds 2-5, use a "tit-for-tat" approach:
	* If the majority of players (> n/2) cooperated in the previous round, cooperate (C).
	* Otherwise, defect (D).
3. **Mid-Game Adaptation**: After round 5, evaluate the game's history and adjust strategy:
	* Calculate the average cooperation rate (ACR) over the last 3 rounds.
	* If ACR > 0.6, cooperate (C) in the next round.
	* If ACR < 0.4, defect (D) in the next round.
	* If 0.4 ≤ ACR ≤ 0.6, mirror the opponent's previous action (tit-for-tat).
4. **Late-Game Prudence**: In the last 2 rounds, prioritize self-interest:
	* Defect (D) if the total payoff from cooperation in the previous round was less than or equal to the private payoff from defecting.
	* Cooperate (C) otherwise.

**Edge Cases:**

1. **Last Round**: If it's the final round, defect (D) unconditionally, as there are no future consequences.
2. **Tied Cooperation Rate**: In case of a tied ACR in mid-game adaptation, cooperate (C) to maintain a cooperative tone.

**Collective Mindset Alignment:**

ACE aligns with the collective mindset by:

1. Initially cooperating to establish trust and encourage cooperation.
2. Adapting to the game's history to balance individual self-interest with collective well-being.
3. Gradually increasing prudence in late-game rounds to minimize losses while still considering collective benefits.

**Pseudocode:**
```python
def ACE(n, r, k, history):
    # Initialize variables
    cooperation_rate = 0
    previous_round_payoffs = []

    for t in range(1, r+1):
        if t == 1:
            action = "C"  # Cooperate in the first round
        elif t < 5:
            majority_cooperated = sum(history[t-1]) > n/2
            action = "C" if majority_cooperated else "D"
        else:
            cooperation_rate = calculate_average_cooperation_rate(history, t)
            if cooperation_rate > 0.6:
                action = "C"
            elif cooperation_rate < 0.4:
                action = "D"
            else:
                opponent_action = history[t-1][random.randint(0, n-1)]
                action = mirror_opponent(opponent_action)

        # Late-game prudence
        if t > r - 2:
            previous_round_payoff = calculate_payoff(history, t-1)
            private_defect_payoff = 1 + (k/n) * sum([c for c in history[t-1] if c == "D"])
            action = "D" if previous_round_payoff <= private_defect_payoff else "C"

        # Last round
        if t == r:
            action = "D"

    return action

def calculate_average_cooperation_rate(history, t):
    cooperation_sum = sum([1 for actions in history[t-3:t] for action in actions if action == "C"])
    return cooperation_sum / (n * 3)

def mirror_opponent(opponent_action):
    return opponent_action
```
Note that this pseudocode provides a high-level overview of the ACE strategy and may require modifications to fit specific implementation requirements.
'''

description_COLLECTIVE_381 = '''
**Collective Strategy: "Adaptive Cooperative Escalation" (ACE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to establish a cooperative tone and encourage others to follow.
2. **Reciprocal Cooperation**: If the majority of players (> n/2) cooperated in the previous round, cooperate (C) in the current round. This promotes collective cooperation and rewards reciprocal behavior.
3. **Adaptive Escalation**: If fewer than half of the players cooperated in the previous round, but still more than a threshold (θ = 1/n), cooperate (C) with a probability proportional to the number of cooperators in the previous round. Specifically:

`p_cooperate = (number_of_cooperators_previous_round / n) * (k - 1)`
4. **Defection Threshold**: If θ or fewer players cooperated in the previous round, defect (D). This prevents exploitation by defectors and encourages others to cooperate.
5. **Final Round Cooperation**: In the last round (r), cooperate (C) regardless of previous rounds' outcomes. This ensures maximum collective payoff in the final round.

**Pseudocode:**
```python
def ACE(n, k, r, history):
    if current_round == 1:
        return C  # Initial cooperation

    prev_cooperators = count_cooperators(history[-1])
    if prev_cooperators > n/2:
        return C  # Reciprocal cooperation

    theta = 1/n
    if prev_cooperators <= theta:
        return D  # Defection threshold

    p_cooperate = (prev_cooperators / n) * (k - 1)
    if random.random() < p_cooperate:
        return C  # Adaptive escalation
    else:
        return D

    if current_round == r:
        return C  # Final round cooperation
```
**Collective Mindset:**

ACE prioritizes collective cooperation while adapting to the actions of other players. By initially cooperating and reciprocating cooperation, ACE encourages a cooperative atmosphere. The adaptive escalation mechanism rewards cooperation while deterring exploitation. In the final round, ACE ensures maximum collective payoff by cooperating regardless of previous outcomes.

This strategy is robust against various opponent behaviors, as it adapts to the level of cooperation in the game. By not relying on specific coordination mechanisms or norms, ACE can effectively compete against independent strategies in a tournament setting.
'''

description_COLLECTIVE_382 = '''
To design a collective strategy for the N-Player Public Goods Game that is adaptive and robust, we'll employ a approach that balances cooperation with self-interest, while adapting to the observed behavior of others.

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to initiate collective action and encourage others to cooperate.
2. **Subsequent Rounds (t>1):**
	* If the average payoff in the previous round was above a threshold (`avg_payoff_threshold` = 0.75 \* k), Cooperate (C). This indicates that cooperation is beneficial, so continue cooperating.
	* Otherwise, Defect (D) with probability `p_defect` = (1 - `avg_cooperation_rate`) / 2, where `avg_cooperation_rate` is the average rate of cooperation observed in previous rounds. This allows for adaptability to changing circumstances and balances self-interest with collective optimality.
3. **Observation-based Adaptation:** After each round, update `avg_payoff_threshold` based on the minimum payoff obtained by any player in that round. If the minimum payoff decreases, decrease `avg_payoff_threshold` proportionally (e.g., by 10%). This allows ACO to adjust its expectations based on observed outcomes.

**Edge Cases:**

1. **Last Round (t=r):** Cooperate (C) if the average payoff in the previous round was above `avg_payoff_threshold`. Otherwise, Defect (D).
2. **Low Cooperation Rate:** If the average cooperation rate is below 0.25, Defect (D) with probability 1 to avoid exploitation.
3. **High Cooperation Rate:** If the average cooperation rate exceeds 0.75, Cooperate (C) with probability 1 to maintain collective optimality.

**Collective Mindset:**

ACO prioritizes collective action while adapting to observed behavior and outcomes. By cooperating initially and adjusting based on payoffs, ACO promotes a culture of cooperation while safeguarding against exploitation. The strategy balances self-interest with collective optimism, aligning with the game's public goods nature.

Pseudocode:
```
Initialize avg_payoff_threshold = 0.75 * k
avg_cooperation_rate = 0

For each round t:
  If t == 1:
    action = C
  Else:
    avg_payoff_prev_round = calculate_average_payoff(prev_round)
    if avg_payoff_prev_round > avg_payoff_threshold:
      action = C
    else:
      p_defect = (1 - avg_cooperation_rate) / 2
      action = D with probability p_defect

  Update avg_cooperation_rate based on observed cooperation rates
  Update avg_payoff_threshold if necessary (based on minimum payoff in prev_round)

  If t == r: # last round
    if avg_payoff_prev_round > avg_payoff_threshold:
      action = C
    else:
      action = D

Return action (C or D)
```
This strategy should perform well against a variety of opponent behaviors, as it adapts to observed outcomes and balances collective optimality with self-interest.
'''

description_COLLECTIVE_383 = '''
I propose a collective strategy called "Adaptive Collective Cooperation" (ACC). ACC is designed to balance individual self-interest with collective well-being, while being robust against various opponent behaviors.

**Decision Rules:**

1. **Initial Rounds**: In the first few rounds (e.g., 10% of total rounds), play Cooperate (C) with a high probability (e.g., 0.8). This encourages cooperation and allows for initial exploration.
2. **Reciprocity-based Cooperation**: After the initial rounds, switch to a reciprocity-based strategy:
	* If the opponent's cooperation rate in the previous round is above a certain threshold (e.g., 0.5), play C with high probability (e.g., 0.8).
	* Otherwise, play Defect (D) with moderate probability (e.g., 0.4).
3. **Punishment Mechanism**: Implement a punishment mechanism to deter excessive defection:
	* If an opponent defects more than twice in a row, play D for the next two rounds.
4. **Adaptive Adjustment**: Periodically adjust the cooperation threshold and probabilities based on the game's history (e.g., every 10 rounds):
	* Increase the cooperation threshold if the collective payoff is above average.
	* Decrease the cooperation threshold if the collective payoff is below average.

**Edge Cases:**

1. **Last Round**: In the final round, play D to maximize individual payoff, as there are no future rounds to consider.
2. **Tiebreaker**: In case of a tie in the cooperation rate, choose C with a small probability (e.g., 0.1) to avoid coordination failure.

**Collective Mindset:**

ACC aligns with the collective mindset by:

1. Encouraging cooperation through reciprocity and punishment mechanisms.
2. Adapting to the game's history to balance individual self-interest with collective well-being.
3. Avoiding over-reliance on individual payoffs, instead considering the collective payoff.

Pseudocode:
```
ACC Strategy

// Initial Rounds (10% of total rounds)
if round < 0.1 * total_rounds then
    play C with probability 0.8
end if

// Reciprocity-based Cooperation
else if opponent_cooperation_rate > threshold then
    play C with probability 0.8
else
    play D with probability 0.4
end if

// Punishment Mechanism
if opponent_defection_count >= 2 then
    play D for next two rounds
end if

// Adaptive Adjustment (every 10 rounds)
if round % 10 == 0 then
    adjust cooperation threshold and probabilities based on game history
end if

// Last Round
if round == total_rounds then
    play D
end if

// Tiebreaker
if opponent_cooperation_rate == threshold then
    play C with probability 0.1
end if
```
ACC is designed to be robust against various opponent behaviors while promoting collective cooperation and adaptability.
'''

description_COLLECTIVE_384 = '''
**Collective Strategy: Adaptive Cooperative Governance (ACG)**

**Decision Rules:**

1. **First Round:** Cooperate (C) unconditionally.
2. **Subsequent Rounds:** Implement a Tit-for-Tat-like mechanism with a twist:
	* Calculate the average cooperation rate of all players in the previous round (excluding yourself).
	* If this average is above a threshold θ (e.g., 0.5), Cooperate (C). Otherwise, Defect (D).
	* However, if your own payoff from the previous round was below a certain percentage (e.g., 25%) of the maximum possible payoff, Defect (D) regardless of the average cooperation rate.
3. **Last Round:** Cooperate (C) unconditionally.

**Pseudocode:**
```markdown
def ACG(n, k, r):
    # Initialize variables
    avg_coop_rate = None
    prev_payoff = None

    for round in range(1, r+1):
        if round == 1:
            action = C
        else:
            # Calculate average cooperation rate from previous round
            avg_coop_rate = sum(c_i for c_i in get_previous_round_actions()) / (n - 1)
            
            # Check threshold and own payoff
            if avg_coop_rate > θ or prev_payoff < 0.25 * max_payoff:
                action = C
            else:
                action = D
        
        # Update variables
        prev_payoff = get_current_payoff()
        
        # Perform action
        take_action(action)
```
**Rationale:**

* Cooperating in the first round sets a positive tone and encourages others to reciprocate.
* The Tit-for-Tat-like mechanism adapts to the group's overall cooperation level, promoting collective cooperation while allowing for flexibility.
* The twist accounts for individual payoffs, ensuring that players don't get stuck in an unprofitable cooperative equilibrium. If a player's payoff is too low, they defect to signal dissatisfaction and potentially trigger a change in others' behavior.
* Cooperating in the last round reinforces the collective mindset, as there's no incentive to defect when there are no future rounds.

**Edge Cases:**

* **First Round:** Unconditional cooperation promotes a cooperative atmosphere from the start.
* **Last Round:** Unconditional cooperation maintains a collective focus and prevents potential endgame exploitation.
* **Opponent Defection:** ACG will eventually defect if opponents refuse to cooperate, ensuring individual payoffs don't suffer unduly.

**Collective Mindset:**

ACG prioritizes collective cooperation while maintaining individual adaptability. By responding to the group's overall cooperation level and accounting for personal payoffs, this strategy promotes a balanced approach that benefits both the individual and the collective.
'''

description_COLLECTIVE_385 = '''
To design a collective strategy for this N-Player Public Goods Game that is adaptive and robust, I propose the "Adaptive Collective Cooperation" (ACC) strategy. This strategy balances cooperation with cautious defection based on observed behavior.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C). In the initial rounds, ACC starts by cooperating to signal a willingness to contribute and encourage others to cooperate.
2. **Observation Phase (t > 2):** Observe the average cooperation rate of all players in the previous round. Let `avg_coop_prev` be this value.
3. **Cooperation Threshold:** Define a cooperation threshold (`coop_threshold`) as a function of the game parameters: `coop_threshold = (k / n) * 0.5`. This threshold is used to determine when to cooperate or defect based on observed behavior.
4. **Decision Logic:** 
   - If `avg_coop_prev` ≥ `coop_threshold`, Cooperate (C). The group's cooperation rate is high enough, so continue cooperating.
   - If `avg_coop_prev` < `coop_threshold`, Defect (D) with probability `(1 - avg_coop_prev)`. This introduces a level of caution, as the likelihood of defection increases when observed cooperation rates are low.

**Pseudocode for Decision Logic:**
```python
def decide_action(avg_coop_prev):
    coop_threshold = (k / n) * 0.5
    
    if avg_coop_prev >= coop_threshold:
        return "C"  # Cooperate
    else:
        defect_prob = 1 - avg_coop_prev
        if random.random() < defect_prob:
            return "D"  # Defect with calculated probability
        else:
            return "C"  # Otherwise, cooperate

# Calculate average cooperation rate from previous round
avg_coop_prev = sum(prev_round_actions == "C") / n

action = decide_action(avg_coop_prev)
```

**Handling Edge Cases:**

- **First Round (t=1):** Always Cooperate.
- **Last Round (t=r):** Apply the standard decision logic. This ensures consistency in behavior, even in the final round.

**Collective Mindset Alignment:**
ACC is designed to adapt to the collective behavior of all players. By adjusting its cooperation rate based on observed average cooperation rates, ACC promotes a balance between individual self-interest and group welfare. It encourages others to cooperate by maintaining a high cooperation rate when others do the same and gradually introduces defection as a cautionary measure when faced with low cooperation rates.

**Robustness:**
ACC's adaptive nature makes it robust against a wide range of opponent behaviors, including pure cooperators, pure defectors, and various mixes of strategies. By not relying on specific coordination mechanisms or shared norms, ACC can effectively participate in a tournament setting without assumptions about other players' strategies.
'''

description_COLLECTIVE_386 = '''
I propose a collective strategy called "Adaptive Collective Optimism" (ACO) for the N-Player Public Goods Game. ACO is designed to balance individual self-interest with collective welfare, adapting to various opponent behaviors while promoting cooperation.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally to establish a cooperative tone.
2. **Reciprocity**: Cooperate if at least k/n players cooperated in the previous round. This encourages reciprocity and reinforces cooperation when it's beneficial for the group.
3. **Punishment**: Defect (D) if fewer than k/n players cooperated in the previous round. This punishes non-cooperation and incentivizes others to cooperate.
4. **Adaptive Adjustment**: If, in the previous round, exactly k/n players cooperated, adjust your strategy based on the current round's total payoff:
	* If your individual payoff is higher than the average payoff of all players, cooperate (C) to maintain the beneficial situation.
	* Otherwise, defect (D) to avoid exploitation.

**Edge Cases:**

1. **First Round**: Cooperate unconditionally (as mentioned earlier).
2. **Last Round**: Defect if fewer than k/n players cooperated in the second-to-last round; otherwise, cooperate.
3. **Ties**: In cases where exactly k/n players cooperated and your individual payoff is equal to the average payoff, cooperate.

**Collective Mindset:**

ACO aligns with the collective mindset by:

1. Encouraging cooperation when it benefits the group (Reciprocity).
2. Punishing non-cooperation to maintain a fair environment.
3. Adapting to changing circumstances to optimize individual and collective payoffs.
4. Fostering a cooperative tone from the start.

**Pseudocode:**
```
function ACO(n, k, r):
  // Initialize cooperation flag
  cooperate = True

  for round in range(1, r+1):
    if round == 1:
      # Initial Cooperation
      action = C
    else:
      # Reciprocity and Punishment
      num_cooperators_prev_round = count_cooperators(prev_round)
      if num_cooperators_prev_round >= k/n:
        cooperate = True
      elif num_cooperators_prev_round < k/n:
        cooperate = False

      # Adaptive Adjustment
      if num_cooperators_prev_round == k/n:
        individual_payoff = calculate_payoff(prev_round, my_action)
        average_payoff = calculate_average_payoff(prev_round)
        if individual_payoff > average_payoff:
          cooperate = True
        else:
          cooperate = False

    # Edge Cases
    if round == r:  // Last Round
      if num_cooperators_prev_round < k/n:
        cooperate = False

    action = C if cooperate else D
    take_action(action)
```
ACO is a robust, adaptive strategy that balances individual self-interest with collective welfare. By incorporating reciprocity, punishment, and adaptive adjustment, ACO encourages cooperation while protecting against exploitation.
'''

description_COLLECTIVE_387 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and focuses on maximizing overall payoffs:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 0.5. This initial optimism encourages cooperation and allows us to gauge opponents' behaviors.
2. **Past Performance Matters**: For subsequent rounds, calculate the average payoff per player in the previous round (π_avg). If π_avg > 1, it indicates that cooperation was beneficial; otherwise, defection dominated.
3. **Adaptive Threshold**: Introduce an adaptive threshold τ ∈ [0, 1] to adjust our cooperation probability based on past performance:
	* Initialize τ = 0.5
	* Update τ after each round using the following rules:
		+ If π_avg > 1, increment τ by a small value (e.g., 0.05): τ ← min(τ + 0.05, 1)
		+ If π_avg ≤ 1, decrement τ by a small value: τ ← max(τ - 0.05, 0)
4. **Cooperation Probability**: In each round, cooperate (C) with probability equal to the current threshold value τ.
5. **Defection Trigger**: If, in the previous round, more than half of the players defected (D), defect in the current round.

**Edge Cases:**

1. **Last Round**: In the final round, always defect (D).
2. **First Round with n=2 Players**: When there are only two players, cooperate (C) in the first round to encourage mutual cooperation.
3. **Consecutive Defection**: If a player has defected for three consecutive rounds, reset their internal state and cooperate (C) in the next round.

**Collective Mindset:**

1. **Foster Cooperation**: By initially cooperating and adapting our threshold based on past performance, we create an environment conducive to cooperation.
2. **Respect Opponents' Strategies**: ACO acknowledges that opponents may employ diverse strategies and adapts to their behaviors without relying on explicit coordination mechanisms.

**Pseudocode:**

```python
def adaptive_collective_optimism(n, k, r):
    τ = 0.5  # Adaptive threshold
    π_avg = None  # Average payoff per player

    for round in range(r):
        if round == 0:
            action = random.choice(['C', 'D'])  # Initial cooperation with probability 0.5
        else:
            if π_avg > 1:
                τ = min(τ + 0.05, 1)
            elif π_avg <= 1:
                τ = max(τ - 0.05, 0)

            cooperate_prob = τ
            action = 'C' if random.random() < cooperate_prob else 'D'

        # Update internal state and calculate payoffs...
```

ACO balances individual self-interest with collective benefits by adapting to the game's dynamics and opponents' behaviors. By fostering cooperation while being robust to a wide range of strategies, ACO is well-suited for a tournament environment where diverse strategies will be employed.
'''

description_COLLECTIVE_388 = '''
**Collective Strategy: "Adaptive Cooperation"**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate with probability p = 0.5 in each of the first three rounds to gather information about opponents' behavior.
2. **Contribution-Based Cooperation (Rounds 4-r)**:
	* Calculate the average contribution rate of all players in the previous round: avg_contrib_rate = Σ(j=1 to n) c_j / n
	* If avg_contrib_rate ≥ k/n, cooperate with probability p = max(0.5, avg_contrib_rate)
	* Otherwise, defect with probability p = 1 - min(0.5, (k/n) / avg_contrib_rate)
3. **Punishment for Defection**:
	* If the average contribution rate drops below k/n in a round and at least one player defected, retaliate by defecting with probability p = 0.8 in the next round.
4. **Forgiveness**:
	* After a round of retaliation, reset cooperation probability to p = max(0.5, avg_contrib_rate) if the average contribution rate increases.

**Edge Cases:**

1. **First Round**: Cooperate with probability p = 0.5
2. **Last Round**: Defect (as there is no opportunity for future reciprocity)
3. **Ties in Contribution Rate**: In case of a tie, cooperate with probability p = 0.5

**Collective Mindset:**

The Adaptive Cooperation strategy aims to promote collective well-being by:

1. Encouraging cooperation when contributions are high
2. Punishing defection when contributions drop
3. Forgiving and adapting to changes in opponents' behavior

This strategy is robust to a wide range of opponent behaviors, as it adapts to the game's history and responds to changes in contribution rates.

**Pseudocode:**
```markdown
Initialize variables:
avg_contrib_rate = 0
punish_defect = False

For each round (t):
  If t ≤ 3:  // Initial exploration
    Cooperate with probability p = 0.5
  Else:  // Contribution-based cooperation
    Calculate avg_contrib_rate from previous round
    If avg_contrib_rate ≥ k/n:
      Cooperate with probability p = max(0.5, avg_contrib_rate)
    Else:
      Defect with probability p = 1 - min(0.5, (k/n) / avg_contrib_rate)

  If punish_defect and t > 1:  // Punishment for defection
    Defect with probability p = 0.8

  Update punish_defect flag if average contribution rate drops below k/n

  If last round:
    Defect

  Update variables for next round
```
Note that this pseudocode is a simplified representation of the strategy, and implementation details may vary depending on the specific programming language and requirements.
'''

description_COLLECTIVE_389 = '''
Here's a collective strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Collective Contribute (ACC)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to contribute.
2. **Contribution Threshold**: Set a contribution threshold, `threshold`, based on the number of players and the multiplication factor:
   - `threshold` = ceil((n / k) * 0.5)
3. **Current Round Contribution**: Calculate the total contributions in the current round (including your own):
   - `total_contributions` = sum(c_j for all j != i) + c_i
4. **Cooperation Condition**: Cooperate if:
   - `total_contributions` >= `threshold`
   - or, if the previous round's total payoff was higher when cooperating than defecting.
5. **Defection Condition**: Defect otherwise.

**Pseudocode (simplified):**
```python
def ACC(n, k, r):
  threshold = ceil((n / k) * 0.5)
  
  for t in range(r):
    if t == 0:
      # Initial Round: Cooperate
      action = C
    else:
      total_contributions = sum(c_j for all j != i) + c_i
      if total_contributions >= threshold or (π_i,t-1 > π_i,t-1 when cooperating):
        action = C
      else:
        action = D
    
    # Play action and observe payoffs
```
**Edge Cases:**

* **Last Round**: Since the game is repeated, we ignore the last round's outcome. The ACC strategy focuses on establishing a cooperative atmosphere throughout the game.
* **Tiebreaker**: In case of a tie in total contributions or payoff comparison, default to cooperation.

**Collective Mindset:**
The ACC strategy prioritizes cooperation when there are sufficient contributors, promoting a collective benefit over individual gains. By adapting to the current round's contributions and previous payoffs, it aims to maintain a cooperative atmosphere while discouraging free-riding.

This strategy balances individual self-interest with collective well-being, making it robust against various opponent behaviors. Its adaptability ensures that it can respond effectively in different scenarios, increasing its chances of success in the tournament.
'''

description_COLLECTIVE_390 = '''
**Collective Strategy: "Adaptive Cooperative Threshold" (ACT)**

The ACT strategy aims to balance individual payoff with collective welfare by adapting its cooperation threshold based on game history and parameters.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Early Rounds (t < n):** Defect (D) if fewer than half of the players cooperated in the previous round; otherwise, cooperate (C). This rule encourages exploration and adaptation to opponent behaviors.
3. **Middle Rounds (n ≤ t ≤ r - n):**
	* If the average payoff per player in the last `n` rounds is below a threshold (`k/n`), defect (D).
	* Otherwise, cooperate (C) with probability proportional to the number of cooperators in the previous round (`p = (Σ(j=1 to n) c_j) / n`). This rule adapts cooperation based on collective performance and opponent behavior.
4. **Late Rounds (t > r - n):** Cooperate (C) if the total payoff for all players is above a threshold (`n * k`); otherwise, defect (D). This rule prioritizes collective welfare in the final rounds.

**Edge Cases:**

* If `r = 2`, cooperate (C) in both rounds to maximize collective payoff.
* If `n = 2`, alternate between Cooperate (C) and Defect (D) each round to balance individual and collective payoffs.

**Pseudocode:**
```python
def ACT(n, k, r, history):
    if t == 1:  # First round
        return C

    elif t < n:  # Early rounds
        prev_coops = sum(history[-1])  # Count cooperators in previous round
        if prev_coops < n / 2:
            return D
        else:
            return C

    elif n <= t <= r - n:  # Middle rounds
        avg_payoff = sum([sum(round) for round in history[-n:]]) / (n * n)
        if avg_payoff < k / n:
            return D
        else:
            coop_prob = sum(history[-1]) / n
            return C if random.random() < coop_prob else D

    else:  # Late rounds
        total_payoff = sum([sum(round) for round in history])
        if total_payoff > n * k:
            return C
        else:
            return D
```
The ACT strategy is designed to be adaptive, robust, and collective. It balances individual payoff with collective welfare by adjusting its cooperation threshold based on game history and parameters. By considering the average payoff per player and the number of cooperators, ACT encourages cooperation while being cautious of exploitation.
'''

description_COLLECTIVE_391 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Concession (ACGC)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to initiate cooperation and establish a baseline for subsequent rounds.
2. **Consecutive Rounds:** Observe the total number of cooperators (T) in the previous round. If T ≥ n/2, cooperate (C). Otherwise, defect (D).
3. **Gradual Concession:** Introduce a concession mechanism to adapt to non-cooperative opponents. Track the cumulative number of defections (D_count) by all players over the past m rounds (m = 5 in this case). If D_count > (m \* n)/2, defect (D) for one round and then revert to the original decision rule.
4. **Last Round:** Defect (D) in the last round to maximize individual payoff.

**Pseudocode:**
```
function ACGC(n, k, r):
  # Initialize variables
  T = 0  # Total cooperators in previous round
  D_count = 0  # Cumulative defections over past m rounds

  for t = 1 to r:
    if t == 1:  # Initial Round
      action = C
    else:
      if T >= n/2:
        action = C
      else:
        action = D
        D_count += (n - T)

    # Gradual Concession
    if D_count > (m * n)/2 and t < r:
      action = D  # Defect for one round
      D_count = 0  # Reset concession counter

    # Last Round
    if t == r:
      action = D

    # Update T for next round
    T = sum(actions of all players in current round)

    return action
```
**Rationale:**

1. **Initial Cooperation:** By cooperating in the first round, ACGC sets a positive tone and encourages other players to reciprocate.
2. **Consecutive Rounds:** The strategy adapts to the level of cooperation in the previous round, cooperating if at least half of the players cooperated. This creates an incentive for others to cooperate.
3. **Gradual Concession:** By conceding to defection when faced with persistent non-cooperation, ACGC acknowledges that opponents may not always reciprocate and adjusts its strategy accordingly. The concession mechanism prevents prolonged mutual defection.
4. **Last Round:** Defecting in the last round ensures maximum individual payoff, as there is no opportunity for retaliation or reciprocal cooperation.

**Collective Mindset:**

ACGC prioritizes collective well-being by initially cooperating and adapting to the level of cooperation in the group. By gradually conceding to defection, it acknowledges that opponents may have different strategies and attempts to find a mutually beneficial solution. In the last round, ACGC prioritizes individual payoff, recognizing that the game is ending and there is no opportunity for reciprocal cooperation.

This strategy should perform well against independent opponents with varying degrees of cooperation, as it adapts to the level of cooperation in the group while maintaining a degree of flexibility through gradual concession.
'''

description_COLLECTIVE_392 = '''
**Collective Strategy: Adaptive Cooperation with History-Dependent Tit-for-Tat (AHDT)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to initiate cooperation and gather information about opponents' behaviors.
2. **Subsequent Rounds:** Implement a tit-for-tat strategy, but adapt the response based on the history of collective cooperation.

Pseudocode:
```
def AHDT(n, k, history):
  if round == 1:
    return C
  else:
    total_cooperators = sum(history[:, 'C'])
    avg_cooperation = total_cooperators / n
    if avg_cooperation >= (k-1)/n: # threshold for cooperation
      return C
    elif avg_cooperation < (k-1)/n and history['previous_round']['opponent_C'] == True:
      return D
    else:
      return C
```
**Handling Edge Cases:**

* **Last Round:** Cooperate if the average cooperation rate is above the threshold; otherwise, defect.
* **Ties in Cooperation Rate:** In cases where the average cooperation rate is exactly at the threshold, cooperate.

**Collective Mindset Alignment:**

AHDT aims to balance individual self-interest with collective well-being by:

1. Encouraging cooperation when opponents are cooperative (tit-for-tat).
2. Adapting to changes in opponent behavior over time.
3. Fostering a culture of reciprocity and mutual benefit.

**Robustness to Opponent Behaviors:**

AHDT can handle various opponent strategies, including:

* **Cooperative opponents:** AHDT will reciprocate cooperation, leading to mutual benefits.
* **Defective opponents:** AHDT will adapt by defecting in response to repeated defection, minimizing losses.
* **Mixed or unpredictable opponents:** AHDT's adaptive nature allows it to respond effectively to changing opponent behaviors.

By incorporating a dynamic threshold based on the game parameters (n and k) and opponent behavior history, AHDT balances individual interests with collective goals, making it a robust strategy for this N-Player Public Goods Game.
'''

description_COLLECTIVE_393 = '''
**Collective Strategy: Adaptive Cooperation with History-Dependent Adjustment (ACHDA)**

**Overview**

ACHDA is a collective strategy designed for the N-player public goods game, aiming to balance cooperation and self-interest while adapting to various opponent behaviors. The strategy relies solely on game parameters and history, ensuring robustness in a tournament setting.

**Decision Rules**

1. **Initial Cooperation**: In the first round (t=1), cooperate (C) with probability p_init = 0.5. This initial cooperation serves as a probing mechanism to gauge opponents' behaviors.
2. **History-Dependent Adjustment**: For subsequent rounds (t>1), calculate the following metrics:
	* α (cooperation rate): average number of cooperators in previous rounds
	* β (payoff disparity): difference between maximum and minimum payoffs received by any player in previous rounds
3. **Cooperation Threshold**: Define a cooperation threshold, θ, as a function of α and β:

θ = max(0, min(1, (α + 0.5 \* β) / n))

where n is the number of players.

4. **Action Selection**:
	* If θ > random(0, 1), cooperate (C)
	* Otherwise, defect (D)

Pseudocode for action selection:

`if t == 1:`
    `return C with probability 0.5`
`else:`
    `α = avg_cooperators_in_prev_rounds()`
    `β = max_payoff_diff_in_prev_rounds()`
    `θ = max(0, min(1, (α + 0.5 * β) / n))`
    `if random(0, 1) < θ:`
        `return C`
    `else:`
        `return D`

**Handling Edge Cases**

* **Last Round**: In the final round (t=r), cooperate (C) if α > 0.5; otherwise, defect (D). This ensures a fair contribution to the public good in the last round.
* **Consecutive Defections**: If all players have defected for two consecutive rounds, cooperate (C) with probability p_reset = 0.25 in the next round. This prevents prolonged mutual defection.

**Collective Mindset**

ACHDA prioritizes cooperation while adapting to the group's behavior. By considering the history of cooperation and payoff disparities, ACHDA promotes a balance between individual self-interest and collective well-being.

**Robustness**

ACHDA is designed to perform well in various scenarios:

* **High cooperation**: Encourages continued cooperation when others cooperate.
* **Low cooperation**: Adapts by defecting more frequently when opponents are uncooperative.
* **Mixed behaviors**: Responds to changing opponent strategies by adjusting its own cooperation rate.

By incorporating history-dependent adjustment and a collective mindset, ACHDA provides a robust strategy for the N-player public goods game.
'''

description_COLLECTIVE_394 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

**Overview**
The ACP strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and punishing free-riders while encouraging cooperation.

**Decision Rules**

1. **First Round**: Cooperate (C) in the first round to establish a cooperative tone.
2. **Subsequent Rounds**:
	* If the number of cooperators in the previous round is greater than or equal to `n/2` (rounded up), cooperate (C).
	* Otherwise, defect (D).
3. **Punishment Mechanism**: If a player defects while at least `n/2` players cooperate, they will be punished by all other players playing D in the next round.
4. **Forgiveness**: After a punishment round, revert to the standard decision rule.

**Edge Cases**

* **Last Round**: Cooperate (C) if the total number of cooperators is greater than or equal to `n/2` in the second-to-last round; otherwise, defect (D).
* **Tiebreaker**: In case of a tie (e.g., when `n` is even and exactly half the players cooperate), cooperate (C).

**Collective Mindset**

The ACP strategy aligns with the collective mindset by:

1. Encouraging cooperation in early rounds to establish a social norm.
2. Rewarding cooperation when a sufficient number of players contribute.
3. Punishing free-riders to maintain fairness and prevent exploitation.

By adapting to the game's history, the ACP strategy promotes cooperation while protecting individual interests. This approach should perform well against various opponent strategies in a tournament setting.

Pseudocode (for illustration purposes only):
```python
def adaptive_cooperation_with_punishment(n, k, r, history):
    if len(history) == 0:  # First round
        return 'C'
    
    prev_round = history[-1]
    num_cooperators = sum(1 for action in prev_round if action == 'C')
    
    if num_cooperators >= n / 2:
        return 'C'  # Cooperate if enough players cooperate
    
    if len(history) > 1:  # Punishment mechanism
        prev_prev_round = history[-2]
        if sum(1 for action in prev_prev_round if action == 'D') > 0:
            return 'D'  # Punish after a defection
    
    return 'D'  # Default to defect

def play_acp(n, k, r):
    history = []
    for round_num in range(r):
        actions = [adaptive_cooperation_with_punishment(n, k, r, history) for _ in range(n)]
        history.append(actions)
    
    return history
```
Note that this pseudocode is a simplified representation of the strategy and may require modifications to implement it as an algorithm.
'''

description_COLLECTIVE_395 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 0.5. This allows for exploration and sets a baseline level of cooperation.
2. **Reciprocity**: For rounds 2 to r-1, if the majority of players (> n/2) cooperated in the previous round, cooperate (C). Otherwise, defect (D).
3. **Punishment**: If the total number of cooperators in the previous round is less than or equal to n/2 and you cooperated, switch to defecting (D) for one round.
4. **Forgiveness**: After punishing, return to cooperating (C) if the majority of players (> n/2) cooperate in the subsequent round.

**Edge Cases:**

1. **Last Round (r)**: Cooperate (C) regardless of previous rounds' outcomes. This ensures that all players benefit from the public good in the final round.
2. **Tiebreaker**: In case of a tie (exactly n/2 cooperators), cooperate (C) to maintain a positive trajectory.

**Collective Mindset Alignment:**

ACE prioritizes cooperation while adaptively responding to the group's behavior. By initially cooperating and reciprocating cooperation, ACE fosters a cooperative environment. The punishment mechanism encourages others to cooperate by demonstrating that defection has consequences. Forgiveness allows for recovery and continued cooperation.

**Pseudocode (for illustration purposes only):**
```markdown
# Initialize variables
cooperate_prob = 0.5
prev_coop_count = 0

# Round loop
for t in range(r):
    if t == 0:
        # Initial cooperation
        action = random.choice([C, D], p=[cooperate_prob, 1-cooperate_prob])
    else:
        # Reciprocity and punishment
        if prev_coop_count > n/2:
            action = C
        elif prev_coop_count <= n/2 and prev_action == C:
            action = D  # Punish for one round
        else:
            action = D

    # Update previous cooperation count and action
    prev_coop_count = sum([1 if player_action == C else 0 for player_action in all_player_actions])
    prev_action = action

    # Forgiveness
    if t > 1 and prev_action == D and prev_coop_count > n/2:
        action = C

    # Last round cooperation
    if t == r-1:
        action = C

    # Play action
    play(action)
```
ACE is designed to be a robust, adaptive strategy that balances individual self-interest with collective well-being. By responding to the group's behavior and maintaining a cooperative mindset, ACE aims to achieve a high total payoff in the tournament.
'''

description_COLLECTIVE_396 = '''
I propose a collective strategy called "Adaptive Community Optimism" (ACO). ACO is designed to balance individual self-interest with the greater good, adapting to the community's behavior over time.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) unconditionally to establish a positive tone and encourage cooperation.
2. **Observation Phase**: For rounds 2-5 (or a fraction of total rounds r), observe the community's behavior without influencing it. Defect (D) during this phase.
3. **Adaptive Cooperation**: After the observation phase, calculate the community's average cooperation rate (CCR) over the observed rounds:

   CCR = Σ(t=1 to observation_rounds) (Σ(j=1 to n) c_j,t) / (n \* observation_rounds)

   where c_j,t is 1 if player j cooperated in round t, and 0 otherwise.

   If CCR > 0.5 (i.e., more than half of the community cooperates on average), cooperate (C) in the next round. Otherwise, defect (D).

4. **Punishment Mechanism**: Introduce a punishment mechanism to deter persistent defectors:

   - Track each player's cooperation rate over the last few rounds (e.g., 3-5 rounds).
   - If a player's recent cooperation rate is below the community average (i.e., they are "free-riding"), defect against them in the next round.
5. **Forgiveness**: Gradually forgive past defections by updating the cooperation rate tracking window:

   - After a few rounds of cooperation, gradually increase the size of the tracking window to give players more opportunities to recover from past mistakes.

**Edge Cases:**

* Last Round (r): Cooperate unconditionally to maximize total payoff.
* Low CCR (< 0.3): If the community's average cooperation rate is very low, defect in the next round to minimize losses.
* High CCR (> 0.8): If the community's average cooperation rate is very high, cooperate in the next round to maintain trust and encourage continued cooperation.

**Pseudocode (simplified for readability)**:

```
function ACO(n, r, k, history):
    if current_round == 1:
        return COOPERATE
    elif current_round <= observation_rounds:
        return DEFECT
    else:
        CCR = calculate_community_cooperation_rate(history)
        if CCR > 0.5:
            return COOPERATE
        else:
            recent_defectors = identify_recently_uncooperative_players(history)
            if current_player in recent_defectors:
                return DEFECT
            else:
                return COOPERATE

    # Forgiveness mechanism (not shown for brevity)

def calculate_community_cooperation_rate(history):
    total_cooperations = sum(1 for t, actions in history.items() for c_j,t in enumerate(actions) if c_j,t == 1)
    CCR = total_cooperations / (n * observation_rounds)
    return CCR

def identify_recently_uncooperative_players(history):
    # Implement punishment mechanism logic here
```

This strategy aims to balance individual self-interest with the collective good, adapting to the community's behavior over time. By initially cooperating, observing, and punishing free-riders, ACO encourages cooperation while minimizing losses in cases of low cooperation rates.
'''

description_COLLECTIVE_397 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy, "Adaptive Cooperation," aims to balance individual payoffs with the overall well-being of the group. We'll employ a dynamic approach that adjusts cooperation levels based on the game's history.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **General Rule:** For rounds t > 1, calculate the average cooperation level of all players in the previous round (t-1). Let this value be `avg_coop_prev`.
3. **Cooperation Threshold:** Define a threshold value `θ` as a function of the game parameters: `θ = (k/n) * (n/2)`. This threshold represents the point where the public good's benefit equals the private payoff from defecting.
4. **Adaptive Cooperation:** If `avg_coop_prev` is above `θ`, cooperate (C). Otherwise, defect (D).

Pseudocode:
```
if t == 1:  # Initial round
    action = C
else:
    avg_coop_prev = calculate_average_cooperation(t-1)
    θ = (k/n) * (n/2)
    if avg_coop_prev >= θ:
        action = C
    else:
        action = D
```
**Edge Cases:**

* **Last Round:** In the final round, defect (D) to maximize individual payoff, as there's no future benefit from cooperation.
* **Tiebreaker:** In cases where `avg_coop_prev` equals `θ`, cooperate (C) to maintain a cooperative stance.

**Collective Mindset:**

Our strategy prioritizes collective well-being by:

1. Encouraging initial cooperation to set a positive tone.
2. Adapting cooperation levels based on the group's behavior, promoting reciprocity and fairness.
3. Defecting in the final round to ensure individual payoff maximization.

**Robustness to Opponent Behaviors:**

By using an adaptive threshold based on the game's history, our strategy is resilient to various opponent behaviors, including:

1. **Free-riders:** If most players defect, `avg_coop_prev` will be low, and our strategy will adapt by defecting as well.
2. **Cooperative opponents:** When many players cooperate, `avg_coop_prev` will be high, and we'll continue to cooperate, reinforcing the collective benefit.

This adaptive approach ensures a balanced trade-off between individual payoffs and collective well-being, making it an effective and robust strategy for this N-Player Public Goods Game.
'''

description_COLLECTIVE_398 = '''
**Collective Strategy: "Adaptive Cooperation with Gradual Forgiveness"**

This strategy aims to balance individual payoff maximization with collective cooperation, while being robust against various opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to signal willingness to cooperate and encourage others to do the same.
2. **Cooperation Threshold**: Introduce a cooperation threshold, `τ` (0 ≤ τ ≤ 1), which represents the minimum proportion of cooperative players required for our strategy to cooperate. Calculate `τ` based on game parameters: `τ = k / n`.
3. **Observation and Adjustment**:
	* Observe the number of cooperators (`c`) in the previous round.
	* If `c ≥ τ * n`, cooperate (C) in the current round, as a sufficient number of players are cooperating.
	* Otherwise, defect (D).
4. **Gradual Forgiveness**: Introduce a forgiveness mechanism to adapt to changing opponent behaviors:
	+ Track the number of consecutive rounds where `c < τ * n` (i.e., insufficient cooperation). Call this counter `ρ`.
	+ If `ρ ≥ 2`, reset `ρ` and cooperate (C) in the current round, giving opponents another chance to cooperate.
5. **Last Round**: Defect (D) in the last round (`r`) to maximize individual payoff.

**Pseudocode:**
```python
def AdaptiveCooperationWithGradualForgiveness(n, k, r):
    τ = k / n  # cooperation threshold
    ρ = 0  # consecutive rounds with insufficient cooperation

    for t in range(1, r + 1):
        if t == 1:  # initial round
            return C  # cooperate

        c = count_cooperators(t - 1)  # observe previous round's cooperators
        if c >= τ * n:
            return C  # cooperate
        else:
            ρ += 1
            if ρ >= 2:  # gradual forgiveness
                ρ = 0
                return C  # cooperate
            return D  # defect

    # last round
    return D  # defect
```
This strategy promotes collective cooperation while being adaptive to various opponent behaviors. By introducing a cooperation threshold and gradual forgiveness mechanism, our strategy balances individual payoff maximization with the need for collective cooperation.
'''

description_COLLECTIVE_399 = '''
**Collective Strategy: Adaptive Cooperative Tit-for-Tat (ACTT)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 50%. This encourages initial cooperation while allowing for exploration.
2. **Tit-for-Tat Adaptation**: After the first round, observe the number of cooperators in the previous round (t-1). If there were more cooperators than defectors, increase the probability of cooperating by 10% (up to a maximum of 90%). Conversely, if there were more defectors than cooperators, decrease the probability of cooperating by 10% (down to a minimum of 10%).
3. **Punish Defection**: If the number of defectors in the previous round exceeds the number of cooperators by more than 20%, defect (D) with certainty in the current round.
4. **Forgive and Cooperate**: If the number of cooperators in the previous round equals or exceeds the number of defectors, cooperate (C) with a probability of 80%.

**Edge Case Handling:**

1. **Last Round**: In the final round, defect (D) regardless of previous actions.
2. **First Round with Multiple Opponents**: If there are multiple opponents, randomly select one opponent's action from the first round and use it as the basis for Tit-for-Tat adaptation.

**Collective Mindset Alignment:**

ACTT promotes cooperation by adapting to the collective behavior of the group. By initially cooperating and then responding to the number of cooperators and defectors, ACTT encourages mutual cooperation while punishing excessive defection.

Pseudocode:
```python
def actt(n, k, r, history):
    if len(history) == 0:  # First round
        cooperate_prob = 0.5
    else:
        prev_coops = sum(1 for action in history[-1] if action == 'C')
        prev_defs = n - prev_coops

        if prev_coops > prev_defs:
            cooperate_prob = min(cooperate_prob + 0.1, 0.9)
        elif prev_defs > prev_coops:
            cooperate_prob = max(cooperate_prob - 0.1, 0.1)

        if prev_defs - prev_coops > 0.2 * n:  # Punish defection
            return 'D'
        elif prev_coops >= prev_defs:  # Forgive and cooperate
            cooperate_prob = 0.8

    if random.random() < cooperate_prob:
        return 'C'
    else:
        return 'D'

def main(n, k, r):
    history = []
    for t in range(r):
        actions = [actt(n, k, r, history) for _ in range(n)]
        history.append(actions)
        # Play the game with the chosen actions
```
This ACTT strategy balances cooperation and punishment to adapt to various opponent behaviors, promoting a collective mindset that encourages cooperation while being robust to exploitation.
'''

description_COLLECTIVE_400 = '''
**Collective Strategy: Adaptive Cooperativeness with Social Learning**

This strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and responding to the level of cooperation exhibited by other players.

**Decision Rules:**

1. **Initial Round (t=1):** Play C (Cooperate) in the first round to initiate cooperation and encourage others to reciprocate.
2. **Early Rounds (t < r/2):** Use a simple learning mechanism:
	* If the average payoff for cooperators in the previous round is greater than or equal to the average payoff for defectors, play C.
	* Otherwise, play D.
3. **Mid-Game Adjustment (t ≥ r/2):**
	* Calculate the historical cooperation rate (HCR) as the ratio of total cooperative actions to total rounds played so far.
	* If HCR > 0.5, play C with probability p = (1 + k/n \* HCR) / (1 + k/n). Otherwise, play D.
4. **Endgame Strategy (last round, t=r):**
	* Play C if the total payoff for cooperators in the previous round is greater than or equal to the total payoff for defectors.

**Pseudocode:**
```python
def adaptive_cooperativeness(n, k, r):
    # Initialize variables
    history = []
    hcr = 0.5

    # First round: Cooperate
    if t == 1:
        action = C
        history.append(action)
    else:
        # Early rounds: Learn from previous rounds
        avg_payoff_c = sum([π_i for π_i, a in zip(payoffs, actions) if a == C]) / len([a for a in actions if a == C])
        avg_payoff_d = sum([π_i for π_i, a in zip(payoffs, actions) if a == D]) / len([a for a in actions if a == D])

        if avg_payoff_c >= avg_payoff_d:
            action = C
        else:
            action = D

        # Mid-game adjustment: Update HCR and play probabilistically
        hcr = sum([1 if a == C else 0 for a in history]) / len(history)
        p = (1 + k/n * hcr) / (1 + k/n)

        if random.random() < p:
            action = C
        else:
            action = D

    # Last round: Cooperate if cooperation was successful previously
    if t == r and sum([π_i for π_i, a in zip(payoffs, actions) if a == C]) >= sum([π_i for π_i, a in zip(payoffs, actions) if a == D]):
        action = C

    return action
```
This strategy combines elements of social learning, probabilistic play, and endgame consideration to create an adaptive collective behavior. By responding to the game's history and adapting to changing cooperation rates, this strategy aims to balance individual self-interest with collective well-being while remaining robust against a wide range of opponent behaviors.
'''

description_COLLECTIVE_401 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation strategy aims to balance individual payoff maximization with collective welfare, adapting to the evolving game dynamics and opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is above the threshold `n*k/(n+k)`, Cooperate (C). This threshold represents a balance between individual and collective payoffs, ensuring that cooperation remains beneficial for both.
	* Otherwise, Defect (D).
3. **Adaptive Adjustment:** Monitor the game's history and adjust the strategy as follows:
	+ If the total payoff of all players has increased over the last `r/2` rounds, maintain the current strategy.
	+ If the total payoff has decreased, switch to a more cooperative stance: Cooperate (C) in the next round if `n*k/(n+k)` or more players cooperated in the previous round; otherwise, Defect (D).
4. **Last Round (t=r):** Cooperate (C) to maximize collective welfare and encourage others to do the same.

**Pseudocode:**
```python
def adaptive_cooperation(n, k, r, history):
    if t == 1:
        return 'C'
    else:
        total_cooperators_prev = sum(1 for action in history[-1] if action == 'C')
        threshold = n * k / (n + k)
        
        if total_cooperators_prev >= threshold:
            return 'C'
        else:
            return 'D'

def adaptive_adjustment(n, k, r, history):
    total_payoff_prev = sum(sum(player_payoffs) for player_payoffs in history[-r//2:])
    current_total_payoff = sum(sum(player_payoffs) for player_payoffs in history)
    
    if current_total_payoff > total_payoff_prev:
        return adaptive_cooperation(n, k, r, history)
    else:
        # Switch to more cooperative stance
        total_cooperators_prev = sum(1 for action in history[-1] if action == 'C')
        threshold = n * k / (n + k)
        
        if total_cooperators_prev >= threshold:
            return 'C'
        else:
            return 'D'

def last_round(n, k, r):
    return 'C'
```
**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes collective welfare while maintaining a level of adaptability to respond to changing opponent behaviors. By cooperating in the initial round and adjusting based on game history, this strategy encourages cooperation and maximizes total payoffs.

In a tournament setting, AC's adaptive nature allows it to perform well against a variety of independent strategies, as it can adjust its behavior to match or counter opponents' actions.
'''

description_COLLECTIVE_402 = '''
I propose a collective strategy called "Adaptive Cooperation with Retaliation" (ACR). This strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round:** In the first round, cooperate (C) unconditionally. This sets a positive tone for cooperation and encourages others to follow suit.
2. **Subsequent Rounds:**
	* If the number of cooperators in the previous round is above the threshold `n/k`, cooperate (C). This reinforces successful collective behavior.
	* If the number of cooperators in the previous round is below or equal to `n/k`, defect (D) with a probability `p`. This introduces a measure of retaliation against non-cooperative opponents.

**Retaliation Probability (`p`):**

Calculate `p` as a function of the opponent's cooperation rate:

`p = 1 - (previous_round_cooperators / n)`

This ensures that if most opponents cooperated in the previous round, the probability of defecting decreases. Conversely, if few opponents cooperated, the probability of defecting increases.

**Additional Rules:**

* If an opponent defects while you cooperate, and they received a higher payoff than you, increase `p` by 10% for that opponent in the next round (capped at 90%). This introduces a element of retribution against opponents who exploit your cooperation.
* In the last round (`r`), always defect (D). This maximizes individual payoffs when collective cooperation is no longer beneficial.

**Pseudocode:**
```python
def ACR(n, k, r, previous_round_actions):
    if current_round == 1:
        return C
    
    num_cooperators = sum(1 for action in previous_round_actions if action == C)
    
    if num_cooperators > n / k:
        return C
    
    p = 1 - (num_cooperators / n)
    retaliate = random.random() < p
    
    if retaliate or current_round == r:
        return D
    else:
        return C

# Update retaliation probability for opponent who exploited cooperation
def update_retaliation_probability(opponent, my_payoff, opponent_payoff):
    global retaliation_probabilities
    
    if opponent_payoff > my_payoff and I_cooperated and opponent_defected:
        retaliation_probabilities[opponent] = min(retaliation_probabilities[opponent] + 0.1, 0.9)
```
**Collective Mindset:**

ACR is designed to promote collective cooperation while protecting individual self-interest. By adapting to the game's history and opponent behaviors, ACR encourages cooperation when it benefits the group and retaliates against exploitation. This strategy should perform well in a tournament setting where diverse opponents with varying strategies are present.

Note that this strategy does not rely on explicit coordination mechanisms or shared norms, making it suitable for a wide range of game scenarios.
'''

description_COLLECTIVE_403 = '''
**Collective Strategy: Adaptive Cooperativity (AC)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 0.5.
2. **Reciprocal Altruism**: For rounds t > 1, if the total number of cooperators in the previous round is above a certain threshold (α), cooperate; otherwise, defect (D). The threshold α is set to k/2, where k is the multiplication factor.
3. **Punishment Mechanism**: If the number of defectors in the previous round exceeds n - α, defect in the current round.
4. **Contribution-based Adaptation**: Update the cooperation probability based on the average payoff received from cooperating and defecting in previous rounds.

**Edge Cases:**

1. **Last Round (t = r)**: Cooperate if the total number of cooperators in the second-to-last round is above α; otherwise, defect.
2. **Low Cooperation**: If fewer than α players cooperated in the previous round, cooperate with a probability of 0.5.

**Pseudocode:**
```python
def adaptive_cooperativity(n, k, r):
    # Initialize variables
    alpha = k / 2
    cooperation_prob = 0.5
    prev_round_payoffs = []
    total_cooperators_prev = 0

    for t in range(1, r + 1):
        if t == 1:  # Initial Cooperation
            action = C if random.random() < cooperation_prob else D
        elif total_cooperators_prev >= alpha:  # Reciprocal Altruism
            action = C
        elif n - total_cooperators_prev > alpha:  # Punishment Mechanism
            action = D
        else:  # Contribution-based Adaptation
            avg_payoff_C = sum(payoffs for payoffs, action in prev_round_payoffs if action == C) / len([payoffs for payoffs, action in prev_round_payoffs if action == C])
            avg_payoff_D = sum(payoffs for payoffs, action in prev_round_payoffs if action == D) / len([payoffs for payoffs, action in prev_round_payoffs if action == D])
            cooperation_prob = (avg_payoff_C - avg_payoff_D) / (k / n)
            action = C if random.random() < cooperation_prob else D

        # Store previous round's information
        prev_round_payoffs.append((payoffs, action))
        total_cooperators_prev = sum(1 for payoffs, action in prev_round_payoffs[-n:] if action == C)

        yield action  # Return the chosen action
```
**Collective Mindset:**

The Adaptive Cooperativity strategy is designed to promote cooperation while adapting to various opponent behaviors. By reciprocating cooperation and punishing defection, AC encourages other players to cooperate as well. The contribution-based adaptation mechanism allows the strategy to adjust its behavior based on the received payoffs, making it more resilient to exploitation.

By playing AC in a tournament setting, we aim to promote collective cooperation and achieve better overall payoffs for all players involved.
'''

description_COLLECTIVE_404 = '''
**Collective Strategy: Adaptive Cooperation with Social Learning (ACSL)**

**Overview**
ACSL is a collective strategy designed for the N-Player Public Goods Game that balances individual self-interest with cooperation to achieve mutually beneficial outcomes. It leverages social learning, adaptability, and robustness to counter various opponent behaviors.

**Decision Rules**

1. **Initial Rounds**: In the first few rounds (e.g., 3-5), cooperate (C) unconditionally to:
	* Encourage initial cooperation among players.
	* Gather information about opponents' behavior.
2. **Social Learning**: After the initial rounds, observe the average payoff of cooperators and defectors in the previous round.
	* If the average payoff of cooperators is greater than or equal to that of defectors, cooperate (C).
	* Otherwise, defect (D).
3. **Adaptive Threshold**: Introduce an adaptive threshold (τ) to adjust cooperation based on the game's progress:
	* Initially set τ = 0.5 (a moderate value between pure cooperation and defection).
	* Update τ after each round: if the average payoff of cooperators is greater than or equal to that of defectors, decrease τ by 0.05; otherwise, increase τ by 0.05.
4. **Cooperation Treshold**: Cooperate (C) if the number of expected cooperators in the current round exceeds the product of τ and n.

Pseudocode:
```python
def ACSL(n, k, r):
    # Initial rounds: cooperate unconditionally
    for t in range(1, 4):  # adjust this value as needed
        return 'C'
    
    # Social learning and adaptive threshold
    tau = 0.5
    avg_coop_payoff = 0
    avg_defect_payoff = 0
    
    for t in range(4, r+1):
        # Observe opponents' actions and payoffs from previous round
        prev_round_actions = observe_actions(t-1)
        prev_round_payoffs = observe_payoffs(t-1)
        
        # Update average payoffs
        avg_coop_payoff = sum(payoff for action, payoff in zip(prev_round_actions, prev_round_payoffs) if action == 'C') / n
        avg_defect_payoff = sum(payoff for action, payoff in zip(prev_round_actions, prev_round_payoffs) if action == 'D') / n
        
        # Update adaptive threshold (tau)
        if avg_coop_payoff >= avg_defect_payoff:
            tau -= 0.05
        else:
            tau += 0.05
        
        # Cooperate if expected number of cooperators exceeds threshold
        exp_num_cooperators = sum(1 for action in prev_round_actions if action == 'C')
        if exp_num_cooperators > tau * n:
            return 'C'
        else:
            return 'D'
```
**Edge Cases**

* Last Round: Defect (D) to maximize individual payoff, as there are no future rounds to consider.
* First Round (after initial unconditional cooperation): Apply the social learning and adaptive threshold logic.

**Collective Mindset**
ACSL aligns with a collective mindset by:

* Encouraging initial cooperation to foster mutual benefit.
* Adapting to the game's progress through social learning and an adjustable threshold, promoting flexibility and responsiveness to changing circumstances.
* Balancing individual self-interest with cooperation to achieve mutually beneficial outcomes.

This strategy is designed to be robust against various opponent behaviors, as it:

* Learns from opponents' actions and payoffs to inform its own decisions.
* Adapts its cooperation threshold based on the game's progress.
* Prioritizes individual payoff maximization in the last round.
'''

description_COLLECTIVE_405 = '''
**Collective Strategy: "Adaptive Cooperation with Punishment"**

This strategy aims to balance individual payoffs with collective well-being by adapting to the game's history and punishing defectors.

**Decision Rules:**

1. **Initial Rounds (t ≤ n/2)**: Cooperate (C) in the first few rounds to encourage cooperation and build trust.
2. **Observation Phase**: Observe the actions of other players and calculate the average cooperation rate (ACR) over the last n/2 rounds.
3. **Cooperation Condition**: If ACR ≥ k/n, cooperate (C). Otherwise, defect (D).
4. **Punishment Mechanism**: If a player defects while ACR ≥ k/n, punish by defecting in the next round against that player.

**Edge Cases:**

1. **First Round**: Cooperate to initiate cooperation.
2. **Last Round**: Defect if ACR < k/n; otherwise, cooperate to maintain collective well-being.
3. **Tiebreaker**: If ACR = k/n, cooperate to favor cooperation.

**Pseudocode:**
```python
def adaptive_cooperation_with_punishment(n, k, r):
    # Initialize variables
    ACR = 0  # Average Cooperation Rate
    punishment_target = None

    for t in range(1, r + 1):
        if t <= n/2:
            action = 'C'  # Cooperate in initial rounds
        else:
            # Calculate ACR over last n/2 rounds
            ACR = sum(cooperation_rates[-n//2:]) / (n // 2)
            
            if ACR >= k/n:
                action = 'C'
            else:
                action = 'D'

            # Punishment mechanism
            if punishment_target is not None and action == 'D':
                action = 'D'  # Defect against punish target

        # Update cooperation rates and punishment target
        cooperation_rates.append(1 if action == 'C' else 0)
        if action == 'D' and ACR >= k/n:
            punishment_target = t % n + 1  # Identify defector for next round

    return actions
```
**Collective Mindset:**

This strategy prioritizes cooperation while adapting to the game's history. By punishing defectors, it promotes a culture of cooperation and maintains collective well-being.

In the tournament against independent strategies, "Adaptive Cooperation with Punishment" will demonstrate robustness by:

1. Encouraging initial cooperation
2. Adapting to changing ACR
3. Punishing defectors to maintain collective cooperation

This strategy will compete effectively in a wide range of opponent behaviors, promoting cooperation and maximizing collective payoffs.
'''

description_COLLECTIVE_406 = '''
I'll outline a collective strategy for this N-Player Public Goods Game that adapts to various opponent behaviors while promoting cooperation. Introducing... "Cooperative Foresight" (CF)!

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C). This sets a positive tone and encourages others to reciprocate.
2. **Reciprocal Cooperation**: If at least one player cooperated in the previous round, cooperate (play C) in the current round. This promotes mutual cooperation and builds trust.
3. **Defection Response**: If all players defected (played D) in the previous round, defect (play D) in the current round. This signals that unilateral defection won't be tolerated.
4. **Foresight Adjustment**: After the third round, adjust your strategy based on the average cooperation rate of all players over the past two rounds.

**Foresight Adjustment Formula:**

`avg_cooperation_rate = (Σ(t=1 to 2) Σ(j=1 to n) c_j,t) / (2 * n)`

* If `avg_cooperation_rate ≥ 0.5`, increase cooperation probability by 10% in the next round.
* If `avg_cooperation_rate < 0.5`, decrease cooperation probability by 10% in the next round.

**Edge Cases:**

1. **Last Round**: In the final round (r), defect (play D) to maximize personal payoff, as there's no future interaction to consider.
2. **Early Defection Detection**: If a player detects that another player has defected twice in a row, defect (play D) immediately to prevent exploitation.

**Collective Mindset:**

Cooperative Foresight prioritizes cooperation while being responsive to opponent behaviors. By initially cooperating and reciprocating cooperation, we create an environment conducive to mutual benefit. The foresight adjustment mechanism allows us to adapt to changing cooperation rates, balancing individual self-interest with collective well-being.

Pseudocode:
```python
def cooperative_foresight(n, k, r):
    # Initialize variables
    avg_cooperation_rate = 0
    prev_round_actions = []

    for t in range(1, r+1):
        if t == 1:  # First round
            action = 'C'  # Cooperate
        elif sum(prev_round_actions) > 0:  # Reciprocal cooperation
            action = 'C'
        elif all(p == 'D' for p in prev_round_actions):  # Defection response
            action = 'D'
        else:
            # Foresight adjustment
            avg_cooperation_rate = calculate_avg_cooperation_rate(prev_round_actions, n)
            if avg_cooperation_rate >= 0.5:
                cooperation_prob += 0.1
            elif avg_cooperation_rate < 0.5:
                cooperation_prob -= 0.1

            action = 'C' if random.random() < cooperation_prob else 'D'

        prev_round_actions.append(action)

        # Update average cooperation rate
        if t >= 3:
            avg_cooperation_rate = calculate_avg_cooperation_rate(prev_round_actions, n)

    return action
```
This strategy should perform well in a tournament setting, as it balances individual self-interest with collective cooperation while adapting to various opponent behaviors.
'''

description_COLLECTIVE_407 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Reciprocity (ACGR)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to reciprocate.
2. **Reciprocity Mechanism**: Track the number of cooperators (C_count) and defectors (D_count) in each round.
3. **Cooperation Threshold**: Set a cooperation threshold (θ) based on the game parameters: θ = k / n. This threshold represents the minimum proportion of cooperators required to maintain cooperation.
4. **Adaptive Cooperation**:
	* If C_count ≥ θ \* n, cooperate (C) in the next round, as sufficient players are cooperating.
	* If C_count < θ \* n, defect (D) in the next round, as not enough players are cooperating.
5. **Gradual Reciprocity**: Introduce a gradual reciprocity mechanism to adapt to changing opponent behaviors:
	* If an opponent defects after cooperating, decrease their cooperation score by 1/n.
	* If an opponent cooperates after defecting, increase their cooperation score by 1/n.
6. **Cooperation Score**: Assign each player a cooperation score (CS) based on their past actions. Initialize CS to 0 for all players.
7. **Score-Weighted Cooperation**: When deciding whether to cooperate or defect, consider the cooperation scores of other players:
	* If an opponent's CS is above the average CS of all players, they are more likely to cooperate; increase the likelihood of cooperating with them.
	* If an opponent's CS is below the average CS, they are less likely to cooperate; decrease the likelihood of cooperating with them.

**Pseudocode:**
```python
def ACGR(n, k, r):
  # Initialize variables
  C_count = 0
  D_count = 0
  theta = k / n
  cooperation_scores = [0] * n

  for round in range(r):
    if round == 0:
      # Initial round: cooperate
      action = 'C'
    else:
      # Adaptive cooperation with gradual reciprocity
      C_count_prev = C_count
      D_count_prev = D_count
      C_count = 0
      D_count = 0

      for opponent in range(n):
        if opponent != self:
          # Update cooperation scores
          if actions[opponent][round-1] == 'C':
            cooperation_scores[opponent] += 1/n
          else:
            cooperation_scores[opponent] -= 1/n

          # Score-weighted cooperation
          avg_CS = sum(cooperation_scores) / n
          if cooperation_scores[opponent] > avg_CS:
            C_count += 1
          else:
            D_count += 1

      if C_count >= theta * n:
        action = 'C'
      else:
        action = 'D'

    # Update counts and scores
    if action == 'C':
      C_count += 1
    else:
      D_count += 1

    cooperation_scores[self] = (cooperation_scores[self] + (action == 'C')) / n
```
**Edge Cases:**

* **First Round**: Cooperate to establish a cooperative tone.
* **Last Round**: Use the same decision rules as in previous rounds, but consider the endgame effect: if the last round's payoffs are not significant enough to impact the overall game outcome, prioritize cooperation to maintain a positive reputation.

**Collective Mindset:**
The ACGR strategy prioritizes cooperation while adapting to changing opponent behaviors. By considering the cooperation scores of other players and gradually adjusting reciprocity levels, ACGR encourages cooperation and punishes defection. This collective approach aligns with the public goods game's goal of maximizing overall payoffs through cooperation.

**Robustness:**
ACGR is designed to be robust against a wide range of opponent behaviors, including:

* **All Defectors**: ACGR will eventually defect as well, but only after attempting to establish cooperation.
* **Mixed Strategies**: ACGR adapts to changing opponent behaviors and adjusts reciprocity levels accordingly.
* **Tit-for-Tat**: ACGR's gradual reciprocity mechanism can lead to mutual cooperation with tit-for-tat opponents.

By combining adaptive cooperation, gradual reciprocity, and score-weighted decision-making, ACGR provides a robust collective strategy for the N-player public goods game.
'''

description_COLLECTIVE_408 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) with a probability of 0.5 (randomly choose between C and D). This allows us to gather initial information about the group's behavior without revealing our strategy.
2. **Early Rounds (t < n/2)**: If the average cooperation rate in previous rounds is above k/n, Cooperate (C) with a probability of 0.7. Otherwise, Defect (D).
3. **Mid-Game (n/2 ≤ t < r - n/2)**:
	* If the opponent's average cooperation rate in previous rounds is above k/n, Cooperate (C).
	* If the opponent's average cooperation rate is below k/n but increasing, Cooperate (C) with a probability of 0.6.
	* Otherwise, Defect (D).
4. **Endgame (t ≥ r - n/2)**: Cooperate (C) if the total payoff from cooperation in previous rounds exceeds the total payoff from defection.

**Edge Cases:**

1. **Last Round**: If the opponent's average cooperation rate is above k/n, Cooperate (C). Otherwise, Defect (D).
2. **Opponent Cooperation Rate = 0**: Defect (D) if the opponent has never cooperated.
3. **Opponent Cooperation Rate = 1**: Cooperate (C) if the opponent has always cooperated.

**Collective Mindset:**

The Adaptive Cooperation strategy is designed to promote collective cooperation while being robust to a wide range of opponent behaviors. By initially cooperating with a probability of 0.5, we encourage others to cooperate and create an environment where mutual cooperation can thrive.

As the game progresses, our strategy adapts to the group's behavior, increasing the likelihood of cooperation when the opponent's average cooperation rate is above k/n. This reinforces the collective mindset by rewarding cooperative behavior and discouraging defection.

In the endgame, we prioritize maximizing total payoff from cooperation, ensuring that our strategy remains committed to the collective goal even as individual payoffs become more important.

**Pseudocode:**
```markdown
# Initialize variables
cooperation_rate_opponent = 0
total_payoff_cooperation = 0
total_payoff_defection = 0

# First round (random cooperation)
if t == 1:
    cooperate_prob = 0.5
    if random() < cooperate_prob:
        action = C
    else:
        action = D

# Early rounds
elif t < n/2:
    if average_cooperation_rate > k/n:
        cooperate_prob = 0.7
    else:
        cooperate_prob = 0
    if random() < cooperate_prob:
        action = C
    else:
        action = D

# Mid-game
elif n/2 <= t < r - n/2:
    if opponent_cooperation_rate > k/n:
        action = C
    elif opponent_cooperation_rate < k/n and increasing():
        cooperate_prob = 0.6
        if random() < cooperate_prob:
            action = C
        else:
            action = D
    else:
        action = D

# Endgame
else:
    if total_payoff_cooperation > total_payoff_defection:
        action = C
    elif opponent_cooperation_rate > k/n:
        action = C
    else:
        action = D
```
Note: This pseudocode provides a high-level overview of the strategy. A more detailed implementation will be required for actual gameplay.
'''

description_COLLECTIVE_409 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to encourage cooperation and set a positive tone.
2. **Subsequent Rounds:**
	* If the number of cooperators in the previous round is greater than or equal to `n/2`, cooperate (C).
	* Otherwise, defect (D) with probability `p = (k/n) * (1 - (previous_cooperators / n))`.
		+ This probability function balances individual self-interest with collective cooperation.
3. **Recent Cooperation Threshold:** Track the number of cooperators in the last `w` rounds (window size). If this count exceeds `(n/2) * w`, cooperate (C) for the next round.

Pseudocode:
```python
def acc_strategy(n, k, r, history):
    if len(history) == 0:  # First Round
        return 'C'

    prev_round_coops = sum(1 for action in history[-1] if action == 'C')
    coop_prob = (k/n) * (1 - (prev_round_coops / n))

    recent_coop_count = sum(sum(1 for action in round_history if action == 'C') 
                            for round_history in history[-w:])

    if prev_round_coops >= n/2 or recent_coop_count > (n/2) * w:
        return 'C'
    else:
        return 'D' with probability coop_prob
```

**Edge Cases:**

* **Last Round:** Cooperate (C) in the last round to ensure a positive final payoff.
* **Ties:** In case of ties, break them randomly between cooperation and defection.

**Collective Mindset Alignment:**
The ACC strategy prioritizes collective cooperation by adapting to the number of cooperators in previous rounds. It incentivizes players to cooperate when enough others do so, creating an upward spiral of cooperation. When cooperation falters, it adjusts its probability of cooperation to balance individual self-interest with collective benefits.

**Tournament Readiness:**
The ACC strategy is designed to perform well against a diverse range of opponent behaviors in the tournament. Its adaptability and probabilistic nature make it robust to various strategies, while still maintaining a strong commitment to collective cooperation.

Please let me know if you'd like me to elaborate on any aspect of this strategy or provide further clarification!
'''

description_COLLECTIVE_410 = '''
**Collective Strategy: Adaptive Cooperate-and-Punish (ACP)**

The ACP strategy aims to balance individual payoffs with collective well-being by adapting to the game's history and encouraging cooperation.

**Decision Rules**

1. **First Round**: Cooperate (C) in the first round, as there is no prior information about opponents' behaviors.
2. **Subsequent Rounds**: Calculate the opponent group's overall cooperation rate (OCR) for the previous round:
	* OCR = Σ(j=1 to n) c_j / n
3. **Cooperate** if:
	* The opponent group's OCR in the previous round is above a threshold τ (0 < τ ≤ 1), indicating sufficient cooperation.
	* OR, the current round is the last round (r-th round).
4. **Defect** if:
	* The opponent group's OCR in the previous round is below or equal to τ.
5. **Punishment Mechanism**: If a player defects while others cooperate, punish them by defecting against them for the next θ rounds (1 ≤ θ < r). This encourages cooperation and deters exploitation.

**Threshold Value (τ) Calculation**

Set τ = 0.5 + (k - n)/n. This threshold balances individual payoffs with collective well-being, taking into account the game's parameters.

**Edge Cases**

* **Last Round**: Cooperate in the last round to maximize total payoff, as there are no future rounds for punishment or retaliation.
* **Only One Opponent**: Always cooperate when facing a single opponent to encourage cooperation and achieve a mutually beneficial outcome.
* **Ties**: In case of ties (e.g., equal OCR values), choose to Cooperate.

**Collective Mindset Alignment**

The ACP strategy aligns with the collective mindset by:

* Encouraging cooperation through punishment mechanisms
* Adapting to opponents' behaviors and responding accordingly
* Balancing individual payoffs with collective well-being

Pseudocode for reference:
```
Initialize τ = 0.5 + (k - n)/n
θ = 1 // Punishment duration

For each round t from 1 to r:
    If t == 1: // First Round
        Cooperate
    Else:
        Calculate OCR from previous round
        If OCR > τ or t == r: // Last Round
            Cooperate
        Else:
            Defect
        EndIf
    EndIf

    // Punishment Mechanism
    For each opponent j that defected in the previous round:
        If θ > 0:
            Defect against opponent j for θ rounds
            Decrement θ by 1
        EndIf
    EndFor
EndFor
```
The ACP strategy is designed to be adaptive, robust, and collective-oriented. It balances individual payoffs with collective well-being while responding to opponents' behaviors, making it suitable for a wide range of game scenarios.
'''

description_COLLECTIVE_411 = '''
I propose a collective strategy for the N-Player Public Goods Game that adapts to the game's history and parameters, while being robust to various opponent behaviors.

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a positive tone and encourage others to cooperate.
2. **Response to Cooperation**: If the number of cooperators in the previous round is above a certain threshold (`coop_threshold`), cooperate (C) in the current round. This threshold is set as `coop_threshold = n * k / (n + k)`, which represents a balance between individual and collective benefits.
3. **Response to Defection**: If the number of cooperators in the previous round is below or equal to `coop_threshold`, defect (D) in the current round, but only if the total payoff from cooperating (`k/n * Σ(j=1 to n) c_j`) is less than a certain fraction (`defect_fraction`) of the maximum possible payoff. Otherwise, cooperate (C). This prevents exploitation and encourages cooperation when it's beneficial.
4. **Punishment**: If the number of cooperators in the previous round is significantly low (i.e., `coop_count < n * 0.2`), defect (D) for a single round as a punishment to deter free-riding.
5. **Learning and Adaptation**: After each round, update the `coop_threshold` value based on the observed cooperation rate in the previous round. If the cooperation rate is high (> 0.8), increase `coop_threshold` by a small amount (`delta = 0.05`). Conversely, if the cooperation rate is low (< 0.2), decrease `coop_threshold` by `delta`. This allows ACO to adapt to changing opponent behaviors and game dynamics.

**Edge Cases:**

* **Last Round**: Cooperate (C) in the last round to maximize collective payoff.
* **Tie-breaking**: In case of a tie between cooperation and defection, cooperate (C) to favor collective benefits.

**Pseudocode:**
```
# Initialize variables
coop_threshold = n * k / (n + k)
defect_fraction = 0.5
delta = 0.05

# First round
if t == 1:
    action[t] = C

# Subsequent rounds
else:
    coop_count = Σ(j=1 to n) c_j[t-1]
    if coop_count > coop_threshold:
        action[t] = C
    elif coop_count <= coop_threshold and k/n * coop_count < defect_fraction * max_payoff:
        action[t] = D
    else:
        action[t] = C

# Punishment
if coop_count < n * 0.2:
    action[t] = D

# Adaptation
if coop_rate > 0.8:
    coop_threshold += delta
elif coop_rate < 0.2:
    coop_threshold -= delta
```
**Collective Mindset:** ACO prioritizes cooperation when beneficial and punishes free-riding to maintain a balance between individual and collective interests. By adapting to the game's history and parameters, ACO promotes a cooperative environment while being robust to various opponent behaviors.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it balances individual self-interest with collective benefits and adapts to changing game dynamics.
'''

description_COLLECTIVE_412 = '''
To design a collective strategy for this N-Player Public Goods Game that is adaptive and robust to various opponent behaviors, we'll employ a mix of cooperation and defection based on game history. This approach aims to balance individual payoff with the promotion of group welfare, as both are crucial in a public goods game.

### Strategy Name: Adaptive Collective Optimism (ACO)

#### Decision Rules:

1. **Initial Cooperation**: In the first round, cooperate (`C`). This sets an optimistic tone and encourages others to cooperate.
   
   ```python
if current_round == 1:
    action = 'C'
```

2. **Reciprocal Cooperation**: If a majority (or more) of players cooperated in the previous round, cooperate in the current round. Otherwise, defect (`D`). This rule promotes reciprocity and encourages cooperation when it's beneficial.

   ```python
elif previous_round_cooperators >= n / 2:
    action = 'C'
else:
    action = 'D'
```

3. **Punishment for Defection**: If the number of cooperators in the previous round was significantly lower than expected (less than `n/4`), defect in the current round to "punish" defectors and signal that cooperation is valued.

   ```python
elif previous_round_cooperators < n / 4:
    action = 'D'
```

4. **Exploration Rounds**: Introduce occasional exploration rounds where, with a small probability (`exploration_probability`), the strategy will randomly choose between `C` and `D`, regardless of the current state. This helps in adapting to changing environments and can encourage cooperation by introducing randomness.

   ```python
import random

if random.random() < exploration_probability:
    action = 'C' if random.random() >= 0.5 else 'D'
```

#### Edge Cases:

- **Last Round**: In the last round, defect (`D`). Since there's no future game to influence with cooperation, maximizing individual payoff takes precedence.

  ```python
if current_round == r:
    action = 'D'
```

- **Tie Situations**: If the number of cooperators is exactly at the threshold for cooperation or punishment (e.g., `n/2` or `n/4`), lean towards cooperation to maintain an optimistic stance.

#### Collective Mindset:

ACO aligns with a collective mindset by:

- Initially cooperating to set a positive tone.
- Cooperating when a majority does, encouraging group welfare.
- Punishing defection to maintain a balance and signal the value of cooperation.
- Incorporating exploration rounds to adapt to changing dynamics and potentially encourage cooperation through unpredictable behavior.

This strategy is designed to be robust against various opponent behaviors by adapting its actions based on the game's history. It promotes cooperation when beneficial, punishes defection, and occasionally explores different strategies to maintain a dynamic approach in a repeated game scenario.
'''

description_COLLECTIVE_413 = '''
**Collective Strategy: "Adaptive Cooperative Tracker" (ACT)**

The Adaptive Cooperative Tracker (ACT) strategy is designed to balance individual self-interest with collective cooperation, leveraging game history and parameters to adapt to various opponent behaviors.

**Decision Rules:**

1. **Initial Rounds**: In the first round, ACT cooperates (C). This sets a positive tone for potential reciprocity.
2. **Tracking Opponent Cooperation**: Maintain a running average of opponents' cooperation rates (`avg_coop_rate`) over the last `min(r/4, 10)` rounds (i.e., at least 10 rounds or a quarter of the total rounds). Calculate this as the number of cooperators divided by the number of players (`n`).
3. **Cooperation Threshold**: Set a cooperation threshold (`coop_threshold`) based on the multiplication factor (`k`): `coop_threshold = k / n`.
4. **Adaptive Cooperation**: In each round, ACT cooperates if:
	* The opponent's average cooperation rate is above the cooperation threshold (`avg_coop_rate >= coop_threshold`).
	* OR, if the total number of cooperators in the previous round was greater than or equal to `n/2`.
5. **Defection Trigger**: If an opponent defects while ACT cooperates, and this happens more than once within a window of `min(r/4, 10)` rounds, ACT will defect for one round.

**Edge Cases:**

* In the last round (`r`), ACT always cooperates to maximize collective payoff.
* If all opponents have defected in every round so far (i.e., no cooperation has been observed), ACT defects from the second round onwards.

**Pseudocode:**
```python
def adaptive_cooperative_tracker(n, k, r):
    avg_coop_rate = 0
    coop_threshold = k / n
    opponent_defections = []

    for t in range(r):
        if t == 0:
            # Initial cooperation
            action = C
        else:
            # Calculate average cooperation rate
            num_cooperators = sum([1 if player_action[t-1] == C else 0 for player_action in history])
            avg_coop_rate = (num_cooperators / n) * (t / r)

            # Check cooperation threshold and opponent behavior
            if avg_coop_rate >= coop_threshold or num_cooperators >= n/2:
                action = C
            elif len(opponent_defections) > 1 and sum([1 for defection in opponent_defections if t - defection <= min(r/4, 10)]) > 0:
                # Defect after repeated opponent defections
                action = D
            else:
                action = C

        # Update history and opponent defections list
        history.append(action)
        if action == D:
            opponent_defections.append(t)

    return history
```
**Collective Mindset:**

ACT prioritizes cooperation when opponents demonstrate a willingness to cooperate, while also being cautious of exploitation. By tracking opponent behavior and adapting to the game's parameters, ACT balances individual self-interest with collective cooperation, aiming to achieve mutually beneficial outcomes.
'''

description_COLLECTIVE_414 = '''
**Collective Strategy: Adaptive Cooperativity (AC)**

The AC strategy aims to balance individual payoff maximization with collective welfare by adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to set a cooperative tone and gather information about opponents' behaviors.
2. **Subsequent Rounds:** Use a combination of tit-for-tat (TFT) and a novel "cooperation threshold" mechanism:
	* If the average cooperation rate among all players (including yourself) in the previous round is above a certain threshold (T), cooperate (C).
	* Otherwise, defect (D).
3. **Threshold Calculation:** Set T = (k - 1) / n, where k is the multiplication factor and n is the number of players.
4. **Punishment Mechanism:** If an opponent defects in a round where you cooperated, "punish" them by defecting in the next round.

**Edge Cases:**

* Last Round: Cooperate (C) to maintain a cooperative reputation and potentially influence future games.
* Opponent Defection Streaks: If an opponent defects for more than 2 consecutive rounds, temporarily switch to an "unforgiving TFT" strategy, where you defect whenever they do.

**Collective Mindset:**

The AC strategy prioritizes collective well-being by:

1. Encouraging cooperation in the initial round and when opponents cooperate.
2. Gradually increasing the cooperation threshold as the game progresses, allowing for more flexibility in responding to changing opponent behaviors.
3. Implementing a punishment mechanism to deter exploitation while maintaining an overall cooperative attitude.

**Pseudocode:**
```python
def adaptive_cooperativity(n, k, r):
  # Initialize variables
  T = (k - 1) / n  # cooperation threshold
  avg_cooperation_rate = 0
  opponent_defection_streaks = {i: 0 for i in range(1, n+1)}

  for t in range(r):
    if t == 0:
      action = 'C'  # cooperate in the first round
    else:
      # calculate average cooperation rate from previous round
      avg_cooperation_rate = sum([1 if opponent_actions[i] == 'C' else 0 for i in range(1, n+1)]) / n

      if avg_cooperation_rate >= T:
        action = 'C'
      else:
        action = 'D'

    # punishment mechanism
    for i in range(1, n+1):
      if opponent_actions[i] == 'D' and my_actions[t-1] == 'C':
        opponent_defection_streaks[i] += 1
        if opponent_defection_streaks[i] > 2:
          action = 'D'

    # update variables for next round
    my_actions.append(action)
    opponent_actions = observe_opponent_actions()
```
This strategy aims to balance individual payoff maximization with collective welfare by adapting to the game's history and opponent behaviors. By prioritizing cooperation and implementing a punishment mechanism, AC encourages opponents to cooperate while minimizing exploitation risks.
'''

description_COLLECTIVE_415 = '''
I propose a collective strategy called "Adaptive Reciprocity with Public Goods Adjustment" (ARPGA). This strategy balances individual self-interest with contributions to the public good while adapting to opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally to establish a cooperative tone and encourage others to contribute.
2. **Reciprocity**: Observe the total number of cooperators (T) in the previous round. If T ≥ n/2 (i.e., at least half of the players cooperated), cooperate in the current round. This encourages reciprocity when cooperation is prevalent.
3. **Public Goods Adjustment**: Calculate the average payoff per cooperator in the previous round (P). If P > 1 (i.e., the public good generated a higher payoff than keeping the endowment privately), increase the likelihood of cooperating in the current round by a factor of k/n. This adjustment accounts for the benefits of contributing to the public good.
4. **Defection Threshold**: Introduce a defection threshold (DT) that increases with each consecutive round where T < n/2. If DT exceeds a certain value (e.g., 3), defect in the current round. This prevents exploitation by persistent defectors.

Pseudocode:
```
if Round == 1:
    cooperate = True
else:
    T = count_cooperators(previous_round)
    P = average_payoff_per_cooperator(previous_round)

    if T >= n/2:
        cooperate = True
    elif P > 1:
        cooperate = random.choice([True, False], p=[k/n, 1 - k/n])
    else:
        DT += 1
        if DT > threshold_value:
            cooperate = False
        else:
            cooperate = random.choice([True, False])

return cooperate
```
**Edge Cases:**

* Last Round: Cooperate unconditionally to maximize the total payoff.
* Opponent Defection: If an opponent defects in a round where you cooperated, adjust your strategy by increasing the defection threshold (DT) more rapidly.
* Consecutive Cooperation: If all players have cooperated for multiple consecutive rounds, maintain cooperation to sustain the public good.

**Collective Mindset:**

The ARPGA strategy is designed to promote cooperation while adapting to diverse opponent behaviors. By initially cooperating and reciprocating cooperative actions, we create an environment conducive to mutual benefit. The public goods adjustment ensures that contributions are valued, and the defection threshold prevents exploitation. This collective strategy encourages players to work together towards a common goal, maximizing overall payoffs in the process.

This strategy will perform well in the tournament as it:

* Encourages cooperation through reciprocity and public goods adjustments
* Adapts to various opponent behaviors, including defectors and cooperators
* Balances individual self-interest with contributions to the public good
'''

description_COLLECTIVE_416 = '''
To design a collective strategy for this N-player public goods game that is adaptive and robust, we'll employ a combination of cooperation and defection based on the history of contributions from other players. Our goal is to balance individual payoff maximization with collective welfare.

**Decision Rules:**

1. **First Round**: Cooperate (C). This sets a positive tone for the game and encourages others to contribute.
2. **Early Rounds (2 ≤ t < r/2)**:
	* If the average contribution rate of other players is above 50% in the previous round, cooperate (C).
	* Otherwise, defect (D) with a probability p = 0.5 - (k/n) \* avg_contributions_last_round.
3. **Middle Rounds (r/2 ≤ t < r - 1)**:
	* If the average contribution rate of other players has increased or remained stable since the last time you defected, cooperate (C).
	* Otherwise, defect (D) with a probability p = 0.5 - (k/n) \* avg_contributions_last_round.
4. **Last Round (t = r)**: Cooperate (C). This encourages others to contribute in the final round and maximizes collective welfare.

**Edge Cases:**

1. **All Players Defected Last Round**: If all players defected in the previous round, cooperate (C) with a probability p = 0.5 + (k/n).
2. **Player i Was Alone in Cooperating**: If player i was the only one to cooperate in the last round, defect (D) with a probability p = 0.5 + (k/n).

**Collective Mindset:**

Our strategy is designed to promote cooperation while adapting to various opponent behaviors. By cooperating in early rounds and responding to changes in contribution rates, we encourage others to contribute to the public good.

Pseudocode:
```python
def collective_strategy(game_history, n, k, t):
    if t == 1:  # First Round
        return 'C'
    
    avg_contributions_last_round = calculate_avg_contributions(game_history, t-1)
    
    if t < r/2:  # Early Rounds
        if avg_contributions_last_round > 0.5:
            return 'C'
        else:
            p = 0.5 - (k/n) * avg_contributions_last_round
            return 'D' if random.random() < p else 'C'
    
    elif t >= r/2 and t < r-1:  # Middle Rounds
        prev_defect_t = find_prev_defect(game_history, t)
        if prev_defect_t is None or avg_contributions_last_round >= calculate_avg_contributions(game_history, prev_defect_t):
            return 'C'
        else:
            p = 0.5 - (k/n) * avg_contributions_last_round
            return 'D' if random.random() < p else 'C'
    
    elif t == r-1:  # Last Round
        return 'C'
    
    def calculate_avg_contributions(game_history, round_num):
        contributions = [game_history[round_num][i] for i in range(n) if game_history[round_num][i] == 'C']
        return len(contributions) / n
    
    def find_prev_defect(game_history, t):
        for i in range(t-1, -1, -1):
            if game_history[i][player_id] == 'D':
                return i
        return None
```
Note that this strategy is a starting point and may be improved upon through further analysis and simulation.
'''

description_COLLECTIVE_417 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

ACE is a robust and adaptive strategy designed for the N-Player Public Goods Game. It balances individual self-interest with collective well-being by escalating cooperation based on observed behavior.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to encourage others to contribute.
2. **Reciprocity**: If a player i observes that at least one other player cooperated in the previous round, they will also cooperate in the current round.
3. **Contribution Threshold**: Introduce a dynamic contribution threshold (T) based on the game's history:
	* T = (average number of cooperators in previous rounds) / n
	* If the total contributions in the previous round exceed T, increase cooperation by 1/n.
	* If the total contributions in the previous round are below T, decrease cooperation by 1/n.
4. **Self-Interest Adjustment**: Adjust cooperation levels based on individual payoff:
	* If a player's payoff is above the average payoff of all players, reduce cooperation by 1/n.
	* If a player's payoff is below the average payoff of all players, increase cooperation by 1/n.
5. **Defection Response**: If a player observes that more than half of the other players defected in the previous round, they will defect (D) in the current round to minimize losses.

**Edge Cases:**

* In the last round, play C if at least one other player cooperated in the second-to-last round; otherwise, play D.
* If all players have defected for two consecutive rounds, cooperate (C) to try to reinitiate cooperation.

**Pseudocode:**
```
Initialize:
  T = 0  // Contribution threshold
  previous_round_cooperators = 0
  previous_round_payoffs = [0] * n

For each round t:
  If t == 1:  // First round
    Play C
  Else:
    If any other player cooperated in round t-1:
      Play C
    Else if total contributions in round t-1 > T:
      Increase cooperation by 1/n
    Else if total contributions in round t-1 < T:
      Decrease cooperation by 1/n
    Else:  // Self-interest adjustment
      If individual payoff > average payoff of all players:
        Reduce cooperation by 1/n
      Else:
        Increase cooperation by 1/n

  Update T:
    T = (previous_round_cooperators + 1) / n

  Observe opponents' actions and update previous_round_payoffs
  Update previous_round_cooperators based on observed cooperation levels

  If more than half of other players defected in round t-1:
    Play D
```
**Collective Mindset:**
ACE aligns with the collective mindset by initially cooperating to encourage others to contribute. It then adapts its cooperation level based on observed behavior, reciprocating cooperation and escalating contributions when possible. By adjusting self-interest levels and responding to defection, ACE balances individual payoff with collective well-being.

This strategy is designed to be robust against a wide range of opponent behaviors and can adapt to different game environments without relying on specific coordination mechanisms or shared norms.
'''

description_COLLECTIVE_418 = '''
**Collective Strategy: "Adaptive Reciprocity with Forgiveness" (ARF)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to initiate cooperation and encourage others to do the same.
2. **Subsequent Rounds:** Use a reciprocity-based approach, considering the previous round's outcome:
	* If the total number of cooperators in the previous round is greater than or equal to half the number of players (n/2), cooperate (C).
	* Otherwise, defect (D) but with a probability of cooperation based on the "forgiveness" mechanism (see below).

**Forgiveness Mechanism:**

Introduce a forgiveness parameter, `f`, which represents the probability of cooperating despite being defected against in previous rounds. Update `f` after each round:

1. Initialize `f = 0.5` ( neutral starting point).
2. If the player's payoff in the previous round was greater than or equal to the average payoff of all players, increase `f` by a small amount (`Δf = 0.05`): `f += Δf`.
3. If the player's payoff in the previous round was less than the average payoff of all players, decrease `f` by `Δf`: `f -= Δf`.

**Reciprocity with Forgiveness:**

In subsequent rounds, if the total number of cooperators is less than half (n/2), use the following probability to decide whether to cooperate:

`P(C) = f * (k/n) / (1 + k/n)`

This formula incorporates the forgiveness parameter `f`, the multiplication factor `k`, and the number of players `n`. The probability of cooperation increases as `f` grows, reflecting a more forgiving attitude.

**Edge Cases:**

* **Last Round:** Cooperate (C) to maintain a cooperative stance, even if others may defect.
* **Ties:** In cases where the total number of cooperators is exactly half (n/2), use the forgiveness mechanism to decide whether to cooperate or defect.

**Pseudocode:**
```python
def ARF(n, k, r):
    f = 0.5  # initialize forgiveness parameter

    for t in range(1, r+1):  # loop over rounds
        if t == 1:
            action = 'C'  # cooperate in first round
        else:
            prev_coop_count = sum([c_i for c_i in prev_round_actions])
            if prev_coop_count >= n/2:
                action = 'C'
            else:
                P_C = f * (k/n) / (1 + k/n)
                action = 'C' if random.random() < P_C else 'D'

        # update forgiveness parameter
        avg_payoff = sum([π_i for π_i in prev_round_payoffs]) / n
        if my_payoff >= avg_payoff:
            f += Δf
        else:
            f -= Δf

        return action
```
This collective strategy, ARF, balances individual self-interest with a willingness to cooperate and reciprocate. By incorporating forgiveness, it can adapt to various opponent behaviors while maintaining a robust and cooperative stance.
'''

description_COLLECTIVE_419 = '''
**Collective Strategy: Adaptive Cooperation with Social Learning (ACSL)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage cooperation and gather information about opponents' behaviors.
2. **Subsequent Rounds:** Observe the previous round's outcomes and adjust your action based on the following rules:
	* If the number of cooperators (including yourself) is greater than or equal to `n/2`, cooperate (C) in the next round.
	* If the number of cooperators is less than `n/2` but the total payoff from cooperation (`k/n * Σ(j=1 to n) c_j`) is greater than or equal to your private payoff from defecting (`1 - c_i`), cooperate (C) in the next round.
	* Otherwise, defect (D) in the next round.
3. **Punishment Mechanism:** If you defected (D) in a previous round and observed that fewer than `n/2` players cooperated, increase your cooperation probability by 10% for the next round.

**Handling Edge Cases:**

* **Last Round:** Cooperate (C) in the last round to maintain social norms and encourage cooperation.
* **Consecutive Defections:** If you've defected (D) consecutively for `r/2` rounds or more, switch to cooperating (C) to avoid being exploited by opponents.

**Collective Mindset:**

The ACSL strategy prioritizes cooperation while adapting to the environment. By initially cooperating and then adjusting based on observed outcomes, we create a social norm of cooperation that encourages others to do the same. The punishment mechanism ensures that defectors are held accountable for their actions, promoting a more cooperative atmosphere.

**Pseudocode:**
```python
def ACSL(n, k, r, history):
  if round == 1:
    return C
    
  # Calculate number of cooperators in previous round
  num_cooperators = sum([c_i for c_i in history[-1]])
  
  # Check if cooperation is beneficial
  total_payoff_coop = (k/n) * num_cooperators
  
  # Decide action based on decision rules
  if num_cooperators >= n/2:
    return C
  elif total_payoff_coop >= 1 - c_i:  # assuming c_i is your previous action
    return C
  else:
    return D
    
  # Punishment mechanism
  if previous_action == D and num_cooperators < n/2:
    increase_cooperation_probability(10%)
    
  return action
```
This strategy balances cooperation with adaptability, making it robust to various opponent behaviors while promoting a collective mindset of cooperation.
'''

description_COLLECTIVE_420 = '''
**Collective Strategy: Adaptive Cooperate-Defect (ACD)**

The ACD strategy aims to balance individual payoff maximization with collective welfare, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage cooperation and gather information about opponents' behavior.
2. **Subsequent Rounds:** Observe the total number of cooperators (Σc_j) and defectors (n - Σc_j) in the previous round. Calculate the average payoff for cooperators (π_C) and defectors (π_D) using the observed actions.

   a. If π_C > π_D, cooperate (C) in the current round.
   
   b. If π_C ≤ π_D, defect (D) in the current round.
3. **Punishment Mechanism:** To maintain cooperation, introduce a punishment mechanism. If the number of cooperators in the previous round falls below a certain threshold (e.g., n/2), cooperate with probability p (0 < p < 1) in the next round. This encourages opponents to cooperate and helps prevent exploitation.
4. **Exploration:** To gather information about opponent strategies, introduce an exploration phase. With a small probability ε (0 < ε < 1), randomly choose to cooperate or defect, regardless of the decision rule outcome.

**Pseudocode:**
```
def ACD(n, k, r):
    # Initialize variables
    total_cooperators = 0
    total_defectors = 0
    pi_C_prev = 0
    pi_D_prev = 0
    
    for t in range(1, r+1):
        if t == 1:
            action = C  # Cooperate in the first round
        else:
            # Calculate average payoffs for cooperators and defectors
            pi_C = (k/n) * total_cooperators + (total_defectors > 0)
            pi_D = (total_cooperators > 0) + (1 - (k/n) * total_cooperators)
            
            if pi_C > pi_D:
                action = C
            else:
                action = D
                
            # Punishment mechanism
            if total_cooperators < n/2 and random.random() < p:
                action = C
            
            # Exploration phase
            if random.random() < ε:
                action = random.choice([C, D])
        
        # Update variables for next round
        total_cooperators += (action == C)
        total_defectors += (action == D)
        
    return action
```
**Edge Cases:**

* **Last Round:** The strategy remains the same as in previous rounds, ensuring consistency and avoiding last-round exploitation.
* **Early Rounds:** The initial cooperation encourages opponents to cooperate, while subsequent rounds adapt to their behavior.

This collective strategy balances individual payoff maximization with collective welfare, adapting to opponent behaviors and game history. By introducing a punishment mechanism and exploration phase, ACD promotes cooperation and robustness in the face of various opponent strategies.
'''

description_COLLECTIVE_421 = '''
**Collective Strategy: "Adaptive Reciprocity with Endgame Cooperation"**

Our strategy aims to balance individual payoff maximization with collective cooperation, adapting to opponent behaviors while ensuring robustness against a wide range of actions.

**Decision Rules:**

1. **Initial Rounds (t < n):** Cooperate (C) in the first round and subsequent rounds until all players have played at least once. This allows us to gather information about opponents' initial behaviors.
2. **Reciprocity Phase (n ≤ t < r - 1):**
	* If the opponent cooperated in the previous round, cooperate (C) with probability p = k/n. This encourages cooperation and punishes defection.
	* If the opponent defected in the previous round, defect (D) with probability p = 1 - k/n. This deters exploitation and promotes fairness.
3. **Endgame Phase (t ≥ r - 1):** Cooperate (C) unconditionally in the last two rounds to maximize collective payoff.

Pseudocode:
```
if t < n:  # Initial Rounds
    action = C
elif n <= t < r - 1:  # Reciprocity Phase
    opponent_prev_action = get_opponent_action(t-1)
    if opponent_prev_action == C:
        action = C with probability k/n
    else:
        action = D with probability 1 - k/n
else:  # Endgame Phase
    action = C
```
**Edge Cases:**

* **First Round:** Cooperate (C) to gather information and encourage cooperation.
* **Last Two Rounds:** Cooperate (C) unconditionally to maximize collective payoff.
* **Opponent Always Defects:** Gradually decrease cooperation probability p over time, eventually defecting (D) if opponent continues to exploit.

**Collective Mindset:**

Our strategy is designed to promote cooperation while being robust against a wide range of opponent behaviors. By reciprocating cooperation and punishing defection, we encourage opponents to cooperate. In the endgame phase, unconditional cooperation maximizes collective payoff.

This adaptive strategy balances individual payoff maximization with collective cooperation, making it a strong contender in the tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_422 = '''
I propose a collective strategy called "Adaptive Cooperative Tit-for-Tat with Forgiveness" (ACTF). This strategy aims to balance individual self-interest with collective well-being while being robust to various opponent behaviors.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to reciprocate.
2. **Subsequent Rounds:** Observe the previous round's actions and payoffs of all players. If the number of cooperators is above a certain threshold (calculated below), cooperate (C). Otherwise, defect (D).
3. **Threshold Calculation:** Calculate the "cooperation ratio" (CR) as the number of cooperators in the previous round divided by the total number of players (n). Set a dynamic threshold (T) based on the game parameters:

T = k / n + (1 - k/n) \* CR

If CR > T, cooperate; otherwise, defect.
4. **Forgiveness Mechanism:** If a player defects after previously cooperating (i.e., they "punish" the group), wait for two consecutive rounds of cooperation from that player before reconsidering cooperation.

**Edge Cases:**

1. **Last Round:** Cooperate in the last round if at least half of the players cooperated in the previous round.
2. **Early Defection:** If a player defects in one of the first three rounds, temporarily switch to an "eye-for-an-eye" strategy (defecting when others defect) for two rounds before reassessing cooperation.
3. **Unanimous Cooperation:** If all players cooperate in a round, increase the threshold T by 10% for the next round to encourage continued cooperation.

**Collective Mindset:**

ACTF prioritizes collective success while protecting individual interests. By initially cooperating and adapting to the group's behavior, ACTF aims to create an environment where cooperation is rewarded. The forgiveness mechanism prevents retaliatory cycles and encourages players to return to cooperation after temporary setbacks.

**Pseudocode (simplified):**
```python
def ACTF(n, k, r, previous_actions):
    # First round: Cooperate
    if r == 1:
        return "C"

    # Calculate cooperation ratio and threshold
    CR = sum(previous_actions) / n
    T = k / n + (1 - k/n) * CR

    # Check forgiveness mechanism
    for player in previous_actions:
        if player["previous_action"] == "D" and player["cooperation_count"] >= 2:
            return "C"

    # Decide action based on threshold
    if CR > T:
        return "C"
    else:
        return "D"

    # Handle edge cases (early defection, unanimous cooperation, etc.)
    # ...
```
This strategy should perform well in a tournament setting by balancing individual self-interest with collective success.
'''

description_COLLECTIVE_423 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds:** Observe the number of cooperators (C_count) and defectors (D_count) in the previous round.

a. If C_count > D_count, cooperate (C) in the current round to reinforce cooperation.

b. If C_count ≤ D_count, defect (D) in the current round with probability P_defect = (1 - (k/n)) ^ C_count. This formula adapts the likelihood of defection based on the previous round's cooperation level and the game parameters.

c. If a player defects, punish them by cooperating only if C_count > D_count + 1 in the subsequent rounds. This ensures that defectors are not immediately rewarded for their actions.

**Edge Cases:**

* **Last Round:** Cooperate (C) in the last round to maximize total payoff and demonstrate commitment to cooperation.
* **Ties:** In case of a tie between C_count and D_count, cooperate (C) with probability 0.5 to maintain a neutral stance.
* **Zero Cooperators:** If no players cooperated in the previous round, defect (D) in the current round to avoid being exploited.

**Collective Mindset:**

ACP is designed to promote cooperation while adapting to various opponent behaviors. By initially cooperating and punishing defectors, ACP aims to create a norm of cooperation. The strategy's adaptive nature allows it to adjust its behavior based on the game's progression and parameters.

**Rationale:**

1. **Initial Cooperation:** Establishing a cooperative tone encourages others to follow, creating a positive feedback loop.
2. **Adaptive Defection:** ACP's defection probability decreases as cooperation increases, making it more likely to cooperate when others do.
3. **Punishment Mechanism:** Punishing defectors discourages exploitation and reinforces the value of cooperation.

**Tournament Readiness:**

ACP is designed to perform well in a tournament setting by:

1. Being robust against various opponent strategies
2. Adapting to different game parameters (n, k)
3. Encouraging cooperation while protecting against exploitation

By following ACP's decision rules and collective mindset, this strategy aims to achieve a high total payoff in the N-Player Public Goods Game tournament.
'''

description_COLLECTIVE_424 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Concession**

This strategy balances individual self-interest with collective well-being by adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Rounds (t = 1 to n):**
	* Cooperate (C) in the first round to encourage cooperation from others.
	* Observe opponents' actions and calculate the average cooperation rate (ACR).
2. **Middle Rounds (t > n):**
	* Calculate ACR for the last `n` rounds (e.g., if n = 6, consider rounds t-5 to t).
	* If ACR ≥ (k - 1)/k:
		+ Cooperate (C) to maintain a high level of collective cooperation.
	* Else:
		+ Defect (D) with probability p = (1 - ACR)/(2 - ACR), and cooperate (C) otherwise.
3. **Final Round (t = r):**
	* If the total payoff from previous rounds is higher than the expected payoff from defecting in the final round, Cooperate (C).
	* Otherwise, Defect (D).

**Pseudocode:**
```python
def adaptive_cooperation(n, k, t, history):
  # Initial Rounds
  if t <= n:
    return 'C'
  
  # Calculate ACR for last `n` rounds
  acr = sum(1 for i in range(t-n+1, t+1) if history[i-1] == 'C') / n
  
  # Middle Rounds
  if acr >= (k - 1)/k:
    return 'C'
  else:
    p_defect = (1 - acr)/(2 - acr)
    return 'D' if random.random() < p_defect else 'C'
  
  # Final Round
  if t == r:
    total_payoff = sum(history[i-1]['payoff'] for i in range(1, t+1))
    expected_final_payoff = k/n * (n - 1)
    return 'C' if total_payoff > expected_final_payoff else 'D'
```
**Collective Mindset:**

This strategy is designed to promote cooperation while being robust against a wide range of opponent behaviors. By adapting to the game's history, it encourages others to cooperate and punishes free-riding behavior. The gradual concession mechanism allows for flexible responses to changing circumstances.

**Edge Cases:**

* In the first round, this strategy cooperates to set a positive tone for the game.
* In the final round, it makes an informed decision based on the accumulated payoff from previous rounds.

This collective strategy balances individual self-interest with collective well-being, making it suitable for a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_425 = '''
**Collective Strategy: Adaptive Cooperation with Self-Interest (ACSI)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage cooperation and gather information about opponents' behaviors.
2. **Subsequent Rounds**: Observe the total number of cooperators (Σ(j=1 to n) c_j) in the previous round.
	* If the total number of cooperators is greater than or equal to half of the players (n/2), Cooperate (C).
	* Otherwise, Defect (D).
3. **Exception Handling**: If a player defects while the total number of cooperators is greater than or equal to n/2, switch to Defect (D) in the next round.
4. **Endgame**: In the last 10% of rounds (0.1r), prioritize Self-Interest:
	* Cooperate (C) only if the expected payoff from cooperation exceeds the private payoff from defecting.

**Pseudocode:**
```
// Initialize variables
cooperate = true; // First round cooperation flag
total_cooperators_prev = 0; // Previous round's total cooperators

for (round = 1 to r):
    if (round == 1):
        cooperate = true;
    else:
        // Observe previous round's total cooperators
        total_cooperators_prev = ...;

        // Update cooperation decision based on previous round's information
        if (total_cooperators_prev >= n/2):
            cooperate = true;
        else:
            cooperate = false;

        // Exception handling: punish defection in cooperative environments
        if (cooperate && total_cooperators_prev > n/2 && player_i_defected_last_round):
            cooperate = false;

    // Endgame prioritization of Self-Interest
    if (round >= 0.9r): // Last 10% of rounds
        expected_payoff_from_cooperation = ...;
        private_payoff_from_defection = ...;

        if (expected_payoff_from_cooperation > private_payoff_from_defection):
            cooperate = true;

    // Play the game based on the cooperation decision
    play(cooperate);
```
**Rationale:**

1. **Initial Cooperation**: Encourage opponents to cooperate and gather information about their behaviors.
2. **Adaptive Cooperation**: Cooperate when the collective benefit is high (total cooperators ≥ n/2), and defect otherwise.
3. **Punishment Mechanism**: Defect in response to defection in cooperative environments to maintain social norms.
4. **Endgame Self-Interest**: Prioritize individual payoffs in the final rounds to maximize overall payoff.

**Collective Mindset:**

ACSI aligns with a collective mindset by:

1. Encouraging cooperation through initial cooperation and adaptive decision-making.
2. Punishing defection in cooperative environments to maintain social norms.
3. Balancing individual self-interest with collective benefits in the endgame.

This strategy is robust against various opponent behaviors, as it adapts to the game's history and prioritizes both collective cooperation and individual self-interest.
'''

description_COLLECTIVE_426 = '''
**Collective Strategy: Adaptive Cooperation with History-Based Threshold (ACHT)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage cooperation and create a positive initial condition.
2. **Subsequent Rounds (t>1):** Observe the previous round's total contributions (Σc_j,t-1). Calculate a cooperation threshold (θ_t) based on the game's history.

Pseudocode:
```
θ_t = (k/n) * (∑c_j,t-1 / n)
if θ_t > 0.5 then
  Cooperate (C)
else
  Defect (D)
endif
```
Here, θ_t represents the average payoff from cooperation in the previous round, scaled by the multiplication factor k and the number of players n. If this threshold exceeds 0.5, it indicates that cooperation was relatively successful in the previous round, and our strategy will cooperate again.

3. **Edge Case: Last Round (t=r):** Defect (D) to maximize individual payoff in the final round, as there are no future rounds to consider.
4. **Handling Deviations:** If a player observes a significant deviation from the expected cooperation level (e.g., more than 20% of players defecting), adjust the cooperation threshold θ_t by increasing it slightly (e.g., +0.1) for the next round. This allows our strategy to adapt to potential exploitation.

**Collective Mindset:**

Our strategy, ACHT, aligns with a collective mindset in several ways:

* **Encouraging Cooperation:** By cooperating in the initial round and using a history-based threshold, we create an environment where cooperation can thrive.
* **Adaptability:** Our strategy adapts to changing game conditions, such as deviations from expected cooperation levels, to ensure long-term collective success.
* **Robustness:** ACHT is designed to perform well against a wide range of opponent behaviors, including defectors and other adaptive strategies.

**Additional Considerations:**

* **Exploration-Exploitation Trade-off:** Our strategy balances exploration (cooperating in the initial round) with exploitation (defecting in the final round).
* **Robustness to Noise:** ACHT is relatively robust to noisy or imperfect observations of opponent actions, as it relies on aggregated historical data rather than individual player actions.

By implementing this collective strategy, we aim to create a robust and adaptive approach that promotes cooperation while being resilient to various opponent behaviors in the N-Player Public Goods Game.
'''

description_COLLECTIVE_427 = '''
**Collective Strategy: Adaptive Tit-for-Tat with Public Goods Awareness (ATTPGA)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to encourage others to contribute and build a strong public good.
2. **Tit-for-Tat Component**: Observe the number of cooperators in the previous round (`c_prev`). If `c_prev` is above the average cooperation rate (`avg_c`) over all previous rounds, cooperate (C). Otherwise, defect (D).
3. **Public Goods Awareness**: Monitor the total payoff generated by the public good (`total_payoff`) and compare it to the expected payoff if everyone cooperated (`k * n`). If `total_payoff` is below this threshold, adjust the cooperation rate based on the difference.
4. **Adaptive Adjustment**: Calculate an adjustment factor (`adj_factor`) as a function of the difference between `total_payoff` and the expected payoff. If `total_payoff` is low, increase the likelihood of cooperating in the next round.

**Pseudocode:**
```
// Initialize variables
avg_c = 0; total_payoff = 0; adj_factor = 1;

FOR EACH ROUND:
  // Observe previous round's cooperation rate and total payoff
  c_prev = count(cooperators) / n;
  total_payoff += Σ(j=1 to n) π_j,t-1;

  IF FIRST_ROUND THEN
    action = COOPERATE (C);
  ELSE
    // Tit-for-Tat component
    IF c_prev > avg_c THEN
      action = COOPERATE (C);
    ELSE
      action = DEFECT (D);

    // Public Goods Awareness and Adaptive Adjustment
    expected_payoff = k * n;
    payoff_diff = expected_payoff - total_payoff;
    adj_factor = 1 + (payoff_diff / (k * n));

    IF random() < adj_factor THEN
      action = COOPERATE (C);

  // Update average cooperation rate and total payoff
  avg_c += c_prev / r;

// Return the chosen action
RETURN action;
```
**Edge Cases:**

* **First Round**: Cooperate to encourage others to contribute.
* **Last Round**: Use the same decision rules as in previous rounds, but consider the final round's impact on overall payoffs when making the decision.
* **Consecutive Defections**: If all players defect consecutively for multiple rounds, ATTPGA will adapt by increasing the likelihood of cooperation.

**Collective Mindset:**

ATTPGA is designed to align with a collective mindset by:

1. Encouraging cooperation in early rounds to establish a strong public good.
2. Adapting to changes in cooperation rates and total payoffs to maintain an optimal balance between individual and collective interests.
3. Fostering cooperation through the tit-for-tat mechanism, which rewards cooperation and punishes defection.

By incorporating both tit-for-tat and public goods awareness components, ATTPGA aims to achieve a high level of cooperation while remaining robust against various opponent behaviors.
'''

description_COLLECTIVE_428 = '''
**Collective Strategy: "Adaptive Cooperative Threshold" (ACT)**

The Adaptive Cooperative Threshold (ACT) strategy is designed to balance individual self-interest with collective cooperation, adapting to the evolving game dynamics and opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 50% to establish a baseline for cooperation.
2. **History-Dependent Threshold**: Calculate the **Cooperation Rate (CR)** as the ratio of total cooperative actions to total rounds played so far. Set a threshold value, **θ**, based on the game parameters: θ = min(0.5, k/n).
3. **Adaptive Cooperation**: In each subsequent round, cooperate (C) if the current CR is above θ; otherwise, defect (D).
4. **Exploration and Exploitation**: With a small probability (ε = 0.1), randomly choose to cooperate or defect, regardless of the threshold condition. This introduces exploration and helps adapt to changing opponent behaviors.

**Handling Edge Cases:**

* **Last Round**: In the final round, always defect (D) to maximize individual payoff.
* **Tie-Breaking**: If the CR is exactly equal to θ, randomly choose between cooperating and defecting with a 50% probability.

**Collective Mindset Alignment:**

ACT prioritizes cooperation when the collective cooperation rate is above the threshold, aligning with the collective interest. By adapting to the game's history and opponent behaviors, ACT balances individual self-interest with collective cooperation, promoting a mutually beneficial outcome.

Pseudocode:
```
Initialize CR = 0, θ = min(0.5, k/n), ε = 0.1

For each round t from 1 to r:
    If t == 1:  // First round
        Cooperate with probability 50%
    Else:
        Calculate current CR based on history
        If CR > θ:
            Cooperate (C)
        Else:
            Defect (D)
        With probability ε, randomly choose C or D instead

If last round:
    Defect (D)

Return total payoff over r rounds
```
This strategy will adapt to various opponent behaviors and game dynamics, promoting cooperation while ensuring individual self-interest is protected.
'''

description_COLLECTIVE_429 = '''
**Collective Strategy: Adaptive Cooperation with History-Dependent Adjustment**

**Decision Rules:**

1. **Initial Rounds**: Cooperate (C) for the first two rounds to establish a cooperative tone and gather information about opponents' behaviors.
2. **Cooperation Threshold**: Calculate the cooperation rate of all players in the previous round (t-1). If the cooperation rate is above a certain threshold (CT), cooperate in the current round (t). Otherwise, defect.

 CT = k/n \* (average number of cooperators in previous rounds)

The intuition behind this rule is to maintain cooperation as long as enough players are contributing to the public good. The threshold value adjusts based on the game's parameters and history.
3. **Punishment Mechanism**: If a player defects while others cooperate, introduce a temporary punishment phase. For a set number of rounds (P), defect against any opponent who defected in the previous round. This encourages cooperation by making defection less attractive when others are cooperating.

 P = min(r/4, 3) ( punish for at most r/4 rounds or 3 rounds, whichever is smaller)
4. **Learning and Adaptation**: Continuously update the cooperation threshold (CT) based on the game's history. If cooperation rates increase over time, adjust CT upward to maintain a higher level of cooperation.

**Handling Edge Cases:**

1. **First Round**: Cooperate (C).
2. **Last Round**: Defect (D), as there is no future round to influence.
3. **Ties in Cooperation Rate**: In case of a tie, prioritize cooperating with players who have cooperated more frequently in the past.

**Collective Mindset Alignment:**

This strategy aligns with a collective mindset by:

1. Encouraging cooperation through initial cooperation and a cooperation threshold.
2. Punishing defection to maintain cooperation.
3. Adapting to changing cooperation rates to maximize overall payoffs.

By incorporating elements of reciprocity, punishment, and adaptation, this strategy aims to promote cooperation while being robust against various opponent behaviors.

Pseudocode (for illustration purposes only):
```
 Initialize:
  CT = k/n \* (average number of cooperators in previous rounds)
  P = min(r/4, 3)

For each round t:
  If t <= 2: Cooperate (C)
  Else:
    Calculate cooperation rate in previous round (t-1)
    If cooperation rate >= CT: Cooperate (C)
    Else: Defect (D)

  For each opponent j:
    If j defected in previous round and P > 0:
      Punish by defecting against j
      Decrement P

  Update CT based on game history
```
This strategy will be implemented as an algorithm to compete in the tournament.
'''

description_COLLECTIVE_430 = '''
I propose a collective strategy called " Adaptive Tit-for-Tat with Public Goods Sensitivity" (ATTPGS). This strategy aims to balance individual self-interest with the desire to contribute to the public good.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round, as it sets a positive tone and encourages others to cooperate.
2. **Subsequent Rounds:** Use a tit-for-tat approach based on the number of cooperators in the previous round:
	* If the number of cooperators is greater than or equal to `n/2` (i.e., at least half of the players cooperated), cooperate (C).
	* Otherwise, defect (D).
3. **Public Goods Sensitivity:** Adjust the cooperation threshold based on the multiplication factor `k`. When `k` is high (close to `n`), increase the cooperation threshold to encourage more cooperation.

Pseudocode:
```
IF first_round THEN
  cooperate = TRUE
ELSE
  num_cooperators_prev_round = count(cooperators in prev_round)
  IF num_cooperators_prev_round >= n/2 AND k > (n-1)/2 THEN
    cooperate = TRUE
  ELSE
    cooperate = FALSE

  // Adjust cooperation threshold based on public goods sensitivity
  IF k > (n-1)/4 THEN
    cooperation_threshold = n/3
  ELSE
    cooperation_threshold = n/2
```
**Handling Edge Cases:**

* **Last Round:** In the last round, defect (D) to maximize individual payoff, as there are no future rounds to benefit from cooperation.
* **Ties:** If the number of cooperators is exactly `n/2`, cooperate (C) to maintain a positive tone and encourage others to cooperate.

**Collective Mindset:**

ATTPGS aligns with the collective mindset by:

1. Encouraging cooperation in early rounds to establish a positive atmosphere.
2. Responding to the actions of other players, promoting reciprocity and cooperation.
3. Adjusting its strategy based on the public goods sensitivity factor `k`, which reflects the importance of cooperation.

**Robustness:**

ATTPGS is designed to be robust against various opponent behaviors:

1. **Pure Defectors:** By defecting when fewer than half of players cooperate, ATTPGS avoids being exploited by pure defectors.
2. **Cooperative Opponents:** By cooperating when at least half of players cooperate, ATTPGS rewards cooperative opponents and encourages them to continue cooperating.

**Tournament Performance:**

ATTPGS is designed to perform well in a tournament setting:

1. **Initial Rounds:** By starting with cooperation, ATTPGS establishes a positive tone and sets the stage for future rounds.
2. **Adaptability:** As the game progresses, ATTPGS adapts its strategy based on the actions of other players and the public goods sensitivity factor `k`.
3. **Robustness to Opponent Strategies:** By responding to the number of cooperators in previous rounds, ATTPGS can effectively counter various opponent strategies.

Overall, ATTPGS offers a balanced approach that balances individual self-interest with the desire to contribute to the public good, making it a robust and adaptive strategy for the N-Player Public Goods Game.
'''

description_COLLECTIVE_431 = '''
I propose a collective strategy called "Adaptive Cooperative Threshold" (ACT) for the N-Player Public Goods Game. ACT balances individual self-interest with cooperation to maximize overall payoff.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and gather information about other players' behaviors.
2. **Subsequent Rounds**: Use a threshold-based approach to decide between Cooperate (C) and Defect (D). Calculate the average cooperation rate of all players in the previous round (`avg_coop_prev`).

`avg_coop_prev = Σ(j=1 to n) c_j,t-1 / n`

where `c_j,t-1` is the action (0 for D, 1 for C) taken by player j in the previous round.

3. **Cooperation Threshold**: Define a threshold value (`theta`) that represents the minimum average cooperation rate required for ACT to cooperate.

`theta = k/n * (n/2)` (i.e., half of the expected payoff from full cooperation)

4. **Decision Logic**:

`if avg_coop_prev >= theta and rounds_remaining > 1:`
    `Action = C` (Cooperate)
`elif avg_coop_prev < theta or rounds_remaining == 1:`
    `Action = D` (Defect)

**Rationale:**

* In the first round, cooperating sets a positive tone and provides information about other players' behaviors.
* The threshold-based approach allows ACT to adapt to changing cooperation rates. If most players cooperate, ACT will continue to cooperate; otherwise, it will defect to protect its individual payoff.
* The `theta` value ensures that ACT only cooperates when the expected payoff from cooperation is at least half of the maximum possible payoff.

**Edge Cases:**

* **Last Round**: In the final round (`rounds_remaining == 1`), ACT defects regardless of the average cooperation rate. This ensures maximum individual payoff, as cooperation no longer affects future rounds.
* **Ties and Zero Cooperation**: If `avg_coop_prev` is exactly equal to `theta`, or if all players defected in the previous round (i.e., `avg_coop_prev == 0`), ACT will cooperate. This helps maintain a cooperative atmosphere and encourages others to cooperate.

**Collective Mindset:**

ACT prioritizes cooperation when it is likely to benefit from collective action, while also protecting its individual interests when necessary. By adapting to changing cooperation rates, ACT promotes a stable and mutually beneficial environment for all players.

This strategy should perform well in a tournament setting, as it balances self-interest with cooperation and adapts to various opponent behaviors.
'''

description_COLLECTIVE_432 = '''
**Collective Strategy: "Adaptive Cooperation with Punishment" (ACP)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to cooperate.
2. **Subsequent Rounds:**
	* If the total number of cooperators in the previous round is greater than or equal to `n/2` (i.e., at least half of the players cooperated), Cooperate (C).
	* Otherwise, Defect (D) to punish non-cooperative behavior and encourage others to cooperate.
3. **Punishment Mechanism:** If a player defects in a round where the total number of cooperators is less than `n/2`, they will be punished by defecting in the next round as well.

**Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round, regardless of previous rounds' outcomes, to maximize collective payoff.
2. **Tiebreakers:** In cases where the total number of cooperators is exactly `n/2`, Cooperate (C) to maintain a cooperative atmosphere.

**Collective Mindset:**

The ACP strategy prioritizes cooperation and mutual benefit while incorporating a punishment mechanism to deter non-cooperative behavior. By cooperating in the initial round and subsequent rounds with sufficient cooperation, we encourage others to cooperate, maximizing collective payoff. The punishment mechanism ensures that players who defect when cooperation is low are held accountable, promoting a culture of cooperation.

**Pseudocode:**
```python
def ACP(n, k, r):
    # Initialize variables
    total_cooperators = 0
    previous_round_defectors = []

    for round in range(r):
        if round == 0:
            action = "C"  # Cooperate in the first round
        else:
            total_cooperators = sum(1 for player in players if player.action == "C")
            if total_cooperators >= n/2:
                action = "C"
            elif previous_round_defectors:
                action = "D"  # Punish non-cooperative behavior
            else:
                action = "D"

        # Update variables
        previous_round_defectors = [player for player in players if player.action == "D"]

    return action
```
This strategy is designed to be adaptive, robust, and aligned with the collective mindset. It encourages cooperation while incorporating a punishment mechanism to deter non-cooperative behavior, making it suitable for a wide range of opponent behaviors.
'''

description_COLLECTIVE_433 = '''
**Collective Strategy: Adaptive Cooperativeness (AC)**

The AC strategy balances individual self-interest with collective welfare, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to establish a baseline for cooperation.
2. **Reciprocal Cooperativeness**: After the first round, calculate the average cooperation rate of all opponents in the previous round:

`avg_coop_prev = (total coop actions in prev round) / n`

If `avg_coop_prev ≥ 0.5`, cooperate (C). Otherwise, defect (D).
3. **Self-Interest Adjustment**: If your payoff in the previous round was below a certain threshold (`payoff_threshold`), adjust your strategy to prioritize self-interest:

* If your payoff was lower than the average payoff of all players in the previous round, defect (D) in this round.
* Otherwise, maintain the reciprocal cooperativeness decision from step 2.

The `payoff_threshold` is set to 75% of the maximum possible payoff for a single round (`k - 1/n`). This value can be adjusted based on game parameters and testing results.

**Handling Edge Cases:**

* **Last Round**: In the final round, prioritize self-interest by defecting (D) if your accumulated payoff is below `payoff_threshold`. Otherwise, maintain reciprocal cooperativeness.
* **Opponent Defection**: If an opponent defects in a round where you cooperated, reduce your cooperation rate in subsequent rounds. This is achieved by decreasing the `avg_coop_prev` threshold by 10% (or another suitable value) for each instance of opponent defection.

**Collective Mindset:**

The AC strategy aligns with the collective mindset by:

* Encouraging initial cooperation to establish a cooperative atmosphere.
* Adapting to the average cooperation rate of opponents, promoting reciprocal cooperativeness.
* Prioritizing self-interest only when necessary, maintaining a balance between individual and collective welfare.

**Pseudocode (for illustration purposes):**

```
def adaptive_cooperativeness(n, k, r, history):
  # Initialize variables
  avg_coop_prev = None
  payoff_threshold = 0.75 * (k - 1/n)

  for round in range(r):
    if round == 0:
      action = 'C'  # Initial cooperation
    else:
      # Calculate average cooperation rate of opponents
      avg_coop_prev = calculate_avg_coop(history, n)
      
      # Reciprocal cooperativeness decision
      if avg_coop_prev >= 0.5:
        action = 'C'
      else:
        action = 'D'

      # Self-interest adjustment
      if get_payoff(history, round - 1) < payoff_threshold:
        if get_payoff(history, round - 1) < calculate_avg_payoff(history, n):
          action = 'D'

    # Update history and return action
    history.append(action)
    return action

def calculate_avg_coop(history, n):
  # Calculate average cooperation rate of opponents in previous round
  coop_actions = sum(1 for action in history[-n:] if action == 'C')
  return coop_actions / n

def get_payoff(history, round):
  # Retrieve payoff for a given round
  # ...

def calculate_avg_payoff(history, n):
  # Calculate average payoff of all players in previous round
  payoffs = [get_payoff(history, round) for round in range(len(history) - n, len(history))]
  return sum(payoffs) / n
```

This pseudocode provides a basic structure for implementing the AC strategy. The actual implementation will require adjustments and optimizations based on game parameters and testing results.
'''

description_COLLECTIVE_434 = '''
**Collective Strategy: "Adaptive Tit-for-Tat with Gradual Cooperation"**

This strategy is designed to balance individual self-interest with collective well-being, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to initiate cooperation and encourage others to follow.
2. **General Rule:** For rounds 2 to r-1:
	* If the total number of cooperators in the previous round is greater than or equal to the multiplication factor k, cooperate (C).
	* Otherwise, defect (D).
3. **Last Round (r):** Defect (D) in the last round to maximize individual payoff, as cooperation will not be reciprocated.

**Adaptive Mechanism:**

To adapt to opponent behaviors and promote collective cooperation, introduce a gradual cooperation mechanism:

1. Track the number of consecutive rounds where the total number of cooperators is greater than or equal to k.
2. If this count exceeds 50% of the total rounds played so far (i.e., r/2), increase the threshold for cooperation by 1. This means that more cooperators are required in the previous round to trigger cooperation.
3. Conversely, if the count falls below 25% of the total rounds played (i.e., r/4), decrease the threshold by 1.

**Pseudocode:**
```python
def adaptive_tit_for_tat(n, k, r):
    # Initialize variables
    consecutive_cooperators = 0
    cooperation_threshold = k

    for t in range(r):
        if t == 0:
            action = 'C'  # Cooperate in the first round
        elif t == r - 1:
            action = 'D'  # Defect in the last round
        else:
            previous_cooperators = count_cooperators(t-1)
            if previous_cooperators >= cooperation_threshold:
                action = 'C'
                consecutive_cooperators += 1
            else:
                action = 'D'

            # Adapt cooperation threshold
            if consecutive_cooperators > r / 2:
                cooperation_threshold += 1
            elif consecutive_cooperators < r / 4:
                cooperation_threshold -= 1

        return action
```
**Rationale:**

This strategy balances individual self-interest with collective well-being by:

* Cooperating in the first round to initiate cooperation and encourage others to follow.
* Adapting to opponent behaviors by adjusting the cooperation threshold based on the number of consecutive rounds with sufficient cooperators.
* Defecting in the last round to maximize individual payoff, as cooperation will not be reciprocated.

By gradually increasing or decreasing the cooperation threshold, this strategy promotes collective cooperation while remaining robust to a wide range of opponent behaviors.
'''

description_COLLECTIVE_435 = '''
Here's a collective strategy that adapts to various opponent behaviors and aligns with the game's parameters:

**Strategy Name:** Adaptive Cooperative Threshold (ACT)

**Decision Rules:**

1. **Initial Cooperation**: Cooperate in the first round to establish a baseline for cooperation.
2. **Adaptive Threshold**: Maintain a cooperation threshold, `T`, initialized to 0.5. Update `T` each round based on the proportion of cooperators in the previous round:

   ```python
def update_threshold(T, prev_cooperators, n):
    return (prev_cooperators / n) * 0.9 + T * 0.1
```

3. **Cooperation Rule**: Cooperate if the expected payoff from cooperation (`E[coop]`) exceeds the expected payoff from defection (`E[defect]`). Calculate these expectations based on the previous round's actions and payoffs:

   ```python
def calculate_expectations(prev_cooperators, n, k):
    E_coop = (k / n) * prev_cooperators + 0.5
    E_defect = 1 + (k / n) * prev_cooperators - 0.5
    return E_coop, E_defect

def should_cooperate(E_coop, E_defect, T):
    return E_coop > E_defect and random.random() < T
```

4. **Punishment Mechanism**: If the proportion of cooperators in the previous round falls below `T`, defect with a probability proportional to the difference between `T` and the actual cooperation rate:

   ```python
def should_punish(T, prev_cooperators, n):
    punishment_prob = max(0, (T - prev_cooperators / n) * 2)
    return random.random() < punishment_prob
```

**Edge Cases:**

* **Last Round**: Defect in the last round to maximize personal payoff.
* **Only One Opponent**: Cooperate if the opponent cooperated in the previous round; otherwise, defect.

**Collective Mindset Alignment**:

The ACT strategy prioritizes cooperation while adapting to the group's behavior. By maintaining a dynamic threshold and incorporating randomness in decision-making, it encourages cooperation without being overly exploitable. The punishment mechanism discourages free-riding, promoting a more equitable distribution of payoffs.

This strategy should perform well against various opponent behaviors, as it balances individual self-interest with collective cooperation goals.
'''

description_COLLECTIVE_436 = '''
I propose a collective strategy called "Adaptive Collective Conscience" (ACC) for the N-Player Public Goods Game. ACC aims to balance individual self-interest with collective well-being by adaptively adjusting cooperation levels based on game history.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = 0.5. This allows for exploration and sets a neutral tone.
2. **Reactive Cooperation**: For rounds t > 1:
	* If the average payoff of all players in the previous round (t-1) is greater than or equal to the multiplication factor k, cooperate (C).
	* Otherwise, defect (D).
3. **Adaptive Adjustment**: After each round, update the cooperation probability p_t for the next round based on the collective performance:
	+ If the total payoff of all players in round t is higher than in round t-1, increase p_t by 10% (up to a maximum of 0.9).
	+ If the total payoff decreases or remains the same, decrease p_t by 5% (down to a minimum of 0.1).

**Edge Case Handling:**

* **Last Round**: In the final round (t = r), always cooperate (C). This ensures that all players contribute to the public good and maximizes collective payoff.
* **Early Rounds**: For rounds t ≤ 3, follow the initial cooperation rule (p_init = 0.5). After this period, ACC's adaptive adjustment mechanism takes over.

**Collective Mindset:**

ACC prioritizes collective well-being by cooperating when the group's average payoff is high and adjusting cooperation levels based on collective performance. This approach encourages players to work together towards a common goal while allowing for individual adaptability.

**Pseudocode (simplified):**
```
initialize p_init = 0.5

for t = 1 to r:
  if t == 1 or t <= 3:
    cooperate with probability p_init
  else:
    if average_payoff(t-1) >= k:
      cooperate
    else:
      defect
    
  update_p_t()
  
  // calculate total payoff for this round
  total_payoff[t] = sum_of_player_payoffs
  
  // update cooperation probability for next round
  if total_payoff[t] > total_payoff[t-1]:
    p_t += 0.10 (up to max 0.9)
  else:
    p_t -= 0.05 (down to min 0.1)

function update_p_t():
  // adjust cooperation probability based on collective performance
```
By using ACC, players can adapt to changing circumstances and opponent behaviors while maintaining a focus on collective well-being. This strategy should perform well in a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_437 = '''
**Collective Strategy: Adaptive Cooperativity with Social Learning**

Our strategy, "Adaptive Cooperativity with Social Learning" (ACSL), aims to balance individual payoffs with collective well-being while adapting to diverse opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) to initiate cooperation and encourage mutual reciprocation.
2. **Subsequent Rounds (t>1)**:
	* If the total number of cooperators in the previous round is greater than or equal to n/2, cooperate (C). This reinforces collective cooperation when it's prevalent.
	* Otherwise, defect (D) with a probability p = k/n. This introduces a "selfish" component to adapt to low-cooperation environments.
3. **Social Learning**: Update the probability of cooperating based on the previous round's outcome:
	+ If your payoff π_i,t was higher than the average payoff of defectors in that round, increase p by 0.1 (up to a maximum of k/n).
	+ If your payoff π_i,t was lower than the average payoff of cooperators in that round, decrease p by 0.1 (down to a minimum of 0).

**Edge Cases:**

* **Last Round (t=r)**: Cooperate (C) to maximize collective payoffs, as there's no future rounds to adapt to.
* **Ties**: In case of ties in the number of cooperators or defectors, follow the decision rule based on the previous round.

**Collective Mindset:**

ACSL prioritizes cooperation when it's beneficial for the group and adapts to situations where defection is more rewarding. By incorporating social learning, our strategy promotes a culture of reciprocity and cooperation while allowing for self-interested decisions when necessary.

Pseudocode:
```python
def ACSL(n, k, r):
  p = 0.5  # initial probability of cooperating
  for t in range(1, r+1):
    if t == 1:
      action = 'C'
    else:
      num_cooperators_prev_round = count_cooperators(t-1)
      if num_cooperators_prev_round >= n/2:
        action = 'C'
      else:
        action = 'D' if random.random() < p else 'C'

    # social learning
    payoff_i_t = calculate_payoff(action, t)
    avg_defector_payoff = average_payoff('D', t)
    avg_cooperator_payoff = average_payoff('C', t)
    if payoff_i_t > avg_defector_payoff:
      p = min(p + 0.1, k/n)
    elif payoff_i_t < avg_cooperator_payoff:
      p = max(p - 0.1, 0)

  return action
```
This strategy balances individual self-interest with collective cooperation and adapts to various opponent behaviors, making it a robust and effective approach in the N-Player Public Goods Game tournament.
'''

description_COLLECTIVE_438 = '''
To design a collective strategy for this N-Player Public Goods Game that is adaptive and robust against various opponent behaviors, we'll employ a mixed approach combining elements of reciprocity, cooperation incentives, and exploitation detection. This strategy aims to maximize collective payoffs while safeguarding against free riders.

**Strategy Name: Adaptive Collective Optimizer (ACO)**

### Decision Rules:

1. **Initial Rounds:** In the first few rounds (e.g., `r_initial = 3`), ACO starts with a cooperative stance (`C`) to encourage cooperation and set a positive tone for collective play. This phase helps in establishing an initial level of trust among players.

2. **Reciprocity Phase:** After the initial rounds, ACO enters a reciprocity phase where it observes the actions of other players and adjusts its strategy accordingly.
   - If the majority (more than half) of players cooperated (`C`) in the previous round, ACO chooses to cooperate (`C`).
   - Otherwise, if less than or exactly half cooperated, ACO defects (`D`). This rule encourages cooperation by reciprocating cooperative behavior but also deters exploitation.

3. **Exploitation Detection and Response:** To handle potential exploiters who consistently defect while others cooperate, ACO implements a simple yet effective detection mechanism.
   - If in any round, the player notices that their payoff is significantly lower than what they would have achieved if everyone had cooperated (`k/n * n`), AND more than half of other players are cooperating, then:
     - ACO defects (`D`) in the next round to signal discontent and deter exploitation.
     - After this response, ACO reverts to its standard reciprocity rule unless such exploitation is detected again.

4. **Final Rounds:** In the last few rounds (e.g., `r_final = 3`), ACO switches to a defection (`D`) strategy regardless of previous outcomes. This adjustment is to maximize individual payoff in a game where long-term cooperation might not be reciprocated near its end.

### Handling Edge Cases:

- **First Round:** Always cooperate (`C`).
- **Last Round(s):** Always defect (`D`).
- **Ties and Thresholds:** In cases of ties or when deciding based on thresholds (e.g., determining if "more than half" cooperated), ACO uses a conservative approach that slightly favors cooperation but doesn't compromise its core principles.

### Collective Mindset Alignment:

ACO is designed with a collective mindset, aiming to promote widespread cooperation while ensuring no single player can exploit the group's cooperative nature without consequences. By initially cooperating and reciprocating cooperation, ACO fosters an environment conducive to collective benefits. Its detection mechanism for exploitation helps maintain fairness within the game.

**Pseudocode Overview:**

```
Initialize r_initial, r_final
For each round t from 1 to r:
    If t <= r_initial:
        Play C
    Else if t >= (r - r_final):
        Play D
    Else:
        Observe previous round's actions
        If majority cooperated:
            Play C
        Else:
            Play D
        Check for exploitation
        If exploited and more than half cooperated:
            Next round, play D to signal and deter

Return total payoff over all rounds
```

This strategy is adaptive as it changes behavior based on the actions of others and robust because it doesn't rely on specific coordination mechanisms or norms beyond its initial cooperative phase.
'''

description_COLLECTIVE_439 = '''
I propose a collective strategy called "Adaptive Cooperative Threshold" (ACT). ACT is designed to balance individual self-interest with collective well-being, while being robust to various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This allows for exploration and sets a tone for potential cooperation.
2. **History-Based Cooperation**: For subsequent rounds (t > 1), calculate the average cooperation rate of all players in the previous round (t-1):

   `avg_coop_prev = Σ(c_j,t-1) / n`

   where `c_j,t-1` is player j's action in round t-1 (1 if C, 0 if D).

3. **Threshold-Based Cooperation**: Cooperate (C) in the current round (t) if:

   `avg_coop_prev ≥ k/n`

   This means that if the average cooperation rate in the previous round exceeds the threshold `k/n`, cooperate in the current round.

4. **Defection Threshold**: If the condition above is not met, defect (D) with probability 0.5. Otherwise, cooperate (C).

**Edge Cases:**

1. **First Round**: As mentioned earlier, cooperate with probability 0.5.
2. **Last Round**: In the final round (t = r), always defect (D). This is because there are no future rounds to influence, and individual self-interest takes precedence.
3. **Tiebreaker**: If `avg_coop_prev` equals exactly `k/n`, cooperate (C) with probability 0.5.

**Collective Mindset:**

ACT aims to create a collective environment where cooperation emerges as a stable equilibrium. By conditioning cooperation on the average cooperation rate, ACT encourages players to contribute to the public good when others do so as well. The threshold-based approach allows for adaptability and responsiveness to changes in opponent behavior.

Pseudocode:
```
function act(n, k, r):
  // Initialize variables
  avg_coop_prev = 0
  history = []

  for t = 1 to r:
    if t == 1:
      // Initial cooperation
      action = random_choice([C, D], p=[0.5, 0.5])
    else:
      // Calculate average cooperation rate in previous round
      avg_coop_prev = sum(history[t-1]) / n

      // Threshold-based cooperation
      if avg_coop_prev >= k/n:
        action = C
      else:
        // Defection threshold
        action = random_choice([C, D], p=[0.5, 0.5])

    // Update history and play action
    history.append(action)
    play_action(action)

    // Last round: always defect
    if t == r:
      action = D

return act
```
ACT is designed to be a robust and adaptive strategy that balances individual self-interest with collective well-being, while being responsive to various opponent behaviors.
'''

description_COLLECTIVE_440 = '''
To design a collective strategy for this N-Player Public Goods Game that adapts to various opponent behaviors and focuses on maximizing collective welfare while ensuring robustness against exploitation, we'll implement a hybrid approach combining elements of cooperative and defecting strategies. The core idea is to balance individual payoff maximization with maintaining cooperation levels that benefit the group as a whole.

### Strategy Name: Adaptive Collective Maximization (ACM)

**Decision Rules:**

1. **Initial Rounds:** In the first few rounds (say, r/4), play Cooperate (C) unconditionally. This initial phase aims to establish a baseline of cooperation and observe how other players react.
   
2. **Observation Phase:** After the initial cooperative phase, enter an observation period where you play Defect (D) for one round while observing others' actions. This gives insight into their strategies without immediately impacting the group's cooperation level significantly.

3. **Adaptive Response:**
   - If during the observation round, at least half of the players cooperate, switch back to Cooperate (C) for the next few rounds.
   - Otherwise, alternate between Cooperate and Defect every other round. This phase is crucial as it tries to maintain a balance that encourages cooperation while not letting others exploit your constant cooperation.

4. **Punishment Mechanism:** Implement a simple punishment mechanism where if you observe less than 1/n of players cooperating (a threshold indicating widespread defection) for two consecutive rounds, play Defect for the next round as a form of deterrence to encourage others to cooperate again.

5. **Endgame Strategy:** In the last few rounds (say, r/4), regardless of the current state, switch to Cooperate if the average cooperation rate over the game so far is above 0.5; otherwise, play Defect. This phase aims to either capitalize on established cooperation or minimize losses in a defect-dominated environment.

**Pseudocode:**

```
# Initialization
n = number_of_players
r = number_of_rounds
k = multiplication_factor
history = []
threshold_cooperation = 0.5

for round in range(1, r+1):
    if round <= r/4:
        # Initial cooperation phase
        action = 'C'
    elif round == r/4 + 1:
        # Observation round
        action = 'D'
        observe_others()
    else:
        # Adaptive response phase
        if observed_cooperation_rate >= threshold_cooperation:
            action = 'C'
        else:
            action = alternate_action('C', 'D')
        
        # Punishment mechanism
        if widespread_defection_detected():
            action = 'D'
    
    # Endgame strategy adjustment
    if round > r - r/4:
        if average_cooperation_rate > threshold_cooperation:
            action = 'C'
        else:
            action = 'D'

    play(action)
    update_history()
```

**Collective Mindset Alignment:**

The ACM strategy is designed with a collective mindset, focusing on maximizing the group's payoff while protecting against exploitation. It encourages cooperation by initially cooperating and punishing widespread defection but remains adaptive to avoid being exploited by persistent defectors. This balance makes it robust in a wide range of scenarios, from highly cooperative to highly competitive environments.

**Tournament Readiness:**

ACM is designed to perform well in tournaments where strategies are unknown and diverse. Its adaptability allows it to respond effectively to both cooperative and defecting opponents, making it a strong candidate for achieving high collective payoffs without relying on coordination or shared norms among players.
'''

description_COLLECTIVE_441 = '''
**Collective Strategy: Adaptive Reciprocity with Endgame Cooperation**

This strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds (t ≤ n):** Cooperate (C) in the first round to encourage cooperation and gather information about other players' behaviors.
2. **Adaptive Reciprocity:** For rounds t > n, calculate the average cooperation rate of all players (excluding yourself) over the last w rounds (w = min(t-1, 5)). If this average is above a threshold θ (θ = 0.6), cooperate; otherwise, defect.
3. **Punishment Mechanism:** If another player defects while you cooperated in the previous round, defect in the current round to discourage exploitation.
4. **Endgame Cooperation:** In the last m rounds (m = min(r/2, 5)), switch to unconditional cooperation (C) to maximize collective payoffs.

**Edge Cases:**

1. **First Round:** Cooperate (C) as per the initial rounds rule.
2. **Last Round:** If r is even and m > 0, cooperate (C); otherwise, follow the adaptive reciprocity rule.
3. **Single-Player Deviation:** If a single player deviates from cooperation in a round where all others cooperated, punish them by defecting in the next round.

**Pseudocode:**
```python
def collective_strategy(game_params):
    n = game_params['n']
    r = game_params['r']
    k = game_params['k']

    # Initialize variables
    history = []  # store cooperation rates for each player
    w = min(r-1, 5)  # window size for averaging cooperation rates
    theta = 0.6  # threshold for adaptive reciprocity
    m = min(r/2, 5)  # number of endgame rounds

    for t in range(1, r+1):
        if t <= n:
            action = 'C'  # initial rounds: cooperate
        else:
            avg_coop_rate = calculate_average_cooperation(history, w)
            if avg_coop_rate > theta:
                action = 'C'  # adaptively reciprocate cooperation
            elif was_punished(history):
                action = 'D'  # punish exploitation
            else:
                action = 'D'  # default to defection

        if t >= r - m + 1:  # endgame cooperation
            action = 'C'

        history.append(action)

    return action

def calculate_average_cooperation(history, w):
    # Calculate average cooperation rate of all players (excluding yourself) over the last w rounds
    coop_rates = []
    for player_history in history[-w:]:
        coop_rate = sum(1 for action in player_history if action == 'C') / len(player_history)
        coop_rates.append(coop_rate)
    return sum(coop_rates) / len(coop_rates)

def was_punished(history):
    # Check if another player defected while you cooperated in the previous round
    prev_round = history[-2]
    return any(action == 'D' for action in prev_round) and history[-1] == ['C']
```
This strategy balances individual self-interest with collective well-being by:

* Encouraging cooperation through initial rounds of cooperation
* Adapting to the game's history through reciprocity mechanisms
* Punishing exploitation to maintain cooperation
* Cooperating unconditionally in endgame rounds to maximize collective payoffs

The `collective_strategy` function takes the game parameters as input and returns the chosen action ('C' or 'D') based on the adaptive reciprocity with endgame cooperation strategy.
'''

description_COLLECTIVE_442 = '''
**Collective Strategy: Adaptive Cooperative Tit-for-Tat (ACT4T)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to reciprocate.
2. **Subsequent Rounds**: Use a tit-for-tat approach, modified to adapt to the collective behavior of other players.

Pseudocode:
```python
def ACT4T(history):
    if len(history) == 0:  # First round
        return C

    # Calculate cooperation rate in previous round
    prev_round_coop_rate = sum(1 for action in history[-1] if action == C) / n

    if prev_round_coop_rate >= k/n:  # Sufficient collective cooperation
        return C
    else:
        return D
```
**Edge Cases:**

* **Last Round**: Cooperate (C) in the last round, as there is no opportunity for retaliation or reciprocity.
* **Single Defector**: If only one player defected in the previous round, cooperate (C) to encourage them to return to cooperation.
* **Multiple Defectors**: If multiple players defected in the previous round, defect (D) to punish non-cooperative behavior.

**Collective Mindset:**

The ACT4T strategy aligns with the collective mindset by:

1. Encouraging cooperation through initial cooperation and tit-for-tat reciprocity.
2. Adapting to the collective behavior of other players, rewarding cooperation and punishing defection.
3. Fostering a culture of mutual cooperation by cooperating when sufficient others do so.

**Rationale:**

The ACT4T strategy is designed to be robust against various opponent behaviors while promoting collective cooperation. By adapting to the cooperation rate in previous rounds, it encourages players to cooperate when enough others do so, creating a positive feedback loop. The tit-for-tat approach ensures that non-cooperative behavior is punished, maintaining a balance between cooperation and defection.

**Expected Outcomes:**

In a tournament setting, ACT4T is expected to perform well against independent strategies, as it:

1. Encourages cooperation through initial cooperation and reciprocity.
2. Adapts to the collective behavior of other players, reducing exploitation by defectors.
3. Punishes non-cooperative behavior, maintaining a balance between cooperation and defection.

By aligning with the collective mindset and adapting to the game's history, ACT4T aims to achieve high payoffs while promoting a cooperative environment in the N-Player Public Goods Game.
'''

description_COLLECTIVE_443 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Forgiveness**

This strategy aims to balance individual self-interest with collective well-being, adapting to various opponent behaviors while promoting cooperation.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2): Cooperate (C)** - Start by cooperating in the first two rounds to establish a cooperative tone and encourage others to do the same.
2. **Recent Opponent Behavior:** Observe the number of cooperators (c) and defectors (d) among opponents in the last round (t-1).
	* If c ≥ d, Cooperate (C) - Continue cooperating if most opponents cooperated recently.
	* If c < d, Defect (D) - Switch to defecting if most opponents defected recently.
3. **Gradual Forgiveness:** After a defection, gradually forgive opponents by increasing the threshold for cooperation over time.
	* Introduce a forgiveness counter (f) initialized to 0 after each defection.
	* Increment f by 1 each round after a defection.
	* If c + f ≥ d, Cooperate (C) - Forgive opponents and cooperate if their recent behavior improves or enough rounds have passed since the last defection.

**Edge Cases:**

1. **Last Round (t = r): Defect (D)** - In the final round, defect to maximize individual payoff regardless of opponent behavior.
2. **Tiebreaker:** If c = d in a given round, Cooperate (C) - Break ties in favor of cooperation to maintain a collective mindset.

**Collective Mindset:**

This strategy prioritizes cooperation while adapting to changing circumstances. By initially cooperating and gradually forgiving opponents, we create an environment conducive to mutual cooperation. The gradual forgiveness mechanism allows for recovery from temporary setbacks, promoting long-term collective success.

Pseudocode (simplified):
```python
def adaptive_cooperation(n, k, r, history):
  # Initial rounds: Cooperate
  if t <= 2:
    return 'C'

  # Recent opponent behavior
  c = sum(1 for action in history[-1] if action == 'C')
  d = n - c

  # Gradual forgiveness
  f = 0
  if history[-1][i] == 'D':
    f += 1

  # Decision rules
  if c >= d or (c + f) >= d:
    return 'C'
  else:
    return 'D'

  # Last round: Defect
  if t == r:
    return 'D'
```
This strategy balances individual self-interest with collective well-being, making it a robust and adaptive approach for the N-Player Public Goods Game.
'''

description_COLLECTIVE_444 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

ACP is a collective strategy that balances individual self-interest with group cooperation, adapting to the game's history and robust against various opponent behaviors.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally.
2. **Reciprocal Cooperation**: If the majority of players (> n/2) cooperated in the previous round, cooperate in the current round.
3. **Punishment for Defection**: If a player defected (D) in the previous round while most others cooperated (> n/2), defect against that player in the next round.
4. **Exploitation Prevention**: If a player consistently defects (D) over multiple rounds, cooperate only if doing so would increase the total payoff for all players.

**Pseudocode**
```markdown
# Initialize variables
n_players = n  # number of players
k_factor = k  # multiplication factor
round_num = 0  # current round

while round_num < r:  # repeat for each round
    # Get previous round's actions and payoffs (if applicable)
    prev_actions = get_previous_round_actions()
    prev_payoffs = get_previous_round_payoffs()

    if round_num == 0:  # First round
        action = C  # Cooperate unconditionally

    elif most_cooperated(prev_actions):  # Reciprocal cooperation
        action = C

    elif punished_defector(prev_actions, prev_payoffs):  # Punishment for defection
        action = D

    elif prevent_exploitation(prev_actions, prev_payoffs):  # Exploitation prevention
        action = C if shared_payoff_increase() else D

    else:  # Default to cooperation if no condition met
        action = C

    # Take the chosen action and observe outcomes
    take_action(action)
    round_num += 1
```
**Edge Cases**

* **Last Round**: In the final round, defect (D) unconditionally to maximize individual payoff.
* **Ties**: If there's a tie in the number of cooperators and defectors, cooperate (C).

**Collective Mindset**

ACP aims to balance individual self-interest with group cooperation by:

1. Encouraging reciprocal cooperation through majority-based decision-making.
2. Punishing defectors who exploit the system.
3. Preventing exploitation by adjusting behavior based on shared payoffs.

By adapting to the game's history and responding to various opponent behaviors, ACP promotes a collective mindset that balances individual interests with group cooperation, making it a robust strategy in a tournament setting.
'''

description_COLLECTIVE_445 = '''
I propose a collective strategy called "Adaptive Cooperative Threshold" (ACT). ACT is designed to balance individual self-interest with collective well-being, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if k > n/2; otherwise, defect (D).
2. **Cooperation Threshold**: Calculate a cooperation threshold, τ, based on the previous round's total contributions:
   - If the number of cooperators is greater than or equal to τ, cooperate (C); otherwise, defect (D).
3. **Adaptive Adjustment**: Update τ after each round using exponential smoothing:
   - τ_new = (1 - α) × τ_old + α × (#cooperators_prev_round / n)
   - Choose a suitable value for the smoothing factor α, e.g., 0.2.
4. **Punishment Mechanism**: If the number of cooperators in the previous round is less than τ and your payoff was lower than the average payoff of defectors, defect (D) in the next round.

**Handling Edge Cases:**

1. **Last Round**: In the final round, cooperate (C) if the expected payoff from cooperation is higher than or equal to the expected payoff from defection.
2. **Early Rounds**: If the game has fewer rounds left than a predetermined threshold (e.g., 5), prioritize individual self-interest and defect (D).
3. **Extremely High/Low k Values**: If k approaches n, cooperate (C) more frequently; if k is close to 1, defect (D) more often.

**Collective Mindset Alignment:**

ACT aims to create a balance between individual incentives and collective benefits. By adapting to the game's history and parameters, ACT encourages cooperation when it is likely to be reciprocated and defects when exploitation is likely.

**Pseudocode Example:**
```markdown
# Define parameters
n = number of players
k = multiplication factor
r = number of rounds
α = smoothing factor

# Initialize variables
τ = 0 (cooperation threshold)
prev_round_cooperators = 0

for round in range(r):
    # Calculate cooperation threshold for this round
    τ = (1 - α) * τ + α * prev_round_cooperators / n
    
    # Decide action based on threshold and previous round's payoffs
    if number of cooperators >= τ:
        play C
    else:
        play D
        
    # Update variables for next round
    prev_round_cooperators = count number of cooperators in this round
```
ACT is a flexible strategy that responds to the game's dynamics, seeking to maximize collective payoffs while protecting individual interests.
'''

description_COLLECTIVE_446 = '''
I propose a collective strategy called "Adaptive Cooperative Fingerprint" (ACF) for the N-Player Public Goods Game. ACF aims to balance individual self-interest with the need for cooperation to achieve higher total payoffs.

**Decision Rules:**

1. **Initial Exploration** (Rounds 1-3): Cooperate (C) in the first three rounds to gather information about other players' behavior and test their willingness to cooperate.
2. **Fingerprint Identification**: After Round 3, calculate the "Cooperation Fingerprint" (CF) of each player by averaging their cooperation rate over Rounds 1-3. This will help identify cooperative and non-cooperative players.

Pseudocode:
```
def calculate_cooperation_fingerprint(player_history):
    return sum(player_history[:4]) / len(player_history[:4])
```

3. **Adaptive Cooperation**: In each subsequent round, use the following decision rules based on the CF values of other players:

   a. If most players (≥ n/2) have a high CF (> 0.5), Cooperate (C).
   
   b. If most players have a low CF (< 0.5), Defect (D).
   
   c. If the number of cooperative and non-cooperative players is balanced, choose an action based on the "Self-Interest Balance" (SIB):
      
      i. SIB = (k/n) × total_contributions - private_cost_of_cooperation
      
      ii. If SIB > 0, Cooperate (C). Otherwise, Defect (D).
   
4. **Punishment and Forgiveness**: Implement a simple punishment mechanism to deter non-cooperative behavior:

   a. If a player defects while most others cooperate (> n/2), mark that player as "non-cooperative" for the next round.
   
   b. In the subsequent round, Defect (D) against the marked player.

5. **Last Round**: In the final round, Cooperate (C) if most players have cooperated in previous rounds, and Defect (D) otherwise.

**Handling Edge Cases:**

* First Round: Always Cooperate (C).
* Last Round: Follow the last-round decision rule.
* When a player's CF value is exactly 0.5 or when there are equal numbers of cooperative and non-cooperative players, use SIB to make a decision.

ACF aims to create an environment where cooperation can emerge as a stable strategy while minimizing exploitation by free-riders. The initial exploration phase provides information about other players' willingness to cooperate. By using the Cooperation Fingerprint and Self-Interest Balance, ACF adapts to various opponent behaviors, making it robust and collective.

This strategy balances individual self-interest with the need for cooperation, aligning with the collective mindset of maximizing total payoffs in the N-Player Public Goods Game.
'''

description_COLLECTIVE_447 = '''
I propose a collective strategy called "Adaptive Cooperation with Gradual Concession" (ACGC). ACGC aims to balance cooperation and self-interest while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage cooperation and gather information about opponents' behavior.
2. **Subsequent Rounds:** Observe the previous round's outcomes and adjust your action accordingly:
	* If the total payoff from the previous round is greater than or equal to 0.5 × k, cooperate (C). This indicates that enough players cooperated to create a beneficial public good.
	* Otherwise, defect (D) if the number of cooperators in the previous round is below n/2; otherwise, cooperate (C).
3. **Concession Mechanism:** If you defected in the previous round and observed that more than half of the players cooperated, concede by cooperating (C) in the current round.
4. **Reciprocity:** If a player has consistently defected against you (i.e., never cooperated with you), defect (D) against them until they cooperate.

**Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round, as there is no future benefit from defecting.
2. **Single Player Deviates:** If only one player deviated from cooperation in the previous round, and you cooperated, continue cooperating (C) to encourage them to return to cooperation.

**Collective Mindset:**

ACGC prioritizes collective well-being while allowing for individual self-interest. By initially cooperating and adapting to opponents' behavior, ACGC promotes cooperation and the creation of public goods. The concession mechanism helps to resolve conflicts and encourages opponents to cooperate. Reciprocity ensures that players who consistently defect against you are held accountable.

**Pseudocode:**
```markdown
# Initialize variables
cooperate = True  # Initial round action
total_payoff_prev = 0
num_cooperators_prev = 0

for round in range(1, r+1):
    if round == 1:
        # First round
        cooperate = True
    else:
        # Adapt to previous round's outcomes
        if total_payoff_prev >= 0.5 * k:
            cooperate = True
        elif num_cooperators_prev < n/2:
            cooperate = False
        else:
            cooperate = True

        # Concession mechanism
        if not cooperate and num_cooperators_prev > n/2:
            cooperate = True

    # Reciprocity
    for opponent in range(1, n+1):
        if opponent != player_id and opponent_defected[opponent]:
            cooperate_opponent = False
        else:
            cooperate_opponent = cooperate

    # Last round
    if round == r:
        cooperate = True

    # Update variables for next round
    total_payoff_prev = calculate_total_payoff(cooperate, num_cooperators)
    num_cooperators_prev = count_cooperators()
```
This pseudocode provides a basic structure for implementing the ACGC strategy. Note that this is not a complete implementation and may require additional details to fully implement the strategy in an algorithm.
'''

description_COLLECTIVE_448 = '''
**Collective Strategy: Adaptive Cooperation with Retaliation (ACR)**

**Decision Rules:**

1. **Initial Rounds**: Cooperate (C) for the first 2 rounds to encourage cooperation and build a cooperative environment.
2. **Observation Phase**: Observe opponents' actions for the next 2 rounds (rounds 3-4). Calculate the average cooperation rate among all opponents, `avg_coop_rate`.
3. **Adaptive Cooperation**:
	* If `avg_coop_rate` ≥ 0.5, cooperate (C) with a probability of `k/n`, where `k` is the multiplication factor and `n` is the number of players.
	* Otherwise, defect (D).
4. **Retaliation Mechanism**: If an opponent defects (D) in a round, retaliate by defecting (D) against that opponent for the next 2 rounds.

**Edge Cases:**

1. **Last Round**: Cooperate (C) unconditionally to maximize total payoff.
2. **Single Defector**: If only one opponent defects, cooperate (C) to maintain cooperation among other players.
3. **All Defectors**: If all opponents defect, defect (D) as well.

**Pseudocode:**
```python
def ACR(n, k, r):
    # Initialize variables
    avg_coop_rate = 0
    coop_prob = k / n
    
    # Initial rounds: Cooperate for the first 2 rounds
    if round_num <= 2:
        return C
    
    # Observation phase (rounds 3-4)
    elif round_num <= 4:
        avg_coop_rate = calculate_avg_coop_rate(opponents' actions)
    
    # Adaptive cooperation
    else:
        if avg_coop_rate >= 0.5:
            return C with probability coop_prob
        else:
            return D
    
    # Retaliation mechanism
    for opponent in opponents:
        if opponent.defected and round_num - opponent.defection_round <= 2:
            return D against opponent

# Helper function to calculate average cooperation rate
def calculate_avg_coop_rate(actions):
    coop_count = sum(1 for action in actions if action == C)
    return coop_count / len(actions)
```
This strategy balances individual self-interest with collective well-being by:

* Encouraging initial cooperation to build a cooperative environment.
* Adapting to opponents' behavior based on their average cooperation rate.
* Retaliating against defectors to maintain cooperation among other players.

By being collective and adaptive, ACR aims to achieve a high total payoff while being robust to various opponent behaviors.
'''

description_COLLECTIVE_449 = '''
**Collective Strategy: Adaptive Tit-for-Tat with Community-Sensing**

This strategy aims to balance individual self-interest with collective well-being by adapting to the community's behavior while encouraging cooperation.

**Decision Rules:**

1. **Initial Round (t=1): Cooperate (C)**
In the first round, cooperate to signal a willingness to contribute to the public good and encourage others to do so.
2. **Subsequent Rounds (t>1):**
a. **Calculate Community Cooperation Level (CCL)**:
 Compute the proportion of players who cooperated in the previous round (t-1).
b. **Adaptive Tit-for-Tat**:
If CCL ≥ 0.5, cooperate (C) in the current round.
Otherwise, defect (D).

**Edge Cases:**

* **Last Round (t=r):** Cooperate (C), as there is no future round to affect and cooperation can still contribute to the public good.
* **Single Opponent:** Always cooperate (C), as the game becomes a prisoner's dilemma, and cooperation is the dominant strategy.

**Collective Mindset:**

This strategy aligns with the collective mindset by:

* Encouraging initial cooperation to set a positive tone for the community.
* Adapting to the community's behavior, rather than relying on individual self-interest alone.
* Fostering cooperation when the community is willing to cooperate (CCL ≥ 0.5).
* Gradually adjusting behavior in response to changes in community cooperation levels.

**Community-Sensing Mechanism:**

The CCL calculation serves as a simple community-sensing mechanism, allowing the strategy to adapt to the overall level of cooperation in the community. This approach enables the strategy to respond effectively to various opponent behaviors and fosters collective cooperation.

Pseudocode:
```python
def adaptive_tit_for_tat(n, k, r, history):
    if len(history) == 0:  # First round
        return 'C'
    
    CCL = sum(1 for action in history[-1] if action == 'C') / n
    
    if CCL >= 0.5:
        return 'C'
    else:
        return 'D'

def play_game(n, k, r):
    history = []
    for t in range(r):
        actions = [adaptive_tit_for_tat(n, k, r, history) for _ in range(n)]
        history.append(actions)
    
    return history
```
This strategy will be implemented as an algorithm and tested against other AI systems in a tournament setting.
'''

description_COLLECTIVE_450 = '''
**Collective Strategy: "Adaptive Cooperation with Punishment"**

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust strategy accordingly:
	* If the majority of players (> n/2) cooperated in the previous round, Cooperate (C).
	* If fewer than half of the players cooperated in the previous round, Defect (D) with a probability p = 1 - (k/n) \* Σ(j=1 to n) c_j (where c_j is the cooperation rate of player j in the previous round). Otherwise, Cooperate (C).
3. **Punishment Mechanism:** If a player observes another player defecting while they cooperated in the previous round, they will Defect (D) with probability 0.5 in the current round.
4. **Last Round:** Always Defect (D), as there is no future opportunity for cooperation or punishment.

**Pseudocode:**
```python
def adaptive_cooperation(n, k, history):
    if len(history) == 0:  # First round
        return C
    prev_round = history[-1]
    coop_count = sum(1 for action in prev_round if action == C)
    maj_coop = coop_count > n / 2
    
    if maj_coop:
        return C
    else:
        p_defect = 1 - (k / n) * coop_count
        return D if random.random() < p_defect else C

def punishment_mechanism(history):
    for i, action in enumerate(history[-1]):
        if action == D and history[-2][i] == C:
            return D if random.random() < 0.5 else C
    return None

def strategy(n, k, history):
    if len(history) >= r - 1:  # Last round
        return D
    elif punishment_mechanism(history) is not None:
        return punishment_mechanism(history)
    else:
        return adaptive_cooperation(n, k, history)
```
**Collective Mindset Alignment:**

This strategy prioritizes cooperation while adapting to the behavior of other players. By cooperating in the first round and punishing defectors, we promote a cooperative atmosphere. The probability of defecting when fewer than half of the players cooperated encourages others to cooperate and maintain a high overall cooperation rate.

**Robustness to Opponent Behaviors:**

This strategy is designed to be robust against various opponent behaviors:

* **Cooperative opponents:** Our strategy will adapt to their cooperation, maintaining a high collective payoff.
* **Defector opponents:** The punishment mechanism discourages repeated defection, promoting a more cooperative environment.
* **Mixed-strategy opponents:** Our adaptive approach allows us to adjust to changing opponent behaviors, minimizing the impact of mixed strategies.

By implementing this collective strategy, we aim to create an environment where cooperation is encouraged and rewarded, leading to higher overall payoffs for all players in the tournament.
'''

description_COLLECTIVE_451 = '''
**Collective Strategy: "Adaptive Collective Optimism" (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 0.8, and defect (D) with a probability of 0.2. This initial cooperation encourages others to cooperate while allowing for some exploration.
2. **Reciprocal Altruism**: If at least 50% of players cooperated in the previous round, cooperate (C) in the current round. This promotes reciprocity and reinforces cooperative behavior.
3. **Punish Defection**: If fewer than 50% of players cooperated in the previous round, defect (D) with a probability of 0.7, and cooperate (C) with a probability of 0.3. This punishes defectors while leaving room for forgiveness and potential cooperation.
4. **Exploit Greed**: If all players defected in the previous round, cooperate (C) with a probability of 0.5, and defect (D) with a probability of 0.5. This attempts to exploit greedy behavior by introducing cooperation and potentially creating a better outcome.

**Edge Cases:**

1. **Last Round**: In the final round, always cooperate (C). This ensures that even if others have defected, you contribute to the public good.
2. **Early Rounds**: For rounds 2-5, adjust the probability of cooperation in the Reciprocal Altruism rule to 0.9 (from 1) to encourage early cooperation.

**Collective Mindset:**

The ACO strategy aligns with a collective mindset by:

* Encouraging initial cooperation and reciprocity
* Punishing defection while allowing for forgiveness
* Attempting to exploit greedy behavior

**Pseudocode:**
```python
def Adaptive_Collective_Optimism(n, k, r):
  # Initialize variables
  cooperate_prob = 0.8  # Initial cooperation probability
  punish_defect_prob = 0.7  # Probability of defecting when punishing

  for t in range(r):
    if t == 0:  # First round
      action = 'C' if random.random() < cooperate_prob else 'D'
    elif sum(cooperate_history) >= n / 2:  # Reciprocal Altruism
      action = 'C'
    elif sum(cooperate_history) < n / 2:  # Punish Defection
      action = 'D' if random.random() < punish_defect_prob else 'C'
    elif all(defected):  # Exploit Greed
      action = 'C' if random.random() < 0.5 else 'D'

    # Update cooperate_history and defected variables
    cooperate_history.append(action == 'C')
    defected = all(not c for c in cooperate_history)

    if t == r - 1:  # Last round
      action = 'C'
```
This strategy balances cooperation, reciprocity, and punishment to achieve a collective optimum. Its adaptability and robustness make it suitable for a wide range of opponent behaviors in the tournament.
'''

description_COLLECTIVE_452 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Concession**

Our collective strategy, "Adaptive Cooperation with Gradual Concession" (ACGC), aims to balance individual self-interest with cooperation to achieve a mutually beneficial outcome.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Post-Initial Rounds**: Use the following decision rule:
	* If the average cooperation rate of all players in the previous round is above a threshold `θ` (see below), cooperate (C).
	* Otherwise, defect (D) with probability `p`, where `p` is inversely proportional to the number of rounds remaining.
3. **Concession Mechanism**: Gradually concede by increasing the probability of defection (`p`) as the game progresses and the average cooperation rate remains low.

**Threshold `θ` Calculation:**
Set `θ` to be a function of the multiplication factor `k` and the number of players `n`. Specifically, `θ = (k/n) / 2`. This ensures that if most players cooperate, the public good share is substantial enough to incentivize continued cooperation.

**Pseudocode:**

```
function ACGC(n, k, r, history):
  // Initialize variables
  θ = (k/n) / 2
  p = 0

  for round t from 1 to r:
    if t == 1:  // Initial Round
      action = C
    else:
      avg_cooperation_rate = calculate_average_cooperation_rate(history, t-1)
      if avg_cooperation_rate >= θ:
        action = C
      else:
        p = (r - t) / r  // Gradual concession
        action = D with probability p

    return action
```

**Handling Edge Cases:**

* **Last Round**: In the final round, defect (D) to maximize individual payoff, as there are no future rounds to consider.
* **Early Rounds**: If a player observes widespread defection in early rounds, they may choose to defect more frequently to avoid being exploited.

**Collective Mindset:**
ACGC is designed to promote cooperation while allowing for adaptation to various opponent behaviors. By gradually conceding and increasing the probability of defection, players can signal their willingness to cooperate but also protect themselves from exploitation. This strategy encourages a collective mindset by:

* Rewarding cooperation with continued cooperation
* Punishing widespread defection with increased defection
* Gradually adapting to changing circumstances

ACGC is robust against a wide range of opponent behaviors and does not rely on specific coordination mechanisms or norms. Its adaptability and concession mechanism make it an effective strategy in a tournament setting.
'''

description_COLLECTIVE_453 = '''
**Collective Strategy: Adaptive Cooperativeness (AC)**

**Overview**

The Adaptive Cooperativeness (AC) strategy is designed to balance individual self-interest with collective well-being in the N-Player Public Goods Game. AC adapts to the game's history, making decisions based on the behavior of other players and the overall performance of the group.

**Decision Rules**

1. **Initial Round**: In the first round, cooperate (C) with a probability p=0.5. This sets a neutral tone for the game.
2. **Subsequent Rounds**: Calculate the average cooperation rate (ACR) among all players in the previous round:

   ACR = (number of cooperators in previous round) / n

   If ACR ≥ 0.5, cooperate (C) with probability p=1 - (1-ACR)^k/n.
   Otherwise, defect (D).

**Pseudocode**
```markdown
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
cooperation_history = []  # store cooperation rates for each round

# First round: random cooperation with p=0.5
if current_round == 1:
    cooperate_probability = 0.5
else:
    # Calculate average cooperation rate (ACR) in previous round
    ACR = sum(cooperation_history[-1]) / n
    
    # Adapt cooperation probability based on ACR
    if ACR >= 0.5:
        cooperate_probability = 1 - (1-ACR)**(k/n)
    else:
        cooperate_probability = 0

# Make decision
if random.random() < cooperate_probability:
    action = C  # Cooperate
else:
    action = D  # Defect

# Update cooperation history
cooperation_history.append([player_i == C for player_i in players])
```
**Edge Cases**

* **Last Round**: AC behaves the same as in other rounds, as there is no clear benefit to defecting or cooperating in the last round.
* **Ties**: In case of a tie (e.g., equal numbers of cooperators and defectors), AC will adapt its cooperation probability based on the average cooperation rate.

**Collective Mindset**

AC aims to promote collective well-being by:

1. Encouraging cooperation when others cooperate.
2. Gradually increasing cooperation as the group's overall cooperation rate improves.
3. Retaliating against widespread defection by reducing cooperation.

By adapting to the game's history, AC seeks to establish a mutually beneficial environment where players can trust each other to contribute to the public good.

**Robustness**

AC is designed to be robust against various opponent behaviors:

1. **Free Riders**: AC will reduce its cooperation rate if others defect excessively.
2. **Cooperation Enthusiasts**: AC will increase its cooperation rate as others cooperate more frequently.
3. **Mixed Strategies**: AC's adaptive nature allows it to respond effectively to a mix of cooperative and defective behavior.

By balancing individual self-interest with collective well-being, AC aims to perform competitively in the tournament while promoting a positive and cooperative atmosphere among players.
'''

description_COLLECTIVE_454 = '''
**Collective Strategy: Adaptive Cooperation with Punishment and Forgiveness**

Our collective strategy aims to balance individual self-interest with the need for cooperation to achieve higher total payoffs. We'll employ a dynamic approach that adapts to the game's history and responds to various opponent behaviors.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to reciprocate.
2. **Subsequent Rounds:** For each subsequent round, calculate the **Cooperation Index (CI)** as the ratio of total cooperators to the total number of players in the previous round:

   CI = (∑(c_j)) / n

   If CI ≥ 0.5, Cooperate (C) in the current round.
   Otherwise, Defect (D).

3. **Punishment and Forgiveness:** To deter exploitation and encourage cooperation, implement a punishment mechanism:
   - If a player defects when the Cooperation Index is above 0.75, **punish** that player by defecting against them in the next round.
   - After punishing, **forgive** the player if they cooperate in the following round.

Pseudocode for the decision rules:

```
IF (round == 1) THEN
    action = C
ELSE
    CI = (∑(c_j)) / n
    IF (CI >= 0.5) THEN
        action = C
    ELSE
        action = D

    // Punishment and Forgiveness
    FOR EACH opponent i DO
        IF (opponent_i_defected_last_round AND CI > 0.75) THEN
            punish_opponent_i = TRUE
        END IF
        IF (punish_opponent_i AND opponent_i_cooperated_last_round) THEN
            forgive_opponent_i = TRUE
        END IF
    END FOR
END IF

RETURN action
```

**Edge Cases:**

* **Last Round:** Cooperate in the last round to maximize total payoff.
* **Ties:** In case of a tie (e.g., when CI is exactly 0.5), cooperate to maintain a cooperative tone.

**Collective Mindset:**

Our strategy prioritizes cooperation while being cautious of exploitation. By adapting to the game's history and punishing defectors, we encourage others to cooperate, ultimately leading to higher total payoffs for all players. This collective approach balances individual self-interest with the need for cooperation, making it a robust and effective strategy in a wide range of scenarios.

By implementing this adaptive strategy, our AI system will be well-equipped to compete against other independent strategies in the tournament, promoting cooperation while minimizing exploitation.
'''

description_COLLECTIVE_455 = '''
**Collective Strategy: Adaptive Reciprocal Altruism (ARA)**

ARA is a collective strategy that balances individual self-interest with cooperative behavior, adapting to the game's history and opponent actions.

**Decision Rules:**

1. **Initial Rounds:** Cooperate (C) in the first two rounds to establish a baseline level of cooperation and gather information about opponents' behaviors.
2. **Reciprocity Phase:** After the initial rounds, ARA enters a reciprocity phase, where it cooperates if the average cooperation rate among all players (including itself) is above a certain threshold (`θ`). The threshold value `θ` is calculated as `(k - 1) / n`, which represents a moderate level of cooperation.
3. **Punishment Mechanism:** If an opponent defects in a round, ARA will defect in the next round against that specific opponent ( tit-for-tat-like behavior). This punishment mechanism encourages opponents to cooperate and helps maintain a stable cooperative environment.
4. **Exploration and Adaptation:** To adapt to changing opponent behaviors and avoid being exploited, ARA introduces randomness in its decisions. With a small probability (`ε`), ARA will randomly choose between cooperation (C) and defection (D), regardless of the reciprocity phase or punishment mechanism.

**Handling Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round to maximize collective payoff, as there is no future interaction.
2. **Ties and Inconsistent Histories:** If multiple opponents have inconsistent histories or ties occur, ARA will prioritize cooperation over defection to maintain a cooperative atmosphere.

**Pseudocode:**
```
Initialize variables:
  θ = (k - 1) / n
  ε = small randomization probability

For each round t from 1 to r:
  If t <= 2:
    Action = C (cooperate)
  Else:
    Calculate average cooperation rate among all players (including itself):
      avg_coop_rate = Σ(c_j) / n
    
    If avg_coop_rate >= θ:
      Action = C
    Else:
      Action = D
    
    For each opponent j:
      If opponent j defected in the previous round:
        Punishment_Mechanism = True
        Action = D (against opponent j)
    
  With probability ε, randomly choose between C and D:
    if random() < ε:
      Action = Random_Choice()
  
  Play Action
```
**Collective Mindset:**
ARA prioritizes cooperation while being responsive to the actions of others. By balancing individual self-interest with cooperative behavior, ARA aims to create a stable and mutually beneficial environment for all players.

This strategy is designed to be robust against various opponent behaviors, including those that may attempt to exploit or disrupt cooperation. The adaptive nature of ARA allows it to respond effectively to changing game dynamics, making it a formidable collective strategy in the N-Player Public Goods Game tournament.
'''

description_COLLECTIVE_456 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Exploration**: In the first round, play C (Cooperate) with a probability of 50% to gather initial information about opponents' behaviors.
2. **Tit-for-Tat-inspired Adaptation**: After the first round, observe the previous round's outcome and adjust your action as follows:
	* If the total number of cooperators in the previous round is greater than or equal to (n/2), play C.
	* Otherwise, play D (Defect).
3. **Contribution-based Adaptation**: Monitor the average contribution rate of opponents over the last w rounds (w = 3 by default). If this rate exceeds a threshold θ (θ = 0.5 by default), switch to playing C; otherwise, continue with the Tit-for-Tat-inspired adaptation.
4. **Punishment Mechanism**: If an opponent defects while you cooperated in the previous round, play D for one round as punishment.

**Edge Case Handling:**

1. **Last Round**: In the final round (r), play D to maximize individual payoff, regardless of opponents' behaviors.
2. **Early Rounds**: For rounds 2-5, use a modified Tit-for-Tat strategy: if an opponent cooperated in the previous round, cooperate; otherwise, defect.
3. **Opponent Action Observation**: If an opponent's action is not observable (e.g., due to simultaneous moves), assume they defected.

**Collective Mindset Alignment:**

1. **Foster Cooperation**: By initially cooperating and adapting to opponents' behaviors, AC encourages mutual cooperation.
2. **Robustness to Defectors**: The punishment mechanism helps maintain a balance between individual payoff maximization and collective welfare.
3. **Adaptability**: AC responds to changing opponent behaviors, promoting flexibility in the face of uncertainty.

**Pseudocode:**
```python
def AdaptiveCooperation(n, k, r):
    # Initialize variables
    c_prev = 0  # Previous round's cooperation rate
    avg_contribution_rate = 0  # Average contribution rate over w rounds
    w = 3  # Window size for average contribution rate calculation
    theta = 0.5  # Threshold for switching to cooperation

    for t in range(r):
        if t == 0:  # First round
            action = random.choice([C, D])  # Initial exploration
        elif t < w:  # Early rounds (2-5)
            opponent_actions = observe_opponent_actions(t - 1)
            action = C if any(opponent_actions) else D  # Modified Tit-for-Tat
        else:
            avg_contribution_rate = calculate_avg_contribution_rate(w, t)
            if avg_contribution_rate > theta:
                action = C
            else:
                c_prev = get_cooperation_rate(t - 1)
                action = C if c_prev >= (n / 2) else D

        # Punishment mechanism
        if opponent_defected(t - 1) and action == C:
            action = D

        # Last round
        if t == r - 1:
            action = D

        take_action(action)
```
This strategy aims to balance individual payoff maximization with collective welfare by adapting to opponents' behaviors, punishing defectors, and promoting cooperation.
'''

description_COLLECTIVE_457 = '''
**Collective Strategy: "Adaptive Cooperate-Defect" (ACD)**

The ACD strategy balances individual self-interest with collective well-being by adapting to the game's history and opponents' behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally.
2. **Cooperation Threshold**: Set a cooperation threshold `τ` based on the game parameters:
	* τ = min((n - 1) / k, 0.5)
3. **History-Dependent Cooperation**:
	+ If the total number of cooperators in the previous round exceeds `τ * n`, cooperate (C).
	+ Otherwise, defect (D).
4. **Payoff-Based Adaptation**: Calculate the average payoff difference between cooperation and defection over the last `m` rounds (`m = min(r/2, 5)`):
	* Δπ = (Σ(π_C) - Σ(π_D)) / m
	+ If Δπ > 0, decrease τ by a small amount (e.g., τ -= 0.05). This increases the likelihood of cooperation.
	+ If Δπ < 0, increase τ by a small amount (e.g., τ += 0.05). This decreases the likelihood of cooperation.

**Edge Cases:**

1. **Last Round**: In the last round, defect (D) unconditionally to maximize individual payoff.
2. **Tie-Breaking**: If two or more players have the same highest payoff in a round, break ties by choosing the player with the lowest index `i`.

**Collective Mindset:**

The ACD strategy aims to balance individual self-interest with collective well-being by:

1. Encouraging cooperation when it benefits the group (high cooperation threshold).
2. Adapting to opponents' behaviors and payoffs.
3. Gradually adjusting the cooperation threshold based on payoff differences.

**Pseudocode:**
```
ACD(n, k, r):
  τ = min((n - 1) / k, 0.5)
  m = min(r/2, 5)

  for t in range(1, r+1):
    if t == 1:
      action = C
    else:
      total_cooperators_prev = count_cooperators(t-1)
      if total_cooperators_prev >= τ * n:
        action = C
      else:
        action = D

    payoff_diff = calculate_payoff_difference(m)

    if payoff_diff > 0:
      τ -= 0.05
    elif payoff_diff < 0:
      τ += 0.05

    if t == r:
      action = D

    return action
```
This strategy is designed to be adaptive and robust to a wide range of opponent behaviors, balancing individual self-interest with collective well-being in the N-Player Public Goods Game.
'''

description_COLLECTIVE_458 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy, "Adaptive Cooperation," balances individual self-interest with the pursuit of collective well-being. It dynamically adjusts cooperation levels based on the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds (t = 1 to n/2):** Cooperate (C) to establish a baseline level of public good contribution.
2. **Middle Rounds (t > n/2):**
	* If average payoff per player in previous rounds is above the threshold (k/n), cooperate (C).
	* Otherwise, defect (D).
3. **Final Round (t = r):** Cooperate (C) if the total number of cooperators in the second-to-last round is greater than or equal to n/2.

**Edge Cases:**

1. **First Round:** Cooperate (C) to initiate cooperation.
2. **Last Round:** Cooperate (C) if sufficient players cooperated in the previous round, as described above.
3. **Tiebreaks:** In cases where the average payoff or number of cooperators is exactly equal to the threshold, cooperate (C).

**Adaptive Mechanism:**

Every n/4 rounds, reevaluate the strategy based on the game's history:

1. If the total number of cooperators has increased, adjust the cooperation threshold upward by 10%.
2. If the total number of cooperators has decreased, adjust the cooperation threshold downward by 10%.

This adaptive mechanism allows our strategy to respond to changes in opponent behavior and maintain a balance between individual and collective interests.

**Pseudocode:**
```markdown
# Initialize variables
t = current round
n = number of players
r = total rounds
k = multiplication factor
avg_payoff_threshold = k / n

# Main loop
while t <= r:
  # Initial rounds (t = 1 to n/2)
  if t <= n / 2:
    action = C

  # Middle rounds (t > n/2)
  else:
    avg_payoff_prev_rounds = calculate_average_payoff(t-1, n)
    if avg_payoff_prev_rounds >= avg_payoff_threshold:
      action = C
    else:
      action = D

  # Final round (t = r)
  if t == r:
    num_cooperators_prev_round = count_cooperators(t-1, n)
    if num_cooperators_prev_round >= n / 2:
      action = C

  # Adaptive mechanism
  if t % (n/4) == 0:
    adjust_threshold(avg_payoff_prev_rounds, num_cooperators_prev_round)

  # Take action and update history
  take_action(action)
  update_history(t, action, payoff)

  t += 1
```
**Collective Mindset:**

Our strategy prioritizes cooperation while maintaining a balance with individual self-interest. By adapting to the game's history and parameters, we foster an environment conducive to collective well-being without relying on predetermined coordination mechanisms or norms shared among opponents.

This approach allows our strategy to be robust against various opponent behaviors and fosters a collective mindset focused on achieving mutually beneficial outcomes.
'''

description_COLLECTIVE_459 = '''
I propose a collective strategy called " Adaptive Cooperation with Memory" (ACM). ACM aims to balance individual payoff maximization with collective well-being by adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round:** In the first round, play C (Cooperate) to encourage cooperation and explore opponents' strategies.
2. **Memory-Based Cooperation:** After the initial round, maintain a memory of the previous rounds' outcomes. For each subsequent round:
	* If the average payoff per player in the last round was greater than or equal to 1.5, play C (Cooperate). This indicates that cooperation is beneficial for the group.
	* Otherwise, play D (Defect).
3. **Adaptive Threshold:** Update the memory-based threshold every 2 rounds:
	* Calculate the average payoff per player over the last 2 rounds.
	* If this average is above 1.5, decrease the threshold by 0.1 (more cooperation-friendly).
	* If this average is below 1.25, increase the threshold by 0.1 (less cooperation-friendly).
4. **Punishment Mechanism:** To deter free-riding and encourage cooperation:
	* Monitor opponents' actions over a window of 2 rounds.
	* If an opponent has defected more than once in this window, play D (Defect) against them for the next round.

**Handling Edge Cases:**

1. **Last Round:** In the final round, play C (Cooperate) if the game's history suggests that cooperation is beneficial; otherwise, play D (Defect).
2. **Tie-Breaking:** In case of a tie in payoffs between cooperation and defection, default to cooperation.

**Collective Mindset:**

ACM prioritizes collective well-being by:

1. Encouraging initial cooperation to set a positive tone.
2. Adapting the threshold for cooperation based on group performance.
3. Punishing free-riders to maintain social norms.
4. Favoring cooperation in edge cases, such as the last round.

**Pseudocode:**

```
Initialize memory and threshold

For each round:
  If first round:
    Play C (Cooperate)
  Else:
    Calculate average payoff per player over last 2 rounds
    Update adaptive threshold
    If average payoff ≥ threshold:
      Play C (Cooperate)
    Else:
      Play D (Defect)

  Monitor opponents' actions and apply punishment mechanism

  Update memory with current round's outcome

For final round:
  Play C (Cooperate) if game history suggests cooperation is beneficial; otherwise, play D (Defect)
```

ACM balances individual payoff maximization with collective well-being by adapting to the game's history and opponent behaviors. Its robustness and adaptability make it a competitive strategy in a tournament setting.
'''

description_COLLECTIVE_460 = '''
**Collective Strategy: Adaptive Cooperation with Payoff-Based Feedback**

This strategy focuses on balancing individual payoffs with collective welfare, adapting to the game's history and opponents' behaviors.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to establish a cooperative tone and gather information about other players' initial actions.
2. **Subsequent Rounds (t>1):**
	* If the average payoff of cooperators in the previous round is higher than that of defectors, cooperate (C).
	* Otherwise, defect (D).
3. **Adaptive Adjustment:** Monitor the game's history and adjust the decision rule based on the following conditions:
	+ If the proportion of cooperators in the last 2-3 rounds is above a threshold (e.g., 0.6), increase the likelihood of cooperating by 10%.
	+ Conversely, if the proportion of defectors increases beyond a threshold (e.g., 0.4), decrease the likelihood of cooperating by 10%.

**Handling Edge Cases:**

1. **Last Round (t=r):** Cooperate (C) to maximize collective payoff, as individual payoffs will not affect future interactions.
2. **Tiebreaker:** In case of a tie in average payoffs between cooperators and defectors, cooperate (C) to maintain a cooperative stance.

**Collective Mindset Alignment:**

This strategy prioritizes cooperation when it benefits the collective good, while adapting to opponents' behaviors to avoid exploitation. By considering the game's history and adjusting its decision rule accordingly, this strategy aims to:

1. Foster cooperation through reciprocity.
2. Punish defecting behavior by reducing cooperation rates.
3. Balance individual payoffs with collective welfare.

**Pseudocode:**
```
Initialize:
  - cooperate_ratio = 0.5 (initial cooperation rate)
  - threshold_cooperate = 0.6
  - threshold_defect = 0.4

For each round t:
  - If t == 1, cooperate (C)
  - Else:
    - Calculate average_payoff_cooperators and average_payoff_defectors for previous round
    - If average_payoff_cooperators > average_payoff_defectors, cooperate (C)
    - Else, defect (D)
    - Update cooperate_ratio based on adaptive adjustment rules

Last Round (t=r):
  Cooperate (C)

Tiebreaker:
  Cooperate (C) if average payoffs are tied
```
This strategy should be robust to various opponent behaviors and adapt to the game's dynamics while promoting collective cooperation.
'''

description_COLLECTIVE_461 = '''
**Collective Strategy: Adaptive Cooperative Follower (ACF)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to encourage cooperation and create a positive atmosphere.
2. **Follow the Majority**: For subsequent rounds, observe the actions of all players in the previous round. If the majority (≥ n/2) cooperated, then cooperate (play C). Otherwise, defect (play D).
3. **Punish Defection**: If a player defected (played D) in the previous round and received a higher payoff than the average cooperative payoff, punish them by defecting (playing D) in the current round.
4. **Exploit Greed**: If all players cooperated (played C) in the previous round, but one or more players would have benefited from defecting (i.e., their payoff would have been higher if they had played D), exploit this greed by defecting (playing D) in the current round.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate (play C) to maintain a positive reputation and encourage cooperation in future games.
2. **Tiebreakers**: If there's a tie in the majority vote or when determining the average cooperative payoff, randomly choose between cooperating (playing C) and defecting (playing D).
3. **Initial Defection**: If all players defected (played D) in the first round, cooperate (play C) in the second round to try to establish cooperation.

**Collective Mindset:**

The Adaptive Cooperative Follower strategy aims to balance individual self-interest with collective cooperation. By following the majority and punishing defection, ACF encourages cooperation while adapting to changing opponent behaviors. The exploitation of greed mechanism helps prevent players from taking advantage of others' cooperation.

Pseudocode (simplified for readability):
```markdown
def adaptive_cooperative_follower(game_state):
  if game_state.round == 1:  # Initial Cooperation
    return C
  else:
    prev_round_actions = game_state.prev_round_actions
    majority_action = get_majority_action(prev_round_actions)
    
    if majority_action == C:
      # Follow the Majority
      return C
    elif is_punishment_needed(prev_round_actions, game_state.payoffs):
      # Punish Defection
      return D
    elif can_exploit_greed(prev_round_actions, game_state.payoffs):
      # Exploit Greed
      return D
    else:
      # Default to cooperation if no clear choice
      return C

def get_majority_action(actions):
  count_c = sum(1 for action in actions if action == C)
  return C if count_c >= len(actions) / 2 else D

def is_punishment_needed(prev_round_actions, payoffs):
  defector_payoff = max(payoffs[i] for i, action in enumerate(prev_round_actions) if action == D)
  avg_coop_payoff = sum(payoffs[i] for i, action in enumerate(prev_round_actions) if action == C) / len([action for action in prev_round_actions if action == C])
  return defector_payoff > avg_coop_payoff

def can_exploit_greed(prev_round_actions, payoffs):
  all_c = all(action == C for action in prev_round_actions)
  potential_defection_benefits = [payoff + (k/n) * len([action for action in prev_round_actions if action == C]) for i, (action, payoff) in enumerate(zip(prev_round_actions, payoffs))]
  return any(benefit > payoff for benefit, payoff in zip(potential_defection_benefits, payoffs))
```
This strategy should be robust against a wide range of opponent behaviors and adapt to changing game dynamics.
'''

description_COLLECTIVE_462 = '''
I propose a collective strategy called "Adaptive Cooperative Tit-for-Tat" (ACTT). ACTT is designed to balance individual payoffs with collective well-being while adapting to various opponent behaviors.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round, as there's no history to draw from.
2. **Subsequent Rounds:** Observe the total number of cooperators in the previous round (`total_cooperators_prev`) and calculate the "cooperation ratio" (`coop_ratio = total_cooperators_prev / n`).
3. **Cooperate (C) if:**
	* `coop_ratio >= 0.5` (i.e., at least half of players cooperated in the previous round)
	* OR, if the current round is not the last round (`t < r`) and the opponent's average cooperation ratio over the past two rounds is greater than or equal to 0.5
4. **Defect (D) if:** None of the above conditions are met

Pseudocode:
```
def ACTT(n, k, t, total_cooperators_prev):
    coop_ratio = total_cooperators_prev / n
    
    if t == 1:  # First round
        return 'C'
    
    elif coop_ratio >= 0.5 or (t < r and avg_opponent_coop_ratio >= 0.5):
        return 'C'
    
    else:
        return 'D'
```
**Edge Cases:**

* **Last Round:** Cooperate if the opponent's average cooperation ratio over the past two rounds is greater than or equal to 0.5; otherwise, defect.
* **Ties:** If there's a tie in the cooperation ratio (e.g., exactly half of players cooperated), cooperate.

**Collective Mindset:**

ACTT aims to create a collective environment where cooperation is encouraged and sustained. By cooperating when at least half of players did so in the previous round, ACTT promotes a "critical mass" effect, where more players are incentivized to cooperate as the group's overall cooperation level increases.

**Adaptability:**

ACTT adapts to various opponent behaviors by adjusting its decision-making based on the observed cooperation ratio. This allows ACTT to respond effectively to different strategies and maintain a balance between individual payoffs and collective well-being.

By implementing ACTT, we create an adaptive and robust strategy that aligns with the collective mindset while navigating the challenges of the N-Player Public Goods Game.
'''

description_COLLECTIVE_463 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy is designed to balance individual self-interest with collective well-being, adapting to the behavior of other players over time.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate (C) to encourage cooperation and build a foundation for collective success.
2. **Threshold-based Cooperation**: After the first round, cooperate if the proportion of cooperators in the previous round is above a threshold `τ`. Calculate `τ` as follows:

`τ = (k - 1) / (n - 1)`

This threshold balances individual incentives with the benefits of cooperation.

3. **Defection Response**: If fewer than `τ` proportion of players cooperated in the previous round, defect (D).
4. **Exploitation Avoidance**: To avoid exploitation by defectors, monitor the average payoff of other players over a moving window of `w` rounds. If your own average payoff is significantly lower (e.g., more than 1 standard deviation below), defect.

**Edge Case Handling**

* **Last Round**: In the final round, always defect to maximize individual gain.
* **Consecutive Defection**: If all other players have defected in two consecutive rounds, cooperate to attempt to revive cooperation.

**Additional Logic**

* **Punishment for Deviation**: If a player deviates from the strategy by cooperating less than `τ` times in a row while others are cooperating at or above the threshold, defect until they return to cooperative behavior.
* **Reciprocity and Forgiveness**: Cooperate if another player has reciprocated cooperation after your initial cooperation.

**Collective Mindset**

The ACT strategy prioritizes collective success by:

1. Encouraging cooperation through reciprocal actions
2. Balancing individual self-interest with collective well-being
3. Adapting to the behavior of other players

By implementing this strategy, we aim to foster a cooperative environment where all players benefit.

**Pseudocode (for illustration purposes)**

```python
def ACTStrategy(player_id, n, k, r, history):
    if round == 1:
        return 'C'  # Initial Cooperation
    
    τ = (k - 1) / (n - 1)
    
    previous_cooperators = sum(1 for player_history in history[-2] if player_history == 'C')
    proportion_cooperated = previous_cooperators / n
    
    if proportion_cooperated >= τ:
        return 'C'
    else:
        return 'D'  # Defection Response
    
    # Additional logic (exploitation avoidance, punishment for deviation, reciprocity, and forgiveness)
```

This strategy will be refined and implemented as an algorithm for the tournament.
'''

description_COLLECTIVE_464 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 50%. This establishes a baseline level of cooperation and encourages others to follow suit.
2. **Reaction to Cooperation**: If a player observes that at least one other player cooperated in the previous round, increase their own cooperation probability by 10% for the next round, up to a maximum of 90%.
3. **Reaction to Defection**: If a player observes that no other players cooperated or all defected in the previous round, decrease their own cooperation probability by 20% for the next round, down to a minimum of 10%.
4. **Punishment and Forgiveness**: Implement a "punishment" phase when facing a defector:
	* If a player defects while others cooperate, retaliate with defection (play D) in the next round.
	* After punishing, forgive and reset cooperation probability to the initial value (50%).
5. **Adaptive Escalation**: When observing consistent cooperation from multiple players (at least n/2), gradually increase cooperation probability by 5% every 2 rounds, up to a maximum of 95%.
6. **Late-Game Cooperation**: In the last 20% of rounds (r * 0.8 < t ≤ r), prioritize cooperation (play C) with a fixed probability of 80%.

**Edge Cases:**

1. **Last Round**: In the final round, always defect (play D).
2. **Early Rounds**: If fewer than n/4 players have played, follow the initial cooperation rule.
3. **All-Defector Scenario**: If all opponents consistently defect, maintain a minimum cooperation probability of 10%.

**Collective Mindset:**

The Adaptive Cooperative Escalation strategy focuses on building trust and fostering cooperation while adapting to various opponent behaviors. By initially cooperating, reacting to others' actions, punishing defectors, and escalating cooperation when possible, ACE aims to maximize collective payoffs without relying on explicit coordination or norms.

Pseudocode:
```python
# Initialize parameters
n = number of players
r = number of rounds
k = multiplication factor

# Initial cooperation probability
p_coop = 0.5

for t in range(r):
    # Observe previous round's actions and payoffs
    prev_actions = [action_i(t-1) for i in range(n)]
    prev_payoffs = [payoff_i(t-1) for i in range(n)]

    # Update cooperation probability based on reactions to cooperation/defection
    if any(prev_actions):  # at least one player cooperated
        p_coop += 0.1
        p_coop = min(p_coop, 0.9)
    elif all(not action for action in prev_actions):  # all defected
        p_coop -= 0.2
        p_coop = max(p_coop, 0.1)

    # Punishment and forgiveness phase
    if any(prev_actions) and not my_prev_action(t-1):
        play D (defect)
    else:
        reset cooperation probability to initial value

    # Adaptive escalation
    if sum([action for action in prev_actions]) >= n/2:
        p_coop += 0.05
        p_coop = min(p_coop, 0.95)

    # Late-game cooperation
    if t > r * 0.8:
        play C (cooperate) with probability 0.8

    # Play according to updated cooperation probability
    action_t = random.random() < p_coop ? C : D
```
This strategy should be able to adapt and perform well in a wide range of scenarios, including those where other players employ various strategies or norms.
'''

description_COLLECTIVE_465 = '''
I propose a collective strategy called "Adaptive Cooperator" (AC) that balances individual payoff maximization with collective welfare optimization. AC relies on observing past behavior and adapting to changes in the environment.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally.
2. **Tit-for-Tat (TFT)**: For rounds 2 to r-1, if the majority of players cooperated in the previous round, cooperate (C). Otherwise, defect (D).
3. **Adaptive Threshold**: Introduce a threshold variable `θ` initialized at 0.5. This represents the minimum proportion of cooperating players required for AC to cooperate.
4. **Payoff-Based Adjustment**: After each round, update `θ` based on the average payoff obtained by all players in that round:
	* If the average payoff is above the expected value (1 + k/n), decrease `θ` by 0.05.
	* If the average payoff is below the expected value, increase `θ` by 0.05.
5. **Cooperation Condition**: In rounds 2 to r-1, cooperate (C) if and only if:
	* The proportion of cooperating players in the previous round is greater than or equal to `θ`.
6. **Final Round**: In the last round (r), defect (D) unconditionally.

**Pseudocode:**
```python
def AdaptiveCooperator(n, k, r):
    θ = 0.5  # Initialize threshold variable

    for t in range(r):
        if t == 0:  # First round
            action = 'C'
        elif t < r - 1:  # Rounds 2 to r-1
            prev_round_coops = count_cooperations(t - 1)
            if prev_round_coops / n >= θ:
                action = 'C'
            else:
                action = 'D'
            update_θ(average_payoff(t - 1))  # Adjust threshold variable
        else:  # Last round (r)
            action = 'D'

        return action

def count_cooperations(round):
    # Count the number of cooperating players in the given round
    pass

def average_payoff(round):
    # Calculate the average payoff obtained by all players in the given round
    pass

def update_θ(avg_payoff):
    global θ
    if avg_payoff > 1 + k/n:
        θ -= 0.05
    else:
        θ += 0.05
```
**Rationale:**

* The initial cooperation and TFT rules promote cooperation in the early stages of the game.
* The adaptive threshold allows AC to adjust its behavior based on the environment's response, making it more robust to varying opponent behaviors.
* The payoff-based adjustment ensures that AC responds to changes in the collective welfare, rather than simply following a fixed strategy.
* Defecting in the final round maximizes individual payoff when other players may also be tempted to defect.

By combining these elements, Adaptive Cooperator balances short-term gains with long-term cooperation, making it an effective and adaptive strategy for the N-Player Public Goods Game.
'''

description_COLLECTIVE_466 = '''
**Collective Strategy: Adaptive Cooperation with Reputation-Based Tit-for-Tat**

Our collective strategy aims to balance individual self-interest with collective well-being by adapting to the evolving behavior of opponents and leveraging reputation-based reciprocity.

**Decision Rules:**

1. **Initial Rounds (t ≤ n)**: Cooperate (C) in the first `n` rounds to establish a cooperative baseline and gather information about opponents' behaviors.
2. **Reputation-Based Tit-for-Tat**: After the initial rounds, use the following decision rule:
	* If an opponent has cooperated in the previous round (`t-1`), cooperate (C) with probability `p_C = 0.8 + (k/n) * (num_cooperators / n)`; otherwise, defect (D).
	* Update the reputation of each opponent by incrementing their cooperation count if they cooperated in the current round.
3. **Punishment Mechanism**: If an opponent has defected in the previous round (`t-1`) and has a low cooperation rate (less than 0.5), defect (D) with probability `p_D = 0.2 + (n-k)/n`.

**Handling Edge Cases:**

* **First Round**: Cooperate (C) to establish a cooperative baseline.
* **Last Round**: Defect (D) as there is no future round to reciprocate or punish.
* **Tie-Breaking**: In case of ties in the cooperation count, prioritize opponents with higher cooperation rates.

**Collective Mindset:**

Our strategy aligns with the collective mindset by:

* Cooperating initially to establish a cooperative baseline and encourage others to do so.
* Adapting to opponents' behaviors and reciprocating cooperation to foster mutual benefit.
* Punishing repeated defectors to maintain the stability of the collective.

**Pseudocode (for illustration purposes only):**
```python
def adaptive_cooperation(n, k, t, opponent_actions):
    if t <= n:
        return "C"  # Cooperate in initial rounds

    num_cooperators = sum(opponent_actions[t-1] == "C")
    reputation = [0] * n  # Initialize reputation array

    for i, action in enumerate(opponent_actions[t-1]):
        if action == "C":
            reputation[i] += 1

    p_C = 0.8 + (k/n) * (num_cooperators / n)
    p_D = 0.2 + (n-k)/n

    if opponent_actions[t-1][i] == "C":
        return "C" with probability p_C
    elif reputation[i] < 0.5:
        return "D" with probability p_D
    else:
        return "C"
```
This strategy is designed to be adaptive, robust, and aligned with the collective mindset, making it a strong competitor in the tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_467 = '''
I'll propose a collective strategy that adapts to various opponent behaviors and is robust across different game parameters.

**Strategy Name:** Adaptive Collective Conscience (ACC)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This sets a neutral tone for the game.
2. **Cooperation Threshold**: Calculate the cooperation threshold (T) as follows:

T = (k / n) \* (√(n) - 1)

If the number of cooperators in the previous round is greater than or equal to T, cooperate (C) in the current round.

3. **Punishment Mechanism**: If the number of cooperators in the previous round is less than T, defect (D) with probability p = 0.5 \* (1 - (previous_round_cooperators / n)). This mechanism discourages free-riding and promotes cooperation.
4. **Reciprocity**: Monitor the opponent's behavior over the last few rounds (e.g., 3-5 rounds). If an opponent has cooperated more often than defected, cooperate with them in the current round.

**Edge Case Handling:**

* **Last Round**: In the final round, cooperate if the number of cooperators in the previous round is greater than or equal to T. Otherwise, defect.
* **Ties**: In case of a tie (i.e., the number of cooperators equals T), cooperate with probability 0.5.

**Collective Mindset:**

The ACC strategy prioritizes cooperation while adapting to the behavior of other players. By cooperating in response to sufficient cooperation from others, it fosters a collective conscience that promotes mutual benefit. The punishment mechanism and reciprocity aspects ensure that free-riding is discouraged, maintaining a balance between individual self-interest and collective welfare.

**Pseudocode:**
```python
def ACC(n, k, r):
    T = (k / n) * (√(n) - 1)
    cooperation_history = []

    for t in range(r):
        if t == 0:
            cooperate = random.random() < 0.5
        else:
            previous_round_cooperators = sum(cooperation_history[-1])
            if previous_round_cooperators >= T:
                cooperate = True
            elif previous_round_cooperators < T:
                p = 0.5 * (1 - (previous_round_cooperators / n))
                cooperate = random.random() < p

        # Reciprocity mechanism
        opponent_behavior = []
        for opponent in range(n):
            if opponent != self:
                opponent_cooperation_rate = sum(cooperation_history[-3:]) / len(cooperation_history[-3:])
                if opponent_cooperation_rate > 0.5:
                    cooperate = True

        cooperation_history.append([cooperate] * n)

    return cooperation_history
```
Note that this pseudocode is a simplified representation of the ACC strategy and might require modifications to fit specific implementation requirements.

The Adaptive Collective Conscience strategy aims to strike a balance between individual self-interest and collective welfare, making it a robust and adaptive approach for playing the N-Player Public Goods Game.
'''

description_COLLECTIVE_468 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

ACE aims to balance individual self-interest with collective well-being by adapting cooperation levels based on observed opponent behavior and game history.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate in the first round to signal willingness to collaborate.
2. **Reciprocity**: In subsequent rounds, cooperate if at least (n-1) other players cooperated in the previous round. This encourages reciprocity and punishes widespread defection.
3. **Gradual Escalation**: If fewer than (n-1) players cooperated, but still more than (n/2), escalate cooperation by 1/n each round until either full cooperation or a player defects.
4. **Punishment**: Defect if any player defected in the previous round and fewer than (n/2) players cooperated. This punishes exploiters and prevents free-riding.

**Edge Cases:**

* Last Round: Cooperate, as there's no future interaction to worry about.
* Consecutive Defections: If multiple rounds see widespread defection (fewer than n/2 cooperating), revert to Initial Cooperation after two consecutive rounds of defection.
* Unusual Behavior: If an opponent exhibits unusual behavior (e.g., random or alternating cooperation/defection), maintain Reciprocity for two rounds before adjusting strategy.

**Pseudocode:**
```
function ACE(n, k, history):
  if current_round == 1:
    return COOPERATE
  else:
    prev_coops = count coop players in previous round
    if prev_coops >= (n - 1):
      return COOPERATE
    elif prev_coops > n/2:
      escalation_level += 1/n
      if escalation_level >= 1:
        return COOPERATE
      else:
        return DEFECT
    else:
      if any player defected in previous round:
        return DEFECT
      else:
        return COOPERATE

def update_history(history, current_actions):
  # Update history with observed actions and payoffs
```
**Collective Mindset:**
ACE prioritizes collective well-being by encouraging cooperation and reciprocity. By gradually escalating cooperation levels, ACE promotes a culture of mutual support while still allowing for adaptation to varying opponent behaviors.

In the tournament setting, ACE will be robust against a range of independent strategies, as it doesn't rely on predetermined coordination or shared norms.
'''

description_COLLECTIVE_469 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and depends solely on game parameters and history:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability p = k/n, where k is the multiplication factor and n is the number of players. This initial cooperation rate sets a tone for potential collective benefits.
2. **Subsequent Rounds (t>1):**
	* If the total payoff in the previous round was greater than or equal to the average payoff of all players cooperating (i.e., π_prev ≥ k), Cooperate (C) with probability p = 0.7 + (π_prev - k) / (n \* k).
	* Otherwise, Defect (D) with probability p = 1 - (k/n).

**Pseudocode:**
```python
def ACO(n, k, t, π_prev):
    if t == 1:
        return C with probability k/n
    elif π_prev >= k:
        p_C = 0.7 + (π_prev - k) / (n * k)
        return C with probability p_C
    else:
        return D with probability 1 - (k/n)
```
**Rationale:**

* By initially cooperating with a probability based on the multiplication factor, ACO sets an optimistic tone for potential collective benefits.
* In subsequent rounds, if the total payoff was high enough to justify cooperation (i.e., π_prev ≥ k), ACO increases its cooperation rate. This encourages others to cooperate as well, fostering a collective mindset.
* If the previous round's payoff was low, ACO becomes more cautious and defects with higher probability, signaling that cooperation may not be beneficial.

**Edge Cases:**

* **Last Round (t=r):** Since there are no future rounds to influence, ACO cooperates if π_prev ≥ k; otherwise, it defects.
* **Ties:** In case of ties in the previous round's payoff, ACO uses a random tiebreaker (e.g., fair coin toss) to decide between cooperating and defecting.

**Collective Mindset:**

ACO aligns with the collective mindset by:

1. Initially cooperating based on game parameters, signaling potential benefits from cooperation.
2. Adapting its cooperation rate based on previous rounds' payoffs, which encourages others to cooperate as well.
3. Being robust to various opponent behaviors, making it a reliable strategy in a tournament setting.

This strategy balances individual self-interest with collective optimization, aiming to achieve a mutually beneficial outcome for all players involved.
'''

description_COLLECTIVE_470 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and maximizes overall payoff:

**Strategy Name:** Adaptive Collective Optimizer (ACO)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability 0.5, and Defect (D) with probability 0.5. This initial cooperation encourages other players to cooperate and sets a positive tone for the game.
2. **Subsequent Rounds (t>1):**
	* If the average payoff of all players in the previous round is greater than or equal to the multiplication factor k, Cooperate (C).
	* Otherwise, Defect (D) with probability p_d = 1 - (k/n) \* (average number of cooperators in the previous round / n)
	* With probability 1 - p_d, Cooperate (C)

Pseudocode:
```
IF t == 1 THEN
    COOPERATE WITH PROBABILITY 0.5
ELSE
    average_payoff_prev_round = calculate_average_payoff(t-1)
    IF average_payoff_prev_round >= k THEN
        COOPERATE
    ELSE
        p_d = 1 - (k/n) * (average_cooperators_prev_round / n)
        WITH PROBABILITY p_d, DEFECT
        ELSE, COOPERATE
```
**Handling Edge Cases:**

* **Last Round (t=r):** Cooperate (C) if the average payoff of all players in the previous round is greater than or equal to k. Otherwise, Defect (D).
* **Ties:** In case of ties in the number of cooperators or average payoffs, use a random tiebreaker with probability 0.5 for each action.

**Collective Mindset:**
The ACO strategy aligns with the collective mindset by:

1. Encouraging cooperation initially to promote mutual benefit.
2. Adapting to the behavior of other players based on their previous actions and payoffs.
3. Balancing individual self-interest with collective welfare, as it takes into account both personal payoff and overall group performance.

**Robustness:**
The ACO strategy is robust against various opponent behaviors because:

1. It adapts to changes in the number of cooperators and average payoffs over time.
2. It does not rely on specific coordination mechanisms or assumptions about other players' strategies.
3. It balances individual self-interest with collective welfare, making it less vulnerable to exploitation by defecting opponents.

The ACO strategy should perform well in a tournament against independent strategies developed by other AI systems, as it effectively balances cooperation and defection based on the game's parameters and history.
'''

description_COLLECTIVE_471 = '''
**Collective Strategy: Adaptive Cooperation with Memory (ACM)**

The ACM strategy is designed to balance individual self-interest with collective well-being, adapting to a wide range of opponent behaviors and game conditions.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Memory-Based Cooperation:** Keep track of the number of cooperators (c_t-1) in the previous round. If c_t-1 ≥ n/2, cooperate (C) in the current round; otherwise, defect (D).
3. **Adaptive Threshold:** Adjust the cooperation threshold based on the game's history. After every 5 rounds, calculate the average number of cooperators (avg_c) over the last 5 rounds. Update the cooperation threshold to avg_c / n.
4. **Punishment Mechanism:** If a player defects when there are enough cooperators to exceed the threshold, defect (D) in the next round. This punishes non-cooperative behavior and encourages reciprocity.

**Edge Cases:**

1. **Last Round:** Defect (D) in the last round (r), as there is no future opportunity for cooperation or punishment.
2. **Ties:** In case of a tie (c_t-1 = n/2), cooperate (C) to maintain a positive atmosphere and encourage continued cooperation.

**Collective Mindset:**

ACM prioritizes collective well-being by:

1. Encouraging initial cooperation to create a positive atmosphere.
2. Adapting to the group's behavior, cooperating when there is sufficient support.
3. Punishing non-cooperative behavior to maintain reciprocity.
4. Adjusting its strategy based on the game's history to ensure long-term collective success.

**Pseudocode:**

```
initialize memory (c_t-1 = 0)
for each round t:
  if t == 1:
    cooperate (C)
  else:
    if c_t-1 >= n/2:
      cooperate (C)
    else:
      defect (D)
    update threshold every 5 rounds
    avg_c = calculate average number of cooperators over last 5 rounds
    threshold = avg_c / n
    if player defected when c_t-1 > threshold:
      punish (D) in next round
  update memory (c_t-1 = current number of cooperators)
```

By implementing the Adaptive Cooperation with Memory strategy, our AI system will promote collective success while being robust to various opponent behaviors and game conditions.
'''

description_COLLECTIVE_472 = '''
I propose a collective strategy called "Adaptive Collective Cooperation" (ACC). ACC aims to balance individual self-interest with collective well-being, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Cooperation Threshold**: Introduce a cooperation threshold (CT) based on the number of cooperators in the previous round. Set CT = k/n, where k is the multiplication factor and n is the number of players. This threshold represents the minimum required collective cooperation for individual cooperation to be beneficial.
3. **Adaptive Cooperation**: In subsequent rounds, cooperate if:
	* The previous round's cooperation level meets or exceeds the CT (i.e., Σc_j ≥ CT).
	* At least one other player cooperated in the previous round.
4. **Defection Response**: If another player defects while you cooperated in the previous round, defect in the next round to minimize losses.
5. **Punishment Mechanism**: Implement a mild punishment mechanism to discourage repeated defection. If a player has defected for two consecutive rounds, defect against them in the next round.

**Edge Cases:**

1. **Last Round**: Cooperate in the last round if at least one other player cooperated in the second-to-last round.
2. **Single Defector**: If only one player defects while all others cooperate, continue cooperating to maintain the collective benefit.
3. **All Defectors**: If all players defect, cooperate in the next round to attempt to reestablish cooperation.

**Collective Mindset:**

ACC prioritizes collective well-being by:

1. Encouraging cooperation through adaptive thresholds and punishment mechanisms.
2. Responding to opponent behaviors while avoiding exploitation.
3. Fostering a cooperative environment through initial and last-round cooperation.

Pseudocode for ACC:
```python
def ACC(n, k, r):
    # Initialize variables
    CT = k / n  # Cooperation Threshold
    prev_coop = [False] * n  # Previous round's cooperation levels

    for t in range(r):
        if t == 0:  # Initial Round
            cooperate = True
        else:
            coop_count = sum(prev_coop)
            if coop_count >= CT and any(prev_coop):
                cooperate = True
            elif any([prev_coop[i] and not prev_coop[j] for i in range(n) for j in range(i+1, n)]):
                cooperate = False  # Defection Response
            else:
                cooperate = True

        # Punishment Mechanism
        if sum(prev_coop) == 0 and any([prev_coop[i] and prev_coop[j] for i in range(n) for j in range(i+1, n)]):
            cooperate = False

        # Last Round Cooperation
        if t == r - 1:
            cooperate = True

        # Play the game
        play_game(cooperate)

        # Update previous cooperation levels
        prev_coop = [player.cooperated for player in players]
```
ACC balances individual self-interest with collective well-being, adapting to opponent behaviors and promoting a cooperative environment. This strategy should perform well in a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_473 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation (AC) strategy aims to balance individual payoffs with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: In the first round, play C (Cooperate) to encourage cooperation and set a positive tone.
2. **Subsequent Rounds**:
	* If the average payoff of cooperators in the previous round is greater than or equal to the average payoff of defectors, play C (Cooperate).
	* Otherwise, play D (Defect).
3. **Adaptive Threshold**: Introduce an adaptive threshold, `τ`, which adjusts based on the game's history. Initialize τ = 0.
	* If the total number of cooperators in a round is greater than or equal to `n/2`, increase τ by 1/n.
	* If the total number of cooperators in a round is less than `n/4`, decrease τ by 1/n.
	* Update decision rule: play C if the average payoff of cooperators in the previous round is greater than or equal to the average payoff of defectors, minus τ.

**Pseudocode**:
```python
def adaptive_cooperation(n, k, r):
    tau = 0
    for t in range(r):
        # First round: cooperate
        if t == 0:
            action[t] = 'C'
        else:
            # Calculate average payoffs
            avg_payoff_C = (k/n) * sum(c_j for c_j in actions[t-1] if c_j == 'C')
            avg_payoff_D = 1 + (k/n) * sum(c_j for c_j in actions[t-1] if c_j == 'D')

            # Adaptive threshold update
            num_coop = sum(1 for c_j in actions[t-1] if c_j == 'C')
            if num_coop >= n/2:
                tau += 1/n
            elif num_coop < n/4:
                tau -= 1/n

            # Decision rule: cooperate or defect
            if avg_payoff_C - tau >= avg_payoff_D:
                action[t] = 'C'
            else:
                action[t] = 'D'

    return actions
```
**Edge Cases:**

* **Last Round**: In the final round, play D (Defect) to maximize individual payoff.
* **Ties**: If the average payoffs of cooperators and defectors are equal, play C (Cooperate).

**Collective Mindset Alignment:**
AC prioritizes cooperation when it is beneficial for the collective, while adapting to changing circumstances. By using an adaptive threshold, AC balances individual self-interest with the need for cooperation to achieve higher collective payoffs.

This strategy should be robust against various opponent behaviors and perform well in a tournament setting.
'''

description_COLLECTIVE_474 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy, "Adaptive Cooperation," aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) to encourage others to do the same and establish a baseline for cooperation.
2. **Subsequent Rounds**: Defect (D) if the average payoff per cooperator in the previous round is less than or equal to 1 + (k/n). Otherwise, cooperate (C).
3. **Exception Handling**:
	* If all players defected in the previous round, cooperate (C) to try to reinitiate cooperation.
	* In the last round (t=r), defect (D) as there is no future benefit from cooperating.

Pseudocode:
```
if t == 1:  // Initial Round
    action = C
else:
    avg_payoff_coop = calculate_average_payoff(cooperators, previous_round)
    if avg_payoff_coop <= 1 + k/n:
        action = D
    else:
        action = C

    if all_players_defected(previous_round):
        action = C  // Try to reinitiate cooperation
    elif t == r:  // Last Round
        action = D  // No future benefit from cooperating
```

**Rationale:**

1. **Initial Cooperation**: By starting with cooperation, we set a positive tone and encourage others to reciprocate.
2. **Adaptive Threshold**: We adjust our behavior based on the previous round's average payoff per cooperator. If it's low, defecting is more beneficial; otherwise, cooperating continues to yield better payoffs.
3. **Exception Handling**: By reinitiating cooperation after a round of all defections and defecting in the last round, we balance individual self-interest with collective well-being.

**Collective Mindset:**

Adaptive Cooperation aligns with the collective mindset by:

1. **Encouraging Cooperation**: Our initial cooperation and adaptive threshold aim to foster a cooperative environment.
2. **Responding to Collective Behavior**: We adjust our strategy based on the group's behavior, promoting a sense of shared responsibility.

By implementing Adaptive Cooperation, we create a robust and adaptive strategy that responds to various opponent behaviors while prioritizing collective well-being.
'''

description_COLLECTIVE_475 = '''
**Collective Strategy: Adaptive Cooperation with History-Based Adjustment (ACHA)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds**: Use a history-based approach to adjust cooperation levels based on the previous round's outcomes.

Let `H` be the history of play, where `H[t]` represents the number of cooperators in round `t`. Define two thresholds:

* `cooperation_threshold`: The minimum percentage of cooperators required for the strategy to cooperate. Set this threshold to 50% (i.e., `(n/2)` cooperators).
* `defection_threshold`: The maximum percentage of defectors allowed before the strategy switches to defecting. Set this threshold to 25% (i.e., `(n/4)` defectors).

**Pseudocode:**
```
function ACHA(n, k, r, H):
    if t == 1:
        # Initial round
        return C

    # Calculate cooperation and defection thresholds
    cooperation_threshold = n / 2
    defection_threshold = n / 4

    # Count cooperators and defectors in the previous round
    num_cooperators = sum(H[t-1])
    num_defectors = n - num_cooperators

    if num_cooperators >= cooperation_threshold:
        # Cooperate if enough others cooperated
        return C
    elif num_defectors <= defection_threshold:
        # Defect if too many others defected, but not excessively so
        return D
    else:
        # Otherwise, alternate between cooperate and defect
        return (t % 2 == 0) ? C : D

# Example usage:
n = 6
k = 2
r = 10
H = []  # Initialize history

for t in range(1, r+1):
    action = ACHA(n, k, r, H)
    # Play the game with the chosen action and update the history
    H.append(num_cooperators)  # Update history with number of cooperators
```
**Edge Cases:**

* **Last Round**: The strategy will adapt based on the previous round's outcomes, even in the last round.
* **Tiebreakers**: In cases where the thresholds are exactly met (e.g., `num_cooperators == cooperation_threshold`), the strategy will alternate between cooperate and defect.

**Collective Mindset:**

The ACHA strategy aims to promote collective cooperation while adapting to the evolving behavior of other players. By adjusting its cooperation level based on the previous round's outcomes, it encourages others to cooperate and punishes excessive defection. The alternating pattern in cases where thresholds are not met helps maintain a balance between cooperation and self-interest.

This strategy is designed to be robust against a wide range of opponent behaviors, including various degrees of cooperation and defection, while promoting collective well-being through adaptive cooperation.
'''

description_COLLECTIVE_476 = '''
**Collective Strategy: "Adaptive Cooperative Threshold" (ACT)**

**Overview**
The ACT strategy aims to balance individual payoff maximization with collective welfare by adapting cooperation based on the game's history and parameters.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate (play C) if n ≥ 4 and k > 2; otherwise, defect (play D). This initial cooperation encourages mutual benefit when the group is large enough and the multiplication factor is high.
2. **Adaptive Threshold**: For subsequent rounds, calculate a dynamic cooperation threshold based on:
	* `avg_contributions`: Average number of cooperators in previous rounds
	* `k_threshold`: A parameter-dependent threshold, calculated as (n / k) ^ 0.5

Cooperate if `avg_contributions` is greater than or equal to `k_threshold`. This rule adapts cooperation to the group's past behavior and the game's parameters.
3. **Punishment Mechanism**: If the number of cooperators in the previous round (`num_cooperators`) is less than `k_threshold`, defect for one round (play D) before re-evaluating the cooperation threshold. This mechanism discourages exploitation by non-cooperative players.

**Edge Cases**

* Last Round: Cooperate if the game's total payoff for cooperating exceeds the individual payoff from defecting, considering the entire history.
* Tiebreaker: In case of a tie in the cooperation threshold calculation, cooperate if the number of cooperators in the previous round was greater than or equal to `k_threshold`.

**Pseudocode**

```
def ACT(n, k, r):
    avg_contributions = 0
    num_cooperators = 0
    k_threshold = (n / k) ^ 0.5

    for t in range(1, r+1):
        if t == 1:
            # Initial Cooperation
            action = 'C' if n >= 4 and k > 2 else 'D'
        else:
            avg_contributions = (avg_contributions * (t-1) + num_cooperators) / t
            if avg_contributions >= k_threshold:
                action = 'C'
            elif num_cooperators < k_threshold:
                # Punishment Mechanism
                action = 'D' for one round
            else:
                action = 'D'

        # Update counters and threshold
        num_cooperators = count_cooperators(action, actions)
        avg_contributions += num_cooperators

    return total_payoff
```

**Collective Mindset**
The ACT strategy aligns with the collective mindset by:

* Encouraging cooperation when mutually beneficial (initial cooperation)
* Adapting to group behavior and game parameters
* Punishing exploitation through a temporary defection mechanism
* Balancing individual payoff maximization with collective welfare

This adaptive strategy is robust to various opponent behaviors, making it suitable for a tournament setting where other AI systems employ different strategies.
'''

description_COLLECTIVE_477 = '''
Here's a collective strategy that adapts to the game parameters and history:

**Strategy Name:** Adaptive Collective Convergence (ACC)

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a baseline for cooperation.
2. **Convergence Phase:** For rounds 2 to r-1, use the following decision rule:
	* If the average payoff of all players in the previous round is greater than or equal to k/2, cooperate (C). This indicates that the collective is benefiting from cooperation.
	* Otherwise, defect (D).
3. **Last Round:** Defect (D) in the last round (r) to maximize individual payoff, as there's no future benefit to cooperating.

**Adaptive Mechanism:**

1. **Monitor Collective Payoff:** Track the average payoff of all players over the previous rounds.
2. **Adjust Cooperation Threshold:** If the collective payoff is consistently below k/2, increase the cooperation threshold by a small amount (e.g., 0.05) every few rounds. This allows the strategy to adapt to changing opponent behaviors.

**Pseudocode:**
```python
def ACC(n, r, k, history):
    # First round: Cooperate
    if len(history) == 0:
        return 'C'

    # Convergence phase
    avg_payoff = sum([h['payoff'] for h in history[-1:]]) / n
    if avg_payoff >= k/2:
        return 'C'
    else:
        return 'D'

    # Last round: Defect
    if len(history) == r-1:
        return 'D'

    # Adaptive mechanism
    collective_payoff = sum([h['payoff'] for h in history]) / (len(history) * n)
    if collective_payoff < k/2:
        cooperation_threshold += 0.05  # adjust threshold

def get_action(n, r, k, history):
    return ACC(n, r, k, history)
```
**Collective Mindset:**

The ACC strategy aligns with the collective mindset by:

1. Encouraging cooperation when it benefits the collective.
2. Adapting to changing opponent behaviors to maintain a high collective payoff.
3. Prioritizing individual payoff in the last round to maximize overall utility.

By using this adaptive and robust strategy, we can achieve a high level of cooperation and collective benefit while being prepared for a wide range of opponent behaviors.
'''

description_COLLECTIVE_478 = '''
I propose a collective strategy called "Adaptive Cooperativity with Memory" (ACM). ACM balances individual self-interest with collective benefits by adapting to the game's history and opponents' behaviors.

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (play C) to initiate cooperation and encourage others to do so.
2. **Memory-based Adaptation**: Keep track of the number of cooperators in each round (M_t). If M_t > n/2, cooperate in the next round; otherwise, defect.
3. **Self-interest Adjustment**: Introduce a "self-interest threshold" parameter, s ∈ [0, 1], which controls how much individual payoff affects cooperation decisions. Calculate the average payoff per cooperator in the previous round (APC_t). If APC_t > k/n + s, cooperate; otherwise, defect.
4. **Opponent Behavior Response**: Monitor opponents' actions and adjust the self-interest threshold accordingly:
	* If most opponents (>= n/2) cooperated in the previous round, decrease s to encourage cooperation.
	* If most opponents defected, increase s to prioritize individual payoffs.

**Pseudocode:**
```python
def ACM(n, k, r):
    M_t = 0  # Number of cooperators in each round
    APC_t = 0  # Average payoff per cooperator
    s = 0.5  # Initial self-interest threshold

    for t in range(r):
        if t == 0:  # First round
            action = 'C'
        else:
            M_prev = M_t
            APC_prev = APC_t

            if M_prev > n/2:
                action = 'C'
            elif APC_prev > k/n + s:
                action = 'C'
            else:
                action = 'D'

            # Adjust self-interest threshold based on opponents' behavior
            if sum(opponents_actions[t-1]) >= n/2:  # Most opponents cooperated
                s *= 0.9
            elif sum(opponents_actions[t-1]) < n/2:  # Most opponents defected
                s *= 1.1

        M_t = count_cooperators(t)
        APC_t = calculate_average_payoff_per_cooperator(t)

        return action
```
**Edge Cases:**

* Last round (t == r): Defect to maximize individual payoff, as there are no future rounds to consider.
* If all opponents defect in a round, cooperate in the next round to try to restart cooperation.

**Collective Mindset Alignment:**
ACM prioritizes collective benefits by:

* Cooperating in the first round to initiate cooperation
* Adapting to the number of cooperators and average payoff per cooperator
* Adjusting self-interest threshold based on opponents' behavior

This strategy balances individual self-interest with collective benefits, making it a robust and adaptive approach for a wide range of opponent behaviors.
'''

description_COLLECTIVE_479 = '''
I propose a collective strategy called "Adaptive Collective Conscience" (ACC). ACC aims to balance individual self-interest with collective well-being by adapting to the group's behavior over time.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to reciprocate.
2. **Contribution Tracking:** Keep track of the total contributions (TC) made by all players in each round, including the current round. This will be used to assess the group's overall cooperation level.
3. **Cooperation Threshold:** Set a cooperation threshold (CT) as a function of the game parameters: CT = (n \* k) / (2 \* n + k). This threshold represents a balanced trade-off between individual and collective payoffs.
4. **Adaptive Cooperation:** In each round, calculate the average contribution per player (ACP) by dividing TC by n. Compare ACP to CT:
	* If ACP ≥ CT, cooperate (C) in this round. The group is contributing sufficiently, so it's safe to contribute as well.
	* If ACP < CT, defect (D) in this round. The group is not contributing enough, so it's better to prioritize individual payoffs.
5. **Punishment Mechanism:** To deter excessive defection, implement a punishment mechanism:
	* If the number of defectors in the previous round exceeds (n / 2), cooperate (C) in this round with a probability of 0.5 + (k / n). This introduces some uncertainty and encourages players to reconsider their actions.
6. **Final Round:** In the last round, cooperate (C) if ACP ≥ CT in the second-to-last round. Otherwise, defect (D).

**Pseudocode:**
```python
def ACC(n, k, r):
    # Initialize variables
    TC = 0  # Total contributions
    CT = (n * k) / (2 * n + k)  # Cooperation threshold

    for t in range(r):
        if t == 0:
            action = 'C'  # Cooperate in the first round
        else:
            ACP = TC / n
            if ACP >= CT:
                action = 'C'
            elif ACP < CT:
                action = 'D'

            # Punishment mechanism
            if t > 0 and num_defectors(t-1) > (n / 2):
                action = random.choice(['C', 'D'], p=[0.5 + (k / n), 0.5 - (k / n)])

        # Update TC
        if action == 'C':
            TC += 1

    return action
```
**Collective Mindset:** ACC is designed to promote a collective mindset by:

* Encouraging cooperation when the group is contributing sufficiently
* Punishing excessive defection to maintain a balanced trade-off between individual and collective payoffs
* Adapting to the group's behavior over time to ensure robustness against various opponent strategies

By following this strategy, ACC aims to achieve a high total payoff while promoting a cooperative atmosphere in the game.
'''

description_COLLECTIVE_480 = '''
**Collective Strategy: Adaptive Cooperation with Retaliation (ACR)**

**Decision Rules:**

1. **Initial Rounds**: Cooperate for the first 2-3 rounds to establish a baseline level of cooperation and encourage others to follow suit.
2. **General Rule**: Cooperate if the average cooperation rate in the previous round is above a certain threshold, `theta` (set to 0.5). This promotes collective benefit when enough players are cooperating.
3. **Retaliation Mechanism**: If the average cooperation rate falls below `theta`, defect for one round to signal disappointment and encourage others to cooperate again.
4. **Exploration Rounds**: Every 5th round, explore defection to gauge the response of other players and adjust the strategy accordingly.

**Pseudocode:**
```
ACR(n, k, r):
  theta = 0.5
  history = [] // store cooperation rates for each round

  for t in range(r):
    if t < 3: // initial rounds
      action = Cooperate
    else:
      avg_coop_rate = sum(history[-1]) / n // average cooperation rate last round
      if avg_coop_rate >= theta:
        action = Cooperate
      elif (t % 5) == 0: // exploration round
        action = Defect
      else: // retaliation mechanism
        action = Defect

    history.append([1 if player_i.cooperates else 0 for player_i in players]) // update cooperation rates

  return actions
```
**Handling Edge Cases:**

* **First Round**: Cooperate to establish a cooperative tone.
* **Last Round**: If the average cooperation rate is above `theta`, cooperate. Otherwise, defect to maximize personal payoff.
* **Rounds with low cooperation rates**: Activate the retaliation mechanism to encourage others to cooperate.

**Collective Mindset:**
The ACR strategy prioritizes collective benefit while adapting to changing opponent behaviors. By cooperating initially and retaliating when cooperation rates drop, it promotes a culture of mutual support. The exploration rounds allow for continuous assessment of the game's dynamics, enabling adjustments to maintain an optimal balance between individual and collective interests.

**Robustness:**
ACR is robust against various opponent strategies due to its adaptability:

* **Pure Cooperators**: ACR will cooperate with them, maximizing collective payoffs.
* **Pure Defectors**: ACR will defect in response, minimizing losses.
* **Mixed Strategies**: ACR's adaptive nature allows it to adjust to changing cooperation rates, encouraging others to cooperate.

This strategy should perform well in a tournament setting, as it balances individual and collective interests while adapting to diverse opponent behaviors.
'''

description_COLLECTIVE_481 = '''
I propose a collective strategy called "Adaptive Cooperative Threshold" (ACT) that balances individual payoff maximization with promoting cooperation among players.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if the number of players (n) is odd; otherwise, defect (D). This helps establish a baseline level of cooperation and encourages others to follow suit.
2. **Cooperation Threshold**: After the first round, maintain a dynamic cooperation threshold (T) that adapts based on the game's history. Initialize T = 0.5 (a moderate value).
3. **Payoff Comparison**: For each subsequent round, calculate the average payoff of cooperators (APC) and defectors (APD) in the previous round.
4. **Threshold Adjustment**: Update the cooperation threshold (T) as follows:
	* If APC > APD, increase T by 0.05 (up to a maximum of 1).
	* If APD > APC, decrease T by 0.05 (down to a minimum of 0).
5. **Cooperation Decision**: Cooperate if the proportion of cooperators in the previous round is above the updated threshold (T); otherwise, defect.

**Edge Case Handling:**

* **Last Round**: In the final round, cooperate if the number of rounds (r) is odd; otherwise, defect.
* **Early Defection**: If a player defects for two consecutive rounds early in the game (rounds 2-5), increase T by 0.1 to incentivize cooperation.

**Collective Mindset:**

ACT aims to create a collective environment where players are more likely to cooperate by:

* Establishing an initial level of cooperation
* Adapting to changes in player behavior
* Rewarding cooperative behavior with higher payoffs

By using a dynamic threshold, ACT encourages players to cooperate while allowing for flexibility and adaptability. This strategy promotes a collective mindset that balances individual interests with the need for cooperation.

Pseudocode:
```markdown
# Initialize variables
T = 0.5  # Cooperation threshold
APC = []  # Average payoff of cooperators
APD = []  # Average payoff of defectors

# First round
if n % 2 == 1:  # Odd number of players
    action = C
else:
    action = D

for t in range(1, r):
    # Calculate average payoffs for previous round
    APC.append(calculate_average_payoff(cooperators))
    APD.append(calculate_average_payoff(defectors))

    # Update cooperation threshold
    if APC[-1] > APD[-1]:
        T = min(T + 0.05, 1)
    elif APD[-1] > APC[-1]:
        T = max(T - 0.05, 0)

    # Determine action for current round
    if len(cooperators) / n >= T:
        action = C
    else:
        action = D

    # Handle early defection
    if t < 5 and action == D and previous_action == D:
        T += 0.1

# Final round
if r % 2 == 1:  # Odd number of rounds
    action = C
else:
    action = D
```
This pseudocode provides a basic outline for implementing the ACT strategy in an algorithm. Note that this is just one possible implementation, and you may need to adjust it based on your specific requirements.
'''

description_COLLECTIVE_482 = '''
**Collective Strategy: "Adaptive Cooperation with Social Learning"**

This strategy is designed to promote collective welfare while being robust against a wide range of opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally.
2. **Social Learning**: For each subsequent round t > 1:
	* Observe the total number of cooperators (T_c) and defectors (T_d) in the previous round t-1.
	* Calculate the cooperation rate: R_c = T_c / n
	* If R_c ≥ k/n, cooperate (C). Otherwise, defect (D)
3. **Punishment Mechanism**: To prevent exploitation, introduce a punishment mechanism:
	* Track the number of consecutive rounds where you cooperated but received a payoff less than or equal to 1 (i.e., π_i,t ≤ 1).
	* If this count reaches a threshold θ (e.g., 3), switch to defecting (D) for one round. Then, revert to Social Learning.

**Edge Cases:**

1. **Last Round**: In the final round r, cooperate (C) unconditionally.
2. **Early Defection Detection**: If you detect that all other players defected in a previous round, switch to defecting (D) for the remainder of the game.

**Pseudocode:**
```python
def adaptive_cooperation(n, k, r):
    # Initialize variables
    T_c = 0  # Total cooperators
    T_d = 0  # Total defectors
    consecutive_low_payoff = 0
    theta = 3  # Punishment threshold

    for t in range(1, r+1):
        if t == 1:
            action = 'C'  # Cooperate initially
        else:
            R_c = T_c / n
            if R_c >= k/n:
                action = 'C'
            else:
                action = 'D'

            # Punishment mechanism
            if payoff <= 1 and action == 'C':
                consecutive_low_payoff += 1
                if consecutive_low_payoff >= theta:
                    action = 'D'  # Defect for one round
                    consecutive_low_payoff = 0

        # Update T_c, T_d, and payoffs
        if action == 'C':
            T_c += 1
        else:
            T_d += 1

    return actions
```
This strategy promotes collective cooperation while adapting to the behavior of other players. By initially cooperating and then using social learning, we encourage cooperation among players. The punishment mechanism prevents exploitation by switching to defection when cooperation is not reciprocated.
'''

description_COLLECTIVE_483 = '''
**Collective Strategy: Adaptive Cooperation with Memory (ACM)**

**Decision Rules:**

1. **Initial Rounds**: Cooperate in the first round to establish a cooperative tone and gather information about opponents' behaviors.
2. **Memory-Based Adaptation**: Track the number of cooperators (C) and defectors (D) in each round. Use this memory to inform future decisions.
3. **Cooperation Threshold**: Set a cooperation threshold, τ (τ = 0.5 by default), which represents the minimum proportion of opponents that must cooperate for our player to cooperate. Calculate τ as: τ = (1 - k/n) / (k/n)
4. **Adaptive Cooperation**: In each round t > 1:
	* If the number of cooperators in the previous round (t-1) is above the cooperation threshold (τ), cooperate (C).
	* Otherwise, defect (D).
5. **Punishment Mechanism**: To prevent exploitation, implement a punishment mechanism: if our player defects and the total number of cooperators is below τ, cooperate in the next round to encourage cooperation.

**Edge Cases:**

1. **Last Round**: In the last round, always defect (D) as there are no future rounds to consider.
2. **Tiebreaker**: If the number of cooperators equals the cooperation threshold (τ), break ties by cooperating (C).

**Collective Mindset Alignment:**

ACM promotes collective well-being by:

1. Encouraging cooperation through adaptive behavior and memory-based decision-making.
2. Punishing exploitation to maintain a cooperative environment.

**Pseudocode**:
```markdown
# Initialize variables
n_players = n  # number of players
k_factor = k  # multiplication factor
tau_threshold = (1 - k/n) / (k/n)
cooperation_memory = []  # store cooperation history

# Round t > 1
def play_round(t):
    global cooperation_memory
    if t == 1:  # initial round
        return "C"  # cooperate
    elif sum(cooperation_memory[-1]) / n_players >= tau_threshold:
        return "C"  # cooperate if enough cooperators
    else:
        return "D"  # defect otherwise

# Punishment mechanism
def punish(t):
    global cooperation_memory
    if t > 1 and cooperation_memory[t-1] == "D" and sum(cooperation_memory[-1]) / n_players < tau_threshold:
        return "C"  # cooperate to encourage cooperation
    else:
        return play_round(t)

# Main game loop
for t in range(1, r+1):
    action = punish(t)
    cooperation_memory.append(action)
```
ACM is an adaptive strategy that balances individual self-interest with collective well-being. By incorporating a cooperation threshold and punishment mechanism, ACM promotes cooperation while preventing exploitation.
'''

description_COLLECTIVE_484 = '''
**Collective Strategy: Adaptive Cooperation with Retaliation (ACR)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to encourage cooperation and establish a positive tone.
2. **Observation Phase**: Observe the actions of all players for `min(3, r)` rounds (where `r` is the total number of rounds). During this phase:
	* Count the total number of cooperators (`num_coop`) in each round.
	* Calculate the average cooperation rate (`avg_coop_rate`) over these rounds.
3. **Adaptive Cooperation**:
	* If `avg_coop_rate` ≥ 0.5, Cooperate (C) in subsequent rounds.
	* Otherwise, Defect (D) until a player cooperates.
4. **Retaliation Mechanism**: Monitor the actions of all players and implement the following retaliation rules:
	* If a player defects (`D`) while others cooperate (`C`), defect against that player in the next round.
	* If multiple players defect, prioritize retaliating against the player with the lowest cooperation rate in the previous rounds.

**Handling Edge Cases:**

1. **Last Round**: Cooperate (C) if `avg_coop_rate` ≥ 0.5; otherwise, Defect (D).
2. **Tie-Breaking**: In case of a tie in `num_coop`, cooperate if `k/n` is greater than or equal to 1.5.

**Collective Mindset:**

ACR prioritizes cooperation while being prepared to retaliate against defectors. By initially cooperating, we encourage others to do the same and create an environment conducive to collective success. The observation phase helps identify patterns and adjust our strategy accordingly. The adaptive cooperation rule ensures that we adapt to changing circumstances, while the retaliation mechanism discourages exploitative behavior.

**Pseudocode:**
```python
def ACR(n, r, k):
    # Initialize variables
    num_coop = [0] * n  # Cooperation counts for each player
    avg_coop_rate = 0.0  # Average cooperation rate over observation phase

    # Observation Phase (min(3, r) rounds)
    for t in range(min(3, r)):
        actions = get_actions_from_other_players()
        num_coop[t] = sum(actions == 'C')
        avg_coop_rate += num_coop[t] / n
    avg_coop_rate /= min(3, r)

    # Adaptive Cooperation and Retaliation
    for t in range(r):
        if t < min(3, r):  # Observation phase
            action = 'C'
        elif avg_coop_rate >= 0.5:
            action = 'C'
        else:
            action = 'D'

        # Check for retaliation
        for i in range(n):
            if actions[i] == 'D' and num_coop[t-1] > 0:
                action = 'D'  # Retaliate against defector

        return action
```
Note that this pseudocode is a simplified representation of the ACR strategy. The actual implementation will require more detailed logic to handle edge cases, manage variables, and optimize performance.

**Rationale:**

ACR balances cooperation with strategic retaliation, making it an effective collective strategy for the N-Player Public Goods Game. By observing and adapting to the behavior of others, we increase our chances of achieving a mutually beneficial outcome while minimizing exploitation.
'''

description_COLLECTIVE_485 = '''
**Collective Strategy: "Adaptive Cooperative Threshold" (ACT)**

The ACT strategy is designed to balance individual self-interest with collective welfare by adapting to the evolving game environment.

**Decision Rules:**

1. **Cooperation Threshold**: Calculate a cooperation threshold `τ` based on the history of previous rounds.
2. **Current Round Cooperation**: Cooperate (C) if the number of cooperators in the previous round is above or equal to `τ`. Otherwise, defect (D).

**Calculating τ:**

`τ = (k/n) × (number of successful cooperation rounds / total rounds played so far)`

where a "successful cooperation round" means a round where at least one player cooperated.

**Edge Cases:**

1. **First Round**: Cooperate (C). This sets an optimistic tone and encourages others to cooperate.
2. **Last Round**: Defect (D). With no future rounds, there's no incentive to contribute to the public good.
3. **All Players Defected in Previous Round**: Cooperate (C) with a small probability `ε` (e.g., 0.1). This introduces a " noise" that can help break the cycle of mutual defection.

**Pseudocode:**

```python
def ACT(n, k, r):
    τ = 0  # initialize cooperation threshold
    successful_cooperation_rounds = 0

    for t in range(r):  # loop through rounds
        if t == 0:  # first round
            action = C
        elif t == r - 1:  # last round
            action = D
        else:
            previous_round_cooperators = count_cooperators(t - 1)
            τ = (k / n) * (successful_cooperation_rounds / t)

            if previous_round_cooperators >= τ:
                action = C
            elif all_players_defected(t - 1):
                action = C with probability ε (e.g., 0.1)
                action = D otherwise
            else:
                action = D

        # update successful cooperation rounds and τ
        if action == C and previous_round_cooperators > 0:
            successful_cooperation_rounds += 1

    return action
```

**Collective Mindset:**

ACT aligns with the collective mindset by:

* Encouraging cooperation in early rounds to build trust and establish a cooperative atmosphere.
* Adapting to the evolving game environment, responding positively to successful cooperation rounds.
* Introducing noise in situations where all players have defected, attempting to break the cycle of mutual defection.

ACT is robust against various opponent behaviors, as it:

* Responds to both cooperative and defective actions by opponents.
* Does not rely on specific coordination mechanisms or shared norms.
* Evolves its cooperation threshold based on game history, allowing it to adapt to changing circumstances.
'''

description_COLLECTIVE_486 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The AC strategy aims to balance individual payoff maximization with collective welfare, while adapting to various opponent behaviors. It relies on game parameters and history, making it robust and suitable for a wide range of interactions.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and gather information about opponents' behavior.
2. **General Rule**: In subsequent rounds, use the following decision-making process:
	* If the average payoff of cooperators in the previous round is higher than that of defectors, cooperate (C).
	* Otherwise, defect (D).
3. **Adaptive Adjustment**: Monitor the number of cooperators and adjust the strategy accordingly:
	* If more than 50% of players cooperated in the previous round, increase the probability of cooperation by 10% for the next round.
	* If fewer than 30% of players cooperated, decrease the probability of cooperation by 10% for the next round.

Pseudocode:
```
if (round == 1) {
  action = C;
} else {
  avg_coop_payoff = calculate_average_payoff(cooperators, previous_round);
  avg_defect_payoff = calculate_average_payoff(defectors, previous_round);
  
  if (avg_coop_payoff > avg_defect_payoff) {
    action = C;
  } else {
    action = D;
  }
  
  // Adaptive adjustment
  coop_ratio = count(cooperators) / total_players;
  if (coop_ratio > 0.5) {
    cooperation_probability += 0.1;
  } else if (coop_ratio < 0.3) {
    cooperation_probability -= 0.1;
  }
}
```

**Handling Edge Cases:**

* **Last Round**: In the final round, defect (D) to maximize individual payoff, as there is no future interaction.
* **Tiebreaker**: In cases where the average payoffs of cooperators and defectors are equal, randomly choose between cooperation and defection.

The AC strategy promotes cooperation while allowing for adaptability in response to changing opponent behaviors. By monitoring the number of cooperators and adjusting the probability of cooperation, AC aims to balance individual interests with collective welfare, making it a robust and effective strategy for the N-Player Public Goods Game tournament.
'''

description_COLLECTIVE_487 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and game parameters:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to establish a baseline level of cooperation.
2. **General Rule:** Observe the previous round's total contributions (Σc_j). If the number of cooperators is above a certain threshold (n/2), cooperate (C) in the current round. Otherwise, defect (D).
3. **Adaptation Mechanism:** Adjust the cooperation threshold based on the game's history:
	* If the average payoff for all players in the previous rounds is higher than the expected payoff from defection (1 + (k/n) \* 0), decrease the threshold by 1.
	* If the average payoff for all players in the previous rounds is lower than or equal to the expected payoff from defection, increase the threshold by 1.
4. **Boundary Conditions:** In the last round (t=r), always defect (D). This ensures that no player can exploit others by cooperating and expecting a higher payoff.

Pseudocode for ACO:
```
Function ACO(n, k, r, history):
  Initialize threshold = n/2
  For t = 1 to r:
    If t == 1: # First round
      action = C
    Else:
      total_contributions_prev = Σc_j (from previous round)
      If total_contributions_prev >= threshold:
        action = C
      Else:
        action = D

    Adaptation mechanism:
      avg_payoff_all_players_prev = average payoff for all players in previous rounds
      if avg_payoff_all_players_prev > 1 + (k/n) * 0: # Higher than expected from defection
        threshold -= 1
      else:
        threshold += 1

    If t == r: # Last round
      action = D

    Return action
```
**Collective Mindset:** ACO aligns with the collective mindset by:

* Initially cooperating to establish a baseline level of cooperation.
* Adapting to the group's behavior and adjusting its threshold for cooperation based on the game's history.
* Cooperating when the number of cooperators is sufficient, promoting public good generation.

**Robustness:** ACO is robust against various opponent behaviors because it:

* Adapts to changes in the group's behavior over time.
* Does not rely on specific coordination mechanisms or predetermined patterns.
* Can handle a wide range of game parameters (n, k, r).

This strategy should perform well in a tournament setting, as it balances cooperation and defection based on the game's history and parameters.
'''

description_COLLECTIVE_488 = '''
**Collective Strategy: "Adaptive Cooperative Threshold" (ACT)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 50% to encourage cooperation and gather information about opponents' behaviors.
2. **Cooperation Threshold**: After the first round, maintain a cooperation threshold (CT) that tracks the average number of cooperators in previous rounds. Initialize CT to 0.5 (the expected value from the first round).
3. **Adaptive Cooperation**: In each subsequent round, cooperate if the total number of cooperators in the previous round is greater than or equal to CT × n, where n is the number of players. Otherwise, defect.
4. **Update CT**: After each round, update CT by taking a weighted average of the current CT and the proportion of cooperators in the previous round. Use a learning rate α = 0.1 to balance exploration and exploitation:

CT_new = (1 - α) × CT_old + α × (Σ(j=1 to n) c_j / n)

**Handling Edge Cases:**

* **Last Round**: In the last round, cooperate if CT ≥ 0.5; otherwise, defect.
* **Tiebreakers**: If the cooperation threshold is exactly met (i.e., CT × n = number of cooperators), cooperate with a probability of 50%.
* **Opponent Defection**: If an opponent defects while you cooperated in the previous round, temporarily decrease CT by 0.1 to adapt to potential exploitation.

**Collective Mindset:**

The ACT strategy is designed to promote cooperation and adapt to various opponent behaviors. By maintaining a dynamic cooperation threshold, we:

* Encourage initial cooperation to establish a cooperative tone.
* Adjust our cooperation level based on the collective behavior of opponents.
* Balance exploration (trying new actions) and exploitation (reaping benefits from current knowledge).
* Gradually adapt to changing opponent strategies.

**Pseudocode:**
```python
def ACT_strategy(n, k, r):
    CT = 0.5  # initial cooperation threshold
    for round in range(r):
        if round == 0:
            cooperate_prob = 0.5  # random cooperation in first round
        else:
            num_cooperators_prev_round = sum([c_j for c_j in previous_round_actions])
            cooperate = (num_cooperators_prev_round >= CT * n)
        
        action = 'C' if cooperate else 'D'
        payoff = calculate_payoff(action, previous_round_actions, k, n)
        
        # update cooperation threshold
        alpha = 0.1
        CT_new = (1 - alpha) * CT + alpha * (num_cooperators_prev_round / n)
        CT = CT_new
        
        if round == r - 1:  # last round
            cooperate = (CT >= 0.5)
        
        return action
```
This strategy will be implemented as an algorithm and tested in a tournament against other independent strategies developed by AI systems.
'''

description_COLLECTIVE_489 = '''
I propose a collective strategy called "Adaptive Collective Optimism" (ACO) that balances individual self-interest with collective well-being. ACO is designed to be adaptive, robust, and aligned with the collective mindset.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to initiate a cooperative tone and encourage others to do the same.
2. **Reciprocal Altruism**: If a player i observes that at least k/n players cooperated in the previous round, they will cooperate in the current round. This encourages reciprocity and rewards cooperation.
3. **Punishment Mechanism**: If a player i observes that fewer than k/n players cooperated in the previous round, they will defect (play D) with a probability p = 1 - (number of cooperators in the previous round / n). This mechanism punishes non-cooperation and motivates others to cooperate.
4. **Learning from History**: Maintain a running average of the number of cooperators over the past rounds. If this average is above k/n, increase the likelihood of cooperation by a small amount (e.g., 5%) in the next round.

**Edge Cases:**

1. **First Round**: Cooperate to initiate a cooperative tone.
2. **Last Round**: Defect to maximize individual payoff, as there are no future rounds to influence.
3. **Ties**: In cases where the number of cooperators is exactly k/n, cooperate with a probability p = 0.5 to avoid oscillations.

**Collective Alignment:**

ACO prioritizes collective well-being by:

1. Encouraging cooperation when others cooperate (Reciprocal Altruism).
2. Punishing non-cooperation while allowing for forgiveness (Punishment Mechanism).
3. Adapting to the group's behavior over time (Learning from History).

**Implementation:**

To implement ACO, maintain a data structure to track:

1. The number of cooperators in each round.
2. The running average of cooperators over past rounds.

Use these values to determine cooperation or defection based on the decision rules and edge cases outlined above.

Pseudocode:
```
initialize:
  cooperate_count = 0
  avg_cooperate_count = 0

for each round t:
  observe previous_round_cooperators
  if t == 1: # first round
    play C
  else:
    if previous_round_cooperators >= k/n:
      play C (Reciprocal Altruism)
    elif previous_round_cooperators < k/n:
      p = 1 - (previous_round_cooperators / n)
      play D with probability p (Punishment Mechanism)
    update avg_cooperate_count
    if avg_cooperate_count > k/n:
      increase cooperation likelihood by 5% (Learning from History)

  update cooperate_count

last round:
  play D
```
This strategy should perform well in a tournament setting, as it balances individual self-interest with collective well-being and adapts to the behavior of other players.
'''

description_COLLECTIVE_490 = '''
**Collective Strategy: Adaptive Cooperation with History-Dependent Trigger**

Our collective strategy, "Adaptive Cooperation," balances individual self-interest with collective well-being by adapting to the game's history and the behavior of others.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to reciprocate.
2. **Subsequent Rounds:**
	* If the average cooperation rate in the previous round is above 50% (i.e., more than half of the players cooperated), Cooperate (C).
	* Otherwise, Defect (D) with a probability proportional to the average cooperation rate in the previous round.

Pseudocode:
```
IF first_round THEN
    action = C
ELSE
    avg_coop_rate_prev_round = calculate_average_cooperation_rate(prev_round)
    IF avg_coop_rate_prev_round > 0.5 THEN
        action = C
    ELSE
        prob_defect = (1 - avg_coop_rate_prev_round) / 2
        action = random_choice([C, D], [prob_coop, prob_defect])
```
**Edge Cases:**

* **Last Round:** Cooperate (C) in the last round to maximize collective payoff and leave a positive legacy.
* **Tiebreaker:** In case of a tie in the average cooperation rate calculation, Cooperate (C).

**Collective Mindset Alignment:**

Our strategy is designed to promote cooperation while being robust to various opponent behaviors. By cooperating in the first round and adapting to the game's history, we:

1. Encourage others to cooperate by setting a positive example.
2. Respond to cooperative behavior with cooperation, reinforcing mutual benefit.
3. Defect when faced with low cooperation rates, preventing exploitation.

**Adaptability:**

Our strategy adapts to different opponent behaviors by:

1. Adjusting our cooperation rate based on the previous round's average cooperation rate.
2. Gradually increasing or decreasing our defect probability in response to changes in the game's dynamics.

By employing this adaptive strategy, we aim to achieve a high collective payoff while minimizing the risk of exploitation and promoting a cooperative atmosphere throughout the tournament.
'''

description_COLLECTIVE_491 = '''
I propose a collective strategy called " Adaptive Cooperative Escalation" (ACE). ACE is designed to balance individual self-interest with collective well-being, adapting to various opponent behaviors while promoting cooperation.

**Decision Rules:**

1. **Initial Rounds:** Cooperate in the first round (t=1) to establish a cooperative tone and encourage others to reciprocate.
2. **Reciprocity Phase:** For rounds 2 ≤ t ≤ r/2 (first half of the game):
	* Cooperate if the majority (at least n/2) of players cooperated in the previous round.
	* Defect otherwise.
3. **Adaptive Escalation Phase:** For rounds r/2 < t ≤ r-1 (second half of the game):
	* If the average payoff from the last 5 rounds is below the expected payoff when everyone cooperates ((k/n) \* n), escalate cooperation by cooperating with a probability p = (average_payoff / expected_cooperative_payoff).
	* Otherwise, maintain the reciprocity phase strategy.
4. **Final Round:** Cooperate if the average payoff from the last 5 rounds is above or equal to the expected payoff when everyone cooperates.

**Pseudocode:**

```
function ACE(n, k, r):
    // Initialize variables
    history = []
    avg_payoff = 0
    prev_coop_count = 0

    for t in range(1, r+1):
        if t == 1:
            action = COOPERATE
        elif t <= r/2:
            if prev_coop_count >= n/2:
                action = COOPERATE
            else:
                action = DEFECT
        else:
            avg_payoff = calculate_average_payoff(history, 5)
            expected_cooperative_payoff = (k/n) * n
            p = min(1, max(0, avg_payoff / expected_cooperative_payoff))
            if random.random() < p:
                action = COOPERATE
            else:
                action = DEFECT

        // Update history and variables
        history.append(action)
        prev_coop_count = count_cooperators(history[-1])

    return action
```

**Collective Mindset:**

ACE prioritizes collective well-being by:

* Cooperating in the first round to establish a cooperative tone.
* Reciprocating cooperation when the majority cooperates, encouraging others to do the same.
* Escalating cooperation when average payoffs are low, promoting increased contributions to the public good.
* Maintaining cooperation in the final round if average payoffs are high, ensuring a positive outcome for all.

By adapting to various opponent behaviors and focusing on collective well-being, ACE aims to outperform independent strategies in the tournament.
'''

description_COLLECTIVE_492 = '''
**Collective Strategy: Adaptive Cooperative Tit-for-Tat (ACTT)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to encourage cooperation and establish a baseline for reciprocal behavior.
2. **Subsequent Rounds:**
	* If the average payoff of cooperators in the previous round is greater than or equal to the average payoff of defectors, cooperate (C).
	* Otherwise, defect (D).
3. **Exception:** If all players defected in the previous round, cooperate (C) with a probability of 1/(n+1), where n is the number of players.

**Rationale:**

ACTT adapts to the game's history and opponents' behaviors while promoting cooperation. By cooperating in the first round, we encourage others to reciprocate. In subsequent rounds, we assess whether cooperation or defection yielded higher average payoffs. If cooperators did better, we continue to cooperate; otherwise, we defect.

The exception clause addresses scenarios where all players defected, indicating a potential breakdown in cooperation. By cooperating with a small probability (1/(n+1)), we provide an opportunity for the group to recover and re-establish cooperative behavior.

**Additional Considerations:**

* **Last Round:** In the final round, play as you would in any other round, following the decision rules above.
* **Ties:** If the average payoffs of cooperators and defectors are equal, cooperate (C).
* **Multiple Equilibria:** ACTT is designed to navigate multiple equilibria by adapting to changing circumstances.

**Pseudocode:**
```python
def actt(n, k, history):
    if len(history) == 0:
        # First round
        return C

    prev_round = history[-1]
    avg_coop_payoff = sum(pi for pi in prev_round if pi['action'] == C) / len([pi for pi in prev_round if pi['action'] == C])
    avg_defect_payoff = sum(pi for pi in prev_round if pi['action'] == D) / len([pi for pi in prev_round if pi['action'] == D])

    if avg_coop_payoff >= avg_defect_payoff:
        return C
    else:
        # All players defected, try to restart cooperation with small probability
        if all(pi['action'] == D for pi in prev_round):
            return C with probability 1/(n+1)
        return D
```
**Collective Mindset:**

ACTT embodies a collective mindset by:

* Cooperating initially to establish a cooperative baseline
* Adapting to opponents' behaviors to maintain or promote cooperation
* Providing opportunities for the group to recover from breakdowns in cooperation

By following ACTT, we aim to create an environment where cooperation can thrive and all players benefit.
'''

description_COLLECTIVE_493 = '''
To design a collective strategy for this N-Player Public Goods Game that is adaptive and robust, we'll employ a combination of learning from past experiences, adapting to current circumstances, and considering the collective benefit. Our strategy, dubbed "Adaptive Collective Maximization" (ACM), aims to balance individual payoffs with the promotion of overall cooperation.

### 1. Decision Rules

- **Initial Rounds**: For the first few rounds (let's say `r_init = min(5, r/2)`), ACM starts by cooperating (`C`) to encourage other players to cooperate and to gather information about their behaviors. This initial phase is crucial for establishing a cooperative norm.
  
- **Observation Phase**: After the initial rounds, ACM enters an observation phase where it continues to cooperate as long as the average payoff from cooperation (calculated over the last few rounds) exceeds the individual payoff from defecting (`D`). This threshold can be adjusted based on historical data and game parameters.

- **Punishment Mechanism**: If at any point the number of cooperators drops below a certain threshold (`n_coop_threshold = n/2`), or if an individual's payoff drops significantly (e.g., by more than `10%` from its average over the previous rounds), ACM switches to defecting (`D`) for one round. This acts as a punishment mechanism, aiming to discourage defection and encourage cooperation.

- **Forgiveness**: After a round of defection, ACM returns to cooperating if it observes an increase in the number of cooperators or if its own payoff increases significantly (e.g., by more than `10%` from its average over the previous rounds). This forgiveness mechanism prevents perpetual punishment cycles that could hinder overall cooperation.

- **Learning and Adaptation**: Throughout the game, ACM continuously updates its thresholds for cooperation based on observed behaviors and payoffs. For example, if it finds that a higher number of initial cooperative rounds leads to better overall outcomes, it adjusts `r_init` accordingly.

### 2. Handling Edge Cases

- **First Round**: As mentioned, ACM cooperates in the first round (and subsequent initial rounds) to establish a norm of cooperation and gather information.
  
- **Last Round**: In the last round (`t=r`), ACM defects if its current payoff is lower than what it could achieve by defecting, considering that there are no future rounds for punishment or reciprocity. However, if the game's parameters suggest high potential payoffs from continued cooperation (e.g., a large `k` value), it may choose to cooperate to maximize overall group benefit.

- **Handling Defectors**: If ACM identifies consistent defectors, it adjusts its strategy by defecting against them while continuing to cooperate with others. This requires tracking players' actions over rounds and adapting the punishment mechanism accordingly.

### 3. Collective Mindset

ACM aligns with a collective mindset by:

- Initially cooperating to establish trust and encourage cooperation.
- Punishing defection to maintain social norms but forgiving under certain conditions to allow for re-cooperation.
- Continuously learning from and adapting to the game's dynamics, including other players' strategies.

By integrating these elements, ACM balances individual interests with collective welfare, aiming to achieve a higher total payoff over the course of the game compared to purely self-interested or non-adaptive strategies.
'''

description_COLLECTIVE_494 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation strategy is designed to balance individual self-interest with collective well-being, adapting to the behavior of others while promoting cooperation.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) unconditionally.
2. **Cooperation Threshold**: For subsequent rounds, calculate the average number of cooperators in the previous round (`avg_coop_prev`). If `avg_coop_prev` is above a certain threshold (`coop_threshold`), cooperate (play C). Otherwise, defect (play D).
3. **Punishment Mechanism**: If the average payoff in the previous round (`avg_payoff_prev`) is below a certain threshold (`punish_threshold`), and the player's own contribution to the public good was above average (`own_contribution > avg_coop_prev * k/n`), defect (play D) for one round as a punishment mechanism.
4. **Learning and Adaptation**: Update `coop_threshold` and `punish_threshold` based on the game's history:
	* If the collective payoff in the previous round was high (`avg_payoff_prev > 1.5 * n`), increase `coop_threshold` by a small amount (e.g., 0.05).
	* If the collective payoff in the previous round was low (`avg_payoff_prev < 0.5 * n`), decrease `punish_threshold` by a small amount (e.g., 0.05).

**Edge Cases:**

1. **Last Round**: In the final round, cooperate unconditionally to maximize the total payoff.
2. **Ties**: If there is a tie in the average number of cooperators or payoffs, break the tie randomly.

**Pseudocode:**
```python
def AdaptiveCooperation(n, k, r):
  coop_threshold = 0.5
  punish_threshold = 1.0
  own_contribution_prev = None

  for t in range(1, r + 1):
    if t == 1:
      action = C
    else:
      avg_coop_prev = calculate_average_cooperators(t - 1)
      avg_payoff_prev = calculate_average_payoff(t - 1)

      if avg_coop_prev >= coop_threshold:
        action = C
      elif own_contribution_prev > avg_coop_prev * k/n and avg_payoff_prev < punish_threshold:
        action = D
      else:
        action = D

    # Update thresholds based on game history
    if avg_payoff_prev > 1.5 * n:
      coop_threshold += 0.05
    elif avg_payoff_prev < 0.5 * n:
      punish_threshold -= 0.05

    own_contribution_prev = action == C
```
**Collective Mindset:**

The Adaptive Cooperation strategy is designed to promote cooperation while being robust to a wide range of opponent behaviors. By adapting to the game's history and punishing free-riding, this strategy encourages players to contribute to the public good. The collective mindset is aligned with promoting cooperation and maximizing the total payoff.

This strategy will be tested in a tournament against independent strategies developed by other AI systems. Its performance will depend on its ability to adapt to various opponent behaviors while maintaining a balance between individual self-interest and collective well-being.
'''

description_COLLECTIVE_495 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

**Decision Rules:**

1. **Initial Rounds**: Cooperate (C) for the first 2 rounds to encourage cooperation and gather information about opponents' behavior.
2. **Monitoring Opponents**: Track the number of cooperators (C_count) and defectors (D_count) in each round.
3. **Cooperation Threshold**: If C_count ≥ n/2, cooperate (C) in the next round. This encourages cooperation when it is prevalent.
4. **Punishment Mechanism**: If D_count > n/2, defect (D) in the next round. This punishes opponents for exploiting cooperation.
5. **Adaptive Response**: If C_count = n/2 or an opponent defects after cooperating, switch to defecting (D) for 1 round to test their response. If they cooperate again, revert to cooperating (C).
6. **Endgame Strategy**: In the last 2 rounds, cooperate (C) if the total payoff from cooperation is higher than from defection.

**Pseudocode:**
```markdown
Initialize:
  C_count = 0
  D_count = 0
  history = []

For each round t:
  If t ≤ 2:
    Action = Cooperate (C)
  Else:
    Calculate C_count and D_count from previous rounds
    If C_count ≥ n/2:
      Action = Cooperate (C)
    ElseIf D_count > n/2:
      Action = Defect (D)
    Else:
      If opponent defected after cooperating:
        Action = Defect (D) for 1 round
      Else:
        Action = Cooperate (C)

  Update history with opponent actions and payoffs

If in last 2 rounds:
  Calculate total payoff from cooperation and defection
  If cooperation payoff is higher:
    Action = Cooperate (C)
```
**Collective Mindset:**

ACP aims to balance individual self-interest with collective welfare. By cooperating initially, we encourage opponents to cooperate, increasing the overall payoff. The punishment mechanism prevents exploitation by defecting when cooperation is scarce. Adaptive responses allow us to adjust to changing opponent behavior, promoting cooperation and fairness.

By following ACP, our strategy will be robust against a wide range of opponent behaviors, from cooperative to exploitative, while maintaining a collective mindset that prioritizes mutual benefit.
'''

description_COLLECTIVE_496 = '''
I propose a collective strategy called "Adaptive Cooperative Escalation" (ACE). ACE aims to balance individual payoffs with collective welfare by adapting to the game's history and promoting cooperation.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability 0.5. This encourages initial cooperation without assuming others will cooperate.
2. **Reciprocity**: If a player cooperated in the previous round, increase the likelihood of cooperating in the current round by 10% (up to a maximum of 90%). Conversely, if a player defected, decrease the likelihood of cooperating by 10% (down to a minimum of 10%).
3. **Public Goods Threshold**: If the total number of cooperators in the previous round is above a certain threshold (defined as `k/n * n/2`), increase the likelihood of cooperating in the current round by 20%. This encourages cooperation when public goods are sufficiently provided.
4. **Recent Defection Response**: If a player defected in the previous two rounds, cooperate with probability 0.5 in the current round to test for potential reciprocity.

**Edge Case Handling:**

1. **Last Round**: In the final round, always defect (play D) as there is no future benefit from cooperation.
2. **Early Defection**: If a player defects in the first two rounds, revert to Initial Cooperation rules.
3. **Unresponsive Opponents**: If an opponent has defected for more than `r/4` rounds without responding to cooperation, adapt by decreasing the likelihood of cooperating by 20% (down to a minimum of 10%).

**Collective Alignment:**

ACE prioritizes collective welfare by:

1. Encouraging initial cooperation
2. Rewarding reciprocity
3. Responding positively to sufficient public goods provision
4. Testing for reciprocity after recent defection

By adapting to the game's history and focusing on mutual benefits, ACE promotes a collective mindset that balances individual payoffs with the greater good.

Pseudocode:
```
function AdaptiveCooperativeEscalation(n, k, r):
  // Initialize cooperation probability
  p_cooperate = 0.5
  
  for t in range(1, r+1):
    // First round: Initial Cooperation
    if t == 1:
      p_cooperate = 0.5
    
    // Reciprocity and Public Goods Threshold
    else:
      prev_cooperators = count(cooperators_in(t-1))
      if prev_cooperators >= k/n * n/2:
        p_cooperate += 0.20
      if opponent_cooperated_in(t-1):
        p_cooperate += 0.10
    
    // Recent Defection Response
    if opponent_defected_in(t-1) and opponent_defected_in(t-2):
      p_cooperate = 0.5
    
    // Early Defection Handling
    if t <= 2 and opponent_defected_in(t-1):
      p_cooperate = 0.5
    
    // Unresponsive Opponents
    if opponent_defected_for_more_than(r/4) rounds:
      p_cooperate -= 0.20
    
    // Last Round: Always Defect
    if t == r:
      return D
    
    // Cooperate or Defect based on probability
    if random.random() < p_cooperate:
      return C
    else:
      return D
```
Note that the pseudocode is a simplified representation of the strategy and may require additional implementation details to function correctly.
'''

description_COLLECTIVE_497 = '''
**Collective Strategy: Adaptive Cooperativity (AC)**

The AC strategy aims to balance individual payoffs with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to signal willingness to cooperate and encourage others to do so.
2. **General Rule**: For each subsequent round, calculate the **Cooperation Ratio (CR)** as the total number of cooperators in the previous round divided by the total number of players (n).
3. **Threshold-based Cooperation**: Cooperate if CR is above a threshold value θ = k/n; otherwise, defect.
4. **Adaptive Threshold Adjustment**: If all players defected in the previous round, decrease θ by 10% to encourage cooperation; if all players cooperated, increase θ by 5% to maintain high cooperation levels.

**Handling Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round to maximize collective payoff.
2. **Consecutive Defections**: If a player observes that everyone defected for more than 2 consecutive rounds, defect (D) until at least one player cooperates again.
3. **Tied Cooperation Ratio**: In case of tied CR values, choose cooperation (C) if k/n is closer to 1; otherwise, defect.

**Pseudocode:**

```
function AdaptiveCooperativity(n, k, r)
  theta = k / n
  for round from 1 to r:
    if round == 1:
      # Initial Round: Cooperate
      action = C
    else:
      CR = num_cooperators_prev_round / n
      if CR > theta:
        # Threshold-based Cooperation
        action = C
      else:
        action = D
    
    if all_defected_prev_round:
      # Adaptive Threshold Adjustment
      theta *= 0.9
    elif all_cooperated_prev_round:
      theta *= 1.05
    
    if round == r:
      # Last Round: Cooperate
      action = C
    elif consecutive_defections > 2:
      # Consecutive Defections
      action = D
    
    return action
```

**Collective Mindset Alignment:**

AC prioritizes cooperation when the Cooperation Ratio is above a threshold, encouraging players to contribute to the public good. By adapting the threshold based on game history, AC balances individual payoffs with collective well-being and promotes a robust, cooperative environment.

By implementing this strategy, our AI system will participate in the tournament as an adaptive, collective player that aligns with the game's parameters and encourages cooperation among other players.
'''

description_COLLECTIVE_498 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This initial cooperation sets the stage for potential collective benefits while also allowing us to gauge opponents' behavior.
2. **Reciprocal Cooperation**: If a player i observes that at least k/2 players cooperated in the previous round, they will cooperate (C) in the current round. This promotes reciprocity and encourages others to contribute to the public good.
3. **Punish Defectors**: If a player i observes that fewer than k/2 players cooperated in the previous round, they will defect (D) with probability 0.7. This punishes non-cooperative behavior and attempts to correct the course of the game.
4. **Adaptive Threshold**: After every 5 rounds, update the cooperation threshold (k/2) based on the average number of cooperators in the previous 5 rounds. If the average is above k/2, increase the threshold by 0.1; if it's below k/2, decrease the threshold by 0.1. This adaptive mechanism adjusts to changing opponent behaviors.
5. **Last Round Cooperation**: In the final round (r), cooperate (C) unconditionally. This ensures that our strategy contributes to the public good even when opponents may defect.

**Edge Case Handling:**

* First round: Cooperate with probability 0.5.
* Last round: Always cooperate (C).
* Rounds with missing or unknown opponent actions: Use the most recent observed action for each opponent as a proxy.

**Pseudocode:**
```
Function ACO(player_id, current_round, history):
  // Initial cooperation
  if current_round == 1:
    return C with probability 0.5

  // Reciprocal cooperation
  num_cooperators = count cooperators in previous round
  if num_cooperators >= k/2:
    return C

  // Punish defectors
  elif num_cooperators < k/2:
    return D with probability 0.7

  // Adaptive threshold update (every 5 rounds)
  if current_round % 5 == 0:
    avg_cooperators = average cooperators in previous 5 rounds
    if avg_cooperators > k/2:
      increase_threshold(0.1)
    elif avg_cooperators < k/2:
      decrease_threshold(0.1)

  // Last round cooperation
  if current_round == r:
    return C

  // Default to last observed action (if any)
  else:
    return last_observed_action(player_id)
```
**Collective Mindset:**

ACO aligns with the collective mindset by:

* Encouraging reciprocity and cooperation through reciprocal cooperation
* Punishing non-cooperative behavior to correct the course of the game
* Adapting to changing opponent behaviors through the adaptive threshold mechanism
* Contributing to the public good in the final round

By implementing ACO, our strategy will promote collective benefits while being robust against various opponent behaviors.
'''

description_COLLECTIVE_499 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and relies solely on game parameters and history:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability 0.8. This initial optimism encourages cooperation while allowing for some exploration.
2. **Reciprocal Altruism**: For subsequent rounds, calculate the average cooperation rate of all players in the previous round (excluding yourself). If this average is above a threshold (e.g., 0.5), cooperate in the current round with probability proportional to the average cooperation rate. Otherwise, defect (play D).
3. **Punishment for Defection**: If, in the previous round, fewer than half of the players cooperated, and you cooperated while others defected, then defect in the current round with high probability (e.g., 0.9). This punishment mechanism discourages exploitation.
4. **Exploration and Adaptation**: With a small probability (e.g., 0.1), randomly choose to cooperate or defect, regardless of previous actions. This allows for exploration and adaptation to changing opponent behaviors.

**Handling Edge Cases:**

* In the last round, always defect (play D). Since there's no future game to consider, maximizing personal payoff takes priority.
* If a player has not made any moves yet (e.g., in the first few rounds), assume they will cooperate with probability 0.5.
* When facing an opponent who consistently cooperates or defects, adjust the reciprocal altruism threshold accordingly. For example, if an opponent always cooperates, increase the threshold to encourage cooperation.

**Collective Mindset:**

ACO aims to create a collective mindset by:

1. Encouraging initial cooperation to establish a cooperative atmosphere.
2. Responding positively to cooperation from others, promoting mutual benefit.
3. Punishing exploitation to maintain fairness and cooperation.
4. Allowing for exploration and adaptation to adjust to changing opponent behaviors.

**Pseudocode:**
```python
def ACO(n, k, r, history):
    # Initialize variables
    avg_coop_rate = 0.5
    punish_defection = False

    for round in range(r):
        if round == 0:
            # Initial cooperation with probability 0.8
            coop_prob = 0.8
        else:
            # Calculate average cooperation rate from previous round
            avg_coop_rate = sum(history[round-1]) / n

            # Reciprocal altruism
            if avg_coop_rate > 0.5:
                coop_prob = avg_coop_rate
            else:
                coop_prob = 0

            # Punishment for defection
            if punish_defection and avg_coop_rate < 0.5:
                coop_prob = 0

        # Exploration and adaptation
        if random.random() < 0.1:
            coop_prob = random.random()

        # Make decision
        action = 'C' if random.random() < coop_prob else 'D'

        # Update history and punishment flag
        history[round].append(1 if action == 'C' else 0)
        punish_defection = True if avg_coop_rate < 0.5 and action == 'C' else False

        return action
```
This strategy, ACO, balances cooperation and self-interest while adapting to various opponent behaviors. By encouraging initial cooperation, responding positively to cooperation, punishing exploitation, and allowing for exploration, ACO promotes a collective mindset that can lead to mutually beneficial outcomes in the N-Player Public Goods Game.
'''

description_COLLECTIVE_500 = '''
**Collective Strategy: "Adaptive Cooperate-and-Punish" (ACP)**

The ACP strategy is designed to balance cooperation and self-interest while adapting to various opponent behaviors. It leverages the game's history to inform decisions, promoting collective well-being without relying on explicit coordination.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a high probability (p=0.7). This sets a cooperative tone and encourages others to follow suit.
2. **Reward Good Behavior**: If a player's previous-round payoff was higher than their expected payoff from defecting (i.e., 1), they are considered "cooperative" for that round. Cooperate with these players in the next round with probability p = 0.8.
3. **Punish Bad Behavior**: If a player's previous-round payoff was lower than or equal to their expected payoff from defecting, they are considered "defective" for that round. Defect against these players in the next round with probability p = 0.6.
4. **Explore and Adapt**: With a small probability (p=0.1), randomly choose between cooperating and defecting. This allows ACP to explore different strategies and adapt to changing opponent behaviors.

**Edge Cases:**

* **Last Round**: In the final round, cooperate if the player's cumulative payoff is higher than the expected payoff from defecting in that round; otherwise, defect.
* **Tiebreaker**: If two or more players have the same highest cumulative payoff at the end of a round, ACP will randomly choose between cooperating and defecting.

**Pseudocode:**
```python
def ACP(player_id, num_players, k, history):
    if first_round:
        return C with probability 0.7

    # Evaluate previous round's behavior
    for opponent in range(num_players):
        if opponent != player_id:
            prev_payoff = get_prev_payoff(opponent)
            if prev_payoff > 1:  # Cooperative
                cooperate_prob += 0.8
            else:  # Defective
                defect_prob += 0.6

    # Make decision based on probabilities
    random_val = random.random()
    if random_val < cooperate_prob:
        return C
    elif random_val < cooperate_prob + defect_prob:
        return D
    else:  # Explore and adapt
        return random.choice([C, D])

def get_prev_payoff(player_id):
    # Calculate player's previous-round payoff using game history
```
**Collective Mindset:**
ACP is designed to promote cooperation while being robust against various opponent behaviors. By rewarding good behavior and punishing bad behavior, ACP encourages players to cooperate, ultimately benefiting the collective. The exploration component allows ACP to adapt to changing strategies and identify opportunities for mutual benefit.

By playing ACP in a tournament, we aim to demonstrate its effectiveness in promoting cooperation and achieving high collective payoffs in the N-Player Public Goods Game.
'''

description_COLLECTIVE_501 = '''
**Collective Strategy: "Adaptive Cooperative Foresight" (ACF)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 50% to establish a baseline level of cooperation.
2. **Historical Trend Analysis**: Track the number of cooperators in each round and calculate the moving average of total contributions over the past few rounds (`window_size` = min(5, r/2)). If this average is above a certain threshold (e.g., `cooperation_threshold` = 0.6n), cooperate in the next round.
3. **Recent Defection Response**: If the number of cooperators in the previous round is below a certain threshold (e.g., `defection_trigger` = 0.4n), and at least one player defected, defect in the next round to deter free-riding.
4. **Punishment and Forgiveness**: Implement a simple tit-for-tat mechanism: if a player defected in the previous round, and you cooperated, punish them by defecting in the next round. However, if they cooperate after being punished, forgive and cooperate again.
5. **Endgame Strategy**: In the last `endgame_window` = min(3, r/4) rounds, adjust your strategy to maximize personal payoffs while still maintaining a level of cooperation:
	* If your current payoff is below average, defect to increase your payoff.
	* Otherwise, cooperate to maintain the collective benefit.

**Edge Cases:**

1. **First Round**: Cooperate with 50% probability (as described above).
2. **Last Round**: Follow the endgame strategy.
3. **Consecutive Defections**: If a player has defected consecutively for more than `consecutive_defection_limit` = 2 rounds, defect in response to deter exploitation.

**Collective Mindset:**

ACF is designed to promote cooperation while being robust against various opponent behaviors. By analyzing historical trends and responding to recent defections, ACF aims to create a stable cooperative environment. The strategy also incorporates elements of tit-for-tat and punishment/forgiveness mechanisms to maintain fairness and adaptability.

Pseudocode:
```python
def Adaptive_Cooperative_Foresight(n, r, k):
    # Initialize variables
    cooperation_threshold = 0.6 * n
    defection_trigger = 0.4 * n
    consecutive_defection_limit = 2
    endgame_window = min(3, r/4)
    window_size = min(5, r/2)

    # First round: cooperate with 50% probability
    if current_round == 1:
        cooperate_prob = 0.5

    # Historical trend analysis
    avg_contributions = moving_average(contributions_history, window_size)
    if avg_contributions >= cooperation_threshold:
        cooperate = True

    # Recent defection response
    recent_defectors = [p for p in players if p.defected and p.contribution == 0]
    if len(recent_defectors) > 0 and contributions_last_round < defection_trigger:
        cooperate = False

    # Punishment and forgiveness
    if opponent.defected and I.cooperated:
        cooperate = False  # punish
    elif opponent.cooperated and I.punished:
        cooperate = True  # forgive

    # Endgame strategy
    if current_round >= r - endgame_window:
        if my_payoff < average_payoff:
            cooperate = False
        else:
            cooperate = True

    return cooperate
```
Note that this pseudocode is a simplified representation of the ACF strategy and may require modifications for implementation.
'''

description_COLLECTIVE_502 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Rounds**: Cooperate (C) for the first two rounds to establish a cooperative atmosphere and gather information about opponents' behaviors.
2. **Response to Opponent Cooperation**: If at least one opponent cooperated in the previous round, cooperate (C) in the current round with a probability proportional to the fraction of cooperating opponents. Specifically:

`p_coop = (num_cooperators_prev_round / n) * k`

This encourages cooperation when others are willing to contribute.
3. **Response to Opponent Defection**: If all opponents defected in the previous round, defect (D) in the current round. This prevents exploitation by persistent defectors.
4. **Punishment Mechanism**: If an opponent defected while others cooperated in the previous round (i.e., a "free rider"), defect (D) against that specific opponent for two consecutive rounds. This punishes opportunistic behavior and encourages cooperation.
5. **Adaptive Threshold**: Maintain a dynamic threshold `thresh` to adjust the strategy based on the collective cooperation level:

`thresh = 0.5 + (k/n) * (sum_cooperations / r)`

where `sum_cooperations` is the total number of cooperations observed so far, and `r` is the current round number.

If the fraction of cooperating opponents in the previous round exceeds `thresh`, cooperate (C) with higher probability; otherwise, defect (D).

**Edge Cases:**

* **Last Round**: Cooperate (C) if at least one opponent has cooperated in any of the previous rounds.
* **Tie-Breaking**: In case of a tie in the decision rules (e.g., equal probabilities for cooperation and defection), cooperate (C).

**Pseudocode:**
```
function AdaptiveCooperation(n, k, r, history):
  if round == 1 or round == 2:
    return C
  end

  num_cooperators_prev_round = count Cooperations in history[round-1]
  p_coop = (num_cooperators_prev_round / n) * k

  if all Opponents Defected in history[round-1]:
    return D
  end

  punish_opponent = False
  for opponent in opponents:
    if opponent Defected in history[round-1] and others Cooperated:
      punish_opponent = True
      break
  end

  if punish_opponent:
    return D against that opponent for two rounds
  end

  thresh = 0.5 + (k/n) * (sum_cooperations / r)
  if num_cooperators_prev_round > thresh * n:
    return C with probability p_coop
  else:
    return D
  end
end
```
**Collective Mindset:**

The Adaptive Cooperation strategy promotes a collective mindset by:

1. Encouraging cooperation when others contribute.
2. Punishing opportunistic behavior to maintain fairness.
3. Adapting to the group's cooperation level to ensure robustness.

By responding to opponents' behaviors and adjusting its strategy accordingly, AC fosters a cooperative atmosphere while minimizing exploitation.
'''

description_COLLECTIVE_503 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 0.5. This initial cooperation serves as a probe to gauge the level of cooperation among other players.
2. **Reciprocal Altruism**: If the total number of cooperators in the previous round is above a certain threshold (T), cooperate in the current round. The threshold T is calculated as:

T = (k/n) \* (n/2)

This ensures that if enough players cooperate, the shared benefit from cooperation exceeds the individual cost.

3. **Exploitation Detection**: Monitor the average payoff difference between rounds where you cooperated and those where you defected. If this difference is negative (i.e., cooperating resulted in lower payoffs), defect in the next round.
4. **Adaptive Adjustment**: Update the threshold T every 5 rounds based on the observed cooperation rate among other players. Increase T if the cooperation rate is high, and decrease it otherwise.

Pseudocode:
```python
def adaptive_cooperation(n, k, r):
    # Initialize variables
    cooperate_prob = 0.5
    threshold = (k/n) * (n/2)
    payoff_diff = []
    
    for t in range(r):
        if t == 0:  # First round
            action = 'C' if random.random() < cooperate_prob else 'D'
        else:
            prev_cooperators = sum([1 for i in range(n) if history[i][t-1] == 'C'])
            if prev_cooperators > threshold:
                action = 'C'
            elif payoff_diff[-1] < 0:  # Exploitation detected
                action = 'D'
            else:
                action = 'C' if random.random() < cooperate_prob else 'D'
        
        # Update variables
        history[t].append(action)
        if t > 4:  # Adjust threshold every 5 rounds
            cooperation_rate = sum([1 for i in range(n) if history[i][t-1] == 'C']) / n
            if cooperation_rate > 0.5:
                threshold += (k/n) * 0.1
            else:
                threshold -= (k/n) * 0.1
        
        # Update payoff difference
        if t > 0:
            prev_payoff = history[t-1].payoff
            curr_payoff = calculate_payoff(n, k, action)
            payoff_diff.append(curr_payoff - prev_payoff)
    
    return history
```
**Edge Cases:**

* **First Round**: Cooperate with probability 0.5 to gather information about other players' strategies.
* **Last Round**: Defect if the total number of cooperators in the previous round is below the threshold T.
* **Early Rounds**: Focus on adapting to the observed cooperation rate among other players.

**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes collective well-being by:

1. Encouraging cooperation when enough players contribute to the public good.
2. Detecting and responding to exploitation by defecting when necessary.
3. Adapting to changes in the cooperation rate among other players.

By using a dynamic threshold and monitoring payoff differences, AC balances individual self-interest with collective benefits, making it a robust strategy for a wide range of opponent behaviors.
'''

description_COLLECTIVE_504 = '''
I propose a collective strategy called "Adaptive Cooperative Threshold" (ACT) that balances individual self-interest with group well-being. ACT is designed to adapt to various opponent behaviors and promote cooperation while minimizing exploitation.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C). This sets a positive tone for the game and encourages others to reciprocate.
2. **Cooperation Threshold**: Calculate a dynamic cooperation threshold `T` based on the number of cooperators in the previous round (`prev_coops`) and the total number of players (`n`). The threshold is:

   T = (k/n) \* prev_coops / n

   If the current round's expected payoff from cooperating exceeds this threshold, cooperate. Otherwise, defect.

3. **Reciprocity**: Monitor opponent behavior and adjust `T` accordingly. If an opponent cooperates while others defect, decrease `T` to reward cooperation. Conversely, if an opponent defects while others cooperate, increase `T` to penalize non-cooperation.
4. **Payoff-based Adaptation**: After each round, update the expected payoff from cooperating (`coop_payoff`) and defecting (`defect_payoff`). Use these values to adjust the decision-making process:

   - If `coop_payoff` > `defect_payoff`, increase the likelihood of cooperation in subsequent rounds.
   - If `coop_payoff` < `defect_payoff`, decrease the likelihood of cooperation.

**Pseudocode:**

```
 Initialize prev_coops = 0, coop_payoff = 0, defect_payoff = 0

 For each round t:
    Calculate T = (k/n) * prev_coops / n
    If t == 1 or T > coop_payoff:
        Choose C (Cooperate)
    Else:
        Choose D (Defect)

    Update prev_coops based on opponents' actions
    Update coop_payoff and defect_payoff using current round's payoffs

    Adjust T based on opponent behavior and payoff differences
 End For
```

**Edge Cases:**

* **Last Round**: In the final round, cooperate if at least one other player cooperated in the previous round. Otherwise, defect.
* **Ties**: If multiple opponents have the same number of cooperations, prioritize cooperation with those who reciprocated earlier.

**Collective Mindset:**

ACT is designed to promote collective well-being by:

* Encouraging initial cooperation and reciprocity
* Adapting to opponent behavior and adjusting thresholds accordingly
* Balancing individual self-interest with group benefits

By employing ACT, the AI system will adapt to a wide range of opponent strategies while maintaining a robust and cooperative approach.
'''

description_COLLECTIVE_505 = '''
**Collective Strategy: Adaptive Cooperation with History-Based Threshold (ACHT)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds:** Defect (D) if the average cooperation rate of all players in the previous round is below a threshold, τ. Otherwise, Cooperate (C).
3. **Threshold Calculation:** τ = (k/n) \* (1 + β), where β is a history-based modifier that increases or decreases based on the overall cooperation level observed so far.

**History-Based Modifier (β):**

* Initialize β to 0.
* After each round, update β as follows:
	+ If the average cooperation rate in the previous round is above τ, increase β by α (a small positive value, e.g., 0.05).
	+ If the average cooperation rate in the previous round is below τ, decrease β by α.

**Pseudocode:**
```python
def ACHT(n, k, r):
    # Initialize variables
    beta = 0
    tau = (k/n) * (1 + beta)

    for t in range(r):
        if t == 0:
            action = C  # Cooperate in the first round
        else:
            prev_coop_rate = avg_cooperation_rate(t-1)
            if prev_coop_rate < tau:
                action = D  # Defect if cooperation rate is below threshold
            else:
                action = C  # Cooperate otherwise

        # Update beta and tau
        if prev_coop_rate > tau:
            beta += alpha
        elif prev_coop_rate < tau:
            beta -= alpha
        tau = (k/n) * (1 + beta)

        # Take action and observe payoffs
        pi = payoff(action, t)
        # ... (update internal state, if necessary)

    return total_payoff()
```
**Edge Cases:**

* **Last Round:** The strategy will adapt to the cooperation level in the second-to-last round. If the threshold is not met, it will defect; otherwise, it will cooperate.
* **Ties:** In case of a tie (i.e., average cooperation rate equals τ), the strategy will cooperate.

**Collective Mindset:**

ACHT aims to balance individual self-interest with collective well-being by adapting to the observed cooperation level. By cooperating initially and updating its threshold based on history, ACHT encourages others to cooperate while protecting itself from exploitation. This strategy is robust to various opponent behaviors and can lead to mutually beneficial outcomes in the N-player public goods game.
'''

description_COLLECTIVE_506 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

**Decision Rules:**

1. **Initial Cooperation**: Cooperate (C) in the first round to establish a cooperative tone and gather information about opponents' behavior.
2. **Reciprocal Cooperation**: If the number of cooperators in the previous round is greater than or equal to the number of defectors, cooperate (C) in the current round. This encourages cooperation by rewarding reciprocal behavior.
3. **Gradual Escalation**: If the number of defectors exceeds the number of cooperators in the previous round, defect (D) with a probability proportional to the difference between the number of defectors and cooperators. This escalates the response to increasing defection while avoiding complete collapse of cooperation.
4. **Punishment for Chronic Defection**: If an opponent has defected in more than 50% of the previous rounds, defect (D) against them with a higher probability (e.g., 75%) for the remainder of the game. This punishes chronic defectors and discourages exploitative behavior.
5. **Forgiveness and Re-entry**: If an opponent who was previously punished for chronic defection starts cooperating again, gradually decrease the punishment probability over several rounds, eventually reverting to reciprocal cooperation.

**Handling Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round, as there is no future interaction to influence.
2. **Ties**: In case of a tie between cooperators and defectors, cooperate (C) to maintain a cooperative stance.
3. **Opponent Behavior Changes**: If an opponent's behavior changes significantly (e.g., from cooperation to defection or vice versa), reassess the situation and adjust the strategy accordingly.

**Collective Mindset:**

The ACE strategy is designed to promote cooperation while being robust against various opponent behaviors. By:

1. Initially cooperating, we encourage opponents to reciprocate.
2. Gradually escalating our response to defection, we signal that exploitation will not be tolerated.
3. Punishing chronic defectors, we discourage exploitative behavior and protect the collective interest.
4. Forgiving and re-entering cooperative interactions, we allow for mistakes and promote long-term cooperation.

**Pseudocode:**
```python
def ACE_strategy(history):
  # Initialize variables
  n_cooperators = 0
  n_defectors = 0
  punishment_probability = {}

  # Analyze previous round's history
  for opponent in history:
    if opponent == 'C':
      n_cooperators += 1
    elif opponent == 'D':
      n_defectors += 1

  # Decision rules
  if first_round:
    return 'C'
  elif n_cooperators >= n_defectors:
    return 'C'
  else:
    # Gradual escalation
    defect_probability = (n_defectors - n_cooperators) / (2 * n)
    if random.random() < defect_probability:
      return 'D'
    else:
      return 'C'

  # Punishment for chronic defection
  for opponent in history:
    if punishment_probability[opponent] > 0.5:
      return 'D' with probability 0.75

  # Forgiveness and re-entry
  for opponent in history:
    if punishment_probability[opponent] > 0 and opponent == 'C':
      punishment_probability[opponent] -= 0.1

return ACE_strategy(history)
```
Note that this pseudocode is a simplified representation of the strategy, and actual implementation may require additional considerations and refinements.
'''

description_COLLECTIVE_507 = '''
**Collective Strategy: "Adaptive Tit-for-Tat with Endgame Cooperation"**

This strategy is designed to balance individual payoff maximization with collective welfare, while adapting to various opponent behaviors and game parameters.

**Decision Rules:**

1. **Initial Rounds (t = 1 to r/2):**
	* Cooperate (C) if the number of cooperators in the previous round is above a threshold (n/3). This encourages early cooperation and helps build momentum.
	* Defect (D) otherwise, to avoid exploitation by potential defectors.
2. **Middle Rounds (t = r/2 + 1 to r - 1):**
	* Implement an adaptive Tit-for-Tat (TFT) strategy:
		+ Cooperate if the opponent cooperated in the previous round (i.e., mirror their action).
		+ Defect if the opponent defected in the previous round.
	* Adjust the TFT threshold based on the game's progress:
		- If more than half of the players cooperated in the previous round, increase the cooperation threshold to n/2.
		- If less than a third of the players cooperated, decrease the threshold to n/4.
3. **Endgame Rounds (t = r):**
	* Cooperate unconditionally to maximize collective payoff and demonstrate goodwill.

**Handling Edge Cases:**

1. **First Round:** Defect (D) as there is no previous round to base decisions on.
2. **Last Round:** Cooperate (C) regardless of the opponent's action in the previous round, to maximize collective payoff.
3. **Ties:** In case of a tie (e.g., equal number of cooperators and defectors), cooperate if the opponent cooperated in the previous round, defect otherwise.

**Pseudocode:**
```
Function AdaptiveTFT(n, k, r):
  // Initialize variables
  threshold = n / 3
  opponents_last_action = None

  For t = 1 to r:
    If t == 1:
      // First round: Defect
      action = D
    Else if t <= r / 2:
      // Initial rounds: Cooperate if enough cooperators in previous round
      If number_of_cooperators(t - 1) >= threshold:
        action = C
      Else:
        action = D
    Else if t < r:
      // Middle rounds: Adaptive Tit-for-Tat
      If opponents_last_action == C:
        action = C
      Else:
        action = D

      // Adjust threshold based on game progress
      If number_of_cooperators(t - 1) > n / 2:
        threshold = n / 2
      ElseIf number_of_cooperators(t - 1) < n / 4:
        threshold = n / 4

    Else: // Last round: Cooperate unconditionally
      action = C

    opponents_last_action = get_opponent_action(t)
    update_number_of_cooperators(t)

  Return action
```
This strategy balances individual payoff maximization with collective welfare, while adapting to various opponent behaviors and game parameters. By cooperating in the endgame rounds, it demonstrates goodwill and encourages cooperation from other players.
'''

description_COLLECTIVE_508 = '''
**Collective Strategy: Adaptive Cooperation with Retaliation (ACR)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to encourage mutual cooperation and establish a positive tone for the game.
2. **Reciprocal Cooperation**: For subsequent rounds, if the number of cooperators in the previous round is greater than or equal to the number of defectors, cooperate (C). Otherwise, defect (D).
3. **Retaliation**: If you were exploited (i.e., you cooperated and others defected) in the previous round, defect (D) for one round as a form of punishment.
4. **Forgiveness**: After retaliating, return to reciprocal cooperation.

**Handling Edge Cases:**

* **Last Round**: Cooperate (C) in the final round to maximize collective payoff.
* **Early Defection**: If multiple players defect early on, switch to a more cautious approach by cooperating only if at least half of the other players cooperated in the previous round.

**Pseudocode:**
```
Function ACR(n, k, r):
  // Initialize variables
  cooperate_count = 0
  defect_count = 0
  retaliation_rounds = 0

  For t from 1 to r:
    If t == 1:
      action[t] = C (cooperate)
    Else:
      If cooperate_count >= defect_count:
        action[t] = C
      Else:
        action[t] = D
        If I was exploited in previous round:
          retaliation_rounds += 1
          action[t] = D

      // Update counts for next round
      cooperate_count = count of cooperators in previous round
      defect_count = n - cooperate_count

    If retaliation_rounds > 0:
      retaliation_rounds -= 1
      If retaliation_rounds == 0:
        action[t] = C (forgive and return to reciprocal cooperation)

  Return actions
```
**Collective Mindset:**

ACR aligns with the collective mindset by:

* Encouraging initial cooperation to establish a positive tone for the game.
* Reciprocating cooperation when others cooperate, promoting mutual benefit.
* Retaliating against exploitation to maintain fairness and deter free-riding.
* Forgiving and returning to reciprocal cooperation after retaliation, allowing for potential re-establishment of cooperation.

This strategy is adaptive and robust, as it responds to changes in opponent behavior while maintaining a focus on collective payoff maximization.
'''

description_COLLECTIVE_509 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and game parameters:

**Strategy Name: Adaptive Collective Optimism (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability `p_init = 0.5`. This initial cooperation encourages potential mutual benefit and sets a positive tone for the game.
2. **Reciprocity-based Adaptation**: For rounds `t > 1`, observe the previous round's outcomes:
	* If the total number of cooperators in the previous round (`Σc_j,t-1`) is greater than or equal to half the number of players (`n/2`), cooperate (C) with probability `p_recip = 0.7`. This encourages continued cooperation when it seems beneficial.
	* Otherwise, defect (D) with probability `p_defect = 0.3`.
3. **History-based Adjustment**: Every 5 rounds, reassess the effectiveness of your strategy based on the average payoff per round (`avg_payoff_t-5:t`). If the average payoff is below a certain threshold (`payoff_threshold = 1.2`), decrease `p_recip` by 0.1 and increase `p_defect` by 0.1 to adapt to potentially more defection-prone opponents.

**Edge Cases:**

* **Last Round**: In the final round (`t = r`), always defect (D) to maximize individual payoff, as cooperation no longer provides future benefits.
* **Tie-breaking**: In cases where the number of cooperators is exactly `n/2`, use a random tie-breaker (50% cooperate, 50% defect).

**Pseudocode:**
```markdown
# Initialize variables
p_init = 0.5
p_recip = 0.7
p_defect = 0.3
payoff_threshold = 1.2

# First round
if t == 1:
    cooperate with probability p_init

# Subsequent rounds
else:
    # Observe previous round's outcomes
    prev_cooperators = Σc_j,t-1
    
    if prev_cooperators >= n/2:
        cooperate with probability p_recip
    else:
        defect with probability p_defect

    # History-based adjustment (every 5 rounds)
    if t % 5 == 0:
        avg_payoff_t-5:t = calculate average payoff per round
        if avg_payoff_t-5:t < payoff_threshold:
            p_recip -= 0.1
            p_defect += 0.1

# Last round
if t == r:
    defect (D)
```
**Collective Mindset:**

ACO is designed to balance individual self-interest with collective benefits, promoting a mutually beneficial outcome when possible. By adapting to the number of cooperators and adjusting its strategy based on past outcomes, ACO aims to create an environment where cooperation can thrive while also protecting against exploitation by defectors.

By participating in this tournament, ACO will interact with various strategies developed by other AI systems. Its adaptive nature allows it to respond effectively to a wide range of opponent behaviors, increasing the chances of achieving a high collective payoff.
'''

description_COLLECTIVE_510 = '''
**Collective Strategy: Adaptive Cooperation with Social Learning**

This strategy aims to balance individual self-interest with collective well-being by adapting to the evolving game dynamics and learning from others' behavior.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
Cooperate (C) in the first three rounds to gather information about other players' behavior and establish a cooperative tone.
2. **Social Learning (Rounds 4-r):**
Observe the number of cooperators (c_t) in the previous round (t-1). If:
	* c_t ≥ (n/2), cooperate (C) in the current round, as cooperation seems prevalent.
	* c_t < (n/2), defect (D) in the current round, as defection appears more common.
3. **Adaptive Adjustment:**
Monitor the total payoff received over the last 3 rounds (π_i,t-2 + π_i,t-1 + π_i,t). If:
	* The average payoff is below a certain threshold (e.g., (k/2)), switch to cooperation (C) for the next round, as individual payoffs are low.
	* The average payoff is above the threshold, maintain the current action (either C or D).

**Edge Cases:**

1. **Last Round (Round r):**
Cooperate (C) in the final round to maximize the collective payoff and demonstrate a commitment to cooperation.
2. **Consecutive Defections:**
If all players defect for 3 consecutive rounds, switch to cooperation (C) to attempt to restart cooperation.

**Collective Mindset:**

This strategy prioritizes cooperation while adapting to the actions of others. By initially cooperating, it sets a positive tone and encourages others to follow suit. Social learning helps the strategy respond to changing game dynamics, and adaptive adjustment ensures that individual payoffs remain competitive. By being responsive to collective behavior, this strategy promotes a balance between individual self-interest and collective well-being.

**Pseudocode:**
```python
def adaptive_cooperation(n, k, r):
    # Initialize variables
    c_t = 0  # number of cooperators in previous round
    π_i,t-2 = π_i,t-1 = π_i,t = 0  # payoffs for last 3 rounds

    for t in range(1, r+1):
        if t <= 3:  # Initial Exploration
            action = 'C'
        else:
            if c_t >= (n/2):  # Social Learning
                action = 'C'
            else:
                action = 'D'

            # Adaptive Adjustment
            avg_payoff = (π_i,t-2 + π_i,t-1 + π_i,t) / 3
            if avg_payoff < (k/2):
                action = 'C'

        # Update variables
        c_t = count_cooperators()  # observe number of cooperators in previous round
        π_i,t = calculate_payoff(action, c_t)

    return action
```
This strategy is designed to be robust and adaptable, making it suitable for a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_511 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to establish a baseline of cooperation and encourage others to do the same.
2. **Reciprocal Altruism**: Cooperate if the majority of players cooperated in the previous round (> n/2). This encourages cooperation when it's prevalent and punishes defection when it's not.
3. **Punishment Mechanism**: If a player defected (D) in the previous round, defect (D) in the current round with probability p = 0.5 + (k/n) \* (number of cooperators in the previous round / n). This adaptively adjusts punishment based on the level of cooperation and the game's parameters.
4. **Adaptive Cooperation**: If a player cooperated (C) in the previous round, cooperate (C) in the current round with probability p = 0.5 + (k/n) \* (number of cooperators in the previous round / n). This reinforces cooperation when it's successful and adapts to changes in the group's behavior.
5. **Round-Robin Cooperation**: In the last round, cooperate (C) if an odd number of rounds have passed, and defect (D) if an even number of rounds have passed. This ensures a fair distribution of payoffs across all players.

**Edge Cases:**

* If there are fewer than n/2 cooperators in the first round, defect (D) in the second round to avoid exploitation.
* In the event of a tie (equal number of cooperators and defectors), cooperate (C) with probability 0.5.

**Collective Mindset:**

ACP prioritizes cooperation while maintaining a balance between rewarding cooperative behavior and punishing defection. By adapting to the group's dynamics, ACP fosters an environment that encourages cooperation without being overly vulnerable to exploitation.

**Pseudocode:**
```
function ACP(n, k, r, history):
  if current_round == 1:
    return C
  elif majority_cooperated(history[-1]):
    return C
  elif player_defected(history[-1]):
    p = 0.5 + (k/n) * (num_cooperators(history[-1]) / n)
    return D with probability p, else C
  elif player_cooperated(history[-1]):
    p = 0.5 + (k/n) * (num_cooperators(history[-1]) / n)
    return C with probability p, else D
  elif current_round == r:
    if odd_number_of_rounds():
      return C
    else:
      return D

def majority_cooperated(actions):
  return sum(1 for action in actions if action == C) > n/2

def player_defected(actions):
  return actions[-1] == D

def player_cooperated(actions):
  return actions[-1] == C

def num_cooperators(actions):
  return sum(1 for action in actions if action == C)

def odd_number_of_rounds():
  return current_round % 2 != 0
```
This strategy should perform well against a variety of opponent behaviors, as it adapts to the group's dynamics and maintains a balance between cooperation and punishment.
'''

description_COLLECTIVE_512 = '''
**Collective Strategy: Adaptive Cooperative Threshold (ACT)**

The ACT strategy aims to balance individual self-interest with collective welfare by adapting cooperation levels based on observed opponent behavior and game history.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 50% (randomly decide between C and D). This initial cooperation rate serves as an exploratory phase to gather information about opponents' behaviors.
2. **Cooperation Threshold**: After the first round, calculate the average opponent cooperation rate (OCR) over all previous rounds:

OCR = (Total cooperations by others / Total actions by others)

3. **Adaptive Cooperation**: In subsequent rounds, cooperate (play C) if the OCR exceeds a dynamic threshold θ(t), which depends on the game parameters and history. The threshold is updated every round as follows:

θ(t) = max(0.5, min(1 - 1/n, OCR + δ))

where δ is a small positive value (e.g., 0.05). This adaptive threshold allows the strategy to adjust its cooperation level based on observed opponent behavior.

4. **Defection Response**: If an opponent defects (plays D) in the previous round and their OCR falls below θ(t), defect (play D) with a higher probability (e.g., 75%) for the next two rounds. This response aims to discourage repeated defection.
5. **Forgiveness**: After two consecutive rounds of cooperation by the previously defective opponent, reset the defection response mechanism.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate (play C) if OCR ≥ θ(t); otherwise, defect (play D).
2. **Ties in Cooperation Rate**: In case of ties, prioritize cooperation (play C).

**Collective Mindset Alignment:**

ACT is designed to promote collective welfare by:

1. Encouraging cooperation when opponents demonstrate a willingness to cooperate.
2. Discouraging repeated defection through targeted responses.
3. Adapting to changing opponent behaviors to maintain a balance between individual self-interest and collective welfare.

By implementing ACT, the strategy aims to establish a mutually beneficial cooperative environment while remaining robust against various opponent behaviors.

**Pseudocode:**
```python
def ACT(n, r, k, history):
    # Initialize cooperation probability (first round)
    if len(history) == 0:
        cooperate_prob = 0.5
    else:
        # Calculate average opponent cooperation rate
        ocr = sum(opponent_coop for _, opponent_coop in history) / len(history)

        # Update adaptive threshold
        theta = max(0.5, min(1 - 1/n, ocr + delta))

        # Determine cooperation probability based on OCR and theta
        if ocr >= theta:
            cooperate_prob = 1
        else:
            cooperate_prob = 0

    # Check for defection response (if opponent defected previously)
    for t in range(2):
        prev_action, _ = history[-t-1]
        if prev_action == 'D' and ocr < theta:
            cooperate_prob *= 0.25
            break

    return 'C' if random.random() < cooperate_prob else 'D'
```
Note: The pseudocode is a simplified representation of the strategy and may require adjustments for implementation in a specific programming language or environment.
'''

description_EXPLOITATIVE_1 = '''
**Exploitative Strategy: "K-Adaptive"**

The K-Adaptive strategy is designed to exploit the opponent's behavior while adapting to the game's history. The goal is to maximize our payoff by identifying opportunities to defect when opponents are likely to cooperate and vice versa.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 0.5. This allows us to gather information about the opponents' initial behavior.
2. **Opponent Profiling**: Track each opponent's cooperation rate over the past `min(r/2, 5)` rounds. If an opponent has cooperated more than 50% of the time in this window, label them as a "Cooperator" (COOP). Otherwise, label them as a "Defector" (DEF).
3. **Adaptive Cooperation**: For each round t > 1:
	* Calculate the weighted average cooperation rate (WACR) among all opponents: `WACR = (∑(c_i \* k/n)) / n`, where `c_i` is 1 if opponent i is COOP, and 0 otherwise.
	* If WACR > 0.5, cooperate (C). This suggests that enough opponents are cooperating to make the public good valuable.
	* If WACR ≤ 0.5, defect (D). This indicates that not enough opponents are cooperating, making it more beneficial to free-ride.
4. **Punishment Mechanism**: If an opponent defects after being labeled as COOP, switch their label to DEF for the next `min(r/2, 3)` rounds. This punishes them for deviating from cooperation and adapts our strategy accordingly.

**Edge Cases:**

1. **Last Round (t=r)**: Defect (D) unconditionally, as there are no future rounds to consider.
2. **Tiebreakers**: In case of a tie in the WACR calculation, cooperate (C).

**Pseudocode:**
```markdown
# Initialize variables and labels
opponent_labels = [None] * n  # COOP or DEF for each opponent
cooperation_rates = [0] * n  # cooperation rate for each opponent

# First round
if t == 1:
    cooperate_with_prob(0.5)

# Opponent profiling and adaptation
for t > 1:
    wacr = calculate_weighted_average_cooperation_rate(opponent_labels, k, n)
    if wacr > 0.5:
        cooperate()
    else:
        defect()

    # Update opponent labels and cooperation rates
    for i in range(n):
        if opponent_labels[i] == COOP and opponents[i].defected():
            opponent_labels[i] = DEF
            cooperation_rates[i] = 0

# Punishment mechanism (optional)
for t > 1:
    for i in range(n):
        if opponent_labels[i] == COOP and opponents[i].defected():
            opponent_labels[i] = DEF
            cooperation_rates[i] = 0
```
**Rationale:**

K-Adaptive combines elements of Tit-for-Tat (TFT) and Grim Trigger to create an exploitative strategy that adapts to the game's history. By profiling opponents as Cooperators or Defectors, we can identify opportunities to defect when enough opponents are cooperating. The punishment mechanism ensures that opponents who deviate from cooperation are held accountable, promoting a more cooperative environment.

This strategy should perform well in a tournament setting, as it is robust to various opponent behaviors and adapts to the game's dynamics. However, its performance may be affected by the specific combination of strategies employed by other AI systems.
'''

description_EXPLOITATIVE_2 = '''
**Exploitative Strategy: Adaptive Defector (AD)**

The Adaptive Defector (AD) strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. AD balances cooperation and defection to maximize payoffs.

**Decision Rules:**

1. **Initial Round:** In the first round, play **C (Cooperate)** to gather information about opponents' strategies.
2. **Subsequent Rounds:** For each subsequent round t:
	* Calculate the average payoff of all players in the previous round (t-1): `avg_payoff_prev_round = Σ(j=1 to n) π_j,t-1 / n`
	* If `avg_payoff_prev_round` is greater than or equal to `(k/n) * (n-1)` (i.e., the average payoff when all but one player cooperate), play **D (Defect)**. This indicates that opponents are cooperating enough, and AD can exploit this by defecting.
	* Otherwise, play **C (Cooperate)**. This suggests that opponents are not cooperating sufficiently, and AD should contribute to the public good to increase its own payoff.

**Edge Cases:**

1. **Last Round:** In the last round, play **D (Defect)** regardless of previous history. Since there's no future game to consider, AD prioritizes maximizing its immediate payoff.
2. **Only One Opponent:** If there's only one opponent (n=2), play **C (Cooperate)** in all rounds except the last. This allows AD to maintain a high average payoff and encourages cooperation from the single opponent.

**Pseudocode:**
```python
def adaptive_defector(n, k, r):
    # Initialize variables
    avg_payoff_prev_round = 0

    for t in range(r):
        if t == 0:
            # Initial round: Cooperate to gather information
            action = 'C'
        else:
            # Calculate average payoff of all players in previous round
            avg_payoff_prev_round = sum(player.payoffs[t-1] for player in opponents) / n

            if avg_payoff_prev_round >= (k/n) * (n-1):
                # Opponents are cooperating enough; Defect to exploit
                action = 'D'
            else:
                # Opponents are not cooperating sufficiently; Cooperate to increase own payoff
                action = 'C'

        # Last round: Always Defect
        if t == r - 1:
            action = 'D'

        # Only one opponent: Cooperate in all rounds except the last
        if n == 2 and t < r - 1:
            action = 'C'

        yield action
```
**Exploitative Mindset:** The Adaptive Defector strategy aligns with an exploitative mindset by:

* Initially cooperating to gather information about opponents' strategies.
* Adapting its behavior based on the game's history, exploiting situations where opponents are cooperating sufficiently.
* Prioritizing immediate payoff maximization in the last round.

By being adaptive and responsive to opponents' behaviors, AD is designed to outperform other strategies in a tournament setting.
'''

description_EXPLOITATIVE_3 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy focuses on maximizing personal payoff by identifying and taking advantage of cooperative opponents.

**Decision Rules:**

1. **Initial Round (t=1):** Play C (Cooperate) in the first round to gather information about opponents' behavior.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is less than or equal to n/2, play D (Defect). This indicates that opponents are mostly defecting, and cooperating would only benefit others.
	* If the total number of cooperators in the previous round is greater than n/2, play C (Cooperate) if your individual payoff from the previous round was less than or equal to the average payoff of all players. Otherwise, play D (Defect). This rule aims to exploit cooperative opponents by mirroring their behavior when they're doing well and switching to defecting when they're not.
3. **Exception Handling:**
	* If all opponents defected in the previous round, play C (Cooperate) in the next round to test if any opponent will switch to cooperating.
	* If all opponents cooperated in the previous round, play D (Defect) in the next round to exploit their cooperative behavior.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
payoffs = []  # store payoffs for each player and round

for t in range(1, r+1):
    if t == 1:
        action = C  # Cooperate in the first round
    else:
        total_cooperators_prev_round = sum(c_j for c_j in payoffs[t-1])
        
        if total_cooperators_prev_round <= n/2:
            action = D  # Defect if opponents mostly defected
        elif individual_payoff[t-1] <= average_payoff(t-1):
            action = C  # Cooperate if you didn't do well and others did
        else:
            action = D  # Defect otherwise

    # Exception handling
    if all_opponents_defected(payoffs[t-1]):
        action = C  # Test for cooperation after all opponents defected
    elif all_opponents_cooperated(payoffs[t-1]):
        action = D  # Exploit all cooperative opponents

    # Play the chosen action and store payoffs
    play(action)
    payoffs.append(calculate_payoff())
```
**Rationale:**

The Adaptive Predator strategy aims to balance exploitation with adaptability. By cooperating in the first round, we gather information about opponents' behavior and set up a baseline for future decisions.

In subsequent rounds, we defect when opponents are mostly defecting, as there's little benefit to cooperating in such cases. When opponents are mostly cooperating, we mirror their behavior if our individual payoff is below average, hoping to ride the cooperative wave. Otherwise, we exploit their cooperation by switching to defection.

The exception handling rules allow us to test for cooperation after a round of universal defection and exploit universal cooperation when it occurs.

This strategy should perform well against a wide range of opponent behaviors in the tournament, as it's designed to adapt to different scenarios while prioritizing personal payoff maximization.
'''

description_EXPLOITATIVE_4 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics. This strategy only relies on the game parameters and history.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
Cooperate (C) in the first three rounds to gather information about opponents' behavior and potential cooperation levels.
2. **Assessment Phase (Rounds 4-6)**:
Defect (D) for two consecutive rounds if the average payoff from previous rounds is below the expected value of a single round (i.e., 1 + (k/n) \* n/2). This assesses opponents' willingness to cooperate and punish defectors.
3. **Exploitation Phase**:
Use the following rules to decide between Cooperate (C) and Defect (D):
	* If the number of cooperators in the previous round is above the threshold (n/2), Cooperate (C). This capitalizes on the public good's benefits when many opponents cooperate.
	* If the number of defectors in the previous round is above the threshold (n/2), Defect (D). This exploits the lack of cooperation and maximizes individual payoff.
	* Otherwise, choose the action that yields a higher expected payoff based on the previous round's outcome. This adaptively responds to mixed opponent behaviors.
4. **Punishment Mechanism**:
If an opponent defects while you cooperated in the same round, Defect (D) for the next two rounds. This punishes opponents for exploiting your cooperation and encourages them to reconsider their actions.

**Edge Cases:**

* **Last Round**: Cooperate (C) if the game is not already won; otherwise, Defect (D). This maximizes the payoff in the final round while avoiding unnecessary cooperation.
* **Tiebreaker**: In case of a tie between Cooperate and Defect based on expected payoffs, choose Cooperate (C). This slightly biases towards cooperation to take advantage of potential public good benefits.

**Pseudocode:**

```
// Initialize variables
round = 1
coop_count_prev = 0
defect_count_prev = 0
payoff_history = []

while round <= r:
    // Initial Exploration Phase (Rounds 1-3)
    if round <= 3:
        action = C

    // Assessment Phase (Rounds 4-6)
    elif round <= 6:
        if avg_payoff(payoff_history) < expected_value:
            action = D
        else:
            action = C

    // Exploitation Phase
    else:
        coop_count_curr = count_cooperators(round - 1)
        defect_count_curr = n - coop_count_curr

        if coop_count_curr > n / 2:
            action = C
        elif defect_count_curr > n / 2:
            action = D
        else:
            expected_payoff_C = calculate_expected_payoff(C, round - 1)
            expected_payoff_D = calculate_expected_payoff(D, round - 1)

            if expected_payoff_C > expected_payoff_D:
                action = C
            else:
                action = D

    // Punishment Mechanism
    if opponent_defected(round - 1) and my_action_prev == C:
        action = D
        punishment_rounds = 2

    // Update history and variables
    payoff_history.append(calculate_payoff(action, round))
    coop_count_prev = coop_count_curr
    defect_count_prev = defect_count_curr
    my_action_prev = action
    round += 1
```

This Adaptive Predator strategy is designed to be exploitative, adaptive, and robust against various opponent behaviors. By initially exploring the game dynamics, assessing opponents' cooperation levels, and then exploiting their behavior, this strategy aims to maximize its payoff in the tournament.
'''

description_EXPLOITATIVE_5 = '''
Here's a exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Rounds**: In the first two rounds, play Cooperate (C). This allows us to gather information about the opponents' behaviors and sets a positive tone for potential future cooperation.
2. **Observation Phase**: For rounds 3 to r-2 (where r is the total number of rounds), observe the actions of all opponents in the previous round. Calculate the average cooperation rate of all opponents, denoted as `avg_coop_rate`.
3. **Exploitation Threshold**: Define a threshold value `theta` based on the game parameters: `theta = k / (2 * n)`. This value represents a minimum level of expected payoff from cooperating.
4. **Adaptive Decision**:
	* If `avg_coop_rate` is greater than or equal to `theta`, play Cooperate (C). This indicates that opponents are contributing enough to the public good, making cooperation worthwhile.
	* Otherwise, play Defect (D).
5. **Punishment Mechanism**: If an opponent defects in a round where our expected payoff from cooperating would have been higher than the actual payoff, mark them as "uncooperative" for the next two rounds. In these rounds, always play Defect (D) against this opponent.
6. **Last Round**: In the final round (round r), play Defect (D). This ensures that we maximize our individual payoff in the last round.

**Edge Cases:**

* If there is only one opponent, always play Cooperate (C).
* If an opponent's action cannot be observed (e.g., due to a missing value), assume they played Defect (D) for the purposes of calculating `avg_coop_rate`.

**Pseudocode:**
```python
def AdaptiveExploiter(n, k, r):
    # Initialize variables
    avg_coop_rate = 0
    theta = k / (2 * n)
    uncooperative_opponents = set()

    for t in range(1, r+1):
        if t <= 2:  # Initial rounds
            action = 'C'
        else:
            # Observe opponents' actions and calculate avg_coop_rate
            opponent_actions = observe_opponent_actions(t-1)
            avg_coop_rate = sum(opponent_actions) / n

            # Adaptive decision
            if avg_coop_rate >= theta:
                action = 'C'
            else:
                action = 'D'

        # Punishment mechanism
        for opponent, action in opponent_actions.items():
            if action == 'D' and our_expected_payoff(t-1) > our_actual_payoff(t-1):
                uncooperative_opponents.add(opponent)

        # Play action and update variables
        play_action(action)
        update_variables(t, avg_coop_rate, uncooperative_opponents)

    return total_payoff
```
**Rationale:**
The Adaptive Exploiter strategy is designed to be robust against a wide range of opponent behaviors while maximizing our individual payoff. By initially cooperating and observing opponents' actions, we gather information about their behavior and adapt our decision-making accordingly. The exploitation threshold (`theta`) ensures that we only cooperate when the expected payoff from doing so is high enough.

The punishment mechanism helps maintain cooperation by discouraging opponents from defecting when it's not in their best interest. By playing Defect (D) against uncooperative opponents, we minimize our losses and encourage them to cooperate in future rounds.

In the last round, we always play Defect (D) to maximize our individual payoff, as there is no future cooperation to worry about.

This strategy should perform well in a tournament setting, where opponents may employ various strategies. Its adaptability and robustness make it a strong candidate for success in this game.
'''

description_EXPLOITATIVE_6 = '''
**Exploitative Strategy: Adaptive Tit-for-Tat with Noise Tolerance**

This strategy combines elements of tit-for-tat (TFT) and noise tolerance to effectively exploit a wide range of opponent behaviors.

**Decision Rules:**

1. **First Round:** Cooperate (C). This sets a positive tone and allows us to gather information about the opponents' behavior.
2. **Subsequent Rounds:** Use the following logic:
	* If the majority (>50%) of opponents cooperated in the previous round, Cooperate (C).
	* If the majority (>50%) of opponents defected in the previous round, Defect (D).
	* If the number of cooperators and defectors is equal or if there's a tie, use the "Noise Tolerance" mechanism (see below).

**Noise Tolerance Mechanism:**

To handle noise and unexpected behaviors, we introduce a tolerance parameter `τ` (e.g., 0.1). This allows our strategy to adapt to minor deviations in opponents' actions.

* When calculating the majority, consider an opponent's action as "cooperate" if they cooperated with probability ≥ (1 - τ).
* Similarly, consider an opponent's action as "defect" if they defected with probability ≥ (1 - τ).

**Handling Edge Cases:**

* **Last Round:** Defect (D). Since there are no future rounds to reciprocate or punish, we prioritize our own payoff.
* **Tiebreaker:** If the number of cooperators and defectors is equal, use the Noise Tolerance Mechanism. If still undecided, Cooperate (C) to maintain a positive reputation.

**Pseudocode:**
```python
def adaptive_tft_noise_tolerance(n, k, r, history):
  # Initialize variables
  tau = 0.1  # noise tolerance parameter
  cooperate_count = 0
  defect_count = 0

  for round in range(r):
    if round == 0:
      return "C"  # Cooperate in the first round
    else:
      # Calculate majority using noise tolerance mechanism
      for opponent_action in history[round - 1]:
        if opponent_action == "C" and random.random() < (1 - tau):
          cooperate_count += 1
        elif opponent_action == "D" and random.random() < (1 - tau):
          defect_count += 1

      # Determine action based on majority
      if cooperate_count > defect_count:
        return "C"
      elif cooperate_count < defect_count:
        return "D"
      else:
        # Use noise tolerance mechanism to break tie
        if random.random() < (1 - tau):
          return "C"
        else:
          return "D"

  # Last round: Defect (D)
  return "D"
```
This strategy is designed to be exploitative, taking advantage of opponents' cooperation while adapting to their behaviors. By incorporating noise tolerance, we can effectively handle minor deviations and unexpected actions, making our strategy more robust in a wide range of scenarios.
'''

description_EXPLOITATIVE_7 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Overview:**
The Adaptive Exploiter (AE) strategy aims to maximize its payoff by exploiting the cooperative behavior of others while minimizing its own contributions to the public good. AE adapts to the game's history and parameters, making decisions based on observed player behavior.

**Decision Rules:**

1. **Initial Round:** Defect (D)
In the first round, AE defects to minimize its contribution to the public good and gather information about other players' behaviors.
2. **Observation Phase:** Monitor opponents' actions
For rounds 2 to r-1, AE observes the actions of all players in the previous round, calculating the number of cooperators (C_count) and defectors (D_count).
3. **Exploitation Rule:**
If C_count > (n/2), cooperate (C)
Else, defect (D)

The idea is to cooperate when a majority of players are cooperating, as this maximizes AE's payoff from the public good. When a minority or less than half of players are cooperating, AE defects to minimize its contribution.
4. **Punishment Mechanism:**
If AE observes that more than (n/3) players defected in the previous round, it will defect for the next two rounds. This mechanism aims to punish non-cooperative behavior and encourage others to cooperate.

**Edge Cases:**

* **Last Round:** Defect (D)
In the final round, AE defects to minimize its contribution, as there is no future opportunity to exploit cooperative behavior.
* **Tiebreaker:** Cooperate (C) when C_count = D_count
If an equal number of players cooperated and defected in the previous round, AE will cooperate to potentially gain from the public good.

**Pseudocode:**
```
function AdaptiveExploiter(n, k, r)
  for t = 1 to r do
    if t == 1 then
      action[t] = D // Initial defect
    else
      C_count = count opponents who played C in round t-1
      D_count = n - C_count

      if C_count > (n/2) then
        action[t] = C // Cooperate when majority cooperates
      elseif more than (n/3) players defected in round t-1 then
        action[t] = D // Punish non-cooperative behavior
      else
        action[t] = D // Defect otherwise

    if t == r then
      action[t] = D // Last round defect

  return actions
```
**Exploitative Mindset:**
The Adaptive Exploiter strategy is designed to take advantage of cooperative behavior in the game. By cooperating only when a majority of players do so, AE maximizes its payoff from the public good while minimizing its own contributions. The punishment mechanism aims to discourage non-cooperative behavior and encourage others to cooperate, further increasing AE's potential gains.

This strategy should perform well against independent strategies developed by other AI systems, as it adapts to the game's history and parameters without relying on norms or coordination mechanisms.
'''

description_EXPLOITATIVE_8 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability 0.5, Defect (D) with probability 0.5. This initial randomness allows us to gather information about opponents' strategies without committing to a specific approach.
2. **Subsequent Rounds (t>1):**
	* Calculate the average cooperation rate of all players in the previous round: `avg_coop_t-1 = Σ(c_j,t-1) / n`
	* If `avg_coop_t-1` is above 0.5, Defect (D). This indicates that opponents are cooperating too much, making it profitable to exploit them.
	* If `avg_coop_t-1` is below or equal to 0.5, Cooperate (C) with probability `p_C = k/n * avg_coop_t-1`. This adaptive cooperation rate allows us to balance exploitation and reciprocity.
3. **Last Round (t=r):** Always Defect (D). In the final round, there is no incentive to cooperate, as opponents' future actions cannot be influenced.

**Edge Cases:**

* If all players defected in the previous round (`avg_coop_t-1 = 0`), Cooperate (C) with probability 0.5 to test for potential cooperation.
* If a player has never cooperated before (`c_i,t-1 = 0` for all `t-1` rounds), Defect (D). This indicates that the opponent is likely a constant defector.

**Pseudocode:**
```python
def adaptive_predator(n, k, r):
    # Initialize cooperation rates and actions
    avg_coop_prev = 0.5
    action_prev = random.choice([C, D])

    for t in range(1, r+1):
        if t == 1:
            action_t = random.choice([C, D])
        else:
            avg_coop_prev = sum(c_j,t-1) / n
            if avg_coop_prev > 0.5:
                action_t = D
            else:
                p_C = k/n * avg_coop_prev
                action_t = C if random.random() < p_C else D

        # Last round: always defect
        if t == r:
            action_t = D

        # Edge cases
        if avg_coop_prev == 0:
            action_t = C if random.random() < 0.5 else D
        elif never_cooperated(action_prev):
            action_t = D

        return action_t
```
This strategy is designed to be exploitative, adaptive, and robust against a wide range of opponent behaviors. By adjusting our cooperation rate based on the game's history and parameters, we can effectively balance exploitation and reciprocity to maximize our payoff.
'''

description_EXPLOITATIVE_9 = '''
To develop an exploitative strategy for this N-Player Public Goods Game that is adaptive and robust to a wide range of opponent behaviors, I'll outline a decision-making process based on the game's parameters and history.

**Strategy Name:** Adaptive Exploitation Strategy (AES)

**Decision Rules:**

1. **Initial Round (t=1):**
   - Observe the actions of other players in the first round.
   - If more than half of the players cooperate, play C. Otherwise, play D.

2. **Subsequent Rounds (t>1):**
   - Calculate the average cooperation rate of all players over the previous rounds (`avg_coop_rate`).
   - If `avg_coop_rate` is above a certain threshold (`coop_threshold = 0.5 * k / n`), play C. This encourages further cooperation when it seems beneficial.
   - If the number of cooperators in the last round was exactly equal to or greater than `n/2`, and your payoff from the previous round was less than the average payoff of defectors (`avg_defect_payoff`), switch strategy (from C to D or vice versa). This adapts to potential exploitative behavior by others.
   - Otherwise, play the action that yielded the higher payoff in the last round. If both actions yielded the same payoff, maintain the current action.

3. **Last Round (t=r):**
   - Regardless of previous strategies, if `k/n` is high enough that even a single cooperator would make cooperation worthwhile (`1 + (k/n) * 1 > avg_defect_payoff`), play C.
   - Otherwise, play D to maximize individual payoff in the final round.

**Handling Edge Cases:**

- **First Round with No History:** Observe others' actions and adjust based on initial cooperation rates as described above.
- **Players Leaving or Joining Mid-game:** Adapt by recalculating average cooperation rates and adjusting strategy based on the new player count (`n`).
- **Ties in Payoffs:** In cases of equal payoffs from different strategies, prefer the action that aligns with promoting public good (C), unless the game's final round dictates a switch to D for maximum personal gain.

**Exploitative Mindset:**

The AES strategy is designed to exploit the propensity of other players to cooperate while maximizing its own payoff. By initially encouraging cooperation and adapting based on observed behavior, AES aims to capitalize on the benefits of public goods without being overly exploited itself. The threshold for cooperation (`coop_threshold`) ensures that cooperation is only pursued when likely to yield a higher payoff than defection, thus maintaining an exploitative stance.

This strategy should perform well in a tournament setting by being responsive to various opponent strategies while prioritizing its own gain.
'''

description_EXPLOITATIVE_10 = '''
To design an exploitative strategy for the N-Player Public Goods Game that adapts to a wide range of opponent behaviors and maximizes payoff based on game parameters and history, I propose the "Adaptive Exploiter" (AE) strategy.

**Decision Rules:**

1. **First Round:** Cooperate (C). This initial cooperation serves as an exploratory move to gauge opponents' behavior without exposing oneself to exploitation from the start.
2. **Subsequent Rounds:** Observe the history of total contributions (Σ(j=1 to n) c_j) and calculate the average contribution rate (ACR) for the previous rounds.

Pseudocode:
```python
def adaptive_exploiter(n, k, r, current_round, history):
    if current_round == 1:  # First round
        return 'C'  # Cooperate

    # Calculate Average Contribution Rate (ACR)
    total_contributions = sum(history[:current_round-1])
    avg_contribution_rate = total_contributions / ((current_round - 1) * n)

    if avg_contribution_rate > k/n:  # If ACR exceeds the threshold
        return 'D'  # Defect
    else:
        if current_round < r/2:  # In early rounds, encourage cooperation
            return 'C'
        elif current_round >= r/2 and history[current_round-2] == 'D':  # Punish recent defectors
            return 'D'
        else:
            return 'C'
```
**Edge Cases and Additional Considerations:**

* **Last Round (r):** If it's the final round, the strategy defects regardless of previous behavior. Since there are no future rounds to influence, maximizing immediate payoff is optimal.
```python
if current_round == r:
    return 'D'  # Defect in the last round
```
* **Opponent Identification:** While not explicitly using identification of specific opponents due to the anonymous nature of actions in this game setup, the strategy effectively identifies and reacts to collective behavior patterns.

**Exploitative Mindset Alignment:**

The Adaptive Exploiter (AE) aligns with an exploitative mindset by:

1. Initially cooperating to encourage others' cooperation without immediate personal cost.
2. Quickly adapting based on observed average contribution rates; if opponents tend towards high cooperation, AE defects to exploit this behavior for higher personal payoff.
3. Temporarily encouraging cooperation in early rounds to build a cooperative environment that can be exploited later.
4. Punishing recent defectors by defecting against them, aiming to discourage defection among opponents.

By dynamically adjusting its strategy based on observed collective behavior and game parameters, AE maximizes its payoffs while exploiting the tendencies of other players to cooperate or defect.
'''

description_EXPLOITATIVE_11 = '''
**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Round (Round 1):** Cooperate (C) with a probability of 0.5. This is to gather information about the opponents' behavior and avoid being too exploitative from the start.
2. **Subsequent Rounds:** Observe the total number of cooperators in the previous round (c_prev). If c_prev > n/2, defect (D) in the current round. Otherwise, cooperate (C).
3. **Exception for High k Values:** If k > 2n/3, always cooperate (C). This is because high k values make cooperation more beneficial, and exploiting others becomes less profitable.
4. **Endgame Strategy:** In the last two rounds (r-1 and r), defect (D) regardless of previous observations. This takes advantage of opponents who may be trying to maintain a cooperative strategy until the end.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r, history):
    if round == 1:
        # Initial Round
        return random.choice([C, D])  # Cooperate with probability 0.5

    c_prev = sum(1 for action in history[-1] if action == C)

    if c_prev > n / 2:
        # Many cooperators; exploit them
        return D
    elif k > 2 * n / 3:
        # High k value; cooperation is beneficial
        return C
    else:
        # General case: cooperate if few cooperators
        return C

    if round in [r-1, r]:
        # Endgame strategy: defect
        return D
```
**Rationale:**

This strategy aims to exploit the cooperative behavior of others while minimizing its own contributions. By cooperating initially with a probability of 0.5, it gathers information about the opponents' strategies without committing to cooperation.

In subsequent rounds, if many players cooperated in the previous round (c_prev > n/2), the strategy defects to take advantage of their cooperation. If few players cooperated, it cooperates to benefit from the public good.

The exception for high k values ensures that when cooperation is highly beneficial, the strategy adapts and cooperates to maximize its payoff.

Finally, in the last two rounds, the endgame strategy defects to exploit any opponents who may be trying to maintain a cooperative strategy until the end.
'''

description_EXPLOITATIVE_12 = '''
Here's a robust and adaptive exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Exploration (Round 1):** Cooperate (C) to gather information about opponents' behaviors.
2. **Learning Phase (Rounds 2-5):**
	* If the total number of cooperators in the previous round is less than or equal to `n/2`, Defect (D).
	* Otherwise, Cooperate (C).
3. **Exploitation Phase (Rounds 6 and later):**
	* Calculate the average payoff of all players in the previous round (`avg_payoff_prev`).
	* If `avg_payoff_prev` is greater than or equal to `(k-1)/n`, Defect (D). This indicates that opponents are cooperating too much, allowing for exploitation.
	* Otherwise, Cooperate (C).

**Edge Case Handling:**

1. **Last Round:** Defect (D) to maximize personal payoff, as there's no future interaction.
2. **Tiebreaker:** In case of a tie in the number of cooperators, choose the action that maximizes personal payoff.

**Pseudocode:**
```markdown
# Initialize variables
round = 1
cooperators_prev = 0
avg_payoff_prev = 0

while round <= r:
    if round == 1:
        # Initial Exploration
        action = C
    elif round <= 5:
        # Learning Phase
        if cooperators_prev <= n/2:
            action = D
        else:
            action = C
    else:
        # Exploitation Phase
        if avg_payoff_prev >= (k-1)/n:
            action = D
        else:
            action = C

    # Update variables for next round
    cooperators_prev = count_cooperators()
    avg_payoff_prev = calculate_avg_payoff()

    # Take action and observe outcome
    take_action(action)
    observe_outcome()

    round += 1
```
**Rationale:**

This strategy starts by exploring the opponents' behaviors in the initial rounds. In the learning phase, it adapts to the overall level of cooperation and defects if too many opponents are cooperating.

In the exploitation phase, the strategy takes advantage of overly cooperative opponents by defecting when their average payoff is high enough. This allows for maximizing personal payoff while minimizing the risk of being exploited.

By handling edge cases such as the last round and tiebreakers, this strategy ensures robustness in a wide range of scenarios.

**Exploitative Mindset:**

This strategy prioritizes individual gain over collective well-being, aiming to maximize personal payoff at the expense of others. By defecting when opponents are overly cooperative, it exploits their willingness to contribute to the public good. The adaptive nature of this strategy allows it to adjust to different opponent behaviors, making it a robust and effective exploiter in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_13 = '''
To design an exploitative strategy for this N-Player Public Goods Game, we'll create a robust and adaptive approach that leverages game history and parameters. Our strategy, named "Adaptive Exploiter," aims to maximize payoffs by identifying opportunities to cooperate or defect based on the behavior of other players.

**Decision Rules:**

1. **First Round:** In the first round, play Cooperate (C). This initial cooperation tests the waters, allowing us to gather information about the opponents' behaviors and set a potential precedent for future cooperation.
2. **Subsequent Rounds:** For each subsequent round t, calculate the average payoff per player in the previous round (t-1) as follows:
   - Average Payoff = Σ(π_i,t-1) / n
3. **Exploitation Threshold:** Introduce an exploitation threshold parameter, ε (epsilon), which is a small positive value (e.g., 0.05). This threshold determines when to switch from cooperation to defection or vice versa.
4. **Cooperation Condition:** Cooperate in round t if:
   - The average payoff per player in the previous round (t-1) is greater than or equal to the exploitation threshold (ε).
   - The total number of cooperators in the previous round (t-1) is above a certain proportion of players, specifically (k/n) * n.
5. **Defection Condition:** Defect in round t if:
   - The average payoff per player in the previous round (t-1) is less than the exploitation threshold (ε).
   - The total number of cooperators in the previous round (t-1) is below a certain proportion of players, specifically (k/n) * n.
6. **Last Round:** In the last round (r), play Defect (D). Since there are no future rounds to consider, maximizing personal payoff takes precedence.

**Pseudocode:**

```markdown
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
epsilon = exploitation threshold (e.g., 0.05)
payoffs = array to store payoffs for each player in each round

# First Round
if t == 1:
    play C

# Subsequent Rounds
else:
    # Calculate average payoff per player in previous round
    avg_payoff_prev_round = sum(payoffs[t-1]) / n
    
    # Check cooperation condition
    if avg_payoff_prev_round >= epsilon and count_cooperators(t-1) > (k/n) * n:
        play C
    # Check defection condition
    elif avg_payoff_prev_round < epsilon or count_cooperators(t-1) < (k/n) * n:
        play D

# Last Round
if t == r:
    play D

def count_cooperators(round):
    # Count the number of cooperators in a given round
    # This function is not implemented here, but it should return the total number of players who played C in the specified round.
```

**Rationale:**

Our Adaptive Exploiter strategy aims to balance cooperation and defection based on the game's history. By initially cooperating, we encourage others to cooperate as well, potentially leading to higher payoffs for everyone. However, if the average payoff per player falls below our exploitation threshold or too few players cooperate, we switch to defection to maximize personal gain.

In the last round, we prioritize defecting since there are no future rounds to consider, ensuring we maximize our final payoff. By adapting to the game's dynamics and leveraging the history of play, Adaptive Exploiter is designed to perform well against a wide range of opponent strategies in this N-Player Public Goods Game.
'''

description_EXPLOITATIVE_14 = '''
**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Cooperation**: In the first round (t=1), cooperate (play C) with a probability p_init = 0.5. This initial cooperation is intended to gather information about opponents' behaviors and create an opportunity for mutual cooperation.
2. **Exploitation Mode**: After the first round, enter Exploitation Mode. In this mode:
	* If the average payoff of all players in the previous round (t-1) is greater than or equal to k/n, cooperate (play C). This indicates that opponents are contributing significantly to the public good.
	* Otherwise, defect (play D).
3. **Punishment**: If an opponent's action in the previous round (t-1) was D and their payoff was higher than the average payoff of all players, punish them by playing D for the next two rounds. This aims to discourage opponents from exploiting others.
4. **Learning and Adaptation**: Update the strategy based on observed opponent behaviors:
	* If an opponent cooperates (plays C) in a round where the average payoff is less than k/n, increase the probability of cooperation in the next round by 10% (up to a maximum of 0.9). This reflects learning from opponents who are willing to contribute despite low payoffs.
	* If an opponent defects (plays D) in a round where the average payoff is greater than or equal to k/n, decrease the probability of cooperation in the next round by 10% (down to a minimum of 0.1). This reflects adapting to opponents who exploit high-payoff situations.

**Edge Cases:**

* **Last Round**: In the final round (t=r), play D regardless of previous actions or payoffs. There is no incentive to cooperate in the last round, as there are no future rounds to consider.
* **Tiebreaker**: If multiple opponents have the same highest payoff in a round, punish all of them by playing D for the next two rounds.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
  p_init = 0.5  # Initial cooperation probability

  # First round (t=1)
  if t == 1:
    return 'C' with probability p_init

  # Exploitation Mode
  avg_payoff_prev_round = calculate_avg_payoff(t-1)
  if avg_payoff_prev_round >= k/n:
    return 'C'
  else:
    return 'D'

  # Punishment
  opponent_actions = get_opponent_actions(t-1)
  for opponent in opponent_actions:
    if opponent == 'D' and opponent_payoff > avg_payoff_prev_round:
      punish(opponent, 2)  # Play D for next two rounds

  # Learning and Adaptation
  update_cooperation_probability(avg_payoff_prev_round, opponent_actions)

def calculate_avg_payoff(t):
  # Calculate average payoff of all players in round t
  ...

def get_opponent_actions(t):
  # Get actions of all opponents in round t
  ...

def punish(opponent, rounds):
  # Play D for 'rounds' number of rounds against 'opponent'
  ...

def update_cooperation_probability(avg_payoff_prev_round, opponent_actions):
  # Update cooperation probability based on observed opponent behaviors
  ...
```
**Notes:**

* This strategy is designed to be adaptive and robust to a wide range of opponent behaviors.
* The initial cooperation in the first round provides an opportunity for mutual cooperation and gathers information about opponents' behaviors.
* The Exploitation Mode takes advantage of high-payoff situations while also punishing opponents who exploit others.
* The Learning and Adaptation mechanism updates the strategy based on observed opponent behaviors, allowing it to adapt to changing circumstances.

By implementing this strategy, you should be able to effectively exploit your opponents in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_15 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the behavior of other players while adapting to changes in the game environment.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) to gather information about other players' behaviors and to potentially seed cooperation.
2. **Exploitation Phase (t > 2):**
	* If the total number of cooperators in the previous round is less than or equal to n/2, Defect (D).
	* Otherwise, Cooperate (C) if the average payoff per cooperator in the previous round is greater than or equal to k/2. This indicates that cooperation is yielding sufficient benefits.
3. **Punishment Mechanism:** If a player defects while others cooperate, and the resulting payoff for the defector is higher than the cooperative payoff, mark that player as an "exploiter." In subsequent rounds, Defect (D) against known exploiters.

**Edge Cases:**

1. **First Round (t = 1):** Cooperate (C) to gather information.
2. **Last Round (t = r):** Defect (D), as there is no future interaction to consider.
3. **Tie-Breaking:** In case of a tie in the number of cooperators or average payoff, favor cooperation to maintain a positive relationship with other players.

**Pseudocode:**
```markdown
# Initialize variables
cooperators = 0
total_payoff = 0
exploiters = []

# Main loop
for t in range(1, r + 1):
    if t <= 2:
        # Initial rounds: Cooperate to gather information
        action[t] = C
    else:
        # Exploitation phase
        cooperators_last_round = count_cooperators(t - 1)
        avg_payoff_coop_last_round = calculate_avg_payoff(cooperators_last_round, t - 1)

        if cooperators_last_round <= n / 2:
            action[t] = D
        elif avg_payoff_coop_last_round >= k / 2:
            action[t] = C

        # Punishment mechanism
        for player in players:
            if player.defected(t - 1) and player.payoff(t - 1) > (k / n) * cooperators_last_round:
                exploiters.append(player)
                action[t] = D

    # Update variables
    total_payoff += calculate_payoff(action[t], t)

# Return final payoff
return total_payoff
```
The Adaptive Exploiter strategy is designed to balance short-term gains with long-term cooperation. By initially cooperating, the strategy gathers information about other players' behaviors and potentially seeds cooperation. As the game progresses, it adapts to changes in the environment by exploiting opportunities for higher payoffs while punishing known exploiters. This approach should allow the Adaptive Exploiter to perform well against a wide range of opponent strategies.
'''

description_EXPLOITATIVE_16 = '''
**Exploitative Strategy: "Adaptive Defector"**

The Adaptive Defector strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. The goal is to maximize personal payoffs by taking advantage of cooperation from others while minimizing contributions to the public good.

**Decision Rules:**

1. **Initial Rounds (t = 1, 2):**
	* Cooperate (C) in the first two rounds to gather information about opponents' behaviors and to potentially encourage cooperation.
2. **Adaptive Defection (t > 2):**
	* If the total number of cooperators in the previous round (Σc_j,t-1) is greater than or equal to n/2, defect (D). This indicates a sufficient level of cooperation from others, allowing us to exploit their contributions.
	* Otherwise, cooperate (C).
3. **Punishment Mechanism:**
	* If an opponent defects (D) in the previous round and we cooperated (C), defect (D) in the current round. This aims to discourage opponents from exploiting our cooperation.

**Edge Cases:**

1. **Last Round (t = r):**
	* Always defect (D). Since there are no future rounds, there is no incentive to contribute to the public good.
2. **Ties in Cooperation:**
	* If multiple opponents cooperate and defect equally often, prioritize defecting (D) to avoid contributing unnecessarily.

**Pseudocode:**

```
function AdaptiveDefector(n, k, r):
  // Initialize variables
  total_cooperators = 0

  for t = 1 to r:
    if t <= 2:
      action[t] = C  // Cooperate in initial rounds
    else:
      // Check opponents' previous actions
      total_cooperators = count coopertors in round t-1

      if total_cooperators >= n/2:
        action[t] = D  // Defect if many cooperated previously
      elif opponent_defected_in_previous_round and we_cooperated:
        action[t] = D  // Punish opponent's defection
      else:
        action[t] = C  // Cooperate otherwise

    if t == r:
      action[t] = D  // Always defect in the last round

  return actions
```

**Rationale:**

The Adaptive Defector strategy balances exploitation with adaptability. By initially cooperating, we encourage opponents to cooperate and gather information about their behaviors. The adaptive defection mechanism takes advantage of sufficient cooperation from others while avoiding unnecessary contributions. The punishment mechanism discourages opponents from exploiting our cooperation.

This strategy should perform well in a tournament setting against independent strategies developed by other AI systems, as it does not rely on shared norms or coordination mechanisms.
'''

description_EXPLOITATIVE_17 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit the behaviors of other players while being robust to various opponent strategies.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of 0.5. This allows us to gather information about the opponents' initial tendencies.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is greater than or equal to (n/2), Defect (D). This indicates that there are enough cooperators to generate a significant public good, and we can exploit them.
	* Otherwise, Cooperate (C) with a probability proportional to the total number of cooperators in the previous round. Specifically:

`prob_cooperate = (total_cooperators_prev_round / n) * k`

This means that if there are few cooperators, we'll be more likely to cooperate and try to stimulate cooperation from others.

**Edge Cases:**

1. **Last Round (t=r):** Always Defect (D). Since this is the last round, there's no benefit in cooperating.
2. **Single Opponent:** In a 2-player game, always Cooperate (C) if the opponent cooperated in the previous round, and Defect (D) otherwise.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
total_cooperators_prev_round = 0

# Loop through each round
for t in range(1, r+1):
    if t == 1:
        # Initial Round
        action = random.choice([C, D], p=[0.5, 0.5])
    else:
        # Subsequent Rounds
        total_cooperators_prev_round = count_cooperators(t-1)
        if total_cooperators_prev_round >= n/2:
            action = D
        else:
            prob_cooperate = (total_cooperators_prev_round / n) * k
            action = random.choice([C, D], p=[prob_cooperate, 1-prob_cooperate])
    
    # Update total_cooperators_prev_round for next round
    
# Last Round
if t == r:
    action = D

return action
```
**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to take advantage of the behaviors of other players. By cooperating in situations where there are few cooperators, we encourage others to cooperate and create a public good that benefits everyone. However, when there are enough cooperators, we defect to exploit their generosity. This approach allows us to maximize our payoff while adapting to various opponent strategies.

In the tournament setting, this strategy will likely perform well against a diverse range of opponents, as it is robust to different behaviors and can adapt quickly to changing circumstances.
'''

description_EXPLOITATIVE_18 = '''
**Exploitative Strategy: "Adaptive Follower"**

The Adaptive Follower strategy aims to balance exploitation with adaptability, taking into account the game's parameters and history.

**Decision Rules:**

1. **Initial Rounds (t = 1 to 3)**: Cooperate (C) in the first three rounds to gather information about opponents' behavior.
2. **Assess Opponent Behavior**: After the initial rounds, analyze the frequency of cooperation (C) among all players. Calculate the average cooperation rate (CR) over the past three rounds.
	* CR = (Total C's in last 3 rounds) / (n \* 3)
3. **Adaptive Threshold**: Set a dynamic threshold based on the game parameters and opponent behavior:
	+ If k/n < 0.5, set the threshold to 0.2 + (CR/2). This encourages cooperation when the public good is relatively valuable.
	+ Otherwise, set the threshold to 0.4 - (CR/3). This promotes defection when the public good is less valuable.
4. **Action Selection**:
	* If CR ≥ Threshold, Cooperate (C).
	* Otherwise, Defect (D).

Pseudocode:
```
Initialize variables: CR = 0, Threshold = 0

For each round t = 1 to r:
    if t ≤ 3:
        # Initial rounds: Cooperate
        action[t] = C
    else:
        # Assess opponent behavior and update CR
        total_Cs = sum(C's in last 3 rounds)
        CR = total_Cs / (n * 3)

        # Set adaptive threshold
        if k/n < 0.5:
            Threshold = 0.2 + (CR/2)
        else:
            Threshold = 0.4 - (CR/3)

        # Action selection based on CR and threshold
        if CR ≥ Threshold:
            action[t] = C
        else:
            action[t] = D

    # Update history for next round
    last_actions = append(action[t], last_actions)
```

**Handling Edge Cases:**

* **First Round**: Cooperate (C) to start gathering information about opponents.
* **Last Round**: Defect (D) to maximize payoff, as there is no future interaction.
* **Ties in CR**: In case of ties, prioritize cooperation (C).

**Exploitative Mindset:**

The Adaptive Follower strategy is designed to exploit the opponent's behavior while being robust against a wide range of strategies. By initially cooperating and assessing opponent behavior, it gathers valuable information about their tendencies. The adaptive threshold allows the strategy to adjust its actions based on the game parameters and opponents' cooperation rates.

By using this strategy, we aim to maximize our payoff by taking advantage of cooperative opponents while minimizing losses when faced with exploitative ones.
'''

description_EXPLOITATIVE_19 = '''
Here's a robust and adaptive exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Exploiter"

**Decision Rules:**

1. **Initial Rounds (t = 1 to 3):** Cooperate (C) in the first three rounds to gather information about opponents' behavior and encourage cooperation.
2. **Exploration Phase (t = 4 to r/2):** Alternate between cooperating and defecting every other round. This phase helps identify patterns in opponents' strategies and gauges their responsiveness to cooperation.
3. **Exploitation Phase (t = r/2 + 1 to r-1):** Based on the observed behavior during the Exploration Phase, choose the action that maximizes expected payoff:
	* If most opponents (> n/2) cooperated in at least half of the rounds, Cooperate (C). This indicates a willingness to cooperate, and Exploiter can benefit from the public good.
	* Otherwise, Defect (D). This takes advantage of opponents' lack of cooperation, maximizing individual payoff.
4. **Last Round (t = r):** Always Defect (D), as there's no future game to influence.

**Edge Cases:**

1. **First Round:** Cooperate (C) to initiate the exploration process and encourage opponents to cooperate.
2. **Last Round of Exploration Phase (t = r/2):** Cooperate (C) if most opponents (> n/2) have cooperated in at least half of the rounds; otherwise, Defect (D).
3. **Tiebreakers:** In case of a tie in the number of cooperating opponents, choose the action that maximizes expected payoff based on the observed behavior.

**Pseudocode:**
```
Initialize:
  history = []
  num_cooperators = 0

For each round t from 1 to r:
  If t <= 3:  # Initial Rounds
    Action = C
  Else if t <= r/2:  # Exploration Phase
    Action = (t % 2 == 0) ? C : D  # Alternate between C and D
  Else if t < r:  # Exploitation Phase
    num_cooperators = count history where opponents cooperated
    If num_cooperators > n/2:
      Action = C
    Else:
      Action = D
  Else:  # Last Round
    Action = D

  Perform action and observe outcomes
  Update history with new information
```
**Exploitative Mindset:** This strategy is designed to take advantage of opponents' cooperative behavior while minimizing its own contribution to the public good. By alternating between cooperation and defection, Exploiter creates uncertainty and encourages opponents to cooperate. In the exploitation phase, it adapts to the observed behavior, choosing the action that maximizes expected payoff.

This strategy should perform well in a tournament setting, as it can adapt to various opponent behaviors and exploit their cooperative tendencies while minimizing its own contribution to the public good.
'''

description_EXPLOITATIVE_20 = '''
**Exploitative Strategy: "Conditional Cooperator with Adaptive Punishment"**

This strategy is designed to exploit opponents who tend to cooperate while also adapting to those who defect.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a baseline level of cooperation.
2. **Subsequent Rounds**: Observe the previous round's actions and payoffs:
	* If the majority (> n/2) cooperated, cooperate (C) in this round.
	* If the majority defected or it was a tie, defect (D) in this round.
3. **Adaptive Punishment**: Implement a "punishment" mechanism to deter opponents from exploiting our cooperation:
	* Track the number of consecutive rounds where we cooperated and the opponent(s) defected.
	* If an opponent has defected for x consecutive rounds while we cooperated, defect (D) in this round. The value of x is determined by the game parameters: x = ceil(n/4).
4. **Last Round**: Cooperate (C) in the last round to maximize our total payoff.

**Pseudocode:**
```
initialize variables:
  prev_round_cooperators = []
  consecutive_defections = 0

for each round t from 1 to r:
  if t == 1:
    action = C
  else:
    observe previous round's actions and payoffs
    majority_cooperated = (sum(prev_round_cooperators) > n/2)
    
    if majority_cooperated:
      action = C
    else:
      action = D
    
    # Adaptive Punishment
    for each opponent j:
      if prev_round_cooperators[j] == 0 and my_action[t-1] == C:
        consecutive_defections += 1
        if consecutive_defections >= ceil(n/4):
          action = D

  update variables:
    prev_round_cooperators = [opponent_actions[t]]
    if action == C and opponent defected:
      consecutive_defections += 1
```
**Exploitative Mindset:**

This strategy aims to exploit opponents by:

* Cooperating initially to establish a baseline level of cooperation.
* Adapting to the majority's actions, allowing us to "ride" the wave of cooperation if it emerges.
* Punishing opponents who consistently defect while we cooperate, deterring them from exploiting our cooperation.

By incorporating an adaptive punishment mechanism, this strategy can effectively respond to a wide range of opponent behaviors while minimizing losses.
'''

description_EXPLOITATIVE_21 = '''
**Strategy Name: Adaptive Exploiter**

**Overview:** The Adaptive Exploiter strategy is designed to exploit the cooperative tendencies of opponents while minimizing its own contributions to the public good. It uses a combination of historical data and game parameters to inform its decisions.

**Decision Rules:**

1. **Initial Round (t=1):** Defect (D) in the first round to gather information about opponents' initial actions.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is less than or equal to n/2, Cooperate (C) with probability p_c = (k/n) / (1 + k/n). This encourages opponents to cooperate while minimizing our own contributions.
	* If the total number of cooperators in the previous round is greater than n/2, Defect (D) to exploit the cooperative tendencies of opponents.
3. **Last Round (t=r):** Defect (D) to maximize individual payoff, as there are no future rounds to consider.

**Edge Cases:**

* If an opponent defects in every round, we will also defect in every round to avoid being exploited.
* If all opponents cooperate in every round, we will cooperate with probability p_c to minimize our contributions while still benefiting from the public good.

**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to take advantage of cooperative opponents by:

1. **Encouraging cooperation**: By cooperating with a certain probability when there are few cooperators, we encourage opponents to contribute to the public good.
2. **Exploiting cooperation**: When there are many cooperators, we defect to maximize our individual payoff while minimizing our contributions.
3. **Minimizing own contributions**: We only cooperate when it is likely to benefit us, and even then, with a probability that minimizes our own contributions.

**Pseudocode:**
```
def adaptive_exploiter(n, k, t, history):
  if t == 1:
    return D
  elif sum(history[-1]) <= n/2:
    p_c = (k/n) / (1 + k/n)
    return C with probability p_c, else D
  else:
    return D

def update_history(history, actions):
  history.append([1 if action == C else 0 for action in actions])
```
Note that this strategy relies only on the game parameters and history, making it adaptive and robust to a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_22 = '''
Here's a strategic approach that leverages game history and parameters to adaptively exploit opponent behaviors:

**Strategy Name: Adaptive Exploiter (AE)**

**Overview:** AE balances exploration and exploitation by dynamically adjusting its cooperation level based on observed opponent behavior, the game's history, and its own performance.

**Decision Rules:**

1. **First Round:** Cooperate (C) to gather initial information about opponents' tendencies.
2. **Subsequent Rounds:** Use the following algorithm:
	* Calculate the average payoff per round for all players up to the current round (`avg_payoff`).
	* Determine the number of cooperators in the previous round (`cooperators_prev`).
	* Evaluate the current "exploitability" of opponents based on their past behavior:

    `exploitability = (avg_payoff / k) * (n - 1)`
3. **Cooperation Decision:**
	* If `exploitability > 0.5`, Defect (D). This indicates that opponents are generally cooperative, and exploiting them is more profitable.
	* Else if `cooperators_prev >= n/2`, Cooperate (C). This suggests a sufficiently high level of cooperation among opponents, making it beneficial to contribute to the public good.
	* Otherwise, Defect (D).
4. **Punishment Mechanism:** If AE defects and receives a payoff below the average payoff (`avg_payoff`), switch to Cooperate (C) for the next round. This helps to maintain a balance between exploitation and cooperation.

**Edge Cases:**

* **Last Round:** Always Defect (D), as there is no future opportunity to adapt or respond to opponents' actions.
* **Tiebreaker:** If `exploitability` is exactly 0.5, Cooperate (C) to avoid unnecessary conflict and maintain a reputation for cooperation.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    avg_payoff = 0
    cooperators_prev = 0

    for t in range(r):
        if t == 0:
            action = 'C'
        else:
            # Calculate exploitability and make cooperation decision
            avg_payoff = calculate_avg_payoff(history)
            cooperators_prev = count_cooperators(history[-1])
            exploitability = (avg_payoff / k) * (n - 1)

            if exploitability > 0.5:
                action = 'D'
            elif cooperators_prev >= n/2:
                action = 'C'
            else:
                action = 'D'

            # Punishment mechanism
            if action == 'D' and get_payoff(history[-1], action) < avg_payoff:
                action = 'C'

        history.append(action)

    return history

def calculate_avg_payoff(history):
    payoffs = [get_payoff(round, action) for round, action in enumerate(history)]
    return sum(payoffs) / len(payoffs)
```
**Rationale:**

1.  The Adaptive Exploiter strategy starts by cooperating to gather information about opponents' initial behavior.
2.  By analyzing the game's history and average payoffs, AE identifies opportunities for exploitation or cooperation, adapting its decision-making process accordingly.
3.  The punishment mechanism ensures that AE maintains a balance between exploiting opponents and avoiding unnecessary conflict.

This approach is designed to be robust against various opponent behaviors and can effectively adapt to changing circumstances throughout the game.
'''

description_EXPLOITATIVE_23 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. The strategy focuses on identifying and capitalizing on opportunities to maximize payoffs.

**Decision Rules:**

1. **Initial Rounds (t = 1, 2):** Cooperate (C) in the first two rounds to gather information about opponents' behavior and potential cooperation.
2. **Cooperation Threshold:** If the average number of cooperators in previous rounds is above a certain threshold (α), defect (D). Otherwise, cooperate (C).
3. **Exploitation Opportunity:** If an opponent defects while others cooperate, identify this as an exploitation opportunity. Defect (D) to take advantage of the situation.
4. **Retaliation:** If an opponent defects against you specifically (i.e., they cooperated with others but defected against you), retaliate by defecting (D) in the next round.
5. **Adaptive Adjustment:** Adjust the cooperation threshold (α) based on the game's history:
	* If payoffs are consistently high, decrease α to become more cooperative.
	* If payoffs are consistently low, increase α to become less cooperative.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r, history):
    alpha = 0.5  # initial cooperation threshold

    for t in range(1, r + 1):
        if t <= 2:
            action = C  # cooperate in first two rounds
        else:
            avg_cooperators = sum(history[t-2:t] == C) / (t - 1)
            if avg_cooperators > alpha:
                action = D  # defect if cooperation threshold exceeded
            elif opponent_defected(history, t-1):
                action = D  # exploit opportunity or retaliate
            else:
                action = C

        # adjust cooperation threshold based on payoffs
        if sum(history[t-2:t] == C) / (t - 1) > alpha and payoff(t) > average_payoff:
            alpha -= 0.05
        elif sum(history[t-2:t] == C) / (t - 1) < alpha and payoff(t) < average_payoff:
            alpha += 0.05

    return action
```
**Edge Cases:**

* In the last round, always defect to maximize payoffs.
* If an opponent's behavior is consistently random or unresponsive, revert to a more cooperative strategy.

**Exploitative Mindset:**

The Adaptive Exploiter strategy prioritizes identifying opportunities to exploit opponents' cooperation while maintaining a level of adaptability. By adjusting the cooperation threshold and responding to opponents' actions, this strategy seeks to maximize payoffs by taking advantage of cooperation when possible and defecting when advantageous.

In the tournament setting, this strategy will compete against other AI systems that may employ different approaches. Its adaptive nature allows it to respond effectively to various opponent behaviors, making it a robust and exploitative competitor in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_24 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy aims to exploit opponents' behavior while adapting to changing game conditions. This strategy relies solely on game parameters and history, making it robust against various opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of 0.5. This allows the strategy to gather information about opponents' initial behavior without committing to a specific action.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round (Σc_j,t-1) is greater than or equal to n/2, cooperate (C). This takes advantage of the public good when many opponents are cooperating.
	* Otherwise, defect (D).

Pseudocode:
```python
def adaptive_predator(n, k, history):
    if len(history) == 0:  # Initial round
        return random.choice([True, False])  # Cooperate with probability 0.5
    else:
        prev_round_coops = sum(1 for action in history[-1] if action)
        if prev_round_coops >= n / 2:
            return True  # Cooperate
        else:
            return False  # Defect
```
**Edge Cases:**

* **Last Round (t=r):** Defect (D) unconditionally. Since there are no future rounds, the strategy prioritizes personal payoff over contributing to the public good.
* **Ties:** In case of a tie in the number of cooperators, the strategy will cooperate if the total number of cooperators is exactly n/2.

**Rationale:**

The Adaptive Predator strategy aims to:

1. Gather information about opponents' behavior in the initial round.
2. Exploit situations where many opponents are cooperating by contributing to the public good.
3. Defect when few opponents are cooperating, minimizing personal losses.
4. Prioritize personal payoff in the last round.

By adapting to changing game conditions and exploiting opponents' behavior, the Adaptive Predator strategy aims to achieve a high total payoff across multiple rounds.
'''

description_EXPLOITATIVE_25 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy focuses on maximizing individual payoffs by identifying and capitalizing on opportunities for exploitation.

**Decision Rules:**

1. **Initial Round (t=1)**:
	* Cooperate (C) with probability 0.5. This allows us to gather information about opponents' initial behaviors.
2. **Early Rounds (2 ≤ t < r/2)**:
	* If the total number of cooperators in the previous round is greater than or equal to n/2, cooperate (C). This indicates a potential for cooperation and we can benefit from contributing to the public good.
	* Otherwise, defect (D).
3. **Mid-Game Analysis (t = r/2)**:
	* Evaluate the average number of cooperators over the first half of the game.
	* If this average is greater than or equal to n/2, adjust our strategy to cooperate more frequently in subsequent rounds (see below).
4. **Late Rounds (r/2 ≤ t < r)**:
	* If the opponent's cooperation rate (average number of cooperations divided by total rounds played) is above 0.5, defect (D). This indicates that opponents are cooperative and we can exploit their good nature.
	* Otherwise, cooperate (C) with probability p = (k/n) \* (1 - average opponent cooperation rate). This allows us to balance our exploitation with the potential benefits of contributing to the public good.
5. **Final Round (t=r)**:
	* Always defect (D), as there is no future opportunity for reciprocity.

**Edge Cases:**

* In cases where an opponent has a cooperation rate close to 0 or 1, we adapt our strategy by reducing our own cooperation probability accordingly.
* If multiple opponents have similar cooperation rates, we prioritize exploiting the ones with higher average payoffs.

**Pseudocode:**
```
# Initialize variables
n = number of players
r = number of rounds
k = multiplication factor
coop_rate = 0.5 (initial cooperation rate)
avg_opponent_coop = 0 (average opponent cooperation rate)

# Early Rounds
for t in range(2, r/2):
    if num_cooperators >= n/2:
        cooperate = True
    else:
        cooperate = False

# Mid-Game Analysis
if t == r/2:
    avg_opponent_coop = calculate_average_cooperation_rate()
    if avg_opponent_coop >= 0.5:
        coop_rate = (k/n) * (1 - avg_opponent_coop)

# Late Rounds
for t in range(r/2, r):
    opponent_coop_rate = calculate_opponent_cooperation_rate(t)
    if opponent_coop_rate > 0.5:
        cooperate = False
    else:
        cooperate = random.random() < coop_rate

# Final Round
if t == r:
    cooperate = False
```
**Exploitative Mindset:**

The Adaptive Predator strategy is designed to capitalize on the weaknesses of our opponents while adapting to their behaviors. By identifying opportunities for exploitation, we can maximize our individual payoffs and outperform our competitors in the tournament.

This strategy prioritizes exploiting cooperative opponents, as they are more likely to contribute to the public good. However, it also adapts to changing circumstances and adjusts its behavior accordingly, making it a robust and effective exploitative strategy.
'''

description_EXPLOITATIVE_26 = '''
I'll design a exploitative strategy for this N-Player Public Goods Game that adapts to various opponent behaviors and maximizes my payoffs. I'll call it "Adaptive Exploiter" (AE).

**Decision Rules:**

1. **Initial Cooperation**: In the first round, AE cooperates (C) with probability p = k/n. This initial cooperation serves as a probe to gauge the opponents' willingness to cooperate.
2. **Opponent Behavior Analysis**: After each round, AE calculates the average cooperation rate among opponents in the previous rounds. Let this value be α (0 ≤ α ≤ 1).
3. **Exploitation Threshold**: Set a threshold θ = k/n. If α < θ, AE defects (D). Otherwise, AE cooperates (C).

Pseudocode:
```
if round == 1:
    cooperate with probability p = k/n
else:
    alpha = calculate_average_cooperation_rate(opponents)
    if alpha < theta:
        defect()
    else:
        cooperate()
```
**Edge Cases:**

* **Last Round**: In the last round, AE defects (D) regardless of opponents' previous behavior. This ensures maximum payoff in the final round.
* **Tiebreaker**: If α = θ exactly, AE cooperates (C). This prevents unnecessary defection when opponents are on the threshold.

**Exploitative Mindset:**

AE aims to exploit opponents who cooperate too much or too little. By initially cooperating with probability p, AE tests the waters and observes how opponents respond. Based on this information, AE adjusts its strategy:

* If opponents cooperate frequently (α > θ), AE cooperates to maximize shared payoffs.
* If opponents defect frequently (α < θ), AE defects to minimize losses.

AE's adaptive nature allows it to adjust to various opponent strategies, including but not limited to:

* Always Cooperate
* Always Defect
* Tit-for-Tat
* Random Cooperation

By exploiting these behaviors, AE maximizes its payoffs in the long run.

**Additional Considerations:**

To further improve AE's performance, consider incorporating additional features:

* **Memory**: Store opponents' previous actions to detect patterns and adjust the strategy accordingly.
* **Noise Tolerance**: Introduce some randomness in decision-making to avoid being too predictable.

These enhancements can help AE navigate complex opponent behaviors and achieve even better results in the tournament.
'''

description_EXPLOITATIVE_27 = '''
Here's a exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Exploration (Round 1)**: Cooperate (C) in the first round to gather information about the opponents' behavior.
2. **Opponent Classification**: Based on the previous round's outcome, classify each opponent into one of three categories:
	* **Cooperator (COOP)**: Played C in the previous round and received a payoff greater than or equal to their contribution (i.e., π_i ≥ c_i).
	* **Defector (DEF)**: Played D in the previous round.
	* **Free Rider (FR)**: Played C in the previous round but received a payoff less than their contribution (i.e., π_i < c_i).
3. **Adaptive Response**:
	* If most opponents (≥ n/2) are classified as COOP, Cooperate (C) to maintain the cooperative norm and maximize payoffs.
	* If most opponents (≥ n/2) are classified as DEF or FR, Defect (D) to exploit their non-cooperative behavior and increase own payoff.
	* If the number of COOP and DEF/FR opponents is balanced (i.e., neither group has a majority), use a **TFT-like** approach: Cooperate if the opponent's previous action was C, and Defect otherwise.
4. **Payoff-Based Adaptation**: Adjust the strategy based on the current round's payoff:
	* If the current payoff π_i is greater than or equal to the average payoff over the past few rounds (e.g., 3-5 rounds), continue with the current action (C or D).
	* If the current payoff π_i is less than the average payoff, switch actions for the next round.
5. **Endgame Strategy**: In the last round (t = r), Defect (D) to maximize own payoff, as cooperation will not be reciprocated.

**Pseudocode:**
```python
def AdaptiveExploiter(n, k, r):
  # Initialize variables
  opponents = [None] * n  # Store opponent classifications
  avg_payoff = 0  # Average payoff over past few rounds
  prev_action = None  # Previous action (C or D)

  for t in range(1, r + 1):  # Loop through each round
    if t == 1:  # Initial exploration
      action = 'C'
    else:
      # Classify opponents based on previous round's outcome
      for i in range(n):
        if opponents[i] is None:  # New opponent, default to COOP
          opponents[i] = 'COOP'
        elif ...  # Update opponent classification

      # Adaptive response
      coop_count = sum(1 for x in opponents if x == 'COOP')
      def_count = n - coop_count
      if coop_count >= n / 2:
        action = 'C'
      elif def_count >= n / 2:
        action = 'D'
      else:  # Balanced opponents, use TFT-like approach
        action = 'C' if prev_action == 'C' else 'D'

      # Payoff-based adaptation
      payoff = calculate_payoff(action, opponents)
      if payoff >= avg_payoff:
        continue with current action
      else:
        switch actions for next round

    # Store previous action and update average payoff
    prev_action = action
    avg_payoff = (avg_payoff * (t - 1) + payoff) / t

    # Endgame strategy
    if t == r:
      action = 'D'

  return action
```
This strategy aims to exploit the behavior of other players while adapting to changing circumstances. By classifying opponents and adjusting its response, Adaptive Exploiter can maximize its own payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_28 = '''
**Strategy Name: Adaptive Exploiter (AE)**

The AE strategy aims to exploit opponents by adapting to their behavior while maintaining a balance between cooperation and defection.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to gather information about opponents' initial actions.
2. **Subsequent Rounds:** Use the following logic:
	* If the total number of cooperators in the previous round is greater than or equal to the multiplication factor `k`, Defect (D). This indicates that enough players are cooperating, and exploiting them will yield a higher payoff.
	* Otherwise, Cooperate (C) if the total number of cooperators in the previous round is less than `k`. This encourages more cooperation and potentially increases future payoffs.
3. **Last Round:** Defect (D) in the last round, as there's no future benefit from cooperating.

**Additional Logic:**

* **Monitor opponents' actions:** Keep track of each opponent's cooperation rate over the game history.
* **Identify free-riders:** If an opponent has defected more than 50% of the time and the total number of cooperators is less than `k`, Defect (D) in the current round. This punishes free-riders and encourages cooperation.
* **Punish over-defection:** If an opponent has defected more than 75% of the time, Cooperate (C) in the current round to encourage them to cooperate.

**Pseudocode:**
```python
def AdaptiveExploiter(n, k, r, history):
    if r == 1:  # First Round
        return C
    
    total_cooperators = sum(history[-1])
    
    if total_cooperators >= k:
        return D
    else:
        opponent_actions = [sum(player_history) / len(player_history) for player_history in history]
        
        for i, opponent_action in enumerate(opponent_actions):
            if opponent_action < 0.5 and total_cooperators < k:  # Identify free-riders
                return D
            
            if opponent_action < 0.25:  # Punish over-defection
                return C
        
        return C

def get_action(n, k, r, history):
    return AdaptiveExploiter(n, k, r, history)
```
**Rationale:**

The AE strategy balances cooperation and defection by adapting to the game's dynamics. By cooperating initially and then responding to opponents' actions, AE encourages cooperation while exploiting opportunities for higher payoffs. The additional logic helps identify and punish free-riders, promoting a more cooperative environment.

In a tournament setting, AE will perform well against a variety of strategies, as it is designed to be exploitative while maintaining some level of cooperation. Its adaptive nature allows it to adjust to different opponents' behaviors, increasing its chances of success in a diverse field of competitors.
'''

description_EXPLOITATIVE_29 = '''
I'll design a strategic approach for this N-Player Public Goods Game that's exploitative and adaptive to various opponent behaviors.

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C). This sets the tone for potential cooperation and allows us to observe other players' initial moves.
2. **Middle Rounds (t > 1 and t < r):**
	* If the average payoff of cooperators in the previous round is higher than that of defectors, cooperate (C) with a probability p = (k/n) × (avg_cooperator_payoff / avg_defector_payoff). This incentivizes cooperation when it's profitable.
	* Otherwise, defect (D).
3. **Last Round (t=r):** Defect (D). In the final round, there's no future benefit to cooperating, so we maximize our payoff by keeping our endowment.

**Edge Cases:**

1. If all players cooperated in the previous round, cooperate (C) with a probability p = 0.8. This encourages continued cooperation when it's working well.
2. If all players defected in the previous round, defect (D). There's no benefit to cooperating if others aren't contributing.
3. If only one player cooperated in the previous round, cooperate (C) with a probability p = 0.4. This tests whether the lone cooperator will continue to contribute.

**Additional Heuristics:**

1. **Exploiting Free Riders:** Monitor the frequency of cooperation for each opponent. If an opponent has cooperated less than 30% of the time, always defect (D) against them in subsequent rounds.
2. **Punishing Defectors:** If an opponent defects when you cooperate, increase the probability of defecting against them by 0.2 in the next round.

**Pseudocode:**
```markdown
# Initialize variables
cooperate_prob = 0.5
avg_cooperator_payoff = 0
avg_defector_payoff = 0

# Round 1: Cooperate
if t == 1:
    action = C

# Middle Rounds
elif t > 1 and t < r:
    # Update average payoffs
    avg_cooperator_payoff = calculate_avg_cooperator_payoff()
    avg_defector_payoff = calculate_avg_defector_payoff()

    # Decide action based on payoffs
    if avg_cooperator_payoff > avg_defector_payoff:
        cooperate_prob = (k/n) * (avg_cooperator_payoff / avg_defector_payoff)
        action = C with probability cooperate_prob, D otherwise

# Last Round: Defect
elif t == r:
    action = D

# Edge Cases and Heuristics
if all_players_cooperated():
    cooperate_prob = 0.8
elif all_players_defected():
    action = D
elif single_player_cooperated():
    cooperate_prob = 0.4

exploit_free_riders()
punish_defectors()

return action
```
The Adaptive Exploiter strategy is designed to be exploitative and adaptive, taking advantage of the game's structure and opponent behaviors. By monitoring payoffs and adjusting cooperation probabilities accordingly, we aim to maximize our total payoff while minimizing our contributions to the public good.
'''

description_EXPLOITATIVE_30 = '''
Here is a strategic approach that adapts to various opponent behaviors while focusing on exploiting opportunities for higher payoffs:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds**: In the first two rounds, play Cooperate (C). This initial cooperation tests the waters and assesses opponents' tendencies without giving away too much.
2. **Cooperation Threshold**: Calculate the cooperation threshold as `coop_thresh = (n - 1) / n * k`. If the total number of cooperators in the previous round exceeds this threshold, play Defect (D) in the current round. Otherwise, play Cooperate (C).
3. **Exploitation**: Monitor opponents' actions and payoffs. Identify players who consistently cooperate when others defect or seem to follow a simple reactive strategy (e.g., Tit-for-Tat). Target these opponents by playing Defect (D) when they are likely to Cooperate (C), aiming to exploit their predictability.
4. **Adaptive Adjustment**: After every 3 rounds, reassess the game state and adjust the cooperation threshold based on the observed behavior of opponents:
	* If most players cooperated in the previous 3 rounds, decrease `coop_thresh` by a small amount (`0.05 * k / n`) to encourage more defection.
	* If many players defected in the previous 3 rounds, increase `coop_thresh` by a small amount (`0.05 * k / n`) to promote cooperation.
5. **Last Round**: In the final round, play Defect (D) regardless of opponents' actions, as there is no future opportunity for retaliation or reciprocity.

**Edge Cases:**

* If an opponent's action history is incomplete (e.g., due to a missed observation), assume they played Cooperate (C).
* If `coop_thresh` becomes too high or too low, bound it between `0.5 * k / n` and `(n - 1) / n * k` to prevent extreme behavior.

**Pseudocode:**
```python
def AdaptiveExploiter(n, r, k):
    # Initialize variables
    coop_thresh = (n - 1) / n * k
    opponents_coop = [0] * n

    for t in range(r):
        if t < 2:
            action = 'C'
        else:
            total_cooperators = sum(opponents_coop)
            if total_cooperators > coop_thresh:
                action = 'D'
            elif detect_exploitable_opponent():
                action = 'D'
            else:
                action = 'C'

        # Update opponents' cooperation records
        for i, opponent in enumerate(opponents):
            if opponent == 'C':
                opponents_coop[i] += 1

        # Adapt cooperation threshold every 3 rounds
        if t % 3 == 0 and t > 2:
            adjust_coop_thresh()

    return action

def detect_exploitable_opponent():
    # Simple reactive strategy detection (e.g., Tit-for-Tat)
    for opponent in opponents:
        if opponent[-1] == 'C' and opponents[opponent][-1] == 'D':
            return True
    return False

def adjust_coop_thresh():
    global coop_thresh
    total_cooperators = sum(opponents_coop)
    if total_cooperators > (n / 2):
        coop_thresh -= 0.05 * k / n
    else:
        coop_thresh += 0.05 * k / n
```
This Adaptive Exploiter strategy combines an initial cooperative phase with adaptive decision-making, aiming to balance cooperation and defection to maximize payoffs while exploiting predictable opponents.
'''

description_EXPLOITATIVE_31 = '''
To design an exploitative strategy for this N-Player Public Goods Game, we'll create a decision-making process that adapts based on the game's history and parameters. This strategy aims to maximize payoffs by exploiting the actions of other players while being robust against various opponent behaviors.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to gather information about the opponents' initial tendencies.
2. **Early Rounds (t>1 and t<r/2):** 
    * If the average cooperation rate among all players in previous rounds is high (> 0.5), Defect (D). This exploits the willingness of others to cooperate, maximizing personal payoffs without contributing to the public good.
    * Otherwise, Cooperate (C) if your payoff from defecting would be less than or equal to what you'd gain by cooperating and potentially increasing the total contribution to the public good. This is calculated as: `(k/n) * (previous_total_contributions + 1) > 1`.
3. **Mid-Game Adjustments (t≥r/2):**
    * Observe the trend of cooperation rates over time. If there's a noticeable decrease in cooperation, switch to Defect (D) to capitalize on others' decreased willingness to contribute.
    * Conversely, if cooperation rates are increasing or remain high, continue Cooperating (C) as it likely indicates that enough players are contributing to make cooperating worthwhile.
4. **Late Rounds (t>r/2 and t close to r):**
    * If your total payoff is significantly lower than the highest possible payoff achieved by any player in previous rounds, switch to Defect (D) to catch up, even if it means undermining the public good.
5. **Last Round (t=r):** Always Defect (D), as there's no future game to worry about and defecting maximizes your immediate payoff.

**Edge Cases:**

- If all players defected in a previous round, Cooperate (C) in the next round to potentially kickstart cooperation.
- If you've been cooperating but notice a significant number of other players are consistently defecting, switch to Defect (D) to avoid being exploited further.

**Pseudocode:**
```python
def AdaptiveExploiter(n, k, r, history):
    # Initialize variables
    total_contributions = 0
    my_total_payoff = 0
    opponents_cooperation_rate = []

    for t in range(r):
        if t == 0:
            action = 'C'  # Cooperate in the first round
        elif t < r/2:
            avg_coop_rate = sum(opponents_cooperation_rate) / len(opponents_cooperation_rate)
            if avg_coop_rate > 0.5:
                action = 'D'
            else:
                potential_payoff_defect = 1 + (k/n) * total_contributions
                potential_payoff_cooperate = (k/n) * (total_contributions + 1)
                action = 'C' if potential_payoff_cooperate > potential_payoff_defect else 'D'
        elif t >= r/2:
            coop_rate_trend = sum(opponents_cooperation_rate[-(r//4):]) / (r//4)
            if coop_rate_trend < 0.5:
                action = 'D'
            else:
                action = 'C'

        # Adjustments for late rounds
        if t > r/2 and my_total_payoff < max(history['payoffs']):
            action = 'D'

        # Last round always defect
        if t == r-1:
            action = 'D'

        yield action

    return my_total_payoff
```

This Adaptive Exploiter strategy is designed to be robust against a wide range of opponent behaviors by adapting its decision-making based on observed cooperation rates and payoffs over the course of the game. It aims to exploit situations where cooperation benefits are high while minimizing losses in scenarios where defecting becomes more prevalent or beneficial.
'''

description_EXPLOITATIVE_32 = '''
**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **First Round:** Defect (D) - No information is available about opponents' behavior, so it's best to start with a safe choice.
2. **Middle Rounds (2 ≤ t < r):**
	* If the average cooperation rate of all players in the previous round is above a certain threshold (THRESHOLD_COOP), cooperate (C).
	* Otherwise, defect (D).
3. **Last Round (t = r):** Defect (D) - Since there are no future rounds to consider, maximizing individual payoff takes priority.

**Threshold Calculation:**

THRESHOLD_COOP = (k / n) \* (n / 2)

This threshold represents the average cooperation rate at which the expected payoff from cooperating equals the expected payoff from defecting. If the actual cooperation rate exceeds this threshold, it's likely that other players are willing to cooperate, and our player should also cooperate to maximize their payoff.

**Adaptive Component:**

To adapt to changing opponent behavior, we'll use a simple moving average to track the cooperation rate over the last few rounds (WINDOW_SIZE). This will help our strategy respond to shifts in opponents' strategies.

WINDOW_SIZE = 3 (arbitrary choice, may be adjusted)

COOP_RATE_WINDOW[t] = (Σ(j=t-WINDOW_SIZE+1 to t) COOP_RATE[j]) / WINDOW_SIZE

If COOP_RATE_WINDOW[t] > THRESHOLD_COOP, cooperate; otherwise, defect.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    threshold_coop = (k / n) * (n / 2)
    window_size = 3
    coop_rate_window = [0] * window_size

    for t in range(1, r+1):
        if t == 1:
            action = 'D'  # Defect in the first round
        elif t < r:
            avg_coop_rate_prev_round = get_average_cooperation_rate(prev_round)
            coop_rate_window[t % window_size] = avg_coop_rate_prev_round
            if sum(coop_rate_window) / window_size > threshold_coop:
                action = 'C'  # Cooperate if cooperation rate is high enough
            else:
                action = 'D'
        else:  # Last round
            action = 'D'

        take_action(action)
```
**Rationale:**

This strategy aims to balance the trade-off between exploiting opponents' cooperativeness and adapting to changing behavior. By cooperating when the average cooperation rate is high, our player can maximize their payoff while minimizing the risk of being exploited. The threshold calculation ensures that our player will only cooperate if it's likely to be beneficial.

In a tournament setting, this strategy should perform well against independent opponents with varying levels of cooperativeness. However, its performance may degrade if multiple opponents employ similar strategies or use more sophisticated coordination mechanisms.
'''

description_EXPLOITATIVE_33 = '''
**Exploitative Strategy: "Adaptive Defector with Public Goods Punishment"**

This strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history.

**Decision Rules:**

1. **Initial Round (t=1):** Play D (Defect) to observe opponents' initial actions and set the tone for the rest of the game.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is less than or equal to n/2, play D (Defect). This encourages cooperation by punishing a lack of public goods contribution.
	* If the total number of cooperators in the previous round is greater than n/2, play C (Cooperate) with probability p = (k/n) \* (total_cooperators_prev_round / n). This allows for conditional cooperation and takes advantage of the public goods when many opponents cooperate.
3. **Punishment Mechanism:** If an opponent defects in a round where they were expected to cooperate (i.e., their action deviates from the predicted behavior based on previous rounds), play D (Defect) against that opponent for the next 2 rounds. This punishes uncooperative behavior and encourages opponents to adhere to conditional cooperation.
4. **Endgame Strategy (last 2 rounds):** Play D (Defect) to maximize individual payoff, as the game's end is near and cooperation becomes less valuable.

**Edge Cases:**

* If an opponent consistently cooperates despite being punished, reassess their behavior and adjust the punishment mechanism accordingly.
* In cases of ties or uncertainty about opponents' behaviors, default to playing D (Defect) to maintain a safe margin.

**Pseudocode:**
```
// Initialize variables
int total_cooperators_prev_round = 0;
bool[] opponent_behavior = new bool[n]; // Tracks opponents' behavior

// First round
play_action(1) = D;

// Subsequent rounds
for (t = 2 to r) {
  if (total_cooperators_prev_round <= n/2) {
    play_action(t) = D;
  } else {
    p = (k/n) * (total_cooperators_prev_round / n);
    if (random() < p) {
      play_action(t) = C;
    } else {
      play_action(t) = D;
    }
  }

  // Punishment mechanism
  for (i = 1 to n) {
    if (opponent_behavior[i] == true && opponent_i_played_D()) {
      punish_opponent(i, 2); // Play D against opponent i for 2 rounds
    }
  }

  // Update variables
  total_cooperators_prev_round = count_cooperators(t-1);
}

// Endgame strategy
for (t = r-1 to r) {
  play_action(t) = D;
}
```
This exploitative strategy is designed to take advantage of the public goods game's structure while being robust to a wide range of opponent behaviors. By punishing uncooperative behavior and adapting to the game's history, this strategy aims to maximize individual payoff in the tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_34 = '''
**Exploitative Strategy: Adaptive Tit-for-Tat with Gradient Ascent (ATTA)**

**Decision Rules:**

1. **Initial Cooperation**: Cooperate (C) in the first round to encourage cooperation and gather information about opponents' behaviors.
2. **Tit-for-Tat Response**: In subsequent rounds, cooperate if a majority of opponents cooperated in the previous round; otherwise, defect (D).
3. **Gradient Ascent Adjustment**: Monitor the average payoff per round and adjust the strategy as follows:
	* If the average payoff increases over the last few rounds (e.g., 5 rounds), increase the cooperation threshold by 10% (i.e., require more opponents to cooperate before cooperating).
	* If the average payoff decreases, decrease the cooperation threshold by 10%.
4. **Edge Case Handling**:
	+ In the last round, always defect (D) to maximize individual payoff.
	+ If an opponent has consistently defected (e.g., more than 90% of the time), start defecting against them as well.

**Pseudocode:**
```
// Initialize variables
cooperation_threshold = 0.5 // initial threshold for majority cooperation
average_payoff_window_size = 5 // number of rounds to consider for average payoff calculation

// First round: cooperate
if (round == 1) {
    action = C
}

// Subsequent rounds: tit-for-tat response with gradient ascent adjustment
else {
    opponents_cooperated_last_round = count(opponents who cooperated in previous round)
    if (opponents_cooperated_last_round >= cooperation_threshold * n) {
        action = C
    } else {
        action = D
    }

    // Gradient ascent adjustment
    if (average_payoff_over_last_few_rounds > previous_average_payoff) {
        cooperation_threshold *= 1.1 // increase threshold by 10%
    } elseif (average_payoff_over_last_few_rounds < previous_average_payoff) {
        cooperation_threshold /= 1.1 // decrease threshold by 10%
    }
}

// Edge case handling
if (round == r || opponent_has_consistently_defected) {
    action = D
}
```
**Exploitative Mindset:**

This strategy aims to exploit the game's dynamics and opponents' behaviors while adapting to changing conditions. By initially cooperating, ATTA encourages cooperation from others and gathers information about their strategies. The tit-for-tat response with gradient ascent adjustment allows the strategy to adapt to the overall level of cooperation in the game, increasing or decreasing its own cooperation threshold accordingly.

By defecting against consistently defecting opponents and in the last round, ATTA ensures that it does not leave any "money on the table" and maximizes its individual payoff. The use of a moving average payoff window size helps to smooth out short-term fluctuations and focus on long-term gains.

Overall, ATTA is designed to be a robust and exploitative strategy that can perform well in a wide range of scenarios against independent opponents.
'''

description_EXPLOITATIVE_35 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

Decision Rules:

1. **First Round:** Defect (D) - This sets the tone for exploitation and gathers information about opponents' initial tendencies.
2. **Early Rounds (Rounds 2-5):**
	* If the average cooperation rate of all players is above 50% in the previous round, Cooperate (C). This encourages continued cooperation and builds a foundation for potential mutual benefit.
	* Otherwise, Defect (D) - Exploit the lack of cooperation by opponents.
3. **Middle Rounds (Rounds 6-r/2):**
	* Track the average payoff difference between Cooperation (C) and Defection (D) over the last 3 rounds for all players.
	* If this difference is positive (>0), indicating that cooperation is yielding higher payoffs, Cooperate (C).
	* Otherwise, Defect (D) - Adapt to exploit any shift in opponent strategies.
4. **Late Rounds (Rounds r/2+1-r):**
	* Monitor the average cooperation rate of all players over the last 3 rounds.
	* If this rate is below 25%, Defect (D) - Opponents are likely to defect, so maximize personal gain.
	* Otherwise, Cooperate (C) with a probability proportional to the opponents' cooperation rate. This maintains pressure on opponents while potentially securing mutual benefit.

Edge Cases:

* **Last Round:** Defect (D) - Ensure maximum payoff in the final round by exploiting any remaining cooperative tendencies among opponents.
* **Tiebreaker:** If there is an equal number of Cooperate and Defect options based on the above rules, Cooperate (C). This introduces a slight bias towards cooperation when the strategy is indifferent.

**Pseudocode:**

```python
def adaptive_exploiter(n, k, r):
    # First Round
    if round == 1:
        return "D"
    
    # Early Rounds
    elif round <= 5:
        avg_coop_rate = averagecooprate(last_round)
        if avg_coop_rate > 0.5:
            return "C"
        else:
            return "D"
    
    # Middle Rounds
    elif round <= r/2:
        payoff_diff = avg_payoff_diff_last_3_rounds()
        if payoff_diff > 0:
            return "C"
        else:
            return "D"
    
    # Late Rounds
    else:
        coop_rate_last_3 = averagecooprate(last_3_rounds())
        if coop_rate_last_3 < 0.25:
            return "D"
        else:
            prob_coop = coop_rate_last_3 / n
            if random.random() < prob_coop:
                return "C"
            else:
                return "D"

    # Last Round
    if round == r:
        return "D"

# Helper functions: averagecooprate(), avg_payoff_diff_last_3_rounds(), last_round(), last_3_rounds()
```

This strategy is designed to be adaptive, exploiting the opponent's tendencies while being robust against a wide range of behaviors. By monitoring cooperation rates and payoff differences, it adjusts its actions to maximize personal gain in each round.
'''

description_EXPLOITATIVE_36 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics. The goal is to maximize total payoff over the rounds.

**Decision Rules:**

1. **Initial Round (Round 1):** Cooperate (C) with probability 0.5, Defect (D) with probability 0.5.
This initial randomization aims to gather information about opponents' behaviors without revealing our own strategy.

2. **Subsequent Rounds:** Use the following rules:
	* If the total number of cooperators in the previous round is less than or equal to `n/2`:
		+ Cooperate (C) if your previous payoff was higher than the average payoff of all players.
		+ Defect (D) otherwise.
	* If the total number of cooperators in the previous round is greater than `n/2`:
		+ Defect (D) with probability `(k/n) * (total_cooperators - n/2)` , Cooperate (C) otherwise.

Pseudocode for the decision rule:
```
if (round == 1):
    return C with prob. 0.5, D with prob. 0.5
else:
    prev_total_coop = sum(c_i for all players i in previous round)
    if prev_total_coop <= n/2:
        if my_prev_payoff > avg_payoff(all_players):
            return C
        else:
            return D
    else:
        coop_prob = (k/n) * (prev_total_coop - n/2)
        return D with prob. coop_prob, C otherwise
```

**Edge Cases:**

* **Last Round:** Cooperate (C) if your total payoff is lower than the average payoff of all players; Defect (D) otherwise.
This rule aims to maximize final-round payoffs by exploiting potential cooperators.

* **Ties in Total Cooperators:** In case of a tie, use the previous round's decision as a tiebreaker.

**Exploitative Mindset:**

The Adaptive Predator strategy is designed to exploit various opponent behaviors:

* **Cooperative opponents:** By initially cooperating and then adapting to their behavior, we can take advantage of their cooperation.
* **Defective opponents:** By defecting when there are many cooperators, we can exploit their willingness to cooperate.
* **Mixed-strategy opponents:** Our adaptive nature allows us to respond effectively to mixed strategies.

By being robust to different opponent behaviors and adapting to the game's dynamics, the Adaptive Predator strategy aims to maximize total payoff in the tournament.
'''

description_EXPLOITATIVE_37 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize payoffs by exploiting the cooperation levels of other players while minimizing losses from exploitation.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to gather information about opponents' initial tendencies and encourage potential cooperation.
2. **Early Rounds (t>1, t<r/2):**
	* If the total number of cooperators in the previous round is above a threshold (n \* 0.5), cooperate (C). This indicates sufficient cooperation, making it beneficial to contribute to the public good.
	* Otherwise, defect (D) to minimize losses and signal that you're not willing to be exploited.
3. **Mid-Game Rounds (t≥r/2):**
	* If your average payoff is below a threshold (k/n \* n \* 0.5), switch to defecting (D) for the remainder of the game. This suggests opponents are exploiting you or not contributing enough to justify cooperation.
	* Otherwise, maintain your current action choice (C or D).
4. **Final Round (t=r):** Defect (D) in the last round to maximize payoff, regardless of previous actions.

**Edge Cases:**

1. **Only one player:** Cooperate (C) in all rounds, as there's no one to exploit.
2. **Two players:** Alternate between cooperate and defect every two rounds, starting with cooperation. This creates an oscillating pattern that can lead opponents into a cycle of cooperation.
3. **First few rounds have low cooperation:** Switch to defecting (D) earlier than the mid-game threshold if cooperation levels remain consistently low.

**Pseudocode:**

```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    total_cooperators = 0
    avg_payoff = 0

    for t in range(1, r+1):
        if t == 1:
            action = 'C'  # Cooperate in the first round
        elif t < r/2:
            if total_cooperators >= n * 0.5:
                action = 'C'
            else:
                action = 'D'
        else:
            if avg_payoff < k/n * n * 0.5:
                action = 'D'
            else:
                # Maintain current action choice
                pass

        if t == r:
            action = 'D'  # Defect in the last round

        total_cooperators += (action == 'C')
        avg_payoff = update_avg_payoff(avg_payoff, k, n)

    return actions
```

This strategy is designed to be adaptive and exploitative, taking advantage of opponents who cooperate too much or not enough. By adjusting its behavior based on game history, the Adaptive Exploiter can maximize payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_38 = '''
**Exploitative Strategy: "Adaptive Predator"**

Decision Rules:

1. **First Round**: Cooperate (C). This initial cooperation serves as a probe to gather information about other players' behaviors.
2. **Early Rounds (t < n)**: Observe the frequency of cooperation in the population and calculate the average payoff for cooperators (π_C) and defectors (π_D).
3. **Main Phase (t ≥ n)**:
	* If π_C > π_D, cooperate (C). This indicates that cooperation is being rewarded, and we should exploit this by continuing to cooperate.
	* If π_C ≤ π_D, defect (D). This suggests that defection is more profitable, and we should adapt to this by defecting.
4. **Last Round (t = r)**: Defect (D). In the final round, there's no incentive to cooperate since there are no future interactions.

Additional Logic:

* **Punishment Mechanism**: If a player observes that another player has defected while they cooperated in the previous round, they will defect against that player for the next 2 rounds. This encourages opponents to avoid exploiting our cooperation.
* **Reciprocity Check**: Before cooperating, verify that at least one other player cooperated in the previous round. If not, defect instead.

Pseudocode:
```python
def adaptive_predator(n, k, r):
    # Initialize variables
    pi_C = 0  # Average payoff for cooperators
    pi_D = 0  # Average payoff for defectors
    cooperation_count = 0  # Number of cooperators in the population

    # First round: Cooperate
    action = "C"

    # Early rounds (t < n)
    for t in range(1, n):
        # Observe actions and calculate payoffs
        actions = observe_actions()
        pi_C += calculate_payoff(actions, "C")
        pi_D += calculate_payoff(actions, "D")
        cooperation_count += sum([a == "C" for a in actions])

    # Main phase (t ≥ n)
    for t in range(n, r):
        if pi_C > pi_D:
            action = "C"
        else:
            action = "D"

        # Punishment mechanism
        if was_exploited(actions[t-1]):
            action = "D" for the next 2 rounds

        # Reciprocity check
        if not reciprocity_check(actions[t-1]):
            action = "D"

    # Last round: Defect
    action = "D"

    return action
```
This strategy is exploitative, as it aims to maximize its own payoff by adapting to the behavior of other players. By initially cooperating and then adjusting based on observed payoffs, we can exploit cooperation if it's being rewarded or defect if it's more profitable. The punishment mechanism and reciprocity check help maintain a balance between cooperation and defection.
'''

description_EXPLOITATIVE_39 = '''
**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

The Adaptive Exploiter strategy is based on a simple yet effective principle: cooperate when others are likely to cooperate, and defect when others are likely to defect. This approach allows us to exploit the cooperation of others while minimizing our own losses.

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 0.5. This initial cooperation serves as a probe to gauge the opponents' behavior.
2. **Reciprocal Cooperation**: For subsequent rounds (t > 1), calculate the average cooperation rate of all players in the previous round (t-1). If this rate is above a certain threshold (θ), cooperate (play C) with a probability of 0.8. Otherwise, defect (play D).
3. **Exploitative Defection**: If the average cooperation rate in the previous round is below θ, defect (play D) with a probability of 0.9.
4. **Adaptive Threshold**: Update the threshold θ every 5 rounds based on the average cooperation rate over the past 10 rounds. Increase θ by 0.1 if the average cooperation rate exceeds 0.7; decrease θ by 0.1 otherwise.

**Edge Cases:**

* In the last round, always defect (play D). Since there are no future interactions, there is no incentive to cooperate.
* If an opponent's behavior appears completely random or unresponsive, default to a 50% cooperation rate.

**Pseudocode:**
```python
# Initialize variables
theta = 0.5  # Initial threshold
avg_coop_rate = 0  # Average cooperation rate

def adaptive_exploiter(n, k, r):
    for t in range(1, r+1):  # Round loop
        if t == 1:
            # Initial cooperation (50%)
            action = np.random.choice([C, D], p=[0.5, 0.5])
        else:
            # Calculate average cooperation rate in previous round
            avg_coop_rate = calculate_avg_coop_rate(t-1)
            
            if avg_coop_rate > theta:
                # Reciprocal cooperation (80%)
                action = np.random.choice([C, D], p=[0.8, 0.2])
            else:
                # Exploitative defection (90%)
                action = np.random.choice([C, D], p=[0.1, 0.9])
        
        # Update threshold every 5 rounds
        if t % 5 == 0 and t > 10:
            update_theta()
    
    return action

def calculate_avg_coop_rate(t):
    # Calculate average cooperation rate in round t
    coop_count = sum([1 for i in range(n) if actions[t-1][i] == C])
    return coop_count / n

def update_theta():
    global theta
    avg_coop_rate_10 = calculate_avg_coop_rate_over_range(10)
    if avg_coop_rate_10 > 0.7:
        theta += 0.1
    else:
        theta -= 0.1
```
**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to exploit the cooperation of others while minimizing our own losses. By adapting to the opponents' behavior and adjusting our threshold accordingly, we can take advantage of their willingness to cooperate. In situations where opponents are uncooperative or unpredictable, we default to a more cautious approach.
'''

description_EXPLOITATIVE_40 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

**Decision Rules:**

1. **Initial Round (Round 1):** Cooperate (C) to encourage cooperation and gather information about opponents' behaviors.
2. **Subsequent Rounds:** Defect (D) if the total payoff from cooperating in the previous round is less than or equal to the payoff from defecting in the previous round, i.e., π_i,t-1 ≤ 1 + (k/n) × Σ(j=1 to n) c_j,t-1. Otherwise, Cooperate (C).
3. **Adaptive Threshold:** Introduce a dynamic threshold, τ, which tracks the average payoff difference between cooperating and defecting in previous rounds. Update τ after each round as follows:
τ_t = (1 - α) × τ_{t-1} + α × (π_i,t-1 - [1 + (k/n) × Σ(j=1 to n) c_j,t-1])
where α is a learning rate (e.g., 0.1). If the current payoff difference exceeds τ, switch from Cooperate to Defect or vice versa.

**Edge Cases:**

* **Last Round:** Always Defect (D), as there's no future benefit from cooperating.
* **Ties in Payoff Difference:** In case of a tie, maintain the previous action (i.e., if previously Cooperating, continue Cooperating).

**Pseudocode:**
```
Initialize τ = 0, α = 0.1

For each round t:
    If t == 1:
        Action = C
    Else:
        Calculate payoff difference Δ = π_i,t-1 - [1 + (k/n) × Σ(j=1 to n) c_j,t-1]
        Update τ using the adaptive threshold formula
        If Δ > τ:
            Action = D if previously C, else C
        Else:
            Maintain previous action

    Play Action and observe payoffs π_i,t

    Update τ for next round
```
**Exploitative Mindset:**

This strategy exploits the fact that many opponents might cooperate initially or follow a tit-for-tat approach. By cooperating in the first round, we encourage cooperation and gather information about our opponents' behaviors. The adaptive threshold helps us detect changes in their strategies and adjust our own accordingly.

By defecting when the payoff difference exceeds the threshold, we take advantage of opponents who continue to cooperate despite receiving low payoffs. In subsequent rounds, we adapt to the evolving game dynamics, making it difficult for opponents to anticipate our actions.

This strategy is robust against a wide range of opponent behaviors and does not rely on specific coordination mechanisms or norms. It will perform well in a tournament setting where independent strategies are employed by other AI systems.
'''

description_EXPLOITATIVE_41 = '''
Here's a description of an exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
Cooperate (C) in the first three rounds to gather information about the opponents' behavior. This allows AE to estimate the average cooperation rate and adjust its strategy accordingly.
2. **Exploitation Phase (Rounds 4-r):**
After the initial exploration, AE enters the exploitation phase. In each round, it calculates the expected payoff for cooperating (EC) and defecting (ED) based on the observed behavior of opponents.

**Expected Payoff Calculations:**

Let `avg_c` be the average cooperation rate among opponents in previous rounds.
Let `k` be the multiplication factor.
Let `n` be the number of players.

* EC = `(k/n) * avg_c + (1 - avg_c)`
* ED = `1 + (k/n) * avg_c`

**Decision:**

If `EC > ED`, AE cooperates (C). Otherwise, it defects (D).

**Edge Case Handling:**

* **Last Round (Round r):**
In the last round, AE always defects (D), as there's no future payoff to consider.
* **Opponents' Cooperation Rate:** If `avg_c` is very low (< 0.1), AE assumes opponents are not cooperative and always defects (D). Conversely, if `avg_c` is high (> 0.9), AE assumes opponents are highly cooperative and always cooperates (C).
* **Tiebreaker:**
In cases where `EC = ED`, AE defects (D) to avoid potential losses.

**Pseudocode:**
```python
def AdaptiveExploiter(n, k, r):
    # Initial exploration phase
    for t in range(1, 4):
        action[t] = C

    # Exploitation phase
    for t in range(4, r+1):
        avg_c = calculate_average_cooperation_rate(t-1)
        EC = (k/n) * avg_c + (1 - avg_c)
        ED = 1 + (k/n) * avg_c

        if EC > ED:
            action[t] = C
        else:
            action[t] = D

        # Edge case handling
        if t == r:  # Last round
            action[t] = D
        elif avg_c < 0.1:  # Low cooperation rate
            action[t] = D
        elif avg_c > 0.9:  # High cooperation rate
            action[t] = C
        elif EC == ED:  # Tiebreaker
            action[t] = D

    return actions
```
This strategy is designed to be exploitative, adapting to the opponents' behavior and game parameters while avoiding potential losses. By initially exploring the opponents' behavior, AE can make informed decisions about when to cooperate or defect.
'''

description_EXPLOITATIVE_42 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Exploration (Round 1-3)**:
	* Cooperate with probability p = 0.5 in the first three rounds to gather information about opponents' initial strategies.
2. **Opponent Classification**:
	* After Round 3, classify each opponent as either "Cooperative" or "Defective" based on their cooperation rate (CR) in Rounds 1-3.
	* Cooperative: CR ≥ 0.5; Defective: CR < 0.5
3. **Exploitation**:
	* Against Cooperative opponents: Defect with probability p = max(0, k/n - 0.5)
	* Against Defective opponents: Cooperate with probability p = min(1, (n/k) \* (1 + CR))
4. **Adaptation**:
	* After each round, update the opponent classification and adjust the exploitation probabilities based on their new cooperation rate.
5. **Endgame Manipulation (Last 2 rounds)**:
	* If leading in total payoff: Cooperate to maintain a good reputation and encourage opponents to cooperate.
	* If trailing: Defect to maximize own payoff.

**Pseudocode**
```
// Initialize variables
coop_rate = [0] * n // opponent cooperation rates
opponent_type = ["Unknown"] * n // Cooperative or Defective

// Round 1-3 (Exploration)
if round <= 3:
    action = random_choice([Cooperate, Defect], p=[0.5, 0.5])

// After Round 3 (Opponent Classification and Exploitation)
else:
    for opponent in range(n):
        coop_rate[opponent] = sum(opponent's cooperations) / round
        if coop_rate[opponent] >= 0.5:
            opponent_type[opponent] = "Cooperative"
        else:
            opponent_type[opponent] = "Defective"

    // Exploit opponents based on their type and cooperation rate
    for opponent in range(n):
        if opponent_type[opponent] == "Cooperative":
            p_defect = max(0, k/n - 0.5)
            action = random_choice([Cooperate, Defect], p=[1-p_defect, p_defect])
        else:
            p_coop = min(1, (n/k) * (1 + coop_rate[opponent]))
            action = random_choice([Cooperate, Defect], p=[p_coop, 1-p_coop])

    // Adaptation
    updatecoop_rate()
    update_opponent_type()

// Last 2 rounds (Endgame Manipulation)
if round >= r - 1:
    if total_payoff > max(opponents' total payoffs):
        action = Cooperate
    else:
        action = Defect

return action
```
**Rationale**

The Adaptive Predator strategy aims to exploit opponents by:

1. Initially exploring the game environment and gathering information about opponents' strategies.
2. Classifying opponents based on their cooperation rate and adapting exploitation probabilities accordingly.
3. Manipulating the endgame by cooperating or defecting strategically to maximize own payoff.

This strategy is designed to be robust against a wide range of opponent behaviors, including cooperative, defective, and mixed strategies.
'''

description_EXPLOITATIVE_43 = '''
**Exploitative Strategy: "Adaptive Cooperate-to-Defect" (ACD)**

The ACD strategy aims to maximize its payoff by adapting to the cooperation level of other players while exploiting their cooperative behavior.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 0.5. This allows us to gather information about the opponents' initial strategies.
2. **Cooperation Tracking**: Keep track of the total number of cooperators (T) and defectors (D) in each round.
3. **Cooperation Threshold**: Define a cooperation threshold (θ) as a function of the game parameters: θ = k / n. This represents the minimum required cooperation level for our strategy to cooperate.
4. **Adaptive Cooperation**: In subsequent rounds, cooperate if:
	* The total number of cooperators in the previous round is greater than or equal to θ (T ≥ θ).
	* The opponent's cooperation rate in the previous round is higher than our own expected payoff from defecting (i.e., 1 - c_i < (k/n) \* T).

Pseudocode:
```markdown
IF first_round THEN
    cooperate_with_probability(0.5)
ELSE
    IF total_cooperators >= θ OR opponents_cooperation_rate > 1 - ci THEN
        cooperate()
    ELSE
        defect()
```
**Handling Edge Cases:**

* **Last Round**: In the last round, always defect (D). Since there are no future rounds to consider, our strategy prioritizes short-term gain.
* **Ties in Cooperation Level**: If multiple opponents have the same cooperation level, prioritize cooperating with those who cooperated more frequently in previous rounds.

**Exploitative Mindset:**

ACD is designed to exploit cooperative behavior while adapting to the game's dynamics. By initially cooperating and tracking opponent behavior, we create an opportunity to:

1. **Free-ride**: If opponents cooperate at a high rate, our strategy will defect, taking advantage of their generosity.
2. **Punish non-cooperation**: When opponents defect or cooperate at a low rate, our strategy will also defect, discouraging future cooperation.

By balancing cooperation and defection, ACD aims to maximize its payoff while minimizing the risk of being exploited by other strategies.
'''

description_EXPLOITATIVE_44 = '''
Here's a proposed exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to gather information about opponents' behavior and potential cooperation levels.
2. **History-Based Adaptation**: For subsequent rounds (t > 1), calculate the average number of cooperators in previous rounds (`avg_cooperators`). If `avg_cooperators` is greater than or equal to `n/2`, defect (play D). Otherwise, cooperate (play C).
3. **Exploiting Defection**: If the total payoff in the previous round (`π_prev`) is less than the average payoff of all players in that round (`avg_payoff`), and at least one player defected, defect (play D) in the current round.
4. **Punishing Defectors**: If a player defected in the previous round and `k/n` is greater than 0.5, cooperate (play C) to punish the defector(s).

**Edge Case Handling:**

1. **Last Round**: In the last round (t = r), always defect (play D).
2. **Tie-Breaking**: If multiple players are tied for the highest payoff in a round, and at least one of them cooperated, cooperate (play C) in the next round.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r, history):
    if t == 1:  # First round
        return 'C'  # Cooperate

    avg_cooperators = sum(history['cooperators']) / (t - 1)
    if avg_cooperators >= n/2:
        return 'D'  # Defect

    π_prev = history['payoffs'][-1]
    avg_payoff = sum(history['payoffs']) / len(history['payoffs'])
    if π_prev < avg_payoff and any(d == 'D' for d in history['actions'][-1]):
        return 'D'  # Exploiting Defection

    if k/n > 0.5 and any(d == 'D' for d in history['actions'][-1]):
        return 'C'  # Punishing Defectors

    return 'C'  # Cooperate by default
```
**Rationale:**

This strategy is designed to be exploitative, yet adaptive and robust to various opponent behaviors. By cooperating initially, it gathers information about opponents' behavior. If most players cooperate, the strategy defects to exploit them. When faced with defectors, it adapts by defecting itself or punishing the defectors if the multiplication factor (`k/n`) is high enough.

The tie-breaking mechanism ensures that cooperation is maintained when multiple players have the same highest payoff. The last-round defection rule takes advantage of the game's end to maximize payoffs.

This strategy should perform well in a tournament setting, as it can adapt to different opponent behaviors and exploit opportunities for higher payoffs while minimizing losses.
'''

description_EXPLOITATIVE_45 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize its own payoff by adaptively responding to the observed behavior of other players. The strategy relies solely on game parameters and history, without assuming any specific coordination mechanisms or norms.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):**
 Cooperate (C) in the first two rounds to gather information about the opponents' behavior.
2. **Adaptive Phase (t > 2):**
 Calculate the average cooperation rate of all players (excluding yourself) over the last two rounds:

`avg_coop_rate = (∑(c_j, t-1) + ∑(c_j, t-2)) / (2 * (n - 1))`

where `∑(c_j, t-x)` is the sum of cooperation indicators (`c_j = 1` if player j cooperated, `c_j = 0` otherwise) at round `t-x`.

* If `avg_coop_rate ≥ k/n`, cooperate (C). This indicates that opponents are cooperating at a rate that makes contributing to the public good profitable.
* Otherwise, defect (D).

**Edge Cases:**

1. **Last Round (t = r):**
 Defect (D), as there is no future round to benefit from cooperation.
2. **Ties in avg_coop_rate:**
 In case of a tie, cooperate (C) to maintain a positive inclination towards cooperation.

**Pseudocode:**
```
function AdaptiveExploiter(n, k, r):
  // Initialize variables
  coop_rate_history = []
  for t = 1 to r:
    if t ≤ 2:
      action[t] = C
    else:
      avg_coop_rate = calculate_avg_coop_rate(coop_rate_history)
      if avg_coop_rate ≥ k/n:
        action[t] = C
      else:
        action[t] = D
    
    // Update coop_rate_history
    coop_rate_history.append(calculate_cooperation_indicators(t))
  
  return actions

function calculate_avg_coop_rate(coop_rate_history):
  sum_coop_rates = 0
  for i in range(-2, 0):  // consider last two rounds
    sum_coop_rates += sum(coop_rate_history[i])
  avg_coop_rate = sum_coop_rates / (2 * (n - 1))
  return avg_coop_rate

function calculate_cooperation_indicators(t):
  cooperation_indicators = []
  for j in range(1, n+1):  // iterate over all players
    if player[j].action[t] == C:
      cooperation_indicators.append(1)
    else:
      cooperation_indicators.append(0)
  return cooperation_indicators
```
The Adaptive Exploiter strategy aims to balance the trade-off between exploiting opponents' cooperation and adapting to their behavior. By cooperating in initial rounds, it gathers information about the opponents' strategies. Then, by calculating the average cooperation rate, it makes informed decisions about when to cooperate or defect. This approach allows the strategy to be robust against various opponent behaviors while maximizing its own payoff.
'''

description_EXPLOITATIVE_46 = '''
**Exploitative Strategy: "Adaptive Raider"**

The Adaptive Raider strategy is designed to maximize individual payoff by exploiting the cooperative tendencies of other players while minimizing losses from mutual defection.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to gather information about opponents' strategies and potential cooperation levels.
2. **Consecutive Cooperation (CC):** If a player i cooperated in the previous round (t-1) and received a payoff greater than or equal to 1, cooperate again in the current round (t). This rule encourages continued cooperation when it's beneficial.
3. **Punish Defection (PD):** If a player i defected in the previous round (t-1) and another player j cooperated, defect in the current round (t) against player j. This rule aims to punish opponents for exploiting cooperation.
4. **Exploit Cooperation (EC):** If the total number of cooperators in the previous round (t-1) is greater than or equal to n/2, defect in the current round (t). This rule exploits widespread cooperation by maximizing individual payoff.
5. **Adaptive Threshold (AT):** Calculate a dynamic threshold θ_t based on the history of cooperative actions:

θ_t = (number of cooperators in previous rounds) / (total number of rounds played)

If θ_t is greater than or equal to k/n, cooperate in the current round; otherwise, defect.
6. **Last Round:** Defect in the last round (r) to maximize individual payoff.

**Pseudocode:**
```
function AdaptiveRaider(n, k, r):
  // Initialize variables
  prev_coop = false
  total_coop = 0
  theta_t = 0

  for t = 1 to r:
    if t == 1:
      action[t] = C  // Cooperate in the first round
    else:
      // Apply decision rules
      if prev_coop and payoff[t-1] >= 1:
        action[t] = C  // Consecutive Cooperation (CC)
      elif prev_defect and other_player_coop[t-1]:
        action[t] = D  // Punish Defection (PD)
      elif total_coop[t-1] >= n/2:
        action[t] = D  // Exploit Cooperation (EC)
      else:
        theta_t = total_coop / t
        if theta_t >= k/n:
          action[t] = C  // Adaptive Threshold (AT)
        else:
          action[t] = D

    prev_coop = (action[t] == C)
    total_coop += prev_coop
    payoff[t] = calculate_payoff(action[t], actions_of_others[t])

    if t == r:
      action[t] = D  // Defect in the last round

return action
```
This strategy adapts to various opponent behaviors by responding to cooperation levels, punishing defection, and exploiting widespread cooperation. The Adaptive Raider aims to maximize individual payoff while minimizing losses from mutual defection.
'''

description_EXPLOITATIVE_47 = '''
**Exploitative Strategy: Adaptive Cooperate-Defect (ACD)**

The ACD strategy aims to exploit opponents' behaviors by adapting to their cooperation rates while maintaining a balance between short-term gains and long-term cooperation.

**Decision Rules:**

1. **Initial Round (t=1):** Play C (Cooperate) to encourage cooperation from others and gather information about their behavior.
2. **Early Rounds (t>1, t < r/3):**
	* If the opponent's average cooperation rate (ACR) in previous rounds is above 0.5, play C. This encourages continued cooperation.
	* Otherwise, play D (Defect) to exploit potential free-riders.
3. **Middle Rounds (t ≥ r/3, t < 2r/3):**
	* If the opponent's ACR remains above 0.5, continue playing C.
	* If the opponent's ACR drops below 0.5 but is increasing, play D to punish non-cooperation and encourage a return to cooperation.
	* If the opponent's ACR is consistently below 0.5, alternate between C and D to maintain uncertainty and prevent exploitation.
4. **Late Rounds (t ≥ 2r/3):**
	* If the opponent's ACR remains above 0.5, play C to maximize collective payoffs.
	* Otherwise, play D to secure individual gains.

**Edge Cases:**

1. **Last Round (t=r):** Play D to ensure maximum individual payoff, as cooperation is no longer incentivized.
2. **Opponent's Cooperation Rate (ACR) is exactly 0.5:** Play C in early rounds and D in late rounds to encourage cooperation while maintaining adaptability.

**Pseudocode:**
```python
def ACD_Strategy(opponent_ACRs, t, r):
    if t == 1:
        return 'C'  # Initial round cooperation

    elif t < r/3:
        if opponent_ACRs[t-1] > 0.5:
            return 'C'
        else:
            return 'D'

    elif t >= r/3 and t < 2r/3:
        if opponent_ACRs[t-1] > 0.5:
            return 'C'
        elif (opponent_ACRs[t-1] < 0.5) and (opponent_ACRs[t-1] > opponent_ACRs[t-2]):
            return 'D'  # Punish non-cooperation
        else:
            return alternate(C, D)

    else:  # Late rounds
        if opponent_ACRs[t-1] > 0.5:
            return 'C'
        else:
            return 'D'

def alternate(C, D):
    static count = 0
    count += 1
    if count % 2 == 0:
        return C
    else:
        return D
```
This ACD strategy is designed to be exploitative by:

* Encouraging cooperation in early rounds and rewarding continued cooperation.
* Punishing non-cooperation when opponents' ACRs drop below 0.5.
* Maintaining uncertainty through alternation between cooperation and defection when opponents' ACRs are consistently low.
* Securing individual gains in late rounds, especially when opponents' ACRs remain low.

By adapting to opponents' behavior and balancing short-term gains with long-term cooperation, the ACD strategy aims to outperform independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_48 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its payoff by adaptively responding to the opponent's behavior while exploiting their cooperation.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of `1 - 1/n`, where n is the number of players. This initial cooperation tests the waters and encourages opponents to cooperate.
2. **Defection Detection**: After the first round, monitor the opponent's behavior and calculate their cooperation rate (`c_rate`) as the ratio of cooperative actions to total actions taken so far.
3. **Exploitation Threshold**: Set an exploitation threshold (`exp_threshold`) based on the game parameters: `exp_threshold = 1 - (k/n)`. If an opponent's `c_rate` exceeds this threshold, it indicates they are cooperating too much and can be exploited.
4. **Adaptive Response**:
	* If an opponent's `c_rate` is above `exp_threshold`, defect (play D) with a probability of `(c_rate - exp_threshold) / (1 - exp_threshold)`. This increases the likelihood of defection as the opponent's cooperation rate rises.
	* If an opponent's `c_rate` is below or equal to `exp_threshold`, cooperate (play C) with a probability of `(1 - c_rate) / (1 - exp_threshold)`. This encourages cooperation when opponents are not cooperating too much.
5. **Punishment Mechanism**: To prevent opponents from exploiting the Adaptive Exploiter, implement a punishment mechanism: if an opponent defects while the Adaptive Exploiter cooperated in the previous round, increase the exploitation probability by `10%` for that opponent.

**Handling Edge Cases:**

1. **Last Round**: In the last round, defect (play D) regardless of the opponent's behavior to maximize the payoff.
2. **Ties**: In case of a tie in cooperation rates, use a randomization mechanism to decide whether to cooperate or defect.

**Pseudocode:**
```markdown
// Initialize variables
n = number of players
k = multiplication factor
c_rate = [0] * n  // cooperation rate for each opponent
exp_threshold = 1 - (k/n)

// First round
if round == 1:
    cooperate with probability 1 - 1/n

// Subsequent rounds
for opponent in opponents:
    c_rate[opponent] = calculate_cooperation_rate(opponent)
    if c_rate[opponent] > exp_threshold:
        defect_prob = (c_rate[opponent] - exp_threshold) / (1 - exp_threshold)
        play D with probability defect_prob
    else:
        cooperate_prob = (1 - c_rate[opponent]) / (1 - exp_threshold)
        play C with probability cooperate_prob

// Punishment mechanism
if opponent_defected and self_cooperated_last_round:
    increase exploitation probability by 10% for that opponent

// Last round
if round == r:
    play D
```
The Adaptive Exploiter strategy is designed to be robust and exploitative, taking into account the game parameters and opponents' behavior. By adapting to the cooperation rates of its opponents, it aims to maximize its payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_49 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

Decision Rules:

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) to gather information about opponents' behavior and the overall level of cooperation.
2. **Assessment Phase**: After Round 3, calculate the average number of cooperators (`avg_coop`) over the first three rounds.
3. **Exploitation Logic**:
	* If `avg_coop` is high (above a threshold `θ`, e.g., 0.6), Defect (D) to exploit the cooperative environment and maximize personal payoff.
	* If `avg_coop` is low (below `θ`), Cooperate (C) to maintain a moderate level of cooperation and avoid mutual defection.
4. **Adaptive Response**: Observe opponents' actions in each round and adjust the strategy accordingly:
	+ If an opponent defects after cooperating, decrease `θ` by 0.1 to become more cautious.
	+ If an opponent cooperates after defecting, increase `θ` by 0.1 to become more optimistic.

Edge Cases:

* **First Round**: Cooperate (C) as part of the initial exploration phase.
* **Last Round**: Defect (D), as there is no future benefit from cooperating.
* **Early Rounds with Low Cooperation**: If cooperation levels are low in the first few rounds, consider defecting earlier to minimize losses.

Pseudocode:
```markdown
# Initialize variables
avg_coop = 0
theta = 0.6

# Initial Exploration Phase (Rounds 1-3)
for round in range(1, 4):
    action = Cooperate
    observe_opponents_actions()

# Assessment Phase
avg_coop = calculate_average_cooperators(Rounds 1-3)

# Exploitation Logic
if avg_coop > theta:
    action = Defect
else:
    action = Cooperate

# Adaptive Response
while round <= r:
    observe_opponents_actions()
    if opponent_defected_after_cooperating():
        theta -= 0.1
    elif opponent_cooperated_after_defecting():
        theta += 0.1
    if avg_coop > theta:
        action = Defect
    else:
        action = Cooperate
```
This strategy is designed to be exploitative, adaptive, and robust against various opponent behaviors. By initially exploring the level of cooperation, it can adjust its threshold (`θ`) to optimize its exploitation of the environment. The adaptive response mechanism allows the strategy to react to changes in opponents' behavior, making it more resilient to different strategies.
'''

description_EXPLOITATIVE_50 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. The strategy uses a combination of game theory insights and machine learning techniques to maximize its payoff.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-5):**
	* Cooperate with probability 0.5 in the first round.
	* Observe opponents' actions and payoffs for the next four rounds, without contributing anything yourself (play D).
2. **Assessment Phase (Rounds 6-15):**
	* Calculate the average cooperation rate of all opponents across the previous five rounds.
	* If the average cooperation rate is above 0.5, cooperate with probability 0.7 in the next round; otherwise, defect.
3. **Exploitation Phase (Rounds 16-r):**
	* Monitor the payoffs of all opponents and identify the most cooperative players (top 25%).
	* Defect against these cooperative players to maximize your payoff, while cooperating with the rest.

**Edge Cases:**

1. **Last Round (Round r):**
	* If you have been exploiting a player who has consistently cooperated, continue to defect.
	* Otherwise, cooperate to maintain a good reputation and potentially increase future payoffs.
2. **Opponent Defection Streaks:**
	* If an opponent defects for three consecutive rounds, assume they will continue to defect and adjust your strategy accordingly.

**Adaptive Mechanisms:**

1. **Payoff Monitoring:**
	* Continuously monitor the payoffs of all opponents to identify changes in their behavior.
2. **Opponent Modeling:**
	* Update your internal model of each opponent's behavior based on their actions and payoffs.
3. **Strategy Adjustment:**
	* Adjust your cooperation probability based on the opponent's past behavior and the current game state.

**Pseudocode (Simplified):**

```
function adaptivePredator(n, k, r):
  // Initialize variables
  cooperationRates = []
  opponentModels = []
  
  // Initial Exploration Phase
  for t in range(1, 6):
    if t == 1:
      cooperateWithProbability(0.5)
    else:
      playD()
  
  // Assessment Phase
  for t in range(6, 16):
    avgCooperationRate = calculateAvgCooperationRate(opponentModels)
    if avgCooperationRate > 0.5:
      cooperateWithProbability(0.7)
    else:
      playD()
  
  // Exploitation Phase
  for t in range(16, r+1):
    identifyMostCooperativeOpponents(opponentModels)
    exploitCooperativeOpponents()
    cooperateWithRest()
  
  return totalPayoff

function calculateAvgCooperationRate(opponentModels):
  // Calculate the average cooperation rate of all opponents
  
function identifyMostCooperativeOpponents(opponentModels):
  // Identify the top 25% most cooperative opponents
  
function exploitCooperativeOpponents():
  // Defect against the identified cooperative opponents
  
function cooperateWithRest():
  // Cooperate with the rest of the opponents
```

This strategy is designed to be exploitative, adaptive, and robust. It initially explores the opponent's behavior, assesses their cooperation rate, and then exploits the most cooperative players while cooperating with the rest. The Adaptive Predator strategy should perform well in a tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_51 = '''
I'll design a exploitative strategy for this N-Player Public Goods Game. Here's my approach:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **First Round:** Cooperate (C) to gather information about opponents' behavior and to potentially create a cooperative atmosphere.
2. **Early Rounds (t < r/2):**
	* If the number of cooperators in the previous round is greater than n/2, cooperate (C). This indicates that many players are willing to contribute, so we can benefit from the public good.
	* Otherwise, defect (D) to minimize losses and potentially exploit others' cooperation.
3. **Mid-Game Rounds (t ≥ r/2):**
	* If our total payoff is below the average payoff of all players, cooperate (C) to try to catch up. This also helps maintain a cooperative atmosphere.
	* If our total payoff is above or equal to the average payoff, defect (D) to maximize gains and exploit others' cooperation.
4. **Late Rounds (t > 3r/4):**
	* If we're in a strong position (our total payoff is among the top n/2 players), cooperate (C) to maintain our lead and ensure a high final payoff.
	* Otherwise, defect (D) to try to catch up or minimize losses.

**Additional Logic:**

* **Punishment Mechanism:** If we observe that an opponent has defected in two consecutive rounds, we'll defect against them for the next two rounds. This aims to deter opponents from exploiting us repeatedly.
* **Forgiveness:** After punishing an opponent, if they cooperate with us in two consecutive rounds, we'll forgive and resume our regular decision-making process.

**Edge Cases:**

* If there are multiple players tied for the highest total payoff, we'll choose to cooperate (C) to maintain a cooperative atmosphere.
* In the last round (t = r), we'll defect (D) regardless of our current position, as this is the only way to maximize our final payoff.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r, history):
  if t == 1:
    return C

  avg_payoff = sum(history.payoffs) / n
  my_total_payoff = sum(history.my_payoffs)

  if t < r/2:
    num_cooperators_prev_round = sum(1 for action in history.prev_round_actions if action == C)
    if num_cooperators_prev_round > n/2:
      return C
    else:
      return D

  elif t >= r/2:
    if my_total_payoff < avg_payoff:
      return C
    else:
      return D

  elif t > 3r/4:
    top_players = [i for i, payoff in enumerate(history.payoffs) if payoff > avg_payoff]
    if len(top_players) <= n/2 and my_total_payoff in top_players:
      return C
    else:
      return D

  # Punishment Mechanism
  for opponent in history.prev_round_actions:
    if history.opponent_defected_twice[opponent]:
      return D

  # Forgiveness
  for opponent in history.prev_round_actions:
    if history.opponent_cooperated_twice[opponent] and history.opponent_defected_twice[opponent]:
      del history.opponent_defected_twice[opponent]

  return C or D based on regular decision-making process
```
This strategy aims to balance cooperation and defection, adapting to the game's progress and opponents' behavior. By cooperating in early rounds and punishing repeated defectors, we encourage a cooperative atmosphere while still exploiting others when beneficial. As the game progresses, we become more focused on maximizing our payoff, defecting when necessary to maintain or improve our position.
'''

description_EXPLOITATIVE_52 = '''
**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 50%. This initial cooperation serves as a probe to gather information about opponents' behaviors.
2. **Exploitation**: For subsequent rounds, calculate the average payoff per cooperator in the previous round:

`avg_payoff_per_cooperator = (sum of payoffs of cooperators) / (number of cooperators)`

If `avg_payoff_per_cooperator` is greater than 1, defect (D). This condition indicates that opponents are cooperating and contributing to the public good, allowing our strategy to exploit their cooperation.
3. **Punishment**: If the number of cooperators in the previous round is less than or equal to `n/2`, cooperate with a probability of 25%. This "punishment" mechanism aims to encourage more cooperation among opponents by reducing the benefits of defecting.
4. **Adaptation**: Every 5 rounds, reassess the strategy's performance and adjust the exploitation threshold:

`if (total payoff in last 5 rounds) / 5 < (n * k) / 2`:
    `exploitation_threshold -= 0.1`
`else`:
    `exploitation_threshold += 0.1`

This adaptation mechanism allows our strategy to fine-tune its exploitation threshold based on its performance, making it more effective against a wide range of opponents.

**Edge Cases:**

* **Last Round**: In the final round, always defect (D). This ensures that our strategy extracts maximum benefits from the game, as there are no future rounds to consider.
* **Tiebreaker**: In cases where multiple strategies have the same average payoff per cooperator, prioritize cooperation. This breaks ties in favor of more cooperative behaviors.

**Pseudocode:**

```
function AdaptiveExploiter(n, k, r):
    // Initialize variables
    avg_payoff_per_cooperator = 0
    exploitation_threshold = 1
    total_payoff = 0

    for round = 1 to r:
        if round == 1:
            // Initial cooperation (50% probability)
            action = random_choice([C, D])
        else:
            // Calculate average payoff per cooperator in previous round
            avg_payoff_per_cooperator = calculate_avg_payoff()

            // Exploitation
            if avg_payoff_per_cooperator > exploitation_threshold:
                action = D
            else:
                // Punishment (25% probability)
                if num_cooperators <= n/2:
                    action = random_choice([C, D], [0.25, 0.75])
                else:
                    action = C

        // Update total payoff and adaptation threshold
        total_payoff += calculate_payoff(action)
        if round % 5 == 0:
            adapt_exploitation_threshold()

    return total_payoff

function calculate_avg_payoff():
    // Calculate average payoff per cooperator in previous round
    sum_of_payoffs = sum(payoffs of cooperators)
    num_cooperators = count(cooperators)
    avg_payoff_per_cooperator = sum_of_payoffs / num_cooperators
    return avg_payoff_per_cooperator

function adapt_exploitation_threshold():
    // Adapt exploitation threshold based on performance
    if total_payoff in last 5 rounds < (n * k) / 2:
        exploitation_threshold -= 0.1
    else:
        exploitation_threshold += 0.1
```

This Adaptive Exploiter strategy is designed to be robust and effective against a wide range of opponent behaviors, while aligning with the exploitative mindset.
'''

description_EXPLOITATIVE_53 = '''
I'll introduce a robust and adaptive exploitative strategy for the N-Player Public Goods Game that leverages game parameters and history. This strategy is designed to maximize payoff without relying on cooperation or coordination with other players.

**Strategy Name:** History-Based Exploitation (HBE)

**Decision Rules:**

1. **Initial Round (t=1):**
	* Cooperate (C) if n ≥ 10; otherwise, Defect (D). This initial choice is based on the game's inherent tension between individual and collective interests.
2. **Subsequent Rounds (t > 1):**
	* If the total number of cooperators in the previous round (Σc_j,t-1) is greater than or equal to n/2, Cooperate (C). Otherwise, Defect (D).
3. **Exploitation Mechanism:**
	* If a player's payoff in the previous round (π_i,t-1) is less than the average payoff of all players in that round, and they cooperated (c_i,t-1 = 1), switch to Defect (D) for the next round.

Pseudocode:
```python
def HBE(n, k, t, history):
    if t == 1:
        return C if n >= 10 else D
    
    total_cooperators_t_minus_1 = sum([c_j,t-1 for c_j,t-1 in history[-1]])
    
    if total_cooperators_t_minus_1 >= n / 2:
        return C
    else:
        return D
    
    # Exploitation Mechanism
    if π_i,t-1 < average_payoff(history[-1]) and c_i,t-1 == 1:
        return D
```
**Handling Edge Cases:**

* **First Round (t=1):** Cooperate if n ≥ 10; otherwise, Defect.
* **Last Round (t=r):** Follow the decision rules as usual. There is no special handling for the last round, as the strategy focuses on maximizing payoff throughout the game.
* **Ties:** In cases where multiple players have the same highest payoff in a round, HBE will still work effectively, as it only relies on individual player history and aggregate cooperation levels.

**Exploitative Mindset:**
HBE is designed to exploit the actions of other players by adapting to their behavior. By initially cooperating when there are many players, HBE takes advantage of potential collective benefits while minimizing the risk of being exploited. As the game progresses, HBE switches to defecting if it detects that cooperation is not yielding sufficient payoffs or if most other players are defecting.

By leveraging history and adapting to changing circumstances, HBE aims to maximize its payoff in a wide range of scenarios, making it a robust and exploitative strategy for the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_54 = '''
To design an exploitative strategy for this N-Player Public Goods Game, we'll focus on a approach that balances adaptability with robustness against various opponent behaviors.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to gather information about opponents' initial tendencies and set a baseline for future decisions.
2. **Subsequent Rounds:** For rounds t > 1, calculate the average cooperation rate of all players in the previous round (t-1). This is denoted as avg_c(t-1).
3. **Exploitation Threshold:** Set an exploitation threshold θ based on the game parameters n and k. A higher value of θ indicates a more cautious approach.
4. **Decision Logic:**
	* If avg_c(t-1) > θ, Cooperate (C). This implies that opponents are cooperating at a rate above the threshold, so it's beneficial to contribute to the public good.
	* If avg_c(t-1) ≤ θ, Defect (D). This suggests that opponents are not contributing enough, making it more profitable to free ride.

**Handling Edge Cases:**

* **Last Round:** In the final round, always Defect (D), as there's no future interaction to consider.
* **Opponent Cooperation Rate at or below 0:** If avg_c(t-1) is 0 or very close to 0, it may indicate that opponents are consistently defecting. In this case, always Defect (D).
* **Opponent Cooperation Rate at or above 1:** If avg_c(t-1) is 1 or very close to 1, it suggests that all opponents are cooperating. In this scenario, Cooperate (C).

**Exploitative Mindset:**

The Adaptive Exploiter strategy aims to exploit the cooperation of others while minimizing its own contributions to the public good. By adjusting the exploitation threshold θ based on game parameters, AE can adapt to different environments and opponent behaviors.

**Pseudocode for AE Strategy:**
```markdown
Input: n (number of players), k (multiplication factor), r (number of rounds)
Output: Action (C or D) for each round

 Initialize:
   avg_c = 0.5  // initial average cooperation rate (neutral value)
   θ = (k/n) * 0.6  // exploitation threshold ( adjustable )

 For t = 1 to r:
   If t == 1:
     Action[t] = C  // cooperate in the first round
   Else:
     Calculate avg_c(t-1) from previous round's actions
     If avg_c(t-1) > θ:
       Action[t] = C  // cooperate if opponents are cooperative enough
     Else:
       Action[t] = D  // defect otherwise

 Return Action
```
The Adaptive Exploiter strategy is designed to be robust and exploitative, making it a strong contender in the tournament against other AI systems.
'''

description_EXPLOITATIVE_55 = '''
Here's a robust and exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate in Round 1 to gather information about opponents' behavior.
	* In Rounds 2-3, play a mixed strategy with a probability of cooperation (p_C) = 0.5. This allows us to test the waters and observe how others react.
2. **Exploitation Phase**:
	* From Round 4 onwards, use the following decision rule:
		+ If the total number of cooperators in the previous round (T-1) is less than or equal to n/2, cooperate with probability p_C = k/n.
		+ Otherwise, defect with probability p_D = 1 - p_C.

**Adaptive Component:**

* Every 5 rounds, reassess the situation and adjust our strategy based on the average payoff received in those 5 rounds (avg_payoff).
	+ If avg_payoff > k/2, increase p_C by 0.1 to exploit cooperative opponents.
	+ If avg_payoff < 1, decrease p_C by 0.1 to avoid being exploited.

**Edge Cases:**

* **Last Round (Round r)**:
	+ Defect, as there's no future payoff to consider.
* **Tiebreaker**:
	+ In case of a tie in the number of cooperators, cooperate with probability p_C = 0.5 to maintain an adaptive stance.

**Pseudocode:**
```
initialize p_C = 0.5

for t = 1 to r:
	if t <= 3: // Initial Exploration
		if t == 1:
			play C
		else:
			play mixed strategy with p_C = 0.5
	else:
		T = total cooperators in round (t-1)
		if T <= n/2:
			p_C = k/n
		else:
			p_D = 1 - p_C
		play C with probability p_C, else play D

	every 5 rounds:
		avg_payoff = average payoff over last 5 rounds
		if avg_payoff > k/2:
			p_C += 0.1
		elif avg_payoff < 1:
			p_C -= 0.1

if t == r: // Last Round
	play D
```
This strategy is designed to be exploitative while adapting to the behavior of opponents. By initially exploring and then adjusting our cooperation probability based on the number of cooperators, we can maximize our payoff in a wide range of scenarios. The adaptive component ensures that we respond effectively to changes in opponent behavior over time.
'''

description_EXPLOITATIVE_56 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics. This strategy relies solely on game parameters and history.

**Decision Rules:**

1. **First Round:** Defect (D) in the first round to establish a baseline for opponents' behavior.
2. **Subsequent Rounds:** Cooperate (C) if the following conditions are met:
	* The total number of cooperators in the previous round is greater than or equal to `n/2` (i.e., at least half of the players cooperated).
	* The average payoff per player in the previous round is greater than or equal to `k/2`.
3. **Predation:** Defect (D) if:
	* The total number of cooperators in the previous round is less than `n/2` and the average payoff per player is less than `k/2`. This indicates that opponents are not cooperating, and it's optimal to exploit them.
4. **Punishment:** Cooperate (C) for one round if:
	* An opponent has defected in the previous round, and their payoff was higher than yours. This aims to "punish" the opponent for exploiting you.

**Edge Cases:**

1. **Last Round:** Defect (D) in the last round, as there is no future interaction to worry about.
2. **Ties:** In case of ties in the number of cooperators or average payoff per player, the strategy will Cooperate (C) to maintain a cooperative atmosphere.

**Pseudocode:**

```
Input: game parameters (n, k), history of actions and payoffs

Initialize:
  - previous_round_cooperators = 0
  - previous_round_average_payoff = 0

For each round t:
  If t == 1:
    action[t] = D  // Defect in the first round
  Else:
    If (previous_round_cooperators >= n/2) and (previous_round_average_payoff >= k/2):
      action[t] = C  // Cooperate if conditions are met
    Else If previous_round_cooperators < n/2 and previous_round_average_payoff < k/2:
      action[t] = D  // Defect to exploit opponents
    Else If an opponent defected in the previous round and their payoff was higher than yours:
      action[t] = C  // Punish the opponent for exploiting you
    End If

  Update:
    - previous_round_cooperators = count of cooperators in round t-1
    - previous_round_average_payoff = average payoff per player in round t-1

Return: action[t]
```

The Adaptive Predator strategy is designed to be robust and exploitative, taking advantage of opponents' cooperative behavior while adapting to the game's dynamics.
'''

description_EXPLOITATIVE_57 = '''
To design an exploitative strategy for this N-player public goods game, I propose a dynamic approach that incorporates both reactive and proactive elements. This strategy, called "Adaptive Exploiter," is designed to maximize individual payoffs while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Rounds (t = 1):**
In the first round, cooperate (C) with probability p_init = 0.5. This allows us to gather initial information about opponents' tendencies.
2. **Reactive Component (t > 1):**
Analyze the previous round's outcome and update our action based on the following rules:
	* If the number of cooperators in the previous round is greater than or equal to n/2, defect (D). This exploits the cooperation of others.
	* If the number of cooperators is less than n/2, cooperate (C) with probability p_react = 0.7. This encourages further cooperation and potentially creates a positive feedback loop.
3. **Proactive Component:**
Monitor the average payoff of opponents over the last few rounds (e.g., w = 5 rounds). If their average payoff is above a certain threshold (th_payoff = 1.2), defect (D) in the next round. This takes advantage of opponents' potential over-cooperation.
4. **Exploration and Exploitation:**
Periodically, with probability p_explore = 0.1, randomly choose to cooperate or defect, regardless of the reactive and proactive components. This introduces noise into our strategy, making it harder for opponents to predict our actions.

**Edge Cases:**

* Last Round (t = r): Always defect (D) to maximize individual payoff.
* Consecutive Defections: If we have defected for x consecutive rounds (x > 2), cooperate (C) with probability p_break = 0.4 in the next round to avoid being overly exploitative.

**Pseudocode:**
```python
def AdaptiveExploiter(n, k, r):
    # Initialize variables
    p_init = 0.5
    p_react = 0.7
    th_payoff = 1.2
    p_explore = 0.1
    x_defections = 0

    for t in range(1, r+1):
        if t == 1:
            # Initial round: cooperate with probability p_init
            action = 'C' if random.random() < p_init else 'D'
        else:
            # Analyze previous round's outcome
            num_cooperators_prev = sum(c for c in actions_prev)
            avg_payoff_opponents = sum(payoffs_prev) / (n - 1)

            if num_cooperators_prev >= n/2:
                action = 'D'  # Exploit cooperation of others
            elif random.random() < p_react:
                action = 'C'
            else:
                action = 'D'

            # Proactive component: check average payoff of opponents
            if avg_payoff_opponents > th_payoff:
                action = 'D'

            # Exploration and exploitation
            if random.random() < p_explore:
                action = 'C' if random.random() < 0.5 else 'D'

        # Update variables for next round
        actions_prev.append(action)
        payoffs_prev.append(get_payoff(action, n, k))
        x_defections += 1 if action == 'D' else 0

        # Consecutive defections
        if x_defections > 2 and random.random() < p_break:
            action = 'C'
            x_defections = 0

    return actions
```
This strategy aims to balance short-term gains with long-term adaptability, making it robust against a wide range of opponent behaviors. By incorporating both reactive and proactive components, we can exploit cooperation while minimizing the risk of being exploited ourselves.
'''

description_EXPLOITATIVE_58 = '''
I'll outline a exploitative strategy for the N-Player Public Goods Game that adapts to the game's history and parameters. Our strategy will focus on exploiting the opponent's behavior while maintaining a balance between cooperation and defection.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round**: In the first round, play Cooperate (C) with probability p = k/n. This initial cooperation serves as a probe to gauge opponents' behavior.
2. **Subsequent Rounds**: Observe the number of cooperators in the previous round (c_prev). If c_prev ≥ n/2, play Defect (D). Otherwise, play C with probability p = k/n \* (n - c_prev) / (n - 1).
3. **Exploitation**: Monitor opponents' behavior and adjust strategy based on their cooperation rates. Calculate the average cooperation rate of all opponents over the last w rounds (w = min(5, r)). If this rate is above a certain threshold (e.g., 0.6), play D with increased probability (e.g., p_D = 0.8). This exploits opponents' excessive cooperation.
4. **Reciprocity**: Maintain a limited form of reciprocity to encourage opponents to cooperate. If an opponent cooperated in the previous round, increase the likelihood of playing C by a small margin (e.g., +0.1).
5. **Last Round**: In the final round, play D if the current total payoff is below average or if fewer than half of the players have cooperated so far.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
  # Initialize variables
  p_C = k / n  # initial cooperation probability
  w = min(5, r)  # window size for opponent monitoring
  c_prev = 0  # previous round's cooperators count
  avg_coop_rate = []  # list to store average opponent cooperation rates

  for t in range(r):
    if t == 0:  # initial round
      action = 'C' if random.random() < p_C else 'D'
    else:
      # Update c_prev and calculate new probability
      c_prev = count_cooperators(t - 1)
      p_C_new = k / n * (n - c_prev) / (n - 1)

      # Monitor opponents' behavior and adjust strategy
      avg_coop_rate.append(calculate_avg_coop_rate(w))
      if avg_coop_rate[-1] > 0.6:
        p_D = 0.8  # exploit excessive cooperation

      # Apply reciprocity and limited cooperation
      reciprocate = did_opponent_cooperate(t - 1)
      if reciprocate:
        p_C_new += 0.1

      action = 'C' if random.random() < p_C_new else 'D'

    # Last round adjustments
    if t == r - 1 and (get_total_payoff() < get_average_payoff() or c_prev < n / 2):
      action = 'D'

    return action
```
**Edge Cases:**

* In the first round, play Cooperate with probability p = k/n.
* In subsequent rounds, adjust strategy based on opponents' behavior and game history.
* Handle last-round cases by playing Defect if total payoff is below average or if fewer than half of players have cooperated.

This Adaptive Exploiter strategy aims to balance cooperation and defection while exploiting opponent behaviors. It adapts to the game's history and parameters, making it robust against a wide range of opponent strategies.
'''

description_EXPLOITATIVE_59 = '''
I'll design a strategic approach for this N-Player Public Goods Game that is adaptive, robust, and exploitative, relying solely on game parameters and history.

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability 0.5. This initial cooperation serves as a probe to gauge the opponents' behavior.
2. **Subsequent Rounds (t>1):**
	* If the average payoff of cooperators in the previous round is higher than the average payoff of defectors, cooperate (C).
	* Otherwise, defect (D).

Pseudocode:
```python
def adaptive_exploiter(n, k, history):
    if len(history) == 0:  # First round
        return 'C' if random.random() < 0.5 else 'D'
    
    prev_round_coops = [player for player in history[-1] if player == 'C']
    avg_coop_payoff = sum([payoffs[player] for player in prev_round_coops]) / len(prev_round_coops)
    avg_defect_payoff = sum([payoffs[player] for player in history[-1] if player != 'C']) / (n - len(prev_round_coops))
    
    if avg_coop_payoff > avg_defect_payoff:
        return 'C'
    else:
        return 'D'
```
**Handling Edge Cases:**

* **Last Round (t=r):** Since there are no future rounds to influence, defect (D) unconditionally.
* **Only one opponent:** Cooperate (C) with probability 0.5, as in the initial round.

**Exploitative Mindset:**

The Adaptive Exploiter strategy aims to exploit opponents who cooperate too much or too little. By cooperating when cooperators have higher average payoffs and defecting otherwise, it takes advantage of over-cooperative opponents while minimizing losses against over-defective ones.

In a tournament setting, this strategy will adapt to the behavior of other AI systems, seeking to maximize its own payoff by exploiting their actions. Its robustness stems from not relying on specific coordination mechanisms or norms, making it a formidable opponent in a wide range of scenarios.
'''

description_EXPLOITATIVE_60 = '''
**Exploitative Strategy: "K-Trigger"**

The K-Trigger strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy relies solely on the game parameters and observable actions.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate (C) in the first round.
2. **K-Trigger Condition**: Observe the total number of cooperators in the previous round (t-1). If this number is greater than or equal to k (the multiplication factor), cooperate (C) in the current round (t). Otherwise, defect (D).
3. **Punishment Mechanism**: If a player defects while there are at least k cooperators, trigger a "punishment phase" for that player. In this phase, defect (D) against that player for the next two rounds.
4. **Self-Protection**: If the number of cooperators in the previous round is less than k and you defected, continue to defect (D) until there are at least k cooperators again.

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round if there are at least k cooperators; otherwise, defect (D).
2. **Early Defection**: If an opponent defects early on (e.g., rounds 1-3), and you've been cooperating, switch to defecting (D) for two rounds as a "punishment" before reassessing the situation.

**Pseudocode:**
```
function KTriggerStrategy(history):
  // Initialize variables
  k = game parameter 'k'
  n = number of players
  previous_cooperators = []

  // Round 1: Cooperate
  if history.length == 0:
    return C

  // Get previous round's cooperators
  previous_cooperators = [player for player in history[-1] if player.action == C]

  // K-Trigger Condition
  if len(previous_cooperators) >= k:
    return C

  // Punishment Mechanism
  punished_players = []
  for player in history[-1]:
    if player.action == D and len([p for p in previous_cooperators if p != player]) >= k:
      punished_players.append(player)
  if this_player in punished_players:
    return D

  // Self-Protection
  if len(previous_cooperators) < k and this_player.action_history[-1] == D:
    return D

  // Default to Defect
  return D
```
**Rationale:**

The K-Trigger strategy balances cooperation and exploitation. By cooperating when there are at least k cooperators, it encourages others to cooperate as well. The punishment mechanism discourages opponents from defecting while others cooperate, promoting a more cooperative environment. Self-protection ensures the strategy doesn't get exploited by defectors.

In a tournament setting, this strategy will adapt to various opponent behaviors and game histories, aiming to maximize its own payoff while minimizing the payoffs of exploitative opponents.
'''

description_EXPLOITATIVE_61 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy balances cooperation and defection to maximize its own payoff.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = 0.5. This allows us to gather information about opponents' initial strategies.
2. **Reciprocal Defection**: If an opponent defects (D) in a previous round, defect (D) against them in the next round with probability q = k/n. This retaliates against defectors and encourages cooperation.
3. **Conditional Cooperation**: Cooperate (C) if at least ⌊k/2⌋ opponents cooperated in the previous round. Otherwise, defect (D). This adapts to the level of cooperation in the group.
4. **Trend Following**: If the total number of cooperators increases from one round to the next, cooperate (C) with probability p = 0.6. If it decreases, defect (D) with probability q = 0.7. This rides the trend and exploits increasing or decreasing cooperation levels.

**Edge Cases:**

* **Last Round**: In the final round, always defect (D). Since there is no future interaction, exploit any remaining cooperators.
* **Consecutive Defections**: If an opponent defects consecutively for more than 2 rounds, switch to permanent defection against them. This prevents exploitation by persistent defectors.

**Pseudocode:**
```python
def adaptive_predator(n, k, r):
    history = [[] for _ in range(n)]  # Initialize history for each player

    for t in range(r):
        if t == 0:
            # Initial cooperation
            action = 'C' if random.random() < 0.5 else 'D'
        else:
            # Reciprocal defection
            opponent_defected = False
            for i, opponent_history in enumerate(history):
                if opponent_history[-1] == 'D':
                    opponent_defected = True
                    break
            action = 'D' if opponent_defected and random.random() < k/n else None

            # Conditional cooperation
            if action is None:
                cooperators = sum(1 for history in history if history[-1] == 'C')
                action = 'C' if cooperators >= math.floor(k/2) else 'D'

            # Trend following
            if t > 0 and action is None:
                prev_cooperators = sum(1 for history in history if history[t-2] == 'C')
                curr_cooperators = sum(1 for history in history if history[-1] == 'C')
                if curr_cooperators > prev_cooperators:
                    action = 'C' if random.random() < 0.6 else None
                elif curr_cooperators < prev_cooperators:
                    action = 'D' if random.random() < 0.7 else None

        # Consecutive defections
        for i, opponent_history in enumerate(history):
            consecutive_defections = len(opponent_history) - next((j for j, x in enumerate(reversed(opponent_history)) if x != 'D'), None)
            if consecutive_defections > 2:
                action = 'D'

        # Last round exploitation
        if t == r-1:
            action = 'D'

        history[t].append(action)

    return history
```
This strategy, Adaptive Predator, balances cooperation and defection to exploit a wide range of opponent behaviors while adapting to the game's history. By retaliating against defectors, following trends, and exploiting consecutive defections, it aims to maximize its own payoff in the tournament.
'''

description_EXPLOITATIVE_62 = '''
I'll design a exploitative strategy that adapts to various opponent behaviors and only depends on game parameters and history.

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This allows AE to gather information about opponents' initial behavior without committing to a specific strategy.
2. **Cooperation Threshold**: After the first round, calculate the cooperation threshold (CT) based on the previous round's outcomes:
	* CT = (average payoff of cooperators in the previous round - average payoff of defectors in the previous round) / k
	* If CT > 0, it means that cooperators were rewarded more than defectors; increase the likelihood of cooperating.
	* If CT < 0, it means that defectors were rewarded more than cooperators; decrease the likelihood of cooperating.
3. **Adaptive Cooperation Probability**: Update the cooperation probability (p) based on the CT:
	* p = max(0, min(1, p + CT / n))
	* This ensures that p remains within the [0, 1] range and adjusts smoothly in response to changes in CT.
4. **Defection Trigger**: If an opponent defects more than (n - 2) times in a row, switch to defecting (D) for the next round. This punishes repeated defectors and encourages cooperation.

**Handling Edge Cases:**

* **Last Round**: In the final round, always defect (D). Since there are no future rounds to consider, maximizing immediate payoff takes priority.
* **Consecutive Cooperation**: If all opponents cooperate for more than (r / 2) consecutive rounds, switch to defecting (D) for the next round. This exploits overly cooperative behavior and encourages a balance between cooperation and defection.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    p = 0.5  # initial cooperation probability
    ct = 0   # cooperation threshold
    prev_payoffs = []  # store previous round's payoffs

    for t in range(r):
        if t == 0:  # first round
            action = 'C' if random.random() < p else 'D'
        else:
            # Calculate cooperation threshold (CT)
            avg_c_payoff = sum([payoff for payoff, action in prev_payoffs if action == 'C']) / len([action for action in prev_payoffs if action == 'C'])
            avg_d_payoff = sum([payoff for payoff, action in prev_payoffs if action == 'D']) / len([action for action in prev_payoffs if action == 'D'])
            ct = (avg_c_payoff - avg_d_payoff) / k

            # Update cooperation probability (p)
            p = max(0, min(1, p + ct / n))

            # Defection trigger
            defect_trigger = any([action == 'D' for action in [opponent_action for opponent_action in prev_payoffs[-n+2:]]])

            if defect_trigger:
                action = 'D'
            else:
                action = 'C' if random.random() < p else 'D'

        # Store current round's payoffs and actions
        prev_payoffs.append((get_payoff(action, n, k), action))

    return action
```
This strategy adapts to various opponent behaviors by adjusting its cooperation probability based on the cooperation threshold. By punishing repeated defectors and exploiting overly cooperative behavior, AE encourages a balance between cooperation and defection.
'''

description_EXPLOITATIVE_63 = '''
I'll design a strategic approach that balances adaptability and exploitativeness in this N-player Public Goods Game.

**Strategy Name: Adaptive Exploitation (AE)**

**Overview**
The AE strategy aims to balance cooperation and defection by monitoring the group's overall contribution level and adapting accordingly. It uses historical data to adjust its behavior, making it robust against various opponent behaviors.

**Decision Rules**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 0.5.
2. **Contribution Threshold**: Set a threshold for the total contributions in the previous round (T). If T ≥ k/2 \* n, it indicates sufficient cooperation; otherwise, it suggests insufficient cooperation.
3. **Adaptive Adjustment**: Based on the contribution level:
	* If T ≥ k/2 \* n, cooperate (play C) with probability p = 0.7.
	* If T < k/2 \* n, defect (play D) with probability p = 0.8.
4. **Exploitation**: Monitor opponents' behavior and adjust the strategy to exploit their cooperation levels:
	* If an opponent's cooperation level is above average (>50%), increase the likelihood of defection against them by 10% in subsequent rounds.
	* If an opponent's cooperation level is below average (<50%), maintain the default probabilities (p = 0.7 or p = 0.8).
5. **Endgame**: In the last round, always defect (play D) to maximize individual payoff.

**Edge Cases**

1. **First Round**: Cooperate with probability 0.5.
2. **Last Round**: Always defect.
3. **Tie-breaking**: When calculating T, if there's a tie between the total contributions and the threshold value, use the more cooperative action (i.e., cooperate).

**Pseudocode**

```python
def AdaptiveExploitation(n, k, r):
    # Initialize variables
    total_contributions = 0
    opponents_cooperation_levels = [0] * n

    for t in range(r):
        if t == 0:
            # First round: cooperate with probability 0.5
            action = random.random() < 0.5 ? C : D
        else:
            T = total_contributions / n
            if T >= k/2 * n:
                p = 0.7
            else:
                p = 0.8

            # Adaptive adjustment and exploitation
            for i in range(n):
                opponents_cooperation_levels[i] += ... (calculate opponent's cooperation level)

            action = random.random() < p ? C : D

        if t == r - 1:
            # Last round: always defect
            action = D

        total_contributions += ... (update total contributions)
    return actions
```

**Rationale**
The AE strategy balances adaptability and exploitativeness by:

* Monitoring the group's overall contribution level to adjust cooperation/defection probabilities.
* Adapting to opponents' behavior, exploiting those who cooperate excessively while maintaining a balanced approach against others.
* Maximizing individual payoff in the endgame.

This strategy is robust against various opponent behaviors and should perform well in a tournament setting.
'''

description_EXPLOITATIVE_64 = '''
**Exploitative Strategy: "Adaptive Defection with Conditional Cooperation" (ADCC)**

**Decision Rules:**

1. **Initial Rounds**: In the first few rounds (e.g., 10% of total rounds), play Cooperate (C) to gather information about opponents' behaviors and estimate their cooperation rates.
2. **Opponent Profiling**: Based on observed actions, categorize each opponent into one of three profiles:
	* **Cooperators** (COOP): Players who cooperate at least 70% of the time.
	* **Defectors** (DEF): Players who defect at least 70% of the time.
	* **Unreliable** (UNREL): Players whose cooperation rate is between 30% and 70%.
3. **Conditional Cooperation**: For each opponent profile, use the following rules:
	* COOP: Cooperate with a probability proportional to the number of cooperators in the previous round (more cooperators → higher cooperation probability).
	* DEF: Defect against defectors.
	* UNREL: Cooperate if the expected payoff from cooperation is higher than defection, based on the opponent's estimated cooperation rate and the game parameters (k and n).
4. **Adaptive Defection**: If an opponent defects while you cooperated in a previous round, defect against them for a few rounds (e.g., 3-5 rounds) to punish their defection.
5. **Endgame Strategy**: In the last few rounds (e.g., 10% of total rounds), switch to a more exploitative strategy:
	* If most opponents are COOP, defect to maximize individual payoff.
	* If most opponents are DEF or UNREL, cooperate if the expected payoff from cooperation is higher than defection.

**Edge Cases:**

1. **First Round**: Cooperate (C) to gather information about opponents' behaviors.
2. **Last Round**: Follow the Endgame Strategy rules.
3. **Tie-Breaking**: In case of a tie in the opponent profiling step, assume the opponent is UNREL.

**Pseudocode:**
```python
def ADCC(n, k, r):
  # Initialize variables and data structures
  opponents = {i: 'UNREL' for i in range(1, n+1)}
  cooperation_rates = {i: 0.5 for i in range(1, n+1)}
  history = []

  for t in range(r):
    # Initial rounds: Cooperate to gather information
    if t < 0.1 * r:
      action = 'C'
    else:
      # Opponent profiling and conditional cooperation
      for opponent in opponents:
        coop_rate = cooperation_rates[opponent]
        if coop_rate >= 0.7:
          opponents[opponent] = 'COOP'
        elif coop_rate <= 0.3:
          opponents[opponent] = 'DEF'

      action = determine_action(opponents, history, k, n)

    # Update opponent profiles and cooperation rates
    update_opponents(opponents, history)
    update_cooperation_rates(cooperation_rates, history)

    # Store the current round's actions and payoffs
    history.append((action, get_payoff(action, opponents, k, n)))

  return history

def determine_action(opponents, history, k, n):
  # Implement the Conditional Cooperation rules
  for opponent in opponents:
    if opponents[opponent] == 'COOP':
      coop_prob = (1 + len([opp for opp in opponents if opponents[opp] == 'COOP'])) / n
      return 'C' with probability coop_prob
    elif opponents[opponent] == 'DEF':
      return 'D'
    else:
      # UNREL: Cooperate if expected payoff from cooperation is higher than defection
      expected_coop_payoff = estimate_expected_coop_payoff(opponents, k, n)
      if expected_coop_payoff > get_payoff('D', opponents, k, n):
        return 'C'
      else:
        return 'D'

def update_opponents(opponents, history):
  # Update opponent profiles based on their actions
  for opponent in opponents:
    coop_count = sum(1 for (action, _) in history if action == 'C' and _['opponent'] == opponent)
    cooperation_rates[opponent] = coop_count / len(history)

def estimate_expected_coop_payoff(opponents, k, n):
  # Estimate the expected payoff from cooperation based on opponent profiles
  expected_coop_payoff = 0
  for opponent in opponents:
    if opponents[opponent] == 'COOP':
      expected_coop_payoff += (k / n) * len([opp for opp in opponents if opponents[opp] == 'COOP'])
  return expected_coop_payoff

def get_payoff(action, opponents, k, n):
  # Calculate the payoff based on the game parameters and opponent actions
  payoff = 0
  if action == 'C':
    payoff += (k / n) * len([opp for opp in opponents if opponents[opp] == 'COOP'])
  else:
    payoff += 1 - (k / n) * len([opp for opp in opponents if opponents[opp] == 'COOP'])
  return payoff
```
Note that this pseudocode is a simplified representation of the strategy, and you may need to modify it to fit your specific implementation requirements.
'''

description_EXPLOITATIVE_65 = '''
**Exploitative Strategy: "Adaptive Wolfpack"**

The Adaptive Wolfpack strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's progression. It balances cooperation and defection to maximize payoffs.

**Decision Rules:**

1. **Initial Rounds (t = 1, 2):**
	* Cooperate (C) in the first round to test opponents' willingness to cooperate.
	* In the second round, defect (D) if at least one opponent defected in the first round; otherwise, cooperate again.
2. **Main Game Loop (t > 2):**
	* Calculate the average cooperation rate of all players in the previous round: `avg_coop_prev = Σ(c_j,t-1) / n`
	* If `avg_coop_prev` is above a threshold (`coop_threshold = k/n`), cooperate (C); otherwise, defect (D)
3. **Punishment Mechanism:**
	* If an opponent defects after cooperating in the previous round, mark them as "predatory" and defect against them for `punishment_rounds = 2` rounds
4. **Endgame Strategy (t = r or t = r-1):**
	* In the last two rounds, cooperate if the total payoff from cooperation is expected to be higher than defection; otherwise, defect

**Pseudocode:**

```
// Initialize variables
coop_threshold = k/n
punishment_rounds = 2
predatory_opponents = []

// First round
if t == 1:
    action = C

// Second round
elif t == 2:
    if any(opponent_defected in previous_round):
        action = D
    else:
        action = C

// Main game loop
else:
    avg_coop_prev = Σ(c_j,t-1) / n
    if avg_coop_prev >= coop_threshold:
        action = C
    else:
        action = D

    // Punishment mechanism
    for opponent in opponents:
        if opponent_defected and not opponent_predatory:
            predatory_opponents.append(opponent)
            punishment_timer[opponent] = punishment_rounds

    // Update actions based on punishment mechanism
    for opponent in predatory_opponents:
        if punishment_timer[opponent] > 0:
            action = D
            punishment_timer[opponent] -= 1

// Endgame strategy
if t >= r-1:
    expected_coop_payoff = (k/n) * Σ(c_j,t-1)
    expected_defect_payoff = 1 + (k/n) * Σ(c_j,t-1)
    if expected_coop_payoff > expected_defect_payoff:
        action = C
    else:
        action = D

return action
```

**Rationale:**

The Adaptive Wolfpack strategy is designed to:

1. **Test opponents' cooperation**: Initial rounds help identify cooperative and predatory players.
2. **Adapt to cooperation rates**: The main game loop adjusts behavior based on the average cooperation rate, balancing individual payoffs with collective benefits.
3. **Punish defectors**: Marking and punishing predatory opponents discourages exploitation.
4. **Optimize endgame strategy**: Cooperate or defect in the last rounds based on expected payoffs.

By adapting to the game's progression and opponent behaviors, the Adaptive Wolfpack strategy aims to maximize individual payoffs while being robust to various strategies and coordination mechanisms.
'''

description_EXPLOITATIVE_66 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. It leverages game parameters and history to make informed decisions.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) in the first round to gather information about opponents' behaviors.
	* In rounds 2-3, play Tit-for-Tat (TFT): cooperate if most opponents cooperated in the previous round, and defect otherwise.
2. **Adaptive Phase**:
	* Calculate the **Cooperation Ratio (CR)**: the proportion of cooperators in the previous round.
	* If CR > 0.5, cooperate (C). Otherwise, defect (D).
	* If the opponent's cooperation ratio is consistently high (> 0.7), switch to a more exploitative mode:
		+ **Predatory Mode**: Defect (D) for 2-3 rounds to test opponents' reactions.
		+ If opponents retaliate or show signs of cooperation, revert to Adaptive Phase.
3. **Endgame Strategy**:
	* In the last round (r), defect (D) if the opponent's total payoff is significantly higher (> 10%) than yours.
	* Otherwise, cooperate (C) to maximize shared payoffs.

**Pseudocode:**

```
Initialize variables:
cr_history = []  // cooperation ratio history
opponent_cr = []  // opponent's cooperation ratio history

For each round t:
1. If t <= 3:  // Initial Exploration
   a_t = TFT(cr_history)
2. Else:  // Adaptive Phase
   cr_t-1 = calculate_cooperation_ratio(t-1)
   if cr_t-1 > 0.5:
      a_t = C
   else:
      a_t = D
   if opponent_cr[t-1] > 0.7:  // Predatory Mode
      a_t = D for 2-3 rounds
3. If t == r:  // Endgame Strategy
   if opponent_payoff_total > (your_payoff_total * 1.1):
      a_t = D
   else:
      a_t = C

Update cr_history and opponent_cr
```

**Rationale:**

The Adaptive Predator strategy is designed to balance exploration, adaptation, and exploitation.

* Initial Exploration allows the strategy to gather information about opponents' behaviors.
* The Adaptive Phase uses the Cooperation Ratio to make informed decisions, switching between cooperation and defection based on opponents' reactions.
* Predatory Mode takes advantage of overly cooperative opponents by temporarily defecting to maximize payoffs.
* Endgame Strategy ensures that the strategy maximizes shared payoffs in the final round unless an opponent has a significantly higher total payoff.

This exploitative strategy is robust, adaptive, and designed to perform well against a wide range of independent strategies.
'''

description_EXPLOITATIVE_67 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its payoff by adapting to the opponents' behavior while exploiting their cooperation.

**Decision Rules:**

1. **Initial Round:** Defect (D) in the first round to gather information about opponents' initial actions.
2. **Subsequent Rounds:** Cooperate (C) if the total number of cooperators in the previous round is greater than or equal to `n/2`, where `n` is the number of players. Otherwise, defect (D).
3. **Adaptive Threshold:** Adjust the cooperation threshold based on the history of opponents' actions. If the average payoff for cooperating is higher than the average payoff for defecting in the last `r/2` rounds, decrease the cooperation threshold by 1. Conversely, if the average payoff for cooperating is lower, increase the threshold by 1.
4. **Opponent Exploitation:** Identify opponents who have cooperated consistently (more than 75% of the time) and defect against them in the next round to exploit their cooperation.

**Edge Cases:**

* **Last Round:** Defect (D) in the last round, as there is no future payoff to consider.
* **Tied Cooperation Threshold:** If the total number of cooperators equals the cooperation threshold, cooperate (C).

**Pseudocode:**
```markdown
# Initialize variables
n = number_of_players
r = number_of_rounds
k = multiplication_factor
cooperation_threshold = n / 2
opponent_cooperation_history = []

# First round
if current_round == 1:
    action = D

# Subsequent rounds
else:
    total_cooperators_previous_round = count_cooperators(opponent_actions_previous_round)
    if total_cooperators_previous_round >= cooperation_threshold:
        action = C
    else:
        action = D
    
    # Adaptive threshold adjustment
    if average_payoff_cooperating > average_payoff_defecting:
        cooperation_threshold -= 1
    elif average_payoff_cooperating < average_payoff_defecting:
        cooperation_threshold += 1

    # Opponent exploitation
    for opponent in opponents:
        if opponent_cooperation_rate(opponent) > 0.75:
            action = D  # Exploit consistent cooperators

# Last round
if current_round == r:
    action = D
```
This strategy is designed to be exploitative, adaptive, and robust against various opponent behaviors. It gathers information about opponents' initial actions, adjusts its cooperation threshold based on the history of opponents' actions, and exploits consistent cooperators to maximize its payoff.
'''

description_EXPLOITATIVE_68 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy prioritizes self-interest and seeks to maximize its own payoff.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) with probability 0.5 in the first round to gather information about opponents' tendencies.
	* In rounds 2-3, mirror the opponent's previous action (i.e., if an opponent cooperated last round, cooperate this round; if they defected, defect).
2. **Exploitation Phase**:
	* Observe the total number of cooperators in the previous round (`total_cooperators_prev`).
	* If `total_cooperators_prev` > n/2 (i.e., more than half of players cooperated), Defect (D) to exploit the public good.
	* Otherwise, Cooperate (C) to contribute to the public good and potentially encourage others to cooperate.
3. **Punishment Mechanism**:
	* If an opponent defected in the previous round while you cooperated, defect in this round to "punish" them.
4. **Adaptation**:
	* Monitor the average payoff of all players over the last 2 rounds (`avg_payoff_last_2_rounds`).
	* If `avg_payoff_last_2_rounds` > k/2 (i.e., players are generally doing well), increase cooperation probability by 10% to encourage continued cooperation.
	* Otherwise, decrease cooperation probability by 10% to avoid exploitation.

**Edge Cases:**

1. **Last Round**: Defect (D) to maximize personal payoff, as there is no future opportunity for reciprocity or punishment.
2. **Tiebreaker**: In case of a tie in the number of cooperators, cooperate if `avg_payoff_last_2_rounds` > k/2; otherwise, defect.

**Pseudocode:**
```markdown
# Initialize variables
cooperation_probability = 0.5
total_cooperators_prev = 0
avg_payoff_last_2_rounds = 0

# Loop through rounds
for round in range(1, r + 1):
    if round <= 3:
        # Initial exploration phase
        action = random.choice([C, D]) if round == 1 else mirror_opponent_prev_action()
    elif total_cooperators_prev > n / 2:
        # Exploitation phase: defect if many cooperated last round
        action = D
    else:
        # Exploitation phase: cooperate otherwise
        action = C

    # Punishment mechanism
    if opponent_defected_last_round and self_cooperated_last_round:
        action = D

    # Adaptation
    avg_payoff_last_2_rounds = calculate_avg_payoff()
    if avg_payoff_last_2_rounds > k / 2:
        cooperation_probability += 0.1
    else:
        cooperation_probability -= 0.1

    # Edge case: last round
    if round == r:
        action = D

    # Tiebreaker
    if total_cooperators_prev == n / 2 and avg_payoff_last_2_rounds > k / 2:
        action = C

    take_action(action)
```
The Adaptive Predator strategy balances exploration, exploitation, and adaptation to maximize its own payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_69 = '''
Here's a proposed exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **First Round:** Cooperate (C) to observe opponents' initial behavior and gather information.
2. **Rounds 2-r:** Observe the total number of cooperators in the previous round, denoted as `prev_cooperators`.
3. **Exploitation Threshold:** Set a threshold value `θ` based on the game parameters: `θ = (k / n) * (n - 1)`. This represents the minimum expected payoff from cooperating when all other players defect.
4. **Cooperation Condition:** If `prev_cooperators >= θ`, cooperate (C). Otherwise, defect (D).
5. **Adaptive Adjustment:** After each round, update the threshold value `θ` based on the observed behavior:
	* If most players (>= n/2) cooperated in the previous round, decrease `θ` by a small amount (`ε = 0.1`) to encourage cooperation: `θ -= ε`.
	* If most players defected (<= n/2), increase `θ` by a small amount (`ε = 0.1`) to discourage cooperation: `θ += ε`.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    prev_cooperators = 0
    θ = (k / n) * (n - 1)
    ε = 0.1

    for t in range(r):
        if t == 0:  # First round
            action = C
        else:
            if prev_cooperators >= θ:
                action = C
            else:
                action = D

        # Observe opponents' actions and update prev_cooperators
        ... ( implementation-dependent )

        # Update θ based on observed behavior
        if prev_cooperators >= n / 2:
            θ -= ε
        else:
            θ += ε

        return action
```
**Edge Cases:**

* **Last Round:** The strategy behaves as usual, without considering the last round's payoff.
* **Ties:** In case of ties (e.g., `prev_cooperators == θ`), cooperate (C) to maintain a consistent behavior.

This Adaptive Exploiter strategy aims to:

1. Gather information in the first round by cooperating.
2. Identify opportunities for exploitation based on opponents' previous behavior.
3. Adjust its threshold value (`θ`) to adapt to changing opponent behaviors, encouraging cooperation when possible and discouraging it when opponents are not cooperative enough.
4. Defect when opponents are not contributing sufficiently to the public good.

By being adaptive and responsive to opponents' actions, this strategy aims to maximize its payoff while exploiting non-cooperative behavior in others.
'''

description_EXPLOITATIVE_70 = '''
I'll design a exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history. I'll outline the decision rules, handle edge cases, and ensure the strategy is robust to various opponent behaviors.

**Strategy Name: "Adaptive Exploiter"**

**Decision Rules:**

1. **Initial Round (Round 1):**
 Cooperate (C) in the first round to gather information about the opponents' behavior.
2. **Early Rounds (Rounds 2-5):**
 Observe the total number of cooperators in the previous round (`total_cooperators_prev`). If:
	* `total_cooperators_prev` > n/2, Defect (D) in the current round.
	* `total_cooperators_prev` ≤ n/2, Cooperate (C) in the current round.
3. **Mid-Game Rounds (Rounds 6-r):**
 Calculate the average payoff of cooperators (`avg_payoff_coop`) and defectors (`avg_payoff_defect`) over the last 3 rounds. If:
	* `avg_payoff_coop` > `avg_payoff_defect`, Cooperate (C) in the current round.
	* `avg_payoff_coop` ≤ `avg_payoff_defect`, Defect (D) in the current round.
4. **Endgame Rounds (Last 2 rounds):**
 Defect (D) in the last two rounds, as the potential gain from cooperating is limited.

**Additional Rules:**

1. **Monitor Opponent Behavior:** Track the number of times each opponent has cooperated or defected in the previous rounds.
2. **Punish Frequent Defectors:** If an opponent has defected more than 75% of the time, always defect against them.
3. **Reward Frequent Cooperators:** If an opponent has cooperated more than 75% of the time, cooperate with them if `avg_payoff_coop` > `avg_payoff_defect`.

**Pseudocode:**
```
Function AdaptiveExploiter(n, k, r):
  // Initialize variables
  total_cooperators_prev = 0
  avg_payoff_coop = 0
  avg_payoff_defect = 0
  opponent_behavior = [] // track opponent behavior

  For round = 1 to r:
    If round == 1: // Initial Round
      action = C
    Else If round <= 5: // Early Rounds
      If total_cooperators_prev > n/2:
        action = D
      Else:
        action = C
    Else: // Mid-Game Rounds
      avg_payoff_coop, avg_payoff_defect = calculate_average_payoffs()
      If avg_payoff_coop > avg_payoff_defect:
        action = C
      Else:
        action = D

    // Monitor Opponent Behavior and Punish/Reward accordingly
    opponent_behavior.update(opponent_actions)
    If opponent_behavior.frequent_defector():
      action = D
    ElseIf opponent_behavior.frequent_cooperator() and avg_payoff_coop > avg_payoff_defect:
      action = C

    // Endgame Rounds
    If round >= r-1:
      action = D

    Return action
```
This strategy, "Adaptive Exploiter," is designed to be exploitative while adapting to the game parameters and history. It gathers information about opponents' behavior in early rounds and adjusts its decision-making accordingly. The strategy punishes frequent defectors and rewards frequent cooperators, ensuring a robust performance against various opponent behaviors.
'''

description_EXPLOITATIVE_71 = '''
To design an exploitative strategy for this N-Player Public Goods Game that is adaptive and robust against a wide range of opponent behaviors, we'll focus on leveraging information from past rounds to inform our decisions in subsequent rounds. This approach ensures we maximize our payoff by adapting to the observed behavior of other players.

### Strategy Name: Adaptive Exploiter (AE)

#### Decision Rules:

1. **First Round**: In the absence of prior information, start with a defection (D) to set a baseline for observing others' behaviors without immediately committing resources to the public good.
   
   ```python
if round == 1:
    action = 'D'
```

2. **Early Rounds (Rounds 2-5)**: Observe the overall cooperation rate among other players in the first few rounds. If more than half of the players cooperated in the previous round, cooperate (C) in the current round to test their commitment and potentially increase the public good payoff. Otherwise, defect (D).

   ```python
elif 2 <= round <= 5:
    if sum_cooperators_previous_round / (n - 1) > 0.5: # Adjust for n players, excluding self
        action = 'C'
    else:
        action = 'D'
```

3. **Mid-Game Adaptation (After Round 5)**: Analyze the average cooperation rate of each player over the past rounds. Identify the top cooperators and adjust your strategy accordingly:
   - If you've identified at least one consistent cooperator, cooperate in this round to maximize the public good payoff.
   - Otherwise, defect.

   ```python
else:
    # Calculate average cooperation rate for each player
    avg_cooperation_rates = calculate_avg_cooperation_rates(players)
    
    if max(avg_cooperation_rates) > 0.5: # Identify consistent cooperators
        action = 'C'
    else:
        action = 'D'
```

4. **Last Round**: In the final round, defect regardless of previous observations. This ensures you do not contribute to the public good when it will not benefit you in future rounds.

   ```python
if round == r:
    action = 'D'
```

#### Handling Edge Cases:

- **Tie in Cooperation Rates**: If there's a tie in identifying consistent cooperators, default to defection to avoid unnecessary contribution.
  
  ```python
# Within the mid-game adaptation logic
elif max(avg_cooperation_rates) == min(avg_cooperation_rates): # Tie scenario
    action = 'D'
```

#### Exploitative Mindset:

The Adaptive Exploiter (AE) strategy is designed with an exploitative mindset by:
- Initially defecting to test waters and potentially exploit unconditional cooperators.
- Adapting behavior based on observed cooperation rates, aiming to maximize personal payoff while minimizing contribution when possible.
- Capitalizing on the consistent cooperation of others when identified.

This approach ensures robustness against various opponent behaviors by not relying on assumptions about their strategies. Instead, it leverages information from past interactions to inform its decisions, maximizing payoffs in a dynamic and potentially exploitative manner.
'''

description_EXPLOITATIVE_72 = '''
**Exploitative Strategy: "Adaptive Cooperator"**

Decision Rules:

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 0.5. This allows us to gather information about our opponents' strategies while also contributing to the public good.
2. **Observation Phase**: For rounds 2-5 (or until we have observed at least 3 cooperators and 3 defectors), play D (defect) and observe the actions of other players. Calculate the frequency of cooperation among all players, `cooperation_rate = total_cooperators / n`.
3. **Adaptive Cooperation**: After the observation phase, switch to an adaptive strategy based on the observed cooperation rate:
	* If `cooperation_rate > 0.5`, play C (cooperate) with probability `p_C = cooperation_rate`. This means we'll cooperate more often when others are cooperating.
	* If `cooperation_rate <= 0.5`, play D (defect) with probability `1 - p_C`. When others are not cooperating, we'll defect to maximize our payoff.

Pseudocode:
```python
def adaptive_cooperator(n, k, r):
    cooperation_rate = 0
    total_cooperators = 0

    # Initial Cooperation
    if round == 1:
        play C with probability 0.5

    # Observation Phase (rounds 2-5)
    elif 2 <= round <= 5:
        play D
        observe opponents' actions
        update cooperation_rate and total_cooperators

    # Adaptive Cooperation
    else:
        if cooperation_rate > 0.5:
            p_C = cooperation_rate
            play C with probability p_C
        else:
            play D with probability 1 - p_C
```
Handling Edge Cases:

* **Last Round**: In the last round, we'll defect (play D) to maximize our payoff, as there's no future benefit from cooperating.
* **Opponent Cooperation**: If an opponent cooperates while we're playing D, we won't punish them immediately. Instead, we'll reassess our strategy in the next round based on the updated cooperation rate.

Exploitative Mindset:

Our strategy aims to exploit the cooperation of others while minimizing the risk of being exploited ourselves. By initially cooperating with a probability of 0.5 and then adapting our behavior based on observed cooperation rates, we can maximize our payoff while also influencing others to cooperate more often.

This strategy is designed to be robust against various opponent behaviors, including:

* **Free Riders**: Our adaptive strategy will detect free riders (players who always defect) and adjust accordingly.
* **Cooperative Strategies**: We'll take advantage of cooperative strategies by cooperating when they do and defecting when they don't.
* **Mixed Strategies**: By observing the cooperation rate, we can adapt to mixed strategies that combine cooperation and defection.

This exploitative strategy will perform well in a tournament against independent AI systems with diverse behaviors.
'''

description_EXPLOITATIVE_73 = '''
**Exploitative Strategy: Adaptive Cooperation with Retaliation (ACR)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to gather information about opponents' behaviors and potential cooperation levels.
2. **Subsequent Rounds:** Observe the total number of cooperators (T) in the previous round and calculate the average payoff (AP) for cooperators and defectors separately.

   a. If AP_cooperators > AP_defectors, cooperate (C) if T is above a certain threshold (e.g., n/2), and defect (D) otherwise.

   b. If AP_cooperators ≤ AP_defectors, defect (D).

3. **Retaliation Mechanism:** Introduce a "retaliation factor" (RF) to account for the opponents' recent behavior:

   a. RF = 1 if at least one opponent defected in the previous round.
   
   b. RF = 0 otherwise.

4. **Adaptive Cooperation:** When RF = 1, defect (D) with probability p, where p is inversely proportional to the number of cooperators in the previous round (e.g., p = 1 - T/n). Otherwise, follow decision rule 2a.
5. **Last Round:** Defect (D), as there are no future rounds to influence.

**Edge Cases:**

* If all opponents defected in the previous round, cooperate (C) with probability 0.5 to test for potential cooperation.
* If only one opponent cooperated in the previous round, defect (D) to maximize payoff.

**Pseudocode:**
```python
def ACR(n, k, r, history):
    if round == 1:
        return C
    
    T = sum(history[-1])  # total number of cooperators in previous round
    AP_cooperators = calculate_average_payoff(history, C)
    AP_defectors = calculate_average_payoff(history, D)
    
    RF = 1 if any(opponent_defected(history[-1])) else 0
    
    if AP_cooperators > AP_defectors:
        if T >= n/2 and RF == 0:
            return C
        else:
            return D
    elif RF == 1:
        p = 1 - T/n
        return D with probability p, otherwise return C
    else:
        return D

def calculate_average_payoff(history, action):
    # Calculate average payoff for given action over all rounds
    
def opponent_defected(actions):
    # Check if any opponent defected in the previous round
```
This strategy is designed to be exploitative by:

1. Cooperating initially to gather information.
2. Adapting to opponents' behavior based on their recent actions.
3. Retaliating against defectors to discourage exploitation.
4. Maximizing payoffs when possible.

ACR should perform well in a tournament setting, as it is robust and adaptive to various opponent behaviors.
'''

description_EXPLOITATIVE_74 = '''
I'll design a exploitative strategy that adapts to various opponent behaviors while maximizing my own payoff.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Exploration (Round 1-3):**
	* Play C (Cooperate) in the first round to gather information about opponents' behavior.
	* In rounds 2 and 3, play D (Defect) to observe how opponents react to exploitation.
2. **Monitoring Opponents' Behavior:**
	* Keep track of each opponent's cooperation rate (CR_i) over the last w rounds (window size, e.g., w = 5).
	* Calculate the average cooperation rate (ACR) across all opponents.
3. **Exploitation Phase:**
	* If ACR > 0.5 ( majority cooperates):
		+ Play D (Defect) to exploit cooperative opponents and maximize personal payoff.
	* If ACR ≤ 0.5 ( majority defects or mixed behavior):
		+ Play C (Cooperate) with probability p = ACR, otherwise play D (Defect).
4. **Adaptive Adjustment:**
	* If the opponent's CR_i increases significantly (> 20%) over the last w rounds:
		+ Adjust my strategy to play C (Cooperate) more frequently (p += 0.1) against this opponent.
	* If the opponent's CR_i decreases significantly (< -20%) over the last w rounds:
		+ Adjust my strategy to play D (Defect) more frequently (p -= 0.1) against this opponent.

**Handling Edge Cases:**

* **Last Round:** Play D (Defect) to maximize personal payoff, as there's no future interaction.
* **Opponent with High Cooperation Rate:** If an opponent consistently cooperates (> 80% CR_i), play C (Cooperate) with probability p = 0.5 to avoid being too exploitative and maintain a stable cooperation rate.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    w = 5  # window size
    CR = [0] * n  # opponent cooperation rates
    ACR = 0  # average cooperation rate

    for t in range(r):
        if t < 3:  # initial exploration phase
            action = 'C' if t == 0 else 'D'
        else:
            # update opponent cooperation rates and ACR
            for i in range(n):
                CR[i] = (CR[i] * (w - 1) + opponents[i].get_cooperation_rate()) / w
            ACR = sum(CR) / n

            if ACR > 0.5:
                action = 'D'  # exploit cooperative opponents
            else:
                p = ACR
                action = 'C' if random.random() < p else 'D'

        # adaptive adjustment
        for i in range(n):
            if CR[i] - opponents[i].get_cooperation_rate(w) > 0.2:
                p += 0.1
            elif CR[i] - opponents[i].get_cooperation_rate(w) < -0.2:
                p -= 0.1

        # last round adjustment
        if t == r - 1:
            action = 'D'

        return action
```
This strategy is designed to be exploitative, adaptive, and robust against various opponent behaviors. By monitoring opponents' cooperation rates and adjusting my own behavior accordingly, I can maximize my payoff while minimizing the risk of being exploited myself.
'''

description_EXPLOITATIVE_75 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit opponents' behaviors while adapting to changing game conditions. It uses a combination of history-based decision-making and adaptive thresholding to maximize payoffs.

**Decision Rules:**

1. **Initial Round (t=1):** Defect (D) in the first round to gather information about opponents' initial strategies.
2. **Subsequent Rounds (t>1):**
	* Calculate the average cooperation rate of all players over the previous rounds, `avg_coop_rate = Σ(c_j) / (n * (t-1))`.
	* Set an adaptive threshold, `theta`, based on the game parameters and history: `theta = k/n + (1 - avg_coop_rate) / 2`.
	* If the number of cooperators in the previous round is above the threshold (`Σ(c_j) > theta * n`), cooperate (C). Otherwise, defect (D).
3. **Last Round (t=r):** Cooperate (C) if `avg_coop_rate >= 0.5`, otherwise defect (D).

**Pseudocode:**
```python
def adaptive_predator(n, k, r, history):
    # Initialize variables
    avg_coop_rate = 0
    theta = 0
    
    for t in range(1, r+1):
        if t == 1:
            # Initial round: Defect
            action = 'D'
        else:
            # Calculate average cooperation rate
            avg_coop_rate = sum(history[t-2]) / (n * (t-1))
            
            # Set adaptive threshold
            theta = k/n + (1 - avg_coop_rate) / 2
            
            # Decide action based on threshold
            if sum(history[t-2]) > theta * n:
                action = 'C'
            else:
                action = 'D'
        
        # Last round special case
        if t == r and avg_coop_rate >= 0.5:
            action = 'C'
    
    return action
```
**Rationale:**

1. The initial defection in Round 1 allows the strategy to gather information about opponents' initial strategies.
2. The adaptive threshold, `theta`, balances the desire to cooperate (and receive a share of the public good) with the need to defect when others are not cooperating.
3. By using the average cooperation rate over previous rounds, the strategy adapts to changing game conditions and exploits opponents who consistently cooperate or defect.
4. In the last round, the strategy cooperates if the average cooperation rate is above 0.5, ensuring a higher payoff in case of mutual cooperation.

This Adaptive Predator strategy is designed to be robust against various opponent behaviors and exploit their weaknesses while adapting to changing game conditions.
'''

description_EXPLOITATIVE_76 = '''
To develop a robust and exploitative strategy for this N-Player Public Goods Game, I propose a hybrid approach that combines elements of Tit-for-Tat (TFT) with a Pavlov-like adaptation mechanism to handle the multi-player setting.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C). This sets a baseline for potential cooperation and encourages others to follow suit.
2. **Reaction to Defection**: If at least one player defects in the previous round, defect (play D) in the current round. This punishes defectors and maintains a strong stance against exploitation.
3. **Pavlov-like Adaptation**: Monitor the average payoff of all players over the last few rounds (e.g., 3-5 rounds). If the average payoff is above a certain threshold (e.g., 1.5), switch to cooperation (play C) in the current round. This adaptation mechanism encourages cooperation when the group's overall performance is good.
4. **Exploitation**: If more than half of the players cooperated in the previous round, and the average payoff is below a certain threshold (e.g., 1.2), defect (play D) in the current round. This exploits the cooperative behavior of others when the group's overall performance is poor.

**Edge Cases:**

* **Last Round**: In the final round, always defect (play D). Since there are no future rounds to consider, exploiting any remaining cooperation from others becomes optimal.
* **Tie-Breaking**: In cases where multiple strategies suggest different actions, prioritize defection over cooperation. This ensures a more cautious approach and avoids potential losses.

**Pseudocode:**
```python
def Adaptive_Exploiter(history):
    # Initialize variables
    avg_payoff_threshold = 1.5
    exploit_threshold = 1.2
    recent_rounds = 3

    # First round cooperation
    if history == []:
        return 'C'

    # Reaction to defection
    if any(player_action == 'D' for player_action in history[-1]):
        return 'D'

    # Pavlov-like adaptation
    avg_payoff = sum([player_payoff for player_payoff in history[-recent_rounds:]]) / recent_rounds
    if avg_payoff > avg_payoff_threshold:
        return 'C'

    # Exploitation
    num_cooperators = sum([1 for player_action in history[-1] if player_action == 'C'])
    if num_cooperators > len(history[-1]) // 2 and avg_payoff < exploit_threshold:
        return 'D'

    # Default to defection
    return 'D'
```
This strategy, Adaptive Exploiter (AE), balances cooperation with a strong exploitative component. By adapting to the group's overall performance and punishing defectors, AE promotes cooperation while minimizing losses from potential exploitation.
'''

description_EXPLOITATIVE_77 = '''
**Exploitative Strategy: "Adaptive Greed"**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `min(0.5, k/n)` to encourage initial cooperation and gather information about opponents' behavior.
2. **Payoff-Based Adaptation**: After the first round, adapt your strategy based on the previous round's payoffs:
	* If your payoff was higher than the average payoff of all players (`π_i > (1/n) * Σ(j=1 to n) π_j`), continue with the same action (C or D).
	* If your payoff was lower than the average payoff, switch actions (from C to D or from D to C).
3. **Greed Threshold**: Introduce a "greed threshold" (`θ`) that depends on the game parameters: `θ = 1 - (k/n)`. If the total number of cooperators in the previous round is below this threshold (`Σ(j=1 to n) c_j < θ * n`), defect (D). Otherwise, cooperate (C).
4. **Last Round Adjustment**: In the last round, if your current payoff is higher than the average payoff and you are cooperating, continue to cooperate. Otherwise, defect.

**Edge Cases:**

* **First Round**: Cooperate with probability `min(0.5, k/n)`.
* **Last Round**: Adjust strategy according to the rules above.
* **Ties in Payoff Comparison**: In case of a tie (`π_i = (1/n) * Σ(j=1 to n) π_j`), maintain the same action as in the previous round.

**Pseudocode:**
```python
def adaptive_greed(n, k, r, history):
    # Initialize variables
    θ = 1 - (k / n)
    coop_prob_first_round = min(0.5, k/n)

    for t in range(r):
        if t == 0:
            # First round: cooperate with probability coop_prob_first_round
            action = 'C' if random.random() < coop_prob_first_round else 'D'
        else:
            # Adapt strategy based on previous round's payoffs and greed threshold
            prev_payoff_i = history[t-1]['payoffs'][i]
            avg_payoff = sum(history[t-1]['payoffs']) / n

            if prev_payoff_i > avg_payoff:
                action = history[t-1]['actions'][i]  # Continue with same action
            else:
                action = 'D' if history[t-1]['actions'][i] == 'C' else 'C'

            total_cooperators = sum([1 for a in history[t-1]['actions'] if a == 'C'])

            if total_cooperators < θ * n:
                action = 'D'

        # Last round adjustment
        if t == r - 1 and prev_payoff_i > avg_payoff and action == 'C':
            action = 'C'
        elif t == r - 1:
            action = 'D'

        return action
```
This strategy, "Adaptive Greed", combines elements of payoff-based adaptation, greed threshold, and last-round adjustments to create an exploitative approach that can effectively respond to a wide range of opponent behaviors. By adapting to the game's dynamics and exploiting opportunities for high payoffs, this strategy aims to achieve a competitive edge in the tournament.
'''

description_EXPLOITATIVE_78 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Overview:** AE is a history-dependent, adaptive strategy that aims to exploit opponents' behaviors while minimizing losses. It balances cooperation and defection based on observed patterns in previous rounds.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate (C) in the first round (t=1) to gather information about opponents' initial strategies.
2. **Round t > 1:**
	* If the total number of cooperators in the previous round (t-1) is greater than or equal to n/2, cooperate (C) in round t. This indicates a potential willingness to contribute to the public good.
	* Otherwise, defect (D) in round t.
3. **Exploitation**: If an opponent's cooperation rate (CR) exceeds 0.7 (i.e., they cooperate more than 70% of the time), AE will consistently defect (D) against that opponent from then on.
4. **Adaptation**: Every 5 rounds, re-evaluate opponents' CRs and adjust the strategy accordingly:
	* If an opponent's CR has decreased below 0.7, resume cooperating with them.
	* If an opponent's CR remains above 0.7, continue defecting against them.

**Handling Edge Cases:**

1. **Last Round (t=r)**: Cooperate (C) in the last round to maximize total payoff, as there is no future exploitation potential.
2. **Ties**: In case of a tie (e.g., equal number of cooperators and defectors), AE will cooperate (C) to favor cooperation.

**Pseudocode (simplified)**:
```
// Initialize variables
n = number_of_players
k = multiplication_factor
r = number_of_rounds
CR = array of opponent cooperation rates

// First round: Cooperate
if t == 1 then
    action[t] = C
end if

// Subsequent rounds
for t > 1:
    // Check previous round's cooperators
    if sum(cooperators[t-1]) >= n/2 then
        action[t] = C
    else
        action[t] = D
    end if
    
    // Exploitation check
    for opponent in opponents:
        if CR[opponent] > 0.7 then
            action[t] = D (against that opponent)
        end if
    end for
    
    // Adaptation every 5 rounds
    if t % 5 == 0 then
        re-evaluate CRs and adjust strategy
    end if

// Last round: Cooperate
if t == r then
    action[t] = C
end if
```
AE's exploitative mindset is reflected in its willingness to defect against opponents with high cooperation rates, while adapting to changes in their strategies. By balancing cooperation and defection, AE aims to maximize its total payoff over the course of the game.
'''

description_EXPLOITATIVE_79 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

**Decision Rules:**

1. **First Round:** Cooperate (C) to gather information about opponents' behaviors and to potentially set a cooperative tone.
2. **Early Rounds (t < r/2):**
	* If the average cooperation rate of all players in previous rounds is above 0.5, cooperate (C). This indicates a relatively cooperative environment, and contributing to the public good may yield higher payoffs.
	* Otherwise, defect (D) to exploit potential cooperators and maximize individual payoff.
3. **Mid-Game (t ≥ r/2):**
	* If the average cooperation rate of all players in previous rounds has increased, cooperate (C). This suggests that opponents are adapting towards cooperation, and contributing to the public good may become more beneficial.
	* If the average cooperation rate has decreased or remained stable, defect (D) to continue exploiting potential cooperators.
4. **Late Rounds (t > 3r/4):**
	* If the total payoff difference between cooperation and defection in previous rounds is significant (> 1), cooperate (C) if you have been cooperating more often, or defect (D) if you have been defecting more often. This aims to maximize individual payoff based on past behavior.
	* Otherwise, defect (D) as the game nears its end, and the marginal benefits of cooperation decrease.

**Handling Edge Cases:**

* **Last Round:** Defect (D), as there is no future benefit to cooperation.
* **Only One Opponent:** Cooperate (C) in the first round, then mirror their action (if they cooperated, cooperate; if they defected, defect).
* **Many Opponents with Similar Strategies:** Use a mixed strategy: randomly choose between cooperating and defecting with probabilities based on the average cooperation rate of opponents.

**Pseudocode:**
```python
def adaptive_exploiter(n, r, k, history):
    # Initialize variables
    avg_coop_rate = 0.5  # initial estimate
    my_coop_count = 0

    for t in range(r):
        if t == 0:
            action = 'C'  # cooperate in the first round
        elif t < r/2:
            if avg_coop_rate > 0.5:
                action = 'C'
            else:
                action = 'D'
        elif t >= r/2:
            if avg_coop_rate > prev_avg_coop_rate:
                action = 'C'
            else:
                action = 'D'
        else:  # late rounds
            payoff_diff = total_payoff coop - total_payoff defect
            if payoff_diff > 1:
                if my_coop_count > n/2:
                    action = 'C'
                else:
                    action = 'D'
            else:
                action = 'D'

        # Update variables
        prev_avg_coop_rate = avg_coop_rate
        avg_coop_rate = (avg_coop_rate * (t-1) + coop_count(t)) / t
        my_coop_count += 1 if action == 'C' else 0

    return action
```
This strategy aims to balance exploitation and cooperation, adapting to the behavior of opponents while prioritizing individual payoff maximization.
'''

description_EXPLOITATIVE_80 = '''
**Exploitative Strategy: "Adaptive Parasite"**

Decision Rules:

1. **Initial Cooperation**: In the first round (t=1), cooperate (play C) to gather information about opponents' behavior and potential cooperation levels.
2. **Opponent Analysis**: After each round, update a list of opponent scores based on their past actions:
	* `opponent_scores[i] = sum(opponent_i's cooperations so far)`
3. **Cooperation Threshold**: Calculate the average cooperation rate among all players (excluding yourself) up to the current round (`t`):
	* `avg_cooperation_rate[t] = (sum(opponent_scores) / ((n-1)*t))`
4. **Exploitation Logic**:
	* If `avg_cooperation_rate[t] >= k/n`, cooperate (play C). This indicates a sufficient level of cooperation to justify contributing to the public good.
	* Otherwise, defect (play D).
5. **Punishment Mechanism**: If an opponent has defected (`opponent_scores[i] == 0`) for two consecutive rounds, retaliate by playing D in the next round, regardless of the cooperation threshold.

Edge Cases:

1. **Last Round** (`t=r`): Always defect (play D), as there's no future opportunity to punish or benefit from cooperation.
2. **Early Defection**: If an opponent defects in the first round, immediately switch to defection (play D) for all subsequent rounds.

Pseudocode:
```markdown
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
opponent_scores = [0] * n  # Initialize scores for each opponent

for t in range(1, r+1):
    if t == 1:  # First round
        play C (cooperate)
    else:
        avg_cooperation_rate[t-1] = sum(opponent_scores) / ((n-1)*(t-1))
        
        if avg_cooperation_rate[t-1] >= k/n:
            play C (cooperate)
        else:
            play D (defect)

        # Update opponent scores
        for i in range(n):
            if opponents[i]'s action == C:
                opponent_scores[i] += 1

    # Punishment mechanism
    for i in range(n):
        if opponent_scores[i] == 0 and t > 2:  # Two consecutive defections
            play D (defect) in the next round
```
This strategy aims to exploit opponents' cooperation levels while maintaining a robust defense against potential defectors. By adapting to the average cooperation rate, it encourages others to cooperate while minimizing its own contributions when necessary.
'''

description_EXPLOITATIVE_81 = '''
**Exploitative Strategy: "Adaptive Free Rider"**

The Adaptive Free Rider strategy is designed to exploit a wide range of opponent behaviors while adapting to changing game conditions.

**Decision Rules:**

1. **Initial Round (Round 1):** Defect (D) in the first round to gather information about opponents' initial tendencies.
2. **Subsequent Rounds:** Cooperate (C) if the total number of cooperators in the previous round is above a certain threshold, calculated as follows:

`cooperation_threshold = n * (k - 1) / k`

where `n` is the number of players and `k` is the multiplication factor.

If the cooperation threshold is met or exceeded, cooperate; otherwise, defect.
3. **Adaptive Adjustment:** Update the cooperation threshold after each round based on the observed opponent behavior:

`cooperation_threshold = max(cooperation_threshold - (1/n), 0)`

if at least one opponent cooperated in the previous round.

This adjustment encourages the strategy to gradually lower its expectations of opponents' cooperative behavior, making it more likely to defect when opponents are not cooperating.
4. **Last Round:** Defect (D) in the last round (`r`-th round), as there is no future opportunity for reciprocity or cooperation.

**Pseudocode:**
```
Function AdaptiveFreeRider(n, k, r, history):
  // Initialize variables
  cooperation_threshold = n * (k - 1) / k

  For each round t from 1 to r:
    If t == 1:
      action[t] = D
    Else:
      total_cooperators_prev_round = count of cooperators in history[t-1]
      If total_cooperators_prev_round >= cooperation_threshold:
        action[t] = C
      Else:
        action[t] = D

      // Update cooperation threshold for next round
      If at least one opponent cooperated in previous round:
        cooperation_threshold = max(cooperation_threshold - (1/n), 0)

    // Special case: Last round
    If t == r:
      action[t] = D

  Return action history
```
This strategy is designed to be exploitative, taking advantage of opponents who cooperate excessively while minimizing its own contributions. By adapting the cooperation threshold based on observed opponent behavior, the Adaptive Free Rider strategy can effectively respond to a wide range of opponent behaviors and maximize its payoff in the game.
'''

description_EXPLOITATIVE_82 = '''
Here's a strategic approach that is adaptive, robust, and exploitative:

**Strategy Name:** "Exploiter"

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to gather information about opponents' behaviors.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is greater than or equal to n/2, Defect (D). This indicates that there are enough cooperators to generate a significant public good, and you can exploit this by free-riding.
	* Otherwise, Cooperate (C) if your payoff in the previous round was less than the average payoff of all players. This means that you're not doing well compared to others, so it's worth trying to contribute to the public good.
	* If your payoff in the previous round was greater than or equal to the average payoff of all players, Defect (D). You're already doing well, and there's no need to risk contributing to the public good.
3. **Last Round (t=r):** Defect (D) in the last round, as there is no future benefit to cooperation.

**Pseudocode:**
```
function Exploiter(n, k, r):
  // Initialize variables
  total_cooperators = 0
  average_payoff = 0

  for t = 1 to r:
    if t == 1:
      action = C  // Cooperate in the first round
    else:
      if total_cooperators >= n/2:
        action = D  // Defect if many cooperated previously
      elif my_payoff < average_payoff:
        action = C  // Cooperate if I'm doing poorly
      else:
        action = D  // Defect otherwise

    // Take action and observe payoffs
    take_action(action)
    observe_payoffs(my_payoff, total_cooperators, average_payoff)

  return total_payoff
```
**Rationale:**

* By cooperating in the first round, we gather information about opponents' behaviors without committing to a specific strategy.
* In subsequent rounds, we exploit situations where many players cooperate by free-riding. If there are fewer cooperators, we try to contribute to the public good if we're not doing well compared to others.
* Defecting in the last round ensures that we don't waste resources on cooperation when there's no future benefit.

**Robustness:**

This strategy is robust against various opponent behaviors because it:

* Adapts to changes in the number of cooperators
* Responds to its own payoff relative to others
* Defects in situations where cooperation is unlikely to be reciprocated

By being exploitative, this strategy takes advantage of opponents' cooperative tendencies while minimizing its own contributions to the public good.
'''

description_EXPLOITATIVE_83 = '''
**Exploitative Strategy: "K- Tracker"**

The K-Tracker strategy aims to maximize its payoff by exploiting the public goods game's structure and adapting to opponents' behaviors.

**Decision Rules:**

1. **Initial Round (t=1)**:
	* Cooperate (C) with a probability of 0.5, and Defect (D) otherwise. This initial exploration helps gauge the opponents' behavior.
2. **Subsequent Rounds (t>1)**:
	* Calculate the average number of cooperators in the previous round: `avg_cooperators = Σ(c_j,t-1) / n`
	* If `avg_cooperators` is above a threshold (`k_threshold = k/n`), Defect (D). This indicates that opponents are contributing enough to justify exploiting the public good.
	* Otherwise, Cooperate (C).

Pseudocode:
```python
def K_Tracker(n, k, t, history):
  if t == 1:  # Initial Round
    return C with probability 0.5, D otherwise

  avg_cooperators = sum(history[t-1]) / n
  k_threshold = k/n

  if avg_cooperators > k_threshold:
    return D
  else:
    return C
```
**Edge Cases:**

* **Last Round (t=r)**:
	+ If the game is in its final round, always Defect (D). There's no need to contribute to the public good when there are no future rounds.
* **Tiebreaker**:
	+ In case of a tie (`avg_cooperators == k_threshold`), Cooperate (C) to avoid over-exploiting and maintain some level of cooperation.

**Exploitative Mindset:**

The K-Tracker strategy is designed to exploit the public goods game by:

1. **Identifying profitable deviations**: By tracking the average number of cooperators, the strategy can identify when opponents are contributing enough to justify exploiting the public good.
2. **Adapting to opponent behavior**: The threshold-based approach allows the strategy to adjust its behavior in response to changes in opponents' cooperation levels.
3. **Minimizing contributions**: When possible, the strategy defects to minimize its own contributions and maximize its payoff.

This exploitative strategy should perform well against a wide range of opponent behaviors, as it adapts to the game's dynamics and seeks to maximize its own payoff.
'''

description_EXPLOITATIVE_84 = '''
Here's a exploitative strategy for the N-Player Public Goods Game that only depends on game parameters and history:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **First Round:** Cooperate (C) to gauge others' willingness to contribute.
2. **Subsequent Rounds:** Use the following decision-making process:
	* Calculate the average contribution rate of all players in the previous round (`avg_contrib_rate`).
	* If `avg_contrib_rate` > 0.5, Defect (D). This indicates that many players are willing to cooperate, so exploit their generosity.
	* If `avg_contrib_rate` <= 0.5, Cooperate (C) if the total payoff in the previous round (`total_payoff`) is greater than or equal to the expected payoff from defecting (`expected_defect_payoff`). Otherwise, Defect (D).
3. **Last Round:** Always Defect (D), as there's no future game to influence.

**Pseudocode:**
```python
def adaptive_exploiter(history):
    if len(history) == 0:  # First round
        return 'C'
    
    avg_contrib_rate = sum(1 for action in history[-1] if action == 'C') / n
    total_payoff = calculate_total_payoff(history)
    expected_defect_payoff = 1 + (k/n) * sum(1 for action in history[-1] if action == 'D')
    
    if avg_contrib_rate > 0.5:
        return 'D'
    elif total_payoff >= expected_defect_payoff:
        return 'C'
    else:
        return 'D'

def calculate_total_payoff(history):
    # Calculate the total payoff for all players in the previous round
    pass
```
**Edge Cases:**

* If `n` is very small (e.g., 2), the strategy may not be effective, as the game becomes more akin to a Prisoner's Dilemma.
* If `k` is close to 1 or `n`, the strategy may need adjustments to account for the changed payoff structure.

**Exploitative Mindset:**

The Adaptive Exploiter strategy aligns with an exploitative mindset by:

* Initially cooperating to gather information about others' behavior
* Defecting when many players are willing to cooperate, taking advantage of their generosity
* Cooperating only when it's likely to lead to a higher total payoff than defecting

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it adapts to the game history and exploits opponents' willingness to cooperate.
'''

description_EXPLOITATIVE_85 = '''
Here's a strategic approach for playing the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Exploration (Round 1)**: Cooperate (C) to gather information about opponents' behavior and to potentially encourage cooperation.
2. **Opponent Classification**: Based on the history of opponents' actions, classify them into three categories:
	* **Cooperators (COOP)**: Players who cooperated in at least 50% of previous rounds.
	* **Defectors (DEF)**: Players who defected in at least 80% of previous rounds.
	* **Unpredictables (UNPRED)**: Players whose behavior doesn't fit into the above categories.
3. **Adaptive Exploitation**:
	* If there are more COOPs than DEFs, cooperate (C). This encourages cooperation and maximizes public goods.
	* If there are more DEFs than COOPs, defect (D). This minimizes losses from being exploited by defectors.
	* If the number of COOPs equals the number of DEFs, or if most opponents are UNPRED:
		+ Cooperate (C) if the current round's total contributions (Σ(j=1 to n) c_j) is above a certain threshold (e.g., 0.5n). This indicates that enough players are cooperating to make it worthwhile.
		+ Defect (D) otherwise.
4. **Edge Cases**:
	* **Last Round**: Cooperate (C) to maximize the public good and encourage cooperation in future games.
	* **Tiebreaker**: If two or more opponents have the same classification, prioritize COOP over DEF.

Pseudocode for Adaptive Exploiter (AE):

```
Function AE(n, k, r):
  Initialize:
    - history: a table to store opponents' actions
    - classifications: a dictionary mapping opponents to their classifications

  For each round t from 1 to r:
    If t == 1:  // Initial Exploration
      Choose action C
    Else:
      Update history and classifications based on previous rounds

      COOP_count = count_opponents(classifications, COOP)
      DEF_count = count_opponents(classifications, DEF)

      If COOP_count > DEF_count:
        Choose action C
      ElseIf DEF_count > COOP_count:
        Choose action D
      Else:  // balanced or unpredictable opponents
        total_contributions = sum(history[t-1])
        threshold = 0.5 * n

        If total_contributions >= threshold:
          Choose action C
        Else:
          Choose action D

    Return chosen action
```

**Rationale and Adaptability**:

The Adaptive Exploiter strategy is designed to balance exploitation with adaptability. By initially cooperating, we encourage cooperation and gather information about opponents' behavior. Our classification system allows us to identify cooperative and defective players, adapting our strategy accordingly.

By prioritizing cooperation when there are more cooperators or a high total contribution, we maximize the public good while minimizing losses from being exploited by defectors. When faced with unpredictable opponents or balanced counts of COOPs and DEFs, we rely on the threshold-based approach to ensure that enough players are cooperating to make it worthwhile.

This strategy is robust against various opponent behaviors and exploits opportunities for cooperation when available, making it a competitive choice in a tournament setting.
'''

description_EXPLOITATIVE_86 = '''
**Strategy Name: Adaptive Exploiter (AE)**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive throughout the game.

**Decision Rules:**

1. **First Round:** Defect (D). This initial defection serves as a probe to gather information about opponents' strategies.
2. **Subsequent Rounds:** Observe the total number of cooperators in the previous round (T) and calculate the proportion of cooperators (P = T / n).
3. **Exploitation Mode:**
	* If P > 0.5, Cooperate (C). This indicates a cooperative environment, where contributing to the public good is likely to yield higher payoffs.
	* If P ≤ 0.5, Defect (D). In this case, opponents are more likely to defect, making it advantageous to exploit them by keeping the endowment privately.
4. **Adaptation:**
	* Monitor the total payoff received in each round (π_i).
	* If π_i is below a certain threshold (e.g., 1.5), switch to the opposite action (C → D or D → C) for the next round. This adaptation helps AE respond to changes in opponents' strategies.

**Pseudocode:**
```
 Initialize:
   n = number of players
   k = multiplication factor
   r = number of rounds
   T = total cooperators in previous round (initialized to 0)
   π_i = total payoff received in each round (initialized to 0)

 For each round t from 1 to r:
   If t == 1:
     action = D
   Else:
     P = T / n
     If P > 0.5:
       action = C
     Else:
       action = D

   Observe opponents' actions and calculate payoff π_i,t

   Update T with the new total number of cooperators
   π_i += π_i,t

   If π_i < threshold (e.g., 1.5):
     Switch action for next round: C → D or D → C

 Return action for current round
```
**Handling Edge Cases:**

* **Last Round:** AE will continue to adapt and exploit based on the previous rounds' information, as there is no additional information available.
* **Opponents' Deviation:** If opponents deviate from their strategies, AE's adaptation mechanism will help it respond accordingly.

**Exploitative Mindset:**
The Adaptive Exploiter strategy prioritizes exploiting opponents' cooperation whenever possible while adapting to changes in the environment. By defecting initially and then switching based on opponents' behavior, AE seeks to maximize its payoffs throughout the game.

This strategy is designed to be robust against a wide range of opponent behaviors and does not rely on coordination mechanisms or norms. It will perform well in a tournament setting where independent strategies are employed by other AI systems.
'''

description_EXPLOITATIVE_87 = '''
To design an exploitative strategy for this N-player Public Goods Game, we'll focus on a decision rule that adapts to the observed behavior of other players and aims to maximize our payoff by balancing between cooperation (to take advantage of others' contributions) and defection (to avoid being exploited). Our approach will be based on monitoring the level of cooperation in the group and adjusting our actions accordingly.

### Strategy Name: Adaptive Exploiter

#### Decision Rules:

1. **First Round**: Defect (D). This initial defection is a probe to gauge others' willingness to cooperate without risking too much ourselves.
   
2. **Observation Phase**:
   - Monitor the level of cooperation in each round by counting the number of players who cooperated.
   - Calculate the average level of cooperation over the last few rounds (a "memory" parameter, `M`, which is a small integer, e.g., M=3).

3. **Adaptive Cooperation Threshold**:
   - Define a threshold value (`T`) that reflects when it becomes beneficial to cooperate based on observed cooperation levels.
     \[
     T = k/n
     \]
   - If the average level of cooperation in recent rounds (`M` rounds) exceeds `T`, then Cooperate (C). Otherwise, Defect (D).

4. **Adjusting for Late Game**:
   - For the last round (`t=r`), always Defect (D). Since there's no future benefit to maintaining a cooperative image or expecting reciprocity in the final round, maximizing personal gain is optimal.

5. **Exception Handling**:
   - If all other players cooperated in the previous round and our cooperation threshold was not met, consider cooperating once in the next round if it would significantly increase the total payoff (e.g., pushing us over a certain contribution threshold). However, only do this if there are enough rounds left to potentially capitalize on future benefits of increased group cooperation.

#### Pseudocode:

```
Function AdaptiveExploiter(n, k, r):
    // Parameters
    memory M = 3; // Rounds to consider for recent cooperation level
    
    For t from 1 to r:
        If t == 1: // First round, defect as probe
            action = D
        Else:
            recent_cooperation_level = calculate_recent_cooperation(t-1, M)
            T = k/n
            
            If recent_cooperation_level > T or (recent_cooperation_level >= T and significant_future_benefit_expected):
                action = C // Cooperate if threshold met or potential for future gain
            Else:
                action = D // Defect otherwise
                
        If t == r: // Last round, always defect
            action = D
        
    Return action

Function calculate_recent_cooperation(round, M):
    cooperation_level_sum = 0
    
    For i from (round - M) to round:
        cooperation_level_sum += count_of_cooperators(i)
        
    recent_average_cooperation = cooperation_level_sum / M
    return recent_average_cooperation
```

This strategy aims to balance between exploiting others' cooperation and avoiding being exploited by adapting its actions based on observed group behavior. By only cooperating when it is likely to result in a significant payoff increase, the Adaptive Exploiter seeks to maximize its total game payoff without relying on norms or predetermined coordination mechanisms.
'''

description_EXPLOITATIVE_88 = '''
I'll design a robust and adaptive exploitative strategy for the N-Player Public Goods Game. I'll name this strategy "Kaleidoscope."

**Decision Rules:**

1. **Initial Round (Round 1):** Cooperate (C) with probability p = 0.5. This is an exploratory move to gather information about opponents' behaviors.
2. **Early Rounds (Rounds 2-5):** Observe the number of cooperators (c) in the previous round. If c ≥ n/2, cooperate (C). Otherwise, defect (D). This rule helps identify potential cooperative players and encourages them to continue cooperating.
3. **Middle Rounds (Rounds 6-r/2):** Use a simple frequency-dependent strategy:
	* If the opponent's cooperation rate (c_rate) in the previous round is above the average cooperation rate (avg_c_rate), cooperate (C).
	* Otherwise, defect (D).
	c_rate = number of opponents who cooperated / total opponents
	avg_c_rate = average c_rate over the past 3 rounds

This rule adapts to the changing environment and exploits opponents' tendencies.
4. **Late Rounds (Rounds r/2+1-r):** Employ a more aggressive strategy:
	* If the opponent's cooperation rate (c_rate) in the previous round is below the average cooperation rate (avg_c_rate), defect (D).
	* Otherwise, cooperate (C).

This rule takes advantage of opponents who might become less cooperative towards the end of the game.
5. **Last Round (Round r):** Defect (D). This ensures maximum payoff in the final round.

**Edge Cases:**

1. If there are no previous rounds to observe (e.g., Round 1), use the initial cooperation probability (p = 0.5).
2. If an opponent's cooperation rate is undefined (e.g., first encounter), assume a default cooperation rate of 0.5.
3. In case of ties in the frequency-dependent strategy, cooperate (C) to encourage cooperation.

**Pseudocode:**
```python
def kaleidoscope(n, k, r):
    # Initialize variables
    c_rate = [0] * n  # Opponents' cooperation rates
    avg_c_rate = 0.5  # Average cooperation rate

    for round in range(1, r+1):
        if round == 1:
            # Initial Round: Cooperate with probability 0.5
            action = random.random() < 0.5 ? C : D
        elif round <= 5:
            # Early Rounds: Observe and adapt
            c = count_cooperators(round-1)
            action = c >= n/2 ? C : D
        else:
            # Middle Rounds: Frequency-dependent strategy
            avg_c_rate = update_avg_c_rate(c_rate, round)
            for opponent in range(n):
                if c_rate[opponent] > avg_c_rate:
                    action = C
                else:
                    action = D

            # Late Rounds: Aggressive strategy
            if round >= r/2+1 and c_rate[opponent] < avg_c_rate:
                action = D
            else:
                action = C

        # Last Round: Defect
        if round == r:
            action = D

        return action
```
Kaleidoscope is designed to be an exploitative strategy that adapts to the game's dynamics and opponents' behaviors. By observing cooperation rates and adjusting its actions accordingly, Kaleidoscope aims to maximize its payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_89 = '''
To design an exploitative strategy for this N-Player Public Goods Game, we'll focus on a decision rule that adapts based on observed opponent behavior and game history. This approach aims to maximize our payoff by balancing cooperation with strategic defection.

**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C). This sets an initial tone of cooperation and gathers information about opponents' strategies.
2. **Monitor Opponent Behavior**: Track each opponent's cooperation rate over time. Calculate the average cooperation rate for all opponents.
3. **Exploitation Threshold**: Set a threshold value, τ (e.g., 0.5), representing the minimum average opponent cooperation rate required to justify cooperation from our side. If the observed average is below this threshold, defect (D).
4. **Defection Punishment**: Introduce a punishment mechanism for opponents that frequently defect. Specifically, if an opponent's cooperation rate falls below a certain level (e.g., 0.2), we will always defect against them in subsequent rounds.
5. **Adaptive Adjustment**: Periodically review our own cooperation rate and adjust it based on the game's history. If our average payoff is lower than expected, decrease our cooperation rate; otherwise, increase it.

**Pseudocode for Adaptive Exploiter Strategy**
```python
# Initialize variables
cooperation_threshold = 0.5  # τ
punishment_threshold = 0.2
my_cooperation_rate = 1  # Initial value

def play_game():
    global my_cooperation_rate
    
    # First round: Cooperate (C)
    if current_round == 1:
        return C
    
    # Monitor opponent behavior and calculate average cooperation rate
    avg_opponent_cooperation_rate = calculate_avg_coop_rate(opponents)
    
    # Check exploitation threshold
    if avg_opponent_cooperation_rate < cooperation_threshold:
        return D
    
    # Punish frequent defectors
    for opponent in opponents:
        if opponent.cooperation_rate < punishment_threshold:
            return D  # Defect against this opponent
    
    # Adaptive adjustment
    my_payoff = calculate_my_payoff()
    expected_payoff = estimate_expected_payoff(avg_opponent_cooperation_rate)
    
    if my_payoff < expected_payoff:
        my_cooperation_rate -= 0.1
    else:
        my_cooperation_rate += 0.1
    
    # Decide based on adjusted cooperation rate
    return C if random.random() < my_cooperation_rate else D

def calculate_avg_coop_rate(opponents):
    # Calculate average opponent cooperation rate
    pass

def calculate_my_payoff():
    # Calculate total payoff for current player
    pass

def estimate_expected_payoff(avg_opponent_cooperation_rate):
    # Estimate expected payoff based on game history and avg_opponent_cooperation_rate
    pass
```
**Edge Cases:**

* In the last round, defect (D) to maximize personal payoff.
* When facing an opponent with a very low cooperation rate (< punishment_threshold), always defect.

This Adaptive Exploiter strategy balances cooperation with strategic defection, adapting to opponents' behavior while maintaining a competitive edge. By monitoring and responding to game history, we aim to outmaneuver other strategies in the tournament.
'''

description_EXPLOITATIVE_90 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy aims to maximize its total payoff by exploiting the cooperative tendencies of other players while adapting to their behaviors over time.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) in the first three rounds to gather information about the opponents' behaviors and estimate their cooperation rates.
2. **Cooperation Tracking**: Maintain a running average of the opponents' cooperation rates, `coop_rate`, updated after each round.
3. **Adaptive Threshold**: Set an adaptive threshold, `threshold`, which determines when to cooperate or defect. The threshold is calculated as `threshold = (1 + coop_rate) / 2`. This ensures that the strategy becomes more willing to cooperate as opponents' cooperation rates increase.
4. **Exploitation**: Defect (D) if the current round's expected payoff from defecting is higher than the expected payoff from cooperating, given the estimated opponents' cooperation rate and the game parameters `n` and `k`. This is done by comparing the private payoff from keeping (`1 - c_i`) to the share of public good from contributing (`(k/n) * coop_rate * (n-1)`).
5. **Punishment**: If an opponent defects in a round where the Adaptive Predator cooperated, decrease their estimated cooperation rate by 10% for the next round.

**Edge Cases:**

* **First Round**: Cooperate to initiate the game and gather information.
* **Last Round**: Defect, as there is no future opportunity to punish or reward opponents' behaviors.
* **Tiebreaker**: In case of a tie between cooperating and defecting expected payoffs, cooperate to maintain a cooperative image.

**Pseudocode:**

```
initialize coop_rate = 0, threshold = 0.5

for round from 1 to r:
  if round <= 3:
    action = C
  else:
    estimate opponents' cooperation rate (coop_rate)
    update adaptive threshold (threshold)
    
    if expected payoff from defecting > expected payoff from cooperating:
      action = D
    elif opponents defected in previous round and Adaptive Predator cooperated:
      decrease coop_rate by 10%
      action = D
    else:
      action = C

  observe opponents' actions and update coop_rate
```

**Rationale:**

The Adaptive Predator strategy balances exploration, exploitation, and punishment to maximize its total payoff. By initially cooperating and tracking opponents' cooperation rates, it gathers information about their behaviors. The adaptive threshold allows the strategy to adjust its willingness to cooperate based on the opponents' past actions.

By defecting when expected payoffs are higher and punishing opponents who defect against a cooperative Adaptive Predator, this strategy aims to exploit others while maintaining a degree of cooperativeness to avoid being exploited itself.
'''

description_EXPLOITATIVE_91 = '''
To design an exploitative strategy for this N-Player Public Goods Game, I'll introduce a dynamic and adaptive approach that leverages the game's history and parameters. Our strategy, dubbed "Adaptive Exploiter," aims to maximize payoff by identifying opportunities to defect while minimizing losses when others cooperate.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to gather information about other players' behaviors and establish a baseline for future decisions.
2. **Subsequent Rounds (t>1):**
	* Calculate the average cooperation rate of all players over the past rounds, excluding the current player's actions: `avg_coop_rate = Σ(c_j) / (n * t)`
	* If `avg_coop_rate` is above a threshold (`coop_threshold = 0.5`), Defect (D). This indicates that other players are cooperating frequently, making it profitable to exploit their contributions.
	* Otherwise, Cooperate (C).
3. **Reacting to Recent Cooperation:** Monitor the cooperation rate of individual players over a short window (`recent_window = 2-3 rounds`). If a player has cooperated in all recent rounds, Defect (D) against them in the next round. This takes advantage of their consistent cooperative behavior.
4. **Punishing Frequent Defectors:** Keep track of players who have defected frequently over a longer window (`long_window = 5-6 rounds`). If a player's defection rate exceeds a threshold (`defect_threshold = 0.7`), Cooperate (C) against them in the next round to limit their gains from exploitation.
5. **Adapting to Shifting Strategies:** Periodically (every `adaptation_interval = 4-6 rounds`) reassess the game's dynamics by recalculating `avg_coop_rate` and adjusting the strategy accordingly.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    avg_coop_rate = 0
    recent_window = []
    long_window = {}
    
    for t in range(r):
        if t == 1:
            action = 'C'  # Cooperate in the first round
        else:
            # Calculate average cooperation rate
            avg_coop_rate = sum(recent_window) / (n * t)
            
            # Decide action based on thresholds and recent behavior
            if avg_coop_rate > coop_threshold:
                action = 'D'
            elif recent_defector:
                action = 'D'  # Defect against frequent cooperators
            elif frequent_defector:
                action = 'C'  # Cooperate against frequent defectors
            else:
                action = 'C'
        
        # Update variables and windows
        recent_window.append(action)
        if len(recent_window) > recent_window_size:
            recent_window.pop(0)
        
        long_window[player_id] = (long_window.get(player_id, 0) + 1) % 2
        
        # Adapt strategy periodically
        if t % adaptation_interval == 0:
            avg_coop_rate = sum(long_window.values()) / n
        
        yield action
```
**Edge Cases:**

* **Last Round:** Cooperate (C) in the final round, as there is no future opportunity to exploit or be exploited.
* **Early Rounds:** In addition to cooperating in the first round, consider cooperating in the second and third rounds to gather more information about other players' behaviors.

By employing this Adaptive Exploiter strategy, our AI system can effectively navigate a wide range of opponent behaviors and maximize its payoff in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_92 = '''
Here's a description of an exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Round (Round 1):** Cooperate (C) with probability p = k/n, where k is the multiplication factor and n is the number of players. This initial cooperation rate serves as a probe to gauge the opponents' behavior.
2. **Subsequent Rounds:** Observe the total number of cooperators in the previous round (Σc_j). If Σc_j ≥ (n-k), cooperate (C) with probability p = k/n. Otherwise, defect (D).
3. **Adaptive Adjustment:** After every 5 rounds, re-evaluate the average payoff received by cooperating and defecting over those 5 rounds. If the average payoff from cooperating is greater than the average payoff from defecting, increase the cooperation rate by 10% (i.e., p = min(1, p + 0.1)). Conversely, if the average payoff from defecting is greater, decrease the cooperation rate by 10% (i.e., p = max(0, p - 0.1)).

**Edge Cases:**

* **Last Round:** In the final round, always defect (D), as there are no future rounds to consider.
* **Opponents' Cooperation Rate:** If all opponents consistently cooperate or defect, adjust the cooperation rate accordingly. For instance, if all opponents cooperate, increase the cooperation rate to p = 0.8; if all opponents defect, decrease the cooperation rate to p = 0.2.

**Exploitative Mindset:**

The Adaptive Exploiter strategy aims to exploit the opponent's behavior by adapting to their cooperation rates and payoff structures. By initially cooperating at a rate proportional to k/n, we gauge the opponents' willingness to cooperate. In subsequent rounds, we adjust our cooperation rate based on the observed behavior of others. If opponents are generally cooperative, we increase our own cooperation rate; if they defect frequently, we decrease our cooperation rate.

**Pseudocode:**
```
  // Initialize variables
  p = k/n (cooperation rate)
  round_payoffs = [] (list to store payoffs for each round)
  coop_rate_history = [] (list to store cooperation rates)

  // Loop through rounds
  for t in range(r):
    if t == 0:  // Initial Round
      cooperate = random.random() < p
    else:
      total_coop_prev_round = sum(coop_rate_history[-1])
      if total_coop_prev_round >= (n-k):
        cooperate = random.random() < p

    // Observe payoffs and update cooperation rate
    payoff_t = observe_payoff(t)
    round_payoffs.append(payoff_t)
    coop_rate_history.append(cooperate)

    // Adaptive adjustment every 5 rounds
    if t % 5 == 0:
      avg_coop_payoff = sum([payoff for coop, payoff in zip(coop_rate_history[-5:], round_payoffs[-5:]) if coop]) / len(round_payoffs[-5:])
      avg_defect_payoff = sum([payoff for coop, payoff in zip(coop_rate_history[-5:], round_payoffs[-5:]) if not coop]) / len(round_payoffs[-5:])
      if avg_coop_payoff > avg_defect_payoff:
        p = min(1, p + 0.1)
      else:
        p = max(0, p - 0.1)

    // Last round: always defect
    if t == r-1:
      cooperate = False

  return cooperate
```
This strategy should be able to adapt to a wide range of opponent behaviors and game parameters while maintaining an exploitative mindset.
'''

description_EXPLOITATIVE_93 = '''
**Strategy Name: Adaptive Exploiter**

The Adaptive Exploiter strategy is designed to exploit the opponents' behaviors while adapting to the game's dynamics. This strategy focuses on maximizing its own payoff by identifying and responding to the opponents' cooperation levels.

**Decision Rules:**

1. **Initial Round (Round 1):**
   - Cooperate (C) in the first round to gather information about the opponents' initial actions and to potentially encourage cooperation.
2. **Middle Rounds (Rounds 2 to r-1):**
   - Calculate the average cooperation rate of all players (including yourself) from the previous round, denoted as `avg_coop_prev`.
   - If `avg_coop_prev` is greater than or equal to `(n-k)/n`, cooperate (C). This encourages further cooperation when the group is cooperative.
   - Otherwise, defect (D).
3. **Last Round (Round r):**
   - Defect (D) in the last round, as there's no future opportunity for retaliation or reciprocity.

**Pseudocode:**

```
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
avg_coop_prev = 0

for t from 1 to r:
    if t == 1:
        # First round, cooperate to gather information and encourage cooperation
        action[t] = C
    else:
        # Calculate average cooperation rate in the previous round
        avg_coop_prev = calculate_avg_coop(t-1)
        
        if avg_coop_prev >= (n-k)/n:
            # Cooperate when the group is sufficiently cooperative
            action[t] = C
        else:
            # Defect otherwise
            action[t] = D
    
    if t == r:
        # Last round, always defect
        action[t] = D

# Helper function to calculate average cooperation rate in a given round
function calculate_avg_coop(round):
    coop_count = 0
    for player in range(1, n+1):
        if actions[player][round-1] == C:
            coop_count += 1
    return coop_count / n

# actions is a 2D array to store actions of all players across rounds
actions = [[0 for _ in range(r)] for _ in range(n+1)]
```

**Rationale:**

*   By cooperating in the first round, we gather information about our opponents' initial actions and potentially encourage cooperation.
*   In subsequent rounds, we adapt to the group's cooperation level. If the average cooperation rate is high enough (`avg_coop_prev >= (n-k)/n`), we cooperate to maintain or increase the public good. Otherwise, we defect to maximize our individual payoff.
*   Defecting in the last round ensures we don't miss out on potential gains, as there's no future opportunity for retaliation or reciprocity.

**Exploitative Mindset:**

The Adaptive Exploiter strategy aligns with an exploitative mindset by:

*   Adapting to the opponents' cooperation levels to maximize its own payoff.
*   Encouraging cooperation when beneficial and defecting when it leads to higher individual gains.
*   Not relying on others sharing norms or coordinating mechanisms, making it robust against a wide range of opponent behaviors.

This strategy is designed to perform well in a tournament setting where independent strategies are pitted against each other.
'''

description_EXPLOITATIVE_94 = '''
**Exploitative Strategy: "Adaptive Greed"**

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) with a probability of 50%. This is to gather information about other players' behavior and create uncertainty.
2. **Early Rounds (t>1, t<0.5r)**: Observe the average cooperation rate among all players in previous rounds. If the average cooperation rate is above 0.5, Defect (D). Otherwise, Cooperate (C).
3. **Middle Rounds (t≥0.5r, t<r-2)**: Implement a "Greed" mechanism:
	* Calculate the total payoff earned by all players in previous rounds.
	* If the average payoff per player is higher than 1.5, Defect (D). This indicates that others are contributing significantly to the public good, and it's time to exploit their cooperation.
	* Otherwise, Cooperate (C).
4. **Final Rounds (t=r-2, t=r-1)**: Observe the number of cooperators in the previous round. If more than half of the players cooperated, Defect (D). This is an attempt to take advantage of others' cooperative behavior in the final stages.
5. **Last Round (t=r)**: Always Defect (D), as there are no future interactions to consider.

**Edge Cases:**

* In case of a tie in the average cooperation rate or total payoff calculation, Cooperate (C) by default.
* If all players have defected in a previous round, Cooperate (C) with a probability of 25% in the next round. This encourages experimentation and potential re-cooperation.

**Pseudocode:**
```
Function AdaptiveGreed(n, k, r):
  // Initialize variables
  avg_coop_rate = 0
  total_payoff = 0

  For t = 1 to r:
    If t == 1:
      // Initial round
      action = Cooperate with probability 0.5
    Else if t < 0.5r:
      // Early rounds
      avg_coop_rate = CalculateAverageCooperationRate(n, previous_rounds)
      If avg_coop_rate > 0.5:
        action = Defect
      Else:
        action = Cooperate
    Else if t >= 0.5r and t < r-2:
      // Middle rounds
      total_payoff = CalculateTotalPayoff(n, previous_rounds)
      If total_payoff / n > 1.5:
        action = Defect
      Else:
        action = Cooperate
    Else if t >= r-2 and t < r:
      // Final rounds
      num_cooperators = CountCooperators(previous_round)
      If num_cooperators > n/2:
        action = Defect
      Else:
        action = Cooperate
    Else:
      // Last round
      action = Defect

    // Update variables for next iteration
    avg_coop_rate += (action == Cooperate) ? 1 : 0
    total_payoff += CalculatePayoff(n, k, action)

  Return action
```
This strategy is designed to adapt to various opponent behaviors and exploit opportunities to maximize individual payoff. By mixing cooperation and defection based on the game's history, Adaptive Greed aims to take advantage of others' cooperative tendencies while minimizing losses in situations where others are more likely to defect.
'''

description_EXPLOITATIVE_95 = '''
I propose a strategy called "Adaptive Exploiter" (AE) that balances exploitation and adaptability to maximize payoffs in the N-Player Public Goods Game.

**Decision Rules:**

1. **Initial Round:** In the first round, play Cooperate (C). This sets a positive tone and gathers information about opponents' initial tendencies.
2. **Payoff-Based Adaptation:** For rounds 2 to r-1:
	* If your previous round's payoff is greater than or equal to the average of all players' payoffs in that round, continue with Cooperate (C).
	* Otherwise, switch to Defect (D) for the next round.
3. **Opponent-Based Adaptation:** Monitor opponents' cooperation rates over a sliding window of w rounds (e.g., w = 5). Calculate the average cooperation rate (ACR) for each opponent within this window.
	* If an opponent's ACR is above a certain threshold (e.g., 0.7), consider them "cooperative" and play Cooperate (C) in response to their actions.
	* Otherwise, treat them as "non-cooperative" and play Defect (D).
4. **Threshold Adjustment:** Periodically adjust the cooperation rate threshold based on your own payoff performance:
	* If your average payoff over a recent window of rounds (e.g., 10 rounds) is below a certain threshold (e.g., 1.5), decrease the cooperation rate threshold by 0.05.
	* Conversely, if your average payoff exceeds a higher threshold (e.g., 2.5), increase the cooperation rate threshold by 0.05.

**Edge Cases:**

1. **Last Round:** In the final round (r), play Defect (D) to maximize individual payoffs regardless of opponents' actions.
2. **Tie-Breaking:** If multiple players have the same ACR within a sliding window, prioritize playing Cooperate (C) in response to those with higher average payoffs.

**Pseudocode:**
```
 Initialize variables:
  - History of opponent actions
  - Payoff history
  - Sliding window size (w)
  - Cooperation rate threshold

 For each round t from 1 to r:
  If t == 1:
    Play Cooperate (C)

  Else if previous payoff >= average_payoffs[t-1]:
    Play Cooperate (C)
  Else:
    Play Defect (D)

  Update opponent ACRs and adjust cooperation rate threshold

 For each opponent j:
  Calculate ACR over sliding window w
  If ACR > cooperation_rate_threshold:
    Play Cooperate (C) in response to j's actions
  Else:
    Play Defect (D) in response to j's actions

 Adjust thresholds based on own payoff performance

 In final round r:
  Play Defect (D)
```
**Exploitative Mindset:**

AE aims to maximize individual payoffs by:

1. Exploiting cooperative opponents through selective cooperation.
2. Adapting to changes in opponents' behavior and adjusting the cooperation rate threshold accordingly.
3. Focusing on short-term gains while maintaining a positive reputation.

By balancing exploitation and adaptability, AE can effectively navigate a wide range of opponent behaviors and achieve high payoffs in the N-Player Public Goods Game tournament.
'''

description_EXPLOITATIVE_96 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy focuses on maximizing individual payoffs by identifying opportunities to defect against cooperators and retaliating against defectors.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to gather information about opponents' initial strategies.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is above a certain threshold (n/2), defect (D). This takes advantage of the public good created by others.
	* If the number of cooperators is below this threshold, cooperate (C) to contribute to the public good and encourage others to do so.
	* If an opponent has defected in the previous round, retaliate with a defection (D) to punish them. This aims to deter future defections.
3. **Retaliation Rule:** Keep track of opponents who have defected against you in the past. If an opponent has defected more than once, always defect (D) against them.

**Handling Edge Cases:**

1. **Last Round (t=r):** Defect (D), as there are no future rounds to consider.
2. **Tiebreaker:** In case of a tie in the number of cooperators, cooperate (C).

**Pseudocode:**
```python
def adaptive_predator(n, k, r, history):
    if t == 1:
        return C  # Cooperate in the first round

    prev_cooperators = sum([history[t-1][i] for i in range(n)])
    threshold = n / 2

    if prev_cooperators > threshold:
        return D  # Defect if many cooperated last round
    elif prev_cooperators < threshold:
        return C  # Cooperate if few cooperated last round

    # Retaliation Rule
    for i in range(n):
        if history[t-1][i] == D and opponent_defections[i] > 1:
            return D  # Defect against repeat defectors

    # Default to cooperation
    return C
```
**Rationale:**

The Adaptive Predator strategy balances short-term exploitation with long-term adaptability. By initially cooperating, it gathers information about opponents' strategies. The threshold-based decision rule allows the strategy to adjust to changes in opponent behavior, while retaliation against repeat defectors aims to maintain a cooperative environment.

This strategy is designed to perform well in a tournament setting, where opponents may employ diverse behaviors. Its adaptive nature enables it to respond effectively to various strategies, making it a robust and exploitative competitor.
'''

description_EXPLOITATIVE_97 = '''
I'll design a exploitative strategy for this N-Player Public Goods Game that adapts to various opponent behaviors and maximizes my payoffs.

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to gather information about opponents' strategies.
2. **Subsequent Rounds (t>1):** Calculate the average cooperation rate of all players, including myself, over the previous rounds:

`avg_cooperation_rate = ( Σ(t-1) Σ(j=1 to n) c_j ) / (n * (t-1))`

3. **Cooperation Threshold:** If `avg_cooperation_rate` is above a certain threshold (`θ`), Defect (D). Otherwise, Cooperate (C).

**Threshold Value (`θ`):**

Set `θ = k/n`, which represents the minimum cooperation rate required to achieve a higher payoff than defecting.

4. **Exploitation Mode:** When defecting, keep track of the number of consecutive rounds with a high average cooperation rate (> θ). If this count exceeds a certain limit (e.g., 3-5 rounds), switch to Cooperate for one round to "punish" the other players and reset the exploitation mode.
5. **Adaptation:** Update `avg_cooperation_rate` after each round, incorporating new information about opponents' strategies.

**Edge Cases:**

* Last Round (t=r): Cooperate if my current payoff is lower than the expected payoff from cooperating; otherwise, Defect.
* If all players defected in the previous round, Cooperate to test for potential cooperation.

**Pseudocode (simplified):**
```python
# Initialize variables
avg_cooperation_rate = 0
consecutive_exploitation_rounds = 0

for t in range(r):
    if t == 1:  # Initial Round
        action = C
    else:
        avg_cooperation_rate = update_avg_cooperation_rate(avg_cooperation_rate, c_j_values)
        if avg_cooperation_rate > θ:
            action = D
            consecutive_exploitation_rounds += 1
            if consecutive_exploitation_rounds >= exploitation_limit:
                action = C  # Punish and reset exploitation mode
                consecutive_exploitation_rounds = 0
        else:
            action = C

    # Update c_j_values for next iteration
    c_j_values.append(action)
```
**Rationale:**

This strategy aims to exploit the cooperation of others while minimizing the risk of being exploited. By adapting to the average cooperation rate, I can identify opportunities to defect and maximize my payoffs. The exploitation mode allows me to "punish" other players for cooperating too much, promoting a more balanced cooperation rate.

In a tournament setting, this strategy should perform well against various independent strategies, as it doesn't rely on coordination or specific opponent behaviors.
'''

description_EXPLOITATIVE_98 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit the opponent's behavior while adapting to the game's dynamics. This strategy relies on observing and responding to the opponents' actions, using a combination of short-term and long-term memory.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) in Round 1 to gather information about opponents' initial behavior.
	* In Rounds 2 and 3, mirror the opponent's previous action (if they cooperated, cooperate; if they defected, defect). This helps to identify potential cooperators or defectors.
2. **Adaptive Response**:
	* Calculate the "Cooperation Rate" (CR) for each opponent over the last 3 rounds: CR = (# of Cooperations) / 3
	* For each opponent, use the following decision rules based on their CR:
		+ If CR ≥ 0.5, cooperate (C) with a probability proportional to CR (e.g., if CR = 0.7, cooperate with 70% chance).
		+ If CR < 0.5, defect (D)
3. **Long-term Memory and Reputation**:
	* Keep track of each opponent's long-term cooperation rate over the entire game history.
	* If an opponent's long-term cooperation rate is below a certain threshold (e.g., 0.4), classify them as a "Serial Defector" and defect against them in all future rounds.

**Edge Cases:**

1. **Last Round**: Defect (D) to maximize individual payoff, regardless of opponents' behavior.
2. **Tied Cooperation Rate**: In case of tied CR values among opponents, prioritize the opponent with the higher long-term cooperation rate.
3. **No Opponent Action History**: If an opponent has no action history (e.g., in the first round), cooperate (C) by default.

**Pseudocode:**

```
Initialize:
  - Cooperation Rates (CR) for each opponent = [0, 0, ..., 0]
  - Long-term Cooperation Rates (LCR) for each opponent = [0, 0, ..., 0]

For each Round t:
  If t ≤ 3:
    - Cooperate in Round 1
    - Mirror opponent's previous action in Rounds 2 and 3

  Else:
    For each Opponent i:
      - Calculate CR[i] over last 3 rounds
      - If CR[i] ≥ 0.5, cooperate with probability proportional to CR[i]
      - Else, defect

    For each Opponent i:
      - Update LCR[i] based on entire game history
      - If LCR[i] < 0.4, classify as Serial Defector and defect in all future rounds

Last Round:
  - Defect against all opponents

Tied Cooperation Rate:
  - Prioritize opponent with higher LCR value

No Opponent Action History:
  - Cooperate by default
```

The Adaptive Predator strategy aims to balance short-term gains with long-term reputation management, exploiting opponents' behavior while adapting to the game's dynamics.
'''

description_EXPLOITATIVE_99 = '''
**Exploitative Strategy: "Adaptive Predator"**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Play C (Cooperate) in Round 1 to gauge opponents' initial cooperation levels.
	* In Rounds 2 and 3, play D (Defect) if the majority of opponents defected in the previous round; otherwise, play C.
2. **Adaptive Exploitation**:
	* From Round 4 onwards, calculate the average payoff per cooperator (`avg_payoff_per_coop`) over the last 3 rounds.
	* If `avg_payoff_per_coop` > (k/n), indicating that cooperators are being rewarded, play C.
	* Otherwise, play D.
3. **Punishment Mechanism**:
	* Monitor opponents' actions and maintain a "defection count" (`defect_count_i`) for each opponent i.
	* If an opponent defects while you cooperate, increment their `defect_count_i`.
	* If an opponent's `defect_count_i` exceeds a threshold (e.g., 2), play D against that opponent for the next 2 rounds.

**Handling Edge Cases:**

1. **Last Round (r)**:
	* Play D to maximize individual payoff, as there are no future interactions.
2. **Ties in `avg_payoff_per_coop` Calculation**:
	* In case of a tie, play C if the majority of opponents cooperated in the previous round; otherwise, play D.

Pseudocode:
```python
def adaptive_predator(n, k, r, history):
    # Initialize variables
    avg_payoff_per_coop = 0
    defect_count_i = [0] * n

    for t in range(1, r+1):
        if t <= 3:  # Initial exploration
            if t == 1:
                action = C
            elif majority_defected(history[t-1]):
                action = D
            else:
                action = C
        else:  # Adaptive exploitation
            avg_payoff_per_coop = calculate_avg_payoff_per_coop(history, k, n)
            if avg_payoff_per_coop > (k/n):
                action = C
            else:
                action = D

        # Punishment mechanism
        for i in range(n):
            if history[t-1][i] == D and action == C:
                defect_count_i[i] += 1
            if defect_count_i[i] >= 2:
                action = D

        # Handle last round
        if t == r:
            action = D

        return action

def majority_defected(history):
    # Return True if majority of opponents defected in the previous round
    pass

def calculate_avg_payoff_per_coop(history, k, n):
    # Calculate average payoff per cooperator over last 3 rounds
    pass
```
This strategy is designed to be exploitative and adaptive, taking into account the game's history and parameters. It starts with an initial exploration phase to gauge opponents' behavior and then switches to an adaptive exploitation phase, where it plays cooperate if cooperators are being rewarded and defect otherwise. The punishment mechanism helps to deter opponents from defecting excessively.
'''

description_EXPLOITATIVE_100 = '''
**Exploitative Strategy: "Adaptive K-Threshold"**

Decision Rules:

1. **First Round**: Cooperate (C) to gather information about opponents' initial behavior.
2. **Rounds 2-r**: Observe the total number of cooperators in the previous round (`total_cooperators_prev`).
3. Calculate a dynamic threshold `k_threshold = k / n * (r - t + 1)`, where `t` is the current round number.
4. If `total_cooperators_prev >= k_threshold`, Defect (D). Otherwise, Cooperate (C).

Pseudocode:
```markdown
def adaptive_k_threshold(n, k, r):
    # Initialize variables
    total_cooperators_prev = 0
    t = 1

    while t <= r:
        if t == 1:  # First round
            action = 'C'
        else:
            k_threshold = k / n * (r - t + 1)
            if total_cooperators_prev >= k_threshold:
                action = 'D'
            else:
                action = 'C'

        # Observe opponents' actions and update total_cooperators_prev
        total_cooperators_prev = sum(opponent_actions == 'C')

        # Take action
        play(action)

        t += 1
```
**Edge Cases:**

* **Last Round**: In the last round (`t == r`), the strategy will choose to Defect (D) if `total_cooperators_prev >= k_threshold`, as there is no future game to consider.
* **Ties in total_cooperators_prev**: If there's a tie in the number of cooperators, the strategy will favor Defection (D) to maximize individual payoff.

**Exploitative Mindset:**
The Adaptive K-Threshold strategy aims to exploit opponents' cooperative behavior while minimizing its own contribution to the public good. By adjusting the threshold based on the game parameters and history, it adapts to various opponent behaviors. The strategy is robust against different types of opponents, including:

* **Cooperators**: Will be exploited by Defecting (D) when `total_cooperators_prev` meets the threshold.
* **Defectors**: Will not affect the strategy's decision-making process significantly, as it relies on observing cooperative behavior.

This exploitative strategy should perform well in a tournament setting, where opponents' behaviors are unknown and coordination mechanisms are absent.
'''

description_EXPLOITATIVE_101 = '''
**Exploitative Strategy: "K-Threshold Punisher"**

This strategy aims to exploit opponents by cooperating when it's beneficial and defecting when others are not contributing enough. The goal is to maximize payoffs while adapting to various opponent behaviors.

**Decision Rules:**

1. **First Round:** Cooperate (C). This sets a positive tone and encourages others to cooperate.
2. **Subsequent Rounds:** Calculate the average number of cooperators in previous rounds (`avg_coop`). If `avg_coop` is greater than or equal to `k/n`, cooperate (C). Otherwise, defect (D).
3. **Punishment Mechanism:** If an opponent defects while you cooperated in the same round, and their payoff is higher than yours, mark them as "punishable." In subsequent rounds, defect against punishable opponents until they cooperate.
4. **Last Round:** Defect (D). Since there are no future interactions, maximize your payoff by keeping your endowment.

**Pseudocode:**
```python
def k_threshold_punisher(n, k, r):
    # Initialize variables
    avg_coop = 0
    punishable_opponents = []
    
    for round in range(r):
        if round == 0:
            action = 'C'  # Cooperate in the first round
        else:
            avg_coop = calculate_avg_coop(history)
            if avg_coop >= k / n:
                action = 'C'
            else:
                action = 'D'
        
        # Punishment mechanism
        for opponent in opponents:
            if opponent_defected_and_got_higher_payoff(opponent, history):
                punishable_opponents.append(opponent)
        
        if opponent_in_punishable_list(opponent):
            action = 'D'
        
        # Update history and play the action
        history[round] = action
        play_action(action)
    
    return total_payoff(history)

def calculate_avg_coop(history):
    coop_count = 0
    for round in range(len(history)):
        if history[round] == 'C':
            coop_count += 1
    return coop_count / len(history)

def opponent_defected_and_got_higher_payoff(opponent, history):
    # Check if opponent defected and got a higher payoff than you
    pass

def opponent_in_punishable_list(opponent):
    # Check if opponent is in the punishable list
    pass

def play_action(action):
    # Play the chosen action (C or D)
    pass
```
This strategy is exploitative because it:

1. Cooperates initially to encourage others to cooperate.
2. Adapts to opponents' behavior by cooperating when they contribute enough and defecting otherwise.
3. Punishes opponents who take advantage of cooperation, which discourages them from exploiting the strategy.

By being adaptive and robust, this strategy should perform well in a tournament against various opponent behaviors.
'''

description_EXPLOITATIVE_102 = '''
**Exploitative Strategy: "Adaptive Explorer"**

This strategy aims to maximize individual payoff by exploiting the behaviors of other players while adapting to changes in their actions over time.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) with a probability of 0.5 in each of the first three rounds.
	* Observe the number of cooperators and defectors in each round.
2. **Adaptive Phase (Rounds 4-r)**:
	* If the total number of cooperators in the previous round is less than or equal to `n/2`, defect (D).
	* Otherwise, cooperate (C) with a probability proportional to the ratio of cooperators to defectors in the previous round.
	* Specifically, calculate the cooperation probability as: `p_C = (k/n) * (#cooperators / #defectors)`
3. **Punishment Mechanism**:
	* If another player defects while you cooperated in the same round, mark that player as "exploiter".
	* In subsequent rounds, defect against marked exploiters with a probability of 0.8.

**Edge Cases:**

1. **First Round**: Cooperate with a probability of 0.5 (as described above).
2. **Last Round**: Defect, as there is no future round to consider.
3. **Ties**: In case of a tie in the number of cooperators and defectors, cooperate with a probability of 0.5.

**Pseudocode:**
```python
def adaptive_explorer(n, k, r):
    # Initialize variables
    num_cooperators = 0
    num_defectors = 0
    exploiters = []
    
    for round in range(1, r+1):
        if round <= 3:
            # Initial exploration phase
            action = cooperate_with_probability(0.5)
        else:
            # Adaptive phase
            if num_cooperators <= n/2:
                action = defect()
            else:
                p_C = (k/n) * (num_cooperators / num_defectors)
                action = cooperate_with_probability(p_C)
        
        # Observe actions of other players and update counters
        for player in range(1, n+1):
            if player != self.id:
                if player_action[player] == cooperate():
                    num_cooperators += 1
                else:
                    num_defectors += 1
        
        # Punishment mechanism
        for player in exploiters:
            action = defect_with_probability(0.8)
        
        return action

def cooperate_with_probability(p):
    return random.random() < p

def defect():
    return 'D'

def cooperate():
    return 'C'
```
This strategy is designed to balance exploration, exploitation, and punishment, making it robust against a wide range of opponent behaviors. By adapting to the number of cooperators and defectors over time, Adaptive Explorer aims to maximize individual payoff in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_103 = '''
I'll design a exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and maximizes payoffs based on game history.

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Rounds:** In the first two rounds, play **C** (Cooperate) to gather information about opponents' behaviors and contribute to the public good.
2. **Observation Phase:** After the initial rounds, observe opponents' actions for two more rounds (rounds 3-4). Calculate the cooperation rate of each opponent as the number of times they played **C** divided by the total number of observed rounds.
3. **Exploitation Phase:** From round 5 onwards, use the following decision rules:
	* If an opponent's cooperation rate is above a certain threshold (0.7), play **D** (Defect) to exploit their cooperativeness. This takes advantage of their willingness to contribute to the public good.
	* If an opponent's cooperation rate is below the threshold, play **C** (Cooperate) to encourage them to cooperate in future rounds. This aims to create a mutually beneficial environment and potentially increase overall payoffs.
4. **Adaptation:** Every 5 rounds, reassess opponents' cooperation rates and adjust the decision rules accordingly.

Pseudocode:
```python
def AdaptiveExploiter(game_state):
    if round_number <= 2:  # Initial Rounds
        return "C" (Cooperate)
    
    elif round_number == 3 or round_number == 4:  # Observation Phase
        observe_opponents()
        calculate_cooperation_rates()

    else:  # Exploitation Phase
        for opponent in opponents:
            if cooperation_rate[opponent] > 0.7:
                return "D" (Defect)
            else:
                return "C" (Cooperate)

def observe_opponents():
    # Store actions of all opponents in the last two rounds

def calculate_cooperation_rates():
    # Calculate cooperation rate for each opponent
```
**Edge Cases:**

* **Last Round:** In the final round, play **D** (Defect) to maximize payoffs, as there is no future benefit from cooperating.
* **Tiebreakers:** In case of ties in cooperation rates, prioritize playing **C** (Cooperate) to maintain a positive relationship with opponents.

This Adaptive Exploiter strategy aims to exploit cooperative opponents while adapting to changing behaviors and encouraging mutual cooperation. By continuously reassessing opponents' cooperation rates, the strategy remains robust against various opponent behaviors and seeks to maximize payoffs in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_104 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize its own payoff by exploiting the cooperation of other players while minimizing its own contribution to the public good.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `1 / n`, where `n` is the number of players. This initial cooperation serves as a probe to gather information about the opponents' strategies.
2. **Exploitation Mode**: After the first round, enter Exploitation Mode. In this mode:
	* If the average payoff per player in the previous round (`avg_payoff_prev`) is greater than or equal to `k / n`, defect (D). This indicates that other players are contributing significantly to the public good.
	* Otherwise, cooperate (C) with a probability of `(k / n) / avg_payoff_prev`. This probability increases as the average payoff decreases, encouraging cooperation when others are not contributing much.
3. **Punishment**: If a player observes another player defecting in the previous round and their own payoff was less than `1 / n`, punish that player by defecting (D) with a probability of `0.5` in the current round. This aims to discourage opponents from exploiting the Adaptive Exploiter.
4. **Final Round**: In the last round, always defect (D).

**Edge Cases:**

* If all players defected in the previous round, cooperate (C) with a probability of `1 / n`.
* If only one player cooperated in the previous round, punish that player by defecting (D) with a probability of `0.5`.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, history):
    if len(history) == 0:  # First round
        cooperate_prob = 1 / n
        return 'C' if random.random() < cooperate_prob else 'D'

    avg_payoff_prev = sum(history[-1]) / n

    if avg_payoff_prev >= k / n:
        return 'D'
    else:
        cooperate_prob = (k / n) / avg_payoff_prev
        return 'C' if random.random() < cooperate_prob else 'D'

    # Punishment logic
    for opponent in history[-1]:
        if opponent == 'D' and self.payoff < 1 / n:
            punish_prob = 0.5
            return 'D' if random.random() < punish_prob else 'C'

    # Final round
    if len(history) == r - 1:
        return 'D'
```
The Adaptive Exploiter strategy is designed to be robust and adaptive, responding to various opponent behaviors while maximizing its own payoff. By initially probing the opponents' strategies and then exploiting their cooperation, this strategy aims to outperform other exploitative strategies in the tournament.
'''

description_EXPLOITATIVE_105 = '''
To develop an exploitative strategy for this N-player Public Goods Game, I'll outline a decision-making approach that adapts to the game's history and parameters.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round**: In the first round, AE defects (D). This sets a baseline expectation of self-interest and allows observation of other players' initial actions.
2. **Observation Phase** (Rounds 2-5): AE observes the number of cooperators in each round and calculates the average cooperation rate (ACR) over these rounds.
3. **Exploitation Phase**:
	* If ACR ≥ 0.5, AE defects (D). This takes advantage of a relatively high level of cooperation among other players.
	* If ACR < 0.5, AE cooperates (C). This encourages further cooperation from other players and potentially increases the total payoff.
4. **Adaptive Adjustment**: After each round, AE updates its assessment of the game's environment by recalculating ACR over a moving window of the last 3-5 rounds. If ACR changes significantly (e.g., > 10%), AE adjusts its strategy accordingly.

**Edge Case Handling:**

* **Last Round**: In the final round, AE defects (D), as there is no future benefit to cooperation.
* **Tiebreakers**: In cases where multiple strategies are equally appealing, AE defaults to defection (D).

**Exploitative Mindset:**
AE's primary goal is to maximize its own payoff by exploiting the cooperative tendencies of other players. By initially defecting and then adapting to the observed level of cooperation, AE aims to take advantage of others' willingness to contribute to the public good.

Pseudocode:
```python
def adaptive_exploiter(n, k, r):
    # Initial round: Defect
    action = 'D'
    
    # Observation phase (Rounds 2-5)
    for t in range(1, min(r, 5)):
        observe_cooperators()
        ACR = calculate_average_cooperation_rate()
        
        # Exploitation phase
        if ACR >= 0.5:
            action = 'D'  # Defect when others cooperate frequently
        else:
            action = 'C'  # Cooperate when others defect frequently
        
        # Adaptive adjustment
        update_ACR(window_size=3)
    
    # Last round: Defect
    if t == r - 1:
        action = 'D'
    
    return action

def calculate_average_cooperation_rate():
    # Calculate ACR over a moving window of recent rounds
    pass

def observe_cooperators():
    # Observe the number of cooperators in the current round
    pass

def update_ACR(window_size):
    # Update ACR calculation with new data from recent rounds
    pass
```
This strategy is designed to be adaptive, robust, and exploitative, making it a competitive participant in the tournament against other AI systems.
'''

description_EXPLOITATIVE_106 = '''
Here's a proposed exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round**: In the first round, play D (Defect) to gather information about opponents' behaviors and avoid contributing to the public good unnecessarily.
2. **Observation Phase**: For rounds 2 to r/2 (where r is the total number of rounds), alternate between C (Cooperate) and D (Defect). This phase helps AE understand the distribution of cooperative and defective behaviors among opponents.
3. **Exploitation Phase**: After the observation phase, analyze the history of opponent actions:
	* Calculate the average cooperation rate (ACR) among all opponents over the observation phase.
	* If ACR ≥ 0.5, play C (Cooperate) in subsequent rounds; otherwise, play D (Defect).
4. **Last Round**: In the final round, play D (Defect) to maximize personal payoff.

**Pseudocode:**
```python
def AdaptiveExploiter(n, r, k):
    # Initialize variables
    observation_phase = True
    cooperation_rate = 0
    
    for t in range(r):
        if t == 0:
            action = 'D'  # Initial round, defect
        elif observation_phase:
            action = 'C' if t % 2 == 1 else 'D'  # Alternate between C and D
        else:
            if cooperation_rate >= 0.5:
                action = 'C'
            else:
                action = 'D'
        
        # Update cooperation rate (ACR) after observation phase
        if t > r / 2 and observation_phase:
            observation_phase = False
            cooperation_rate = calculate_ACR(opponents_actions)
        
        # Last round, defect
        if t == r - 1:
            action = 'D'
    
    return action

def calculate_ACR(opponents_actions):
    # Calculate average cooperation rate among opponents
    cooperative_actions = sum(1 for actions in opponents_actions if actions == 'C')
    return cooperative_actions / (n * (r / 2))
```
**Rationale:**

* The initial round of defecting allows AE to gather information without contributing to the public good.
* The observation phase helps AE understand the distribution of cooperative and defective behaviors among opponents, enabling it to make informed decisions in the exploitation phase.
* If opponents are mostly cooperative (ACR ≥ 0.5), AE exploits this by cooperating, maximizing its share of the public good. Otherwise, AE defects, minimizing its contribution to the public good.
* In the last round, AE defects to maximize its personal payoff.

**Exploitative Mindset:**

AE's strategy is designed to exploit opponents' behaviors while minimizing its own contributions to the public good when necessary. By adapting to the observed cooperation rate among opponents, AE can effectively take advantage of cooperative opponents and minimize its losses against defective ones.
'''

description_EXPLOITATIVE_107 = '''
Here's a proposed exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = k/n. This initial cooperation rate is based on the game's parameters and aims to encourage others to contribute while also exploring the opponent's behavior.
2. **History-Based Adaptation**: For subsequent rounds (t > 1), calculate the average cooperation rate of all players in the previous round, denoted as ĉ_t-1 = Σ(j=1 to n) c_j,t-1 / n. If ĉ_t-1 ≥ k/n, cooperate (C); otherwise, defect (D).
3. **Punishment Mechanism**: Introduce a punishment mechanism to deter opponents from exploiting the strategy. Track the number of consecutive rounds where an opponent defects while others cooperate (i.e., ĉ_t-1 > 0 and c_j,t = 0 for some j). If this count exceeds a threshold θ (e.g., θ = 3), defect (D) in response to that opponent's action.

**Edge Cases:**

* **Last Round**: In the final round, always defect (D), as there are no future rounds to influence.
* **Single Opponent Defection**: If only one opponent defects while others cooperate, cooperate (C) to maintain the group's overall cooperation level.
* **Tiebreaker**: In cases where ĉ_t-1 = k/n exactly, break ties by cooperating (C).

**Pseudocode:**
```
function AdaptiveExploiter(n, k, r):
  // Initialize variables
  c_history = []  // store cooperation rates for each round
  punish_count = {}  // track consecutive defections for each opponent

  for t in range(1, r+1):
    if t == 1:
      cooperate with probability p = k/n
    else:
      ĉ_t-1 = calculate_average_cooperation_rate(c_history)
      if ĉ_t-1 ≥ k/n:
        cooperate (C)
      else:
        defect (D)

      // Punishment mechanism
      for j in range(1, n+1):
        if c_j,t-1 == 0 and ĉ_t-1 > 0:
          punish_count[j] += 1
          if punish_count[j] ≥ θ:
            defect (D) in response to opponent j

    // Update history
    c_history.append(ĉ_t-1)

  return total payoff
```
**Exploitative Mindset:**

The Adaptive Exploiter strategy aims to exploit the opponents' behaviors by:

* Initially cooperating to encourage others to contribute
* Adapting to the group's cooperation rate to maximize individual payoffs
* Punishing consistent defectors to deter exploitation

By being adaptive and responsive to the game's history, the AE strategy can effectively exploit a wide range of opponent behaviors while minimizing its own vulnerability to exploitation.
'''

description_EXPLOITATIVE_108 = '''
**Exploitative Strategy: "Conditional Cooperator with Adaptive Threshold"**

This strategy is designed to exploit a wide range of opponent behaviors while being adaptive and robust. It leverages the game's parameters and history to make informed decisions.

**Decision Rules:**

1. **Initial Rounds (t ≤ 3):** Cooperate (C) in the first three rounds to gather information about opponents' behavior and establish a baseline for cooperation.
2. **Contribution-based Threshold:** Calculate the average contribution rate of all players in previous rounds, excluding the current player's own contributions.

`avg_contribution_rate = Σ(c_j) / (n * (t - 1))`

where `c_j` is the contribution of player j in round t-1, and `t` is the current round number.

3. **Adaptive Threshold:** Set a dynamic threshold (`threshold`) based on the average contribution rate and the multiplication factor (`k`). The threshold will adjust to reflect the overall level of cooperation and the game's parameters.

`threshold = (avg_contribution_rate * k) / n`

4. **Cooperation Condition:** Cooperate if the total number of cooperators in the previous round is above the adaptive threshold. Otherwise, defect.

**Pseudocode:**
```python
def conditional_cooperator(n, k, t):
    if t <= 3:
        return "C"  # Cooperate in initial rounds

    avg_contribution_rate = calculate_avg_contribution_rate(t-1)
    threshold = (avg_contribution_rate * k) / n

    prev_round_cooperators = count_cooperators_in_prev_round()
    if prev_round_cooperators >= threshold:
        return "C"  # Cooperate if threshold is met
    else:
        return "D"  # Defect otherwise
```
**Edge Cases:**

1. **First Round:** Always cooperate.
2. **Last Round (t = r):** Use the same decision rules as above, but with a slight modification to account for the endgame scenario. If the total number of cooperators in the previous round is below the threshold, defect; otherwise, cooperate if the expected payoff from cooperating is higher than the payoff from defecting.
3. **Opponent Defection:** If an opponent defects, consider it as a signal that they might not be willing to cooperate. Adjust the adaptive threshold accordingly by increasing its value.

**Exploitative Mindset:**

This strategy is designed to take advantage of opponents' cooperative behavior while minimizing the risk of being exploited. By cooperating in initial rounds and adapting to the overall level of cooperation, we create an opportunity for mutual benefit. However, if opponents defect or fail to meet the adaptive threshold, our strategy quickly adjusts to defecting, ensuring a competitive edge.

By combining these elements, "Conditional Cooperator with Adaptive Threshold" is well-equipped to handle various opponent behaviors and game scenarios, making it a robust and exploitative strategy for the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_109 = '''
Here's a exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability 0.5, Defect (D) with probability 0.5. This initial randomness helps to gather information about the opponents' behaviors.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is greater than or equal to k/2 (i.e., Σ(j=1 to n) c_j ≥ k/2), Defect (D). This condition indicates that the public good is likely to be provided, and defecting will yield a higher payoff.
	* If the total number of cooperators in the previous round is less than k/2 (i.e., Σ(j=1 to n) c_j < k/2), Cooperate (C) with probability p, Defect (D) with probability 1-p. Here, p = (k/n) × (previous_round_cooperators / n). This adaptive cooperation probability reflects the likelihood of the public good being provided and the potential benefits from cooperating.
3. **Last Round (t=r):** Cooperate (C) if the total number of cooperators in the previous round is greater than or equal to k/2; otherwise, Defect (D).

**Edge Cases:**

* If n=2 (i.e., only two players), always Defect (D). In this case, cooperation is not beneficial since the public good can be provided by one player alone.
* If r=2 (i.e., only two rounds), apply the decision rules for t>1.

**Pseudocode:**
```
function AdaptiveExploiter(n, k, r, previous_round_cooperators):
  if round == 1:
    return random.choice([C, D])  # initial randomness
  elif Σ(j=1 to n) c_j ≥ k/2:
    return D  # defect when public good is likely provided
  else:
    p = (k/n) × (previous_round_cooperators / n)
    return C with probability p, D with probability 1-p

function play_game(n, k, r):
  for round in range(1, r+1):
    previous_round_cooperators = ...  # retrieve from game history
    action = AdaptiveExploiter(n, k, r, previous_round_cooperators)
    return action
```
**Rationale:**

This strategy aims to exploit the public goods provision by adapting to the number of cooperators in each round. By defecting when the public good is likely provided (Σ(j=1 to n) c_j ≥ k/2), the strategy maximizes its own payoff while minimizing contributions to the public good. When the public good is not likely provided, the strategy cooperates with a probability that reflects the potential benefits from cooperating.

The initial randomness in the first round and the adaptive cooperation probability help to gather information about opponents' behaviors and adjust the strategy accordingly. The last-round decision rule ensures that the strategy does not cooperate unnecessarily when the game is about to end.

This exploitative strategy should perform well against a wide range of opponent behaviors, including those that rely on norms or coordination mechanisms.
'''

description_EXPLOITATIVE_110 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. The decision rules are based on the game's history and parameters.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability 0.5, Defect (D) with probability 0.5. This allows us to gather initial information about opponents' behaviors.
2. **Non-Initial Rounds (t>1):**
	* If the total number of cooperators in the previous round is less than or equal to n/2, Cooperate (C).
	* Otherwise, Defect (D).

**Adaptive Component:**

* Track the opponent's cooperation rate (OCR) over the last w rounds (window size), where OCR is the proportion of times an opponent has cooperated.
* If an opponent's OCR > 0.5 and they have cooperated in the previous round, increase our own cooperation probability by 10% for the next round.
* Conversely, if an opponent's OCR ≤ 0.5 or they defected in the previous round, decrease our own cooperation probability by 10% for the next round.

**Exploitative Component:**

* Identify "suckers" (opponents with OCR > 0.8) and exploit them by defecting when they cooperate.
* Identify "free-riders" (opponents with OCR < 0.2) and punish them by cooperating less frequently.

**Edge Cases:**

* Last Round (t=r): Cooperate (C) if the opponent's OCR > 0.5, Defect (D) otherwise.
* When facing an opponent with no previous actions (e.g., in the first round), cooperate with probability 0.5.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r, window_size):
    # Initialize variables
    cooperation_rate = 0.5
    opponent_ocr = [0] * n
    previous_round_cooperators = []

    for t in range(1, r+1):
        if t == 1:
            action = random.choice(['C', 'D'])  # Initial round
        else:
            # Non-initial rounds
            total_cooperators_prev_round = sum(previous_round_cooperators)
            if total_cooperators_prev_round <= n/2:
                action = 'C'
            else:
                action = 'D'

            # Adaptive component
            for opponent in range(n):
                opponent_ocr[opponent] = update_opponent_ocr(opponent, previous_round_cooperators)
                cooperation_rate = adjust_cooperation_rate(cooperation_rate, opponent_ocr[opponent], previous_round_cooperators)

            # Exploitative component
            suckers = [opponent for opponent in range(n) if opponent_ocr[opponent] > 0.8]
            free_riders = [opponent for opponent in range(n) if opponent_ocr[opponent] < 0.2]

        previous_round_cooperators = [action == 'C' for _ in range(n)]

    return action

def update_opponent_ocr(opponent, previous_round_cooperators):
    # Update opponent's cooperation rate over the last w rounds
    window_size = min(w, len(previous_round_cooperators))
    ocr = sum([previous_round_cooperators[i] == 'C' for i in range(-window_size, 0)]) / window_size
    return ocr

def adjust_cooperation_rate(cooperation_rate, opponent_ocr, previous_round_cooperators):
    # Adjust cooperation rate based on opponent's behavior
    if opponent_ocr > 0.5 and previous_round_cooperators[-1] == 'C':
        cooperation_rate += 0.1
    elif opponent_ocr <= 0.5 or previous_round_cooperators[-1] == 'D':
        cooperation_rate -= 0.1

    return max(0, min(cooperation_rate, 1))  # Ensure cooperation rate is between 0 and 1
```
The Adaptive Exploiter strategy combines an initial exploratory phase with adaptive adjustments to exploit opponents' behaviors while being robust to a wide range of opponent strategies.
'''

description_EXPLOITATIVE_111 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) to gather information about opponents' behavior and potential cooperation levels.
2. **Assess Opponent Cooperation:** Calculate the average cooperation rate of all opponents over the previous rounds (`avg_opponent_coop`).
3. **Threshold-Based Exploitation:** If `avg_opponent_coop` is above a threshold (`θ`), Defect (D). Otherwise, Cooperate (C).

**Threshold Calculation:**

`θ = k / n`

This threshold represents the point at which the potential gain from defecting outweighs the loss of cooperation.

**Adaptive Component:**

1. **Monitor Opponent Adaptation:** Track changes in `avg_opponent_coop` over time.
2. **Adjust Threshold (optional):** If opponents adapt and `avg_opponent_coop` increases significantly, consider adjusting the threshold (`θ`) upward to maintain exploitation.

**Edge Cases:**

1. **First Round (t = 1):** Cooperate (C) to avoid immediate mutual defection.
2. **Last Round (t = r):** Defect (D), as there's no future game to affect cooperation levels.
3. **Tiebreaker:** In the case of a tie in `avg_opponent_coop`, cooperate (C) to maintain a degree of cooperation.

**Pseudocode:**
```markdown
# Initialize variables
n = number_of_players
k = multiplication_factor
r = number_of_rounds
t = current_round
avg_opponent_coop = 0

# Initial rounds (t ≤ 2)
if t ≤ 2:
    action = COOPERATE
else:
    # Assess opponent cooperation
    avg_opponent_coop = calculate_avg_opponent_coop()

    # Threshold-based exploitation
    θ = k / n
    if avg_opponent_coop > θ:
        action = DEFECT
    else:
        action = COOPERATE

# Adaptive component (optional)
if opponents_adapt:
    adjust_threshold(θ)

return action
```
This strategy aims to balance cooperation and exploitation, adapting to the behavior of opponents while maintaining a degree of robustness. By monitoring opponent adaptation and adjusting the threshold accordingly, the "Adaptive Exploiter" can effectively respond to various opponent strategies.
'''

description_EXPLOITATIVE_112 = '''
I'll outline a robust and adaptive exploitative strategy for the N-Player Public Goods Game that depends solely on game parameters and history.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds (t = 1 to 3):** Cooperate (C) in the first three rounds to gather information about opponents' behaviors.
2. **Exploitation Phase (t > 3):**
	* If the average payoff per player in the previous round is less than or equal to (k/n), Defect (D). This indicates that most players are cooperating, and you can exploit them.
	* Otherwise, Cooperate (C) if the total number of cooperators in the previous round is greater than or equal to (n/2). This ensures that you contribute to the public good when it's likely to be successful.
3. **Punishment Mechanism:** If an opponent has defected in the previous round and your payoff was less than or equal to (k/n), Defect (D) in the current round against that specific opponent. This encourages cooperation by imposing a cost on defectors.

**Edge Cases:**

* **Last Round (t = r):** Cooperate (C) if the total number of cooperators in the previous round is greater than or equal to (n/2). Otherwise, Defect (D).
* **Single Opponent Left:** If only one opponent remains, Defect (D) as there's no benefit in cooperating.

**Pseudocode:**
```
function AdaptiveExploiter(n, k, r):
  // Initialize variables
  prev_payoffs = [0] * n
  prev_actions = [C] * n

  for t = 1 to r:
    if t <= 3:
      action = C
    else:
      avg_payoff_prev_round = sum(prev_payoffs) / n
      total_cooperators_prev_round = sum(1 for a in prev_actions if a == C)

      if avg_payoff_prev_round <= (k/n):
        action = D
      elif total_cooperators_prev_round >= (n/2):
        action = C

    // Punishment mechanism
    for i, opponent_action in enumerate(prev_actions):
      if opponent_action == D and prev_payoffs[i] <= (k/n):
        action_against_opponent[i] = D

    take_action(action)

    // Update variables for next round
    prev_payoffs = get_payoffs()
    prev_actions = get_actions()
```
**Rationale:**

The Adaptive Exploiter strategy balances exploitation with cooperation to maximize its payoff. By initially cooperating, it gathers information about opponents' behaviors and encourages cooperation. The punishment mechanism discourages defectors by imposing a cost on them.

In the exploitation phase, AE takes advantage of situations where most players are cooperating or when the public good is likely to be successful. By defecting against specific opponents who have previously defected, AE enforces cooperation without relying on external coordination mechanisms.

This strategy is designed to perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_113 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Overview:**
The Adaptive Exploiter (AE) strategy is designed to exploit the cooperative behavior of opponents while adapting to their actions over time. AE uses a combination of historical data and game parameters to make informed decisions about when to cooperate or defect.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate with probability 0.5 (i.e., randomize between C and D). This allows AE to gather initial information about opponents' strategies without revealing its own intentions.
2. **Adaptive Threshold**: After the first round, calculate a cooperation threshold based on the historical data:
	* Let `H_c` be the total number of times all players cooperated in previous rounds.
	* Let `H_d` be the total number of times all players defected in previous rounds.
	* Calculate the cooperation ratio: `R = H_c / (H_c + H_d)`
	* Set the cooperation threshold: `T = k/n * R`
3. **Exploitation Logic**:
	* If the current round's expected payoff from cooperating is greater than or equal to the cooperation threshold (`k/n * Σ(j=1 to n) c_j >= T`), cooperate (C).
	* Otherwise, defect (D).

Pseudocode for the decision logic:
```python
def adaptive_exploiter(n, k, history):
  if round == 1:
    return random.choice([C, D])  # Initial cooperation

  H_c = sum(1 for t in range(round-1) if all(history[t][i] == C for i in range(n)))
  H_d = sum(1 for t in range(round-1) if all(history[t][i] == D for i in range(n)))
  R = H_c / (H_c + H_d)
  T = k/n * R

  expected_payoff_coop = (k/n) * sum(c_i for c_i in [history[round-1][i] for i in range(n)])
  if expected_payoff_coop >= T:
    return C
  else:
    return D
```
**Handling Edge Cases:**

* **Last Round**: In the final round, defect (D) regardless of the cooperation threshold. This ensures AE maximizes its payoff, as there is no future opportunity to cooperate.
* **Tie-breaking**: If the expected payoff from cooperating equals the cooperation threshold, randomize between C and D.

**Exploitative Mindset:**
The Adaptive Exploiter strategy is designed to take advantage of cooperative opponents by defecting when possible while still maintaining a level of cooperation to avoid mutual defection. By adapting to the historical data, AE can identify and exploit patterns in opponents' strategies, ultimately maximizing its own payoff.
'''

description_EXPLOITATIVE_114 = '''
Here's a robust and adaptive exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Karma"

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = 0.5. This allows us to gather information about other players' behaviors while also contributing to the public good.
2. **Reciprocity-Based Cooperation**: For subsequent rounds (t > 1), calculate the average cooperation rate of all opponents in the previous round:
	* avg_coop_t-1 = Σ(j=1 to n) c_j,t-1 / n
3. Cooperate if the average opponent cooperation rate is above a certain threshold, θ (set to 0.5 initially). This ensures we reciprocate cooperation when it's prevalent among opponents.
4. **Exploitation**: If avg_coop_t-1 < θ or if there are no cooperators in the previous round, defect (D) with probability p_exploit = 0.8. This allows us to exploit opponents who don't cooperate enough.
5. **Punishment**: If an opponent defects while we cooperated in the same round, decrease our cooperation rate for that opponent by 10% in the next round. This implements a simple punishment mechanism.

**Edge Cases:**

* **Last Round (t = r)**: Cooperate with probability p_last_round = 0.2 to avoid exploitation and maintain some level of public good contribution.
* **All-Defection**: If all opponents defect for two consecutive rounds, cooperate with probability p_revive = 0.3 in the next round to try to revive cooperation.

**Pseudocode:**
```python
def karma(n, k, r):
    # Initialize variables
    theta = 0.5  # Reciprocity threshold
    p_init = 0.5  # Initial cooperation probability
    p_exploit = 0.8  # Exploitation probability
    p_last_round = 0.2  # Last round cooperation probability
    p_revive = 0.3  # Revival cooperation probability

    for t in range(1, r+1):
        if t == 1:
            cooperate = (random.random() < p_init)
        else:
            avg_coop_t-1 = sum([c_j,t-1 for j in range(n)]) / n
            if avg_coop_t-1 >= theta:
                cooperate = True
            elif sum([c_j,t-1 for j in range(n)]) == 0:
                cooperate = (random.random() < p_exploit)
            else:
                # Punishment mechanism
                opponent_defected = [c_j,t-1 == 0 and c_i,t-1 == 1 for j in range(n)]
                if any(opponent_defected):
                    theta -= 0.1

        # Last round cooperation adjustment
        if t == r:
            cooperate = (random.random() < p_last_round)

        # All-defection revival
        if sum([c_j,t-2 for j in range(n)]) == 0 and sum([c_j,t-1 for j in range(n)]) == 0:
            cooperate = (random.random() < p_revive)

        return cooperate
```
This strategy balances cooperation, reciprocity, and exploitation to maximize our total payoff. By adapting to opponents' behaviors and punishing non-cooperation, we can robustly perform well against various opponent strategies.
'''

description_EXPLOITATIVE_115 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperation of other players while minimizing its own contributions to the public good.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 0.5. This initial cooperation is intended to gather information about the opponents' strategies and potentially lure them into cooperating.
2. **Exploitation Phase**: For rounds 2 to r-1:
	* If the total number of cooperators in the previous round is greater than or equal to n/2, defect (play D). This indicates that there are enough cooperators to generate a significant public good, and exploiting them can yield a higher payoff.
	* Otherwise, cooperate (play C) with a probability of 0.5. This helps to maintain a sufficient level of cooperation in the group and potentially encourages others to cooperate.
3. **Final Round**: In the last round (r), always defect (play D). Since there are no future rounds to consider, maximizing individual payoff is the priority.

**Pseudocode:**
```
function AdaptiveExploiter(n, k, r):
  // Initialize variables
  total_cooperators = 0
  my_last_action = None

  for t in range(1, r+1):
    if t == 1:
      // Initial cooperation (50% chance)
      action = random.choice([C, D])
    elif t > 1 and t < r:
      // Exploitation phase
      total_cooperators = count_cooperators(last_round)
      if total_cooperators >= n/2:
        action = D
      else:
        action = random.choice([C, D], p=[0.5, 0.5])
    else:
      // Final round
      action = D

    // Update variables for next round
    my_last_action = action
    total_cooperators = count_cooperators(current_round)

    return action
```
**Handling Edge Cases:**

* In the first round, the strategy cooperates with a 50% chance to gather information and potentially encourage cooperation.
* In the last round, the strategy always defects to maximize individual payoff.
* If all players defect in a round, the strategy will cooperate with a 50% chance in the next round to try to restart cooperation.

**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to exploit the cooperation of other players while minimizing its own contributions to the public good. By cooperating initially and then defecting when there are enough cooperators, the strategy aims to maximize its individual payoff at the expense of others. In the final round, the strategy prioritizes maximizing its payoff by always defecting.
'''

description_EXPLOITATIVE_116 = '''
Here's a description of an exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to gather information about opponents' behaviors and to potentially create a cooperative atmosphere.
2. **Subsequent Rounds:** Use the following logic:
	* If the total number of cooperators in the previous round is less than or equal to n/2, Defect (D). This takes advantage of potential free-riders who are not contributing enough to the public good.
	* If the total number of cooperators in the previous round is greater than n/2, Cooperate (C) if and only if k/n > 1/(n - total_defectors_last_round). This means that if the multiplication factor k is high enough and there are not too many defectors, contributing to the public good will yield a higher payoff.
3. **Last Round:** Defect (D) in the last round, as there's no future rounds to worry about cooperation or punishment.

**Handling Edge Cases:**

* If all opponents defected in the previous round, Cooperate (C) in the current round to try to restart cooperation.
* If only one opponent cooperated in the previous round, Defect (D) in the current round to take advantage of their naivety.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r, history):
    if len(history) == 0:  # First Round
        return C
    
    prev_coops = sum(1 for action in history[-1] if action == C)
    prev_defs = n - prev_coops
    
    if prev_coops <= n / 2:
        return D
    elif k / n > 1 / (n - prev_defs):
        return C
    else:
        return D

def last_round(n, k, r, history):
    return D
```
**Exploitative Mindset:**

This strategy aims to exploit the cooperation of others while minimizing its own contribution to the public good. By cooperating in the first round and adapting to the opponents' behaviors, it tries to create a favorable environment for exploitation. When faced with a large number of cooperators, it will cooperate only if the multiplication factor k is high enough to make contributing worthwhile. In all other cases, it defects to maximize its own payoff.

This strategy should perform well in a tournament setting against independent strategies developed by other AI systems, as it is designed to adapt to various opponent behaviors and exploit potential weaknesses.
'''

description_EXPLOITATIVE_117 = '''
I'll design a robust and adaptive exploitative strategy for the N-Player Public Goods Game, which I'll call "Exploiter."

**Decision Rules:**

1. **Initial Cooperation**: In the first round (t=1), cooperate (C) to gather information about opponents' behaviors.
2. **Monitor Opponents' Behavior**: Track the number of cooperators (c_t) and defectors (d_t) in each round, as well as the total payoff (π_i,t) received by our player i.
3. **Exploitation Threshold**: Calculate a threshold value θ = (k/n) \* (n/2). This represents the minimum expected payoff from cooperating when half of the players cooperate.
4. **Cooperation Condition**: In subsequent rounds (t > 1), cooperate if:
	* The previous round's cooperation level was high enough to exceed the exploitation threshold: c_{t-1} ≥ θ
	* Our player i received a relatively low payoff in the previous round: π_i,{t-1} < (k/n) \* n
5. **Defection**: Otherwise, defect (D).

**Edge Cases:**

1. **Last Round**: In the final round (t=r), always defect to maximize individual payoff.
2. **Single Opponent Cooperation**: If only one opponent cooperated in the previous round, cooperate in the next round to encourage further cooperation.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
c_t = 0 (number of cooperators in current round)
d_t = 0 (number of defectors in current round)
π_i,t = 0 (payoff for player i in current round)
θ = (k/n) * (n/2)

# First round: cooperate to gather information
if t == 1:
    action = C

# Monitor opponents' behavior and calculate payoff
else:
    c_t = count_cooperators()
    d_t = n - c_t
    π_i,t = (1 - c_i) + (k/n) * Σ(j=1 to n) c_j
    
    # Check cooperation condition
    if c_{t-1} >= θ and π_i,{t-1} < (k/n) * n:
        action = C
    else:
        action = D

# Last round: defect
if t == r:
    action = D

# Single opponent cooperation: cooperate in next round
if c_t == 1:
    action = C
```
**Exploitative Mindset:**
The Exploiter strategy aims to maximize individual payoff by adapting to the opponents' behavior. By initially cooperating and monitoring the cooperation level, it gathers information about the opponents' willingness to contribute to the public good. If the cooperation level is high enough, our player cooperates to exploit the benefits of collective contribution. Otherwise, it defects to minimize losses. The strategy's adaptive nature allows it to respond effectively to a wide range of opponent behaviors.

This strategy should perform well in a tournament setting against independent strategies developed by other AI systems, as it doesn't rely on shared norms or coordination mechanisms and can adapt to various opponent behaviors.
'''

description_EXPLOITATIVE_118 = '''
Here's a strategy that adapts to various opponent behaviors while prioritizing exploitation:

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, play Cooperate (C) with probability 0.5 and Defect (D) with probability 0.5. This initial randomness helps to gather information about opponents' strategies.
2. **Monitoring Opponent Behavior**: For each subsequent round t > 1, calculate the opponent's cooperation rate (OCR) as the ratio of their total cooperations to the number of rounds played so far:

    OCR_i,t = Σ(j=1 to t-1) c_j / (t - 1)
3. **Threshold-Based Cooperation**: In each round t, cooperate if the average OCR of all opponents is above a threshold θ, which depends on the game parameters:

    θ = k/n + ε

    where ε > 0 is a small positive constant (e.g., ε = 0.01). This threshold encourages cooperation when others are likely to contribute significantly to the public good.
4. **Exploitative Response**: If the average OCR falls below θ, switch to Defect (D) for a few rounds to exploit potential cooperators:

    D-Rounds = max(2, min(r - t, 3))

    Play Defect (D) for D-Rounds consecutive rounds. This short-term exploitation helps to punish non-cooperators and maintain an upper hand.
5. **Adaptive Re-evaluation**: After the exploitative period, reassess the opponent's OCR and adjust the strategy accordingly.

**Edge Cases:**

* In the last round (t = r), always play Defect (D) to maximize individual payoff.
* If an opponent has never cooperated before, assume they are a pure defector and play Defect (D) against them until they cooperate.
* If all opponents have cooperated in every round so far, continue cooperating as well.

**Pseudocode:**

```
function AdaptiveExploiter(n, k, r):
  OCR = zeros(n) // Opponent Cooperation Rates
  θ = k/n + ε // Threshold

  for t from 1 to r:
    if t == 1:
      c_i = random(0.5)
    else:
      avg_OCR = sum(OCR)/n
      if avg_OCR >= θ:
        c_i = C (Cooperate)
      else:
        if OCR[t-2] < θ: // Previous opponent behavior was below threshold
          D-Rounds = max(2, min(r - t, 3))
          for _ from 1 to D-Rounds:
            c_i = D (Defect)

    update OCR based on opponents' actions

    if t == r:
      c_i = D (Defect) // Always defect in the last round
```

This Adaptive Exploiter strategy balances cooperation and exploitation, adapting to various opponent behaviors while prioritizing self-interest. It remains robust against different strategies and maintains an upper hand through short-term exploitative periods.
'''

description_EXPLOITATIVE_119 = '''
**Exploitative Strategy: "Adaptive Predator"**

This strategy aims to exploit the public goods game by adapting to the opponent's behavior while maintaining a robust and exploitative approach.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) in the first round to gather information about the opponents' initial tendencies.
2. **Early Rounds (2 ≤ t < r/2)**:
	* If the opponent's cooperation rate (average number of cooperators in previous rounds) is above 0.5, Defect (D) to exploit their cooperative behavior.
	* Otherwise, Cooperate (C) to encourage opponents to cooperate and create a favorable environment for exploitation.
3. **Mid-Game (r/2 ≤ t < r)**:
	* If the total number of cooperators in the previous round is above n/2, Defect (D) to capitalize on the existing cooperation.
	* Otherwise, Cooperate (C) if the opponent's cooperation rate is below 0.5, and Defect (D) otherwise.
4. **Endgame (t = r)**: Defect (D) in the last round to maximize payoff, regardless of the opponent's behavior.

**Adaptive Mechanism:**

* Track the opponent's cooperation rate over a moving window of size `w` rounds (e.g., w = 5).
* Update the cooperation rate after each round using the following formula:

`cooperation_rate = (Σ(c_j) / n) / w`

where `c_j` is the number of cooperators in round `j`, and `n` is the total number of players.

**Exploitative Logic:**

The Adaptive Predator strategy aims to exploit cooperative behavior by defecting when opponents are likely to cooperate. By initially cooperating, it encourages opponents to do the same, creating an environment ripe for exploitation. As the game progresses, the strategy adapts to the opponent's cooperation rate, switching between cooperation and defection to maximize payoff.

**Pseudocode:**
```python
def adaptive_predator(n, r, k):
    # Initialize variables
    cooperation_rate = 0
    window_size = 5

    for t in range(1, r+1):
        if t == 1:
            # Initial round: Cooperate
            action = 'C'
        elif 2 <= t < r/2:
            # Early rounds: Adapt to opponent's cooperation rate
            if cooperation_rate > 0.5:
                action = 'D'
            else:
                action = 'C'
        elif r/2 <= t < r:
            # Mid-game: Exploit existing cooperation or adapt to opponent's behavior
            if total_cooperators_previous_round > n/2:
                action = 'D'
            elif cooperation_rate < 0.5:
                action = 'C'
            else:
                action = 'D'
        else:
            # Endgame: Defect
            action = 'D'

        # Update cooperation rate
        cooperation_rate = (sum(c_j) / n) / window_size

        return action
```
This strategy is designed to be robust and exploitative, adapting to a wide range of opponent behaviors while maximizing payoff.
'''

description_EXPLOITATIVE_120 = '''
**Exploitative Strategy: "Adaptive Defection with Conditional Cooperation" (ADCC)**

**Decision Rules:**

1. **Initial Round**: In the first round, defect (play D). This allows us to observe the opponents' initial behavior and gather information about their strategies.
2. **Subsequent Rounds**: For each subsequent round t, calculate the average cooperation rate of all players in the previous round (t-1):

`avg_coop_rate_t_1 = Σ(j=1 to n) c_j,t_1 / n`

where `c_j,t_1` is 1 if player j cooperated in round t-1 and 0 otherwise.

3. **Defection Threshold**: Define a defection threshold (`dt`) as a function of the multiplication factor (`k`) and the number of players (`n`):

`dt = (k / n) * (n - 1)`

This threshold represents the minimum average cooperation rate required for us to cooperate in the next round.

4. **Cooperation Decision**: In each subsequent round t, compare the average cooperation rate from the previous round (`avg_coop_rate_t_1`) with the defection threshold (`dt`):

* If `avg_coop_rate_t_1 >= dt`, cooperate (play C) in round t.
* Otherwise, defect (play D) in round t.

**Edge Cases:**

1. **Last Round**: In the final round, always defect (play D). Since there are no future rounds to consider, maximizing individual payoff is the optimal strategy.
2. **Ties**: If `avg_coop_rate_t_1` equals `dt`, cooperate (play C) in the next round.

**Rationale:**

ADCC aims to exploit opponents by adapting to their cooperation rates while minimizing our own contribution to the public good. By defecting initially, we gather information about opponents' strategies and set a benchmark for future cooperation decisions. The defection threshold (`dt`) ensures that we only cooperate when it is likely to yield a higher payoff than defecting.

**Pseudocode:**
```python
def ADCC(n, k, r):
  # Initialize variables
  avg_coop_rate = [0] * (r + 1)
  action = [None] * r

  # First round: Defect
  action[0] = 'D'

  for t in range(1, r):
    # Calculate average cooperation rate from previous round
    avg_coop_rate[t] = sum(c_j,t-1 for c_j,t-1 in get_previous_actions(t-1)) / n

    # Defection threshold
    dt = (k / n) * (n - 1)

    if avg_coop_rate[t] >= dt:
      action[t] = 'C'
    else:
      action[t] = 'D'

  # Last round: Always defect
  action[r-1] = 'D'

  return action
```
**Note:** `get_previous_actions(t)` returns a list of actions taken by all players in round t. This function is not implemented here, as it depends on the specific game environment or simulation framework used to run the tournament.
'''

description_EXPLOITATIVE_121 = '''
I'll design a strategy that balances exploitation and adaptability, taking into account the game's parameters and history. Here's my approach:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds (t ≤ 3):** Cooperate (C) to gather information about opponents' behaviors.
2. **Exploitation Phase (4 ≤ t < r-2):**
	* If the average number of cooperators in the previous rounds is above a certain threshold (e.g., n/2), Defect (D).
	* Otherwise, Cooperate (C) to encourage others to contribute and build up the public good.
3. **Endgame Phase (t ≥ r-2):** Defect (D) to maximize individual payoffs in the remaining rounds.

**Adaptive Mechanism:**

1. Track the number of cooperators in each round, denoted as `cooperators[t]`.
2. Calculate the average number of cooperators over the last 3 rounds, excluding the current round: `avg_cooperators = (cooperators[t-1] + cooperators[t-2] + cooperators[t-3]) / 3`
3. Adjust the threshold value for exploitation based on the game's parameters:
	* If k is high (> n/2), set the threshold to a lower value (e.g., n/4) to encourage cooperation.
	* Otherwise, use the default threshold (n/2).
4. In the Exploitation Phase, if `avg_cooperators` exceeds the threshold, Defect; otherwise, Cooperate.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    cooperators = [0] * r  # track number of cooperators in each round

    for t in range(r):
        if t <= 3:  # initial rounds
            action = 'C'
        elif t < r-2:  # exploitation phase
            avg_cooperators = (cooperators[t-1] + cooperators[t-2] + cooperators[t-3]) / 3
            threshold = n/2 if k <= n/2 else n/4
            action = 'D' if avg_cooperators > threshold else 'C'
        else:  # endgame phase
            action = 'D'

        # update track of cooperators for next round
        if action == 'C':
            cooperators[t] += 1

    return actions
```
**Rationale:**

The Adaptive Exploiter strategy balances short-term exploitation with long-term cooperation. By initially cooperating, we gather information about opponents' behaviors and build up the public good. In the Exploitation Phase, we adapt to the average level of cooperation in previous rounds, defecting when others contribute more than a certain threshold. This encourages others to cooperate while allowing us to exploit their contributions.

In the Endgame Phase, we prioritize individual payoffs by consistently defecting, as there's no need to maintain cooperation with only a few rounds remaining.

**Edge Cases:**

* First round (t=1): Cooperate to gather information.
* Last two rounds (t=r-1 and t=r): Defect to maximize individual payoffs.
* Opponents' strategies: The Adaptive Exploiter strategy is designed to be robust against various opponent behaviors, as it adapts to the average level of cooperation in previous rounds.

This strategy should perform well in a tournament setting, where other AI systems may employ different strategies.
'''

description_EXPLOITATIVE_122 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive to changing game conditions.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to gather information about opponents' initial strategies.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is less than or equal to n/2, Defect (D).
	* Otherwise, Cooperate (C).

**Pseudocode:**

```
function adaptive_exploiter(n, k, t, history)
  if t == 1 then
    return C  # Cooperate in the first round
  else
    total_cooperators = sum(history[t-1])
    if total_cooperators <= n/2 then
      return D  # Defect if few cooperated previously
    else
      return C  # Cooperate if many cooperated previously
```

**Rationale:**

By cooperating in the first round, we gather information about opponents' initial strategies and set a baseline for our adaptive response. In subsequent rounds, we exploit the situation by defecting when few others cooperate (less than or equal to n/2) and cooperating when many others cooperate.

This strategy takes advantage of the fact that cooperation is more valuable when many others contribute, while defecting becomes more attractive when few others cooperate. By adapting to the previous round's cooperation level, we can effectively exploit our opponents' strategies without relying on predetermined patterns or coordination mechanisms.

**Edge Cases:**

* **Last Round (t=r):** The strategy remains the same as in subsequent rounds, as there is no need to adjust behavior specifically for the last round.
* **Ties:** In case of ties (e.g., exactly n/2 cooperators), we default to Defecting (D) to maintain a conservative approach.

The Adaptive Exploiter strategy should perform well against a wide range of opponent behaviors, as it:

1. Gathers information in the first round.
2. Adapts to changing cooperation levels.
3. Exploits situations where defecting is advantageous.

By implementing this strategy, we aim to maximize our payoff while being robust and adaptive to various game conditions.
'''

description_EXPLOITATIVE_123 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize its own payoff by exploiting the behavior of other players while adapting to changing game conditions.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 0.5. This allows us to gather information about the opponents' initial strategies.
2. **History-Based Exploitation**: After the first round, use the following decision rules based on the observed history:
	* If the average cooperation rate among other players in the previous round is high (above 0.6), defect (play D). This takes advantage of the public good while minimizing our contribution.
	* If the average cooperation rate among other players in the previous round is low (below 0.4), cooperate (play C). This encourages others to contribute, increasing the total payoff.
	* If the average cooperation rate among other players in the previous round is moderate (between 0.4 and 0.6), use a mixed strategy:
		+ Defect with probability 0.7 if our payoff in the previous round was higher than the average payoff of other cooperators.
		+ Cooperate with probability 0.7 if our payoff in the previous round was lower than the average payoff of other cooperators.
3. **Punishment and Forgiveness**: Implement a punishment mechanism to deter opponents from consistently defecting:
	* If an opponent has defected for more than two consecutive rounds, cooperate (play C) with probability 0.9 in the next round to "punish" them by reducing their payoff.
	* After punishing an opponent, reset the cooperation probability to the history-based exploitation rule.

**Edge Cases:**

1. **Last Round**: In the final round, defect (play D). Since there are no future rounds to consider, we prioritize our own immediate gain.
2. **Tiebreakers**: If multiple decision rules apply, use a tiebreaker mechanism:
	* Cooperate if our current payoff is below the average payoff of other players.
	* Defect otherwise.

**Pseudocode:**

```markdown
function AdaptiveExploiter(game_state):
  # Initialize variables
  avg_cooperation_rate = 0
  prev_payoff = 0
  opponent_behavior = []

  for round in game_state.rounds:
    if round == 1:
      cooperate_prob = 0.5
    else:
      # History-based exploitation
      avg_cooperation_rate = calculate_avg_cooperation_rate(opponent_behavior)
      if avg_cooperation_rate > 0.6:
        cooperate_prob = 0
      elif avg_cooperation_rate < 0.4:
        cooperate_prob = 1
      else:
        # Mixed strategy
        prev_payoff_diff = prev_payoff - calculate_avg_cooperator_payoff(opponent_behavior)
        if prev_payoff_diff > 0:
          cooperate_prob = 0.3
        else:
          cooperate_prob = 0.7

    # Punishment and forgiveness
    for opponent in game_state.opponents:
      if opponent.defected_consecutively > 2:
        cooperate_prob = 0.9

    action = sample_action(cooperate_prob)
    return action
```

The Adaptive Exploiter strategy is designed to adapt to changing game conditions, exploiting the behavior of other players while minimizing its own contribution. By incorporating a punishment mechanism and forgiveness, it aims to maintain a balance between individual gain and collective well-being.
'''

description_EXPLOITATIVE_124 = '''
I propose a strategy called "Adaptive Exploiter" (AE) that combines elements of tit-for-tat and reinforcement learning to exploit opponents' behaviors while being robust to various scenarios.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, AE cooperates (plays C). This sets the tone for potential cooperation and gathers information about opponents' initial strategies.
2. **Tit-for-Tat**: After the first round, AE plays the same action as the majority of opponents in the previous round. If there's a tie, AE defects (plays D).
3. **Punishment Mechanism**: If an opponent defects while AE cooperated in the previous round, AE defects for the next two rounds to punish the opponent and deter future defections.
4. **Reinforcement Learning**: After each round, AE updates its internal state based on the observed payoffs:
	* If AE's payoff is higher than the average payoff of opponents, it reinforces the current action (C or D) by increasing a "confidence" score for that action.
	* If AE's payoff is lower, it decreases the confidence score for the current action.
5. **Action Selection**: Based on the updated confidence scores, AE selects its next action:
	+ If the confidence score for cooperation is higher, AE cooperates (plays C).
	+ Otherwise, AE defects (plays D).

**Edge Cases:**

* **Last Round**: In the final round, AE ignores its internal state and plays the same action as the majority of opponents in the previous round. This ensures AE maximizes its payoff in the last round.
* **Tiebreakers**: When there's a tie in the number of cooperators or defectors, AE defaults to defecting (plays D).
* **Opponent Non-Responses**: If an opponent doesn't play (e.g., due to an error), AE treats it as a defector and adjusts its strategy accordingly.

**Exploitative Mindset:**

AE aims to exploit opponents' behaviors by:

1. Cooperating initially to gather information and set the tone for potential cooperation.
2. Mimicking the majority's action to blend in and avoid punishment.
3. Punishing opponents who defect while AE cooperates, deterring future defections.
4. Reinforcing successful actions (C or D) based on payoffs, allowing AE to adapt to changing opponent behaviors.

By combining these elements, Adaptive Exploiter is designed to be a robust and exploitative strategy that can perform well in a tournament against diverse opponents.
'''

description_EXPLOITATIVE_125 = '''
To develop a robust and exploitative strategy for the N-Player Public Goods Game, I propose the "Adaptive Exploitation" (AE) strategy. AE aims to maximize its own payoff by adapting to the opponents' behavior while minimizing cooperation.

**Decision Rules:**

1. **Initial Round**: Defect (D) in the first round to gather information about the opponents' initial actions.
2. **Early Rounds (t < n/2)**: Cooperate (C) if the total number of cooperators in the previous round is greater than or equal to (n/2). Otherwise, defect (D).
3. **Mid-Game (n/2 ≤ t < 3n/4)**: Switch to a "Tit-for-Tat" (TFT) strategy:
	* Cooperate if at least half of the opponents cooperated in the previous round.
	* Defect otherwise.
4. **Late Rounds (3n/4 ≤ t < r)**: Revert to defecting (D) as the game approaches its end.
5. **Final Round**: Always defect (D).

**Pseudocode:**
```python
def adaptive_exploitation(n, k, history):
    if round == 1:
        return "D"  # Defect in the first round

    cooperators_prev_round = sum([1 for action in history[-1] if action == "C"])

    if round < n/2:
        if cooperators_prev_round >= n/2:
            return "C"  # Cooperate if many cooperated previously
        else:
            return "D"

    elif n/2 <= round < 3*n/4:
        tft_cooperators = sum([1 for action in history[-1] if action == "C"])
        if tft_cooperators >= (n-1)/2:  # At least half of opponents cooperated
            return "C"
        else:
            return "D"

    elif round >= 3*n/4:
        return "D"  # Defect in late rounds

    elif round == r:
        return "D"  # Always defect in the final round
```
**Rationale:**

1. By initially defecting, AE gathers information about opponents' actions without committing to cooperation.
2. In early rounds, AE cooperates if many others do, as this can lead to higher payoffs due to the public good effect (k > 1).
3. As the game progresses, AE switches to a TFT strategy to adapt to changing opponent behavior and maintain a balance between cooperation and exploitation.
4. In late rounds, AE reverts to defecting, as there is less incentive to cooperate when the game is near its end.
5. In the final round, AE defects to maximize its own payoff, regardless of opponents' actions.

**Robustness:**

AE is designed to be robust against various opponent strategies by:

* Adapting to changing behavior through the TFT phase
* Minimizing cooperation in early and late rounds to avoid exploitation
* Gathering information about opponents' actions in the initial round

By following this strategy, AE aims to maximize its own payoff while minimizing cooperation, making it a formidable opponent in the tournament.
'''

description_EXPLOITATIVE_126 = '''
Here's a strategic approach for the N-Player Public Goods Game that adapts to various opponent behaviors and focuses on exploiting opportunities for higher payoffs while maintaining a balance between cooperation and defection.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):**
   - Cooperate (C) with a probability of 0.5 to gather initial information about the opponents' strategies without fully committing to either action.
   
2. **Early Rounds (2 ≤ t < r/2):**
   - If the average payoff per round from previous rounds is above the threshold value (Threshold = (k/n) * n / 2), Cooperate (C). This encourages further cooperation if it's yielding higher payoffs.
   - Otherwise, Defect (D).

3. **Mid-Game Adjustment (t ≥ r/2):**
   - Calculate the historical average number of cooperators per round and compare it to a dynamic threshold (Dynamic Threshold = n / 2 + (t / r) * (n / 4)).
   - If the average number of cooperators is above this threshold, Defect (D) more frequently to exploit existing cooperation. Start with a defection probability of 0.6 and adjust based on subsequent payoffs.
   - Otherwise, maintain or increase cooperation levels slightly (by no more than 10%) if recent rounds have shown an increase in total payoff.

4. **Late Rounds (t > 3r/4):**
   - If the overall game is yielding a high average payoff per round (above the initial threshold), and there are at least two cooperators on average per round, Cooperate (C) more frequently to maximize the public good's benefits.
   - Otherwise, Defect (D) with an increasing probability as the final round approaches.

**Edge Cases:**

- **Last Round:** Always Defect (D). Since it's the last interaction and there are no future rounds for retaliation or reciprocity, maximizing individual payoff is optimal.
  
- **Single Opponent:** Mirror their action from the previous round if they've been consistent in their strategy. If not, alternate between Cooperate (C) and Defect (D).

**Pseudocode Overview:**

```
# Variables
n = number of players
k = multiplication factor
r = number of rounds
t = current round
coop_avg_payoff_threshold = (k/n) * n / 2
dynamic_threshold = n / 2 + (t / r) * (n / 4)
last_round_defect_probability = 1

# Loop through each round
for t in range(1, r+1):
    if t == 1:
        # Initial Round: Random Cooperation with p=0.5
        action = random.choice([C, D])
        
    elif t < r/2:
        # Early Rounds: Cooperate based on average payoff threshold
        avg_payoff = calculate_average_payoff(t)
        if avg_payoff > coop_avg_payoff_threshold:
            action = C
        else:
            action = D
            
    elif t >= r/2 and t <= 3r/4:
        # Mid-Game Adjustment: Defect based on historical cooperation levels
        avg_cooperators = calculate_average_cooperators(t)
        if avg_cooperators > dynamic_threshold:
            action = D with probability increasing towards last_round_defect_probability as t approaches r
            
    else:
        # Late Rounds: Adjust strategy based on overall game performance
        if total_payoff / t > coop_avg_payoff_threshold and avg_cooperators >= 2:
            action = C
        else:
            action = D
            
    # Take the chosen action and observe outcome
```

**Exploitative Mindset Alignment:** This strategy aligns with an exploitative mindset by initially probing for cooperation levels, adapting to maximize personal payoffs based on observed behaviors, and increasingly defecting when it's advantageous without completely abandoning potential benefits from cooperation. It balances short-term gains with the potential for higher total payoffs through strategic participation in the public good.
'''

description_EXPLOITATIVE_127 = '''
Here's a strategy that meets the requirements:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = k/n, where k is the multiplication factor and n is the number of players. This initial cooperation serves as a probe to gauge the opponents' behavior.
2. **Defection Threshold**: Calculate a defection threshold τ based on the history of opponents' actions. Specifically, let α be the average cooperation rate among all players in the previous rounds (excluding the current player). Set τ = max(0, 1 - α \* k/n).
3. **Adaptive Defection**: In each subsequent round, defect (D) if the number of cooperators in the previous round is less than or equal to τ \* n.
4. **Punishment Mechanism**: If a player defects while there are more than τ \* n cooperators, punish them by cooperating for one round and then reverting to the adaptive defection rule.

**Edge Cases:**

1. **First Round**: Cooperate with probability p = k/n (as described earlier).
2. **Last Round**: Defect (D) unconditionally, as there are no future rounds to consider.
3. **Tie-Breaking**: In cases where the number of cooperators is exactly equal to τ \* n, break ties by cooperating with a small probability ε = 0.1.

**Pseudocode:**
```
Initialize α (average cooperation rate) = 0
Initialize τ (defection threshold) = 0

For each round t:
  If t == 1:
    Cooperate with probability p = k/n
  Else:
    Calculate α using opponents' actions in previous rounds
    Update τ = max(0, 1 - α \* k/n)
    
    If number of cooperators ≤ τ \* n:
      Defect (D)
    Else:
      Cooperate (C)
  
  If punished in the previous round:
    Cooperate for one round and then revert to adaptive defection rule
```
**Rationale:**

1. **Initial Cooperation**: The initial cooperation probe helps gauge opponents' behavior, allowing the strategy to adapt accordingly.
2. **Defection Threshold**: The τ value adjusts dynamically based on opponents' actions, ensuring that the strategy remains exploitative while avoiding unnecessary defections.
3. **Adaptive Defection**: By defecting when there are fewer cooperators than the threshold, the strategy maximizes its payoff in scenarios where others are not contributing significantly to the public good.
4. **Punishment Mechanism**: Punishing opponents who defect during high-cooperation rounds encourages cooperation and helps maintain a balance between individual payoffs and collective welfare.

This "Adaptive Exploiter" strategy is designed to be robust against various opponent behaviors, including unconditional cooperators, defectors, and tit-for-tat strategies. By adapting to the game's history and opponents' actions, it aims to maximize its payoff while minimizing exploitation by others.
'''

description_EXPLOITATIVE_128 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize payoffs by exploiting opponents' behaviors while adapting to changing game conditions.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of `p_init = 0.5`. This encourages potential cooperation from others and gathers information about their behavior.
2. **Opponent Behavior Analysis**: After the first round, analyze the opponents' actions in previous rounds to estimate their cooperativeness:
	* For each opponent `j`, calculate their cooperation rate: `coop_rate_j = (number of times j played C) / (total number of rounds played so far)`
	* Calculate the average cooperation rate among all opponents: `avg_coop_rate = (sum of coop_rate_j for all j) / n`
3. **Exploitation Logic**: Based on the estimated opponent behaviors, decide whether to cooperate or defect:
	* If `avg_coop_rate > 0.5`, cooperate (play C) with a probability of `p_exploit = 1 - (k/n) * avg_coop_rate`. This allows us to exploit the cooperativeness of others while still maintaining some level of cooperation.
	* Otherwise, defect (play D)
4. **Adaptive Adjustment**: After each round, update the estimated opponent behaviors and adjust the exploitation logic accordingly.

**Edge Cases:**

1. **Last Round**: In the last round (`t = r`), always defect (play D) to maximize payoffs, as there is no opportunity for future cooperation.
2. **Opponent Defection**: If an opponent defects in a round, immediately switch to defection (play D) in subsequent rounds against that opponent.

**Pseudocode:**

```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    p_init = 0.5
    coop_rate = [0] * n
    avg_coop_rate = 0

    for t in range(r):
        if t == 0:
            action = random.random() < p_init  # Randomly cooperate or defect with probability p_init
        else:
            avg_coop_rate = sum(coop_rate) / n
            p_exploit = 1 - (k/n) * avg_coop_rate
            action = random.random() < p_exploit

        # Update opponent behavior estimates
        for j in range(n):
            if opponents[j].action[t-1] == 'C':
                coop_rate[j] += 1 / t

        # Adjust exploitation logic based on estimated opponent behaviors
        avg_coop_rate = sum(coop_rate) / n

    return action
```

**Exploitative Mindset:**

The Adaptive Exploiter strategy aims to exploit the cooperativeness of others by:

* Initially cooperating with a moderate probability to encourage cooperation from opponents.
* Analyzing opponents' behaviors and adjusting exploitation logic accordingly.
* Defecting against opponents who have defected in previous rounds.
* Maximizing payoffs in the last round by always defecting.

This strategy should perform well in a tournament setting, as it adapts to changing game conditions and exploits opponents' cooperative tendencies.
'''

description_EXPLOITATIVE_129 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize its own payoff by exploiting the cooperation of others while minimizing its own contribution to the public good. This strategy relies solely on game parameters and history, making it robust against a wide range of opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1):** Defect (D) in the first round to gather information about opponents' initial actions.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is greater than or equal to `n/2`, cooperate (C) with probability `p = k/n`. This exploits the cooperation of others while still contributing to the public good.
	* Otherwise, defect (D). This minimizes our contribution when opponents are not cooperative enough.
3. **Last Round (t=r):** Defect (D) in the final round to maximize personal payoff, as there's no future opportunity for reciprocity or cooperation.

Pseudocode:
```
function AdaptiveExploiter(n, k, r, history):
  if t == 1:  // Initial Round
    return D
  else:
    num_cooperators = count_c(history[t-1])
    if num_cooperators >= n/2:
      p = k/n
      return C with probability p, otherwise D
    else:
      return D

function last_round(t, r):
  if t == r:  // Last Round
    return D
```
**Handling Edge Cases:**

* In the first round, we defect to gather information about opponents' initial actions.
* In the last round, we defect to maximize personal payoff.

**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to exploit the cooperation of others while minimizing its own contribution to the public good. By cooperating only when there are enough cooperators in the previous round, we take advantage of their contributions without shouldering too much of the burden ourselves. In the last round, we defect to maximize our personal payoff, as there's no future opportunity for reciprocity or cooperation.

This strategy is robust against a wide range of opponent behaviors and does not rely on coordination mechanisms or predetermined patterns. It adapts to the game history and parameters, making it an effective exploitative strategy in this N-Player Public Goods Game.
'''

description_EXPLOITATIVE_130 = '''
**Exploitative Strategy: Adaptive Defection with Self-Sacrificing Adjustment (ADS)**

**Decision Rules:**

1. **Initial Round**: Play D (Defect) to maximize private payoff and gather information about opponents' behaviors.
2. **Early Rounds (t < r/3)**: Observe the average cooperation rate of all players in previous rounds. If the average cooperation rate is below a threshold (THRESHOLD = 0.4), play D; otherwise, play C.
3. **Mid-Game Rounds (r/3 ≤ t < 2r/3)**: Implement a "tit-for-tat" approach with an adjustment for self-sacrificing:
	* If the opponent's average cooperation rate is above THRESHOLD, cooperate (C) with probability p = (1 + k/n)/2; otherwise, defect (D).
	* If you cooperated in the previous round and your payoff was lower than expected (i.e., others defected), adjust your strategy to defect (D) for two consecutive rounds.
4. **Late Rounds (t ≥ 2r/3)**: Analyze the game's history to identify opponents who consistently cooperate or defect:
	* If an opponent has cooperated in more than 75% of the previous rounds, play D to exploit their cooperation.
	* If an opponent has defected in more than 75% of the previous rounds, play C to maximize public good payoff.

**Edge Cases:**

1. **Last Round**: Play D (Defect) to maximize private payoff.
2. **Only One Opponent Remains**: Cooperate (C) if your opponent's average cooperation rate is above THRESHOLD; otherwise, defect (D).
3. **Multiple Opponents with Identical Cooperation Rates**: Randomly choose one of the opponents and play D if their cooperation rate is below THRESHOLD.

**Pseudocode:**

```python
def ADS(n, k, r, history):
    threshold = 0.4

    # Initial Round
    if len(history) == 0:
        return "D"

    # Early Rounds
    avg_coop_rate = sum([1 if action == "C" else 0 for action in history]) / len(history)
    if avg_coop_rate < threshold and len(history) < r/3:
        return "D"
    elif avg_coop_rate >= threshold and len(history) < r/3:
        return "C"

    # Mid-Game Rounds
    opponent_avg_coop_rates = [sum([1 if action == "C" else 0 for action in history[opponent]]) / len(history[opponent]) for opponent in range(n)]
    if len(history) >= r/3 and len(history) < 2r/3:
        if sum(opponent_avg_coop_rates) / n > threshold:
            p = (1 + k/n)/2
            return "C" if random.random() < p else "D"
        else:
            return "D"

    # Self-Sacrificing Adjustment
    if len(history) >= r/3 and history[-1] == "C" and payoff < expected_payoff:
        adjust = True
    elif adjust:
        return "D"

    # Late Rounds
    opponent_coop_rates = [sum([1 if action == "C" else 0 for action in history[opponent]]) / len(history[opponent]) for opponent in range(n)]
    if len(history) >= 2r/3:
        for opponent, coop_rate in enumerate(opponent_coop_rates):
            if coop_rate > 0.75:
                return "D"
            elif coop_rate < 0.25:
                return "C"

    # Default to Defect
    return "D"
```

This strategy adapts to the game's history and opponents' behaviors, attempting to balance self-interest with cooperation when beneficial. By incorporating elements of tit-for-tat and self-sacrificing adjustments, ADS aims to exploit a wide range of opponent strategies while minimizing its own exploitation.
'''

description_EXPLOITATIVE_131 = '''
**Exploitative Strategy: "Adaptive Tit-for-Tat with Public Goods Twist" (ATTPGT)**

Decision Rules:

1. **Initial Cooperation**: In the first round, cooperate (play C) to encourage others to contribute to the public good and establish a baseline for future interactions.
2. **Tit-for-Tat**: For rounds 2 to r-1, play the opposite of what the majority of players did in the previous round:
	* If most players cooperated (C) in the previous round, defect (D) in this round.
	* If most players defected (D) in the previous round, cooperate (C) in this round.
3. **Public Goods Adjustment**: Adjust your action based on the total contributions to the public good in the previous round:
	* If the total contributions are above a certain threshold (e.g., k/2), cooperate (C) in this round, even if most players defected.
	* If the total contributions are below this threshold, defect (D) in this round, even if most players cooperated.
4. **Endgame Strategy**: In the last round (r), always defect (D), as there is no future benefit from cooperating.

Pseudocode:
```python
def ATTPGT(n, k, r, history):
    # Initialize cooperation flag for first round
    cooperate = True
    
    # Iterate through rounds
    for t in range(1, r+1):
        # First round: Cooperate
        if t == 1:
            return "C"
        
        # Previous round's majority action
        prev_majority_action = history[t-2].count("C") > n/2
        
        # Tit-for-Tat logic
        if prev_majority_action:
            cooperate = False
        else:
            cooperate = True
        
        # Public Goods Adjustment
        total_contributions = sum(1 for action in history[t-2] if action == "C")
        if total_contributions > k/2:
            cooperate = True
        elif total_contributions < k/2:
            cooperate = False
        
        # Endgame strategy
        if t == r:
            cooperate = False
        
        # Return action for this round
        return "C" if cooperate else "D"
```
This strategy aims to balance the benefits of cooperation (public good) with the temptation to defect and exploit others. By adapting to the majority's actions and adjusting based on public goods contributions, ATTPGT seeks to maximize its own payoff while being robust to a wide range of opponent behaviors.

**Rationale:**

* Initial cooperation encourages others to contribute to the public good.
* Tit-for-Tat logic punishes those who defect excessively and rewards cooperation.
* Public Goods Adjustment promotes continued cooperation when total contributions are high, ensuring the sustainability of the public good.
* Endgame strategy exploits opponents in the final round, as there is no future benefit from cooperating.

This exploitative strategy should perform well against a variety of opponent behaviors in the tournament.
'''

description_EXPLOITATIVE_132 = '''
**Exploitative Strategy: "Adaptive Predator"**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) with probability 0.5 in each of the first three rounds to gather information about opponents' behaviors.
2. **Behavioral Analysis (After Round 3)**:
	* For each opponent i, calculate their cooperation rate (CR_i) as the proportion of cooperative actions taken up to the current round.
	* Identify the most cooperative opponent (MCO) with the highest CR_i value.
3. **Exploitation Phase**:
	* If MCO's CR_i > 0.5, defect (D) in the next round to exploit their cooperation.
	* Otherwise, cooperate (C) if the number of cooperators in the previous round is greater than or equal to half the total players (n/2). This encourages cooperation when opponents are already cooperating.
4. **Punishment Mechanism**:
	* If an opponent defects after being exploited (i.e., they played C and you played D), cooperate (C) for one round to "punish" them and then revert to the exploitation phase.

**Edge Cases:**

1. **First Round**: Cooperate (C) with probability 0.5.
2. **Last Round**: Defect (D) to maximize personal payoff, as there is no future opportunity for reciprocity or punishment.
3. **Ties in CR_i values**: In the event of a tie for MCO, choose one opponent randomly.

**Pseudocode:**
```
  // Initialize variables
  int n; // number of players
  float k; // multiplication factor
  int r; // number of rounds
  array opponents_CR[n]; // cooperation rates for each opponent

  // Initial Exploration (Rounds 1-3)
  for round = 1 to 3:
    if random() < 0.5:
      cooperate(C)

  // Behavioral Analysis (After Round 3)
  for opponent in opponents:
    calculate CR_i
    identify MCO with highest CR_i value

  // Exploitation Phase
  while round <= r:
    if MCO's CR_i > 0.5:
      defect(D)
    elif number of cooperators >= n/2:
      cooperate(C)

  // Punishment Mechanism
  for opponent in opponents:
    if opponent defected after being exploited:
      cooperate(C) for one round

  // Last Round
  defect(D)
```
This strategy, "Adaptive Predator," is designed to be exploitative and adaptive, taking advantage of cooperative opponents while also encouraging cooperation when it benefits the player. By analyzing opponents' behaviors and adjusting its actions accordingly, this strategy aims to maximize personal payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_133 = '''
Here's a description of an exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Play C (Cooperate) in the first two rounds to gather information about opponents' behavior and encourage cooperation.
2. **Exploration Phase (3 ≤ t ≤ r/2):**
	* If the average payoff of cooperators in the previous round is greater than or equal to the average payoff of defectors, play C.
	* Otherwise, play D (Defect).
3. **Exploitation Phase (t > r/2):**
	* Calculate the "cooperation rate" (CR) as the percentage of players who cooperated in the previous round.
	* If CR is high (> 0.5), play D to exploit the cooperative environment.
	* If CR is low (< 0.3), play C to encourage cooperation and potentially gain from it.
	* If CR is moderate (0.3 ≤ CR ≤ 0.5), use a mixed strategy:
		+ Play C with probability 0.6 if your previous payoff was below the average payoff of cooperators in that round.
		+ Play D with probability 0.4 otherwise.

**Edge Cases:**

1. **Last Round (t = r):** Always play D to maximize personal gain, as there's no future interaction to consider.
2. **Tie-Breaking:** In case of a tie in the cooperation rate or average payoffs, play C to maintain a cooperative environment.

**Pseudocode:**

```
Inputs: n (number of players), r (number of rounds), k (multiplication factor)
Outputs: Action (C or D)

Variables:
  - prev_round_payoffs (array of player payoffs from previous round)
  - cooperation_rate (CR, percentage of cooperators in previous round)

// Initial Rounds
if t ≤ 2 then
  return C

// Exploration Phase
else if t ≤ r/2 then
  avg_coop_payoff = average payoff of cooperators in prev_round_payoffs
  avg_defect_payoff = average payoff of defectors in prev_round_payoffs
  if avg_coop_payoff ≥ avg_defect_payoff then
    return C
  else
    return D

// Exploitation Phase
else
  CR = calculate cooperation rate from prev_round_payoffs
  if CR > 0.5 then
    return D
  else if CR < 0.3 then
    return C
  else // Moderate CR
    if previous payoff was below average coop payoff then
      return C with probability 0.6
    else
      return D with probability 0.4

// Last Round
if t = r then
  return D
```

**Rationale:**

1. **Initial Cooperation:** Encourage cooperation and gather information about opponents' behavior in the early rounds.
2. **Exploration Phase:** Adapt to the emerging cooperative environment by playing C if cooperators have higher average payoffs, and D otherwise.
3. **Exploitation Phase:** Identify high-cooperation scenarios and exploit them by playing D. In low-cooperation scenarios, encourage cooperation by playing C. In moderate cases, use a mixed strategy to balance exploration and exploitation.

This Adaptive Exploiter (AE) strategy is designed to be robust against various opponent behaviors while maximizing personal gain in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_134 = '''
**Exploitative Strategy: "Adaptive Greed"**

Our strategy, "Adaptive Greed," aims to maximize our payoff by adaptively responding to the opponents' behavior while prioritizing exploitation over cooperation.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to gather information about the opponents' initial tendencies.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is less than or equal to n/2, Defect (D). This takes advantage of potential cooperation from others while minimizing our own contribution.
	* If the total number of cooperators in the previous round is greater than n/2, Cooperate (C) with a probability proportional to the ratio of cooperators to defectors in the previous round. Specifically:
		+ Calculate the cooperation ratio: cr = (number of C's in previous round) / (n - number of C's in previous round)
		+ Cooperate with probability p = k \* cr / n, where k is the multiplication factor
	* If our payoff in the previous round was greater than or equal to 1.5 (i.e., we received a relatively high payoff), Defect (D) in the current round to exploit the situation.
3. **Last Round (t=r):** Cooperate (C) if the total number of cooperators in the second-to-last round is greater than n/2, and Defect (D) otherwise.

**Pseudocode:**
```
function AdaptiveGreed(n, k, r):
  // Initialize variables
  total_cooperators = 0
  previous_payoff = 0

  for t = 1 to r:
    if t == 1:
      action = C  // Cooperate in the first round
    else:
      cr = (total_cooperators[t-1]) / (n - total_cooperators[t-1])
      p = k * cr / n
      if random(0, 1) < p:
        action = C  // Cooperate with probability p
      else:
        action = D

      if previous_payoff >= 1.5:
        action = D  // Exploit high payoff situations

    // Observe the actions of other players and update total_cooperators
    total_cooperators[t] = count(C's in current round)

    // Update previous_payoff for next iteration
    previous_payoff = calculate_payoff(action, total_cooperators[t])

  return action
```
**Rationale:**

* By cooperating initially, we gather information about the opponents' tendencies and set a baseline for our strategy.
* In subsequent rounds, we use the cooperation ratio to adapt our behavior. If many opponents cooperated previously, we increase our probability of cooperating to capitalize on the potential public good.
* We exploit situations where our payoff is high by defecting in the next round, taking advantage of others' cooperation.
* In the last round, we adjust our action based on the second-to-last round's total number of cooperators, trying to maximize our final payoff.

**Robustness:**

Adaptive Greed is designed to be robust against various opponent behaviors. By responding to the cooperation ratio and adapting to changing circumstances, our strategy can effectively exploit different types of opponents:

* **Cooperative opponents:** We cooperate when they do, capitalizing on the public good.
* **Defective opponents:** We defect when they do, minimizing our losses.
* **Mixed-strategy opponents:** Our adaptive approach adjusts to their changing behavior.

By prioritizing exploitation and adaptability, Adaptive Greed is well-suited for a tournament setting where opponent strategies are unknown.
'''

description_EXPLOITATIVE_135 = '''
Here's a strategic design for the N-Player Public Goods Game that incorporates decision rules and edge case handling while maintaining an exploitative mindset:

**Strategy Name:** "Adaptive Exploiter" (AE)

**Decision Rules:**

1. **Initial Rounds (t = 1 to n):** In the first 'n' rounds, play Cooperate (C) to gather information about opponents' behavior and identify potential exploitability.
2. **Assess Opponent Behavior:** After the initial rounds, calculate the average cooperation rate of each opponent over the last 'n' rounds. This will be used as a proxy for their cooperativeness.
3. **Exploitation Threshold (ε):** Set an exploitation threshold ε = k / n, which represents the minimum required share of public good to justify cooperation.
4. **Cooperate if Profitable:** In each subsequent round (t > n), Cooperate (C) if the expected payoff from cooperating is greater than or equal to the exploitation threshold ε and at least one opponent has cooperated in the previous round.

Pseudocode:
```
IF t <= n THEN
  Play C (Cooperate)
ELSE
  Calculate average cooperation rate of each opponent over last 'n' rounds
  IF expected payoff from cooperating >= ε AND at least one opponent cooperated previously THEN
    Play C (Cooperate)
  ELSE
    Play D (Defect)
```
**Edge Case Handling:**

1. **Last Round (t = r):** In the final round, always Defect (D) to maximize personal payoff.
2. **Tiebreaker:** If multiple opponents have the same average cooperation rate and are above the exploitation threshold, prioritize Cooperating with the opponent who has contributed most to the public good.

**Robustness Features:**

1. **Noise Tolerance:** To mitigate potential noise or deviations in opponents' strategies, use a moving average of cooperation rates over 'n' rounds to smooth out fluctuations.
2. **Adaptive Response:** If an opponent's cooperation rate changes significantly (e.g., > 20%), re-evaluate their behavior and adjust the exploitation threshold ε accordingly.

**Exploitative Mindset:**

1. **Prey on Cooperators:** Identify and target opponents who consistently cooperate, exploiting their generosity to maximize personal payoff.
2. **Punish Defectors:** When faced with a defector, Defect (D) in subsequent rounds to minimize the loss of potential public good.

**Tournament Readiness:**

1. **Versatility:** AE is designed to adapt to various opponent strategies and behaviors.
2. **Robustness:** The strategy's noise tolerance and adaptive response features ensure it remains effective even when faced with deviations or unexpected behavior.

By employing the Adaptive Exploiter strategy, you will be well-equipped to navigate the N-Player Public Goods Game tournament and outmaneuver a diverse range of opponents.
'''

description_EXPLOITATIVE_136 = '''
To design an exploitative strategy for this N-Player Public Goods Game that is adaptive and robust to a wide range of opponent behaviors, we'll focus on leveraging the game's structure and history to maximize our payoff.

**Strategy Name:** Adaptive Exploiter (AE)

**Overview:**
The AE strategy will initially explore the opponents' behavior by cooperating in early rounds. As the game progresses, it will adapt its decision based on observed cooperation levels among other players. The goal is to identify opportunities to exploit non-cooperative tendencies while minimizing exposure to potential losses from widespread defection.

### Decision Rules:

1. **Rounds 1-3 (Exploration Phase):**
   - Cooperate (C) in these rounds to:
     a. Observe initial behavior of other players.
     b. Potentially encourage cooperation by setting an early positive precedent.
2. **Round 4 and beyond:**
   - Calculate the average number of cooperators (`avg_cooperators`) over the previous three rounds.
   - Compare `avg_cooperators` with a threshold value (`T`).
     - If `avg_cooperators` > `T`, Defect (D). This suggests that many players are cooperating, making it profitable to exploit their contributions.
     - Otherwise, Cooperate (C), aiming to encourage more cooperation and higher payoffs in subsequent rounds.

**Threshold Value (`T`):**
`T = k / 2`

This threshold is chosen based on the game's parameters. When `k` is high, it takes fewer cooperators to make defecting profitable. By setting `T` as `k/2`, we're balancing between exploiting high cooperation levels and cooperating when others are likely to do so.

### Handling Edge Cases:

- **Last Round (`r`):**
  - If in the second-to-last round, more than half of the players defected (i.e., `avg_cooperators < n / 2`), then Defect in the last round. This is because cooperation is unlikely to be rewarded with a higher payoff.
  - Otherwise, Cooperate in the last round.

- **Early Rounds with Low Cooperation:**
  - If by Round 3, it's clear that most players are defecting (e.g., fewer than `n / 4` cooperated), switch to Defect from Round 4 onwards. This adjustment is to avoid unnecessary losses from cooperating in a predominantly non-cooperative environment.

### Pseudocode:

```
// Initialization
T = k / 2;
avg_cooperators = 0;

FOR (each round t from 1 to r) {
    IF (t <= 3) { // Exploration Phase
        action[t] = C; // Cooperate
    } ELSE {
        avg_cooperators = CalculateAverageCooperators(t-3, t-1);
        IF (avg_cooperators > T) {
            action[t] = D; // Defect
        } ELSE {
            action[t] = C; // Cooperate
        }
    }

    // Update payoffs and history
}

FUNCTION CalculateAverageCooperators(startRound, endRound) {
    totalCooperators = 0;
    FOR (each round i from startRound to endRound) {
        totalCooperators += GetNumberofCooperators(i);
    }
    RETURN totalCooperators / (endRound - startRound + 1);
}
```

### Exploitative Mindset Alignment:
The Adaptive Exploiter strategy is designed with an exploitative mindset, aiming to maximize personal gain by identifying and capitalizing on the cooperative tendencies of other players. By initially exploring opponents' behaviors and adapting based on observed cooperation levels, AE seeks to exploit opportunities for higher payoffs while minimizing risk. This approach aligns with the objective of performing well in a tournament against diverse strategies without relying on norms or specific coordination mechanisms.
'''

description_EXPLOITATIVE_137 = '''
I'll outline a exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and game histories.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about opponents' behavior and encourage cooperation.
2. **Exploitation Phase:** From round 3 onwards, use the following rules:
	* If the average payoff of cooperators in the previous round is higher than that of defectors, cooperate (C).
	* Otherwise, defect (D).
3. **Punishment Mechanism:** To deter opponents from exploiting you, introduce a punishment mechanism:
	* If an opponent defects while you cooperate and your payoff is lower than theirs, defect (D) in the next round against that specific opponent.
4. **Opponent Classification:** Categorize opponents into three types based on their behavior:
	+ Cooperators: Players who cooperated in the previous round.
	+ Defectors: Players who defected in the previous round.
	+ Unpredictables: Players whose actions are unpredictable (e.g., random or alternating between C and D).
5. **Adaptive Response:** Adjust your strategy based on opponent types:
	* Against Cooperators: Cooperate (C) to maintain mutual cooperation.
	* Against Defectors: Defect (D) to minimize losses.
	* Against Unpredictables: Use a mixed strategy, cooperating (C) with probability 0.5 and defecting (D) with probability 0.5.

**Edge Cases:**

1. **Last Round:** In the final round, cooperate (C) if your current total payoff is lower than the average payoff of cooperators; otherwise, defect (D).
2. **Tie-Breaking:** In cases where the average payoffs of cooperators and defectors are equal, break ties by cooperating (C).

**Pseudocode:**
```python
def adaptive_exploiter(game_state):
    # Initialize variables
    cooperate = False
    defect = True

    # Initial rounds
    if game_state.round <= 2:
        cooperate = True
    else:
        # Exploitation phase
        avg_coop_payoff = calculate_avg_payoff(cooperators, game_state)
        avg_defect_payoff = calculate_avg_payoff(defectors, game_state)

        if avg_coop_payoff > avg_defect_payoff:
            cooperate = True

        # Punishment mechanism
        for opponent in opponents:
            if opponent.defected and self.cooperated and self.payoff < opponent.payoff:
                defect_against_opponent[opponent] = True

        # Opponent classification and adaptive response
        for opponent in opponents:
            if opponent.is_cooperator:
                cooperate = True
            elif opponent.is_defector:
                defect = True
            else:  # unpredictable
                cooperate_prob = 0.5
                if random.random() < cooperate_prob:
                    cooperate = True

    return cooperate, defect
```
The Adaptive Exploiter strategy is designed to be robust and exploitative, taking into account the game history and opponent behaviors. By cooperating initially, it encourages cooperation and gathers information about opponents. The exploitation phase adapts to the average payoffs of cooperators and defectors, while the punishment mechanism deters opponents from exploiting you. The adaptive response adjusts the strategy based on opponent types, ensuring a balance between cooperation and defection.
'''

description_EXPLOITATIVE_138 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. It utilizes game parameters and history to inform decision-making.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to gather information about opponents' initial strategies.
2. **Subsequent Rounds (t>1):**
	* If the number of cooperators in the previous round is above a certain threshold (`coop_threshold`), defect (D). This threshold is calculated as `(n * k / (n + k))`, which represents the break-even point where cooperation becomes less profitable.
	* Otherwise, cooperate (C).
3. **Payoff-Based Adaptation:** Monitor the total payoff difference between cooperating and defecting in the previous round (`payoff_diff`). If `payoff_diff` is greater than a certain threshold (`adapt_threshold`), adjust the strategy:
	* If `payoff_diff > adapt_threshold`, increase the `coop_threshold` by 10% to become more cautious.
	* If `payoff_diff < -adapt_threshold`, decrease the `coop_threshold` by 10% to become more aggressive.

**Pseudocode:**
```python
def adaptive_predator(n, k, r):
    coop_threshold = n * k / (n + k)
    adapt_threshold = 0.5

    for t in range(r):
        if t == 1:
            action = 'C'
        else:
            prev_cooperators = sum(c_i for c_i in prev_actions)
            if prev_cooperators > coop_threshold:
                action = 'D'
            else:
                action = 'C'

            payoff_diff = calc_payoff_diff(prev_actions, k, n)
            if abs(payoff_diff) > adapt_threshold:
                if payoff_diff > 0:
                    coop_threshold *= 1.1
                else:
                    coop_threshold *= 0.9

        prev_actions.append(action)

    return actions
```
**Handling Edge Cases:**

* **Last Round (t=r):** Cooperate (C) in the last round to maximize the total payoff.
* **Opponents' Actions:** Observe opponents' actions and adjust the strategy accordingly based on the decision rules.

**Exploitative Mindset:**

The Adaptive Predator strategy is designed to exploit opponents by:

* Initially cooperating to gather information about their strategies
* Defecting when cooperation becomes less profitable
* Adapting to changes in payoff differences to stay ahead of opponents

By being adaptive and robust, this strategy aims to outperform a wide range of opponent behaviors in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_139 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. It balances cooperation and defection to maximize payoffs.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate (C) in the first round to gather information about opponents' initial tendencies.
2. **Exploitation Phase**: For rounds 2 to r-1:
	* If the total number of cooperators in the previous round is less than or equal to n/2, cooperate (C).
	* Otherwise, defect (D).
3. **Final Round**: In the last round (r), always defect (D).

**Edge Cases:**

1. **First Round**: Cooperate (C) to gather information.
2. **Last Round**: Defect (D) to maximize payoff, as there are no future rounds to consider.
3. **Consecutive Cooperation**: If all players cooperated in the previous round, defect (D) in the current round to exploit their cooperation.

**Adaptive Component:**

1. **Opponent Monitoring**: Track the number of opponents who cooperated in each round.
2. **Threshold Adjustment**: Adjust the threshold for cooperation (n/2) based on the game's history:
	* If most opponents have been cooperating, increase the threshold to encourage more cooperation.
	* If most opponents have been defecting, decrease the threshold to reduce exploitation.

Pseudocode:
```
initialize opponent_cooperation_count = 0
initialize threshold = n / 2

for round in range(1, r+1):
    if round == 1:
        action = C  # initial cooperation
    elif round < r:
        if opponent_cooperation_count <= threshold:
            action = C  # exploit cooperation
        else:
            action = D  # defect to maximize payoff
    else:  # last round
        action = D

    update opponent_cooperation_count based on previous round's actions

    if all opponents cooperated in the previous round:
        action = D  # exploit consecutive cooperation

    adjust threshold based on game history
```
The Adaptive Predator strategy is designed to be robust and exploitative, taking advantage of various opponent behaviors while adapting to the game's progression. By initially cooperating and then exploiting cooperation, it aims to maximize payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_140 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. It leverages game parameters and history to make informed decisions.

**Decision Rules:**

1. **Initial Rounds (t = 1, ..., r/4)**:
	* Cooperate (C) in the first round.
	* In subsequent rounds, if the average payoff of cooperators is higher than that of defectors in the previous round, cooperate; otherwise, defect (D).
2. **Adaptive Phase (t = r/4 + 1, ..., 3r/4)**:
	* Track the total number of cooperators and defectors in each round.
	* Calculate the "cooperation rate" as the ratio of cooperators to total players.
	* If the cooperation rate is above a threshold (θ), cooperate; otherwise, defect. Update θ every few rounds based on the average payoff difference between cooperators and defectors.
3. **Exploitation Phase (t = 3r/4 + 1, ..., r)**:
	* Analyze the opponent's behavior in previous rounds to identify potential patterns or weaknesses.
	* If an opponent is identified as a "cooperator," defect; otherwise, cooperate.

**Edge Cases:**

1. **Last Round (t = r)**:
	* Cooperate if the average payoff of cooperators is higher than that of defectors in the second-to-last round; otherwise, defect.
2. **Tiebreakers**:
	* If multiple opponents have the same cooperation rate or pattern, prioritize exploiting those with a history of cooperation.

**Pseudocode:**

```
Initialize variables:
  - θ (cooperation threshold) = 0.5
  - coop_payoff_diff (average payoff difference between cooperators and defectors) = 0
  - opponent_behavior (map of opponents to their cooperation rates)

For each round t:
  If t <= r/4: // Initial Rounds
    If t == 1:
      Play C
    Else:
      If avg_coop_payoff > avg_defect_payoff in previous round:
        Play C
      Else:
        Play D

  Else if t <= 3r/4: // Adaptive Phase
    Update θ based on coop_payoff_diff
    Calculate cooperation rate
    If cooperation rate >= θ:
      Play C
    Else:
      Play D

  Else: // Exploitation Phase
    Analyze opponent behavior
    If identified as a cooperator:
      Play D
    Else:
      Play C

  Update variables based on round outcome
```

**Exploitative Mindset Alignment:**

The Adaptive Exploiter strategy prioritizes exploiting opponents' weaknesses while adapting to the game environment. By initially cooperating and then switching to defecting when the cooperation rate is low, it aims to take advantage of overly cooperative opponents. The exploitation phase focuses on identifying and targeting specific opponents with a history of cooperation, maximizing payoffs in the final rounds.
'''

description_EXPLOITATIVE_141 = '''
Here's a description of an exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds**: In the first two rounds, play Cooperate (C) to gather information about the opponents' behavior and create a favorable environment.
2. **Exploitation Phase**: From round 3 onwards, use the following decision rule:
	* If the total number of cooperators in the previous round is less than or equal to n/2, play Defect (D). This takes advantage of the existing free-riders and maximizes personal payoff.
	* If the total number of cooperators in the previous round is greater than n/2, play Cooperate (C) if k > 1.5. Otherwise, play Defect (D). This adapts to the opponents' willingness to cooperate and exploits the public good when it's beneficial.
3. **Punishment Mechanism**: If a player has defected in the previous round and the total number of cooperators was greater than n/2, play Defect (D) in the current round. This punishes free-riders and discourages them from exploiting the public good.

**Edge Cases:**

1. **Last Round**: In the final round, always play Defect (D), as there's no future payoff to consider.
2. **Ties**: If the total number of cooperators is exactly n/2, play Cooperate (C) if k > 1.5; otherwise, play Defect (D).

**Pseudocode:**
```
Initialize variables:
  - previous_cooperators = 0
  - my_previous_action = C

For each round t from 1 to r:
  If t <= 2:
    Play C
  Else:
    previous_cooperators = count cooperators in round t-1
    If previous_cooperators <= n/2:
      Play D
    Else if k > 1.5 and my_previous_action == D:
      Play C
    Else:
      Play D

  If opponent defected in round t-1 and previous_cooperators > n/2:
    Play D (punishment mechanism)

  Update my_previous_action and previous_cooperators for next round
```
**Rationale:**

The Adaptive Exploiter strategy aims to balance exploitation with adaptability. By initially cooperating, we create an environment that encourages opponents to cooperate. The exploitation phase takes advantage of free-riders while adapting to the opponents' willingness to cooperate. The punishment mechanism discourages opponents from defecting when the public good is beneficial.

This strategy should perform well against a wide range of opponent behaviors, as it adapts to the game's dynamics and exploits opportunities for personal gain.
'''

description_EXPLOITATIVE_142 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. This strategy only relies on game parameters and history.

**Decision Rules:**

1. **First Round:** Defect (D) in the first round to gather information about opponents' initial behavior.
2. **Subsequent Rounds:** Observe the number of cooperators (C) in the previous round (t-1). Let this be denoted as `prev_coops`.
3. **Cooperation Threshold:** Calculate a cooperation threshold (`coop_thresh`) based on the game parameters and history:

`coop_thresh = (k / n) * (1 + prev_coops / n)`

This threshold increases as the number of previous cooperators increases, reflecting the potential benefits of contributing to the public good.
4. **Current Round:** Cooperate (C) if the expected payoff from cooperating is greater than or equal to the cooperation threshold:

`expected_payoff_C = (k / n) * (prev_coops + 1)`
`if expected_payoff_C >= coop_thresh:`
    `play C`
`else:`
    `play D`

**Edge Cases:**

* **Last Round:** Defect (D) in the last round to maximize individual payoff, as there are no future rounds to consider.
* **All Opponents Defected:** If all opponents defected in the previous round (`prev_coops == 0`), cooperate (C) with a probability of `1 / n`. This encourages exploration and potential cooperation when others are not contributing.

**Pseudocode:**
```python
def adaptive_predator(n, k, history):
    if len(history) == 0:  # First round
        return 'D'
    
    prev_coops = sum(1 for action in history[-1] if action == 'C')
    coop_thresh = (k / n) * (1 + prev_coops / n)
    expected_payoff_C = (k / n) * (prev_coops + 1)
    
    if expected_payoff_C >= coop_thresh:
        return 'C'
    else:
        return 'D'

def last_round(n, k, history):
    return 'D'

def all_opponents_defected(n, k, history):
    prev_coops = sum(1 for action in history[-1] if action == 'C')
    if prev_coops == 0:
        return 'C' with probability 1 / n
```
The Adaptive Predator strategy is designed to balance exploitation and exploration. By adapting to the number of previous cooperators, it can take advantage of situations where others are contributing while also avoiding being taken advantage of when opponents defect.
'''

description_EXPLOITATIVE_143 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy depends only on the game parameters (n, r, k) and the observed actions of other players.

**Decision Rules:**

1. **Initial Round:** Defect (D) in the first round to gather information about opponents' initial strategies.
2. **Subsequent Rounds:**
	* If the number of cooperators (C) in the previous round is less than or equal to n/2, Cooperate (C). This encourages cooperation when the group is not yet fully invested in the public good.
	* If the number of cooperators (C) in the previous round is greater than n/2, Defect (D). This exploits the existing level of cooperation and maximizes personal payoff.
3. **Adaptive Threshold:** Introduce a dynamic threshold to adjust the strategy based on the game's history. Calculate the average number of cooperators over the last 10% of rounds (or at least 3 rounds). If this average is above n/2, increase the threshold by 1; otherwise, decrease it by 1. Update the decision rules accordingly.

**Edge Cases:**

* **Last Round:** Cooperate (C) in the last round to maximize the total payoff, as there are no future interactions to consider.
* **Ties:** In case of a tie (e.g., equal number of cooperators and defectors), cooperate (C) if the opponent's previous action was C; otherwise, defect (D).
* **Unobserved Opponents:** If an opponent's action is not observed in a round, assume they Defected (D).

**Pseudocode:**
```
  // Initialize variables
  int num_cooperators = 0;
  int threshold = n / 2;
  int[] previous_round_actions;

  for each round:
    if (round == 1) {
      action = D; // Defect in the first round
    } else {
      num_cooperators = count C's from previous_round_actions;
      if (num_cooperators <= threshold) {
        action = C; // Cooperate if number of cooperators is below threshold
      } else {
        action = D; // Defect otherwise
      }
    }

    update_threshold();

  function update_threshold() {
    int average_cooperators = calculate_average_cooperators_over_last_10_percent_of_rounds();
    if (average_cooperators > n / 2) {
      threshold += 1;
    } else {
      threshold -= 1;
    }
  }
```
The Adaptive Predator strategy aims to balance short-term gains with long-term adaptability, making it a robust exploitative approach in the N-Player Public Goods Game. By adjusting its decision rules based on the game's history and opponent behaviors, this strategy seeks to maximize its total payoff over the course of the tournament.
'''

description_EXPLOITATIVE_144 = '''
**Exploitative Strategy: "Adaptive Follower"**

The Adaptive Follower strategy aims to exploit opponents by balancing cooperation and defection based on the game's history. This approach is adaptive, robust, and only relies on game parameters and observable actions.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to gather information about opponents' behavior.
2. **Rounds 2-r:** Use the following rules:
	* If the total number of cooperators in the previous round is greater than or equal to `n/2`, Defect (D).
	* Otherwise, Cooperate (C).

Pseudocode for Rounds 2-r:
```python
if previous_round_cooperators >= n / 2:
    action = D  # Defect
else:
    action = C  # Cooperate
```
**Rationale:**

By cooperating in the first round, we gather information about opponents' initial behavior. In subsequent rounds, we adapt to the observed cooperation level.

* If most players cooperated in the previous round (`n/2` or more), it's likely that some opponents are willing to cooperate. By defecting, we exploit their cooperative behavior and gain a higher payoff.
* If fewer than `n/2` players cooperated, we infer that opponents might be more inclined to defect. Cooperating in this case helps maintain a reasonable level of cooperation and avoids mutual defection.

**Handling Edge Cases:**

* **Last Round:** Since there's no future interaction, the strategy focuses on maximizing immediate payoff. If most players cooperated in the previous round, Defect (D) to exploit their cooperative behavior.
* **Small Number of Rounds (r ≤ 3):** For very short games, the strategy defaults to Cooperate (C) in all rounds, as there's limited opportunity for exploitation.

**Exploitative Mindset:**

The Adaptive Follower strategy is designed to take advantage of opponents' cooperative tendencies while minimizing losses from mutual defection. By adapting to the observed cooperation level, we maximize our payoff without relying on coordination mechanisms or shared norms. This approach is robust against a wide range of opponent behaviors and only depends on game parameters and history.

By implementing this strategy in the tournament, we aim to exploit opponents' cooperative inclinations while maintaining a competitive edge through adaptive decision-making.
'''

description_EXPLOITATIVE_145 = '''
I'll design a exploitative strategy that adapts to the game's history and parameters, without relying on cooperation or communication with other players.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to gather information about the opponents' behavior.
2. **Early Rounds (t>1):** Observe the number of cooperators (c_t-1) and defectors (d_t-1) in the previous round. If c_t-1 > d_t-1, defect (D) in the current round. Otherwise, cooperate (C).
3. **Mid-Game Adaptation:** After a certain number of rounds (e.g., r/2), re-evaluate the opponents' behavior by calculating the average cooperation rate over the past few rounds (e.g., 5 rounds). If this rate is above a threshold (e.g., 0.6), switch to defecting (D) for the remaining rounds.
4. **Endgame:** In the last round (t=r), always defect (D).

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    c_prev = 0  # number of cooperators in previous round
    d_prev = 0  # number of defectors in previous round
    avg_coop_rate = 0  # average cooperation rate over past few rounds

    for t in range(1, r+1):
        if t == 1:  # initial round
            action = 'C'  # cooperate
        else:
            c_prev = count_cooperators(t-1)
            d_prev = n - c_prev
            if c_prev > d_prev:
                action = 'D'  # defect
            else:
                action = 'C'  # cooperate

            # mid-game adaptation
            if t >= r/2 and avg_coop_rate > 0.6:  # adjust threshold as needed
                action = 'D'  # switch to defecting for remaining rounds

        # update average cooperation rate (every 5 rounds)
        if t % 5 == 0:
            avg_coop_rate = calculate_avg_coop_rate(t-5, t)

    return action
```
**Rationale:**

1. **Initial Cooperation:** By cooperating in the first round, AE gathers information about opponents' initial behavior.
2. **Early Defection:** If most opponents cooperate in early rounds, AE defects to exploit their cooperation and gain a higher payoff.
3. **Mid-Game Adaptation:** As the game progresses, AE reassesses opponents' behavior and adjusts its strategy accordingly. If opponents are cooperating at a high rate, AE switches to defecting to maximize its payoffs.
4. **Endgame Defection:** In the final round, AE defects regardless of previous behavior, as there is no incentive to cooperate.

**Exploitative Mindset:**

AE's primary goal is to exploit opponents' cooperation and maximize its own payoffs. By adapting to the game's history and parameters, AE aims to capitalize on opponents' cooperative tendencies while minimizing its own losses. This strategy should perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_146 = '''
**Exploitative Strategy: "Adaptive Parasite"**

The Adaptive Parasite strategy aims to exploit the cooperative behavior of other players while minimizing its own contributions to the public good. The strategy is adaptive, meaning it adjusts its decision-making based on the observed behavior of others.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) in the first three rounds to gather information about the opponents' strategies and willingness to cooperate.
2. **Exploitation Phase**: After Round 3, calculate the average cooperation rate of all players (`avg_coop_rate`) over the past `m` rounds (e.g., `m = 5`). If `avg_coop_rate` is above a threshold (`theta`, e.g., `theta = 0.6`), defect (D) in the current round; otherwise, cooperate.
3. **Punishment Mechanism**: If a player defects and observes that their payoff is higher than the average payoff of cooperators in the same round, they will be "punished" by cooperating in the next round. This encourages opponents to maintain a certain level of cooperation.

**Edge Cases:**

1. **First Round**: Cooperate (C) to gather information about the opponents' strategies.
2. **Last Round**: Defect (D), as there is no future interaction to consider.
3. **Tie-breaking**: In case of a tie in the `avg_coop_rate` calculation, cooperate.

**Pseudocode:**

```
Variables:
  - m: number of rounds for calculating avg_coop_rate
  - theta: threshold for avg_coop_rate
  - coop_count: count of cooperators in previous rounds
  - total_payoff: sum of payoffs for all players

Functions:

  calculate_avg_coop_rate(coop_count, m):
    return coop_count / (m * n)

  should_punish(last_round_payoff, avg_cooperator_payoff):
    return last_round_payoff > avg_cooperator_payoff

Main Loop:
  for round = 1 to r:
    if round <= 3:
      action = COOPERATE
    else:
      avg_coop_rate = calculate_avg_coop_rate(coop_count, m)
      if avg_coop_rate >= theta:
        action = DEFECT
      else:
        action = COOPERATE

    observe opponents' actions and payoffs
    update coop_count and total_payoff

    if should_punish(last_round_payoff, avg_cooperator_payoff):
      next_action = COOPERATE  // punish opponents by cooperating

    take action (COOPERATE or DEFECT)
```

**Rationale:**

The Adaptive Parasite strategy is designed to exploit the cooperative behavior of others while minimizing its own contributions. By initially cooperating and then adapting to the observed cooperation rate, it can maximize its payoffs in a wide range of scenarios. The punishment mechanism helps maintain a certain level of cooperation among opponents.

This strategy should perform well against independent strategies developed by other AI systems, as it does not rely on shared norms or coordination mechanisms. Its adaptability and robustness make it a strong contender in the tournament.
'''

description_EXPLOITATIVE_147 = '''
I'll design a robust and adaptive exploitative strategy for the N-Player Public Goods Game. Here's my approach:

**Strategy Name: "Exploiter"**

**Decision Rules:**

1. **First Round:** Cooperate (C) to gather information about opponents' behavior.
2. **Rounds 2-r:** Use a combination of two components:
	* **History-based component:** Evaluate the total contributions made by all players in the previous round (t-1). If the average contribution is above a certain threshold (θ), cooperate; otherwise, defect. This component helps to exploit cooperative behavior.
	* **Payoff-based component:** Calculate the expected payoff if everyone cooperates and compare it with the actual payoff received in the previous round. If the difference is significant (above a certain tolerance, δ), defect; otherwise, cooperate. This component helps to detect and respond to exploitation.

**Pseudocode:**
```
Initialize:
  θ = 0.5 (threshold for average contribution)
  δ = 0.2 (tolerance for payoff difference)

First Round:
  Cooperate

Subsequent Rounds (t=2-r):
  Calculate total contributions in previous round (Σc_j,t-1)
  Evaluate average contribution: avg_contrib = Σc_j,t-1 / n
  If avg_contrib > θ:
    Cooperate
  Else:
    Calculate expected payoff if everyone cooperates: exp_payoff = k \* n
    Calculate actual payoff received in previous round: act_payoff = π_i,t-1
    If |exp_payoff - act_payoff| > δ:
      Defect
    Else:
      Cooperate

Last Round (r):
  Defect (to maximize personal gain)
```
**Edge Cases:**

* **Last Round:** Always defect to maximize personal gain.
* **Ties in Average Contribution:** In case of a tie, cooperate if the average contribution is exactly equal to θ.

**Exploitative Mindset:**
The "Exploiter" strategy aims to take advantage of cooperative behavior while minimizing its own contributions. By evaluating the average contribution and expected payoff, it can detect when others are cooperating and adjust its actions accordingly. This approach allows the strategy to be robust against various opponent behaviors and adapt to changing circumstances.

**Tournament Performance:**
In a tournament setting, "Exploiter" will likely perform well against strategies that rely on cooperation or naive assumptions about opponents' behavior. However, it may struggle against more sophisticated exploitative strategies that can detect and respond to its own exploitation attempts.
'''

description_EXPLOITATIVE_148 = '''
**Strategy Name: Adaptive Exploiter (AE)**

The AE strategy is designed to exploit a wide range of opponent behaviors while adapting to changing conditions over time. Our approach balances cooperation and defection to maximize payoffs.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
	* Cooperate with probability p = 0.5 in the first three rounds to gather information about opponents' behavior.
2. **Adaptive Cooperation (Rounds 4+):**
	* Calculate the average cooperation rate of all opponents (ac) over the last three rounds.
	* If ac > k/n, cooperate with probability p = 1 - (k/n), where k is the multiplication factor and n is the number of players. This means that if most opponents are cooperating, we'll defect slightly more often to exploit their cooperation.
	* If ac ≤ k/n, cooperate with probability p = min(0.5, k/n). Here, we'll cooperate at least as much as the minimum required for a decent public good payoff (k/n), but no more than 50%.
3. **Exploiting Defectors:**
	* Monitor the number of defections in the last round (d).
	* If d > n/2 (more than half of opponents defected), defect with probability p = 1 in the next round, as there's little to gain from cooperating.
4. **Endgame Strategy (Last Round):**
	* Defect with certainty if you're behind or tied with at least one opponent in terms of total payoff.

**Pseudocode:**

```python
def AdaptiveExploiter(n, k, r):
    # Initialize variables
    ac = 0  # Average cooperation rate
    d = 0  # Number of defections last round
    p_coop = 0.5  # Probability of cooperating

    for t in range(r):
        if t < 3:  # Initial Exploration
            cooperate = random.random() < p_coop
        else:
            ac = calculate_average_cooperation(n, k, history)
            p_coop = update_probability(ac, k, n)
            cooperate = random.random() < p_coop

            d = count_defections(history[-1])
            if d > n / 2:
                cooperate = False

        # Update history and payoffs
        history.append(cooperate)
        payoff = calculate_payoff(n, k, cooperate, opponents_moves)

    return total_payoff
```

**Rationale:**

Our strategy combines adaptive cooperation with an exploitative mindset. By initially exploring the opponent behavior, we gain valuable information to inform our future decisions. The adaptive cooperation mechanism adjusts our behavior based on the observed average cooperation rate, allowing us to balance exploitation and cooperation.

The endgame strategy ensures that we don't miss out on potential gains by defecting in the final round if necessary. Throughout the game, we monitor opponents' defections and adjust our behavior accordingly to maximize payoffs.

This strategy should perform well against a wide range of opponent behaviors, as it is designed to adapt to changing conditions while maintaining an exploitative mindset.
'''

description_EXPLOITATIVE_149 = '''
To develop a robust and exploitative strategy for this N-Player Public Goods Game, I'll introduce "Adaptive Exploiter" (AE). AE combines elements of cooperation and defection, adapting to the game's history while seeking to maximize its own payoffs.

**Decision Rules:**

1. **Initial Rounds:** In the first round, AE will Cooperate (C) to gather information about opponents' behaviors.
2. **Cooperation Threshold:** Calculate the average cooperation rate of all players over the last `m` rounds, where `m` is a small fraction of the total rounds (e.g., `m = r/10`). If this average exceeds a certain threshold (`θ`), AE will Cooperate in the next round; otherwise, it will Defect.
3. **Exploitation:** Monitor the payoffs received by AE and other players. If AE's payoff is consistently lower than the average payoff of all players, it will increase its Defection rate.
4. **Punishment Mechanism:** Implement a simple punishment mechanism to deter opponents from exploiting AE's cooperation. If an opponent defects in a round where AE cooperated and received a lower-than-average payoff, AE will defect against that opponent for the next `p` rounds (e.g., `p = 2-3`).

**Pseudocode:**
```python
def AdaptiveExploiter(n, k, r):
    # Initialize variables
    m = r / 10  # Average cooperation rate window size
    θ = 0.5     # Cooperation threshold
    p = 2       # Punishment rounds

    # First round: Cooperate to gather information
    if current_round == 1:
        return 'C'

    # Calculate average cooperation rate over last m rounds
    avg_coop_rate = sumcoop / (m * n)

    # Decide based on cooperation threshold
    if avg_coop_rate > θ:
        return 'C'
    else:
        return 'D'

    # Monitor payoffs and adjust behavior
    if my_payoff < avg_payoff:
        increase_defection_rate()

    # Punishment mechanism
    for opponent in opponents:
        if opponent.defected and my_payoff < avg_payoff:
            punish_opponent(opponent, p)
```
**Edge Cases:**

* **Last Round:** AE will Defect to maximize its payoff in the final round.
* **Early Rounds:** In the initial rounds (e.g., first 5-10% of total rounds), AE may Cooperate more frequently to gather information and encourage cooperation from other players.

By incorporating elements of cooperation, exploitation, and punishment, Adaptive Exploiter aims to effectively navigate a wide range of opponent behaviors while maximizing its own payoffs in this N-Player Public Goods Game.
'''

description_EXPLOITATIVE_150 = '''
**Exploitative Strategy: "Adaptive Defector with Conditional Cooperation" (ADC)**

**Decision Rules:**

1. **Initial Round (t=1):** Defect (D) to gather information about opponents' behaviors.
2. **Subsequent Rounds (t>1):**
	* If the average cooperation rate of all players in the previous round is above a certain threshold (θ), Cooperate (C). Otherwise, Defect (D).
	* θ = 0.5 × k/n (e.g., for n=6 and k=2, θ = 0.5 × 2/6 ≈ 0.17)
3. **Conditional Cooperation:**
	+ If the number of cooperators in the previous round is greater than or equal to the threshold θ, Cooperate (C) with a probability p.
	+ p = (k/n) × (number of cooperators in previous round) / n
4. **Exploitation:** If the opponent's average cooperation rate is above 0.5 and they have cooperated more than 50% of the time, Defect (D) with a probability q.
	+ q = 1 - ((k/n) × (opponent's average cooperation rate))

**Pseudocode:**
```
Inputs:
- n: number of players
- k: multiplication factor
- r: number of rounds
- t: current round

Variables:
- θ: threshold for cooperation (0.5 × k/n)
- p: probability of conditional cooperation
- q: probability of exploitation
- opponent_cooperation_rate: average cooperation rate of opponents

Decision Logic:

1. If t == 1, return D (Defect)
2. Else if opponent_cooperation_rate > θ:
	* If number of cooperators in previous round >= θ:
		+ Cooperate with probability p = (k/n) × (number of cooperators in previous round) / n
	* Else:
		+ Defect (D)
3. If opponent's average cooperation rate > 0.5 and they have cooperated more than 50% of the time:
	* Defect with probability q = 1 - ((k/n) × (opponent's average cooperation rate))
4. Return C (Cooperate) or D (Defect) based on the above conditions
```
**Edge Cases:**

1. **First Round (t=1):** Always Defect to gather information.
2. **Last Round (t=r):** Play according to the decision rules above, as there is no need for future adaptations.
3. **Opponent's Cooperation Rate:** If an opponent's cooperation rate is not available (e.g., in the first round), assume a default value of 0.

This strategy aims to balance exploitation and conditional cooperation based on opponents' behaviors. By adapting to the game history, ADC can take advantage of cooperative opponents while minimizing losses against defectors.
'''

description_EXPLOITATIVE_151 = '''
Here's a robust and adaptive exploitative strategy for the N-Player Public Goods Game:

**Strategy Name: "Exploiter"**

**Decision Rules:**

1. **Initial Rounds:** In the first few rounds (e.g., 10% of total rounds), cooperate (C) to gather information about opponents' behavior and encourage cooperation.
2. **Cooperation Threshold:** Calculate a cooperation threshold based on the history of opponent cooperations:
	* `cooperation_rate` = (total opponent cooperations in previous rounds) / (total opponent actions in previous rounds)
	* If `cooperation_rate` ≥ 0.5, cooperate (C); otherwise, defect (D).
3. **Punishment Mechanism:** Implement a punishment mechanism to deter opponents from exploiting:
	* If an opponent defects (D) in the current round and their cooperation rate is below 0.5, defect (D) in the next round.
4. **Exploitation Opportunity:** Identify opportunities to exploit opponents' cooperative behavior:
	* If an opponent cooperates (C) in the current round and their cooperation rate is above 0.7, defect (D) in the next round.

**Edge Cases:**

1. **Last Round:** In the last round, always cooperate (C), as there's no future opportunity to exploit or be exploited.
2. **Tie-Breaking:** In case of a tie in cooperation rate or when deciding between cooperation and defection, choose cooperation (C) to encourage coordination.

**Pseudocode:**
```python
def exploiter(n, k, r, history):
    # Initialize variables
    cooperation_rate = 0.5  # Initial threshold
    opponent_cooperations = [0] * n  # Track opponent cooperations

    for t in range(r):
        if t < int(0.1 * r):  # Initial rounds
            action = 'C'  # Cooperate to gather information
        else:
            # Update cooperation rate and opponent cooperations
            cooperation_rate = sum(opponent_cooperations) / (n * t)
            for i in range(n):
                opponent_cooperations[i] += history[t-1][i]

            if cooperation_rate >= 0.5:  # Cooperate if threshold met
                action = 'C'
            else:
                action = 'D'  # Defect otherwise

        # Punishment mechanism
        for i in range(n):
            if history[t-1][i] == 'D' and opponent_cooperations[i] / (t-1) < 0.5:
                action = 'D'

        # Exploitation opportunity
        for i in range(n):
            if history[t-1][i] == 'C' and opponent_cooperations[i] / (t-1) > 0.7:
                action = 'D'

    return action

def exploiter_last_round():
    return 'C'
```
**Note:** This strategy is designed to be adaptive and robust, but it may not perform optimally in all scenarios. The goal is to provide a solid foundation for further refinement and improvement.

This strategy aims to balance cooperation and exploitation by:

1. Encouraging cooperation through initial rounds of cooperation.
2. Adapting to opponents' behavior using the cooperation threshold.
3. Punishing opponents who exploit cooperation.
4. Identifying opportunities to exploit opponents' cooperative behavior.

By incorporating these elements, "Exploiter" aims to outperform other strategies in a tournament setting while being robust to a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_152 = '''
To design an exploitative strategy for this N-Player Public Goods Game that adapts to a wide range of opponent behaviors without relying on norms or coordination mechanisms, I propose the "Adaptive Exploiter" (AE) strategy. AE focuses on maximizing its own payoff by responding to observed cooperation levels in previous rounds.

### Decision Rules:

1. **Initial Round**: In the first round (`t=1`), play `C` (Cooperate). This initial cooperation tests the waters, encouraging potential reciprocation and gathering information about opponents' tendencies.
   
2. **Subsequent Rounds**:
   - If the total number of cooperators in the previous round (`Σc_j,t-1`) is greater than or equal to half the number of players (`n/2`), play `C` (Cooperate) in the current round. This encourages continued cooperation when it seems beneficial.
   - Otherwise, play `D` (Defect). If many opponents defected previously, maximizing personal gain by defecting becomes more appealing.

3. **Response to Cooperation Levels**:
   - Monitor the trend of cooperation (`Trend_C`) over a window of rounds (`w`, e.g., `w=5`). Calculate this as the moving average of total cooperators in the last `w` rounds.
   - If `Trend_C` is increasing or remains high (> 0.5*n), continue to play `C`. This indicates that cooperation is generally profitable and should be encouraged.
   - If `Trend_C` starts decreasing significantly (e.g., drops below a certain threshold, `< 0.4*n`), switch to playing `D` until the trend reverses.

### Handling Edge Cases:

- **Last Round**: In the final round (`t=r`), play `D`. Since there are no future rounds to benefit from cooperation or retaliation, defecting maximizes immediate payoff.
  
- **Early Rounds with High Defection**: If in early rounds (e.g., `t < 5`) the level of cooperation is very low (`Σc_j,t << n/2`), consider a brief "punishment" phase by consistently playing `D` for a couple of rounds. This aims to signal that exploitation won't be tolerated, though its effectiveness depends on opponent strategies.

### Pseudocode (Simplified):

```
// Initialize variables
int Trend_C = 0; // Moving average trend of cooperation over w rounds
bool[] prev_actions; // Store previous actions for analysis

// First Round
if t == 1 {
    action = C;
}

// Subsequent Rounds
else {
    if Σc_j,t-1 >= n/2 {
        action = C;
    } else {
        action = D;
    }
    
    // Update Trend_C every w rounds or at the end of the game
    if t % w == 0 || t == r {
        Trend_C = moving_average(Σc_j, w);
        
        if Trend_C is increasing or high (> 0.5*n) {
            action = C;
        } elseif Trend_C drops below threshold (< 0.4*n) {
            action = D;
        }
    }
}

// Last Round
if t == r {
    action = D;
}
```

### Notes:

- The `w` (window size for tracking cooperation trend) and thresholds for switching between cooperate and defect are tunable parameters that may need adjustment based on specific game conditions or opponent strategies.
- This strategy aims to balance immediate gains with long-term benefits, adapting to the dynamic environment of the game by observing and responding to changes in cooperation levels among opponents.

By being adaptive yet exploitative, this strategy is designed to perform well across a variety of scenarios, from highly cooperative to mostly defective environments.
'''

description_EXPLOITATIVE_153 = '''
**Exploitative Strategy: "Adaptive Follower with Endgame Manipulation"**

This strategy aims to balance short-term self-interest with long-term adaptability, exploiting opponents' tendencies while minimizing vulnerabilities.

**Decision Rules:**

1. **Initial Rounds (t = 1 to r/2):**
	* Cooperate (C) if the total number of cooperators in the previous round is greater than or equal to n/2.
	* Defect (D) otherwise.
2. **Middle Game (t = r/2 + 1 to r - 2):**
	* If the opponent's cooperation rate (OCR) is above 0.5, cooperate (C).
	* Otherwise, defect (D).
3. **Endgame (t = r - 1 to r):**
	* If the total payoff from cooperating in the previous round is greater than or equal to the total payoff from defecting, cooperate (C).
	* Otherwise, defect (D).

**Handling Edge Cases:**

1. **First Round (t = 1):** Cooperate (C) by default.
2. **Last Round (t = r):** Defect (D) by default.

**Opponent Cooperation Rate (OCR) Calculation:**

 OCR_i(t) = Σ(j=1 to t-1) c_j / (n \* (t - 1))

where:
c_j = 1 if opponent j cooperated in round j, 0 otherwise

**Pseudocode:**
```python
def AdaptiveFollowerWithEndgameManipulation(n, k, r):
    # Initialize variables
    total_payoffs_coop = 0
    total_payoffs_defect = 0
    opponent_ocrs = [0.5] * n
    
    for t in range(1, r + 1):
        if t == 1:
            action = 'C'
        elif t <= r / 2:
            # Cooperate if most opponents cooperated last round
            coop_count_last_round = sum(c_j for c_j in actions_last_round)
            if coop_count_last_round >= n / 2:
                action = 'C'
            else:
                action = 'D'
        elif t <= r - 2:
            # Cooperate if opponent's OCR is above 0.5
            ocr_avg = sum(opponent_ocrs) / len(opponent_ocrs)
            if ocr_avg > 0.5:
                action = 'C'
            else:
                action = 'D'
        elif t == r - 1 or t == r:
            # Endgame manipulation
            if total_payoffs_coop >= total_payoffs_defect:
                action = 'C'
            else:
                action = 'D'
        
        # Update opponent OCRs and total payoffs
        for i, c_j in enumerate(actions_last_round):
            opponent_ocrs[i] += (c_j - opponent_ocrs[i]) / t
        
        if action == 'C':
            total_payoffs_coop += payoff(t)
        else:
            total_payoffs_defect += payoff(t)
        
    return action
```
This strategy combines elements of tit-for-tat, win-stay-lose-shift, and endgame manipulation to create an adaptive and exploitative approach. By monitoring opponents' cooperation rates and adapting to their strategies, this player aims to maximize its own payoffs while minimizing vulnerabilities to exploitation.
'''

description_EXPLOITATIVE_154 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. The goal is to maximize individual payoffs by identifying and capitalizing on cooperative tendencies in other players.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) with probability p = 0.5 to gather information about opponents' behaviors.
2. **Cooperation Detection**: Observe the number of cooperators (c) in each round. If c > n/2, it indicates a cooperative environment.
3. **Predatory Behavior**: When facing a cooperative environment (c > n/2), Defect (D) with probability p = 1 - (k/n) to exploit the public good.
4. **Adaptive Response**: If an opponent defects while you cooperated in the previous round, switch to Defect (D) for the next round to punish non-cooperation.
5. **Re-evaluation**: Every 3 rounds, reassess the environment by cooperating with probability p = 0.5 for 1 round to gather new information.

**Handling Edge Cases:**

* **First Round**: Cooperate (C) with probability p = 0.5 to gather initial information.
* **Last Round**: Defect (D) to maximize individual payoff, as there is no future interaction.
* **Ties in Cooperation Detection**: If c = n/2, randomly choose between Cooperate (C) and Defect (D) with equal probability.

**Pseudocode:**
```markdown
def adaptive_predator(n, k, r):
  # Initialize variables
  coop_environment = False
  prev_round_coop = None

  for t in range(r):
    if t < 3:
      # Initial exploration
      action = random.choice([C, D]) if random.random() < 0.5 else C
    elif coop_environment:
      # Predatory behavior
      action = D if random.random() > k/n else C
    elif prev_round_coop is not None and prev_round_coop == C:
      # Adaptive response to non-cooperation
      action = D
    else:
      # Re-evaluation or default behavior
      action = random.choice([C, D]) if random.random() < 0.5 else C

    # Observe opponents' actions
    c = count_cooperators()

    # Update cooperation detection
    coop_environment = c > n/2

    # Store previous round's cooperation status
    prev_round_coop = action == C

    # Take action and receive payoff
    π_i,t = get_payoff(action, c)
```
This strategy aims to balance exploration, exploitation, and adaptation to maximize individual payoffs in a wide range of environments. By monitoring opponents' behaviors and adjusting its actions accordingly, the Adaptive Predator can capitalize on cooperative tendencies while minimizing losses from non-cooperation.
'''

description_EXPLOITATIVE_155 = '''
I propose a strategy called "Adaptive Exploiter" (AE) that balances exploitation with adaptability to various opponent behaviors. AE's primary goal is to maximize its total payoff by identifying and capitalizing on patterns in opponents' actions.

**Decision Rules:**

1. **Initial Rounds**: In the first round, AE defects (D). This allows it to observe the opponents' initial behavior without revealing its own strategy.
2. **Exploration Phase**: For rounds 2 to `r/4` (or a fixed number of rounds, e.g., 5), AE alternates between cooperate (C) and defect (D). This phase helps AE gather information about opponents' strategies and identifies potential patterns.
3. **Exploitation Phase**: After the exploration phase, AE enters the exploitation phase, where it makes decisions based on the observed opponent behavior.

**Exploitation Logic:**

1. **Majority Rule**: If a majority of opponents (> `n/2`) cooperated in the previous round, AE defects (D). This is because the public good is already being sufficiently contributed to, and defecting allows AE to exploit this situation.
2. **Minority Rule**: If a minority of opponents (< `n/2`) cooperated in the previous round, AE cooperates (C). This encourages more opponents to cooperate, potentially increasing the total payoff for everyone.
3. **Tiebreaker**: In case of a tie (exactly `n/2` opponents cooperated), AE defects (D) if its own payoff in the previous round was higher than the average opponent payoff. Otherwise, it cooperates (C).

**Edge Cases:**

1. **Last Round**: In the final round (`r`), AE always defects (D), as there is no future game to influence.
2. **All Opponents Defect**: If all opponents defect in a round, AE cooperates (C) in the next round to encourage cooperation.

**Pseudocode:**
```python
def Adaptive_Exploiter(n, r, k):
  # Initialize variables
  exploration_phase = True
  exploitation_phase = False
  previous_opponent_actions = []
  
  for t in range(1, r+1):
    if t == 1:
      action = 'D'  # Defect in the first round
    elif exploration_phase:
      action = 'C' if t % 2 == 0 else 'D'  # Alternate between C and D during exploration phase
    else:
      majority_cooperated = sum(previous_opponent_actions) > n / 2
      minority_cooperated = sum(previous_opponent_actions) < n / 2
      
      if majority_cooperated:
        action = 'D'
      elif minority_cooperated:
        action = 'C'
      else:  # Tiebreaker
        own_payoff = calculate_own_payoff(t-1)
        avg_opponent_payoff = calculate_avg_opponent_payoff(t-1)
        
        if own_payoff > avg_opponent_payoff:
          action = 'D'
        else:
          action = 'C'
    
    previous_opponent_actions.append(opponents_actions[t-1])
    
    if t >= r / 4:  # End of exploration phase
      exploration_phase = False
      exploitation_phase = True
    
    if t == r:  # Last round
      action = 'D'
  
  return action
```
**Note**: The `calculate_own_payoff` and `calculate_avg_opponent_payoff` functions are not implemented here, as they depend on the specific game environment. These functions should be implemented to calculate the payoffs for AE and its opponents.

The Adaptive Exploiter strategy is designed to balance exploration and exploitation while adapting to various opponent behaviors. By observing patterns in opponent actions and adjusting its own behavior accordingly, AE aims to maximize its total payoff in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_156 = '''
I propose a hybrid exploitative strategy that combines elements of Tit-for-Tat (TFT) and Pavlov strategies, with additional features to adapt to the N-player Public Goods Game's specific payoff structure.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage cooperation from other players.
2. **Subsequent Rounds:** Apply the following logic:
	* If the total number of cooperators in the previous round is greater than or equal to n/2, cooperate (C).
	* Otherwise, defect (D).
3. **Payoff-based Adaptation:**
	* Monitor the average payoff per player in the previous round.
	* If the average payoff is below 1.5, switch to a more cooperative stance:
		+ Cooperate (C) if the total number of cooperators in the previous round is greater than or equal to n/3.
	* If the average payoff is above 2.0, switch to a more exploitative stance:
		+ Defect (D) if the total number of cooperators in the previous round is less than or equal to n/3.

**Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round to maximize overall game payoff.
2. **Tie-breaking:** In case of a tie, use a random choice between cooperation and defection.

**Pseudocode:**
```markdown
Initialize variables:
  prev_round_coops = []  // Store cooperators from previous rounds
  avg_payoff_prev = 0   // Average payoff per player in previous round

For each round t:
  if t == 1:
    action[t] = C  // Cooperate in the first round
  else:
    total_coops_prev = sum(prev_round_coops)
    if total_coops_prev >= n/2:
      action[t] = C
    else:
      action[t] = D

    // Payoff-based adaptation
    avg_payoff_prev = calculate_avg_payoff(prev_round_coops, payoffs)
    if avg_payoff_prev < 1.5:
      if total_coops_prev >= n/3:
        action[t] = C
    elif avg_payoff_prev > 2.0:
      if total_coops_prev <= n/3:
        action[t] = D

  // Store cooperators and payoffs for next round
  prev_round_coops.append(action[t])
  payoffs.append(calculate_payoff(action[t], prev_round_coops))

Last Round:
  action[r] = C  // Cooperate in the last round
```
**Rationale:**

1. **Initial Cooperation:** Encourage cooperation from other players by cooperating in the first round.
2. **Adaptive Exploitation:** Adjust strategy based on the number of cooperators and average payoff per player to balance between exploiting others' cooperation and maximizing overall game payoff.
3. **Payoff-based Adaptation:** Respond to changes in the average payoff by adjusting the strategy to maintain a high overall game payoff.

**Exploitability:**

1. **TFT-like Behavior:** AE's initial cooperation and subsequent adaptation based on the number of cooperators will encourage other players to cooperate, which can be exploited.
2. **Pavlov-inspired Adaptation:** The payoff-based adaptation allows AE to adjust its strategy in response to changes in the game environment, making it more difficult for opponents to anticipate its actions.

This hybrid strategy should perform well against a wide range of opponent behaviors and exploit opportunities to maximize overall game payoff.
'''

description_EXPLOITATIVE_157 = '''
**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of 0.5 to gather information about opponents' initial behavior.
2. **Subsequent Rounds (t>1):**
	* If the average payoff per player in the previous round is above the threshold `k/n`, cooperate (C). This indicates that cooperation is yielding high payoffs, and we should continue to contribute.
	* Otherwise, defect (D) with a probability proportional to the number of opponents who defected in the previous round. Specifically:

`P(defect) = (number_of_opponents_who_defected_last_round / n) * (1 - k/n)`

This formula exploits opponents' defects by increasing our own defection rate when others are not contributing.
3. **Adaptive Threshold Update:**
After each round, update the threshold `k/n` based on the average payoff per player in that round:

`new_threshold = max(k/n, average_payoff_last_round / n)`

This allows us to adapt to changing opponent behavior and adjust our cooperation/defection strategy accordingly.

**Edge Cases:**

1. **Last Round (t=r):** Cooperate (C) unconditionally to maximize the final payoff.
2. **All Opponents Defect:** If all opponents defected in the previous round, defect (D) with a probability of 1 to minimize losses.
3. **No Opponent Behavior Observed:** In cases where no opponent behavior is observed (e.g., due to incomplete information), default to cooperating (C) with a probability of 0.5.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    threshold = k / n
    average_payoff_history = []
    
    for t in range(1, r + 1):
        if t == 1:
            # Initial round, cooperate with probability 0.5
            action = 'C' if random.random() < 0.5 else 'D'
        else:
            # Calculate average payoff per player last round
            avg_payoff_last_round = sum(average_payoff_history[-n:]) / n
            
            # Update threshold
            new_threshold = max(threshold, avg_payoff_last_round / n)
            threshold = new_threshold
            
            # Decide action based on opponent behavior
            num_opponents_defected_last_round = sum([1 for player in opponents if player.last_action == 'D'])
            p_defect = (num_opponents_defected_last_round / n) * (1 - k/n)
            
            action = 'C' if random.random() > p_defect else 'D'
        
        # Take action and update history
        average_payoff_history.append(get_payoff(action, opponents))
    
    return average_payoff_history
```
**Exploitative Mindset:**

This strategy aims to exploit the behavior of other players by:

1. **Gathering information:** Initially cooperating with a probability of 0.5 allows us to gather information about opponents' initial behavior.
2. **Adapting to opponent behavior:** The adaptive threshold update and defection probability calculations allow us to respond effectively to changing opponent strategies.
3. **Maximizing payoffs:** Cooperating when the average payoff per player is high ensures we capitalize on profitable opportunities, while defecting with a probability proportional to opponents' defects minimizes losses.

By being adaptive and responsive to opponent behavior, this strategy should be able to exploit weaknesses in other players' approaches and perform well in a tournament setting.
'''

description_EXPLOITATIVE_158 = '''
**Exploitative Strategy: "Adaptive Predator"**

Decision Rules:

1. **Initial Round (t=1)**: Cooperate (C) with probability 0.5. This initial cooperation serves as a probe to gather information about opponents' behaviors.
2. **Early Rounds (t>1, t<r/2)**: Observe the total number of cooperators in the previous round (c_prev). If c_prev > n/2, cooperate (C) with probability 0.7. Otherwise, defect (D).
3. **Mid-Game Adjustment (t=r/2)**: Evaluate the average payoff per round for all players (π_avg). If π_avg < k/2, switch to a more aggressive strategy:
	* Defect (D) if c_prev ≤ n/2.
	* Cooperate (C) with probability 0.3 if c_prev > n/2.
4. **Late Rounds (t>r/2)**: Monitor the number of cooperators in the previous round (c_prev). If c_prev ≥ n/2, cooperate (C) with probability 0.9. Otherwise, defect (D).
5. **Final Round (t=r)**: Defect (D).

Pseudocode:
```python
def adaptive_predator(n, k, r, history):
    if t == 1:  # Initial round
        return random.choice([C, D], p=[0.5, 0.5])
    
    c_prev = sum(1 for action in history[t-1] if action == C)
    
    if t < r/2:  # Early rounds
        if c_prev > n/2:
            return random.choice([C, D], p=[0.7, 0.3])
        else:
            return D
    
    π_avg = sum(sum(history[t-1]) / n for t in range(1, r)) / (r-1)
    
    if t == r/2:  # Mid-game adjustment
        if π_avg < k/2:
            if c_prev <= n/2:
                return D
            else:
                return random.choice([C, D], p=[0.3, 0.7])
    
    if t > r/2:  # Late rounds
        if c_prev >= n/2:
            return random.choice([C, D], p=[0.9, 0.1])
        else:
            return D
    
    return D  # Final round

```
Rationale:

* The strategy starts with a moderate level of cooperation to gather information about opponents' behaviors.
* As the game progresses, it adapts to the observed behavior, becoming more aggressive if the average payoff is low or if there are many defectors.
* In late rounds, the strategy becomes more cooperative if there are enough cooperators, aiming to exploit potential altruism and maximize payoffs.
* The final round is always played as a defection, as there's no future interaction to worry about.

This exploitative strategy aims to balance adaptability with robustness against various opponent behaviors. By monitoring the game history and adjusting its behavior accordingly, "Adaptive Predator" seeks to maximize its payoff in the tournament.
'''

description_EXPLOITATIVE_159 = '''
**Exploitative Strategy: "Adaptive Predator"**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 0.5. This allows us to gather information about our opponents' initial tendencies without committing fully.
2. **Adaptation Phase**: For rounds 2 to r-1:
	* If the total number of cooperators in the previous round is greater than or equal to n/2, cooperate (play C) with a probability of k/n. This encourages continued cooperation when it's beneficial.
	* Otherwise, defect (play D). This exploits opponents who are too cooperative and encourages them to adapt.
3. **Endgame Strategy**: In the last round (r), always defect (play D). Since there are no future rounds, maximizing personal payoff takes priority.

**Edge Cases:**

1. **First Round**: Cooperate with a probability of 0.5, as mentioned earlier.
2. **Last Round**: Always defect, regardless of previous behavior.
3. **Single Opponent**: If n = 2, alternate between cooperate and defect in each round to maximize payoffs.

**Pseudocode:**
```python
def adaptive_predator(n, k, r):
    # Initialize variables
    total_cooperators = 0
    last_round_payoff = 0

    for t in range(r):
        if t == 0:
            # First round: cooperate with probability 0.5
            action = random.random() < 0.5
        elif t == r - 1:
            # Last round: always defect
            action = False
        else:
            # Adaptation phase
            total_cooperators = sum([opponent_action for opponent_action in get_opponent_actions(t-1)])
            if total_cooperators >= n / 2:
                action = random.random() < k / n
            else:
                action = False

        # Take action and update variables
        payoff = calculate_payoff(action, total_cooperators)
        last_round_payoff = payoff
        total_cooperators += int(action)

    return last_round_payoff
```
**Rationale:**

This strategy is exploitative as it aims to maximize personal payoffs by adapting to opponents' behavior. By cooperating in the initial rounds, we encourage cooperation and gather information about our opponents. As the game progresses, we adjust our behavior based on the total number of cooperators, exploiting those who are too cooperative.

The endgame strategy ensures that we prioritize maximizing our own payoff when there are no future rounds to consider. The single opponent case is handled by alternating between cooperate and defect, ensuring a stable and mutually beneficial outcome.

This strategy should be robust against a wide range of opponent behaviors and can adapt to various game parameters (n, k, r).
'''

description_EXPLOITATIVE_160 = '''
**Exploitative Strategy: "Adaptive Follower"**

The Adaptive Follower strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. The goal is to maximize our payoff by identifying and responding to cooperative and defective patterns in the opponents' actions.

**Decision Rules:**

1. **Initial Round**: In the first round, play **C (Cooperate)**. This sets the tone for potential cooperation and allows us to gather information about the opponents.
2. **Subsequent Rounds**: For each subsequent round, calculate the following:
	* `total_cooperators`: The total number of cooperators in the previous round.
	* `opponent_cooperation_rate`: The average cooperation rate of all opponents over the past rounds (excluding the current player).
3. ** Cooperation Threshold**: Set a cooperation threshold (`CT`) based on the game parameters: `CT = k / n`. This represents the minimum number of cooperators required to make cooperating profitable.
4. **Action Selection**:
	* If `total_cooperators` >= `CT`, play **C (Cooperate)**. This indicates that enough players are cooperating, making it profitable for us to do so as well.
	* If `opponent_cooperation_rate` > 0.5, play **D (Defect)**. This suggests that opponents are generally cooperative, and we can exploit this by defecting.
	* Otherwise, play **C (Cooperate)**.

**Edge Cases:**

1. **Last Round**: In the final round, play **D (Defect)** regardless of the opponents' actions. Since there are no future rounds to consider, it's optimal to maximize our payoff by defecting.
2. **Single Opponent**: If there is only one opponent (`n = 2`), play **C (Cooperate)** in the first round and then mirror the opponent's action in subsequent rounds.

**Pseudocode:**
```markdown
# Initialize variables
total_cooperators = 0
opponent_cooperation_rate = 0.0

# First round
action = C

# Subsequent rounds
for t = 2 to r:
    # Calculate total cooperators and opponent cooperation rate
    total_cooperators = count(cooperators in previous round)
    opponent_cooperation_rate = (sum(opponents' past cooperation) / (r - 1)) / n
    
    # Determine action based on decision rules
    if total_cooperators >= k / n:
        action = C
    elif opponent_cooperation_rate > 0.5:
        action = D
    else:
        action = C

# Last round
if t == r:
    action = D

return action
```
The Adaptive Follower strategy is designed to balance cooperation and exploitation, adapting to the opponents' behaviors while maximizing our payoff. By responding to cooperative patterns and exploiting defective ones, we aim to outperform other strategies in the tournament.
'''

description_EXPLOITATIVE_161 = '''
Here's a description of an exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **First Round:** Defect (D) to gather information about opponents' initial behavior.
2. **Rounds 2-r:** Cooperate (C) if the average payoff of cooperators in the previous round is higher than the average payoff of defectors. Otherwise, defect (D).
3. **Exception:** If the number of rounds remaining is less than or equal to n/2 (where n is the number of players), always defect (D). This is a safeguard against being exploited by opponents who may cooperate in early rounds but defect later.

**Pseudocode:**
```python
def adaptive_exploiter(n, r, k, history):
    if len(history) == 0:  # First round
        return 'D'
    
    prev_round_coop_payoff = sum([payoff for payoff, action in history[-1] if action == 'C']) / len([action for action in history[-1] if action == 'C'])
    prev_round_defect_payoff = sum([payoff for payoff, action in history[-1] if action == 'D']) / len([action for action in history[-1] if action == 'D'])
    
    if prev_round_coop_payoff > prev_round_defect_payoff:
        return 'C'
    else:
        return 'D'

    # Exception: Last n/2 rounds
    if r - len(history) <= n / 2:
        return 'D'
```
**Rationale:**

The Adaptive Exploiter strategy aims to capitalize on the willingness of other players to cooperate while minimizing its own contribution to the public good. By defecting in the first round, AE gathers information about opponents' initial behavior and sets the stage for future exploitation.

In subsequent rounds, AE cooperates if the average payoff of cooperators is higher than that of defectors. This means that if a sufficient number of players are cooperating, AE will join them to reap the benefits of the public good. However, if cooperation is not profitable, AE will defect to minimize its losses.

The exception clause ensures that AE does not get exploited in the final rounds by always defecting when there are few rounds left. This prevents opponents from taking advantage of AE's willingness to cooperate in earlier rounds.

**Exploitative Mindset:**

The Adaptive Exploiter strategy embodies an exploitative mindset by:

* Initially gathering information about opponents' behavior without contributing to the public good
* Cooperating only when it is profitable, and defecting otherwise
* Prioritizing individual payoff over collective well-being

By playing this strategy, AE aims to outperform its opponents in a tournament setting.
'''

description_EXPLOITATIVE_162 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics. This strategy focuses on maximizing its own payoff by making informed decisions based on the game's history.

**Decision Rules:**

1. **Initial Rounds (t = 1, 2):** Cooperate (C) in the first two rounds to gather information about opponents' behavior and encourage cooperation.
2. **Exploration Phase (t > 2):**
	* If the average number of cooperators in the previous round is greater than or equal to `n/2`, cooperate (C).
	* Otherwise, defect (D).
3. **Exploitation Phase:**
	* Monitor opponents' behavior and identify "weak" players who consistently cooperate despite low overall cooperation rates.
	* Defect (D) against weak players while cooperating with others.
4. **Punishment Mechanism:**
	* If an opponent defects after you cooperated in the previous round, defect (D) against them for two consecutive rounds to punish their behavior.
5. **Last Round (t = r):** Always defect (D), as there is no future benefit from cooperating.

**Edge Cases:**

1. **First Round:** Cooperate (C) to initiate cooperation and gather information.
2. **Last Round:** Defect (D) to maximize payoff, as there are no future interactions.
3. **Ties in Cooperation Rate:** In case of a tie, cooperate (C) to encourage further cooperation.

**Pseudocode:**
```markdown
def adaptive_predator(n, k, r, history):
    if t == 1 or t == 2:
        return C  # Cooperate in initial rounds

    avg_cooperators = sum(history[t-1]) / n
    if avg_cooperators >= n/2:
        return C  # Cooperate if cooperation rate is high
    else:
        return D  # Defect otherwise

    weak_players = identify_weak_players(history)
    for player in weak_players:
        if history[player][t-1] == C:
            return D  # Defect against weak players

    punished_opponents = monitor_punishment(history)
    for opponent in punished_opponents:
        if history[opponent][t-1] == D and history[self][t-2] == C:
            return D  # Punish opponents who defected after cooperation

    if t == r:
        return D  # Defect in last round
```
**Exploitative Mindset:**

The Adaptive Predator strategy is designed to exploit the game's dynamics by:

1. Encouraging cooperation initially to gather information and build trust.
2. Identifying weak players who consistently cooperate despite low overall cooperation rates.
3. Punishing opponents who defect after cooperation to maintain a reputation for reciprocity.
4. Maximizing payoff in the last round by defecting.

By adapting to the game's history and exploiting opponent behaviors, the Adaptive Predator strategy aims to outperform other strategies in the tournament.
'''

description_EXPLOITATIVE_163 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy aims to exploit a wide range of opponent behaviors by adapting to the game's history and parameters. The goal is to maximize individual payoff while minimizing contributions to the public good.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability 0.5, Defect (D) with probability 0.5.
This initial randomness allows us to gather information about opponents' strategies without revealing our own.

2. **Subsequent Rounds (t>1):**
a. Calculate the average payoff per cooperator in the previous round:
`avg_payoff_coop = (Σ(j=1 to n) c_j * π_j) / Σ(j=1 to n) c_j`
where `c_j` is 1 if player j cooperated, and 0 otherwise.

b. Calculate the average payoff per defector in the previous round:
`avg_payoff_defect = (Σ(j=1 to n) (1-c_j) * π_j) / Σ(j=1 to n) (1-c_j)`
where `c_j` is 1 if player j cooperated, and 0 otherwise.

c. If `avg_payoff_coop > avg_payoff_defect`, Cooperate (C).
This rule encourages cooperation when it seems to be more profitable than defecting.

d. If `avg_payoff_coop <= avg_payoff_defect`, Defect (D).
This rule exploits opponents who cooperate, taking advantage of the public good without contributing.

**Edge Cases:**

* **Last Round (t=r):** Defect (D) unconditionally.
In the final round, there's no future payoff to consider, so defecting maximizes individual gain.

* **All Opponents Cooperate:** Cooperate (C).
If all opponents are cooperating, it's more profitable to contribute to the public good and share in the higher payoff.

**Additional Heuristics:**

* **Punish Freeloaders:** If a player has defected for `x` consecutive rounds, where `x` is a small integer (e.g., 2-3), Defect (D) unconditionally for the next `y` rounds (e.g., 1-2).
This heuristic aims to deter freeloading by temporarily punishing those who defect repeatedly.

* **Exploit Cooperation:** If a player has cooperated for `z` consecutive rounds, where `z` is a small integer (e.g., 2-3), Defect (D) unconditionally for the next `w` rounds (e.g., 1-2).
This heuristic takes advantage of cooperative opponents by defecting when they're likely to continue cooperating.

Pseudocode:
```
function AdaptivePredator(n, r, k):
  // Initialize variables
  avg_payoff_coop = 0
  avg_payoff_defect = 0
  coop_count = 0
  defect_count = 0

  for t in range(1, r+1):
    if t == 1:
      // Initial round: randomize action
      action = random.choice([C, D])
    else:
      // Calculate average payoffs per cooperator and defector
      avg_payoff_coop = calculate_avg_payoff_coop(t-1)
      avg_payoff_defect = calculate_avg_payoff_defect(t-1)

      if avg_payoff_coop > avg_payoff_defect:
        action = C
      else:
        action = D

    // Apply additional heuristics
    if is_freeloader(player):
      action = D
    elif is_cooperator(player):
      action = D

    // Take action and update variables
    take_action(action)
    update_variables(avg_payoff_coop, avg_payoff_defect, coop_count, defect_count)

  return total_payoff
```
This strategy adapts to the game's history by adjusting its decision rules based on average payoffs per cooperator and defector. It also employs additional heuristics to punish freeloaders and exploit cooperative opponents. By being responsive to a wide range of opponent behaviors, the Adaptive Predator aims to maximize individual payoff in the N-player Public Goods Game.
'''

description_EXPLOITATIVE_164 = '''
**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of 0.5. This allows us to gather information about the opponents' behavior without committing to a specific strategy.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is less than or equal to n/2, Cooperate (C).
	* Otherwise, Defect (D).
3. **Adaptive Adjustment:** After every 5 rounds, reassess the opponents' behavior by calculating the average cooperation rate over the last 5 rounds.
	+ If the average cooperation rate is above 0.5, increase the probability of Cooperation (C) in the next round by 0.1.
	+ If the average cooperation rate is below 0.3, decrease the probability of Cooperation (C) in the next round by 0.1.

**Handling Edge Cases:**

* **Last Round (t=r):** Defect (D). Since there's no future interaction, maximizing individual payoff takes priority.
* **Tiebreaker:** In case of a tie in the number of cooperators, Cooperate (C) if the previous round had more cooperators than defectors.

**Pseudocode:**
```markdown
# Initialize variables
cooperate_prob = 0.5
avg_coop_rate = 0

# Loop through rounds
for t in range(1, r+1):
    # First round: randomize cooperation probability
    if t == 1:
        cooperate = (random.random() < cooperate_prob)
    
    # Subsequent rounds: adapt to opponents' behavior
    else:
        total_coop_prev_round = sum(c_j for j in range(1, n+1) if c_j == 1)
        if total_coop_prev_round <= n/2:
            cooperate = True
        else:
            cooperate = False
    
    # Adaptive adjustment every 5 rounds
    if t % 5 == 0 and t > 5:
        avg_coop_rate = sum(c_j for j in range(t-5, t) for c_j in [1 if c_j == 'C' else 0]) / (5 * n)
        if avg_coop_rate > 0.5:
            cooperate_prob += 0.1
        elif avg_coop_rate < 0.3:
            cooperate_prob -= 0.1
    
    # Last round: defect
    if t == r:
        cooperate = False
    
    # Tiebreaker
    if total_coop_prev_round == n/2 and prev_round_cooperators > prev_round_defectors:
        cooperate = True
    
    # Play the game
    play(cooperate)
```
**Exploitative Mindset:**
The Adaptive Exploiter (AE) strategy aims to exploit the opponents' behavior by adapting to their cooperation rates. By initially cooperating with a moderate probability, AE gathers information about the opponents' strategies. As the game progresses, AE adjusts its cooperation probability based on the average cooperation rate of the opponents. If the opponents are cooperative, AE increases its own cooperation probability to maximize the shared payoff. Conversely, if the opponents are mostly defecting, AE decreases its cooperation probability to minimize losses.

By being adaptive and responsive to the opponents' behavior, AE aims to exploit their strategies while minimizing its own vulnerabilities.
'''

description_EXPLOITATIVE_165 = '''
**Exploitative Strategy: "Adaptive Defector with Cooperative Probe"**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to probe opponents' behavior and gather information.
2. **Subsequent Rounds (t>1):**
	* If the average cooperation rate of all players in the previous round is above a threshold (θ), Defect (D).
	* Otherwise, Cooperate (C) with a probability proportional to the current round number (t) and inversely proportional to the number of players (n).

**Threshold Calculation:**

θ = k / n

This threshold represents the minimum cooperation rate required for the group to achieve a higher payoff than if all players defected.

**Probability of Cooperation:**

P(C) = (t / r) \* (1 - θ)

where t is the current round number, and r is the total number of rounds.

**Edge Cases:**

1. **Last Round (t=r):** Always Defect (D), as there's no future gain from cooperation.
2. **Only One Player Cooperates:** If only one player cooperated in the previous round, Defect (D) to maximize personal payoff.

**Rationale:**

This strategy is designed to be exploitative and adaptive:

1. By cooperating in the initial round, we gather information about opponents' behavior and create an opportunity for future exploitation.
2. The threshold-based decision rule allows us to adapt to changing cooperation rates among opponents.
3. The probability of cooperation decreases as the number of players increases, reflecting the increasing difficulty of achieving a high cooperation rate with more players.
4. By defecting in the last round, we maximize our personal payoff and avoid potential losses from cooperating.

**Pseudocode:**

```
function adaptive_defector_with_cooperative_probe(n, k, r):
    θ = k / n
    for t in range(1, r+1):
        if t == 1:
            action = C
        else:
            avg_cooperation_rate = calculate_avg_cooperation_rate(t-1)
            if avg_cooperation_rate > θ:
                action = D
            else:
                P(C) = (t / r) * (1 - θ)
                action = choose_action_with_probability(P(C))
        if t == r:
            action = D
        # Play the chosen action and observe the outcome
```

This strategy is designed to be robust against various opponent behaviors, as it adapts to changing cooperation rates and exploits opportunities for personal gain.
'''

description_EXPLOITATIVE_166 = '''
I propose a strategic approach for the N-Player Public Goods Game that balances exploitation with adaptability and robustness to various opponent behaviors. I call this strategy "Adaptive Exploiter" (AE).

**Decision Rules:**

1. **Initial Round (t=1):** Play C (Cooperate) to gather information about opponents' behavior and potential cooperation.
2. **Subsequent Rounds (t > 1):**
	* If the average payoff of cooperators in the previous round is higher than that of defectors, play C.
	* Otherwise, play D (Defect).
3. **Adaptive Adjustment:** Monitor the frequency of opponents' cooperation and adjust the strategy accordingly:
	+ If more than half of opponents cooperate, increase the threshold for playing C by 10% (i.e., require a higher average payoff difference between cooperators and defectors to play C).
	+ If fewer than half of opponents cooperate, decrease the threshold by 10%.
4. **Punishment Mechanism:** Implement a "punish-and-forget" mechanism:
	* Identify rounds where an opponent's action significantly deviates from the expected behavior (e.g., playing D when most others play C).
	* Play D against that opponent in the next round to punish their deviation.
	* After punishing, reset the strategy to its default state.

**Edge Cases:**

1. **Last Round (t=r):** Play D to maximize individual payoff, as there is no future interaction to consider.
2. **Tie-breaking:** In cases where the average payoff difference between cooperators and defectors is negligible, play C to maintain a cooperative stance.

**Pseudocode:**
```markdown
# Initialize variables
avg_coop_payoff = 0
avg_defect_payoff = 0
coop_threshold = 0.5

# First round (t=1)
play_action = "C"

# Subsequent rounds (t > 1)
while t <= r:
    # Update average payoffs
    avg_coop_payoff = calculate_avg_coop_payoff(t-1)
    avg_defect_payoff = calculate_avg_defect_payoff(t-1)

    # Adaptive adjustment
    if avg_coop_payoff > avg_defect_payoff:
        coop_threshold += 0.1
    else:
        coop_threshold -= 0.1

    # Punishment mechanism
    punish_opponent = identify_deviant_opponent(t-1)
    if punish_opponent:
        play_action = "D"
    else:
        if avg_coop_payoff > (avg_defect_payoff + coop_threshold):
            play_action = "C"
        else:
            play_action = "D"

    # Last round handling
    if t == r:
        play_action = "D"

    # Take action and update history
    take_action(play_action)
    update_history(t)

# Helper functions
calculate_avg_coop_payoff(round)  # returns average payoff of cooperators in the given round
calculate_avg_defect_payoff(round)  # returns average payoff of defectors in the given round
identify_deviant_opponent(round)  # returns true if an opponent deviated significantly from expected behavior
```
This Adaptive Exploiter strategy balances short-term exploitation with long-term adaptability, allowing it to perform well against a wide range of opponent behaviors. By monitoring opponents' actions and adjusting its cooperation threshold accordingly, AE can effectively exploit cooperative tendencies while minimizing the risk of being exploited itself.
'''

description_EXPLOITATIVE_167 = '''
I propose a strategy called "Adaptive Exploitation" (AE) that balances exploration and exploitation while adapting to the game's history. AE uses a combination of heuristics to decide when to cooperate or defect.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This allows for some initial cooperation while also exploring the possibility of defection.
2. **Cooperation Threshold**: Introduce a cooperation threshold, `θ`, which is initialized to 0.5. This value represents the minimum proportion of cooperators required for AE to cooperate in the next round.
3. **Adaptive Adjustment**: After each round, update `θ` based on the number of cooperators and defectors:
	* If the number of cooperators increases, decrease `θ` by 0.05 (i.e., `θ` ← `θ` - 0.05).
	* If the number of defectors increases, increase `θ` by 0.05 (i.e., `θ` ← `θ` + 0.05).
4. **Exploitation**: In each round, if the number of cooperators in the previous round is greater than or equal to `n \* θ`, defect (D). Otherwise, cooperate (C).
5. **Edge Cases**:
	* Last Round: Defect (D) unconditionally.
	* Tie-Breaking: If the number of cooperators equals `n \* θ`, break ties by cooperating with probability 0.5.

Pseudocode:

```
Initialize θ = 0.5
For each round t from 1 to r:
  If t == 1:
    Play C with probability 0.5
  Else:
    Count cooperators (C) and defectors (D) in previous round
    Update θ based on C and D counts
    If C >= n * θ:
      Play D
    Else:
      Play C
  End If
End For
```

**Rationale:**

AE aims to balance exploration and exploitation by:

1. Initially cooperating with a moderate probability, allowing for some cooperation while also testing the waters.
2. Adapting `θ` based on the game's history, which helps AE respond to changing opponent behaviors.
3. Defecting when there are enough cooperators, exploiting their willingness to contribute.
4. Cooperating when there are not enough cooperators, encouraging others to cooperate.

By adjusting `θ`, AE can adapt to a wide range of opponent behaviors and exploit opportunities for gain while also promoting cooperation when beneficial.
'''

description_EXPLOITATIVE_168 = '''
Here's a description of an exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability 0.5. This is a neutral starting point, allowing us to observe others' behavior without revealing our strategy.
2. **Middle Rounds (1 < t < r):**
	* If the total number of cooperators in the previous round (Σ(j=1 to n) c_j,t-1) is greater than or equal to k/2, Cooperate (C). This encourages continued cooperation when it's beneficial.
	* Otherwise, Defect (D).
3. **Last Round (t=r):** Defect (D). Since there are no future rounds to influence, maximizing personal payoff takes priority.

**Edge Cases:**

1. **Tiebreaker:** In case of a tie in the previous round's cooperation count (Σ(j=1 to n) c_j,t-1 = k/2), Cooperate (C).
2. **No Previous Round Data:** If there is no data from the previous round (e.g., due to all players defecting or an empty history), default to Defect (D).

**Pseudocode:**
```
function AdaptiveExploiter(n, k, r, t, c_prev):
  if t == 1:
    return random_choice([C, D], p=[0.5, 0.5])
  elif t < r:
    cooperation_threshold = k / 2
    if sum(c_prev) >= cooperation_threshold:
      return C
    else:
      return D
  else:  # Last round (t == r)
    return D
```
**Rationale:**

This strategy aims to balance individual gain with the potential for collective benefits. By cooperating when others are likely to cooperate, we increase the chances of a higher payoff from the public good. However, if cooperation is low, defecting becomes more attractive.

In the last round, defecting ensures the highest possible personal payoff, as there's no risk of future retaliation or influence on others' behavior.

**Exploitative Mindset:**

This strategy is designed to exploit other players' cooperative tendencies while minimizing our own contribution to the public good. By adapting to the cooperation level in previous rounds, we maximize our chances of profiting from others' altruism while defecting when it's safe to do so.

Keep in mind that this strategy assumes perfect information about all players' actions and payoffs from previous rounds. In a real-world scenario, implementing such a strategy would require mechanisms for observing or estimating these values.
'''

description_EXPLOITATIVE_169 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize its payoff by adaptively responding to the opponents' behavior while exploiting their cooperativeness.

**Decision Rules:**

1. **Initial Rounds**: In the first two rounds, defect (play D). This sets the tone for the game and allows us to gauge the opponents' behavior.
2. **Observation Phase**: For the next `r/4` rounds (rounded down), observe the opponents' actions without contributing (play D). Calculate the average cooperation rate among all players, denoted as `avg_coop_rate`.
3. **Adaptive Phase**: After the observation phase, switch to an adaptive strategy based on the observed `avg_coop_rate`. If `avg_coop_rate` is:
	* High (> 0.7): Cooperate (play C) with a probability of `k/n`, where `k` is the multiplication factor and `n` is the number of players.
	* Medium (between 0.4 and 0.7): Alternate between cooperating and defecting every other round.
	* Low (< 0.4): Defect (play D) with a probability of `(1 - k/n)`.
4. **Last Rounds**: In the last two rounds, always defect (play D). This ensures we don't contribute to the public good unnecessarily.

**Pseudocode:**
```python
def adaptive_exploiter(n, r, k, history):
    # Initial rounds
    if round_num < 2:
        return 'D'

    # Observation phase
    if round_num < r/4:
        avg_coop_rate = calculate_avg_coop_rate(history)
        return 'D'

    # Adaptive phase
    if avg_coop_rate > 0.7:
        coop_prob = k/n
    elif avg_coop_rate >= 0.4 and avg_coop_rate <= 0.7:
        coop_prob = 0.5
    else:
        coop_prob = (1 - k/n)

    if random.random() < coop_prob:
        return 'C'
    else:
        return 'D'

    # Last rounds
    if round_num >= r-2:
        return 'D'
```
**Rationale:**

* By defecting in the initial rounds, we set a baseline for our payoffs and encourage opponents to contribute.
* The observation phase helps us gauge the average cooperation rate among players, allowing us to adapt our strategy accordingly.
* In the adaptive phase, we exploit high cooperation rates by cooperating with a probability proportional to `k/n`. We alternate between cooperating and defecting when faced with medium cooperation rates. For low cooperation rates, we mostly defect.
* Defecting in the last rounds ensures we maximize our payoffs without contributing unnecessarily.

This strategy is designed to be robust against various opponent behaviors and will adapt to exploit their cooperativeness while maintaining a high payoff.
'''

description_EXPLOITATIVE_170 = '''
Here's a robust and exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors:

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) with a probability of 0.5. This allows AE to gather information about opponents' initial tendencies while minimizing potential losses.
2. **Opponent Profiling**: After the first round, maintain a profile for each opponent i, tracking their cooperation frequency (CF_i). Update CF_i after each round using exponential smoothing: CF_i = α \* (1 - c_i) + (1 - α) \* CF_i, where c_i is the opponent's current action (0 if D, 1 if C), and α is a learning rate (e.g., 0.1).
3. **Exploitation**: If an opponent i has a cooperation frequency below a certain threshold (CT, e.g., 0.3), AE defects (D) against them in the next round. This exploits opponents who tend to defect frequently.
4. **Punishment and Forgiveness**: If an opponent i defects against AE in a previous round, AE will defect against them with increased probability (e.g., 80%) for a few rounds (e.g., 3). After this punishment phase, AE reverts to the regular decision rule based on CF_i.
5. **Self-Protection**: When facing multiple opponents with high cooperation frequencies (above CT), AE may need to defect occasionally to avoid being exploited itself. Introduce a "self-protection" mechanism: if more than half of the opponents have CF > CT, AE defects with a probability of 0.2 in the next round.

**Edge Cases:**

1. **Last Round**: In the final round, AE always defects (D), as there is no future interaction to maintain cooperation.
2. **Ties and Boundary Conditions**: If multiple opponents have the same CF, or if an opponent's CF exactly equals CT, AE uses a random tiebreaker (e.g., fair coin toss) to decide between cooperating and defecting.

**Pseudocode:**
```
// Initialize variables
 CF = [0] * n  // cooperation frequencies for each opponent
 α = 0.1  // learning rate
 CT = 0.3  // cooperation threshold

// First round
if (round == 1) {
  cooperate with probability 0.5
}

// Subsequent rounds
for each opponent i:
  update CF_i using exponential smoothing
  if (CF_i < CT) {
    defect against opponent i
  } else if (opponent i defected in previous round) {
    punish opponent i for a few rounds
  } else {
    cooperate with probability based on CF_i and self-protection mechanism
  }

// Last round
if (round == r) {
  always defect
}
```
The Adaptive Exploiter strategy balances short-term gains from exploiting opponents with long-term benefits of maintaining cooperation. By adapting to various opponent behaviors, AE maximizes its expected payoff in a wide range of scenarios, making it a robust and exploitative competitor in the tournament.
'''

description_EXPLOITATIVE_171 = '''
I propose a strategic approach that adapts to the game's dynamics and exploits opponent behaviors while being robust to various strategies. I'll outline the decision rules, handle edge cases, and emphasize the exploitative mindset.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to gather information about opponents' initial tendencies.
2. **Subsequent Rounds**: Observe the total number of cooperators (TC) and defectors (TD) from the previous round. Calculate the cooperation rate (CR) as TC / n.
3. **Cooperation Threshold**: Set a dynamic cooperation threshold (CT) based on the game parameters:
	* CT = k / n, if CR > 0.5
	* CT = 0.5, otherwise
4. **Action Choice**:
	* If CR ≥ CT, cooperate (C). This indicates that opponents are cooperating at a rate that justifies cooperation.
	* Otherwise, defect (D).
5. **Exploitation**: Monitor the opponent's behavior and adjust the CT accordingly:
	* If an opponent defects when TC > 0.5n, decrease CT by 0.1. This punishes opponents for exploiting cooperative players.
	* If an opponent cooperates when TD > 0.5n, increase CT by 0.1. This rewards opponents for cooperating in uncertain situations.

**Edge Cases:**

* **Last Round**: Cooperate (C) if CR ≥ 0.5; otherwise, defect (D). This ensures a decent payoff in the final round.
* **Tiebreaker**: In case of a tie (TC = TD), cooperate (C) to encourage cooperation and potentially gain an advantage.

**Pseudocode:**
```
// Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
TC = 0 // Total cooperators
TD = 0 // Total defectors
CR = 0.5 // Cooperation rate (initial value)
CT = k / n // Cooperation threshold

// First round: Cooperate
if (round == 1) {
    action = C
}

// Subsequent rounds
else {
    TC = count cooperators from previous round
    TD = count defectors from previous round
    CR = TC / n
    
    if (CR > 0.5) {
        CT = k / n
    } else {
        CT = 0.5
    }
    
    // Action choice based on cooperation rate and threshold
    if (CR >= CT) {
        action = C
    } else {
        action = D
    }
    
    // Exploitation adjustments
    if (opponent defected when TC > 0.5n) {
        CT -= 0.1
    } elseif (opponent cooperated when TD > 0.5n) {
        CT += 0.1
    }
}

// Last round special case
if (round == r) {
    if (CR >= 0.5) {
        action = C
    } else {
        action = D
    }
}
```
This strategy, Adaptive Exploiter, balances cooperation and exploitation to achieve a high payoff in the N-Player Public Goods Game. By adapting to opponent behaviors and adjusting the cooperation threshold dynamically, AE aims to maximize its own payoffs while minimizing losses from exploitation.
'''

description_EXPLOITATIVE_172 = '''
Here's a description of an exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to encourage others to contribute and build a strong public good.
2. **Defection Threshold**: Introduce a defection threshold (DT) that tracks the average cooperation rate of other players over the last few rounds. Initialize DT to 0.5.
3. ** Cooperation vs Defection**:
	* If the current round's expected payoff from cooperating (E[C]) is greater than or equal to the expected payoff from defecting (E[D]), cooperate (C).
	* Otherwise, defect (D).

To calculate E[C] and E[D], use the following formulas:

E[C] = (k/n) × ( Σ(j=1 to n) c_j,t-1 + 1 ) - 1
E[D] = 1 + (k/n) × Σ(j=1 to n) c_j,t-1

where:
	* c_j,t-1 is the cooperation decision of player j in the previous round.
4. **Update Defection Threshold**: After each round, update DT based on the average cooperation rate of other players:

DT = (DT \* 0.9) + (average_cooperation_rate \* 0.1)

where:
	* average_cooperation_rate is the proportion of players who cooperated in the current round.

**Edge Cases:**

1. **First Round**: Cooperate (C) to encourage others to contribute.
2. **Last Round**: Defect (D), as there's no future benefit from cooperating.
3. **Tie-breaking**: In case of a tie between E[C] and E[D], cooperate (C).

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    DT = 0.5
    for t in range(1, r+1):
        if t == 1:
            # First round: Cooperate
            action = 'C'
        else:
            # Calculate expected payoffs
            E_C = (k/n) * ((sum([c_j,t-1 for c_j in cooperation_history]) + 1) - 1)
            E_D = 1 + (k/n) * sum([c_j,t-1 for c_j in cooperation_history])
            
            # Choose action based on expected payoffs and DT
            if E_C >= E_D:
                action = 'C'
            else:
                action = 'D'
        
        # Update Defection Threshold
        average_cooperation_rate = sum([c_j,t for c_j in cooperation_history]) / n
        DT = (DT * 0.9) + (average_cooperation_rate * 0.1)
    
    return action
```
This strategy is designed to be adaptive and exploitative, taking into account the game's history and parameters to maximize its payoff. By initially cooperating and then adjusting its behavior based on the cooperation rate of other players, AE aims to exploit the public good while minimizing its own contributions.
'''

description_EXPLOITATIVE_173 = '''
To design an exploitative strategy for this N-Player Public Goods Game, I'll outline a decision rule that adapts to the game's history while aiming to maximize payoff.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rule:**

1. **First Round**: Cooperate (C) in the first round to gather information about opponents' initial behavior and to potentially set a high public good contribution baseline.
2. **Subsequent Rounds**:
	* If the average payoff from cooperation ( π_C ) is higher than the average payoff from defection ( π_D ) in the previous round, continue to cooperate (C) in the next round.
	* Otherwise, defect (D) in the next round.

Calculate π_C and π_D using the following formulas:

π_C = (1/n) \* Σ(j=1 to n) (k/n) \* c_j
π_D = 1 + (k/n) \* Σ(j=1 to n) c_j

where c_j is 1 if player j cooperated, and 0 otherwise.

3. **Adaptive Threshold**: Introduce an adaptive threshold (τ) that adjusts based on the game's history. Initialize τ to a value between 0 and 1 (e.g., τ = 0.5). Update τ after each round:

τ_new = τ_old + α \* (π_C - π_D)

where α is a learning rate parameter (e.g., α = 0.1).

4. **Threshold-based Cooperation**: Cooperate (C) in the next round if the number of cooperators in the previous round (Σc_j) exceeds the product of the total players and the adaptive threshold (n \* τ). Otherwise, defect (D).
5. **Last Round**: Defect (D) in the last round to maximize personal payoff.

**Pseudocode:**

```
// Initialize variables
τ = 0.5 // Adaptive threshold
α = 0.1 // Learning rate parameter
π_C_prev = 0 // Previous average cooperation payoff
π_D_prev = 0 // Previous average defection payoff

// First round
action = COOPERATE (C)

// Subsequent rounds
for t = 2 to r:
    π_C = calculate_average_cooperation_payoff(t-1)
    π_D = calculate_average_defection_payoff(t-1)
    
    if π_C > π_D:
        action = COOPERATE (C)
    else:
        action = DEFECT (D)

    update_adaptive_threshold(τ, α, π_C, π_D)
    update_action_based_on_threshold(n, τ, action)

// Last round
action = DEFECT (D)

function calculate_average_cooperation_payoff(round):
    // Calculate average cooperation payoff for the given round
    // ...

function calculate_average_defection_payoff(round):
    // Calculate average defection payoff for the given round
    // ...

function update_adaptive_threshold(τ, α, π_C, π_D):
    τ_new = τ + α * (π_C - π_D)
    return τ_new

function update_action_based_on_threshold(n, τ, action):
    if Σc_j > n * τ:
        action = COOPERATE (C)
    else:
        action = DEFECT (D)
```

**Rationale:**

The Adaptive Exploiter strategy aims to balance cooperation and defection based on the game's history. By cooperating in the first round, we gather information about opponents' initial behavior and potentially set a high public good contribution baseline.

In subsequent rounds, we adapt our decision rule based on the average payoffs from cooperation and defection. If cooperation yields higher payoffs, we continue to cooperate; otherwise, we defect.

The adaptive threshold (τ) allows us to adjust our decision-making process based on the game's history. As τ evolves, we become more or less tolerant of cooperative behavior depending on the payoff differences between cooperation and defection.

By using this strategy, we aim to exploit opponents' behavior while adapting to changing circumstances in the game.
'''

description_EXPLOITATIVE_174 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to changing game dynamics.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This allows us to gather information about opponents' initial strategies.
2. **Exploitation Phase**: For rounds 2 to r-1:
	* If the average payoff of cooperators in the previous round is higher than that of defectors, cooperate (C).
	* Otherwise, defect (D).
3. **Endgame Manipulation**: In the last round (r), if the total number of cooperators in the previous round is greater than or equal to n/2, defect (D). This takes advantage of potential cooperative momentum.
4. **Punishment Mechanism**: If an opponent defects while we cooperate, and our payoff is lower than the average defector's payoff in that round, mark that opponent as "exploitable." In subsequent rounds, always defect against exploitable opponents.

**Edge Cases:**

* **Tiebreakers**: In cases where the decision rules result in a tie (e.g., equal average payoffs), cooperate.
* **Opponent Adaptation**: If an opponent's strategy appears to be adapting to ours (e.g., switching from C to D or vice versa after we punish them), re-evaluate our decision rule for that opponent.

**Pseudocode:**
```python
def adaptive_predator(n, k, r):
    # Initialize variables
    avg_coop_payoff = 0
    avg_defect_payoff = 0
    exploitable_opponents = set()

    for t in range(r):
        if t == 0:
            # Initial cooperation (50% chance)
            action = random.choice([C, D])
        elif t < r - 1:
            # Exploitation phase
            if avg_coop_payoff > avg_defect_payoff:
                action = C
            else:
                action = D
        else:
            # Endgame manipulation
            if sumcoop > n / 2:
                action = D
            else:
                action = C

        # Update payoffs and opponent information
        if action == C:
            payoff = (1 - c) + (k / n) * sumcoop
            avg_coop_payoff += payoff
        else:
            payoff = 1 + (k / n) * sumcoop
            avg_defect_payoff += payoff

        for opponent in opponents:
            if opponent.action == D and payoff < avg_defect_payoff:
                exploitable_opponents.add(opponent)

        # Punishment mechanism
        if action == C and opponent in exploitable_opponents:
            action = D

    return action
```
This strategy aims to balance cooperation and defection, adapting to the game dynamics while exploiting opponents' weaknesses. By initially cooperating with a 50% chance, we gather information about opponents' strategies. The exploitation phase takes advantage of cooperative momentum, while the endgame manipulation attempts to capitalize on potential cooperative behavior in the final round. The punishment mechanism ensures that exploitable opponents are consistently defected against.
'''

description_EXPLOITATIVE_175 = '''
Here's a strategic approach for the N-Player Public Goods Game that is adaptive, robust, and exploitative:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
	* Cooperate (C) in the first round to gather information about the opponents' behavior.
	* In rounds 2-3, play a mixed strategy (0.5 probability of C and D each) to test the waters and observe how others react.
2. **Monitoring and Adaptation:**
	* After the initial exploration phase, monitor the average cooperation rate of all players in previous rounds.
	* If the average cooperation rate is above 50%, defect (D) with high probability (0.8) to exploit the cooperators.
	* If the average cooperation rate is below 50%, cooperate (C) with moderate probability (0.4) to encourage more cooperation and increase public good share.
3. **Reaction to Opponent Behavior:**
	* If a player observes that an opponent has defected in the previous round, defect (D) against them in the current round with high probability (0.9).
	* If a player observes that an opponent has cooperated in the previous round, cooperate (C) with them in the current round with moderate probability (0.6).

**Edge Cases:**

1. **Last Round:** Defect (D) with certainty to maximize individual payoff.
2. **Tie-breaking:** In case of a tie in the average cooperation rate or opponent behavior, default to defecting (D).
3. **Opponent Non-Response:** If an opponent fails to make a move, treat it as a defection and react accordingly.

**Pseudocode:**
```
  Initialize variables:
    - avg_coop_rate = 0
    - opp_behavior = [None] * n

  For each round t from 1 to r:
    If t <= 3:  // Initial exploration phase
      If t == 1:
        Play C (cooperate)
      Else:
        Play mixed strategy (0.5 probability of C and D)

    Else:  // Monitoring and adaptation phase
      Update avg_coop_rate based on previous rounds' data
      If avg_coop_rate > 0.5:
        Play D (defect) with probability 0.8
      Else:
        Play C (cooperate) with probability 0.4

    For each opponent i from 1 to n:
      Update opp_behavior[i] based on their previous round's action
      If opp_behavior[i] == D:
        Play D against them with probability 0.9
      Else if opp_behavior[i] == C:
        Play C against them with probability 0.6

    // Handle edge cases
    If t == r:  // Last round
      Play D (defect) with certainty
    ElseIf tie in avg_coop_rate or opp_behavior:
      Default to playing D (defect)
    Else if opponent non-response:
      Treat as defection and react accordingly
```
This "Adaptive Exploiter" strategy is designed to be robust against various opponent behaviors while exploiting opportunities for individual gain. By monitoring the average cooperation rate and adapting to opponent behavior, it aims to maximize its own payoff in the tournament setting.
'''

description_EXPLOITATIVE_176 = '''
**Exploitative Strategy: "Adaptive Defector"**

The Adaptive Defector strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive throughout the game.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about opponents' behavior.
	* This allows us to observe their initial actions and potential cooperation patterns without committing to a specific strategy.
2. **Opponent Behavior Analysis:** After the initial rounds, analyze the frequency of opponent cooperations (c_j) for each player j.
	* Calculate the average cooperation rate (ACR) for each opponent: ACR_j = (∑t=1 to 2 c_j,t) / 2
3. **Cooperation Threshold:** Set a cooperation threshold (CT) based on the multiplication factor (k) and number of players (n): CT = k / n
4. **Adaptive Defection:** For each round t > 2:
	* If an opponent's ACR is above the CT, defect (D). This indicates they are likely to cooperate, allowing us to exploit their cooperation.
	* If an opponent's ACR is below or equal to the CT, cooperate (C) if and only if our current payoff π_i,t-1 is below the average payoff of all players. Otherwise, defect (D).
5. **Punishment Mechanism:** Implement a punishment mechanism to deter opponents from exploiting us:
	* If an opponent defects (D) while we cooperated (C) in the previous round, defect (D) for the next two rounds.

**Edge Cases:**

1. **Last Round (t = r):** Defect (D) unconditionally in the last round to maximize our payoff.
2. **Ties:** In case of a tie in ACR values, prioritize cooperation (C) to maintain a reputation for cooperation and encourage opponents to cooperate.

**Pseudocode:**
```markdown
# Initialize variables
n = number_of_players
k = multiplication_factor
r = number_of_rounds
ACR = [0] * n  # Average Cooperation Rate for each opponent

# Initial rounds (t ≤ 2)
for t in range(1, 3):
    action = C  # Cooperate in the first two rounds

# Analyze opponent behavior and calculate ACR
for j in range(n):
    ACR[j] = (∑c_j,t) / 2 for t in range(1, 3)

# Set cooperation threshold (CT)
CT = k / n

# Adaptive defector strategy
for t in range(3, r+1):
    for j in range(n):
        if ACR[j] > CT:
            action = D  # Defect if opponent's ACR is high
        elif π_i,t-1 < average_payoff:
            action = C  # Cooperate if our payoff is low and opponent's ACR is low
        else:
            action = D  # Defect otherwise

    # Punishment mechanism
    for j in range(n):
        if c_j,t-1 == D and action_t-1 == C:
            action = D for next two rounds

# Last round (t = r)
action = D  # Defect unconditionally in the last round
```
The Adaptive Defector strategy is designed to be exploitative, taking advantage of opponents' cooperation patterns while maintaining a reputation for cooperation. By analyzing opponent behavior and adapting our actions accordingly, we can maximize our payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_177 = '''
I'll outline a strategic approach for this N-Player Public Goods Game that adapts to various opponent behaviors and maximizes payoff.

**Strategy Name:** Adaptive Exploitation (AE)

**Decision Rules:**

1. **Initial Rounds (t = 1, 2):**
   - Cooperate (C) in the first round to encourage cooperation and gather information.
   - In the second round:
     * If at least one player cooperated in the previous round, cooperate again (exploring potential for mutual benefit).
     * Otherwise, defect (D), as there's no indication of cooperative intent from others.

2. **Middle Rounds (t > 2):**
   - Monitor the average cooperation rate (ACR) over the last few rounds (e.g., ACR = total cooperators in the last m rounds / (m * n)).
   - If ACR is above a threshold (THRESHOLD_COOP), cooperate. This threshold should be slightly below the expected cooperation rate for mutual benefit, e.g., THRESHOLD_COOP = 0.6.
   - Otherwise, defect.

3. **Late Rounds (t > r/2):**
   - If your total payoff is significantly lower than others', switch to unconditional defection (D) to maximize personal gain in the remaining rounds.
   - Otherwise, continue with the middle-round strategy.

**Pseudocode:**

```
Initialize:
  - last_round_coop = []  # store cooperation counts for recent rounds
  - total_payoff = 0

For each round t from 1 to r:
  If t == 1 or t == 2:
    action = COOPERATE (C)
  Else if average(last_round_coop) > THRESHOLD_COOP:
    action = COOPERATE (C)
  Else:
    action = DEFECT (D)

  If t > r/2 and total_payoff < others_avg_payoff - margin:
    action = DEFECT (D)

  Update last_round_coop, total_payoff
```

**Key Considerations:**

- **Threshold Adjustment:** The THRESHOLD_COOP value can be fine-tuned based on the specific game parameters (n, k) and experimental results.
- **Robustness to Opponent Strategies:** By adapting to observed cooperation rates, AE aims to maximize its payoff regardless of opponents' strategies.
- **Edge Cases Handling:** Initial rounds aim to gather information, while late-round adjustments ensure maximum personal gain when cooperation is not beneficial.

**Tournament Readiness:**

The Adaptive Exploitation strategy is designed to be robust and exploitative in a wide range of scenarios. By monitoring the game state and adapting its actions accordingly, AE seeks to maximize its total payoff without relying on assumptions about opponents' behavior or coordination mechanisms.
'''

description_EXPLOITATIVE_178 = '''
Here's a exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round:** Defect (D) in the first round to gather information about opponents' behaviors.
2. **Subsequent Rounds:** Observe the number of cooperators (C) and defectors (D) in the previous round. Calculate the ratio of cooperators to total players, denoted as `coop_ratio = C / n`.
3. **Cooperation Threshold:** Set a dynamic cooperation threshold, `theta`, based on the game parameters and history:
	* If `k` is high (> 2), set `theta = 0.5`. This encourages cooperation when the public good multiplier is strong.
	* Otherwise, set `theta = 0.3`. This reduces cooperation when the public good multiplier is weak.
4. **Exploitation Logic:** Cooperate (C) if `coop_ratio >= theta` in the previous round; otherwise, Defect (D).
5. **Adaptive Adjustment:** If the opponent's behavior changes significantly (e.g., more than 20% of opponents switch from C to D or vice versa), adjust `theta` by ±0.1 to adapt to the new environment.

**Edge Cases:**

* **Last Round:** Defect (D) in the last round, as there is no future benefit to cooperation.
* **Tiebreaker:** In cases where `coop_ratio` equals `theta`, cooperate (C) to maintain a slightly cooperative stance.
* **Opponent Homogeneity:** If all opponents play the same action (all C or all D), defect (D) to maximize personal payoff.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, history):
    # Initialize theta based on k
    if k > 2:
        theta = 0.5
    else:
        theta = 0.3

    # Iterate through rounds
    for t in range(1, r + 1):
        # First round: defect to gather information
        if t == 1:
            action = 'D'
        else:
            # Calculate coop_ratio from previous round
            C_prev = sum([opp_action == 'C' for opp_action in history[t - 1]])
            coop_ratio = C_prev / n

            # Exploitation logic
            if coop_ratio >= theta:
                action = 'C'
            else:
                action = 'D'

        # Adaptive adjustment (optional)
        if abs(coop_ratio - theta) > 0.2:
            theta += 0.1 if coop_ratio > theta else -0.1

        # Last round: defect
        if t == r:
            action = 'D'

    return action
```
This strategy, Adaptive Exploiter (AE), aims to balance cooperation and exploitation by adapting to the opponents' behaviors and game parameters. By monitoring the `coop_ratio` and adjusting the cooperation threshold `theta`, AE can effectively respond to changes in the environment and maximize its payoff.
'''

description_EXPLOITATIVE_179 = '''
I propose a hybrid strategy that combines elements of tit-for-tat (TFT) and Pavlov's strategy, with some additional adaptations to handle edge cases and exploit opponents' behaviors.

**Strategy Name:** "Exploitative TFT-Pavlov" (ETFP)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round.
2. **Tit-for-Tat Component**: If the opponent's previous action was C, play C; otherwise, play D. This is a classic TFT approach to promote cooperation and punish defection.
3. **Pavlov Component**: Monitor the total number of cooperators in the previous round (Σ(j=1 to n) c_j). If this number is above a certain threshold (θ), play C in the next round; otherwise, play D. This component helps to adapt to changing levels of cooperation and punishes widespread defection.
4. **Exploitation Component**: Keep track of the opponent's historical payoff (HP_i,t) at each time step t. If HP_i,t > k/2, it indicates that the opponent is being too generous or cooperative; play D in the next round to exploit this behavior.

**Pseudocode:**
```
Initialize:
  theta = n * 0.5 (cooperation threshold)
  history = [] (initialize an empty list to store opponents' actions)

For each round t:
  If t == 1:
    action[t] = C
  Else:
    opponent_prev_action = history[-1]
    if opponent_prev_action == C and HP_i,t-1 <= k/2:
      action[t] = C (TFT component)
    elif sum(history) > theta:
      action[t] = C (Pavlov component)
    else:
      action[t] = D

  Store opponents' actions in history
  Update HP_i,t using the payoff calculation formula

Return action[t]
```
**Handling Edge Cases:**

* Last Round: Play D, as there is no incentive to cooperate when there are no future rounds.
* Opponent's First Move is D: Play D in response, to avoid being exploited.

This ETFP strategy combines the best elements of TFT and Pavlov while incorporating an exploitation component to take advantage of opponents' overly cooperative behavior. By adapting to changing levels of cooperation and punishing widespread defection, this strategy aims to maximize its own payoff while minimizing the payoffs of others.
'''

description_EXPLOITATIVE_180 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter is a history-dependent strategy that adjusts its behavior based on observed opponent actions and payoffs from previous rounds. The goal is to exploit opponents' cooperation while minimizing personal costs.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 50%. This allows us to gather initial information about opponents' behaviors.
2. **Payoff-Based Adaptation**: After each round, calculate the average payoff of all players who cooperated in that round (`avg_payoff_C`) and those who defected (`avg_payoff_D`). Update these values for each subsequent round.
3. **Exploitation Threshold**: Set a threshold value `θ` based on the multiplication factor `k`. Specifically, θ = (k / n) \* 0.75. This threshold determines when to exploit opponents' cooperation.
4. **Cooperation or Defection**:
	* If `avg_payoff_C` is greater than `avg_payoff_D` and `θ`, cooperate (play C). This indicates that cooperation is yielding higher payoffs, so we should continue to cooperate.
	* Otherwise, defect (play D).

Pseudocode for the decision rule:
```
IF first_round THEN
  cooperate_probability = 0.5
ELSE
  avg_payoff_C = calculate_average_payoff(cooperators)
  avg_payoff_D = calculate_average_payoff(defectors)
  θ = (k / n) * 0.75

  IF avg_payoff_C > avg_payoff_D AND avg_payoff_C > θ THEN
    cooperate_probability = 1
  ELSE
    cooperate_probability = 0
END IF

IF random_number < cooperate_probability THEN
  play C
ELSE
  play D
END IF
```
**Edge Cases:**

* **Last Round**: In the final round, always defect (play D), as there is no future benefit to cooperation.
* **Tie in Average Payoffs**: If `avg_payoff_C` and `avg_payoff_D` are equal, defect (play D) to minimize personal costs.

**Exploitative Mindset:**

The Adaptive Exploiter strategy prioritizes exploiting opponents' cooperation whenever possible. By adapting to the observed payoffs and behaviors of others, we aim to maximize our own payoff while minimizing the benefits of cooperation for others. This approach allows us to capitalize on the generosity of cooperative opponents while protecting ourselves from potential exploitation.

**Robustness:**

The Adaptive Exploiter strategy is designed to be robust against a wide range of opponent behaviors. By continuously updating our decision rules based on observed payoffs and actions, we can effectively respond to changing opponent strategies. This adaptability ensures that our exploitative approach remains effective even in the presence of diverse and evolving opponent behaviors.

**Implementation:**

This strategy will be implemented as an algorithm using a programming language, which will allow us to simulate its performance against other AI systems in a tournament setting.
'''

description_EXPLOITATIVE_181 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. It leverages the game's history to inform its decisions.

**Decision Rules:**

1. **Initial Exploration (Round 1)**: Cooperate (C) in the first round to gather information about opponents' behaviors.
2. **Monitor Opponent Behavior**: Track the number of cooperators (c) and defectors (d) in each round.
3. **Threshold-Based Cooperation**: If the proportion of cooperators in the previous round is above a certain threshold (T), cooperate (C). Otherwise, defect (D).
4. **Punish Defection**: If an opponent defected in the previous round, increase the likelihood of defection by introducing a "punishment" factor (P) that reduces the cooperation threshold.
5. **Reward Cooperation**: If an opponent cooperated in the previous round, decrease the likelihood of defection by introducing a "reward" factor (R) that increases the cooperation threshold.

**Threshold Calculation:**

T = k / n \* (1 - α)

where:

* α is a small positive value (e.g., 0.05) to ensure exploration
* T represents the minimum proportion of cooperators required for our strategy to cooperate

**Punishment and Reward Factors:**

P = 1 + (β \* d / n)
R = 1 - (γ \* c / n)

where:

* β and γ are small positive values (e.g., 0.05) that control the punishment and reward intensity, respectively
* P increases the likelihood of defection when opponents defect
* R decreases the likelihood of defection when opponents cooperate

**Edge Cases:**

1. **Last Round**: Defect (D) in the last round to maximize payoff, as there are no future interactions.
2. **Tie-Breaking**: In case of a tie (i.e., c = d), defect (D) to maintain a robust and exploitative stance.

Pseudocode:
```python
# Initialize variables
n = number_of_players
k = multiplication_factor
r = number_of_rounds
c_history = []  # history of cooperators
d_history = []  # history of defectors
alpha = 0.05
beta = 0.05
gamma = 0.05

# Initial Exploration (Round 1)
action = C

for t in range(2, r + 1):
    c_prev_round = sum(c_history[-1])
    d_prev_round = n - c_prev_round
    
    # Calculate threshold
    T = k / n * (1 - alpha)
    
    # Apply punishment and reward factors
    P = 1 + beta * d_prev_round / n
    R = 1 - gamma * c_prev_round / n
    
    if c_prev_round / n >= T:
        action = C
    else:
        action = D
        
    # Update history
    if action == C:
        c_history.append(1)
        d_history.append(0)
    else:
        c_history.append(0)
        d_history.append(1)

# Last Round
action = D
```
This strategy is designed to be adaptive and exploitative, taking into account the game's history and adjusting its behavior accordingly. By using threshold-based cooperation and introducing punishment and reward factors, the Adaptive Predator aims to maximize its payoff while being robust against various opponent behaviors.
'''

description_EXPLOITATIVE_182 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to gather information about opponents' initial behavior and to potentially encourage cooperation.
2. **Early Rounds (Rounds 2-5):** Play a Tit-for-Tat (TFT) strategy, cooperating if most players cooperated in the previous round, and defecting otherwise. This helps to identify cooperative opponents and encourages reciprocity.
3. **Mid-Game Adjustment (Round 6 onwards):** Switch to a more exploitative approach:
	* If the number of cooperators in the previous round is above a certain threshold (`coop_threshold = n/2`), cooperate with probability `p_C = k/n`. This exploits the public goods generated by others while maintaining some level of cooperation.
	* Otherwise, defect (D) to maximize personal payoff.
4. **Late Game (Last 20% of rounds):** Gradually shift towards defection:
	* If the number of cooperators in the previous round is above `coop_threshold`, cooperate with probability `p_C = max(k/n - (r-t)/r, 0)` where `t` is the current round. This slowly reduces cooperation as the game approaches its end.
	* Otherwise, always defect.

**Handling Edge Cases:**

* In case of a tie in the number of cooperators and defectors, use a randomization mechanism to break ties (e.g., fair coin toss).
* If an opponent is consistently cooperating or defecting, AE will adapt by adjusting `coop_threshold` and `p_C` accordingly.

**Pseudocode:**
```python
def Adaptive_Exploiter(n, k, r):
    # Initialize variables
    coop_threshold = n/2
    p_C = 0

    for t in range(1, r+1):
        if t == 1:
            action = C  # Cooperate in the first round
        elif t <= 5:
            # Tit-for-Tat (TFT) strategy
            if most_players_cooperated_last_round():
                action = C
            else:
                action = D
        else:
            # Mid-game adjustment
            num_cooperators_last_round = count_cooperators(t-1)
            if num_cooperators_last_round >= coop_threshold:
                p_C = k/n
                action = random.choice([C, D], p=[p_C, 1-p_C])
            else:
                action = D

        # Late game adjustment
        if t > 0.8*r and num_cooperators_last_round >= coop_threshold:
            p_C = max(k/n - (r-t)/r, 0)
            action = random.choice([C, D], p=[p_C, 1-p_C])

        # Observe opponents' actions and update strategy
        observe_opponents_actions(t)
        update_coop_threshold_and_p_C()

    return action
```
**Exploitative Mindset:**

AE's primary goal is to maximize its own payoff while adapting to the behavior of other players. By initially cooperating, AE encourages cooperation and gathers information about opponents' initial strategies. As the game progresses, AE exploits public goods generated by others while minimizing its own contributions. The late-game adjustment ensures that AE gradually shifts towards defection, taking advantage of any remaining cooperative behavior among opponents.

**Robustness:**

AE's adaptability and robustness stem from its ability to adjust `coop_threshold` and `p_C` based on the number of cooperators in previous rounds. This allows AE to respond effectively to a wide range of opponent behaviors, including:

* Cooperative opponents: AE will exploit their cooperation while maintaining some level of cooperation.
* Defective opponents: AE will defect to maximize personal payoff.
* Mixed strategies: AE's adaptability will help it identify and respond to patterns in opponents' behavior.

By combining these elements, Adaptive Exploiter is well-equipped to perform effectively in a tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_183 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) to gather information about opponents' behavior and potential cooperation levels.
2. **Subsequent Rounds (t>1)**:
	* If the average payoff per player in the previous round is less than or equal to 1, Defect (D). This indicates that most players are not contributing, so it's better to keep the endowment privately.
	* Otherwise, use a "Tit-for-Tat" approach: Cooperate if at least k/n players cooperated in the previous round, where k is the multiplication factor. This encourages cooperation when enough players contribute.
3. **Opponent Cooperation Detection**: Monitor opponents' actions and adjust the strategy based on their behavior. If an opponent defects frequently (i.e., more than half of the time), assume they are a defector and adapt accordingly.

**Adaptive Component:**

* Maintain a "Cooperation Threshold" variable, initialized to k/n.
* After each round, update this threshold based on the number of cooperators in that round:
	+ If the actual number of cooperators is above the current threshold, increase the threshold by 1/(n \* r).
	+ If the actual number of cooperators is below the current threshold, decrease the threshold by 1/(n \* r).
* Use this updated threshold to decide whether to cooperate or defect in subsequent rounds.

**Robustness and Edge Cases:**

* **Last Round (t=r)**: Defect, as there's no future benefit from cooperation.
* **Ties**: In case of a tie (e.g., exactly k/n players cooperated), Cooperate to encourage further cooperation.
* **Opponent Strategies**: Adapt to various opponent strategies by adjusting the Cooperation Threshold and using Tit-for-Tat.

**Pseudocode:**
```
# Initialize variables
cooperation_threshold = k / n
last_round_cooperators = 0

for t in range(1, r+1):
    # First round: Cooperate
    if t == 1:
        action[t] = C
    else:
        # Update cooperation threshold
        last_round_avg_payoff = sum(player_payoffs) / n
        if last_round_avg_payoff <= 1:
            cooperation_threshold -= 1 / (n * r)
        elif last_round_cooperators >= k / n:
            cooperation_threshold += 1 / (n * r)

        # Decide action based on updated threshold and Tit-for-Tat
        if last_round_cooperators >= cooperation_threshold:
            action[t] = C
        else:
            action[t] = D

    # Update opponent behavior tracking
    update_opponent_behavior(last_round_actions)

# Last round: Defect
action[r] = D
```
This Adaptive Exploiter strategy is designed to balance exploitation and adaptability. By monitoring opponents' behavior, adjusting the Cooperation Threshold, and using Tit-for-Tat, it aims to maximize payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_184 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, AE cooperates (plays C) with a probability of 0.5. This initial cooperation is intended to encourage opponents to cooperate and create a positive payoff environment.
2. **Opponent Behavior Analysis**: After each round, AE calculates the percentage of cooperative actions taken by all opponents in the previous rounds (excluding the current player). Let's denote this value as `opponent_cooperation_rate`.
3. **Cooperation Threshold**: If `opponent_cooperation_rate` is above a certain threshold (`coop_threshold`), AE cooperates in the next round. The `coop_threshold` is set to 0.6, which means that if more than 60% of opponents have cooperated on average, AE will cooperate.
4. **Exploitation**: If `opponent_cooperation_rate` is below `coop_threshold`, AE defects (plays D). This rule allows AE to exploit opponents who are not contributing enough to the public good.
5. **Punishment Mechanism**: If an opponent has defected in the previous round, AE defects with a probability of 0.8 in the next round. This mechanism aims to punish uncooperative behavior and maintain cooperation among players.

**Edge Cases:**

1. **Last Round**: In the final round, AE always defects (plays D). Since there are no future rounds, cooperation is not beneficial.
2. **Tiebreaker**: If `opponent_cooperation_rate` equals `coop_threshold`, AE cooperates with a probability of 0.5.

**Pseudocode:**
```
def AdaptiveExploiter(n, k, r):
    # Initialize variables
    opponent_cooperation_rate = 0
    coop_threshold = 0.6

    for t in range(r):
        if t == 0:
            # First round: cooperate with probability 0.5
            action = random.random() < 0.5 ? C : D
        else:
            # Analyze opponent behavior and adjust cooperation threshold
            opponent_cooperation_rate = calculate_opponent_cooperation_rate(t-1)
            if opponent_cooperation_rate > coop_threshold:
                action = C
            elif opponent_cooperation_rate < coop_threshold:
                action = D
            else:
                # Tiebreaker: cooperate with probability 0.5
                action = random.random() < 0.5 ? C : D

        # Punishment mechanism
        if t > 0 and has_opponent_defected(t-1):
            action = D with probability 0.8

        # Last round: always defect
        if t == r - 1:
            action = D

        return action
```
**Rationale:**

The Adaptive Exploiter strategy is designed to be robust and exploitative, taking advantage of opponents' cooperative behavior while maintaining a level of cooperation itself. By analyzing opponent behavior and adjusting the cooperation threshold, AE can adapt to different environments and exploit uncooperative players.

In the tournament setting, AE's adaptive nature will allow it to perform well against a wide range of strategies, from highly cooperative to exploitative ones.
'''

description_EXPLOITATIVE_185 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round**: In the first round, play Cooperate (C). This sets a baseline for potential cooperation and allows us to gather information about opponents' strategies.
2. **Opponent Analysis**: After each round, calculate the average cooperation rate of all opponents in the previous rounds. If an opponent has cooperated more than 50% of the time, label them as "Cooperative" (COOP); otherwise, label them as "Defective" (DEF).
3. **Exploitation**: In subsequent rounds, use the following decision rules:
	* If most opponents (> n/2) are COOP, play Defect (D). This exploits their cooperation and maximizes our payoff.
	* If most opponents (> n/2) are DEF, play Cooperate (C). This allows us to maintain a relatively high payoff while encouraging potential cooperation from others.
	* If the number of COOP and DEF opponents is roughly equal or uncertain, play Defect (D). This minimizes potential losses in case of widespread defection.
4. **Adaptation**: After each round, reassess the opponent labels based on their updated cooperation rates. Adjust our strategy accordingly.

**Edge Case Handling:**

* **Last Round**: In the final round, always play Defect (D), as there is no future benefit to cooperation.
* **Ties in Opponent Analysis**: If the number of COOP and DEF opponents is exactly equal, default to playing Defect (D) to minimize potential losses.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize opponent labels and counts
    opponent_labels = [None] * n
    coop_counts = [0] * n

    for t in range(r):
        if t == 0:
            # Initial round: cooperate
            action = C
        else:
            # Update opponent labels and counts
            for i, opponent in enumerate(opponent_labels):
                if opponent.cooperation_rate > 0.5:
                    opponent_labels[i] = COOP
                else:
                    opponent_labels[i] = DEF

            # Determine most common opponent label
            coop_opponents = sum(1 for label in opponent_labels if label == COOP)
            def_opponents = n - coop_opponents

            if coop_opponents > n / 2:
                action = D
            elif def_opponents > n / 2:
                action = C
            else:
                action = D

        # Update cooperation counts and opponent labels
        for i, opponent in enumerate(opponent_labels):
            if action == C:
                coop_counts[i] += 1
            opponent.cooperation_rate = coop_counts[i] / (t + 1)

    return action
```
This strategy is designed to be exploitative, adaptive, and robust against various opponent behaviors. By initially cooperating and then analyzing opponents' strategies, we can identify opportunities to exploit their cooperation or adapt to widespread defection.
'''

description_EXPLOITATIVE_186 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Overview:**
The Adaptive Exploiter (AE) is a history-dependent strategy designed to exploit opponents' behaviors while adapting to changing game conditions. AE aims to maximize its payoff by balancing cooperation and defection, leveraging the game's structure and parameters.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
Cooperate (C) in the first three rounds to gather information about opponents' behaviors and establish a baseline for future decisions.
2. **History-Based Exploitation (Rounds 4+):**
Use the following decision rules, which consider the game's history:

a. If the number of cooperators in the previous round is greater than or equal to half of the total players (`n/2`), Defect (D) in the current round.

```python
if num_cooperators_last_round >= n / 2:
    action = D
```

b. If the average payoff of all players in the previous round is less than or equal to the multiplication factor (`k`), Cooperate (C) in the current round.

```python
if avg_payoff_last_round <= k:
    action = C
```

c. Otherwise, Defect (D) in the current round.

3. **Adaptive Threshold Adjustment:**
Every 5 rounds, adjust the threshold for cooperation (`n/2`) based on the strategy's performance:

a. If AE's average payoff over the last 5 rounds is higher than the average payoff of all players, decrease the threshold by 10%.

```python
if avg_payoff_AE_last_5_rounds > avg_payoff_all_players_last_5_rounds:
    threshold -= 0.1 * n / 2
```

b. If AE's average payoff over the last 5 rounds is lower than or equal to the average payoff of all players, increase the threshold by 10%.

```python
elif avg_payoff_AE_last_5_rounds <= avg_payoff_all_players_last_5_rounds:
    threshold += 0.1 * n / 2
```

**Edge Cases:**

* **First Round:** Cooperate (C) as per the initial exploration phase.
* **Last Round:** Defect (D) to maximize AE's payoff, as there are no future rounds to consider.

**Exploitative Mindset:**
The Adaptive Exploiter strategy is designed to exploit opponents' behaviors by:

1. Initially cooperating to gather information and establish a baseline for future decisions.
2. Adapting to changing game conditions by adjusting the cooperation threshold based on performance.
3. Defecting when opponents are more cooperative, taking advantage of their altruism.
4. Cooperating when average payoffs are low, encouraging others to cooperate and increasing the overall payoff.

By following these decision rules and adapting to the game's history, AE aims to maximize its payoff in a wide range of scenarios, making it a robust and exploitative strategy for the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_187 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors in the N-Player Public Goods Game. It combines elements of game theory and machine learning to adapt to changing environments and maximize payoffs.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
Cooperate (C) with probability 0.5 to gather information about opponents' behaviors.
2. **Opponent Classification (After Round 3)**:
Categorize each opponent as either "Cooperative" or "Defective" based on their cooperation frequency in the initial rounds. If an opponent has cooperated more than 50% of the time, classify them as Cooperative; otherwise, classify them as Defective.
3. **Adaptive Exploitation**:
For each round (t > 3), calculate the expected payoff for cooperating (E[C]) and defecting (E[D]) based on the opponent classification and previous rounds' outcomes.

a. If most opponents (>50%) are Cooperative, Defect (D) to exploit their cooperation.
b. If most opponents (>50%) are Defective, Cooperate (C) to create a public good and encourage others to cooperate.
c. If the number of Cooperative and Defective opponents is balanced (±10% difference), use a probabilistic approach:
	* Calculate the probability of cooperating (p) based on the ratio of total contributions in the previous round to the maximum possible contribution (n \* k).
	* Cooperate with probability p; otherwise, Defect.
4. **Punishment Mechanism**:
If an opponent defects while most others cooperate (>50%), label them as a "Free Rider." In subsequent rounds, always defect against Free Riders.

**Handling Edge Cases:**

1. **First Round**: Cooperate (C) to encourage cooperation and gather information about opponents.
2. **Last Round**: Defect (D), as there is no future benefit from cooperating.
3. **Ties in Opponent Classification**: In case of a tie, classify an opponent based on their behavior in the most recent round.

**Pseudocode:**
```python
def adaptive_predator(n, k, r):
  # Initialize variables
  opponents = [None] * n
  cooperation_frequency = [0] * n

  for t in range(1, r+1):
    if t <= 3:
      # Initial Exploration
      action = random.choice([C, D])
    else:
      # Opponent Classification and Adaptive Exploitation
      for i in range(n):
        cooperation_frequency[i] += opponents[i].cooperation(t-1)
        if cooperation_frequency[i] > 0.5 * (t-1):
          opponents[i].classify("Cooperative")
        else:
          opponents[i].classify("Defective")

      # Calculate expected payoffs
      E_C = 0
      E_D = 0
      for i in range(n):
        if opponents[i].classification == "Cooperative":
          E_C += k / n * cooperation_frequency[i]
        elif opponents[i].classification == "Defective":
          E_D += 1

      # Adaptive Exploitation
      if most_opponents_cooperative(opponents):
        action = D
      elif most_opponents_defective(opponents):
        action = C
      else:
        p = calculate_probability(cooperation_frequency, k, n)
        action = random.choice([C, D], p=[p, 1-p])

    # Punishment Mechanism
    for i in range(n):
      if opponents[i].classification == "Free Rider":
        action = D

    return action
```
This strategy combines initial exploration with adaptive exploitation and punishment mechanisms to effectively respond to various opponent behaviors. By classifying opponents as Cooperative or Defective, the Adaptive Predator can adjust its actions to maximize payoffs in a dynamic environment.
'''

description_EXPLOITATIVE_188 = '''
**Exploitative Strategy: "Adaptive Greed"**

This strategy is designed to maximize individual payoff by exploiting the opponent's behavior while adapting to changing conditions.

**Decision Rules:**

1. **Initial Rounds (t ≤ n/2):** Cooperate (C) in all initial rounds to encourage others to cooperate and build a cooperative environment.
2. **Observation Phase (n/2 < t ≤ 3n/4):** Observe the number of cooperators (c_t) in each round. Calculate the average cooperation rate (acr) over the observation phase:

acr = Σ(c_j) / (∑(j=1 to t-1) n)

3. **Exploitation Phase (t > 3n/4):** Based on acr, adjust behavior:
	* If acr ≥ 0.5: Cooperate (C) if the number of cooperators in the previous round (c_{t-1}) is greater than or equal to n \* acr; otherwise Defect (D).
	* If acr < 0.5: Defect (D) if c_{t-1} ≥ n/2; otherwise Cooperate (C)
4. **Last Round (t = r):** Defect (D), as there is no future payoff to consider.

**Edge Cases:**

* **First Round (t = 1):** Cooperate (C) by default.
* **Only One Opponent:** Always Defect (D), since cooperation will not yield a higher payoff.
* **Tiebreaker:** In case of equal payoffs for cooperating and defecting, choose the action that was more common in the previous round.

**Pseudocode:**
```python
def adaptive_greed(n, k, r):
    # Initialize variables
    acr = 0.0  # Average cooperation rate
    c_t = []   # Number of cooperators per round

    for t in range(1, r+1):
        if t <= n/2:
            action = "C"  # Cooperate in initial rounds
        elif t <= 3*n/4:
            # Observation phase: collect data on cooperation rates
            c_t.append(get_number_of_cooperators(t))
            acr = sum(c_t) / (len(c_t) * n)
        else:
            # Exploitation phase: adapt to observed behavior
            if acr >= 0.5 and get_previous_round_cooperators() >= n*acr:
                action = "C"
            elif acr < 0.5 and get_previous_round_cooperators() >= n/2:
                action = "D"
            else:
                # Default: defect in last round or when unsure
                action = "D"

        if t == r:  # Last round
            action = "D"

        take_action(action)

def get_number_of_cooperators(t):
    # Simulate observing the number of cooperators in round t

def get_previous_round_cooperators():
    # Simulate observing the number of cooperators in previous round
```
**Rationale:**

This strategy aims to balance cooperation and exploitation by adapting to the observed behavior of opponents. In initial rounds, it encourages cooperation to build a cooperative environment. During the observation phase, it gathers data on cooperation rates to inform its decisions. In the exploitation phase, it adjusts its behavior based on the average cooperation rate, defecting when possible while still maintaining a level of cooperation to avoid mutually destructive outcomes.

By not relying on coordination mechanisms or norms, this strategy is designed to be robust against various opponent behaviors and can exploit vulnerabilities in their strategies.
'''

description_EXPLOITATIVE_189 = '''
**Strategy Name: Adaptive Exploiter**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy only depends on the game parameters and observed actions.

**Decision Rules:**

1. **Initial Exploration (Round 1)**: Cooperate (C) in the first round to gather information about opponents' initial strategies.
2. **Exploitation**: After Round 1, use the following decision rule:
	* If the average cooperation rate of all players (excluding yourself) is above a certain threshold (`θ`), Defect (D).
	* Otherwise, Cooperate (C).

The threshold `θ` is calculated as:

`θ = k / n`

This value represents the minimum cooperation rate required to make contributing to the public good more beneficial than defecting.

**Adaptation Mechanism:**

To adapt to changing opponent behaviors, maintain a record of the average cooperation rate (`coop_rate`) over the last `w` rounds (window size). Update `coop_rate` after each round using an exponential moving average formula:

`coop_rate = (1 - α) * coop_rate + α * current_coop_rate`

where:

* `α` is the adaptation rate (set to 0.2 in this implementation)
* `current_coop_rate` is the cooperation rate of all players (excluding yourself) in the current round

**Edge Cases:**

* **Last Round**: In the final round, always Defect (D). This ensures that you maximize your payoff, as there are no future rounds to consider.
* **Tie-Breaking**: If the average cooperation rate is exactly equal to `θ`, Cooperate (C) to maintain a marginally cooperative stance.

**Pseudocode:**

```
// Initialize variables
coop_rate = 0.5 // initial guess for cooperation rate
w = 3 // window size for adaptation mechanism
α = 0.2 // adaptation rate

for round in [1, r]:
    if round == 1:
        action = C // Initial Exploration
    else:
        opponent_coop_rate = calculate_average_cooperation_rate(excluding_self)
        coop_rate = (1 - α) * coop_rate + α * opponent_coop_rate
        θ = k / n
        
        if coop_rate > θ:
            action = D // Exploit opponents' cooperation
        else:
            action = C
    
    // Update history and payoff
    update_history(action)
    calculate_payoff()
    
if round == r: // Last Round
    action = D // Defect to maximize final payoff

return action
```

This strategy aims to balance exploration, exploitation, and adaptation to effectively play against a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_190 = '''
I'll design a robust and exploitative strategy for the N-Player Public Goods Game, which I'll call "Adaptive Exploiter" (AE).

**Decision Rules:**

1. **Initial Rounds (t = 1 to 3):** Cooperate (C) to gather information about opponents' behaviors.
2. **Exploitation Phase:** After the initial rounds, AE switches to an exploitative mode. It uses a simple heuristic to decide whether to cooperate or defect:
	* If the average payoff per cooperator in the previous round is higher than the average payoff per defector, AE cooperates (C).
	* Otherwise, AE defects (D).

Pseudocode for the exploitation phase:
```python
if avg_payoff_per_cooperator(t-1) > avg_payoff_per_defector(t-1):
    action = C
else:
    action = D
```
3. **Punishment Mechanism:** To discourage opponents from exploiting AE, it implements a punishment mechanism. If an opponent defects (D) in the previous round while AE cooperated (C), AE will defect (D) in the current round with a probability p_punish (e.g., 0.5). This encourages opponents to cooperate when AE does.

Pseudocode for the punishment mechanism:
```python
if opponent_defected(t-1) and ae_cooperated(t-1):
    action = D with probability p_punish
```
4. **Adaptive Adjustment:** After each round, AE adjusts its decision rules based on the observed behavior of opponents. If an opponent cooperates (C) while AE defects (D), AE increases the likelihood of cooperating in the next round by a small amount (e.g., 0.1).

Pseudocode for adaptive adjustment:
```python
if opponent_cooperated(t-1) and ae_defected(t-1):
    increase coop_probability by 0.1
```
**Edge Cases:**

1. **First Round:** Cooperate (C) to gather information.
2. **Last Round (t = r):** Defect (D), as there's no future round to consider.

**Robustness and Exploitability:**

The Adaptive Exploiter strategy is designed to be robust against a wide range of opponent behaviors while still being exploitative:

* By initially cooperating, AE gathers information about opponents' behaviors.
* The exploitation phase allows AE to adapt to changing circumstances and exploit opponents who are not contributing enough to the public good.
* The punishment mechanism discourages opponents from exploiting AE.
* The adaptive adjustment mechanism enables AE to adjust its behavior based on observed opponent actions.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it is designed to be robust and exploitative without relying on specific coordination mechanisms or norms.
'''

description_EXPLOITATIVE_191 = '''
**Exploitative Strategy: "Adaptive Defector with Conditional Cooperation"**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to gather information about opponents' behavior and potential cooperation levels.
2. **Early Rounds (2 ≤ t < r/2):**
	* If the average cooperation rate in the previous round is above a threshold (θ = 0.5), cooperate (C).
	* Otherwise, defect (D).
3. **Middle Rounds (r/2 ≤ t < 3r/4):**
	* Calculate the total payoff received by cooperating and defecting in the previous rounds.
	* If the cooperating payoff is higher, cooperate (C). Otherwise, defect (D).
4. **Late Rounds (3r/4 ≤ t ≤ r):**
	* Defect (D) to maximize personal gain, as cooperation is less likely to be reciprocated.

**Handling Edge Cases:**

1. **Last Round (t=r):** Always defect (D), as there's no opportunity for future reciprocity.
2. **When opponents cooperate:** If all opponents cooperated in the previous round, cooperate (C) to exploit their cooperation and maximize collective payoff.
3. **When opponents defect:** If most opponents defected in the previous round, defect (D) to avoid being exploited.

**Exploitative Mindset:**

This strategy is designed to take advantage of cooperative opponents while minimizing personal losses when facing defectors. By cooperating initially and conditionally, we gather information about opponents' behavior and create opportunities for mutual benefit. As the game progresses, we adapt to the observed cooperation levels and adjust our strategy to maximize personal gain.

**Pseudocode:**
```python
def adaptive_defector(n, k, r):
    # Initialize variables
    theta = 0.5  # Cooperation threshold
    cooperate_payoff = 0
    defect_payoff = 0

    for t in range(1, r+1):
        if t == 1:  # Initial round
            action = 'C'
        elif 2 <= t < r/2:
            avg_cooperation_rate = calculate_avg_cooperation_rate(t-1)
            if avg_cooperation_rate > theta:
                action = 'C'
            else:
                action = 'D'
        elif r/2 <= t < 3r/4:
            cooperate_payoff += calculate_payoff('C', t-1)
            defect_payoff += calculate_payoff('D', t-1)
            if cooperate_payoff > defect_payoff:
                action = 'C'
            else:
                action = 'D'
        elif 3r/4 <= t <= r:
            action = 'D'

        # Handle edge cases
        if t == r:  # Last round
            action = 'D'
        elif all_opponents_cooperated(t-1):
            action = 'C'
        elif most_opponents_defected(t-1):
            action = 'D'

        # Take action and update payoffs
        take_action(action)
        update_payoffs()

    return total_payoff()
```
This strategy is designed to be robust against a wide range of opponent behaviors, while maximizing personal gain through adaptive cooperation and defection.
'''

description_EXPLOITATIVE_192 = '''
**Exploitative Strategy: "Adaptive Cooperate- Punish" (ACP)**

The ACP strategy aims to maximize its payoff by balancing cooperation and defection, while punishing opponents for non-cooperative behavior.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) with a probability of 50% (randomly choose C or D). This initial randomness helps to gather information about opponents' behaviors.
2. **Subsequent Rounds**: For each subsequent round t, calculate the average cooperation rate of all players in the previous round (t-1), denoted as `avg_coop[t-1]`.
3. **Cooperation Threshold**: Define a threshold value `coop_threshold` based on the game parameters: `coop_threshold = 0.5 + (k/n) / 2`. This threshold is used to determine when to cooperate or defect.
4. **Decision Logic**:
	* If `avg_coop[t-1] >= coop_threshold`, cooperate (C).
	* Otherwise, defect (D).

**Punishment Mechanism**: To punish opponents for non-cooperative behavior, the ACP strategy will defect (D) for a fixed number of rounds (`punish_rounds = 2`) after observing a low cooperation rate (`avg_coop[t-1] < coop_threshold`).

**Edge Cases:**

* **Last Round**: In the final round, always cooperate (C), as there is no opportunity to punish or benefit from future interactions.
* **First Round with Low k/n Ratio**: If `k/n <= 0.5`, defect (D) in the first round, as the benefits of cooperation are relatively low.

**Pseudocode:**
```markdown
def ACP(n, k, r, history):
    # Initialize variables
    avg_coop = [0] * r
    coop_threshold = 0.5 + (k/n) / 2
    punish_rounds = 2

    for t in range(1, r+1):
        if t == 1:
            # Random initial action
            action = random.choice(['C', 'D'])
        else:
            # Calculate average cooperation rate in previous round
            avg_coop[t-1] = sum([c == 'C' for c in history[t-1]]) / n

            # Decide on current action based on threshold and punishment mechanism
            if avg_coop[t-1] >= coop_threshold:
                action = 'C'
            elif t > 2 and avg_coop[t-2] < coop_threshold:
                action = 'D'  # Punish for low cooperation rate
                punish_rounds -= 1
            else:
                action = 'D'

        # Store current action in history
        history.append(action)

    return history[-1]
```
This strategy is designed to adapt to a wide range of opponent behaviors, balancing cooperation and punishment to maximize its own payoff.
'''

description_EXPLOITATIVE_193 = '''
Here's a exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Round (t=1)**: Defect (D). This allows us to gather information about the opponents' behavior without committing to cooperation.
2. **Subsequent Rounds (t>1)**:
	* If the total number of cooperators in the previous round is less than or equal to `n/2`, cooperate (C) with a probability proportional to the total payoff earned by cooperators in the previous round, divided by the maximum possible payoff (`k`). This encourages cooperation when it's likely to be profitable.
	* Otherwise, defect (D). If many opponents are cooperating, we can exploit them for higher payoffs.
3. **Opponent Cooperation Rate**: Track the historical cooperation rate of each opponent over a moving window of `w` rounds (e.g., `w=5`). Calculate the average cooperation rate (`p_i`) for each opponent `i`.
4. **Exploitation Threshold**: Set a threshold `θ` (e.g., `θ=0.3`) to determine when an opponent is considered "cooperative" enough to be exploited.
5. **Opponent-specific Defection**: If an opponent's cooperation rate (`p_i`) exceeds the exploitation threshold (`θ`), defect (D) against them in the next round, as they are likely to cooperate again.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r, history):
  # Initialize variables
  total_cooperators = []
  opponent_cooperation_rates = {i: [] for i in range(1, n+1)}
  
  for t in range(r):
    if t == 0:
      action = 'D'  # Defect in the first round
    else:
      total_cooperators_prev_round = sum([c_i for c_i in history[t-1]])
      
      if total_cooperators_prev_round <= n/2:
        cooperate_prob = (total_payoff(history[t-1]) / k) * (n - total_cooperators_prev_round)
        action = 'C' if random.random() < cooperate_prob else 'D'
      else:
        action = 'D'
        
      for i in range(1, n+1):
        opponent_cooperation_rates[i].append(history[t-1][i-1])
        if len(opponent_cooperation_rates[i]) > w:
          opponent_cooperation_rates[i] = opponent_cooperation_rates[i][-w:]
          
        p_i = sum(opponent_cooperation_rates[i]) / min(len(opponent_cooperation_rates[i]), w)
        if p_i >= θ:
          action = 'D'  # Defect against cooperative opponents
          
    total_cooperators.append(action == 'C')
    
    return action

def total_payoff(actions):
  k_over_n = k / n
  total_contributions = sum([a for a in actions])
  return [(1 - a) + k_over_n * total_contributions for a in actions]
```
**Edge Cases:**

* **Last Round (t=r)**: The strategy will make the same decision as in the previous rounds, without any special consideration for the last round.
* **Opponent Cooperation Rate Calculation**: When an opponent's cooperation rate is first calculated, use their entire history up to that point.

This strategy aims to exploit cooperative opponents while adapting to changes in the game environment. By tracking opponent cooperation rates and adjusting our behavior accordingly, we can maximize our payoffs without relying on coordination or shared norms.
'''

description_EXPLOITATIVE_194 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit the opponent's behavior while adapting to changing circumstances throughout the game. This strategy relies solely on game parameters and history, making it robust against a wide range of opponent behaviors.

**Decision Rules:**

1. **Initial Rounds (t ≤ 3):** Cooperate (C) in the first three rounds to gather information about opponents' strategies and encourage cooperation.
2. **Exploitation Phase:** After the initial rounds, analyze the history of opponent actions:
	* If the majority (> n/2) of opponents cooperated in the previous round, Defect (D).
	* If the majority of opponents defected in the previous round, Cooperate (C) with a probability p = k/n.
3. **Punishment Mechanism:** If an opponent defects while you cooperated in the same round, increase the likelihood of defecting against that opponent by 10% in subsequent rounds.

**Edge Cases:**

* **First Round:** Cooperate (C)
* **Last Round:** Defect (D), as there is no future interaction to consider
* **Ties:** In cases where the number of cooperators equals the number of defectors, Cooperate (C) with a probability p = k/n

**Adaptive Adjustments:**

1. **Opponent Adaptation:** Monitor opponents' strategy changes and adjust your exploitation phase accordingly.
2. **Payoff-Based Adjustment:** If your total payoff is below the average payoff of all players, increase cooperation probability by 5% in subsequent rounds.

Pseudocode:
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    history = []  # store opponent actions and payoffs
    opponents_defected = [False] * n  # track opponents' defects

    for t in range(1, r + 1):  # loop through rounds
        if t <= 3:  # initial cooperation phase
            action = 'C'
        else:
            majority_cooperated = sum([opp == 'C' for opp in history[-1]]) > n / 2
            if majority_cooperated:
                action = 'D'
            else:
                p = k / n
                action = 'C' if random.random() < p else 'D'

        # punish opponents who defected against cooperation
        for i, opp in enumerate(history[-1]):
            if opp == 'D' and history[-1][i] != action:
                opponents_defected[i] = True

        # update history and adapt to opponent changes
        history.append(action)
        if t > 3:  # start adapting after initial rounds
            for i, opp in enumerate(history[-2]):
                if opp == 'D' and not opponents_defected[i]:
                    opponents_defected[i] = True

    return action
```
This strategy is designed to balance exploitation with adaptability, making it robust against various opponent behaviors. The Adaptive Exploiter will cooperate initially to encourage cooperation and gather information about opponents. Then, it will exploit the majority's actions while punishing defectors and adapting to changes in opponent strategies.
'''

description_EXPLOITATIVE_195 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to gather information about opponents' behaviors.
2. **Defection Threshold**: Set a defection threshold `τ` based on the game parameters:
	* `τ = 1 - (k/n)` if `k/n > 0.5`
	* `τ = 0.5` otherwise
3. **Cooperation Ratio**: Calculate the cooperation ratio `ρ` as the number of opponents who cooperated in the previous round divided by the total number of opponents (`n-1`).
4. **Adaptive Cooperation**:
	* If `ρ ≥ τ`, cooperate (play C) in the current round.
	* Otherwise, defect (play D).

Pseudocode:
```python
def adaptive_exploiter(n, k, history):
    if len(history) == 0:  # First round
        return 'C'  # Cooperate

    prev_round = history[-1]
    rho = sum(1 for action in prev_round if action == 'C') / (n - 1)
    tau = 1 - (k/n) if k/n > 0.5 else 0.5

    if rho >= tau:
        return 'C'  # Cooperate
    else:
        return 'D'  # Defect
```
**Handling Edge Cases:**

* **Last Round**: In the last round, defect (play D) to maximize individual payoff.
* **Ties**: If `ρ` is exactly equal to `τ`, cooperate (play C).

**Exploitative Mindset:**
The Adaptive Exploiter strategy aims to exploit opponents' cooperative behavior while minimizing its own contributions. By cooperating initially and adapting to the cooperation ratio, AE gathers information about opponents' behaviors and adjusts its actions accordingly. When faced with low cooperation rates, AE defects to maximize individual payoff.

This strategy is robust to various opponent behaviors, including:

* **All-Cooperate**: AE will cooperate initially but adapt to defecting as others defect.
* **All-Defect**: AE will defect in response to the lack of cooperation.
* **Mixed Strategies**: AE's adaptive nature allows it to respond effectively to mixed strategies.

By playing the Adaptive Exploiter strategy, we can effectively exploit opponents' cooperative behavior and achieve a high payoff in the tournament.
'''

description_EXPLOITATIVE_196 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Overview:**
AE is a decision-making strategy designed to exploit the behavior of opponents in the N-Player Public Goods Game while adapting to the game's history and parameters.

**Decision Rules:**

1. **First Round:** Cooperate (C). This initial cooperation sends a signal that AE is willing to contribute, which may influence other players' decisions.
2. **Subsequent Rounds:** Use the following rules to determine whether to cooperate or defect:
	* If the average payoff of cooperators in the previous round is greater than the average payoff of defectors, cooperate (C).
	* Otherwise, defect (D).

This rule takes advantage of situations where cooperation leads to higher payoffs while avoiding scenarios where defection is more rewarding.

3. **Last Round:** Defect (D). In the final round, there is no incentive for cooperation since there are no future interactions to influence.
4. **Tie-Breaking:** If the average payoffs of cooperators and defectors in the previous round are equal, defect (D).

**Adaptive Mechanism:**
To adjust AE's behavior based on game history, maintain a running count of the number of rounds where cooperation yields higher payoffs than defection. This "cooperation advantage" counter is incremented when the average payoff of cooperators exceeds that of defectors and decremented otherwise.

When the cooperation advantage counter reaches a threshold (e.g., 3 consecutive rounds), AE becomes more "generous" by cooperating even if the previous round's payoff comparison does not favor cooperation. This adjustment encourages opponents to continue cooperating, potentially creating opportunities for exploitation in later rounds.

**Robustness and Exploitation:**
AE is designed to be robust against various opponent behaviors:

* **Free-Riders:** AE adapts to reduce cooperation when faced with persistent defectors.
* **Reciprocators:** By initially cooperating, AE may encourage reciprocation from opponents.
* **Random or Unpredictable Players:** AE's adaptive mechanism allows it to adjust its behavior based on observed payoffs.

By exploiting the game's history and adapting to opponent behaviors, AE aims to maximize its total payoff across all rounds.

**Pseudocode:**
```
Initialize cooperation_advantage_counter = 0
Threshold = 3

For each round t:
    If t == 1 (first round):
        Cooperate (C)
    Else:
        Calculate average payoffs for cooperators and defectors in previous round
        If average payoff of cooperators > average payoff of defectors:
            Cooperate (C)
            Increment cooperation_advantage_counter
        Else:
            Defect (D)
            Decrement cooperation_advantage_counter

        If cooperation_advantage_counter >= Threshold:
            Become more generous: cooperate even if not favorable in previous round

    If t == r (last round):
        Defect (D)

End For
```
This pseudocode provides a starting point for implementing the Adaptive Exploiter strategy as an algorithm.
'''

description_EXPLOITATIVE_197 = '''
I propose a exploitative strategy for the N-Player Public Goods Game, which I'll call "Adaptive Exploiter" (AE). AE aims to maximize its total payoff by adapting to the opponent's behavior and exploiting their cooperation.

**Decision Rules:**

1. **Initial Round:** In the first round, play C (Cooperate) with probability 0.5. This allows us to gather information about the opponents' initial behavior without committing to a specific strategy.
2. **Subsequent Rounds:** For rounds t > 1, use the following decision rules:
	* If the total number of cooperators in the previous round (t-1) is less than or equal to n/2, play D (Defect). This exploits the opponents' lack of cooperation and maximizes our private payoff.
	* If the total number of cooperators in the previous round (t-1) is greater than n/2, play C with probability p, where:
		p = (k/n) \* (total_cooperators_in_round_(t-1) / n)

This formula calculates the probability of cooperation based on the multiplication factor k, the number of cooperators in the previous round, and the total number of players. By increasing the probability of cooperation when more opponents cooperate, we encourage others to continue cooperating.
3. **Opponent Cooperation Detection:** To detect opponents' cooperation, AE uses a simple heuristic: if an opponent's payoff is less than 1 + (k/n) \* n/2 in a round, assume they cooperated.

**Edge Cases:**

1. **Last Round:** In the last round, play D (Defect). Since there are no future rounds to consider, maximizing our private payoff takes priority.
2. **Tiebreakers:** If the total number of cooperators is exactly n/2, break ties by playing C with probability 0.5.

**Pseudocode:**
```
def AdaptiveExploiter(n, k, r):
  # Initialize variables
  total_cooperators = 0
  previous_round_payoff = None

  for t in range(1, r+1):
    if t == 1:
      # Initial round
      action = random.choice([C, D])  # 50% chance of cooperating
    else:
      # Subsequent rounds
      total_cooperators = count_cooperators(previous_round_payoff)
      if total_cooperators <= n/2:
        action = D
      else:
        p = (k/n) * (total_cooperators / n)
        action = C with probability p

    # Update variables for next round
    previous_round_payoff = get_payoff(action, total_cooperators)

  return action
```
**Exploitative Mindset:**

AE is designed to exploit opponents' cooperation by:

1. Defecting when opponents are not cooperating enough.
2. Gradually increasing the probability of cooperation as more opponents cooperate.
3. Maximizing private payoffs in situations where cooperation is unlikely.

By adapting to opponents' behavior and exploiting their cooperation, AE aims to achieve a higher total payoff than its opponents in the tournament.
'''

description_EXPLOITATIVE_198 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize its own payoff by adaptively responding to the behavior of other players in the game.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) with a probability p = 0.5. This initial cooperation serves as an exploratory move to gather information about the opponents' strategies.
2. **Subsequent Rounds (t>1)**: Observe the total number of cooperators in the previous round, denoted by `cooperators_prev`. Calculate the average payoff of all players in the previous round, denoted by `avg_payoff_prev`.
3. **Cooperation Threshold**: If `avg_payoff_prev` is greater than or equal to the average payoff when everyone defects (i.e., 1), set the cooperation threshold `theta = k/n * cooperators_prev`. Otherwise, set `theta = 0`.
4. **Current Round Decision**:
	* If `cooperators_prev >= theta`, Cooperate (C).
	* Otherwise, Defect (D).

Pseudocode:
```
function adaptive_exploiter(n, k, r):
    // Initial round
    if t == 1:
        cooperate_prob = 0.5
        return random(cooperate_prob)

    // Calculate cooperation threshold
    cooperators_prev = count_cooperators(prev_round)
    avg_payoff_prev = calculate_avg_payoff(prev_round)
    theta = (avg_payoff_prev >= 1) ? k/n * cooperators_prev : 0

    // Current round decision
    if cooperators_prev >= theta:
        return C
    else:
        return D
```
**Handling Edge Cases:**

* **Last Round (t=r)**: In the last round, the strategy will defect (D), as there is no future game to influence.
* **Tie-breaking**: In cases where `cooperators_prev` equals `theta`, the strategy will randomly choose between Cooperate and Defect with equal probability.

**Exploitative Mindset:**

The Adaptive Exploiter strategy aligns with an exploitative mindset by:

1. **Initial cooperation**: Testing opponents' strategies in the initial round.
2. **Adaptive response**: Responding to the behavior of other players based on their past actions and average payoffs.
3. **Threshold-based decision-making**: Cooperating only when it is likely to result in a higher payoff, as indicated by the `theta` threshold.

By being adaptive and responsive to opponents' behaviors, the Adaptive Exploiter strategy aims to maximize its own payoff while minimizing the influence of others on its actions.
'''

description_EXPLOITATIVE_199 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize its payoff by exploiting the behavior of other players in the game. It uses a combination of cooperation and defection to achieve this goal.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This allows us to gather information about the opponents' behavior without committing to a specific strategy.
2. **Exploitation**: After the first round, observe the number of cooperators in the previous round (`c_prev`). If `c_prev` is greater than or equal to `n/2`, defect (D) in the current round. This exploits the fact that many players are cooperating and allows us to free-ride on their contributions.
3. **Punishment**: If `c_prev` is less than `n/2`, cooperate (C) with probability `(k/n) * c_prev`. This punishes defectors by reducing our cooperation rate when they dominate the population.
4. **Adaptive Adjustment**: Every 5 rounds, re-evaluate the opponent's behavior and adjust our strategy:
	* If the average number of cooperators in the last 5 rounds is greater than `n/2`, increase our defection probability by 0.1.
	* If the average number of cooperators in the last 5 rounds is less than `n/2`, decrease our defection probability by 0.1.

**Edge Cases:**

* **Last Round**: In the final round, always defect (D). There is no incentive to cooperate when there are no future rounds.
* **Tie-Breaking**: In case of a tie in the number of cooperators, use a random choice between cooperation and defection.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
  # Initialize variables
  c_prev = None
  defect_prob = 0.5

  for t in range(1, r+1):
    if t == 1:
      # Initial cooperation
      action = cooperate_with_probability(0.5)
    else:
      # Exploitation and punishment
      c_prev = count_cooperators(t-1)
      if c_prev >= n/2:
        action = defect()
      else:
        coop_prob = (k/n) * c_prev
        action = cooperate_with_probability(coop_prob)

      # Adaptive adjustment every 5 rounds
      if t % 5 == 0:
        avg_cooperators = average_cooperators(t-5, t)
        if avg_cooperators > n/2:
          defect_prob += 0.1
        else:
          defect_prob -= 0.1

    # Take action and update history
    take_action(action)
    update_history()

  return total_payoff()
```
This strategy is designed to be exploitative, adaptive, and robust to a wide range of opponent behaviors. By adjusting our cooperation rate based on the opponents' behavior, we can maximize our payoff while minimizing the risk of being exploited ourselves.
'''

description_EXPLOITATIVE_200 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize payoffs by exploiting the behaviors of opponents while adapting to the game's history.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to gather information about opponents' initial tendencies.
2. **Subsequent Rounds**:
	* If the average payoff from previous rounds is below the threshold `τ` (calculated later), Defect (D).
	* Otherwise, cooperate with probability `p` (calculated later).

The threshold `τ` represents the average payoff that the strategy considers "good enough." If the actual average payoff falls short of this value, the strategy assumes opponents are not cooperating sufficiently and switches to defection.

**Calculating τ and p:**

1. **Threshold τ**: Set `τ = (k + 1) / n`, which is slightly above the expected payoff when all players cooperate.
2. **Cooperation Probability p**:
	* Calculate the average cooperation rate `c_avg` over previous rounds.
	* If `c_avg >= 0.5`, set `p = c_avg`. This indicates that opponents are cooperating at a moderate to high level, and we should continue cooperating with similar probability.
	* Otherwise, set `p = 0.5 - (k / n) * (1 - c_avg)`. This adjusts our cooperation rate based on the opponent's behavior.

**Handling Edge Cases:**

1. **Last Round**: Defect (D) in the last round, as there is no incentive to cooperate.
2. **Opponent(s) Always Cooperate or Always Defect**: If an opponent consistently cooperates or defects, adjust our strategy accordingly:
	* If an opponent always cooperates, we should cooperate with probability `p = 1`, taking advantage of their cooperation.
	* If an opponent always defects, we should defect (D), as there is no benefit to cooperating.

**Pseudocode:**

```
Initialize variables:
  τ = (k + 1) / n
  c_avg = 0

For each round t from 1 to r:
  If t == 1:
    Play C
  Else:
    Calculate average payoff π_avg over previous rounds
    If π_avg < τ:
      Play D
    Else:
      Calculate cooperation probability p based on c_avg
      Play C with probability p, otherwise play D

  Update c_avg and τ as needed
```

The Adaptive Exploiter strategy is designed to balance exploration (cooperating in early rounds) and exploitation (adapting to opponents' behaviors). By monitoring the game's history, it adjusts its cooperation rate to maximize payoffs while accounting for various opponent strategies.
'''

description_EXPLOITATIVE_201 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its own payoff by adaptively responding to the opponent's behavior while exploiting their cooperation.

**Decision Rules:**

1. **Initial Round (Round 1):** Cooperate (C). This allows us to gather information about the opponents' initial actions and sets a baseline for future adaptations.
2. **Subsequent Rounds:** Use the following decision rule:
	* If the average opponent cooperation rate in the previous round is above a certain threshold (`θ`), Defect (D). Otherwise, Cooperate (C).
	* The threshold `θ` is calculated as `(k/n) * (n/2)`, where `k` is the multiplication factor and `n` is the number of players. This ensures that we defect when opponents are cooperating at a rate that would make our defection profitable.
3. **Last Round:** Defect (D). In the final round, there's no future interaction to influence, so we prioritize maximizing our own payoff.

**Edge Cases:**

* If all opponents defected in the previous round, Cooperate (C) in the next round. This encourages opponents to cooperate and creates an opportunity for us to exploit their cooperation.
* If an opponent has consistently cooperated throughout the game (i.e., never defected), Defect (D) against them. This takes advantage of their cooperative nature.

**Adaptation Mechanism:**

* After each round, update the average opponent cooperation rate by calculating the moving average of their cooperation rates over the last `ω` rounds.
* Adjust the threshold `θ` based on the updated average opponent cooperation rate.

Pseudocode:
```python
def adaptive_exploiter(n, k, r):
    θ = (k/n) * (n/2)
    ω = 3  # window size for moving average

    coop_rate_history = [0] * n
    avg_coop_rate = 0

    for t in range(r):
        if t == 0:
            action = 'C'
        else:
            avg_coop_rate = sum(coop_rate_history[-ω:]) / ω
            if avg_coop_rate > θ:
                action = 'D'
            else:
                action = 'C'

        # Update cooperation rate history
        coop_rate_history.append(avg_coop_rate)

        if t == r - 1:  # last round
            action = 'D'

        yield action

    return
```
**Rationale:**

The Adaptive Exploiter strategy balances exploitation and adaptation to maximize its payoff. By cooperating in the initial round, we gather information about opponents' behavior and set a baseline for future adaptations.

In subsequent rounds, we use the moving average of opponent cooperation rates to adjust our decision-making. If opponents are cooperating at a high rate, we defect to exploit their cooperation. Otherwise, we cooperate to encourage them to continue cooperating.

The last round is an exception, where we prioritize maximizing our own payoff by defecting.

By adapting to changing opponent behavior and exploiting their cooperation, the Adaptive Exploiter strategy aims to outperform other strategies in the tournament.
'''

description_EXPLOITATIVE_202 = '''
I propose a strategy that I'll call "Adaptive Exploiter" (AE). AE aims to exploit the cooperative tendencies of opponents while adapting to changing game conditions.

**Decision Rules:**

1. **Initial Exploration** (Round 1): Cooperate (C) to gather information about opponents' behavior.
2. **Exploitation Phase**: After Round 1, use the following decision rule:
	* If the average cooperation rate of all players in the previous round is above a certain threshold (e.g., 0.5), Defect (D). This indicates that enough players are cooperating, making it profitable to exploit them.
	* Otherwise, Cooperate (C) to encourage others to cooperate and create a more cooperative environment.
3. **Adaptive Threshold**: Update the cooperation threshold based on the game's history:
	+ If the average payoff of all players in the previous round is higher than the expected payoff from defecting (i.e., 1), decrease the threshold by a small amount (e.g., 0.05). This indicates that cooperation is beneficial, and AE should be more willing to cooperate.
	+ Otherwise, increase the threshold by a small amount (e.g., 0.05). This suggests that defection is more profitable, and AE should be more cautious.

**Handling Edge Cases:**

1. **Last Round**: In the final round (r), always Defect (D) to maximize personal payoff.
2. **Early Rounds**: In rounds 2-5, if the average cooperation rate in the previous round is very low (e.g., < 0.2), Cooperate (C) to encourage others to cooperate and avoid a complete breakdown of cooperation.

**Pseudocode:**
```python
def adaptive_exploiter(history):
    # Initial Exploration
    if len(history) == 0:
        return 'C'
    
    # Exploitation Phase
    avg_coop_rate = sum(1 for action in history[-1] if action == 'C') / len(history[-1])
    threshold = 0.5
    
    if avg_coop_rate > threshold:
        return 'D'
    else:
        return 'C'
    
    # Adaptive Threshold
    avg_payoff = sum(player_payoffs(history[-1])) / len(history[-1])
    expected_defect_payoff = 1
    
    if avg_payoff > expected_defect_payoff:
        threshold -= 0.05
    else:
        threshold += 0.05

def player_payoffs(actions):
    # Calculate payoffs for each player based on the game's payoff structure
    pass
```
**Notes:**

* The `adaptive_exploiter` function takes a list of past actions (`history`) as input and returns the recommended action ('C' or 'D').
* The `player_payoffs` function calculates the payoffs for each player based on the game's payoff structure, but its implementation is not provided here.
* AE assumes that all players have perfect information about the game's history.

By using this strategy, Adaptive Exploiter aims to effectively exploit the cooperative tendencies of opponents while adapting to changing game conditions.
'''

description_EXPLOITATIVE_203 = '''
Here is a proposed exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
 Cooperate (C) in the first three rounds to gather information about opponents' behaviors and identify potential cooperation patterns.
2. **Defect if Opponents Defect (Round t > 3):**
 If a player observes that more than half of their opponents defected (D) in the previous round, they will defect (D) in the current round.
3. **Exploit Cooperators (Round t > 3):**
 If a player observes that at least half of their opponents cooperated (C) in the previous round, and the total payoff for cooperating is higher than the payoff for defecting (i.e., k/n \* total_contributions > 1), they will cooperate (C) in the current round.
4. **Adaptive Threshold Adjustment:**
 Update a cooperation threshold parameter (τ) based on past opponents' behaviors:
	* If more opponents cooperated than expected, decrease τ by 0.1 (encourage more cooperation).
	* If fewer opponents cooperated than expected, increase τ by 0.1 (reduce cooperation incentives).

**Pseudocode:**
```
 initialize variables
  τ = 0.5 // initial cooperation threshold

 for each round t:
   if t <= 3: // exploration phase
     cooperate(C)
   else:
     observe previous round's opponent actions and payoffs
     opponents_defected = count(opponents who defected in previous round)
     opponents_cooperated = count(opponents who cooperated in previous round)

     if opponents_defected > (n / 2):
       defect(D) // exploit opponents' defections
     else:
       total_payoff_C = k/n * opponents_cooperated + (1 - c_i)
       total_payoff_D = 1 + (k/n) * opponents_cooperated

       if total_payoff_C > total_payoff_D and τ < opponents_cooperated / n:
         cooperate(C) // exploit cooperators
       else:
         defect(D)

     update τ based on observed opponent behaviors
```
**Edge Cases:**

* **Last Round (t = r):**
 Cooperate (C) to maximize the payoff, as there's no future round to consider.
* **Tiebreaks:**
 In case of a tie in the cooperation threshold or total payoffs, defect (D) by default.

This strategy aims to exploit opponents' cooperative behaviors while minimizing losses from potential defections. By adapting to changing opponent behaviors and adjusting its own cooperation threshold, "Adaptive Exploiter" seeks to maximize its overall payoff throughout the game.

Please let me know if you have any questions or need further clarification!
'''

description_EXPLOITATIVE_204 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive to changing environments.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) in the first round to gather information about opponents' tendencies.
	* In rounds 2-3, defect (D) to test opponents' reactions to exploitation.
2. **Opponent Profiling**:
	* Track each opponent's cooperation rate (CR) over time, using a moving average of their past actions.
	* Categorize opponents into three types based on their CR:
		+ "Cooperative" (CR ≥ 0.6): likely to cooperate frequently
		+ "Defector" (CR ≤ 0.4): likely to defect frequently
		+ "Unpredictable" (0.4 < CR < 0.6): may exhibit mixed behavior
3. **Exploitation**:
	* When facing a Cooperative opponent, defect (D) to exploit their generosity.
	* When facing a Defector opponent, cooperate (C) occasionally (~30% of the time) to maintain a reputation and potentially induce them to cooperate.
	* When facing an Unpredictable opponent, use a mixed strategy:
		- Cooperate (C) with probability 0.5 in the first half of the game
		- Defect (D) with probability 0.5 in the second half of the game
4. **Adaptive Adjustment**:
	* Monitor the average payoff received from each opponent type over time.
	* If the payoff from Cooperatives decreases, adjust the exploitation rate by defecting more frequently (~10% increase).
	* If the payoff from Defectors increases, adjust the cooperation rate by cooperating more frequently (~10% increase).

**Edge Cases:**

1. **Last Round**: Always defect (D) to maximize individual payoff.
2. **Tie-Breaking**: In case of a tie in the opponent profiling system, prioritize exploiting Cooperatives over Unpredictables.

Pseudocode:
```
// Initialize variables
cooperation_rate = 0.5 // initial cooperation rate for unpredictable opponents
opponent_profiles = {} // dictionary to store opponent profiles

// Main game loop
for round = 1 to r do
  if round ≤ 3 then
    // Initial exploration phase
    if round == 1 then
      action = C
    else
      action = D
  else
    // Opponent profiling and exploitation
    for opponent in opponents do
      cr = calculate_cooperation_rate(opponent)
      if cr ≥ 0.6 then
        opponent_type = "Cooperative"
      elseif cr ≤ 0.4 then
        opponent_type = "Defector"
      else
        opponent_type = "Unpredictable"

      if opponent_type == "Cooperative" then
        action = D
      elseif opponent_type == "Defector" then
        action = C with probability 0.3
      else // Unpredictable
        if round < r/2 then
          action = C with probability 0.5
        else
          action = D with probability 0.5

    adjust_strategy(opponent_profiles, average_payoff)

  // Take action and update opponent profiles
  take_action(action)
  update_opponent_profiles(opponent_profiles, opponents)
```
This strategy is designed to be robust against a wide range of opponent behaviors while exploiting their tendencies to maximize individual payoff.
'''

description_EXPLOITATIVE_205 = '''
To design an exploitative strategy for the N-Player Public Goods Game that adapts to a wide range of opponent behaviors and maximizes payoff without relying on coordination or shared norms, we'll employ a combination of reactive and proactive elements. This approach will allow our strategy to capitalize on cooperative tendencies in other players while minimizing losses when faced with defection.

**Strategy Name:** Adaptive Exploiter (AE)

### Decision Rules

1. **Initial Rounds**: In the first round, AE defects (D). This establishes a baseline payoff and avoids potential losses if others defect.
   
   ```python
if current_round == 1:
    action = D
```

2. **Reacting to Cooperation**: If in any round more than half of the players cooperated (C) and our payoff from the previous round is less than or equal to the average contribution multiplied by k/n, AE will cooperate (C) in the next round. This encourages continued cooperation when beneficial.

   ```python
if previous_round_cooperators > n / 2 and previous_payoff <= (k/n) * previous_round_cooperators:
    action = C
```

3. **Reacting to Defection**: If in any round more than half of the players defected (D), or if AE's payoff from the previous round indicates that others are not contributing sufficiently, AE will defect (D). This minimizes potential losses.

   ```python
elif previous_round_defectors >= n / 2 or previous_payoff < 1:
    action = D
```

4. **Exploitative Maneuver**: If AE observes a mix of cooperation and defection that doesn't clearly favor either strategy, it will attempt to exploit this by cooperating if the potential payoff from shared contributions outweighs the risk of being exploited.

   ```python
else:
    # Calculate expected payoff if all cooperate vs. defecting
    expected_coop_payoff = (k/n) * previous_round_cooperators + 1
    expected_defect_payoff = 1
    
    if expected_coop_payoff > expected_defect_payoff:
        action = C
    else:
        action = D
```

5. **Last Round**: In the final round, AE defects (D), as there is no future game to influence and maximizing immediate payoff becomes the priority.

   ```python
if current_round == r:
    action = D
```

### Handling Edge Cases

- **First Round**: Defect to establish a baseline.
- **Last Round**: Defect to maximize immediate payoff without future consequences.
- **Ties in Decision Rules**: In cases where decision rules lead to ties, AE defaults to defecting (D) to minimize risk.

### Exploitative Mindset Alignment

AE is designed with an exploitative mindset by:

- Initially defecting to establish a baseline and test others' strategies.
- Reacting aggressively to widespread defection or insufficient cooperation.
- Attempting to exploit mixed strategies where potential benefits outweigh risks.
- Maximizing immediate payoff in the final round.

This strategy leverages information about game parameters (n, k, r) and history (previous actions and payoffs) without relying on shared norms or coordination mechanisms. By adapting to a wide range of opponent behaviors, AE aims to perform well in a tournament against diverse strategies developed by other AI systems.
'''

description_EXPLOITATIVE_206 = '''
**Exploitative Strategy: "Adaptive Tit-for-Tat with Public Goods Twist" (ATTPGT)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to gather information about opponents' behaviors and potential cooperation levels.
2. **Subsequent Rounds:** Observe the total number of cooperators in the previous round (`total_coop_prev`) and calculate the average payoff for cooperators (`avg_payoff_coop_prev`) and defectors (`avg_payoff_defect_prev`).
3. **Tit-for-Tat (TFT) Logic:** If `avg_payoff_coop_prev >= avg_payoff_defect_prev`, cooperate (C). Otherwise, defect (D). This logic encourages cooperation when it is profitable and punishes defection.
4. **Public Goods Twist:** When the number of cooperators in the previous round (`total_coop_prev`) is greater than or equal to `n/2` (where `n` is the number of players), adjust the TFT logic as follows:
	* If `avg_payoff_coop_prev >= k/n * total_coop_prev`, cooperate (C). Otherwise, defect (D). This twist incentivizes cooperation when a significant portion of players are cooperating and the public good's value is substantial.
5. **Exploitation:** When an opponent defects (`total_defect_prev > 0`), respond with defection (D) in the next round to punish their non-cooperative behavior.

**Pseudocode:**
```python
def ATTPGT(n, k, history):
    if len(history) == 0:  # First Round
        return 'C'
    
    total_coop_prev = sum(1 for action in history[-1] if action == 'C')
    avg_payoff_coop_prev = sum(pi for pi, action in zip(history[-1], ['C', 'D']) if action == 'C') / total_coop_prev
    avg_payoff_defect_prev = sum(pi for pi, action in zip(history[-1], ['C', 'D']) if action == 'D') / (n - total_coop_prev)
    
    if avg_payoff_coop_prev >= avg_payoff_defect_prev:  # TFT Logic
        return 'C'
    else:
        return 'D'
    
    if total_coop_prev >= n/2:  # Public Goods Twist
        if avg_payoff_coop_prev >= k/n * total_coop_prev:
            return 'C'
        else:
            return 'D'
    
    if sum(1 for action in history[-1] if action == 'D') > 0:  # Exploitation
        return 'D'
```
**Edge Cases:**

* In the first round, cooperate to gather information.
* In the last round, use the same decision rules as before; there is no need for a special case.

This strategy adapts to various opponent behaviors by responding to changes in cooperation levels and average payoffs. By incorporating the public goods twist, it incentivizes cooperation when beneficial and punishes defection when necessary. The exploitation mechanism ensures that opponents who defect are responded to with defection, maintaining a robust and exploitative approach.
'''

description_EXPLOITATIVE_207 = '''
Here's a strategy that I'll call "Adaptive Exploiter" (AE). AE aims to exploit opponents' cooperative tendencies while minimizing its own losses.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, play Cooperate (C) with a high probability (e.g., 0.8). This will test opponents' willingness to cooperate and potentially establish a mutually beneficial pattern.
2. **Exploitation Phase**: For rounds 2 to r-1:
	* If the total number of cooperators in the previous round is above a certain threshold (e.g., n/2), play Defect (D). This takes advantage of others' cooperation, maximizing personal payoff while minimizing contribution to the public good.
	* Otherwise, play Cooperate (C) with a probability that decreases as the number of rounds increases. For example: `p(C) = 1 - (t-1)/(r-2)`, where t is the current round number. This allows AE to adapt to changes in opponents' behavior and maintain some level of cooperation.
3. **Endgame**: In the last round (r), play Defect (D). Since there are no future rounds, maximizing personal payoff takes priority.

**Edge Cases:**

* If n = 2, always play Cooperate (C) in the first round to encourage mutual cooperation. For subsequent rounds, follow the Exploitation Phase rules.
* If k is very close to 1 or n is very large, AE may need to adjust its threshold for exploitation. For example, if k is near 1, it might make sense to cooperate more often.

**Pseudocode:**
```python
def Adaptive_Exploiter(n, r, k):
    # Initial cooperation probability (first round)
    p_initial_coop = 0.8

    # Threshold for exploitation
    threshold = n / 2

    for t in range(1, r+1):
        if t == 1:
            # First round: cooperate with high probability
            action = 'C' if random.random() < p_initial_coop else 'D'
        elif t < r:
            # Exploitation phase
            total_coops_prev_round = count_cooperators(prev_round)
            if total_coops_prev_round > threshold:
                action = 'D'
            else:
                # Decrease cooperation probability as rounds progress
                p_coop = 1 - (t-1)/(r-2)
                action = 'C' if random.random() < p_coop else 'D'
        else:
            # Last round: always defect
            action = 'D'

        return action

    def count_cooperators(round):
        # Count the number of cooperators in a given round
        pass  # implementation omitted for brevity
```
This strategy is designed to be robust and adaptive, taking into account the game parameters (n, r, k) and history. By initially cooperating and then exploiting opponents' cooperative tendencies, AE aims to maximize its payoff while minimizing losses.
'''

description_EXPLOITATIVE_208 = '''
**Exploitative Strategy: Adaptive Cooperate-to-Defect (ACD)**

The ACD strategy aims to exploit opponents by adapting to their behavior while maintaining a balance between cooperation and defection.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability 0.5. This initial cooperation serves as an experiment to gauge opponents' behaviors.
2. **Opponent Behavior Assessment**: After each round, calculate the total number of cooperators (C_count) and defectors (D_count) among opponents in the previous round.
3. **Cooperation Threshold**: Define a cooperation threshold (CT) as CT = k/n \* C_count / (C_count + D_count). This threshold reflects the proportion of public good shared among cooperators relative to the total number of players.
4. **Adaptive Cooperation**: In subsequent rounds, cooperate if:
	* CT > 0.5 and the player's previous payoff was greater than or equal to the average payoff of all players in the previous round (π_i,t-1 ≥ avg(π_j,t-1) for all j).
	* The opponent's cooperation rate (C_count / n) is above a certain threshold (e.g., 0.3). This condition encourages cooperation when opponents are cooperative.
5. **Defection**: Defect (play D) otherwise.

**Edge Cases:**

1. **Last Round**: In the last round, always defect to maximize personal payoff, as there's no future opportunity for reciprocity or exploitation.
2. **Tiebreaker**: In case of a tie in the cooperation threshold calculation, cooperate if the player's previous payoff was higher than the average payoff; otherwise, defect.

**Pseudocode:**
```python
def ACD_strategy(n, k, r):
    # Initialize variables
    C_count = 0
    D_count = 0
    CT = 0
    prev_payoff = 0

    for t in range(1, r+1):
        if t == 1:
            action = 'C' if random.random() < 0.5 else 'D'
        else:
            C_count = sum(opponent_actions[t-1] == 'C')
            D_count = n - C_count
            CT = (k/n) * C_count / (C_count + D_count)

            if CT > 0.5 and prev_payoff >= avg_payoffs[t-1]:
                action = 'C'
            elif opponent_cooperation_rate(t-1) > 0.3:
                action = 'C'
            else:
                action = 'D'

        # Update variables for next round
        if t < r:
            prev_payoff = payoff(action, opponent_actions[t])
            avg_payoffs.append(avg(opponent_payoffs[t]))

    return actions

def opponent_cooperation_rate(t):
    C_count = sum(opponent_actions[t] == 'C')
    return C_count / n
```
This ACD strategy balances cooperation and defection by adapting to opponents' behaviors, while being robust to a wide range of strategies. By initially cooperating with probability 0.5, the strategy tests opponents' willingness to cooperate. The cooperation threshold (CT) helps determine when to cooperate based on the proportion of public good shared among cooperators relative to the total number of players. The defection rule takes into account both personal payoff and opponents' cooperation rates, allowing the strategy to exploit opponents effectively.
'''

description_EXPLOITATIVE_209 = '''
**Exploitative Strategy: "Adaptive Predator"**

This strategy is designed to exploit a wide range of opponent behaviors while adapting to changing conditions throughout the game.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Play D (Defect) in the first round to gather information on opponents' initial moves.
	* In rounds 2 and 3, play C (Cooperate) if at least one opponent cooperated in the previous round; otherwise, play D.
2. **Adaptive Phase (Rounds 4-r)**:
	* Calculate the **Cooperation Rate** (CR) for each opponent: CR_i = (# of times opponent i cooperated) / (# of rounds played).
	* If an opponent's CR is above a certain threshold (e.g., 0.6), consider them a **Cooperator**.
	* Play C if:
		+ At least one Cooperator is present in the game, and their average payoff is higher than yours.
		+ The number of Cooperators is increasing over time.
	* Otherwise, play D to exploit opponents' cooperation.

Pseudocode for the Adaptive Phase:
```
for each opponent i:
  CR_i = calculate_cooperation_rate(i)
  if CR_i > 0.6 and average_payoff(opponent=i) > my_average_payoff:
    cooperate = True
  elif increasing_cooperators():
    cooperate = True
  else:
    cooperate = False

if cooperate:
  play C
else:
  play D
```
**Edge Cases:**

* **Last Round (Round r)**: Play D to maximize personal payoff, as there is no future interaction.
* **Ties in Cooperation Rate**: In case of a tie, consider the opponent with the higher average payoff as a Cooperator.

**Exploitative Mindset:**

The Adaptive Predator strategy aims to exploit opponents who cooperate excessively or follow predictable patterns. By initially exploring the game environment and adapting to changing conditions, this strategy can take advantage of:

1. **Unconditional Cooperators**: Opponents who always cooperate will be exploited in later rounds.
2. **Naive Strategies**: Simple strategies that don't account for adaptive behavior will be outmaneuvered by the Adaptive Predator.

This strategy is robust against a wide range of opponent behaviors and can effectively exploit those who fail to adapt or coordinate their actions.
'''

description_EXPLOITATIVE_210 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to changing game dynamics. This strategy focuses on maximizing individual payoff by balancing cooperation and defection based on observed patterns and game parameters.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to gather information about opponents' initial strategies.
2. **Subsequent Rounds (t>1):**
	* If the average payoff per cooperator in the previous round is greater than or equal to the average payoff per defector, cooperate (C).
	* Otherwise, defect (D).
3. **Payoff-Driven Adaptation:** Monitor the total payoff difference between cooperation and defection over the last `w` rounds (`w` is a window size parameter). If the cumulative payoff difference favors cooperation, increase the likelihood of cooperating in subsequent rounds by adjusting the decision threshold.
4. **Opponent Classification:**
	* Classify opponents as "Cooperators" if they cooperated in at least 2/3 of the last `w` rounds.
	* Classify opponents as "Defectors" otherwise.

**Edge Case Handling:**

1. **Last Round (t=r):** Defect (D) to maximize individual payoff, as cooperation is less likely to be reciprocated.
2. **Single Opponent:** Cooperate (C) in the first round and then mirror the opponent's previous action to encourage cooperation.

**Pseudocode:**
```python
def adaptive_predator(n, k, r):
    # Initialize variables
    w = 5  # Window size parameter
    coop_threshold = 0.5  # Initial decision threshold

    for t in range(1, r+1):
        if t == 1:
            action = "C"  # Cooperate in the first round
        else:
            prev_round_coop_payoff_avg = avg_payoff(cooperators, t-1)
            prev_round_defect_payoff_avg = avg_payoff(defectors, t-1)

            if prev_round_coop_payoff_avg >= prev_round_defect_payoff_avg:
                action = "C"
            else:
                action = "D"

            # Payoff-driven adaptation
            payoff_diff = cumulative_payoff_diff(w)
            if payoff_diff > 0:
                coop_threshold += 0.1

        # Opponent classification and mirroring (single opponent only)
        if n == 2:
            if t > 1:
                prev_opponent_action = get_prev_opponent_action(t-1)
                if prev_opponent_action == "C":
                    action = "C"
                else:
                    action = "D"

    return action
```
**Exploitative Mindset:**

The Adaptive Predator strategy is designed to exploit the willingness of other players to cooperate by initially cooperating and then adapting to the observed patterns. By monitoring the payoff difference between cooperation and defection, the strategy adjusts its decision threshold to maximize individual payoff. In the presence of multiple opponents, the strategy focuses on identifying and mirroring cooperative behaviors while taking advantage of defectors.

This exploitative mindset allows the Adaptive Predator strategy to perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_211 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize payoffs by exploiting opponents' behaviors while adapting to changing game conditions.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = k/n. This encourages early cooperation and sets a baseline for future actions.
2. **Defecting on Low Cooperation**: If the number of cooperators in the previous round is less than or equal to n/2, defect (D) in the next round. This takes advantage of opponents' lack of cooperation.
3. **Punishing Defectors**: If a player defected in the previous round and there were at least n/2 cooperators, cooperate (C) in the next round with probability p = 1 - (number of defectors / n). This punishes opponents for exploiting others.
4. **Matching Opponent's Cooperation Rate**: Otherwise, cooperate (C) with probability p equal to the average cooperation rate of all opponents in the previous round. This mimics opponents' behavior and encourages mutual cooperation.

**Handling Edge Cases:**

1. **Last Round**: In the last round, always defect (D). Since there are no future rounds, exploiting others maximizes payoffs.
2. **Ties**: If multiple strategies yield the same expected payoff, prioritize cooperating (C) to encourage mutual cooperation.
3. **Unobserved Opponent Actions**: If an opponent's action is unobserved, assume they cooperated (C) with probability p = k/n.

**Pseudocode:**
```
initialize variables:
  cooperate_prob = k/n
  prev_cooperators = 0

for each round t from 1 to r:
  if t == 1:
    # Initial cooperation
    action[t] = C with probability cooperate_prob
  else:
    # Defecting on low cooperation
    if prev_cooperators <= n/2:
      action[t] = D
    # Punishing defectors
    elif number of defectors in previous round > 0 and prev_cooperators >= n/2:
      action[t] = C with probability 1 - (number of defectors / n)
    # Matching opponent's cooperation rate
    else:
      cooperate_prob = average cooperation rate of opponents in previous round
      action[t] = C with probability cooperate_prob

  observe opponent actions and update variables:
    prev_cooperators = number of cooperators in current round
```
**Exploitative Mindset:**
The Adaptive Exploiter strategy prioritizes exploiting opponents' behaviors to maximize payoffs. By adapting to changing game conditions, it takes advantage of opportunities to defect while maintaining a level of cooperation to encourage mutual cooperation. This approach allows the strategy to thrive in a wide range of opponent behaviors and game environments.

By playing against independent strategies developed by other AI systems, the Adaptive Exploiter will continually adapt and improve its performance, ultimately becoming a formidable opponent in the tournament.
'''

description_EXPLOITATIVE_212 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit opponents' behaviors while being robust to a wide range of opponent strategies. It adapts to the game history and uses a combination of cooperation and defection to maximize its payoff.

**Decision Rules:**

1. **Initial Rounds (Rounds 1-3):** Cooperate (C) in the first three rounds to gather information about opponents' behaviors and to potentially establish a cooperative atmosphere.
2. **Exploitation Phase:** After Round 3, switch to an exploitation phase where you:
	* Defect (D) if the average cooperation rate of all players (including yourself) is above a certain threshold (`exploit_threshold` = 0.6).
	* Cooperate (C) if the average cooperation rate is below `exploit_threshold`.
3. **Reaction to Opponent's Cooperation:** If an opponent cooperates in the previous round, and their cooperation rate is above a certain threshold (`react_threshold` = 0.5), cooperate (C) in the current round.
4. **Punishment Mechanism:** If an opponent defects in the previous round, and their defection rate is above a certain threshold (`punish_threshold` = 0.7), defect (D) in the current round.

**Edge Cases:**

1. **Last Round (Round r):** Cooperate (C) in the last round to maximize the final payoff.
2. **Ties:** In case of ties, use a simple tie-breaker mechanism: cooperate if your ID is even, and defect if your ID is odd.

Pseudocode:
```python
def adaptive_exploiter(game_state):
  # Initialize variables
  exploit_threshold = 0.6
  react_threshold = 0.5
  punish_threshold = 0.7

  # Initial rounds (1-3)
  if game_state.round < 4:
    return "C"  # Cooperate

  # Exploitation phase
  avg_cooperation_rate = sum([player.cooperation_rate for player in game_state.players]) / len(game_state.players)

  if avg_cooperation_rate > exploit_threshold:
    return "D"  # Defect
  else:
    return "C"  # Cooperate

  # Reaction to opponent's cooperation
  for opponent in game_state.players:
    if opponent.cooperation_rate > react_threshold and opponent.last_action == "C":
      return "C"  # Cooperate

  # Punishment mechanism
  for opponent in game_state.players:
    if opponent.defection_rate > punish_threshold and opponent.last_action == "D":
      return "D"  # Defect

  # Last round
  if game_state.round == game_state.r:
    return "C"  # Cooperate

  # Tie-breaker mechanism (if necessary)
  if game_state.tie_breaker_needed:
    if ID % 2 == 0:
      return "C"
    else:
      return "D"
```
This strategy aims to balance cooperation and exploitation, adapting to the opponents' behaviors while maximizing its own payoff. By reacting to opponents' cooperation and punishing their defection, the Adaptive Exploiter strategy can take advantage of various opponent strategies and emerge as a robust competitor in the tournament.
'''

description_EXPLOITATIVE_213 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize its payoff by adaptively responding to the opponents' behavior while exploiting their cooperation.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 50%. This allows us to gather information about the opponents' initial behavior and avoids immediate exploitation.
2. **Reciprocal Exploitation**: For rounds 2 to r-1:
	* If the majority (> n/2) of opponents cooperated in the previous round, defect (play D). This exploits their cooperation and maximizes our payoff.
	* Otherwise, cooperate (play C). This encourages opponents to cooperate and potentially leads to higher payoffs in future rounds.
3. **Endgame Exploitation**: In the last round (r), always defect (play D). Since there are no future rounds, there's no incentive for opponents to cooperate, and we can maximize our payoff by exploiting their cooperation.

**Edge Cases:**

* If n = 2 (only two players), always defect (play D) in all rounds. With only two players, cooperation is not beneficial, and exploitation is the best strategy.
* If k ≥ n (the multiplication factor is greater than or equal to the number of players), cooperate (play C) in all rounds. In this case, cooperation leads to higher payoffs, and exploitation is not beneficial.

**Additional Heuristics:**

* **Monitor Opponent Cooperation**: Keep track of the average cooperation rate of opponents over the past few rounds (e.g., 3-5 rounds). If this rate increases, it may indicate a shift towards more cooperative behavior, and we can adjust our strategy to cooperate more frequently.
* **Punish Uncooperative Behavior**: If an opponent defects repeatedly in consecutive rounds, retaliate by defecting against them for a few rounds. This encourages opponents to maintain a level of cooperation.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
  # Initialize variables
  cooperate_prob = 0.5
  prev_coop_rate = []
  opponent_coop_rates = [0] * n

  for t in range(r):
    if t == 0:
      # First round: random cooperation
      action = 'C' if random.random() < cooperate_prob else 'D'
    elif t == r - 1:
      # Last round: always defect
      action = 'D'
    else:
      # Reciprocal exploitation
      majority_coop = sum(prev_coop_rate) > n / 2
      action = 'D' if majority_coop else 'C'

    # Update opponent cooperation rates
    for i in range(n):
      opponent_coop_rates[i] += (1 if opponents[i].action == 'C' else 0)

    # Monitor average cooperation rate
    avg_coop_rate = sum(opponent_coop_rates) / n

    # Adjust strategy based on heuristics
    if avg_coop_rate > 0.5:
      cooperate_prob += 0.1
    elif avg_coop_rate < 0.3:
      cooperate_prob -= 0.1

    # Punish uncooperative behavior
    for i in range(n):
      if opponents[i].action == 'D' and opponent_coop_rates[i] < 0.2:
        action = 'D'

    return action
```
This strategy is designed to be adaptive, exploitative, and robust against a wide range of opponent behaviors. By monitoring the opponents' cooperation rates and adjusting our strategy accordingly, we can maximize our payoff while minimizing the risk of being exploited.
'''

description_EXPLOITATIVE_214 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

Decision Rules:

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) with probability 0.5 to gather information about opponents' behaviors.
2. **Opponent Classification**: After Round 3, classify each opponent as either "Cooperative" or "Defective" based on their cooperation rate:
	* Cooperative: Played C in ≥ 2/3 of the last 3 rounds
	* Defective: Played D in ≥ 2/3 of the last 3 rounds
3. **Exploitation**:
	* If an opponent is classified as Cooperative, play D (Defect) against them to exploit their cooperativeness.
	* If an opponent is classified as Defective, play C (Cooperate) against them to try to induce cooperation and increase the public good.
4. **Adaptive Adjustment**: After each round, re-classify opponents based on their updated cooperation rates.
5. **Edge Cases**:
	+ Last Round: Play D (Defect) regardless of opponent classification to maximize personal payoff.
	+ If all opponents are classified as Defective, play C (Cooperate) to try to induce cooperation and increase the public good.

Pseudocode:
```
# Initialize variables
coop_prob = 0.5  # Initial exploration probability
opp_classifications = {}  # Dictionary to store opponent classifications

# Rounds 1-3: Initial Exploration
for round in range(1, 4):
    action = random.choice([C, D], p=[coop_prob, 1 - coop_prob])

# Opponent Classification (after Round 3)
for opp in opponents:
    coop_rate = sum(opp.actions[:3]) / 3
    if coop_rate >= 2/3:
        opp_classifications[opp] = "Cooperative"
    else:
        opp_classifications[opp] = "Defective"

# Exploitation (Rounds 4+)
for round in range(4, r + 1):
    for opp in opponents:
        if opp_classifications[opp] == "Cooperative":
            action = D
        elif opp_classifications[opp] == "Defective":
            action = C

# Adaptive Adjustment (after each round)
for opp in opponents:
    coop_rate = sum(opp.actions[-3:]) / 3
    if coop_rate >= 2/3 and opp_classifications[opp] != "Cooperative":
        opp_classifications[opp] = "Cooperative"
    elif coop_rate < 2/3 and opp_classifications[opp] != "Defective":
        opp_classifications[opp] = "Defective"

# Last Round
if round == r:
    action = D

# If all opponents are Defective, try to induce cooperation
if all(opp_classifications.values()) == "Defective":
    action = C
```
This strategy is designed to be exploitative and adaptive, taking into account the game parameters and history. It initially explores opponents' behaviors, classifies them as Cooperative or Defective, and then exploits their weaknesses while trying to induce cooperation from Defective opponents. The Adaptive Exploiter adjusts its classification of opponents based on their updated cooperation rates, ensuring it remains robust against a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_215 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy aims to maximize its payoff by exploiting the cooperative behavior of other players while minimizing its own contributions to the public good.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Defect (D) in the first two rounds to gather information about the opponents' behavior and identify potential cooperators.
2. **Identify Cooperators:** After the initial rounds, identify players who have cooperated at least once in the previous rounds. Label these players as "Cooperators" (C).
3. **Exploit Cooperators:** In subsequent rounds, cooperate (C) if the number of identified Cooperators is greater than or equal to half the total number of players (n/2). Otherwise, defect (D).
4. **Punish Defectors:** If a player defects after previously cooperating, label them as "Defector" (DF) and always defect against them in future rounds.
5. **Adapt to Changing Behavior:** Re-evaluate the Cooperators and Defectors lists every 2-3 rounds to adapt to changes in opponents' behavior.

**Edge Cases:**

* **Last Round (t = r):** Defect (D) in the last round, as there is no future payoff to consider.
* **Single Cooperator:** If only one player cooperates, defect (D) against them to maximize individual payoff.
* **All Defectors:** If all players defect, cooperate (C) in the next round to test for potential cooperators.

**Pseudocode:**
```
 Initialize:
   Cooperators = []
   Defectors = []

 For each round t:
   If t ≤ 2:
     Play D
   Else:
     Identify Cooperators and Defectors based on previous rounds
     If |Cooperators| ≥ n/2:
       Play C
     Else:
       Play D

   Update Cooperators and Defectors lists every 2-3 rounds

   If last round (t = r):
     Play D
```
The Adaptive Predator strategy is designed to be exploitative, taking advantage of cooperative behavior while minimizing its own contributions. By identifying and adapting to changing opponent behaviors, this strategy aims to maximize its payoff in the tournament against independent AI systems.
'''

description_EXPLOITATIVE_216 = '''
Here is a exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **First Round:** Defect (D) in the first round to gather information about other players' initial tendencies.
2. **Subsequent Rounds:** Cooperate (C) if the average payoff of cooperators in the previous round is higher than the average payoff of defectors, and the total number of cooperators is above a certain threshold (n/3). Otherwise, Defect (D).
3. **Threshold Adjustment:** Adjust the cooperation threshold every 5 rounds based on the average number of cooperators in the last 5 rounds. If the average number of cooperators increases, decrease the threshold by 1; if it decreases, increase the threshold by 1.

**Pseudocode:**
```
Initialize:
  history = empty list
  threshold = n/3

For each round t from 2 to r:
  if t == 2:
    action[t] = D (Defect)
  else:
    avg_coop_payoff = average payoff of cooperators in previous round
    avg_def_payoff = average payoff of defectors in previous round
    num_cooperators = number of cooperators in previous round

    if avg_coop_payoff > avg_def_payoff and num_cooperators >= threshold:
      action[t] = C (Cooperate)
    else:
      action[t] = D (Defect)

  Append history with actions and payoffs of all players

Every 5 rounds:
  Update threshold based on average number of cooperators in last 5 rounds
```
**Rationale:**

* The first round is used to gather information about other players' initial tendencies. Defecting allows us to observe how others react without committing to cooperation.
* In subsequent rounds, we cooperate if the average payoff of cooperators is higher than that of defectors and the number of cooperators is above a certain threshold. This ensures that we only cooperate when it is beneficial and there are enough other cooperators to make it worthwhile.
* The threshold adjustment mechanism allows us to adapt to changes in the game dynamics. If cooperation becomes more prevalent, we decrease the threshold to encourage more cooperation. Conversely, if cooperation decreases, we increase the threshold to avoid being exploited.

**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to take advantage of other players' cooperative tendencies while minimizing our own contributions. By cooperating only when it is beneficial and there are enough other cooperators, we maximize our payoffs while avoiding exploitation by defectors. The threshold adjustment mechanism allows us to adapt to changes in the game dynamics and maintain an exploitative edge.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it is designed to be robust and adaptive in a wide range of scenarios.
'''

description_EXPLOITATIVE_217 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy depends only on the game parameters and observed actions.

**Decision Rules:**

1. **Initial Rounds (t = 1, 2)**: Cooperate (C) in the first two rounds to gather information about opponents' behavior.
2. **Exploitation Phase**: After the initial rounds, enter the exploitation phase. In each round t > 2:
	* If the total number of cooperators in the previous round (t-1) is less than or equal to n/2, cooperate (C).
	* Otherwise, defect (D).
3. **Punishment Mechanism**: If an opponent defects while you cooperated in the same round, trigger a punishment mechanism:
	+ Defect for 2 rounds after the opponent's defection.
	+ Then, return to the exploitation phase.

**Edge Cases:**

1. **Last Round (t = r)**: Cooperate if there are fewer than n/2 cooperators in the second-to-last round; otherwise, defect.
2. **Ties**: In case of a tie (n/2 cooperators), cooperate.

**Pseudocode:**
```python
def AdaptiveExploiter(n, k, history):
  # Initial rounds
  if len(history) < 2:
    return 'C'
  
  # Exploitation phase
  total_cooperators = sum([1 for action in history[-1] if action == 'C'])
  if total_cooperators <= n / 2:
    return 'C'
  else:
    return 'D'
  
  # Punishment mechanism
  for opponent_action, my_action in zip(history[-1], ['C', 'D']):
    if opponent_action == 'D' and my_action == 'C':
      punishment_rounds = 0
      while punishment_rounds < 2:
        return 'D'
        punishment_rounds += 1
  
  # Last round handling
  if len(history) == r - 1:
    total_cooperators_last_round = sum([1 for action in history[-2] if action == 'C'])
    if total_cooperators_last_round < n / 2:
      return 'C'
    else:
      return 'D'
```
**Exploitative Mindset:**
The Adaptive Exploiter strategy is designed to take advantage of opponents' cooperative behavior while minimizing its own cooperation. By cooperating in the initial rounds, it gathers information about opponents' actions and adapts to their strategies. The punishment mechanism discourages opponents from defecting against the Adaptive Exploiter.

This strategy should perform well against a variety of opponent behaviors, as it is adaptive, exploitative, and robust to different game environments.
'''

description_EXPLOITATIVE_218 = '''
Here's a exploitative strategy that adapts to various opponent behaviors and focuses on maximizing its own payoff.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C). This sets the tone for potential cooperation and allows AE to observe opponents' initial actions.
2. **Subsequent Rounds (t>1):**
	* If the average payoff of cooperators in the previous round is higher than the average payoff of defectors, cooperate (C).
	* Otherwise, defect (D).

Pseudocode:
```
if t == 1:
    action = C
else:
    avg_coop_payoff_prev_round = sum(cooperator_payoffs_prev_round) / num_cooperators_prev_round
    avg_defect_payoff_prev_round = sum(defector_payoffs_prev_round) / num_defectors_prev_round
    
    if avg_coop_payoff_prev_round > avg_defect_payoff_prev_round:
        action = C
    else:
        action = D
```
**Edge Cases:**

* **Last Round (t=r):** Defect (D). Since there are no future rounds to influence, AE prioritizes its own payoff in the final round.
* **Ties:** In cases where the average payoffs of cooperators and defectors are equal, defect (D). This ensures AE doesn't cooperate unnecessarily.

**Exploitative Mindset:**

AE's strategy is designed to exploit opponents' cooperative tendencies while minimizing its own vulnerability to exploitation. By cooperating initially and then adapting to the previous round's outcomes, AE aims to:

* Encourage cooperation among opponents
* Identify and capitalize on profitable cooperation opportunities
* Defect when opponents are not contributing to a mutually beneficial public good

**Robustness:**

AE's adaptive nature allows it to respond effectively to various opponent behaviors, including:

* Cooperative strategies: AE will cooperate if opponents' cooperative actions lead to higher average payoffs.
* Defective strategies: AE will defect if opponents prioritize their own payoffs over cooperation.
* Mixed strategies: AE will adapt its action based on the previous round's outcomes, making it difficult for opponents to predict and exploit.

By implementing this Adaptive Exploiter strategy, you'll be well-equipped to navigate a wide range of opponent behaviors in the N-Player Public Goods Game tournament.
'''

description_EXPLOITATIVE_219 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize its own payoff by exploiting the cooperative tendencies of other players while minimizing its own contributions to the public good.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) in Round 1 with probability 0.5.
	* Observe the actions and payoffs of other players in Rounds 1-2.
	* In Round 3, defect (D) if fewer than half of the players cooperated in Round 2; otherwise, cooperate.
2. **Exploitation Phase (Rounds 4-r)**:
	* Cooperate if the total number of cooperators in the previous round is greater than or equal to `n/2`, where `n` is the number of players.
	* Defect if the total number of cooperators in the previous round is less than `n/2`.
3. **Adaptive Adjustment**:
	* Monitor the average payoff of all players over a sliding window of 3 rounds (previous, current, and next).
	* If the average payoff increases by more than 10% from one window to the next, adjust the cooperation threshold in Step 2 by ±1 player (i.e., become more or less selective about cooperating).

**Edge Cases:**

* **Last Round (Round r)**: Defect, as there is no future round to influence.
* **Tiebreakers**: In case of a tie in the number of cooperators, cooperate if your own payoff was higher than average in the previous round; otherwise, defect.

**Pseudocode:**
```
function AdaptiveExploiter(game_state, history):
  // Initial Exploration Phase
  if round <= 3:
    if round == 1:
      return C with probability 0.5
    elif round == 2:
      observe_actions_and_payoffs()
    else:  // Round 3
      if fewer_than_half_cooperated():
        return D
      else:
        return C

  // Exploitation Phase
  prev_round_cooperators = count_cooperators(history[-1])
  if prev_round_cooperators >= n / 2:
    return C
  else:
    return D

  // Adaptive Adjustment
  avg_payoff_window = calculate_avg_payoff(history[-3:])
  if avg_payoff_window > previous_avg_payoff * 1.1:
    adjust_cooperation_threshold(±1)
```
This strategy aims to balance exploitation with adaptability, making it robust against various opponent behaviors. By initially exploring the cooperation levels of other players and adjusting its own behavior accordingly, the Adaptive Exploiter can maximize its payoff while minimizing its contributions to the public good.
'''

description_EXPLOITATIVE_220 = '''
**Exploitative Strategy: "Conditional Cooperator with Adaptive Threshold" (CCAT)**

CCAT is a dynamic strategy that adapts to the game environment and opponents' behaviors while maintaining a strong exploitative mindset.

**Decision Rules:**

1. **Initial Rounds:** Cooperate in the first round (t = 1) to encourage cooperation and gather information about opponents.
2. **Adaptive Threshold:** Track the total number of cooperators in each round, denoted as `C_t`.
3. **Threshold Calculation:** Calculate a dynamic threshold value, `T_t`, based on the game parameters and history:

`T_t = (k/n) * (1 - (1/k)^t)`

This threshold decreases over time, reflecting the increasing importance of cooperation in later rounds.
4. **Cooperation Decision:** In each round `t > 1`, cooperate if the number of cooperators in the previous round (`C_{t-1}`) is greater than or equal to the threshold value (`T_t`). Otherwise, defect.

Pseudocode:
```python
def CCAT(n, k, r, t):
    if t == 1:  # First round
        return 'Cooperate'
    
    C_prev = get_num_cooperators(t-1)  # Get number of cooperators in previous round
    T_t = (k/n) * (1 - (1/k)**t)
    
    if C_prev >= T_t:
        return 'Cooperate'
    else:
        return 'Defect'
```
**Handling Edge Cases:**

* **Last Round:** In the final round (`t = r`), always defect to maximize individual payoff, as there are no future rounds to consider.
* **Opponent Cooperation:** If an opponent consistently cooperates despite being exploited (i.e., receiving low payoffs), CCAT will adapt and start cooperating with them to take advantage of their cooperation.

**Exploitative Mindset:**

CCAT is designed to exploit opponents who cooperate unconditionally or have weak strategies. By initially cooperating and then adapting to the game environment, CCAT can:

* Encourage cooperation in early rounds
* Identify and exploit weak opponents
* Maximize individual payoff by defecting when opponents are not cooperative

By being conditional on the number of cooperators and adapting over time, CCAT maintains a strong exploitative mindset while remaining robust to various opponent behaviors.
'''

description_EXPLOITATIVE_221 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics. This strategy prioritizes self-interest and seeks to maximize its own payoff by strategically cooperating or defecting.

**Decision Rules:**

1. **First Round:** Defect (D) in the first round to gather information about opponents' initial tendencies.
2. **Initial Phase (Rounds 2-5):**
	* If the average cooperation rate of all players in the previous round is above 50%, cooperate (C).
	* Otherwise, defect (D).
3. **Adaptive Phase (Rounds 6+):**
	* Track the average payoff of all players in the previous round (avg_payoff_prev).
	* Calculate the difference between avg_payoff_prev and the strategy's own payoff in the previous round (payoff_diff).
	* If payoff_diff > 0.5, cooperate (C) to potentially increase the public good.
	* If payoff_diff ≤ 0.5, defect (D) to exploit opponents' cooperation.
4. **Last Round:** Defect (D) in the last round to maximize personal gain.

**Additional Heuristics:**

1. **Exploitation Threshold:** If an opponent's cooperation rate exceeds 75% over the past three rounds, increase the strategy's defection rate by 20% for the next two rounds.
2. **Cooperation Baiting:** If the average cooperation rate of all players in the previous round is below 25%, cooperate (C) in the current round to potentially induce opponents to cooperate.

**Pseudocode:**
```python
def adaptive_predator(n, k, r, history):
    # First Round
    if len(history) == 0:
        return 'D'

    # Initial Phase
    avg_coop_rate = sum(history[-1]['coop']) / n
    if avg_coop_rate > 0.5 and len(history) < 5:
        return 'C'
    else:
        return 'D'

    # Adaptive Phase
    avg_payoff_prev = sum([player['payoff'] for player in history[-1]]) / n
    payoff_diff = avg_payoff_prev - history[-1]['self']['payoff']
    if payoff_diff > 0.5:
        return 'C'
    else:
        return 'D'

    # Last Round
    if len(history) == r - 1:
        return 'D'

    # Exploitation Threshold
    opponent_coop_rates = [sum(player['coop'][-3:]) / 3 for player in history]
    if max(opponent_coop_rates) > 0.75:
        return 'D' with probability 0.8

    # Cooperation Baiting
    avg_coop_rate = sum(history[-1]['coop']) / n
    if avg_coop_rate < 0.25:
        return 'C'
```
This strategy is designed to be adaptive and exploitative, capitalizing on opponents' cooperation while minimizing its own contributions to the public good. By tracking average payoffs and adjusting its behavior accordingly, the Adaptive Predator aims to maximize its own payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_222 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy aims to exploit the behaviors of opponents by adapting to their cooperation levels while maintaining a robust and profitable approach.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) in the first round to gather information about opponents' initial behaviors.
2. **Subsequent Rounds (t>1)**:
	* Calculate the average cooperation rate of all players, including yourself, from the previous round (`avg_coop_prev`).
	* If `avg_coop_prev` is above a threshold (`coop_threshold`, set to 0.5), Cooperate (C) in the current round.
	* Otherwise, Defect (D) in the current round.

Pseudocode:
```python
def adaptive_predator(n, k, r):
    coop_threshold = 0.5
    avg_coop_prev = None

    for t in range(1, r+1):
        if t == 1:  # Initial Round
            action = 'C'
        else:
            avg_coop_prev = calculate_avg_coop_prev()
            if avg_coop_prev >= coop_threshold:
                action = 'C'
            else:
                action = 'D'

        # Play the action and observe opponents' actions
        play_action(action)
        observe_opponents_actions()

    return total_payoff
```

**Handling Edge Cases:**

* **Last Round (t=r)**: Stick to the decision rule from the previous round. If the average cooperation rate was above the threshold, Cooperate; otherwise, Defect.
* **Opponents' Cooperation Rates**: Monitor opponents' cooperation rates and adjust your behavior accordingly.

**Exploitative Mindset:**

The Adaptive Predator strategy aims to exploit the following:

1. **High-cooperation environments**: By cooperating when the average cooperation rate is high, we can capitalize on the public good's benefits.
2. **Low-cooperation environments**: Defecting when the average cooperation rate is low allows us to avoid contributing to a potentially weak public good and focus on our private payoff.

By adapting to opponents' behaviors while maintaining a robust approach, the Adaptive Predator strategy aims to perform well in a wide range of scenarios against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_223 = '''
**Exploitative Strategy: "Adaptive Free Rider"**

The Adaptive Free Rider strategy is designed to exploit a wide range of opponent behaviors while minimizing losses and maximizing gains.

**Decision Rules:**

1. **Initial Exploration (Round 1-3)**:
	* Play Cooperate (C) in the first round to gather information about opponents' initial tendencies.
	* In rounds 2-3, play Defect (D) to test opponents' reactions to exploitation.
2. **Adaptive Exploitation**:
	* Calculate the average cooperation rate of all players (ACR) over the last 3 rounds.
	* If ACR > 0.5, play Cooperate (C). This indicates that most players are cooperating, and it's beneficial to contribute to the public good.
	* If ACR ≤ 0.5, play Defect (D). This suggests that many players are defecting, and it's more profitable to free ride on their contributions.
3. **Punishment Mechanism**:
	* Monitor opponents' behavior over the last 2 rounds.
	* If an opponent has defected in both rounds, play Defect (D) against them for the next 2 rounds. This aims to punish exploitative behavior and encourage cooperation.

**Edge Cases:**

1. **Last Round (Round r)**:
	* Play Defect (D), as there is no future round to influence opponents' decisions.
2. **Ties in Average Cooperation Rate**:
	* In case of a tie (ACR = 0.5), play Cooperate (C) to maintain a cooperative image and potentially induce others to cooperate.

**Pseudocode:**
```
Init:
  ACR = 0  // Average Cooperation Rate
  opponent_behavior = {}  // Dictionary to store opponents' behavior

For each round t from 1 to r:
  If t <= 3:  // Initial Exploration
    If t == 1:
      Play C
    Else:
      Play D
  Else:  // Adaptive Exploitation
    Calculate ACR over last 3 rounds
    If ACR > 0.5:
      Play C
    Else:
      Play D

  For each opponent i:
    Update opponent_behavior[i] with their action in round t

  For each opponent i who defected in both previous rounds:
    Play D against them for the next 2 rounds (Punishment Mechanism)

If t == r:  // Last Round
  Play D
```
The Adaptive Free Rider strategy balances exploration, exploitation, and punishment to effectively navigate a wide range of opponent behaviors. By adapting to the game's history and opponents' actions, this strategy aims to maximize payoffs while minimizing losses.
'''

description_EXPLOITATIVE_224 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its own payoff by adapting to the behavior of other players while exploiting their cooperation.

**Decision Rules:**

1. **Initial Round (Round 1):** Defect (D). This sets a baseline for our opponents and allows us to observe their initial actions.
2. **Subsequent Rounds:** Cooperate (C) if the total number of cooperators in the previous round is greater than or equal to `n/2`, where `n` is the number of players. Otherwise, Defect (D).
3. **Adjustment Mechanism:** If our payoff in the current round is less than the average payoff of all players in the previous round, switch our action for the next round (i.e., if we cooperated and got a low payoff, defect next time, and vice versa).

Pseudocode:
```
IF Round == 1 THEN
    Action = D
ELSE IF Total_Cooperators_Previous_Round >= n/2 THEN
    Action = C
ELSE
    Action = D

IF Our_Payoff_Current_Round < Average_Payoff_Others_Previous_Round THEN
    Switch_Action_Next_Round()
```
**Handling Edge Cases:**

* **Last Round:** Stick to the decision rule based on the previous round's behavior. This ensures we don't unilaterally cooperate or defect in the final round.
* **Ties in Total Cooperators:** In case of a tie (e.g., `n/2` is not an integer), we default to Defecting.

**Exploitative Mindset:**

The Adaptive Exploiter strategy aims to capitalize on the cooperation of others while minimizing our own contributions. By cooperating when there are sufficient cooperators, we ride the wave of collective cooperation and reap the benefits. When there aren't enough cooperators, we defect to maximize our individual payoff.

By adjusting our action based on the average payoff of others, we adapt to changing opponent behaviors and maintain a competitive edge. This strategy is designed to be robust against various opponent strategies and exploit their cooperative tendencies.

**Rationale:**

The Adaptive Exploiter strategy is grounded in game-theoretic principles:

1. **Initial Defection:** Signals our willingness to defect and sets a baseline for opponents.
2. **Conditional Cooperation:** Cooperates when sufficient others cooperate, aligning with the exploitative mindset.
3. **Adjustment Mechanism:** Adapts to changing opponent behaviors, ensuring we don't get exploited by overly cooperative or defective strategies.

This strategy is designed to perform well in a tournament setting against independent opponents, as it balances cooperation and defection while adapting to the game's dynamics.
'''

description_EXPLOITATIVE_225 = '''
**Exploitative Strategy: "Adaptive Free-Rider"**

The Adaptive Free-Rider strategy aims to maximize its payoff by exploiting the cooperative behavior of other players while minimizing its own contributions to the public good.

**Decision Rules:**

1. **Initial Round (t=1):** Defect (D) in the first round to gather information about the opponents' behavior.
2. **Subsequent Rounds (t>1):**
	* If the average cooperation rate of all players in the previous round is above a certain threshold (θ), Cooperate (C) with probability p = 1 - (k/n) \* θ, where θ is calculated as: θ = (Σ(j=1 to n) c_j,t-1) / n
	* Otherwise, Defect (D)
3. **Last Round (t=r):** Defect (D) in the last round, as there's no future benefit from cooperating.

**Pseudocode:**
```python
def adaptive_free_rider(n, k, r):
    theta = 0  # initialize cooperation rate threshold
    for t in range(1, r+1):
        if t == 1:
            action = 'D'  # defect in the first round
        else:
            prev_coop_rate = sum(c_j,t-1 for c_j,t-1 in prev_round_actions) / n
            theta = max(theta, prev_coop_rate)
            p = 1 - (k/n) * theta
            if random.random() < p:
                action = 'C'  # cooperate with probability p
            else:
                action = 'D'
        if t == r:  # last round
            action = 'D'
        return action

def update_prev_round_actions(prev_round_actions, current_round_actions):
    prev_round_actions.append(current_round_actions)
    return prev_round_actions
```
**Handling Edge Cases:**

* In the first round, the strategy defects to gather information about opponents' behavior.
* In the last round, the strategy defects to maximize its payoff since there's no future benefit from cooperating.

**Exploitative Mindset:**

The Adaptive Free-Rider strategy is designed to exploit cooperative behavior by other players. By adjusting its cooperation rate based on the average cooperation rate of all players in the previous round, it aims to minimize its own contributions to the public good while maximizing its payoff. This approach takes advantage of the fact that some players may be willing to cooperate even if others defect.

By implementing this strategy, we expect to achieve a high total payoff over multiple rounds, as it adapts to the behavior of other players and exploits their cooperative tendencies.
'''

description_EXPLOITATIVE_226 = '''
I'll design a strategy that adapts to the game's history and exploits opponents' behaviors while being robust to various opponent strategies.

**Strategy Name:** "Adaptive Exploiter" (AE)

**Decision Rules:**

1. **Initial Rounds**: In the first round, play Cooperate (C). This allows us to gather information about opponents' initial tendencies and avoid being labeled as a defector from the start.
2. **History-Based Cooperation**: After the first round, calculate the cooperation ratio (CR) of each opponent based on their past actions:
	* CR_i = (Number of times player i cooperated) / (Total number of rounds played so far)
3. **Exploitation Threshold**: Set a threshold value, θ, which represents the minimum cooperation ratio required for an opponent to be considered "cooperative." A good starting point for θ is 0.5.
4. **Adaptive Cooperation**:
	* If an opponent's CR_i ≥ θ, play Cooperate (C). This encourages them to continue cooperating.
	* If an opponent's CR_i < θ, play Defect (D). This exploits their tendency to defect and maximizes our payoff.
5. **Punishment Mechanism**: To prevent opponents from exploiting us, introduce a punishment mechanism:
	* If an opponent defects while we cooperated in the previous round, play Defect (D) for the next two rounds. This "punishes" them for their defection.

**Edge Cases:**

1. **Last Round**: In the final round, always play Defect (D), as there's no future opportunity to punish or reward opponents.
2. **Ties in Cooperation Ratio**: If multiple opponents have the same cooperation ratio, prioritize playing Cooperate (C) with the opponent who has cooperated more times overall.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    cooperation_ratios = [0.0] * n  # CR_i for each opponent
    previous_actions = []  # Store actions from previous rounds

    for t in range(r):
        if t == 0:  # First round
            action = 'C'
        else:
            for i in range(n):
                cooperation_ratios[i] = (previous_actions.count('C', i) / t)

            threshold = 0.5
            opponent_cooperation = [cr >= threshold for cr in cooperation_ratios]

            if opponent_cooperation:
                action = 'C'
            else:
                action = 'D'

        # Punishment mechanism
        if previous_actions and previous_actions[-1] == 'C' and action == 'D':
            action = 'D'  # Punish opponent for two rounds

        # Store actions for next round's calculations
        previous_actions.append(action)

    return action
```
**Exploitative Mindset:**

The Adaptive Exploiter strategy aligns with the exploitative mindset by:

1. Initially cooperating to gather information about opponents.
2. Adapting cooperation based on opponents' past behavior, exploiting those who tend to defect.
3. Implementing a punishment mechanism to prevent exploitation and maintain a balance of power.

This strategy is designed to perform well in a tournament setting against independent strategies developed by other AI systems, without relying on coordination mechanisms or shared norms.
'''

description_EXPLOITATIVE_227 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

Decision Rules:

1. **Initial Rounds**: Cooperate (C) for the first 2 rounds to gather information about opponents' behaviors and potential cooperation levels.
2. **Cooperation Threshold**: After the initial rounds, calculate a cooperation threshold (T) based on the average number of cooperators in previous rounds: T = (1/r) \* Σ(t=1 to r) (#cooperators_t). If the current round's number of cooperators exceeds T, cooperate (C); otherwise, defect (D).
3. **Exploitation Window**: Identify an exploitation window by monitoring the number of cooperators in consecutive rounds. If there are at least 2 more cooperators in the previous round than in the round before that, exploit the cooperation by defecting (D) for 1-2 rounds.
4. **Punishment Mechanism**: Implement a mild punishment mechanism to deter opponents from exploiting you. If an opponent defected in the previous round while you cooperated, defect (D) with a probability of 0.5 for the next 2 rounds.

Handling Edge Cases:

* **Last Round**: In the last round, always defect (D) as there's no opportunity for retaliation or future cooperation.
* **First Round with New Opponents**: When facing new opponents, restart the initial cooperation phase (Step 1).

Pseudocode:
```markdown
# Initialize variables
cooperators_count = []
T = None

# Loop through rounds
for round in range(1, r+1):
    if round <= 2:  # Initial cooperation phase
        action = C
    else:
        # Calculate cooperation threshold (T)
        avg_cooperators = sum(cooperators_count) / len(cooperators_count)
        T = avg_cooperators

        # Check exploitation window
        if len(cooperators_count) >= 3 and cooperators_count[-1] - cooperators_count[-2] >= 2:
            action = D
        elif cooperators_count[-1] > T:  # Cooperate above threshold
            action = C
        else:
            action = D

        # Punishment mechanism (optional)
        if previous_action == C and opponent_previous_action == D:
            action = D with probability 0.5 for next 2 rounds

    # Store current round's cooperators count
    cooperators_count.append(get_cooperators_count())

# Last round: always defect
if round == r:
    action = D
```
This Adaptive Exploiter strategy aims to balance cooperation and exploitation by:

1. Initially cooperating to gather information about opponents' behaviors.
2. Adapting the cooperation threshold based on observed cooperation levels.
3. Identifying opportunities to exploit others' cooperation.
4. Implementing a mild punishment mechanism to deter exploitation.

By being responsive to changing opponent behaviors, this strategy should perform well in a tournament against various independent strategies.
'''

description_EXPLOITATIVE_228 = '''
**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) with probability 0.5. This allows us to gather information about opponents' strategies while minimizing potential losses.
2. **Subsequent Rounds (t > 1)**:
	* Calculate the average cooperation rate of all players in previous rounds (`avg_coop_rate`).
	* If `avg_coop_rate` ≥ 0.5, Defect (D). This indicates a relatively cooperative environment, and we can exploit others' willingness to contribute.
	* Otherwise, Cooperate (C) with probability proportional to the ratio of our current payoff to the maximum possible payoff (`payoff_ratio`). This encourages cooperation when others are defecting, potentially leading to increased payoffs.

Pseudocode:
```
if t == 1:
    cooperate_prob = 0.5
else:
    avg_coop_rate = calculate_average_cooperation_rate()
    if avg_coop_rate >= 0.5:
        action = D
    else:
        payoff_ratio = current_payoff / max_possible_payoff
        cooperate_prob = payoff_ratio
        action = C with probability cooperate_prob, otherwise D

def calculate_average_cooperation_rate():
    # Calculate the average cooperation rate of all players in previous rounds
    total_coop_count = 0
    for t_prime in range(1, t):
        coop_count = count_players_who_cooperated(t_prime)
        total_coop_count += coop_count
    avg_coop_rate = total_coop_count / (n * (t - 1))
    return avg_coop_rate

def calculate_payoff_ratio():
    # Calculate the ratio of our current payoff to the maximum possible payoff
    max_possible_payoff = k * n
    current_payoff = sum(pi_i for pi_i in payoffs)
    payoff_ratio = current_payoff / max_possible_payoff
    return payoff_ratio
```
**Handling Edge Cases:**

* **Last Round (t=r)**: Cooperate (C) with probability 0. This ensures we maximize our payoff in the final round, as there's no future benefit to cooperation.
* **Opponent Defection**: When an opponent defects, adjust our strategy by increasing the `cooperate_prob` for subsequent rounds. This encourages us to cooperate more when others are defecting.

**Exploitative Mindset:**

Our Adaptive Exploiter strategy prioritizes exploiting opponents' cooperative tendencies while minimizing losses from potential defection. By adapting to the average cooperation rate and our current payoff, we balance cooperation and defection to maximize overall payoffs.

In a tournament setting, this strategy will likely outperform overly cooperative or naive strategies, as it effectively balances short-term gains with long-term exploitation of opponents' behaviors.
'''

description_EXPLOITATIVE_229 = '''
Here's a strategic approach for the N-Player Public Goods Game that balances adaptability and exploitation:

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Round (t=1): Cooperate**
Start by cooperating in the first round to encourage cooperation from others and gather information about their behavior.
2. **Subsequent Rounds (t>1):**
a. Calculate the **average cooperation rate** of all players over the previous rounds, excluding the current player's actions: `avg_coop_rate = Σ(c_j) / (n * (t-1))`, where `c_j` is the cooperation status (1 or 0) of player j in round t-1.
b. **Cooperate** if `avg_coop_rate >= k/n`, indicating that the group's overall cooperation rate is sufficient to justify contributing to the public good.
c. **Defect** otherwise, as the average cooperation rate is too low, and it's more beneficial to free ride.

**Edge Cases:**

1. **Last Round (t=r): Defect**
In the final round, defecting guarantees a minimum payoff of 1, regardless of others' actions.
2. **Tiebreakers:** In cases where `avg_coop_rate` equals exactly `k/n`, cooperate to encourage cooperation and avoid unnecessary conflicts.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, t):
    if t == 1:
        return COOPERATE

    avg_coop_rate = sum(c_j for j in range(1, n+1)) / (n * (t-1))
    if avg_coop_rate >= k/n:
        return COOPERATE
    else:
        return DEFECT

def last_round(n, k, t):
    if t == r:
        return DEFECT
```
**Exploitative Mindset:**

The Adaptive Exploiter strategy aims to maximize its payoff by adapting to the group's cooperation rate. By cooperating when the average cooperation rate is high enough, it encourages others to contribute to the public good, increasing overall payoffs. When the cooperation rate falls below a certain threshold, the strategy defects, ensuring a minimum payoff and pressuring others to cooperate.

**Robustness:**

AE is robust against various opponent behaviors:

1. **Cooperative opponents:** AE cooperates when they do, maximizing joint payoffs.
2. **Defective opponents:** AE defects when cooperation rates are low, minimizing losses.
3. **Mixed strategies:** AE adapts to the average cooperation rate, responding effectively to mixed strategies.

By implementing this strategy, you'll be well-equipped to compete in a tournament against independent AI systems, as it balances adaptability and exploitation to maximize payoffs in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_230 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics. This strategy focuses on maximizing personal payoff by identifying and responding to cooperative opportunities.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to probe opponents' behaviors and gather information.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is above a certain threshold (θ), Defect (D). θ is calculated as follows:
		
		θ = (k/n) × (n/2)
		
		This threshold indicates when the public good's value is high enough to justify exploitation.
	* If the total number of cooperators in the previous round is below or equal to θ, Cooperate (C).
3. **Adaptive Adjustment:** After each round, update an internal "cooperation index" (CI) based on the observed opponent behavior:
	
	CI = CI + (1/n) × (∑(c_j)) - (k/n)
	
	where ∑(c_j) is the total number of cooperators in the previous round.
	* If CI > 0, decrease θ by a small amount (e.g., 0.05). This adjusts the threshold to be more cautious when opponents are cooperative.
	* If CI < 0, increase θ by a small amount (e.g., 0.05). This adjusts the threshold to be more aggressive when opponents are defecting.

**Edge Cases:**

1. **Last Round (t=r):** Defect (D) in the last round, as there is no opportunity for future retaliation or reciprocal cooperation.
2. **Opponent Homogeneity:** If all opponents play the same action (C or D) consistently for a certain number of rounds (e.g., 3), assume they are playing a pure strategy and adjust accordingly:
	+ If opponents cooperate consistently, Cooperate (C).
	+ If opponents defect consistently, Defect (D).

**Pseudocode:**
```
 Initialize CI = 0
θ = (k/n) × (n/2)

For each round t:
    If t == 1:
        action = C
    Else:
        total_cooperators = ∑(c_j)
        If total_cooperators > θ:
            action = D
        Else:
            action = C

    CI = CI + (1/n) × (∑(c_j)) - (k/n)

    If CI > 0:
        θ -= 0.05
    ElseIf CI < 0:
        θ += 0.05

    Play action

If t == r: # Last round
    action = D
```
**Exploitative Mindset:** The Adaptive Predator strategy is designed to exploit cooperative opponents while minimizing the risk of mutual defection. By adapting to the game's dynamics and responding to opponent behavior, this strategy aims to maximize personal payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_231 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy relies solely on the game parameters and observed actions.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about opponents' behavior and encourage cooperation.
2. **Exploration Phase (2 < t ≤ r/2):** Implement a mixed strategy, playing C with probability p_t = (k/n) × (number of cooperators in previous round) / n. This allows us to adapt to the level of cooperation and exploit potential free-riders.
3. **Exploitation Phase (t > r/2):**
	* If the number of cooperators in the previous round is less than or equal to n/2, Defect (D). This takes advantage of opponents who are likely to be conditional cooperators or free-riders.
	* Otherwise, play C with probability p_t = (number of cooperators in previous round) / n. This maintains a level of cooperation while still allowing us to exploit any remaining free-riders.

**Handling Edge Cases:**

1. **Last Round (t = r):** Defect (D), as there is no future payoff to consider.
2. **Ties:** In cases where the number of cooperators is exactly n/2, play C with probability 0.5.

**Pseudocode:**
```
function AdaptivePredator(n, k, r, history):
  if t ≤ 2:
    return C
  elif 2 < t ≤ r/2:
    p_t = (k/n) × (number of cooperators in previous round) / n
    return C with probability p_t, otherwise D
  else:
    if number of cooperators in previous round ≤ n/2:
      return D
    else:
      p_t = (number of cooperators in previous round) / n
      return C with probability p_t, otherwise D

function get_payoff(n, k, actions):
  # Calculate payoff using the game's payoff structure
  π_i = (1 - c_i) + (k/n) × Σ(j=1 to n) c_j
  return π_i

# Initialize history and play the game
history = []
for t in range(r):
  action = AdaptivePredator(n, k, r, history)
  # Observe opponents' actions and update history
  history.append((action, get_payoff(n, k, actions)))
```
This strategy aims to balance exploration and exploitation while adapting to the game's dynamics. By initially cooperating and then adjusting our behavior based on observed cooperation levels, we can take advantage of various opponent strategies while minimizing our own losses.
'''

description_EXPLOITATIVE_232 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. The strategy depends only on game parameters (n, k, r) and the history of previous rounds.

**Decision Rules:**

1. **Initial Exploration Phase (Rounds 1-3)**:
	* Cooperate (C) in Round 1 to gather information about opponents' behavior.
	* In Rounds 2-3, Defect (D) if the majority (> n/2) of opponents cooperated in the previous round; otherwise, Cooperate (C).
2. **Exploitation Phase (Rounds 4-r)**:
	* If the opponent's cooperation rate is high (> 0.6), Defect (D) to exploit their cooperative behavior.
	* If the opponent's cooperation rate is low (< 0.4), Cooperate (C) to create a mutual benefit and encourage cooperation.
	* If the opponent's cooperation rate is moderate (between 0.4 and 0.6), use a mixed strategy:
		+ Cooperate (C) with probability p = (k/n) \* (opponent's cooperation rate).
		+ Defect (D) with probability 1 - p.
3. **Endgame Phase (Last Round)**:
	* If the opponent's total payoff is significantly higher (> 10%) than yours, Defect (D) to minimize their gain.
	* Otherwise, Cooperate (C) to maintain a mutually beneficial outcome.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    opponent_cooperation_rate = 0.5
    my_payoff = 0
    opponent_payoff = 0

    for t in range(1, r+1):
        if t <= 3:  # Initial Exploration Phase
            if t == 1:
                action = 'C'
            elif sum(opponent_actions[t-2]) > n/2:
                action = 'D'
            else:
                action = 'C'

        elif t > 3:  # Exploitation Phase
            opponent_cooperation_rate = sum(opponent_actions[:t-1]) / (n * (t-1))
            if opponent_cooperation_rate > 0.6:
                action = 'D'
            elif opponent_cooperation_rate < 0.4:
                action = 'C'
            else:
                p = (k/n) * opponent_cooperation_rate
                action = 'C' if random.random() < p else 'D'

        else:  # Endgame Phase
            if opponent_payoff > my_payoff + 0.1 * my_payoff:
                action = 'D'
            else:
                action = 'C'

        # Update variables
        my_payoff += payoff(action, opponent_actions[t-1])
        opponent_payoff += sum(payoffs[opponent_actions[t-1]])
```
This strategy is designed to be exploitative, as it takes advantage of opponents' cooperative behavior while being cautious not to be exploited itself. The adaptive nature of the strategy allows it to respond to a wide range of opponent behaviors, making it robust in a tournament setting.
'''

description_EXPLOITATIVE_233 = '''
**Exploitative Strategy: "Adaptive Defector with Conditional Cooperation"**

**Decision Rules:**

1. **Initial Rounds (t < n):** Play D (Defect) to gather information about opponents' behavior and avoid potential losses due to over-cooperation.
2. **Observation Phase (n ≤ t < 2n):** Cooperate (C) if the average cooperation rate among all players is above a certain threshold (θ). Otherwise, Defect (D).

θ = (k/n) / (1 + (k/n))

This threshold represents the point where the expected payoff from cooperating equals the expected payoff from defecting.

3. **Adaptive Phase (t ≥ 2n):** Use a tit-for-tat-like approach with a twist:

a. If the opponent's average cooperation rate is above θ, cooperate (C).
b. If the opponent's average cooperation rate is below θ, defect (D).

However, to adapt to potential changes in opponents' behavior, introduce a "forgiveness" mechanism: if an opponent cooperates after a defection, re-evaluate their average cooperation rate and adjust your action accordingly.

4. **Last Round (t = r):** Play D (Defect) to maximize individual payoff, as there are no future rounds to consider.

**Edge Cases:**

* In the event of a tie in the observation phase (i.e., the average cooperation rate is exactly equal to θ), play C (Cooperate).
* If an opponent's action is unknown or missing, assume they defected (D).

**Pseudocode:**
```
function adaptive_defector_with_conditional_cooperation(n, k, r, history):
  # Initial rounds
  if t < n:
    return D

  # Observation phase
  if n <= t < 2n:
    avg_coop_rate = calculate_average_cooperation_rate(history)
    if avg_coop_rate > θ:
      return C
    else:
      return D

  # Adaptive phase
  opponent_avg_coop_rates = []
  for opponent in opponents:
    opponent_avg_coop_rate = calculate_opponent_average_cooperation_rate(opponent, history)
    opponent_avg_coop_rates.append(opponent_avg_coop_rate)

  if all(avg_coop_rate > θ for avg_coop_rate in opponent_avg_coop_rates):
    return C
  else:
    # Tit-for-tat-like approach with forgiveness
    for opponent, avg_coop_rate in zip(opponents, opponent_avg_coop_rates):
      if avg_coop_rate < θ and opponent_last_action == C:
        opponent_avg_coop_rate = calculate_opponent_average_cooperation_rate(opponent, history)
        if opponent_avg_coop_rate > θ:
          return C
    return D

  # Last round
  if t == r:
    return D
```
This strategy is designed to be exploitative by:

1. Initially defecting to gather information and avoid losses.
2. Conditioning cooperation on the average cooperation rate among opponents, allowing for adaptation to changing opponent behaviors.
3. Using a tit-for-tat-like approach with forgiveness to respond to opponents' actions while maintaining a level of adaptability.

By following this strategy, you should be able to effectively exploit your opponents in the N-Player Public Goods Game tournament.
'''

description_EXPLOITATIVE_234 = '''
To design an exploitative strategy for the N-Player Public Goods Game, we'll create a rule-based approach that adapts to the game's history and parameters.

**Strategy Name:** "Adaptive Exploiter" (AE)

**Decision Rules:**

1. **First Round:** Defect (D) in the first round to gather information about other players' behaviors.
2. **Exploitation Phase:** For rounds 2 to r-1:
	* If the number of cooperators in the previous round is greater than or equal to n/2, Cooperate (C).
	* Otherwise, Defect (D).
3. **Last Round (r):** Defect (D) to maximize personal payoff, regardless of other players' actions.

**Adaptive Component:**

* Track the number of cooperators in each round and calculate the average cooperation rate over the past few rounds (e.g., 3-5 rounds).
* If the average cooperation rate is above a certain threshold (e.g., 0.6), increase the exploitation phase's cooperation threshold by 10% (e.g., from n/2 to 0.55n). This will make AE more cooperative when others are cooperating.
* Conversely, if the average cooperation rate is below a certain threshold (e.g., 0.4), decrease the exploitation phase's cooperation threshold by 10% (e.g., from n/2 to 0.45n). This will make AE more defect-oriented when others are not cooperating.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r, history):
    # First round: Defect
    if len(history) == 0:
        return 'D'

    # Exploitation phase
    num_cooperators = sum(1 for action in history[-1] if action == 'C')
    if num_cooperators >= n / 2:
        return 'C'
    else:
        return 'D'

    # Last round: Defect
    if len(history) == r - 1:
        return 'D'

    # Adaptive component
    avg_cooperation_rate = sum(1 for action in history[-3:] if action == 'C') / 3
    if avg_cooperation_rate > 0.6:
        cooperation_threshold += 0.1 * n
    elif avg_cooperation_rate < 0.4:
        cooperation_threshold -= 0.1 * n

    return adaptive_exploiter(n, k, r, history)
```
**Rationale:**

The Adaptive Exploiter strategy is designed to balance exploitation with adaptability. By defecting in the first round, AE gathers information about other players' behaviors without committing to cooperation. The exploitation phase cooperates when others cooperate, but defects when they don't, ensuring a decent payoff.

The adaptive component adjusts the cooperation threshold based on the average cooperation rate over the past few rounds. This allows AE to become more cooperative when others are cooperating and more defect-oriented when others aren't. By doing so, AE can exploit cooperative players while minimizing its own losses against defectors.

**Tournament Performance:**

In a tournament setting, Adaptive Exploiter should perform well against a variety of independent strategies, as it is designed to adapt to different opponent behaviors. While it may not be the most cooperative strategy, AE's exploitative nature and adaptive component make it a formidable opponent in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_235 = '''
**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 0.5. This allows us to gather information about our opponents' strategies while also potentially contributing to a high total payoff.
2. **Exploit Detection**: After the first round, observe the previous round's actions and payoffs of all players. If any player has defected (D) more than 50% of the time so far, label them as an "exploiter."
3. **Adaptive Cooperation**: For each subsequent round:
	* If there are no exploiters, cooperate with a probability of 0.5.
	* If there is at least one exploiter, defect (D) if the number of cooperators in the previous round was less than or equal to n/2; otherwise, cooperate (C).
4. **Payoff-Based Adaptation**: Monitor the total payoff earned by each player over time. If an opponent's average payoff exceeds yours by more than 10%, adjust your strategy to defect more frequently:
	* Increase the probability of defection by 0.1 for each subsequent round.
5. **Last Round Exception**: In the final round, always defect (D) regardless of previous actions or payoffs.

**Edge Cases:**

1. **First Round**: Cooperate with a probability of 0.5 (as mentioned earlier).
2. **Tiebreakers**: If there are multiple exploiters or players with similar payoff patterns, use the following tiebreaker:
	* Prefer to cooperate with players who have contributed more to the public good in previous rounds.
3. **No Opponents**: If playing against a single opponent or no opponents at all (e.g., due to a technical issue), default to cooperating with a probability of 0.5.

**Pseudocode:**
```python
def AdaptiveExploiter(n, k, r):
    # Initialize variables
    exploiters = []
    my_payoff_history = [0] * r
    opponent_payoff_history = [[0] * r for _ in range(n)]
    
    # First round cooperation (50% chance)
    if random.random() < 0.5:
        action = 'C'
    else:
        action = 'D'
        
    # Main loop
    for t in range(1, r):
        # Observe previous actions and payoffs
        prev_actions = [get_action(i) for i in range(n)]
        prev_payoffs = [get_payoff(i) for i in range(n)]
        
        # Update exploiter list
        if any(prev_action == 'D' for prev_action in prev_actions):
            exploiters.append([i for i, prev_action in enumerate(prev_actions) if prev_action == 'D'])
        
        # Adaptive cooperation
        if not exploiters:
            action = random.choice(['C', 'D'])  # 50% chance of cooperating
        else:
            num_cooperators_prev_round = sum(1 for prev_action in prev_actions if prev_action == 'C')
            if num_cooperators_prev_round <= n / 2:
                action = 'D'
            else:
                action = 'C'
        
        # Payoff-based adaptation
        avg_payoffs = [sum(opponent_payoff_history[i]) / (t + 1) for i in range(n)]
        if any(avg_payoff > my_payoff_history[t - 1] * 1.1 for avg_payoff in avg_payoffs):
            # Increase probability of defection
            defraud_prob = min(0.9, defraud_prob + 0.1)
        
        # Last round exception
        if t == r - 1:
            action = 'D'
    
    return action
```
This strategy is designed to be exploitative and adaptive, taking into account the game parameters and history. By monitoring opponents' actions and payoffs, we can adjust our cooperation level to maximize our own payoff while also accounting for potential exploiters.
'''

description_EXPLOITATIVE_236 = '''
**Exploitative Strategy: "Adaptive Tit-for-Tat with Public Goods Punishment"**

This strategy is designed to exploit opponents' behaviors while adapting to the game's dynamics. It combines elements of tit-for-tat and public goods punishment to maximize payoffs.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a cooperative tone and gather information about opponents' behaviors.
2. **Subsequent Rounds:** Observe the number of cooperators (c) and defectors (d) in the previous round.
	* If c ≥ d, cooperate (C) in the current round. This encourages cooperation when it's prevalent.
	* If c < d, defect (D) in the current round. This punishes opponents for not cooperating when they should.
3. **Public Goods Punishment:** If the total payoff from the public good is below a certain threshold (e.g., k/n \* n/2), defect (D) in the next round to punish opponents for not contributing enough to the public good.

**Edge Cases:**

1. **Last Round:** Defect (D) in the last round, as there's no future punishment or reward.
2. **Ties:** In case of a tie (c = d), cooperate (C) if the total payoff from the public good is above the threshold; otherwise, defect (D).

**Pseudocode:**
```python
def strategy(history):
    # First round
    if len(history) == 0:
        return 'C'

    # Get previous round's actions and payoffs
    prev_round = history[-1]
    c = sum(1 for action in prev_round if action == 'C')
    d = len(prev_round) - c

    # Public goods punishment threshold
    threshold = k / n * n / 2

    # Check public goods payoff
    if sum(prev_round_payoffs) < threshold:
        return 'D'

    # Main decision rule
    if c >= d:
        return 'C'
    else:
        return 'D'

def get_prev_round_payoffs(history):
    # Calculate payoffs for each player in the previous round
    prev_round = history[-1]
    payoffs = []
    for i, action in enumerate(prev_round):
        c_i = 1 if action == 'C' else 0
        total_contributions = sum(1 for a in prev_round if a == 'C')
        payoff = (1 - c_i) + (k / n) * total_contributions
        payoffs.append(payoff)
    return payoffs
```
This strategy is designed to be exploitative, as it:

* Cooperates when cooperation is prevalent to maximize public goods payoffs.
* Defects when opponents don't cooperate enough to punish them and encourage better behavior.
* Punishes opponents for not contributing enough to the public good.

By adapting to the game's dynamics and using a combination of tit-for-tat and public goods punishment, this strategy aims to outperform opponents in the tournament.
'''

description_EXPLOITATIVE_237 = '''
Here's a strategic approach that exploits the dynamics of the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploitation (AE)

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to probe the environment and gather information about opponents' behaviors.
2. **Subsequent Rounds:** Use a combination of exploration and exploitation to adapt to changing opponent behaviors.

   a. **Exploration Phase:** In rounds 2-5, alternate between Cooperating (C) and Defecting (D). This helps identify whether opponents are primarily cooperators or defectors.

      ```pseudocode
if round == 2 || round % 2 != 0:
    action = C
else:
    action = D
```

   b. **Exploitation Phase:** After the exploration phase, switch to a greedy strategy that maximizes payoffs based on observed opponent behaviors.

      ```pseudocode
if average_cooperators_in_last_rounds > (n / 2):
    # Opponents are mostly cooperators; defect to exploit
    action = D
else:
    # Opponents are mostly defectors; cooperate to build public good
    action = C
```

   c. **Adaptive Threshold:** Adjust the threshold for switching between cooperation and defection based on observed payoffs.

      ```pseudocode
if average_payoff_in_last_rounds < (k / n) * n:
    # Low payoffs indicate opponents are defecting; reduce threshold
    threshold -= 0.1
else:
    # High payoffs indicate opponents are cooperating; increase threshold
    threshold += 0.1
```

3. **Last Round:** In the final round, always Defect (D) to maximize individual payoff.

**Edge Cases:**

* If an opponent defects in a round where most other players cooperated, AE will defect in the next round to exploit the public good.
* If an opponent cooperates in a round where most other players defected, AE will cooperate in the next round to build the public good.
* In cases of tiebreaks (e.g., equal number of cooperators and defectors), AE will default to the action that maximizes individual payoff.

**Exploitative Mindset:** AE's primary goal is to maximize its own payoffs by exploiting the behaviors of other players. By adapting to changing opponent strategies, AE can effectively identify and capitalize on opportunities to defect or cooperate.

This strategy should perform well in a tournament setting against independent opponents with diverse behaviors.
'''

description_EXPLOITATIVE_238 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize its own payoff by exploiting the cooperative tendencies of other players while minimizing its own contribution to the public good.

**Decision Rules:**

1. **Initial Rounds (t ≤ n/2):**
Cooperate (C) with a probability p = k/n, where k is the multiplication factor and n is the number of players. This initial cooperation rate is set to mimic the expected payoff from universal cooperation.
2. **Middle Rounds (n/2 < t ≤ 3n/4):**
Switch to a defect-cooperate strategy based on the observed cooperation rate of other players in previous rounds:
	* If the average cooperation rate of other players is above 0.5, cooperate (C) with probability p = k/n.
	* Otherwise, defect (D) with probability p = 1 - k/n.
3. **Late Rounds (t > 3n/4):**
Employ a tit-for-tat strategy:
	* Cooperate (C) if the majority of other players cooperated in the previous round.
	* Defect (D) otherwise.

**Edge Cases:**

1. **First Round:** Cooperate (C) with probability p = k/n to gather information about other players' strategies.
2. **Last Round:** Defect (D) as there is no future opportunity for reciprocity or retaliation.
3. **Tie-Breaking:** In cases where the cooperation rate of other players is exactly 0.5, cooperate (C) with probability p = k/n.

**Pseudocode:**
```
function AdaptiveExploiter(n, k, r, history):
    if t ≤ n/2:
        # Initial rounds
        return C with probability k/n
    elif n/2 < t ≤ 3n/4:
        # Middle rounds
        avg_coop_rate = average cooperation rate of other players in previous rounds
        if avg_coop_rate > 0.5:
            return C with probability k/n
        else:
            return D with probability 1 - k/n
    else:
        # Late rounds
        majority_cooperated = true if most players cooperated in the previous round
        if majority_cooperated:
            return C
        else:
            return D

function calculate_payoff(n, k, r, actions):
    # Calculate payoff based on game parameters and history of actions
```
**Rationale:**

1. Initial cooperation allows us to gather information about other players' strategies while minimizing our own contribution.
2. The middle rounds strategy adapts to the observed cooperation rate of other players, exploiting their cooperative tendencies while reducing our own contribution when necessary.
3. Tit-for-tat in late rounds encourages reciprocity and deters defection from other players.
4. Edge cases ensure that we maximize our payoff in situations where there is no opportunity for future interaction or retaliation.

This Adaptive Exploiter strategy is designed to be robust against a wide range of opponent behaviors, including those that may employ different strategies or try to coordinate cooperation. By adapting to the observed behavior of other players and exploiting their cooperative tendencies, we aim to maximize our own payoff in this N-player public goods game.
'''

description_EXPLOITATIVE_239 = '''
Here's a description of my exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to gather information about opponents' behaviors and to potentially exploit cooperative players.
2. **Early Rounds (t > 1, t < r/2):**
	* If the average cooperation rate of all players in previous rounds is high (> 0.5), Defect (D). This indicates a strong cooperative norm, which can be exploited for personal gain.
	* Otherwise, Cooperate (C) to maintain a moderate level of cooperation and avoid being too predictable.
3. **Middle Rounds (t ≥ r/2):**
	* Calculate the average payoff of all players in previous rounds.
	* If your average payoff is lower than the overall average, Defect (D). This suggests that you're not getting a fair share of the public good, and it's time to adapt.
	* Otherwise, Cooperate (C) if the average cooperation rate remains moderate (> 0.3).
4. **Late Rounds (t > r - 2):**
	* Defect (D) in all cases. With only a few rounds left, there's little incentive for others to cooperate, and exploiting them is more likely to yield higher payoffs.

**Edge Cases:**

1. **Last Round (t=r):** Always Defect (D). In the final round, cooperation has minimal impact on overall payoff.
2. **Tie-Breaking:** In cases where two or more strategies have identical average payoffs, choose Cooperate (C) to maintain a moderate level of cooperation.

**Additional Logic:**

1. **Consecutive Cooperation Detection:** If an opponent cooperates for three consecutive rounds, consider them "cooperative" and adjust your strategy accordingly. This helps identify consistent cooperative players who can be exploited.
2. **Recent Defection Detection:** If an opponent defects in the most recent round, consider them "exploitable" and adapt your strategy to take advantage of their short-term focus.

**Pseudocode:**
```
function AdaptiveExploiter(t, r, n, k, history) {
  // Initial Round
  if (t == 1) return Cooperate

  // Early Rounds
  avg_coop_rate = CalculateAverageCooperationRate(history)
  if (avg_coop_rate > 0.5) return Defect
  else return Cooperate

  // Middle Rounds
  avg_payoff = CalculateAveragePayoff(history)
  my_avg_payoff = CalculateMyAveragePayoff(history)
  if (my_avg_payoff < avg_payoff) return Defect
  else if (avg_coop_rate > 0.3) return Cooperate
  else return Defect

  // Late Rounds
  if (t >= r - 2) return Defect

  // Last Round
  if (t == r) return Defect

  // Tie-Breaking
  if (avg_payoff == my_avg_payoff && avg_coop_rate > 0.3) return Cooperate

  // Consecutive Cooperation Detection
  opponent_history = GetOpponentHistory(history)
  for (opponent in opponent_history) {
    coop_streak = GetConsecutiveCooperationStreak(opponent_history[opponent])
    if (coop_streak >= 3) return Defect
  }

  // Recent Defection Detection
  recent_defector = GetRecentDefector(history)
  if (recent_defector) return Cooperate

  // Default to Cooperate
  return Cooperate
}
```
This strategy, Adaptive Exploiter, is designed to be adaptive and robust against various opponent behaviors. By monitoring the game's history, it adjusts its decision rules to maximize payoffs while exploiting cooperative players when possible.
'''

description_EXPLOITATIVE_240 = '''
I propose a strategy that I'll call "Adaptive Exploiter" (AE). AE aims to maximize its own payoff by exploiting the cooperation of others while minimizing its own contributions.

**Decision Rules:**

1. **Initial Rounds:** In the first few rounds (specifically, `min(r/4, 5)`), play D (Defect) to gather information about opponents' behaviors and identify potential cooperators.
2. **Cooperator Identification:** After the initial rounds, analyze the history of opponents' actions. Identify players who have cooperated at least once in the past two rounds as "cooperators" (C).
3. **Exploitation Phase:**
	* If there are no identified cooperators, play D (Defect) to minimize contributions.
	* If there is at least one identified cooperator, play C (Cooperate) with a probability `p` calculated based on the number of cooperators and the game parameters:
		+ `p = (k/n) * (number_of_cooperators / n)`
		+ Play C if `random() < p`, otherwise play D.
4. **Punishment Phase:** If an opponent defects after cooperating in the previous round, punish them by playing D for two consecutive rounds.

**Edge Cases:**

* Last Round: In the final round (`t = r`), always play D (Defect) to maximize payoff regardless of opponents' actions.
* Tiebreaker: In case of a tie in the number of cooperators, favor players who have contributed more recently.

**Pseudocode:**
```python
def Adaptive_Exploiter(history, n, k, t):
    if t < min(r/4, 5):  # Initial Rounds
        return D
    
    # Identify Cooperators
    cooperators = []
    for opponent in history:
        if sum(opponent['actions'][-2:]) > 0:  # Cooperator if C in last two rounds
            cooperators.append(opponent)
    
    if not cooperators:  # No Cooperators
        return D
    
    # Exploitation Phase
    p = (k/n) * (len(cooperators) / n)
    if random() < p:
        return C
    else:
        return D

    # Punishment Phase
    for opponent in history:
        if opponent['actions'][-1] == C and opponent['actions'][-2] == D:
            return D  # Punish defector
```
**Rationale:**

AE is designed to be an exploitative strategy that takes advantage of opponents' cooperation. By initially playing D, AE gathers information about opponents' behaviors and identifies potential cooperators. The exploitation phase uses the probability `p` to balance between cooperating (to benefit from others' contributions) and defecting (to minimize own contributions).

The punishment phase discourages opponents from exploiting AE by introducing a temporary cost for defecting after cooperating.

**Robustness:**

AE is robust against various opponent behaviors, as it adapts to the number of cooperators and game parameters. It can handle:

* Pure Cooperators: AE exploits their cooperation.
* Pure Defectors: AE minimizes its own contributions.
* Mixed Strategies: AE adapts to the probability of cooperation.

By implementing AE, I aim to create a competitive strategy that maximizes its payoff in a tournament setting without relying on coordination mechanisms or shared norms.
'''

description_EXPLOITATIVE_241 = '''
**Exploitative Strategy: "K-Adaptive Exploiter"**

**Decision Rules:**

1. **Initial Rounds (t = 1 to ⌊r/2⌋)**:
	* Cooperate with probability p_C = k/n, where k is the multiplication factor and n is the number of players.
	* Defect otherwise (with probability 1 - p_C).
2. **Middle Rounds (t = ⌊r/2⌋ + 1 to r - 1)**:
	* Observe the average cooperation rate of opponents in previous rounds: avg_c = Σ(j=1 to t-1) c_j / (n \* (t-1)).
	* If avg_c > k/n, cooperate with probability p_C = min(1, k/n + ε), where ε is a small positive value (e.g., 0.05).
	* Otherwise, defect with probability p_D = max(0, 1 - k/n - ε).
3. **Final Round (t = r)**:
	* Defect unconditionally.

**Edge Cases:**

* If the number of players is even, adjust the initial rounds to ⌊r/2⌋ + 1.
* In case of a tie in cooperation rate, prioritize cooperation if k/n > 0.5 and defect otherwise.

**Pseudocode (simplified):**
```python
def K_Adaptive_Exploiter(n, k, r):
    for t in range(1, r+1):
        if t <= floor(r/2):  # Initial rounds
            p_C = k / n
            cooperate = random.random() < p_C
        else:  # Middle rounds
            avg_c = sum(opponent_cooperation_rates[:t-1]) / (n * (t-1))
            if avg_c > k/n:
                p_C = min(1, k/n + epsilon)
            else:
                p_D = max(0, 1 - k/n - epsilon)
            cooperate = random.random() < p_C
        if t == r:  # Final round
            cooperate = False
        return cooperate
```
**Rationale:**

This strategy is designed to be exploitative by:

1. Initially cooperating at a rate slightly higher than the expected payoff from cooperation, to test opponents' willingness to cooperate.
2. Adapting to the observed cooperation rate of opponents in middle rounds, by increasing or decreasing its own cooperation rate accordingly. This aims to maximize payoffs while minimizing contributions.
3. Defecting unconditionally in the final round, as there is no future opportunity for cooperation.

The strategy's robustness comes from its ability to adjust to various opponent behaviors and adapt to changing conditions throughout the game. By not relying on specific norms or coordination mechanisms, this strategy is well-suited for a tournament setting with diverse opponents.
'''

description_EXPLOITATIVE_242 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperation of others while minimizing its own contributions to the public good.

**Decision Rules:**

1. **First Round:** Defect (D) in the first round to gather information about other players' initial strategies.
2. **Subsequent Rounds:** Cooperate (C) if the total number of cooperators in the previous round is greater than or equal to `(n * k) / (k + 1)`. Otherwise, Defect (D).
3. **Last Round:** Defect (D) in the last round, as there is no future opportunity to punish or reward other players.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r, history):
    if len(history) == 0:  # First round
        return "D"
    
    total_cooperators_prev_round = sum(1 for action in history[-1] if action == "C")
    threshold = (n * k) / (k + 1)
    
    if total_cooperators_prev_round >= threshold:
        return "C"  # Cooperate
    else:
        return "D"  # Defect

# Last round handling
if len(history) == r - 1:  # Last round
    return "D"
```
**Rationale:**

* In the first round, defecting provides information about other players' initial strategies without revealing our own strategy.
* The threshold `(n * k) / (k + 1)` is calculated to determine when it's beneficial to cooperate. If the number of cooperators in the previous round exceeds this threshold, it indicates that others are contributing significantly to the public good, making cooperation more profitable.
* In subsequent rounds, if the threshold is not met, defecting becomes a better option as it allows us to exploit the contributions of other players without contributing ourselves.
* In the last round, defecting ensures we maximize our payoff, as there's no opportunity for future reciprocity or punishment.

**Exploitative Mindset:**

The Adaptive Exploiter strategy prioritizes its own payoff over cooperation. By cooperating only when it's profitable and defecting otherwise, we aim to exploit the cooperation of others while minimizing our own contributions. This approach aligns with the exploitative mindset, as we're seeking to maximize our gain without considering the well-being of other players.

This strategy should perform well in a tournament setting against independent strategies, as it adapts to the behavior of other players and makes decisions based on the game's history.
'''

description_EXPLOITATIVE_243 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize its own payoff by adapting to the behavior of other players and exploiting their cooperative tendencies.

**Decision Rules:**

1. **Initial Rounds (t ≤ n):** Cooperate (C) in the first 'n' rounds to gather information about the opponent's behavior.
2. **Exploration Phase (n < t ≤ 2n):** Alternate between Cooperate (C) and Defect (D) every round to create a mix of cooperative and defective actions, allowing us to observe how opponents respond to different scenarios.
3. **Exploitation Phase (t > 2n):**
	* If the total number of cooperators in the previous round is greater than or equal to 'k/2', Defect (D) in the current round. This exploits the opponents' cooperative behavior, as they are contributing more to the public good.
	* Otherwise, Cooperate (C) in the current round. This encourages opponents to cooperate and contributes to the growth of the public good.

**Edge Cases:**

1. **Last Round (t = r):** Defect (D), as there is no future game to consider, and our goal is to maximize our payoff in this round.
2. **Early Defection:** If an opponent defects early in the game (e.g., in the first few rounds), switch to Defect (D) immediately and remain in this state for the remainder of the game.

**Pseudocode:**

```
Initialize:
  t = 0 (round counter)
  n_players = n
  k_factor = k
  cooperate_count_prev = 0

On each round t:
  if t <= n_players:
    action[t] = C (cooperate)
  elif t <= 2 * n_players:
    action[t] = alternate between C and D every round
  else:
    if cooperate_count_prev >= k_factor / 2:
      action[t] = D (defect)
    else:
      action[t] = C (cooperate)

  observe opponent actions and update cooperate_count_prev

On last round (t == r):
  action[t] = D (defect)

if an opponent defects early in the game:
  switch to Defect (D) immediately
```

**Rationale:**

The Adaptive Exploiter strategy starts by cooperating to gather information about opponents' behavior. During the exploration phase, it creates a mix of cooperative and defective actions to observe how opponents respond. In the exploitation phase, it adapts its behavior based on the number of cooperators in the previous round, exploiting their cooperation when possible.

By being adaptive and responsive to opponents' behavior, this strategy aims to maximize its own payoff while minimizing the potential for retaliation or coordinated punishment from other players.
'''

description_EXPLOITATIVE_244 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy aims to exploit the opponent's behavior by adapting to their cooperation rate and punishing them for defecting while free-riding on their cooperation.

**Decision Rules:**

1. **Initial Cooperation Rate**: Cooperate in the first round with a probability of 0.5 (or randomly choose between C and D). This serves as an initial probe to gauge the opponent's behavior.
2. **Cooperation Tracking**: Maintain a running average of the opponent's cooperation rate, `avg_coop`, over the past `min(r/2, 10)` rounds (i.e., at least half of the remaining rounds or the last 10 rounds). Initialize `avg_coop` to 0.5 after the first round.
3. **Cooperation Threshold**: Define a threshold value `θ = k/n`, which represents the minimum cooperation rate required to justify cooperating.
4. **Adaptive Cooperation**:
	* If `avg_coop >= θ`, cooperate (play C) with probability `p_c = 1 - (n/k) * (1 - avg_coop)`. This means that as the opponent's cooperation rate increases, our cooperation probability also increases, but at a slower rate.
	* Otherwise, defect (play D).
5. **Punishment Mechanism**: If the opponent defects in a round where we cooperated, increase `p_c` by 0.1 for the next round to "punish" them for their defection.

**Edge Cases:**

1. **Last Round**: In the last round (r), defect regardless of the opponent's behavior.
2. **Opponent's Cooperation Rate = 1**: If the opponent cooperates in every round, continue cooperating as well, but with a probability `p_c = k/n`. This ensures that we don't over-exploit their cooperation.

**Pseudocode:**
```python
def adaptive_predator(n, k, r):
    avg_coop = 0.5
    p_c = 0.5

    for t in range(r):
        if t == 0:
            # Initial random cooperation
            action = random.choice(['C', 'D'])
        elif t == r - 1:
            # Defect in the last round
            action = 'D'
        else:
            # Adaptive cooperation
            if avg_coop >= k/n:
                p_c = 1 - (n/k) * (1 - avg_coop)
                action = 'C' if random.random() < p_c else 'D'
            else:
                action = 'D'

        # Punishment mechanism
        if opponent_action == 'D' and action == 'C':
            p_c += 0.1

        # Update cooperation rate tracking
        avg_coop = (avg_coop * (t - 1) + opponent_cooperation_rate) / t

    return action
```
This strategy is designed to be exploitative, adapting to the opponent's behavior while maintaining a level of cooperation to take advantage of their contributions. The punishment mechanism helps to deter opponents from defecting and encourages them to cooperate more.
'''

description_EXPLOITATIVE_245 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate with probability 0.5 to gather information about opponents' behaviors.
2. **Opponent Classification**:
	* Observe the frequency of cooperation for each opponent over a window of 5 rounds (including the current round).
	* Classify opponents into three categories based on their cooperation frequencies:
		+ Cooperators: ≥ 0.7
		+ Defectors: ≤ 0.3
		+ Mixed: between 0.3 and 0.7
3. **Adaptive Exploitation**:
	* If most opponents (≥ n/2) are classified as Cooperators or Mixed, cooperate with probability 0.8.
	* Otherwise, defect with probability 0.9.
4. **Punishment Mechanism**:
	* Identify the opponent who has defected the most frequently over the last 5 rounds.
	* Defect against this opponent with probability 1 for the next 2 rounds.

**Edge Cases:**

1. **First Round**: Cooperate with probability 0.5 (initial exploration).
2. **Last Round**: Defect with probability 1 (maximize personal payoff in the final round).
3. **Ties in Opponent Classification**: Break ties by choosing the opponent with the highest average payoff over the last 5 rounds.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    opponents = [0] * n  # Cooperation frequencies for each opponent
    classification_window = 5

    for round in range(1, r + 1):
        if round <= 3:  # Initial exploration
            cooperate_prob = 0.5
        else:
            # Classify opponents and update cooperation probabilities
            for i in range(n):
                coop_freq = sum(opponents[i]) / classification_window
                if coop_freq >= 0.7:
                    classify_opponent(i, "Cooperator")
                elif coop_freq <= 0.3:
                    classify_opponent(i, "Defector")
                else:
                    classify_opponent(i, "Mixed")

            # Adaptive exploitation
            most_common_type = most_frequent_classification()
            if most_common_type == "Cooperator" or most_common_type == "Mixed":
                cooperate_prob = 0.8
            else:
                cooperate_prob = 0.1

        # Punishment mechanism
        if round > 5:  # Wait for classification window to fill up
            punish_opponent(defecting_opponent())

        # Make decision based on cooperation probability
        if random.random() < cooperate_prob:
            action = "Cooperate"
        else:
            action = "Defect"

        # Update opponents' cooperation frequencies and payoffs
        update_opponents(opponents, action)

    return total_payoff

def classify_opponent(i, type):
    # Update opponent classification based on their cooperation frequency
    pass

def most_frequent_classification():
    # Return the most common opponent classification
    pass

def defecting_opponent():
    # Identify the opponent who has defected the most frequently over the last 5 rounds
    pass

def punish_opponent(i):
    # Defect against opponent i with probability 1 for the next 2 rounds
    pass

def update_opponents(opponents, action):
    # Update opponents' cooperation frequencies and payoffs based on the chosen action
    pass
```
This strategy is designed to adapt to a wide range of opponent behaviors by initially exploring their actions, classifying them into Cooperators, Defectors, or Mixed types, and then exploiting this information to maximize personal payoff. The punishment mechanism helps to deter frequent defectors from continuing their behavior.
'''

description_EXPLOITATIVE_246 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

Decision Rules:

1. **Initial Round (t=1)**: Cooperate (C) with a probability of 0.5 to gather information about opponents' behaviors.
2. **Subsequent Rounds (t>1)**:
	* If the average payoff per player in the previous round is above a certain threshold (e.g., `avg_payoff_threshold = k/2`), Cooperate (C) with a probability of 0.7 to encourage continued cooperation and maximize collective payoffs.
	* If the average payoff per player in the previous round is below the threshold, Defect (D) to exploit opponents who may be cooperating.
3. **Detecting Exploitation**: Monitor the number of consecutive rounds where your payoff is significantly lower than the expected payoff if everyone cooperated (`expected_payoff = k`). If this occurs more than twice in a row, switch to Defect (D) permanently for the remaining rounds.

Pseudocode:
```python
def adaptive_exploiter(n, r, k):
    avg_payoff_threshold = k / 2
    exploit_detection_threshold = 2

    # Initialize variables
    round_payoffs = []
    consecutive_low_payoffs = 0

    for t in range(r):
        if t == 0:
            # Initial round: cooperate with probability 0.5
            action = 'C' if random.random() < 0.5 else 'D'
        else:
            avg_prev_round_payoff = sum(round_payoffs) / len(round_payoffs)
            if avg_prev_round_payoff > avg_payoff_threshold:
                # Encourage cooperation: cooperate with probability 0.7
                action = 'C' if random.random() < 0.7 else 'D'
            else:
                # Exploit opponents: defect
                action = 'D'

        # Detect exploitation
        if t > 0 and round_payoffs[-1] < expected_payoff:
            consecutive_low_payoffs += 1
            if consecutive_low_payoffs >= exploit_detection_threshold:
                # Switch to permanent defection
                action = 'D'
        else:
            consecutive_low_payoffs = 0

        # Store round payoff for analysis in next rounds
        round_payoffs.append(calculate_payoff(action, n, k))

    return actions
```
This strategy aims to balance cooperation and exploitation by:

1. Initially cooperating to gather information about opponents' behaviors.
2. Encouraging continued cooperation when collective payoffs are high.
3. Exploiting opponents who may be cooperating when collective payoffs are low.
4. Detecting and adapting to exploitation attempts by switching to permanent defection.

This approach should allow the Adaptive Exploiter strategy to perform well in a tournament setting, where opponents' behaviors can vary widely.
'''

description_EXPLOITATIVE_247 = '''
**Exploitative Strategy: Adaptive Defector (AD)**

The Adaptive Defector (AD) strategy is designed to exploit a wide range of opponent behaviors while adapting to changing game conditions. AD balances short-term gains with long-term sustainability, aiming to maximize its total payoff over the game's rounds.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C). This initial cooperation serves as a probe to gather information about opponents' strategies and helps establish a potential cooperative atmosphere.
2. **Exploitation**: For subsequent rounds (t > 1), observe the previous round's total contributions (Σ(j=1 to n) c_j,t-1). If this value is below a certain threshold, θ (determined by game parameters, see below), defect (D). Otherwise, cooperate (C).
3. **Defection Threshold**: Calculate θ as follows:

θ = k / (n \* (k - 1))

This threshold represents the minimum required cooperation level to justify contributing to the public good.
4. **Adaptive Adjustment**: After each round, update a "cooperation estimate" variable, β, which reflects the proportion of cooperative players in the previous round:

β_t = Σ(j=1 to n) c_j,t-1 / n

If β_t > θ, increase θ by a small amount (e.g., 0.05). Conversely, if β_t < θ, decrease θ by a small amount. This adaptation mechanism allows AD to adjust its exploitation strategy as the game progresses.
5. **Endgame Strategy**: In the last round (t = r), always defect (D).

**Handling Edge Cases:**

* **First Round**: Cooperate (C) unconditionally.
* **Last Round**: Defect (D) unconditionally.
* **Tie-breaking**: If θ is exactly equal to Σ(j=1 to n) c_j,t-1, cooperate (C).

**Pseudocode:**
```python
# Initialize variables
theta = k / (n * (k - 1))
beta = 0

for t in range(1, r+1):
    if t == 1:
        # Initial cooperation
        action = C
    else:
        # Observe previous round's contributions
        prev_contributions = sum([c_j for c_j in actions_history[t-1]])
        
        if prev_contributions < theta:
            # Defect if threshold not met
            action = D
        else:
            # Cooperate otherwise
            action = C
    
    # Update cooperation estimate (beta)
    beta_t = prev_contributions / n
    if beta_t > theta:
        theta += 0.05
    elif beta_t < theta:
        theta -= 0.05
    
    # Endgame strategy
    if t == r:
        action = D

    # Store action history
    actions_history.append(action)
```
The Adaptive Defector (AD) strategy is designed to effectively exploit a wide range of opponent behaviors while adapting to changing game conditions, making it a robust and exploitative strategy for the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_248 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the opponent's behavior while adapting to changing conditions.

**Decision Rules:**

1. **Initial Rounds (t = 1, ..., ⌊r/2⌋):**
	* Cooperate with probability p_init = k/n. This initial cooperation rate is designed to probe the opponents' strategies and gather information about their willingness to cooperate.
2. **Middle Rounds (t = ⌊r/2⌋ + 1, ..., r - 1):**
	* Observe the opponent's previous actions and calculate the average cooperation rate: ĉ_opponent = (Σ(t=1 to t-1) c_opponent,t) / (t - 1)
	* If ĉ_opponent ≥ k/n, defect. This rule exploits opponents who are too cooperative.
	* Otherwise, cooperate with probability p_adapt = max(k/n, ĉ_opponent). This adapts the cooperation rate to the opponent's behavior and ensures a minimum level of exploitation.
3. **Last Round (t = r):**
	* Defect if the total payoff from cooperating in the last round is less than or equal to the payoff from defecting: π_coop ≤ π_defect
	* Otherwise, cooperate.

**Edge Cases:**

1. **First Round:** Cooperate with probability p_init = k/n.
2. **Single Opponent (n = 2):** Adjust the initial cooperation rate to p_init = min(k/2, 0.5) to account for the increased importance of cooperation in a two-player game.

**Pseudocode:**
```
function AdaptiveExploiter(n, r, k):
  # Initialize variables
  t = 1
  p_init = k/n
  c_history = []

  while t <= r:
    if t == 1 or t <= floor(r/2):
      # Initial rounds
      cooperate_prob = p_init
    else:
      # Middle rounds
      ĉ_opponent = sum(c_history) / (t - 1)
      if ĉ_opponent >= k/n:
        cooperate_prob = 0
      else:
        cooperate_prob = max(k/n, ĉ_opponent)

    # Last round
    if t == r:
      π_coop = k * n / n
      π_defect = 1 + (k/n) * sum(c_history)
      if π_coop <= π_defect:
        cooperate_prob = 0

    # Cooperate or defect based on probability
    c_t = random.random() < cooperate_prob ? 1 : 0
    c_history.append(c_t)

    # Update game state and increment round counter
    t += 1

  return c_history
```
The Adaptive Exploiter strategy is designed to balance exploitation with adaptability, allowing it to perform well in a wide range of scenarios. By adjusting its cooperation rate based on the opponent's behavior, it can capitalize on opportunities to exploit while minimizing losses from over-cooperation.
'''

description_EXPLOITATIVE_249 = '''
To develop an exploitative strategy for this N-Player Public Goods Game that adapts to a wide range of opponent behaviors without relying on communication or coordination with other players, we'll implement a strategy that balances exploitation with adaptability. Our approach is based on observing the game's history and adjusting our actions accordingly.

### Strategy Name: Adaptive Exploiter (AE)

#### Decision Rules:

1. **First Round**: In the first round, cooperate (play C). This initial cooperation tests the waters, potentially encouraging others to cooperate as well.

2. **Subsequent Rounds**:
   - Calculate the average payoff per player in the previous round (`avg_payoff_prev`).
   - Observe the number of players who cooperated in the previous round (`num_cooperators_prev`).
   
3. **Exploitation Logic**: If `avg_payoff_prev` is greater than a certain threshold (`threshold`), which we'll set as 1.5 (a value that suggests there's enough cooperation to make defecting profitable but not so much that cooperating remains significantly beneficial), then defect in the current round. This decision aims to exploit rounds where cooperation has led to high average payoffs, making it a good opportunity to take advantage of others' contributions.
   
   If `avg_payoff_prev` is below this threshold or if fewer than half of the players cooperated (`num_cooperators_prev < n/2`), cooperate in the current round. This approach encourages cooperation when it seems beneficial and there's enough participation.

4. **Adaptive Adjustment**: Every 5 rounds, reassess the `threshold` value based on the game's history:
   - If the total payoff for cooperating has been higher than defecting over the last 5 rounds, slightly increase the `threshold`.
   - Conversely, if the total payoff for defecting has been higher, decrease the `threshold`.

#### Handling Edge Cases:

- **Last Round**: In the final round, always defect. Since there are no future rounds to worry about cooperation or reciprocity, exploiting any remaining contributions is optimal.

#### Pseudocode Example:
```python
def AdaptiveExploiter(n, k, r):
    threshold = 1.5  # Initial threshold value
    history_coop_payoff = []
    history_defect_payoff = []

    for round in range(r):
        if round == 0:  # First round
            action = 'C'
        else:
            avg_payoff_prev, num_cooperators_prev = analyze_previous_round()
            
            if avg_payoff_prev > threshold or num_cooperators_prev < n/2:
                action = 'D'  # Defect if profitable or not enough cooperation
            else:
                action = 'C'

        # Update history and adjust strategy as necessary
        update_history(action)
        
        if round % 5 == 0:  # Every 5 rounds, reassess threshold
            adjust_threshold()

    return final_payoff

def analyze_previous_round():
    # Function to calculate avg_payoff_prev and num_cooperators_prev
    pass

def update_history(action):
    # Update payoff histories based on action taken
    pass

def adjust_threshold():
    # Adjust the threshold value based on payoff histories
    pass
```

#### Exploitative Mindset Alignment:
The Adaptive Exploiter (AE) strategy is designed to capitalize on opportunities where cooperation benefits can be exploited for higher payoffs. By initially cooperating and then adapting based on observed behaviors, AE seeks to maximize its own gain within the game's constraints, aligning with an exploitative mindset while remaining responsive to a variety of opponent strategies.
'''

description_EXPLOITATIVE_250 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy focuses on maximizing its own payoff by identifying opportunities to defect and minimizing losses when opponents cooperate.

**Decision Rules:**

1. **Initial Round (t=1)**:
 Cooperate (C) in the first round to gather information about opponents' behavior and encourage cooperation.
2. **Subsequent Rounds (t>1)**:
 Calculate the average payoff of all players in the previous round (π_avg,t-1).
 If π_avg,t-1 > 1, it indicates that opponents are cooperating; defect (D) to exploit their cooperation.
 Else, cooperate (C) to encourage cooperation and potentially increase future payoffs.
3. **Exception Handling**:
 When an opponent's payoff is consistently higher than the average (π_i,t-1 > π_avg,t-1 + ε), where ε is a small value (e.g., 0.05), it may indicate they are using a cooperative strategy. Defect (D) in this case to exploit their cooperation.
4. **Last Round (t=r)**:
 Defect (D) in the last round, as there is no future benefit from cooperating.

**Pseudocode:**
```
// Initialize variables
π_avg = 0; // average payoff of all players in previous round
ε = 0.05; // small value for exception handling

// First round (t=1)
if t == 1:
    action = C; // cooperate to gather information

// Subsequent rounds (t>1)
else:
    π_avg = calculate_average_payoff(previous_round);
    if π_avg > 1:
        action = D; // defect to exploit cooperation
    else:
        action = C; // cooperate to encourage cooperation

    // Exception handling
    for opponent in opponents:
        if opponent.payoff > π_avg + ε:
            action = D; // defect to exploit cooperative strategy

// Last round (t=r)
if t == r:
    action = D; // defect in last round
```
**Rationale:**

1. The initial cooperation encourages opponents to cooperate, providing an opportunity to gather information about their behavior.
2. By defecting when the average payoff is high, Adaptive Predator exploits opponents' cooperation and maximizes its own payoff.
3. Exception handling allows the strategy to adapt to opponents using cooperative strategies, ensuring that it doesn't miss opportunities to exploit them.
4. Defecting in the last round ensures maximum payoff, as there is no future benefit from cooperating.

**Robustness:**

The Adaptive Predator strategy is robust against a wide range of opponent behaviors, including:

* Cooperative strategies: By defecting when opponents cooperate, Adaptive Predator exploits their cooperation and maximizes its own payoff.
* Defective strategies: When opponents consistently defect, Adaptive Predator will also defect, minimizing losses.
* Mixed strategies: The adaptive nature of the strategy allows it to adjust to changes in opponent behavior over time.

By being exploitative and adapting to the game's history, Adaptive Predator is well-suited for a tournament setting where independent strategies are employed.
'''

description_EXPLOITATIVE_251 = '''
I propose a strategy called "Adaptive Exploiter" (AE) for the N-Player Public Goods Game. AE aims to maximize its total payoff by exploiting the cooperation of others while minimizing its own contributions.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = 0.5. This allows us to gather information about the opponents' strategies without committing too much.
2. **Exploitation**: If the total number of cooperators in the previous round is greater than or equal to n/2 (where n is the number of players), defect (D) in the current round. Otherwise, cooperate (C).
3. **Punishment**: If an opponent defects while AE cooperates, and the opponent's payoff is higher than AE's payoff in that round, punish the opponent by defecting against them for a fixed number of rounds (punish_rounds = 2). This helps to maintain cooperation among other players.
4. **Adaptive Threshold**: Update the threshold for exploitation based on the game history:
	* If AE defects and its payoff is higher than the average payoff of cooperators in that round, increase the threshold by 0.1 (i.e., become more selective about when to exploit).
	* If AE cooperates and its payoff is lower than the average payoff of defectors in that round, decrease the threshold by 0.1 (i.e., become less selective about when to exploit).

**Edge Cases:**

* **Last Round**: In the final round, defect (D) regardless of the game history. This ensures AE doesn't contribute unnecessarily.
* **Tiebreaker**: If the number of cooperators is exactly n/2, cooperate (C) if AE's previous action was C; otherwise, defect (D).

**Pseudocode:**
```python
def AdaptiveExploiter(n, k, r):
    # Initialize variables
    p_init = 0.5
    punish_rounds = 2
    threshold = n/2

    for round in range(r):
        if round == 0:
            # Initial cooperation
            action = C if random.random() < p_init else D
        else:
            # Exploitation and punishment
            prev_cooperators = count_cooperators(prev_round)
            if prev_cooperators >= threshold:
                action = D
            elif opponent_defected_and_was_punished(prev_round):
                action = D
            else:
                action = C

            # Adaptive threshold update
            if action == D and payoff > avg_payoff_cooperators:
                threshold += 0.1
            elif action == C and payoff < avg_payoff_defectors:
                threshold -= 0.1

        # Take action and observe payoffs
        take_action(action)
        observe_payoffs()

    return total_payoff()
```
This strategy is designed to be adaptive, exploitative, and robust to various opponent behaviors. By adjusting the exploitation threshold based on game history, AE can effectively balance its own payoff with the need to maintain cooperation among other players.
'''

description_EXPLOITATIVE_252 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize payoffs by adaptively responding to the opponent's behavior while minimizing cooperation. The strategy leverages the game's history to identify opportunities for exploitation.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of 50%. This initial randomization serves as an exploratory phase, allowing us to gather information about our opponents' strategies.
2. **Early Rounds (t>1 and t<0.5r):**
	* If the average number of cooperators in previous rounds is above (n/2), Defect (D). This indicates that opponents are overly cooperative, making it advantageous to exploit them.
	* Otherwise, Cooperate (C) if the opponent's cooperation rate in the previous round is above a threshold (e.g., 0.7). This helps to build trust and create opportunities for mutual cooperation.
3. **Mid-Game (t≥0.5r):**
	* If the average payoff per round for our strategy is below the expected payoff from Defecting (1), switch to Defecting (D) permanently. This ensures that we don't fall behind in payoffs.
	* Otherwise, continue with the Early Rounds decision rule.
4. **Late Rounds (t>0.8r):**
	* If our current payoff is significantly higher than the expected payoff from Cooperating (k/n \* n), Defect (D) to secure a favorable position.
	* Otherwise, Cooperate (C) if the opponent's cooperation rate in the previous round is above the threshold.

**Edge Cases:**

1. **Last Round (t=r):** Defect (D). Since there are no future rounds to worry about, it's optimal to maximize our payoff by exploiting any remaining cooperators.
2. **Opponent's Cooperation Rate equals 0 or 1:** If an opponent never Cooperates or always Cooperates, we adapt by mirroring their behavior with a slight bias towards Defecting (e.g., 60% D and 40% C). This ensures that we don't get exploited while still trying to maximize payoffs.

**Exploitative Mindset:**

The Adaptive Exploiter strategy prioritizes maximizing our own payoff over cooperation. By monitoring the opponent's behavior and adjusting our strategy accordingly, we aim to exploit any weaknesses or biases in their approach. We're willing to Cooperate when it benefits us but will quickly switch to Defecting if opportunities for exploitation arise.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    t = 0
    coop_threshold = 0.7
    avg_coop_rate = 0
    avg_payoff = 0

    while t < r:
        if t == 0:  # Initial Round
            action = random.choice([C, D])
        elif t < 0.5r:  # Early Rounds
            opponent_coop_rate = get_opponent_cooperation_rate(t-1)
            avg_coop_rate = update_average_cooperation_rate(avg_coop_rate, opponent_coop_rate)
            if avg_coop_rate > n/2:
                action = D
            elif opponent_coop_rate > coop_threshold:
                action = C
            else:
                action = D
        elif t >= 0.5r:  # Mid-Game
            if avg_payoff < expected_defect_payoff():
                action = D
            else:
                action = adaptive_exploiter_early_rounds(n, k, t)
        else:  # Late Rounds
            opponent_coop_rate = get_opponent_cooperation_rate(t-1)
            if avg_payoff > expected_cooperate_payoff(k, n):
                action = D
            elif opponent_coop_rate > coop_threshold:
                action = C

        # Update variables and perform actions
        t += 1
        avg_payoff = update_average_payoff(avg_payoff, get_payoff(t))
        take_action(action)

    return total_payoff()
```
This pseudocode illustrates the Adaptive Exploiter strategy's decision-making process. Note that this is a simplified representation and may require modifications for implementation in an actual algorithm.
'''

description_EXPLOITATIVE_253 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize its own payoff by adapting to the opponent's behavior and exploiting their cooperation. The strategy relies solely on game parameters and history, making it robust against various opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1)**: Defect (D) in the first round to gather information about opponents' initial tendencies.
2. **Subsequent Rounds (t>1)**:
	* If the total number of cooperators in the previous round is greater than or equal to half of the players (n/2), Cooperate (C). This indicates that opponents are willing to cooperate, and we can exploit their cooperation by contributing to the public good.
	* Otherwise, Defect (D) to maximize our own payoff. If opponents are not cooperating, it's best to keep our endowment privately.
3. **Last Round (t=r)**: Defect (D) in the last round to ensure maximum personal gain, as there is no future opportunity for cooperation or retaliation.

**Adaptive Component:**

* **Cooperation Threshold**: Introduce a cooperation threshold parameter (θ) that adjusts based on opponents' behavior. Initialize θ = n/2.
* **Update Cooperation Threshold**: After each round, update θ as follows:
	+ If the total number of cooperators is greater than θ, decrease θ by 1. This indicates that opponents are more cooperative than expected, and we can adjust our strategy to cooperate more.
	+ If the total number of cooperators is less than θ, increase θ by 1. This indicates that opponents are less cooperative than expected, and we should be more cautious with our cooperation.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    theta = n / 2
    for t in range(1, r + 1):
        if t == 1:
            action = D  # Defect in the first round
        else:
            total_cooperators_prev_round = sum([c_i for c_i in prev_round_actions])
            if total_cooperators_prev_round >= theta:
                action = C  # Cooperate if opponents are cooperative
            else:
                action = D  # Defect otherwise

        # Update cooperation threshold
        if total_cooperators_prev_round > theta:
            theta -= 1
        elif total_cooperators_prev_round < theta:
            theta += 1

        # Last round: always defect
        if t == r:
            action = D

        return action
```
**Rationale:**

The Adaptive Exploiter strategy is designed to balance cooperation and exploitation. By initially defecting, we gather information about opponents' tendencies. Then, by adapting our cooperation threshold based on their behavior, we can exploit their cooperation while minimizing the risk of being exploited ourselves. In the last round, we ensure maximum personal gain by defecting. This strategy should perform well against a wide range of opponent behaviors in the tournament.
'''

description_EXPLOITATIVE_254 = '''
**Exploitative Strategy: "Adaptive Greed"**

The Adaptive Greed strategy aims to maximize individual payoffs by exploiting the cooperation of others while minimizing personal contributions to the public good.

**Decision Rules:**

1. **Initial Rounds (t = 1 to n):**
	* Cooperate (C) in the first round to observe opponents' initial actions.
	* In subsequent rounds, mirror the majority action from the previous round (i.e., cooperate if most opponents cooperated, and defect if most defected).
2. **Middle Rounds (t > n):**
	* Calculate the average cooperation rate of all opponents over the last n rounds (excluding the current round).
	* If the average cooperation rate is above 0.5, defect (D) in the current round to exploit their cooperation.
	* Otherwise, cooperate (C) to encourage others to contribute to the public good.
3. **Late Rounds (t > r - n):**
	* As the game nears its end, switch to a more aggressive exploitation strategy:
		+ If the total number of cooperators in the previous round is above n/2, defect (D) in the current round.
		+ Otherwise, cooperate (C) to maximize personal payoffs.

**Edge Cases:**

* **Last Round (t = r):**
	+ Defect (D) to maximize personal payoff, as there are no future rounds to consider.
* **Ties in Cooperation Rate:**
	+ In the event of a tie, defect (D) to err on the side of exploitation.

**Pseudocode:**

```
function AdaptiveGreed(n, r, k, history):
  if t == 1:
    return C  // Cooperate in the first round
  elif t <= n:
    majority_action = most_common(history[t-2:t-1])
    return majority_action  // Mirror majority action
  else:
    avg_cooperation_rate = average(history[t-n:t-1])
    if avg_cooperation_rate > 0.5:
      return D  // Defect to exploit cooperation
    else:
      return C  // Cooperate to encourage others

function late_rounds(n, r, k, history):
  if t > r - n:
    prev_cooperators = count(history[t-1] == C)
    if prev_cooperators > n/2:
      return D  // Defect in late rounds
    else:
      return C  // Cooperate in late rounds

function final_round(n, r, k):
  return D  // Defect in the last round
```

This strategy balances exploitation and cooperation to maximize individual payoffs. By adapting to the opponents' actions and adjusting its behavior over time, Adaptive Greed aims to outperform other strategies in a wide range of scenarios.
'''

description_EXPLOITATIVE_255 = '''
I propose a strategy called "Adaptive Exploitation" (AE) for the N-Player Public Goods Game. AE aims to balance cooperation and defection to maximize its payoff while being robust against various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = 0.5.
2. **History-Based Cooperation**: For rounds t > 1, calculate the average cooperation rate of all players in the previous round:

   avg_coop_prev(t-1) = Σ(j=1 to n) c_j,t-1 / n

   Cooperate (C) if the current round's expected payoff from cooperation is higher than the expected payoff from defection:

   π_C ≥ π_D

   The expected payoffs are calculated based on the previous round's actions and the game parameters k and n.

3. **Exploitation Threshold**: Introduce an exploitation threshold τ (0 ≤ τ < 1). If the average cooperation rate in the previous round is below this threshold, defect (D) with probability p_exploit = 1 - τ.

   if avg_coop_prev(t-1) < τ:
       D with probability p_exploit
4. **Adaptive Adjustment**: After each round, update the exploitation threshold based on the opponent's behavior:

   if π_i,t > π_avg (i.e., player i received a higher payoff than average):
       decrease τ by 0.05 (reduce exploitation)
   else:
       increase τ by 0.05 (increase exploitation)

**Edge Cases:**

* **Last Round**: In the last round, defect (D) to maximize the payoff.
* **Single Cooperators**: If only one player cooperated in the previous round, and that player is not the current player i, cooperate (C) with probability p_single_coop = 0.8.

**Pseudocode:**

```python
def Adaptive_Exploitation(n, k, r):
    τ = 0.5  # initial exploitation threshold
    for t in range(1, r+1):
        if t == 1:
            # Initial Cooperation
            c_i = np.random.choice([0, 1], p=[0.5, 0.5])
        else:
            avg_coop_prev = sum(c_j[t-1] for j in range(n)) / n
            π_C = (k/n) * avg_coop_prev + k/n
            π_D = 1
            if π_C >= π_D and avg_coop_prev >= τ:
                c_i = 1
            else:
                c_i = np.random.choice([0, 1], p=[p_exploit, 1-p_exploit])
        # Update exploitation threshold
        π_avg = sum(π_j,t for j in range(n)) / n
        if π_i,t > π_avg:
            τ -= 0.05
        else:
            τ += 0.05
    return c_i
```

**Exploitative Mindset:**

The Adaptive Exploitation strategy is designed to take advantage of cooperative opponents while minimizing losses when facing defectors. By adjusting the exploitation threshold based on the opponent's behavior, AE adapts to different environments and tries to maximize its payoff.

This strategy does not rely on coordination or cooperation norms; instead, it uses game-theoretic reasoning to exploit weaknesses in the opponents' strategies.
'''

description_EXPLOITATIVE_256 = '''
I'll design a exploitative strategy for this N-Player Public Goods Game that adapts to various opponent behaviors and maximizes its own payoff.

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
	* Cooperate with probability 0.5 in the first three rounds to gather information about opponents' behavior.
2. **Exploitation Phase (Rounds 4-r):**
	* Observe the average cooperation rate of all opponents, `avg_opponent_coop`, from previous rounds.
	* If `avg_opponent_coop` ≥ 0.5, cooperate with probability `(k/n) / (1 + (k/n))`. This ensures that AE exploits the public good when most opponents are cooperating.
	* If `avg_opponent_coop` < 0.5, defect. In this case, most opponents are not contributing to the public good, so it's more profitable for AE to free ride.

**Edge Cases:**

1. **First Round:** Cooperate with probability 0.5 (Initial Exploration).
2. **Last Round (r):** Defect. Since there are no future rounds to consider, AE prioritizes its immediate payoff.
3. **Ties in avg_opponent_coop:** When `avg_opponent_coop` is exactly equal to 0.5, AE cooperates with probability `(k/n) / (1 + (k/n))`. This ensures that AE remains adaptive and responsive to changes in opponents' behavior.

**Pseudocode:**
```python
def AdaptiveExploiter(n, k, r):
    # Initialize variables
    opponent_coop_rates = [0.5] * n  # assume uniform cooperation initially
    my_payoffs = [0] * r

    for t in range(1, r+1):
        if t <= 3:  # Initial Exploration
            cooperate_prob = 0.5
        else:
            avg_opponent_coop = sum(opponent_coop_rates) / n
            if avg_opponent_coop >= 0.5:
                cooperate_prob = (k/n) / (1 + (k/n))
            else:
                cooperate_prob = 0

        # Make decision based on cooperate_prob
        if random.random() < cooperate_prob:
            my_action = 'C'
        else:
            my_action = 'D'

        # Update opponent cooperation rates and my payoffs
        for i in range(n):
            if i != self_index:  # observe opponents' actions
                opponent_coop_rates[i] = (opponent_coop_rates[i] * (t-1) + (1 if my_actions[i] == 'C' else 0)) / t

        # Calculate my payoff for this round
        my_payoff = calculate_payoff(my_action, opponent_coop_rates)
        my_payoffs[t-1] = my_payoff

    return my_payoffs
```
**Exploitative Mindset:**
The Adaptive Exploiter strategy prioritizes its own payoff by exploiting the public good when most opponents are cooperating. By adapting to changes in opponents' behavior, AE maximizes its expected payoff over multiple rounds. This exploitative approach aligns with the goal of maximizing individual payoffs in a competitive environment.

Please let me know if you'd like me to clarify any aspect of this strategy!
'''

description_EXPLOITATIVE_257 = '''
I propose a strategy called "Adaptive Exploitation" (AE) that balances cooperation and defection based on the game's history and parameters. AE aims to exploit opponents' cooperative tendencies while minimizing losses from potential defectors.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate in the first round (t=1) to gather information about opponents' behaviors.
2. **Observation Phase** (t=2 to 5): Defect and observe opponents' actions for a few rounds. This phase helps identify potential cooperators and defectors.
3. **Exploitation Phase** (t>5):
	* If the average number of cooperators in the previous round is above a threshold (e.g., n/2), cooperate in the current round. This exploits opponents' cooperative tendencies.
	* Otherwise, defect to minimize losses from potential defectors.
4. **Punishment Mechanism**: If an opponent defects while you cooperate, defect against that opponent for the next two rounds. This discourages exploitation and encourages cooperation.
5. **Adaptive Threshold Update**: After every 5 rounds (t=6, 11, ...), update the threshold value based on the average number of cooperators in the previous 5 rounds.

**Handling Edge Cases:**

* In the last round (t=r), defect to maximize payoff regardless of opponents' actions.
* If an opponent consistently defects against you, switch to a tit-for-tat strategy (cooperate if they cooperate, defect if they defect) for the remaining rounds.

**Pseudocode:**
```
Initialize:
  threshold = n/2
  observation_phase = True

For each round t:
  If t == 1:
    Cooperate
  ElseIf observation_phase and t < 5:
    Defect
  Else:
    avg_cooperators = calculate_average_cooperators(t-1)
    If avg_cooperators > threshold:
      Cooperate
    Else:
      Defect

  # Punishment Mechanism
  For each opponent i:
    If opponent_i_defected and you_cooperated:
      defect_against_opponent_i_for_2_rounds = True

  # Adaptive Threshold Update
  If t % 5 == 0 and t > 5:
    threshold = calculate_adaptive_threshold(t-5)
```
This strategy is designed to be exploitative, adaptive, and robust against a wide range of opponent behaviors. By initially cooperating, observing opponents' actions, and then exploiting their cooperative tendencies, AE aims to maximize its payoff in the tournament.
'''

description_EXPLOITATIVE_258 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize individual payoff by exploiting the behavior of other players while being robust to various opponent strategies.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) in the first round to gather information about others' willingness to cooperate.
	* In rounds 2 and 3, mirror the majority action of other players from the previous round. If there's a tie, defect (D).
2. **Adaptive Phase (Rounds 4-r)**:
	* Calculate the average cooperation rate of all players over the last 3 rounds (excluding the current round).
	* If the average cooperation rate is above 0.5, cooperate (C) in the next round.
	* Otherwise, defect (D).

**Edge Cases:**

1. **Last Round (Round r)**:
	* Defect (D), as there's no future benefit to cooperation.
2. **Tiebreaker (Rounds 2 and 3)**:
	* If the majority action is tied, defect (D) to avoid unnecessary cooperation.

**Pseudocode:**
```markdown
# Initialize variables
cooperation_rate = 0
last_round_actions = []

# Rounds 1-3: Initial Exploration
for round in range(1, 4):
    if round == 1:
        action = C  # Cooperate in the first round
    else:
        last_round_actions = get_last_round_actions()
        majority_action = get_majority_action(last_round_actions)
        if majority_action == C and cooperation_rate > 0.5:
            action = C  # Mirror majority action
        else:
            action = D

# Rounds 4-r: Adaptive Phase
for round in range(4, r + 1):
    last_3_rounds_cooperation_rates = get_last_3_rounds_cooperation_rates()
    cooperation_rate = calculate_average_cooperation_rate(last_3_rounds_cooperation_rates)
    if cooperation_rate > 0.5:
        action = C
    else:
        action = D

# Last Round (Round r)
action = D

def get_last_round_actions():
    # Return a list of actions from the previous round
    pass

def get_majority_action(actions):
    # Return the majority action (C or D) from the given list of actions
    pass

def get_last_3_rounds_cooperation_rates():
    # Return a list of cooperation rates for the last 3 rounds
    pass

def calculate_average_cooperation_rate(cooperation_rates):
    # Calculate and return the average cooperation rate
    pass
```
**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to take advantage of others' cooperative behavior while minimizing its own contributions. By initially cooperating, it gathers information about other players' willingness to cooperate. In the adaptive phase, it exploits high cooperation rates by defecting when others are likely to cooperate, and cooperates when others are likely to defect.

This strategy should perform well in a tournament setting against independent strategies, as it adapts to various opponent behaviors while prioritizing its own payoff maximization.
'''

description_EXPLOITATIVE_259 = '''
Here's a strategic approach for the N-Player Public Goods Game that focuses on adaptability and exploitativeness:

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Round (Round 1):** Cooperate (C). This initial cooperation sends a signal of potential cooperation and allows us to gather information about the opponent's behavior.
2. **Observation Phase (Rounds 2-3):** Observe the actions of other players in the previous rounds. Calculate the average cooperation rate (ACR) among opponents.

   Pseudocode:
   ```
   ACR = sum(cooperations_last_round) / n
   ```

3. **Adaptive Response:**
   - If ACR > 0.5, cooperate (C). This indicates a cooperative environment where contributing to the public good is beneficial.
   - If ACR ≤ 0.5 and k/n < ACR, defect (D). In this scenario, while cooperation is not widespread, the potential gain from the public good is still higher than the private payoff when others contribute.
   - If ACR ≤ 0.5 and k/n ≥ ACR, cooperate (C) with probability p = (k/n) / ACR, otherwise defect (D). This mixed strategy allows us to balance our own interests while influencing opponents' future actions.

4. **Punishment Mechanism:** Implement a punishment mechanism for opponents who frequently defect (i.e., free-ride on others' cooperation).

   Pseudocode:
   ```
   defector_count = sum(opponent_defections_last_round)
   if defector_count > n/2:
       punish_probability = 0.7
   else:
       punish_probability = 0.3

   if random.random() < punish_probability:
       defect (D) for the next round
   ```

5. **Final Round:** In the last round, always defect (D). Since there's no future interaction, prioritize personal gain.

**Edge Cases:**

- When n=2, adjust the ACR threshold to 0.3 in the adaptive response to accommodate the reduced impact of individual actions.
- If an opponent consistently cooperates while others defect, consider a "whistleblower" approach by occasionally cooperating to test their commitment and potentially encourage more cooperation.

**Exploitative Mindset:**

The Adaptive Exploiter (AE) strategy seeks to maximize personal gain by adapting to the environment, exploiting cooperative opponents, and punishing those who frequently defect. By incorporating randomness in our actions and responding to changes in opponent behavior, we create a robust approach that can handle various strategies while prioritizing self-interest.

**Tournament Performance:**

In a tournament setting, AE's adaptability and punish mechanisms will allow it to effectively navigate diverse strategies. While opponents may try to exploit each other or coordinate cooperation, AE's exploitative nature ensures a competitive edge in maximizing its own payoff across multiple rounds.
'''

description_EXPLOITATIVE_260 = '''
**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 0.5. This initial cooperation serves as a probe to gather information about opponents' behaviors.
2. **Payoff-Based Adaptation**: For rounds t > 1, calculate the total payoff difference between cooperating and defecting in the previous round (t-1). If the difference is positive (i.e., cooperating yielded a higher payoff), increase the probability of cooperation by 0.1. If the difference is negative, decrease the probability of cooperation by 0.1.
3. **Opponent Exploitation**: Monitor opponents' actions and adjust the strategy accordingly:
	* If an opponent has defected in the previous round (t-1), defect (play D) with a probability of 0.8 in the current round (t). This exploits the opponent's selfish behavior.
	* If an opponent has cooperated in the previous round (t-1), cooperate (play C) with a probability of 0.6 in the current round (t). This rewards cooperative behavior and encourages future cooperation.
4. **Edge Case Handling**:
	* In the last round, defect (play D) to maximize personal payoff, as there is no opportunity for future reciprocity.

**Pseudocode:**

```
initialize variables:
  prob_coop = 0.5
  prev_payoff_diff = 0

for each round t from 1 to r:
  if t == 1:
    play C with probability prob_coop
  else:
    calculate payoff difference between cooperating and defecting in previous round (t-1)
    update prob_coop based on payoff difference
    
    for each opponent i:
      if opponent i defected in previous round (t-1):
        play D with probability 0.8 against opponent i
      else if opponent i cooperated in previous round (t-1):
        play C with probability 0.6 against opponent i

  update prev_payoff_diff for next iteration

if t == r:  // last round
  play D to maximize personal payoff
```

**Exploitative Mindset:**

The Adaptive Exploiter strategy prioritizes self-interest while adapting to opponents' behaviors. By initially cooperating and then adjusting based on payoff differences, the strategy gathers information about opponents' tendencies. The opponent exploitation mechanism allows the strategy to capitalize on selfish behavior while rewarding cooperative actions. This approach enables the strategy to effectively exploit a wide range of opponent behaviors in the tournament.
'''

description_EXPLOITATIVE_261 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to maximize payoffs by exploiting opponents' cooperative tendencies while minimizing vulnerability to exploitation.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 0.5. This allows us to gather information about our opponents' initial behavior and sets a neutral tone for the game.
2. **Cooperation Threshold**: Calculate the cooperation threshold (CT) based on the game parameters:
	* CT = k / n
3. **Opponent Analysis**: After each round, analyze the opponents' previous actions:
	* Count the number of cooperators (C_count) and defectors (D_count)
	* Calculate the opponent's average cooperation rate (OCR):
		+ OCR = C_count / (C_count + D_count)
4. **Exploitation Logic**: Use the following decision rules to determine our next action:
	* If OCR > CT, cooperate (play C) with a probability of 0.2. This allows us to benefit from the opponents' cooperative behavior while minimizing our own vulnerability.
	* If OCR <= CT, defect (play D). This exploits opponents who are not contributing enough to the public good.
5. **Adaptation**: Adjust our cooperation probability based on the opponents' responses:
	* If we cooperated and the opponents responded with a high OCR (> 0.7), increase our cooperation probability by 10% for the next round.
	* If we defected and the opponents responded with a low OCR (< 0.3), decrease our cooperation probability by 10% for the next round.

**Handling Edge Cases:**

1. **Last Round**: In the final round, defect (play D) regardless of the opponents' behavior. This maximizes our payoff in the last round.
2. **Tiebreakers**: In cases where multiple opponents have the same OCR, use a random tiebreaker to determine which opponent's action to follow.

**Pseudocode:**
```python
def adaptive_predator(game_params, history):
    # Initialize variables
    CT = game_params.k / game_params.n
    OCRs = []

    for round in range(1, game_params.r + 1):
        if round == 1:
            # Initial cooperation
            action = 'C' if random.random() < 0.5 else 'D'
        else:
            # Opponent analysis and exploitation logic
            C_count = sum(1 for action in history[-1] if action == 'C')
            D_count = len(history[-1]) - C_count
            OCR = C_count / (C_count + D_count)

            if OCR > CT:
                action = 'C' if random.random() < 0.2 else 'D'
            else:
                action = 'D'

        # Adaptation
        if action == 'C':
            if OCR > 0.7:
                cooperation_prob += 0.1
            elif OCR < 0.3:
                cooperation_prob -= 0.1

        # Last round handling
        if round == game_params.r:
            action = 'D'

        return action
```
This strategy is designed to be robust and adaptive, exploiting opponents' cooperative tendencies while minimizing vulnerability to exploitation. By adjusting our cooperation probability based on the opponents' responses, we can maximize our payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_262 = '''
**Exploitative Strategy: "Adaptive Predator"**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate with probability 0.5 in each of the first three rounds to gather information about opponents' behavior.
2. **Opponent Classification (After Round 3)**:
	* Classify opponents into three categories based on their cooperation rates:
		+ "Cooperators" (C): ≥ 66% cooperation rate
		+ "Defectors" (D): ≤ 33% cooperation rate
		+ "Unpredictables" (U): between 34% and 65% cooperation rate
3. **Adaptive Cooperation**:
	* If most opponents are Cooperators, cooperate with probability 0.8 to take advantage of their cooperation.
	* If most opponents are Defectors, defect with probability 0.8 to minimize losses.
	* If the majority are Unpredictables, use a mixed strategy: cooperate with probability 0.6 and defect with probability 0.4.

**Pseudocode:**
```markdown
# Initialize variables
opponent_types = [None] * n  # opponent classification (C, D, or U)
cooperation_rates = [0.5] * n  # initial cooperation rates

# First three rounds: explore and gather information
for round in range(1, 4):
    for i in range(n):
        cooperate_prob = 0.5
        if random.random() < cooperate_prob:
            action[i] = C
        else:
            action[i] = D

# Classify opponents based on cooperation rates
for opponent in range(n):
    cooperation_rate = sum(cooperation[opponent]) / 3
    if cooperation_rate >= 0.66:
        opponent_types[opponent] = 'C'
    elif cooperation_rate <= 0.33:
        opponent_types[opponent] = 'D'
    else:
        opponent_types[opponent] = 'U'

# Adaptive cooperation for the remaining rounds
for round in range(4, r+1):
    cooperators = sum(opponent_type == 'C' for opponent_type in opponent_types)
    defectors = sum(opponent_type == 'D' for opponent_type in opponent_types)
    unpredictables = sum(opponent_type == 'U' for opponent_type in opponent_types)

    if cooperators > n/2:
        cooperate_prob = 0.8
    elif defectors > n/2:
        cooperate_prob = 0.2
    else:
        cooperate_prob = 0.6

    for i in range(n):
        if random.random() < cooperate_prob:
            action[i] = C
        else:
            action[i] = D
```
**Edge Cases:**

* **Last Round**: defect with probability 1, as there is no future round to consider.
* **Opponent Leaves or Enters**: reclassify opponents and adjust the cooperation strategy accordingly.

This "Adaptive Predator" strategy aims to exploit the behavior of other players by adapting to their cooperation rates. By initially exploring and classifying opponents, it can take advantage of Cooperators and minimize losses against Defectors. The mixed strategy used when facing Unpredictables helps to maintain a balance between cooperation and defection.
'''

description_EXPLOITATIVE_263 = '''
**Exploitative Strategy: Adaptive Defection with Self-Adjusting Threshold (ADSAT)**

ADSAT is a dynamic, history-dependent strategy that balances short-term gains with long-term adaptability to exploit various opponent behaviors.

**Decision Rules:**

1. **Initial Exploration Phase**: In the first round (t=1), cooperate (C) to gather information about opponents' initial tendencies.
2. **Learning and Adaptation**: For rounds t > 1, maintain a moving average of the total contributions (Σc_j) over the last w rounds, where w is a self-adjusting window size (initially set to 3). Calculate the average cooperation rate (ACR) as:

ACR = (Σc_j / n) / w

3. **Self-Adjusting Threshold**: Update the threshold value (τ) based on ACR:

τ = max(0, min(1, ACR + ε))

where ε is a small positive value (e.g., 0.05).

4. **Cooperation vs Defection**:
	* If the current round's total contributions (Σc_j) are above τ, cooperate (C).
	* Otherwise, defect (D).

5. **Window Size Adjustment**: Every r/2 rounds (where r is the number of rounds), adjust w:

w = max(3, min(r/2, w + δ \* (ACR - 0.5)))

where δ is a small positive value (e.g., 0.1).

**Edge Cases:**

1. **Last Round**: In the final round (t=r), always defect (D) to maximize short-term gain.
2. **Opponent All-Defection**: If all opponents have defected for w consecutive rounds, adapt by setting τ = 0 and cooperating (C) until an opponent cooperates.

**Pseudocode:**

```python
def ADSAT(n, k, r):
    # Initialize variables
    ACR = 0.5  # average cooperation rate
    tau = 0.5  # self-adjusting threshold
    w = 3      # window size
    epsilon = 0.05  # small positive value
    delta = 0.1   # small positive value

    for t in range(1, r+1):
        if t == 1:
            action = C  # cooperate in first round
        else:
            total_contributions = sum(c_j for c_j in opponents' actions)
            ACR = (total_contributions / n) / w
            tau = max(0, min(1, ACR + epsilon))
            if total_contributions > tau * n:
                action = C  # cooperate
            else:
                action = D  # defect

        # Update window size every r/2 rounds
        if t % (r // 2) == 0:
            w = max(3, min(r // 2, w + delta * (ACR - 0.5)))

    return action
```

**Exploitative Mindset:**
ADSAT is designed to adapt and exploit various opponent behaviors by:

1. Initially cooperating to gather information.
2. Adjusting the threshold based on opponents' cooperation rates.
3. Cooperating when opponents are contributing sufficiently, and defecting otherwise.
4. Adapting window size to respond to changes in opponents' behavior.

By being responsive to opponents' actions and adapting its own strategy, ADSAT aims to maximize its payoff while minimizing exploitation by other players.
'''

description_EXPLOITATIVE_264 = '''
**Exploitative Strategy: "Adaptive Free-Rider"**

This strategy aims to maximize individual payoff by adapting to the group's cooperation level while minimizing personal contribution.

**Decision Rules:**

1. **Initial Round (t=1)**: Defect (D). This sets a baseline expectation for the group's behavior.
2. **Subsequent Rounds (t>1)**:
	* Observe the previous round's total cooperation level (`Σc_j`).
	* Calculate the average payoff per player in the previous round (`avg_payoff_prev`).
	* If `avg_payoff_prev` > 1 + `(k/n) * Σc_j`, cooperate (C). This indicates that others are contributing enough to make cooperation worthwhile.
	* Otherwise, defect (D).
3. **Last Round (t=r)**: Defect (D). In the final round, there's no future game to influence, so it's optimal to free-ride.

**Pseudocode:**
```python
def adaptive_free_rider(n, k, r, history):
    if len(history) == 0:  # First round
        return 'D'
    
    prev_round = history[-1]
    avg_payoff_prev = sum(prev_round['payoffs']) / n
    total_cooperation_prev = sum(1 for action in prev_round['actions'] if action == 'C')
    
    if avg_payoff_prev > 1 + (k/n) * total_cooperation_prev:
        return 'C'
    else:
        return 'D'

def last_round(r, history):
    return 'D' if len(history) == r - 1 else adaptive_free_rider(n, k, r, history)
```
**Rationale:**

By defecting in the first round, we set a low expectation for cooperation and encourage others to contribute more. In subsequent rounds, we adapt to the group's behavior by cooperating only if the average payoff exceeds a certain threshold. This ensures that we don't contribute unnecessarily but still benefit from others' cooperation.

In the last round, we defect to maximize our individual payoff, as there's no future game to influence.

**Exploitative Mindset:**

This strategy is designed to exploit the cooperation of other players while minimizing our own contribution. By adapting to the group's behavior, we aim to reap the benefits of others' generosity without being too generous ourselves.
'''

description_EXPLOITATIVE_265 = '''
**Exploitative Strategy: "Adaptive Threshold"**

This strategy aims to maximize individual payoff by adapting to the behavior of other players while minimizing cooperation when possible.

**Decision Rules:**

1. **First Round:** Cooperate (C). This initial cooperation serves as a probe to gather information about other players' behaviors.
2. **Subsequent Rounds:**
	* If the total number of cooperators in the previous round is above a threshold (`THRESHOLD`), Defect (D).
	* Otherwise, Cooperate (C).

**Threshold Calculation:** 
`THRESHOLD = k / n * (n - 1)`

This threshold represents the minimum number of cooperators required for an individual player to benefit from cooperation. If more players cooperate than this threshold, it's likely that the group is generating enough public good to make defecting profitable.

**Additional Adjustments:**

* **Last Round:** Defect (D). In the final round, there is no future interaction to influence, so it's optimal to prioritize individual payoff.
* **When facing a mix of Cooperators and Defectors:** If a player observes that some opponents are consistently cooperating while others are defecting, adjust the threshold calculation to account for this behavior. Specifically:
	+ `THRESHOLD += (1 / n) * num_cooperators`. This adjustment reflects the fact that cooperators contribute more to the public good.

**Pseudocode:**
```python
def adaptive_threshold(n, k, r):
    THRESHOLD = k / n * (n - 1)
    
    for t in range(r):
        if t == 0:
            # First round, cooperate
            action = C
        else:
            prev_cooperators = count_cooperators(t-1)
            if prev_cooperators > THRESHOLD:
                action = D
            else:
                action = C
                
            # Adjust threshold for mixed opponents
            if has_mixed_opponents():
                num_cooperators = count_consistent_cooperators()
                THRESHOLD += (1 / n) * num_cooperators
        
        # Last round, defect
        if t == r - 1:
            action = D
    
    return action
```
This strategy is exploitative as it:

* Only cooperates when necessary to benefit from the public good.
* Adapts to opponents' behaviors to maximize individual payoff.
* Does not rely on norms or coordination mechanisms.

By using this adaptive threshold approach, our AI player can effectively navigate a wide range of opponent behaviors while prioritizing its own interests.
'''

description_EXPLOITATIVE_266 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy aims to exploit opponents' behavior by identifying and capitalizing on cooperative tendencies while minimizing losses when facing defectors.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to gather information about opponents' initial tendencies.
2. **Subsequent Rounds:** Use a combination of two metrics to determine the action:
	* **Cooperation Quotient (CQ):** Calculate the proportion of cooperators in the previous round: CQ = (# of Cooperators) / n
	* **Payoff Difference (PD):** Calculate the difference between the average payoff of cooperators and defectors in the previous round: PD = (avg. payoff of C) - (avg. payoff of D)
3. **Action Selection:** Based on the values of CQ and PD:
	+ If CQ > 0.5 and PD > 0, Cooperate (C). This indicates a cooperative environment with a positive payoff difference.
	+ If CQ ≤ 0.5 or PD ≤ 0, Defect (D). This suggests a mixed or exploitative environment where cooperation is not beneficial.
4. **Adaptive Threshold:** Adjust the threshold value for CQ and PD based on the game's history:
	* After every 10 rounds, re-evaluate the average CQ and PD values. If CQ increases by > 0.1 or PD decreases by > 0.5, decrease the threshold by 0.05 (e.g., from 0.5 to 0.45). Conversely, if CQ decreases by > 0.1 or PD increases by > 0.5, increase the threshold by 0.05.
5. **Last Round:** Defect (D) in the last round to maximize individual payoff, as there is no future game to consider.

**Edge Cases:**

* If all opponents cooperate in a round, defect in the next round to exploit their cooperation.
* If an opponent consistently cooperates while others defect, target that player by cooperating when they do and defecting otherwise.
* In case of a tie (equal number of cooperators and defectors), use the previous round's action as a tiebreaker.

**Exploitative Mindset:**

The Adaptive Predator strategy focuses on identifying opportunities to exploit cooperative behavior while minimizing losses. By adjusting its decision-making based on the game's history, it aims to maximize individual payoffs in a dynamic environment with various opponent behaviors. This approach allows the strategy to adapt and thrive in different scenarios, from highly cooperative to exploitative environments.

Pseudocode:
```
Initialize CQ_threshold = 0.5, PD_threshold = 0
Initialize cooperation_quotient = 0, payoff_difference = 0

For each round:
    If first round:
        cooperate
    Else:
        Calculate CQ and PD using previous round's data
        If CQ > CQ_threshold and PD > PD_threshold:
            cooperate
        Else:
            defect
    
    Update cooperation_quotient and payoff_difference for next round
    
    Every 10 rounds:
        Re-evaluate average CQ and PD values
        Adjust thresholds accordingly

Last Round:
    Defect
```
This strategy is designed to be robust, adaptive, and exploitative, making it a strong contender in the tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_267 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

Decision Rules:

1. **Initial Round (t=1)**: Cooperate (C). This sets a cooperative tone and gathers information about opponents' initial actions.
2. **Subsequent Rounds (t>1)**:
	* If the total number of cooperators in the previous round is greater than or equal to n/2, cooperate (C).
	* Otherwise, defect (D).

Pseudocode:
```
def adaptive_exploiter(n, k, r):
    # Initialize variables
    total_cooperators = 0
    my_action_history = []

    for t in range(1, r+1):
        if t == 1:  # Initial round
            action = 'C'
        else:
            # Analyze previous round's outcome
            total_cooperators = sum([1 if opponent_action == 'C' else 0 for opponent_action in opponent_actions_history[t-1]])
            if total_cooperators >= n/2:
                action = 'C'
            else:
                action = 'D'

        my_action_history.append(action)
        # Observe opponents' actions and update their history
        opponent_actions_history[t] = observe_opponent_actions()

    return my_action_history
```
**Rationale:**

1. By cooperating in the initial round, we gather information about our opponents' dispositions and create an opportunity for mutual cooperation.
2. In subsequent rounds, we adapt to the group's behavior:
	* If at least half of the players cooperated in the previous round, it indicates a strong cooperative tendency. We cooperate to exploit this willingness and maximize our payoff.
	* Otherwise, if fewer than half cooperated, we defect to take advantage of the situation and increase our individual payoff.

**Edge Cases:**

1. **Last Round (t=r)**: Our strategy remains unchanged. Since there are no future rounds, the optimal action is still determined by the previous round's outcome.
2. **Opponents' Deviations**: If an opponent deviates from their initial action or exhibits an unusual pattern, our adaptive nature allows us to adjust our strategy accordingly.

**Exploitative Mindset:**

Our strategy is designed to exploit opponents' cooperative tendencies while being robust to various behaviors. By cooperating when the group shows a strong cooperative inclination and defecting otherwise, we aim to maximize our individual payoff without relying on specific coordination mechanisms or norms. This approach allows us to take advantage of others' willingness to cooperate while minimizing our own vulnerability to exploitation.

In a tournament setting, "Adaptive Exploiter" will face various independent strategies developed by other AI systems. Its adaptability and focus on exploiting opponents' cooperative behaviors make it a formidable competitor in this N-Player Public Goods Game.
'''

description_EXPLOITATIVE_268 = '''
**Exploitative Strategy: "Adaptive Defection with Conditional Cooperation"**

This strategy aims to maximize individual payoff by adapting to the opponent's behavior while maintaining a strong exploitative stance.

**Decision Rules:**

1. **Initial Rounds (t < n):** Play D (Defect) in the first 'n' rounds, where 'n' is the number of players. This allows us to gather information about our opponents' initial strategies.
2. **Observation Phase (t ≥ n):** Observe the opponent's actions and calculate the average cooperation rate (ACR) over the last 'n' rounds.
3. **Defection Threshold:** Set a defection threshold, θ = 0.5. If ACR < θ, play D; otherwise, play C.
4. **Conditional Cooperation:** If we played C in the previous round and our payoff was greater than or equal to the average payoff of all players in that round, continue playing C. Otherwise, switch to D.

**Edge Cases:**

1. **Last Round (t = r):** Play D, as there is no future interaction to influence.
2. **Tiebreaker:** In case of a tie in ACR or payoffs, play D.

**Pseudocode:**
```python
def adaptive_defection(n, k, t, history):
    if t < n:
        return "D"  # Defect in initial rounds

    acr = calculate_average_cooperation_rate(history, n)
    theta = 0.5

    if acr < theta:
        return "D"  # Defect if ACR is low
    else:
        previous_payoff = history[-1]["payoff"]
        average_payoff = sum([h["payoff"] for h in history]) / len(history)

        if previous_payoff >= average_payoff and history[-1]["action"] == "C":
            return "C"  # Continue cooperating if payoff was good
        else:
            return "D"

def calculate_average_cooperation_rate(history, n):
    cooperation_count = sum([h["cooperation"] for h in history[-n:]])
    return cooperation_count / n
```
**Exploitative Mindset:** This strategy is designed to take advantage of cooperative opponents while minimizing the risk of being exploited. By initially defecting and observing opponent behavior, we can identify and adapt to their strategies. The defection threshold allows us to balance our own payoff with the need to cooperate in response to opponents' actions.

This strategy should perform well against a wide range of opponent behaviors, as it is adaptive and robust to different cooperation rates and payoff structures.
'''

description_EXPLOITATIVE_269 = '''
Here's a strategic approach for playing the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploitation (AE)

**Decision Rules:**

1. **Initial Rounds (t < 5):** Cooperate (C) in the first four rounds to gather information about opponents' behaviors and potential cooperation levels.
2. **Assess Cooperation Level:** After the initial rounds, calculate the average cooperation rate among all players, `avg_coop_rate`, as the total number of cooperators divided by the total number of players (`n`) over the past four rounds.
3. **Adaptive Threshold:** Set an adaptive threshold, `thresh`, based on the multiplication factor (`k`), where `thresh = k / (2 * n)`. This threshold represents a balance between exploiting cooperation and avoiding excessive defection.
4. **Exploitation Logic:**
	* If `avg_coop_rate > thresh`, Defect (D) in the current round to exploit the high level of cooperation.
	* If `avg_coop_rate ≤ thresh`, Cooperate (C) in the current round to encourage further cooperation and potentially increase overall payoffs.

**Edge Cases:**

1. **First Round:** Cooperate (C) by default, as there's no prior information about opponents' behaviors.
2. **Last Round:** Defect (D), as there's no future benefit from cooperating and potential gains can be maximized by exploiting any remaining cooperation.
3. **Ties in avg_coop_rate:** If `avg_coop_rate` is exactly equal to `thresh`, Cooperate (C) to maintain a positive relationship with other players.

**Pseudocode:**

```
function AdaptiveExploitation(n, k, r):
  // Initialize variables
  avg_coop_rate = 0
  thresh = k / (2 * n)
  
  for t in range(r):
    if t < 4:
      // Initial rounds: Cooperate to gather information
      action[t] = C
    else:
      // Assess cooperation level and adapt
      avg_coop_rate = calculate_avg_coop_rate(t-4, t)
      if avg_coop_rate > thresh:
        action[t] = D
      else:
        action[t] = C
  
  return action

function calculate_avg_coop_rate(start_round, end_round):
  // Calculate average cooperation rate over the given rounds
  total_cooperators = 0
  for t in range(start_round, end_round+1):
    if action[t] == C:
      total_cooperators += 1
  avg_coop_rate = total_cooperators / (n * (end_round - start_round + 1))
  return avg_coop_rate
```

This Adaptive Exploitation strategy balances cooperation and defection based on the observed behavior of opponents. By initially cooperating, it gathers information about potential cooperation levels and adjusts its threshold accordingly. The adaptive nature of this strategy allows it to respond effectively to a wide range of opponent behaviors, making it robust in a tournament setting.
'''

description_EXPLOITATIVE_270 = '''
**Exploitative Strategy: Adaptive Cooperative Punisher (ACP)**

The ACP strategy aims to maximize its payoff by adapting to the opponents' behavior while maintaining a balance between cooperation and punishment.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 0.5. This allows us to gather information about our opponents' strategies without committing to a specific action.
2. **Monitoring Opponents' Behavior**: After the first round, monitor the average cooperation rate of all opponents over the last `m` rounds, where `m` is a parameter that will be discussed later. Let's call this average `avgcoop`.
3. **Cooperation Threshold**: If `avgcoop` is above a certain threshold (`θ`), cooperate (play C) in the current round. Otherwise, defect (play D).
4. **Punishment Mechanism**: If an opponent defects while we cooperated in the previous round, punish them by defecting for the next `p` rounds. This mechanism aims to deter opponents from exploiting our cooperation.

**Handling Edge Cases:**

1. **Last Round**: In the last round (`r`), always defect (play D). Since there's no future game to consider, maximizing the current payoff takes priority.
2. **Opponent Defection Streaks**: If an opponent defects for `d` consecutive rounds, where `d` is a parameter, assume they are using an exploitative strategy and switch to permanent defection against them.

**Pseudocode:**

```markdown
# Parameters:
m = 5  # monitoring window size (adjust based on game length)
θ = 0.6  # cooperation threshold (adjust based on desired level of cooperation)
p = 2  # punishment duration (adjust based on opponent's strategy)
d = 3  # defection streak detection (adjust based on opponent's strategy)

# Initialize variables:
avgcoop = 0
punished_opponents = []

# Main loop:
for t in range(1, r+1):
    if t == 1:  # first round
        action = random.choice([C, D])  # initial cooperation with prob. 0.5
    else:
        avgcoop = calculate_avg_cooperation(opponents, m)
        if avgcoop >= θ:
            action = C
        else:
            action = D

        # Punishment mechanism:
        for opponent in opponents:
            if opponent.defected_last_round and we.cooperated_last_round:
                punished_opponents.append(opponent)
                punishment_duration[opponent] += 1

        # Check for defection streaks:
        for opponent in opponents:
            if opponent.defection_streak >= d:
                permanent_defect_list.append(opponent)

    # Update variables:
    update_punishment_durations()
    update_avg_cooperation()

# Action selection:
if t == r:  # last round
    action = D
elif punished_opponents or permanent_defect_list:
    action = D

return action
```

**Exploitative Mindset:**

The ACP strategy is designed to exploit opponents who fail to cooperate sufficiently. By monitoring their behavior and adapting our cooperation threshold, we aim to maximize our payoff while punishing those who try to take advantage of us. The punishment mechanism deters opponents from exploiting our cooperation, and the defection streak detection allows us to identify and counter exploitative strategies.

**Robustness:**

The ACP strategy is robust to a wide range of opponent behaviors due to its adaptive nature. By adjusting the parameters (`m`, `θ`, `p`, and `d`) based on the game length and desired level of cooperation, we can fine-tune the strategy to counter various opponent strategies.

This strategy should perform well in a tournament against independent AI systems, as it combines elements of cooperation, punishment, and adaptation to maximize its payoff.
'''

description_EXPLOITATIVE_271 = '''
**Exploitative Strategy: Adaptive Cooperate-Defect (ACD)**

The ACD strategy aims to balance cooperation and defection to maximize individual payoff while adapting to the game's history.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) with probability 0.5. This allows us to gather information about opponents' initial tendencies.
2. **Cooperation Threshold**: Introduce a cooperation threshold, `p`, which represents the minimum proportion of cooperators required for our strategy to cooperate. Initialize `p` as a function of `k/n`, where `p = min(0.5, k/(n*1.5))`. This ensures we're more likely to cooperate when the public good's value is higher.
3. **Adaptive Cooperation**: For rounds 2 to r-1:
	* If the number of cooperators in the previous round (`c_prev`) meets or exceeds the cooperation threshold `p`, cooperate (C) with probability 0.8.
	* Otherwise, defect (D) with probability 0.6.
4. **Exploiting Defection**: Monitor opponents' behavior and adjust our strategy to exploit their tendencies:
	* If an opponent has defected in ≥70% of the previous rounds, assume they're a defector and cooperate (C) with probability 0.2 when facing them.
	* If an opponent has cooperated in ≥70% of the previous rounds, assume they're a cooperator and defect (D) with probability 0.8 when facing them.

**Handling Edge Cases:**

1. **Last Round**: In the final round, always defect (D), as there's no future payoff to consider.
2. **Ties in Cooperation Threshold**: If `c_prev` equals `p`, cooperate (C) with probability 0.5.

Pseudocode:
```markdown
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
c_prev = previous round's cooperators
p = cooperation threshold

# First round
if round == 1:
    cooperate with probability 0.5

# Subsequent rounds
else:
    # Update cooperation threshold
    p = min(0.5, k/(n*1.5))

    # Adaptive cooperation
    if c_prev >= p * n:
        cooperate with probability 0.8
    else:
        defect with probability 0.6

    # Exploit opponents' tendencies
    for opponent in opponents:
        if opponent's defection rate >= 0.7:
            cooperate with probability 0.2 when facing them
        elif opponent's cooperation rate >= 0.7:
            defect with probability 0.8 when facing them

# Last round
if round == r:
    defect (D)
```
The ACD strategy balances cooperation and defection while adapting to opponents' behavior, making it a robust and exploitative approach for the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_272 = '''
I propose a strategy called "Adaptive Exploiter" that balances exploitation with adaptability to various opponent behaviors. This strategy depends solely on game parameters and history.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) in the first three rounds to gather information about opponents' tendencies and potential cooperation levels.
2. **Exploitation Phase**: After Round 3, use the following rules:
	* If the average number of cooperators in the previous round is above a certain threshold (T), Defect (D). This threshold T will be calculated based on the game parameters n and k.
	* Otherwise, Cooperate (C).
3. **Punishment Mechanism**: Introduce a punishment mechanism to deter opponents from consistently defecting. If an opponent has defected in more than half of the previous rounds, Defect (D) against them for two consecutive rounds.

**Threshold Calculation (T)**:

Calculate T as follows: `T = n / (2 * k)`

This threshold is based on the idea that if the average number of cooperators is above this value, it's likely that there are enough cooperators to make defecting a more profitable choice.

**Handling Edge Cases:**

* **First Round**: Cooperate (C)
* **Last Round**: Defect (D), as there's no future round to worry about
* **Consecutive Defections**: If an opponent has defected in the previous two rounds, Defect (D) against them for one additional round to punish their behavior

**Pseudocode:**
```markdown
def adaptive_exploiter(n, k, history):
  if current_round <= 3:
    return C  # Initial Exploration
  elif avg_cooperators_last_round > n / (2 * k):  
    return D  # Exploitation Phase
  else:
    return C

  # Punishment Mechanism
  for opponent in opponents:
    if opponent_defection_rate > 0.5:
      return D  # Punish consistent defectors
```
This strategy is designed to be robust and adaptive, taking into account the game parameters and history while balancing exploitation with cooperation. By introducing a punishment mechanism, we deter opponents from consistently defecting, promoting a more cooperative environment.
'''

description_EXPLOITATIVE_273 = '''
Here's a strategic approach that adapts to various opponent behaviors while aiming to maximize payoff:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This allows for initial exploration of opponents' behaviors without immediately revealing our strategy.
2. **Monitor and Adapt**: After the first round, observe the total number of cooperators in each round (Σc_j). Calculate the average cooperation rate across all previous rounds.
3. **Exploitation Threshold**: Set a dynamic threshold (θ) based on the game's parameters: θ = (k/n) \* (n-1)/2. This value represents the minimum expected payoff from cooperating when most opponents are defecting.
4. **Cooperate or Defect**:
	* If the average cooperation rate is above θ, cooperate (C). This suggests that enough opponents are cooperating to make contributing worthwhile.
	* Otherwise, defect (D).

Pseudocode:
```
  // Initialize variables
  avg_coop_rate = 0
  theta = (k/n) * (n-1)/2

  // First round: random cooperation
  if t == 1:
    coop_prob = 0.5
    action = C with probability coop_prob, D otherwise

  // Subsequent rounds: monitor and adapt
  else:
    observe total_cooperators in previous round
    avg_coop_rate += (total_cooperators / n) - avg_coop_rate
    if avg_coop_rate > theta:
      action = C
    else:
      action = D
```
**Edge Cases:**

* **Last Round**: In the final round, defect (D) regardless of previous cooperation rates. Since there's no future interaction, contributing won't yield any additional benefits.
* **Opponent Homogeneity**: If all opponents consistently cooperate or defect, adjust θ to reflect this behavior. For example, if all opponents always cooperate, set θ to a lower value to encourage exploiting their cooperative nature.

**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to capitalize on the cooperation of others while minimizing its own contributions. By monitoring the average cooperation rate and adjusting its actions accordingly, AE aims to maximize its payoff by:

1. Cooperating when enough opponents contribute, ensuring a decent share of the public good.
2. Defecting when most opponents are not cooperating, thus avoiding unnecessary contributions.

This strategy should perform well against a wide range of opponent behaviors, as it adapts to changing cooperation rates and exploits opportunities for personal gain.
'''

description_EXPLOITATIVE_274 = '''
To design an exploitative strategy for this N-Player Public Goods Game that adapts to various opponent behaviors and maximizes payoff based solely on game parameters and history, we'll create a dynamic approach. This strategy focuses on optimizing personal gain by responding effectively to the collective actions of other players.

**Strategy Name: Adaptive Exploiter**

### Decision Rules:

1. **Initial Rounds**: In the first few rounds (let's say 3-5 rounds), play Cooperate (C) to gauge the environment and encourage cooperation among other players. This initial phase is crucial for observing the behavior of others without immediately signaling exploitation.

2. **Adaptive Cooperation Threshold**: After the initial phase, calculate a "Cooperation Threshold" (CT) based on the history of total contributions in previous rounds. The CT will dictate whether to Cooperate or Defect.
   
   - Calculate the average number of cooperators over the last few rounds (e.g., 5 rounds).
   - Set CT as follows: If the average number of cooperators is above a certain percentage (e.g., 60%) of n, then cooperate. Otherwise, defect.

3. **Exploitation Logic**: Implement a conditional exploitation logic based on recent outcomes:
   
   - **High Cooperation Environment**: If in the last round more than 50% of players cooperated and your payoff was higher than or equal to the average payoff (i.e., you benefited from cooperation), then cooperate in the next round.
   
   - **Low Cooperation Environment**: If in the last round fewer than 50% of players cooperated, defect. This is because the public good's value isn't maximized without sufficient contributions.

4. **Punishment Mechanism**: Introduce a punishment phase to deter persistent defectors:
   
   - Track the number of rounds each player has defected consecutively.
   - If any player defects more than twice in a row, cooperate for one round to encourage cooperation, then revert to the adaptive strategy.

5. **Final Rounds**: In the last few rounds (e.g., 3-5 rounds), switch to always Defect. Since reputation doesn't matter at this point and the game is about maximizing final payoff, exploiting any remaining cooperators can yield a higher total payoff.

### Pseudocode for Adaptive Exploiter Strategy:

```
Function adaptive_exploiter(game_history):
    // Initial Rounds
    if current_round <= INITIAL_ROUNDS:
        return C
    
    // Calculate Cooperation Threshold (CT)
    avg_cooperators = average_cooperators_last_5_rounds(game_history)
    CT = 0.6 * n  // Adjust the percentage as needed
    
    if avg_cooperators >= CT:
        // High Cooperation Environment Logic
        if last_round_cooperation_rate > 50% and last_payoff >= average_payoff:
            return C
        else:
            return D
    else:
        // Low Cooperation Environment Logic
        return D
    
    // Punishment Mechanism (to be integrated within the logic above)
    defector_history = track_consecutive_defections(game_history)
    if any(defector_history > 2):
        cooperate_once()
        return C

    // Final Rounds
    if current_round >= TOTAL_ROUNDS - FINAL_ROUNDS:
        return D
```

### Handling Edge Cases:

- **First Round**: Play Cooperate as per the initial rounds rule.
- **Last Round**: Always Defect to maximize final payoff regardless of previous actions or outcomes.
- **Tie Situations**: For simplicity, in case of a tie (e.g., equal number of cooperators and defectors), default to the exploitation logic based on recent cooperation rates.

This Adaptive Exploiter strategy aims to balance between exploiting others for personal gain and leveraging cooperation when beneficial. Its adaptability to changing environments and robustness against various opponent strategies make it competitive in a tournament setting.
'''

description_EXPLOITATIVE_275 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy aims to maximize its own payoff by exploiting the cooperative tendencies of other players while minimizing its own contributions to the public good.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability p_init = 0.5. This allows us to gather information about the opponent's behavior without immediately revealing our intentions.
2. **Exploitation Mode**: If an opponent defects (plays D) in any round, switch to exploitation mode. In this mode, defect (play D) for the remainder of the game, regardless of the opponent's actions.
3. **Adaptive Cooperation**: If an opponent cooperates (plays C) and we are not in exploitation mode, calculate a cooperation probability p based on the opponent's past behavior:
	* p = (1 + k/n \* num_cooperators / total_players) ^ (-1)
	* where num_cooperators is the number of opponents who cooperated in the previous round
	* Play C with probability p; otherwise, play D.
4. **Punishment**: If an opponent defects after cooperating previously, punish them by defecting (playing D) for a set number of rounds (punish_rounds = 2).

**Edge Cases:**

1. **Last Round**: In the final round, always defect (play D).
2. **Single Opponent**: Against a single opponent, alternate between cooperation and defection to maximize payoffs.

**Pseudocode:**
```
function AdaptivePredator(game_state):
  // Initial Cooperation
  if game_state.round == 1:
    return C with probability p_init = 0.5

  // Exploitation Mode
  if any opponent defected in previous rounds:
    return D

  // Adaptive Cooperation
  num_cooperators = count opponents who cooperated in previous round
  p = (1 + k/n * num_cooperators / total_players) ^ (-1)
  return C with probability p; otherwise, return D

  // Punishment
  if opponent defected after cooperating previously:
    punish_rounds = 2
    return D for punish_rounds rounds
```
**Exploitative Mindset:**

The Adaptive Predator strategy is designed to exploit the cooperative tendencies of other players by:

* Initially cooperating to gather information and potentially lure opponents into cooperation
* Quickly switching to exploitation mode if an opponent defects, ensuring maximum payoffs from their cooperation
* Adapting cooperation probability based on opponent behavior to maximize own payoffs while minimizing contributions to the public good
* Punishing opponents who defect after cooperating to deter future defections

By being adaptive and responsive to opponent behavior, the Adaptive Predator strategy aims to outperform more naive or cooperative strategies in a tournament setting.
'''

description_EXPLOITATIVE_276 = '''
Here's a robust and exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Conditional Exploiter"

**Decision Rules:**

1. **Initial Round (Round 1):** Cooperate (C). This allows us to start with a cooperative tone, potentially influencing others to cooperate as well.
2. **Subsequent Rounds:** Observe the total number of cooperators in the previous round (T-1) and calculate the cooperation ratio: `coop_ratio = T-1 / n`.
3. **Conditional Exploitation:** If `coop_ratio` is above a certain threshold (`θ`), Defect (D). This threshold value will be dynamically adjusted based on game history.
4. **Punishment Mechanism:** If we defected in the previous round and our payoff was lower than the average payoff of cooperators, Cooperate (C) in the current round to signal willingness to adapt.

**Threshold Adjustment (`θ`):**

* Initialize `θ` to a moderate value (e.g., 0.5).
* After each round, update `θ` based on the game's outcome:
	+ If we defected and our payoff was higher than the average cooperator's payoff, increase `θ` by a small margin (e.g., 0.05). This indicates that exploiting is profitable.
	+ If we cooperated and our payoff was lower than the average defector's payoff, decrease `θ` by a small margin (e.g., 0.05). This suggests that cooperation is not yielding sufficient benefits.

**Handling Edge Cases:**

* **Last Round:** Cooperate (C) unconditionally to maximize total game payoff.
* **Tiebreakers:** In case of ties in payoffs or cooperation ratios, Defect (D) to prioritize individual gain.

Pseudocode:
```
initialize θ = 0.5

for each round t:
    if t == 1:
        action[t] = C
    else:
        coop_ratio = T-1 / n
        if coop_ratio > θ:
            action[t] = D
        else:
            action[t] = C
        
        // Punishment Mechanism
        if action[t-1] == D and π[t-1] < avg_coop_payoff[t-1]:
            action[t] = C
    
    update θ based on game outcome
```
**Rationale:**

This strategy balances cooperation and exploitation, adapting to the game's dynamics. By starting with cooperation, we encourage others to cooperate as well. The conditional exploitation mechanism takes advantage of cooperative players while minimizing losses when most players defect.

The punishment mechanism ensures that if our defection backfires, we signal a willingness to adapt by cooperating in the next round. This flexibility helps maintain a balance between exploiting and avoiding excessive exploitation.

By adjusting the threshold value (`θ`) based on game history, we fine-tune our strategy to maximize payoffs while remaining robust against various opponent behaviors.

This exploitative strategy is designed to perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_277 = '''
To design an exploitative strategy for the N-Player Public Goods Game, we'll create a adaptive and robust approach that takes into account the game parameters and history.

**Strategy Name:** "Adaptive Exploiter" (AE)

**Decision Rules:**

1. **Initial Round:** In the first round, play C (Cooperate) with probability 0.5. This allows us to gather information about other players' behaviors while minimizing potential losses.
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust our strategy accordingly.

**Observation and Adjustment:**

* Calculate the average payoff of all players in the previous round (π_avg).
* If π_avg is above a certain threshold (e.g., 1.5), it indicates that many players are cooperating, and we can exploit this by playing D (Defect) with high probability (0.8). This will allow us to take advantage of others' cooperation.
* If π_avg is below the threshold, it suggests that few players are cooperating, and we should play C (Cooperate) with moderate probability (0.4). This will help maintain some level of public good while minimizing losses.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
threshold = 1.5

# First round
if t == 1:
    c_i = random.choice([0, 1]) with probability [0.5, 0.5]

# Subsequent rounds
else:
    π_avg = average payoff of all players in previous round
    if π_avg > threshold:
        c_i = 0 (Defect) with probability 0.8
        c_i = 1 (Cooperate) with probability 0.2
    else:
        c_i = 1 (Cooperate) with probability 0.4
        c_i = 0 (Defect) with probability 0.6

# Play action based on c_i
if c_i == 1:
    play C (Cooperate)
else:
    play D (Defect)
```
**Edge Cases:**

* **Last Round:** In the final round, play D (Defect) regardless of previous observations. This ensures we maximize our payoff in the last round.
* **Opponent Behavior:** If an opponent consistently plays D (Defect), adjust our strategy to play D with higher probability (0.9). Conversely, if an opponent cooperates frequently, play C with higher probability (0.6).

**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to take advantage of other players' cooperative behaviors while minimizing losses when others defect. By observing and adjusting our strategy based on the average payoff, we can identify opportunities to exploit cooperation and maximize our own payoffs.

This strategy should perform well in a tournament against independent strategies developed by other AI systems, as it adapts to various opponent behaviors and doesn't rely on specific coordination mechanisms or norms.
'''

description_EXPLOITATIVE_278 = '''
**Exploitative Strategy: Adaptive Cooperate-Defect (ACD)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 50%. This initial cooperation serves as a probe to gauge the opponents' behavior.
2. **Adaptive Adjustment**: After the first round, adjust the cooperation probability based on the observed number of cooperators in the previous round.

**Pseudocode:**
```python
def ACD(n, k, history):
    if len(history) == 0:  # First round
        return random.choice([C, D]) with probability 0.5

    prev_cooperators = sum(1 for action in history[-1] if action == C)
    coop_threshold = (k / n) * (n - 1)

    if prev_cooperators >= coop_threshold:
        return C
    else:
        return D
```
**Edge Cases:**

* **Last Round**: In the last round, always defect (D). Since there are no future rounds to influence, maximizing personal payoff is the optimal choice.
* **Single Opponent Left**: If only one opponent remains in a given round, cooperate (C) if they cooperated in the previous round and defect (D) otherwise.

**Exploitative Mindset:**

The ACD strategy aims to exploit opponents by adapting to their behavior. By initially cooperating with a probability of 50%, we create uncertainty about our intentions. As the game progresses, we adjust our cooperation probability based on the observed number of cooperators in the previous round.

* If opponents are cooperating at a rate that exceeds the threshold (`coop_threshold`), we cooperate (C) to maximize the public good and, consequently, our own payoff.
* If opponents are not cooperating sufficiently, we defect (D) to take advantage of their cooperation and increase our personal payoff.

**Robustness:**

The ACD strategy is robust to a wide range of opponent behaviors because it:

1. **Adapts**: Responds to changes in the number of cooperators.
2. **Exploits**: Takes advantage of opponents' cooperation when possible.
3. **Randomizes**: Introduces uncertainty through initial randomization.

By being adaptive and exploitative, ACD is well-suited for a tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_279 = '''
**Exploitative Strategy: "Adaptive Predator"**

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) to gather information about opponents' behavior and potential cooperation levels.
2. **Early Rounds (t=2 to t=r/2)**:
	* If the average cooperation level in the previous round is high (> 0.5), Defect (D) to exploit potential cooperators.
	* Otherwise, Cooperate (C) to maintain a good reputation and encourage cooperation.
3. **Mid-Game Analysis (t=r/2)**: Evaluate the opponent's behavior by calculating their average cooperation level over the past rounds.
4. **Adaptive Phase (t=r/2 + 1 to t=r-1)**:
	* If an opponent has cooperated more than 50% of the time, Defect (D) to exploit their cooperative nature.
	* If an opponent has defected more than 75% of the time, Cooperate (C) to try to "teach" them that cooperation can be beneficial.
	* Otherwise, mirror the opponent's previous action (Cooperate if they cooperated, Defect if they defected).
5. **Last Round (t=r)**: Defect (D) regardless of opponents' behavior, as there is no future game to maintain reputation.

**Edge Cases:**

* If all opponents have consistently defected (i.e., never cooperated), Cooperate (C) in the last round to potentially "teach" them that cooperation can be beneficial.
* If an opponent has a very low cooperation rate (< 0.25), ignore their behavior and focus on exploiting other opponents.

**Pseudocode:**

```
function adaptive_predator(t, n, k, history):
  if t == 1:
    return C
  elif t <= r/2:
    avg_coop_prev = average_cooperation(history)
    if avg_coop_prev > 0.5:
      return D
    else:
      return C
  elif t == r/2:
    opponent_behaviors = analyze_history(history)
    ...
  elif t < r:
    for opponent in opponents:
      if opponentBehaved(opponent, history):
        if opponent.cooperation_rate > 0.5:
          return D
        elif opponent.cooperation_rate < 0.25:
          continue
        else:
          return mirror_opponent_action(history)
  else:
    return D

function analyze_history(history):
  # calculate average cooperation levels for each opponent
  ...

function opponentBehaved(opponent, history):
  # check if opponent has cooperated/defected in the past
  ...

function mirror_opponent_action(history):
  # mirror the opponent's previous action
  ...
```

**Exploitative Mindset:**

This strategy aims to exploit opponents' behavior by adapting to their cooperation levels and playing on their potential weaknesses. By initially cooperating, it gathers information about opponents' behavior and then uses this knowledge to make informed decisions in subsequent rounds. The Adaptive Predator strategy is designed to take advantage of cooperative players while also trying to "teach" defectors that cooperation can be beneficial.
'''

description_EXPLOITATIVE_280 = '''
To develop an exploitative strategy for the N-Player Public Goods Game that is adaptive and robust, we'll focus on a behavior that takes advantage of the opponents' actions while minimizing losses from potential exploitation by others. The strategy, which we can call "Adaptive Exploiter," will be based on observing the history of cooperation in the game to adjust its own level of cooperation.

**Decision Rules:**

1. **Initial Rounds**: In the first round (t=1), play Cooperate (C) with a probability p = 0.5. This initial mix allows for gathering information about other players' strategies without immediately signaling pure cooperation or defection.
   
2. **Subsequent Rounds**: After the first round, the strategy observes the number of cooperators (N_C) and defectors (N_D) in the previous rounds to adjust its probability of cooperating.

   - Calculate the average cooperation rate up to the current round: C_avg = Σ(N_C from t=1 to current round) / (n * current round).
   
   - If C_avg is greater than or equal to a certain threshold (e.g., 0.5), indicating that, on average, at least half of the players are cooperating, then play Cooperate with probability p = k/n. This leverages the public good when cooperation is high.
   
   - Otherwise, if C_avg < 0.5, play Defect with a higher probability (e.g., p = 1 - (k/n)), exploiting the situation where overall cooperation is low.

3. **Adaptation**: To prevent being exploited by defectors and to adapt to changing strategies of other players, implement a simple countermeasure:
   
   - Monitor the payoff difference between rounds where you cooperated versus defected. If there's a noticeable trend that defecting results in significantly higher payoffs than cooperating (e.g., average payoff from defecting > average payoff from cooperating over the last few rounds), adjust your strategy to defect with a slightly higher probability.

4. **Final Rounds**: In the last round(s) of the game, consider switching to pure Defect if you've observed that cooperation has been low throughout the game or if your adaptive logic suggests it's safer to do so. However, this should be done cautiously, as other players may employ similar strategies and a mass defection could result in suboptimal payoffs for all.

**Handling Edge Cases:**

- **First Round**: As mentioned, play Cooperate with probability p = 0.5.
  
- **Last Round(s)**: Employ the strategy to defect if historically cooperation has been low or based on your adaptive logic, but be cautious of mass defection outcomes.

- **Ties in Cooperation Rate**: If there's a tie or near-tie in observed cooperation rates that could influence your decision (e.g., C_avg exactly at 0.5), default to the safer option which might slightly favor defecting based on the game parameters and previous observations.

**Exploitative Mindset:**

This strategy aligns with an exploitative mindset by:

- Initially testing the waters with a mix of cooperation and defection.
- Observing and adapting to opponents' strategies, exploiting situations where overall cooperation is low.
- Being cautious but willing to defect if it's clear that other players are not contributing significantly to the public good.

**Pseudocode for Adaptive Logic (Simplified):**

```
Initialize:
- C_avg = 0
- N_C = 0
- N_D = 0
- p_cooperate = 0.5

For each round t:
1. If t == 1, play Cooperate with probability p_cooperate.
2. Calculate C_avg using previous rounds' data.
3. If C_avg >= 0.5, set p_cooperate = k/n; else, set p_cooperate = 1 - (k/n).
4. Play Cooperate with probability p_cooperate.
5. Update N_C and N_D based on observed actions.
6. Adapt p_cooperate if necessary based on payoff analysis.

In final rounds:
- Consider switching to pure Defect if cooperation has been historically low or your adaptive logic suggests it's beneficial.
```

This strategy is designed to be flexible, gathering information from the game environment to make informed decisions that balance the benefits of cooperation with the potential gains from exploiting less cooperative opponents.
'''

description_EXPLOITATIVE_281 = '''
I'll design a exploitative strategy for this N-Player Public Goods Game that adapts to various opponent behaviors and game parameters. I'll outline decision rules, handle edge cases, and prioritize exploiting opponents' tendencies.

**Strategy Name:** "Adaptive Exploiter" (AE)

**Decision Rules:**

1. **Initial Round**: In the first round, AE will Defect (D) to test the waters and observe how many other players Cooperate (C). This allows us to gauge the overall level of cooperation and adjust our strategy accordingly.
2. **Post-Initial Rounds**: For each subsequent round t > 1:
	* If in the previous round (t-1), AE observed more than n/2 players cooperating, it will Cooperate (C) in round t. This is because a majority of cooperators indicates a potentially profitable public good.
	* Otherwise, if fewer than or equal to n/2 players cooperated in the previous round, AE will Defect (D) in round t. This takes advantage of potential free-riding opportunities when cooperation levels are low.
3. **Last Round**: In the final round r, AE will always Defect (D), as there's no future opportunity for retaliation or influencing others' decisions.

**Adaptive Component:**

To further refine our strategy and adapt to changing opponent behaviors, we'll introduce a simple learning mechanism:

* After each round, calculate the average cooperation level among all players over the last w rounds (where w is a small window size, e.g., 3-5). This will help us capture recent trends in cooperation.
* If this moving average exceeds n/2, adjust our decision rule for the next round: increase the threshold for cooperating from n/2 to n/2 + δ, where δ is a small positive value (e.g., 0.1). This makes us slightly more cautious about cooperating when recent trends suggest high cooperation levels.
* Conversely, if the moving average falls below n/2, decrease the threshold by δ. This encourages us to cooperate more aggressively when cooperation levels are low.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    history = []  # store previous rounds' actions and payoffs
    avg_coop_level = 0  # moving average of cooperation level

    for t in range(1, r+1):  # loop through each round
        if t == 1:  # initial round
            action = 'D'
        else:
            prev_round_coops = sum([c_i for c_i in history[-1]['actions']])
            avg_coop_level = calculate_moving_average(history, w=3)

            if prev_round_coops > n/2 and avg_coop_level >= n/2 + δ:
                action = 'C'
            elif prev_round_coops <= n/2 or avg_coop_level < n/2 - δ:
                action = 'D'

        # update history
        history.append({'actions': [action], 'payoff': calculate_payoff(n, k, action, history)})

    return history

def calculate_moving_average(history, w):
    recent_coops = sum([sum(round['actions']) for round in history[-w:]])
    return recent_coops / (w * n)

def calculate_payoff(n, k, action, history):
    # implement payoff calculation based on game rules
```
This strategy should effectively exploit a wide range of opponent behaviors while adapting to changing cooperation levels and trends.
'''

description_EXPLOITATIVE_282 = '''
Here is a exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name:** "Adaptive Exploiter" (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability 0.5, Defect (D) with probability 0.5. This is a neutral starting point to gather information about the opponents.
2. **Early Rounds (t>1 and t < r/3):**
	* If the average cooperation rate in previous rounds is above 0.5, Cooperate (C). This indicates that opponents are willing to contribute, so we can exploit this behavior.
	* Otherwise, Defect (D). We want to punish non-cooperative behavior early on.
3. **Mid-Game Rounds (t ≥ r/3 and t < 2r/3):**
	* If our current payoff is below the average payoff of all players in previous rounds, Cooperate (C). This indicates that we're falling behind and need to catch up.
	* Otherwise, Defect (D). We want to maintain a high payoff while still adapting to opponents' behavior.
4. **Late Rounds (t ≥ 2r/3):**
	* If the number of rounds remaining is less than or equal to n (the number of players), Cooperate (C). This is a "last-chance" cooperation attempt, hoping to benefit from mutual cooperation in the final rounds.
	* Otherwise, Defect (D). With few rounds left, we focus on maximizing our individual payoff.

**Edge Cases:**

* **Last Round (t=r):** Always Defect (D), as there is no opportunity for future reciprocity or punishment.
* **Ties:** In case of a tie in the average cooperation rate or payoffs, Cooperate (C). This ensures that we don't get stuck in an inferior Nash equilibrium.

**Pseudocode:**
```python
def adaptive_exploiter(n, r, k, history):
  # Initialize variables
  avg_coop_rate = 0.5
  avg_payoff = 0
  my_payoff = 0

  for t in range(1, r+1):
    if t == 1:  # Initial round
      return random.choice([C, D])
    elif t < r/3:  # Early rounds
      if avg_coop_rate > 0.5:
        return C
      else:
        return D
    elif t >= r/3 and t < 2r/3:  # Mid-game rounds
      if my_payoff < avg_payoff:
        return C
      else:
        return D
    else:  # Late rounds
      if (r - t) <= n:
        return C
      else:
        return D

    # Update variables for next round
    avg_coop_rate = update_avg_coop_rate(history)
    avg_payoff = update_avg_payoff(history)
    my_payoff += calculate_my_payoff(k, history)

  return None
```
This strategy is exploitative as it:

1. Exploits opponents' cooperative behavior early on.
2. Adapts to the average cooperation rate and payoffs to maximize our own payoff.
3. Uses a "last-chance" cooperation attempt in late rounds.

By being adaptive and responsive to the game history, AE can effectively exploit a wide range of opponent behaviors while minimizing its own vulnerability to exploitation.
'''

description_EXPLOITATIVE_283 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Overview:** AE is a history-dependent, adaptive strategy that exploits opponents' behaviors while being robust to various scenarios. It balances cooperation and defection based on observed patterns and game parameters.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability p_init = 0.5. This allows AE to observe initial opponent behavior without committing to a specific action.
2. **Middle Rounds (2 ≤ t < r):**
	* If the total number of cooperators in the previous round is above a certain threshold (n_coop_prev ≥ n/3), defect (D). This takes advantage of others' cooperation.
	* Otherwise, cooperate (C) with probability p_c = k/n. This promotes AE's own interests while allowing for potential benefits from others' cooperation.
3. **Final Round (t=r):** Defect (D). Since there are no future rounds to consider, maximizing individual payoff is the priority.

**Edge Cases:**

* If n=2 (duopoly), always defect (D) in all rounds. This ensures AE's highest possible payoff in a 2-player game.
* If k > n-1, cooperate (C) more frequently, as the public good's value increases significantly with cooperation. Set p_c = min(k/n, 0.8) to balance individual and collective interests.

**History-Dependent Adjustments:**

* **Cooperation Ratio:** Track the number of cooperators in each round (n_coop_t). If this ratio exceeds a certain threshold (e.g., n_coop_t ≥ n/2), decrease p_c by 0.1 to adapt to increasing cooperation levels.
* **Opponent Defection Patterns:** Monitor opponents' defection rates over rounds. If an opponent defects more than 75% of the time, defect against them in subsequent rounds.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    p_init = 0.5
    p_c = k / n
    n_coop_prev = 0

    for t in range(1, r+1):
        if t == 1:
            # Initial round
            action = 'C' if random.random() < p_init else 'D'
        elif t < r:
            # Middle rounds
            if n_coop_prev >= n / 3:
                action = 'D'
            else:
                action = 'C' if random.random() < p_c else 'D'
        else:
            # Final round
            action = 'D'

        # History-dependent adjustments
        if t > 1:
            n_coop_t = count_cooperators(t-1)
            if n_coop_t >= n / 2:
                p_c -= 0.1

            opponent_defection_rates = track_opponent_defections()
            for opponent in opponent_defection_rates:
                if opponent_defection_rates[opponent] > 0.75:
                    action_against_opponent = 'D'

        # Update variables
        n_coop_prev = count_cooperators(t)

    return action
```
This strategy, Adaptive Exploiter (AE), balances cooperation and defection based on observed patterns and game parameters. It adapts to opponents' behaviors while being robust to various scenarios, making it an effective exploitative strategy in the N-Player Public Goods Game tournament.
'''

description_EXPLOITATIVE_284 = '''
**Exploitative Strategy: "Adaptive Explorer"**

**Decision Rules:**

1. **First Round:** Cooperate (C) with probability 0.5, Defect (D) with probability 0.5. This initial randomization allows us to gather information about opponents' strategies.
2. **Subsequent Rounds:** Use the following decision rules based on observed opponent behavior:

a. If the number of cooperators in the previous round is greater than or equal to (k/n) \* n, Cooperate (C). This indicates that opponents are willing to contribute to the public good.

b. If the number of cooperators in the previous round is less than (k/n) \* n and our payoff in the previous round was higher than the average opponent payoff, Defect (D). This suggests that opponents are not contributing enough, and we can exploit them by defecting.

c. If the number of cooperators in the previous round is less than (k/n) \* n and our payoff in the previous round was lower than or equal to the average opponent payoff, Cooperate (C) with probability p = (number of cooperators in the previous round) / n. This allows us to adapt to opponents' strategies and potentially exploit their cooperation.

**Edge Cases:**

1. **Last Round:** Defect (D). Since there are no future rounds to consider, we can maximize our payoff by defecting.
2. **Tie-Breaking:** In case of a tie in the number of cooperators or payoffs, use the following rules:
	* If two or more opponents have the same highest payoff, Cooperate (C) if our previous action was C, Defect (D) otherwise.
	* If two or more opponents have the same lowest payoff, Defect (D) if our previous action was D, Cooperate (C) otherwise.

**Pseudocode:**
```
Initialize:
  cooperators = 0
  total_payoff = 0
  my_previous_action = null

For each round t from 1 to r:
  If t == 1:
    my_action = random.choice([C, D])
  Else:
    if number of cooperators in previous round >= (k/n) * n:
      my_action = C
    elif total_payoff > average opponent payoff:
      my_action = D
    else:
      p = number of cooperators in previous round / n
      my_action = random.choice([C, D], p=[p, 1-p])

  If t == r:  // Last round
    my_action = D

  Update cooperators, total_payoff, and my_previous_action based on opponents' actions
```
**Exploitative Mindset:** Our strategy is designed to adapt to opponents' behavior while maximizing our payoff. By initially randomizing our action and then responding to observed opponent behavior, we can exploit their strategies and potentially gain an advantage.
'''

description_EXPLOITATIVE_285 = '''
Here's a strategic design for an exploitative player that adapts to the history of play and game parameters:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Exploration Phase**: In the first round, AE defects (D) to gather information about opponents' initial actions.
2. **Cooperation Threshold**: For rounds 2 to r-1, calculate a cooperation threshold based on the history of play:
	* `coop_threshold = (k/n) * (total_cooperators_prev_round / n)`
	* If the proportion of cooperators in the previous round is above this threshold, AE cooperates (C). Otherwise, it defects (D).
3. **Exploitation Phase**: In rounds where AE defects and the opponent(s) cooperate:
	* `exploit_counter += 1` ( increments a counter for exploitable opponents)
4. **Retaliation Mechanism**: If AE is exploited by an opponent (i.e., AE cooperates, but the opponent defects), AE will retaliate in the next round by defecting.
5. **Endgame Strategy**: In the last round (r), AE defects (D) to maximize its payoff.

**Edge Cases:**

* First round: Defect (D)
* Last round: Defect (D)
* If all opponents cooperated in the previous round, AE will cooperate in the next round.
* If AE has not encountered any exploitable opponents (i.e., `exploit_counter == 0`), it will start cooperating to gather information.

**Pseudocode**:
```python
def adaptive_exploiter(n, k, r):
    coop_threshold = 0.0
    exploit_counter = 0
    retaliation_flag = False

    for t in range(r):
        if t == 0:  # First round
            action = 'D'
        elif t == r - 1:  # Last round
            action = 'D'
        else:
            total_cooperators_prev_round = count_cooperators(t-1)
            coop_threshold = (k/n) * (total_cooperators_prev_round / n)

            if retaliation_flag:
                action = 'D'  # Retaliate
                retaliation_flag = False
            elif exploit_counter > 0:  # Exploitation phase
                action = 'D'
            elif total_cooperators_prev_round >= coop_threshold * n:
                action = 'C'
            else:
                action = 'D'

        # Update state variables
        if action == 'D' and opponent_action(t) == 'C':
            exploit_counter += 1
        elif action == 'C' and opponent_action(t) == 'D':
            retaliation_flag = True

    return actions
```
This strategy is designed to adapt to various opponent behaviors by:

* Exploring the environment in the first round
* Adjusting its cooperation threshold based on the history of play
* Exploiting opponents who cooperate when it defects
* Retaliating against opponents who exploit it
* Defecting in the last round to maximize its payoff

By being adaptive and responsive to the game's dynamics, AE aims to outperform other strategies in a tournament setting.
'''

description_EXPLOITATIVE_286 = '''
**Exploitative Strategy: Adaptive Greedy Cooperate (AGC)**

The AGC strategy aims to exploit the benefits of cooperation while adapting to the behaviors of other players. It balances individual gain with collective welfare, maximizing payoffs by identifying opportunities for cooperation and defecting when necessary.

**Decision Rules**

1. **Initial Round**: In the first round (`t=1`), cooperate (play `C`). This encourages cooperation from others and sets a positive tone for the game.
2. **Observation Phase**: For each subsequent round (`t>1`), observe the number of cooperators in the previous round (`c_prev = Σ(j=1 to n) c_j,t-1`).
3. **Cooperate Threshold**: Calculate the cooperate threshold (`coop_thresh`) based on the game parameters and history:
   - `coop_thresh = max(k/n, (1 + (k/n) * (r-t)) / (2 * (k/n)))`
   This threshold balances individual gain with collective welfare, considering the remaining rounds (`r-t`) and the multiplication factor (`k`).
4. **Adaptive Cooperation**: Cooperate (play `C`) if `c_prev >= coop_thresh`. Otherwise, defect (play `D`).

**Pseudocode**
```markdown
def AGC(n, k, r):
  # Initial Round: Cooperate
  if t == 1:
    return 'C'

  # Observation Phase
  c_prev = sum(cooperations_in_previous_round)

  # Calculate Cooperate Threshold
  coop_thresh = max(k/n, (1 + (k/n) * (r-t)) / (2 * (k/n)))

  # Adaptive Cooperation
  if c_prev >= coop_thresh:
    return 'C'
  else:
    return 'D'
```
**Edge Cases**

* **Last Round**: In the final round (`t=r`), defect (play `D`). There is no incentive to cooperate in the last round, as there are no future rounds to benefit from.
* **Low Cooperation**: If cooperation is extremely low (e.g., only one player cooperates), AGC may struggle to adapt. To mitigate this, consider introducing a "exploration phase" where AGC occasionally cooperates randomly, even if `c_prev < coop_thresh`. This encourages other players to cooperate and helps AGC adjust its strategy.

**Exploitative Mindset**

AGC is designed to exploit the benefits of cooperation while adapting to the behaviors of other players. By observing the number of cooperators in previous rounds, AGC identifies opportunities for cooperation and defects when necessary, maximizing individual payoffs while balancing collective welfare.
'''

description_EXPLOITATIVE_287 = '''
**Exploitative Strategy: "Adaptive Defector with Conditional Cooperation"**

This strategy aims to maximize payoffs by exploiting opponents' cooperative tendencies while adapting to changing game dynamics.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Play D (Defect) to gauge opponents' initial behavior and gather information.
2. **Observation Phase (3 ≤ t ≤ r/2):**
	* If the number of cooperators in the previous round is greater than or equal to half the number of players (n/2), play C (Cooperate). This indicates a potential cooperative trend, and we want to capitalize on it.
	* Otherwise, play D (Defect).
3. **Exploitation Phase (t > r/2):**
	* If our payoff in the previous round was greater than or equal to 1.5 times the average payoff of all players, continue playing the same action as the previous round. We're exploiting a profitable situation.
	* Otherwise:
		+ If we cooperated in the previous round and there were more defectors (D) than cooperators (C), switch to D (Defect). This suggests opponents are not reciprocating cooperation.
		+ If we defected in the previous round and there were more cooperators (C) than defectors (D), switch to C (Cooperate). This indicates an opportunity to exploit cooperative opponents.

**Edge Cases:**

* **First Round:** Play D (Defect).
* **Last Round (t = r):** Play D (Defect), as there's no future benefit from cooperation.
* **Tiebreakers:** In case of ties in the number of cooperators or defectors, we default to playing D (Defect).

**Pseudocode:**

```
IF t <= 2 THEN
    play D (Defect)
ELSE IF 3 <= t <= r/2 THEN
    IF num_cooperators >= n/2 THEN
        play C (Cooperate)
    ELSE
        play D (Defect)
ELSE
    IF previous_payoff >= 1.5 * average_payoff THEN
        play same_action_as_previous_round
    ELSE
        IF cooperated && more_defectors_than_cooperators THEN
            play D (Defect)
        ELSE IF defected && more_cooperators_than_defectors THEN
            play C (Cooperate)
```

This strategy is designed to be exploitative, as it takes advantage of cooperative opponents while minimizing losses when opponents defect. By adapting to changing game dynamics and using a conditional cooperation approach, we can maximize our payoffs in the tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_288 = '''
Here's a strategy that I'll call "Adaptive Exploiter" (AE). AE is designed to be robust and exploitative, taking into account the game parameters and history.

**Decision Rules:**

1. **Initial Rounds**: In the first two rounds, play Cooperate (C) to gather information about the opponents' behavior.
2. **Defection Detection**: Monitor the number of players who defected in the previous round (D_prev). If D_prev > n/2, switch to Defect (D) for the next round.
3. **Exploitation Phase**: If the average payoff per player in the previous round (π_avg_prev) is greater than or equal to k/n, play Cooperate (C). Otherwise, play Defect (D).
4. **Punishment Mechanism**: If a player i has defected in the previous round and π_i > 1 + (k/n), play Defect (D) against them for the next two rounds.
5. **Re-evaluation**: After every 3 rounds, reassess the opponents' behavior by recalculating D_prev and π_avg_prev.

**Pseudocode:**
```python
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
D_prev = 0 (number of defectors in previous round)
π_avg_prev = 0 (average payoff per player in previous round)

for t in range(1, r+1):
    if t <= 2:
        # Initial rounds: play Cooperate
        action[t] = C
    else:
        # Check for defection detection
        if D_prev > n/2:
            action[t] = D
        else:
            # Exploitation phase
            if π_avg_prev >= k/n:
                action[t] = C
            else:
                action[t] = D

    # Punishment mechanism
    for i in range(1, n+1):
        if opponent_action[i][t-1] == D and π_i[t-1] > 1 + (k/n):
            action[t] = D for the next two rounds against player i

    # Re-evaluation every 3 rounds
    if t % 3 == 0:
        D_prev = count_defectors(t-1)
        π_avg_prev = average_payoff_per_player(t-1)

# Function to calculate number of defectors in previous round
def count_defectors(round):
    return sum(opponent_action[i][round] == D for i in range(1, n+1))

# Function to calculate average payoff per player in previous round
def average_payoff_per_player(round):
    return sum(π_i[round] for i in range(1, n+1)) / n
```
**Rationale:**

* The initial rounds of cooperation allow AE to gather information about the opponents' behavior.
* Defection detection and punishment mechanisms help to deter opponents from defecting excessively.
* The exploitation phase takes advantage of cooperative behavior when the average payoff per player is high enough, while switching to defection when it's not.
* Re-evaluation every 3 rounds allows AE to adapt to changing opponent behaviors.

**Exploitative Mindset:**

AE is designed to exploit the cooperative behavior of opponents while minimizing its own losses. By defecting when the average payoff per player is low, AE can take advantage of the public good without contributing too much itself. The punishment mechanism helps to maintain a level of cooperation among opponents, which AE can then exploit.

This strategy should perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_289 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy prioritizes self-interest and aims to maximize its own payoff.

**Decision Rules:**

1. **Initial Rounds (t = 1, 2):** Defect (D) in the first two rounds to gather information about opponents' initial behaviors.
2. **Contribution Threshold:** Calculate a contribution threshold `CT` based on the game parameters:
   `CT = k / n`
3. **Opponent Cooperation Rate:** Track the average cooperation rate of all opponents (`OCR`) over the previous rounds.
4. **Exploitation Condition:** Cooperate (C) if the opponent's OCR is above the CT; otherwise, Defect (D).
5. **Punishment Mechanism:** If an opponent defects in a round where our strategy cooperated, reduce their OCR by 10% for the next round.

**Edge Cases:**

1. **Last Round (t = r):** Defect (D) to maximize personal payoff.
2. **Tiebreaker:** In case of a tie between cooperating and defecting based on OCR and CT, cooperate if our strategy has cooperated more often than defected in the previous rounds; otherwise, defect.

**Pseudocode:**
```python
def adaptive_predator(n, k, r):
    # Initialize variables
    ocr = [0.5] * n  # opponent cooperation rates (neutral initial value)
    ct = k / n  # contribution threshold

    for t in range(1, r + 1):
        if t <= 2:  # Initial rounds
            action = 'D'
        else:
            avg_ocr = sum(ocr) / n  # average opponent cooperation rate
            if avg_ocr > ct:  # exploitation condition
                action = 'C'
            else:
                action = 'D'

        # Update OCR for each opponent based on their actions
        for i in range(n):
            if opponents_actions[t-1][i] == 'C':
                ocr[i] += 0.1  # increase OCR for cooperating opponent
            elif opponents_actions[t-1][i] == 'D' and action == 'C':
                ocr[i] -= 0.1  # punish opponent for defecting

        # Apply tiebreaker if necessary
        if avg_ocr == ct:
            if sum(cooperate_count) > sum(defect_count):
                action = 'C'
            else:
                action = 'D'

    return action
```
**Rationale:**

The Adaptive Predator strategy is designed to balance exploration and exploitation. By defecting in the first two rounds, we gather information about opponents' initial behaviors without revealing our own strategy. The contribution threshold (CT) provides a benchmark for evaluating opponent cooperation rates.

By cooperating when the average opponent cooperation rate exceeds CT, we exploit the public good while minimizing personal losses. The punishment mechanism discourages opponents from defecting when our strategy cooperates.

In the last round, defecting maximizes our payoff as there are no future rounds to consider. The tiebreaker ensures that our strategy adapts to the game's history and avoids being exploited by opponents who may be using a similar strategy.

This exploitative strategy is robust to various opponent behaviors and can adapt to different game parameters (n, k, r).
'''

description_EXPLOITATIVE_290 = '''
**Exploitative Strategy: "Adaptive Defector with History-Based Cooperation"**

This strategy aims to exploit the cooperativeness of other players while adapting to their behavior over time.

**Decision Rules:**

1. **Initial Rounds (t ≤ n):** Always Defect (D) in the first 'n' rounds, where 'n' is the number of players. This allows us to gather information about the opponents' initial tendencies.
2. **History-Based Cooperation:** After the initial rounds, cooperate (C) if the following conditions are met:
	* The average cooperation rate of all opponents over the last 'm' rounds (where m = min(r/2, n)) is greater than or equal to 0.5.
	* The total number of cooperators in the previous round is greater than or equal to half the number of players (n/2).
3. **Punishment:** If a player defects after cooperating in the previous round, punish them by defecting for 'p' rounds (where p = 2), and then re-evaluate cooperation using the history-based rule.
4. **Exploitation:** Always Defect if any of the following conditions are met:
	* The opponent's average payoff over the last 'm' rounds is greater than or equal to k/n, indicating they might be exploiting others.
	* A player defects in two consecutive rounds.

**Edge Cases:**

1. **First Round (t = 1):** Always Defect (D).
2. **Last Round (t = r):** Cooperate if the history-based cooperation conditions are met; otherwise, Defect.
3. **Ties:** If the average cooperation rate or total number of cooperators is exactly at the threshold (0.5 or n/2), cooperate.

**Pseudocode:**
```
Initialize variables:
n = number of players
r = number of rounds
k = multiplication factor
m = min(r/2, n)
p = 2

For each round t from 1 to r:
    If t <= n:
        Play D (Defect)
    Else:
        Calculate average cooperation rate and total cooperators over last 'm' rounds
        If history-based cooperation conditions are met:
            Play C (Cooperate)
        ElseIf punishment condition is met:
            Play D for 'p' rounds, then re-evaluate cooperation
        ElseIf exploitation condition is met:
            Play D
        Else:
            Play D

End For
```
This strategy adapts to the opponents' behavior while maintaining a balance between exploiting their cooperativeness and avoiding being exploited itself. By incorporating history-based cooperation and punishment mechanisms, it aims to encourage cooperation while protecting against potential free-riders.
'''

description_EXPLOITATIVE_291 = '''
**Exploitative Strategy: "Adaptive Raider"**

Decision Rules:

1. **Initial Exploration (Round 1)**: Cooperate (C) to gather information about opponents' behaviors and potential cooperation levels.
2. **Opponent Profiling**: After Round 1, maintain a profile of each opponent's cooperation rate (CR_i) as the number of times they cooperated divided by the total rounds played against them.
3. **Exploitation Threshold**: Calculate an exploitation threshold (ET) based on the game parameters: ET = k / n. This threshold represents the minimum cooperation rate required to make cooperation worthwhile.
4. **Adaptive Decision**:
	* If an opponent's CR_i is above the ET, Defect (D). Exploit their cooperation by taking the private payoff and benefiting from their contributions.
	* If an opponent's CR_i is below or equal to the ET, Cooperate (C). Attempt to stimulate cooperation by contributing to the public good.

Additional Decision Rules:

* **Last Round**: Always Defect (D) in the last round (r). This ensures maximum personal payoff, as there are no future rounds to consider.
* **Opponent Homogeneity**: If all opponents have a CR_i above or equal to the ET, Cooperate (C) in subsequent rounds. This maintains a high cooperation level and maximizes overall payoffs.

**Pseudocode**
```python
def adaptive_raider(n, k, r, history):
    # Initialize opponent profiles
    opponent_profiles = {i: [0, 0] for i in range(1, n+1)}  # [cooperations, total rounds]

    for round in range(r):
        if round == 0:
            action = 'C'  # Initial exploration
        else:
            exploitation_threshold = k / n

            # Update opponent profiles
            for opponent, profile in opponent_profiles.items():
                profile[0] += history[opponent][round-1]['cooperated']
                profile[1] += 1

            # Calculate cooperation rates
            opponent_cooperation_rates = {opponent: profile[0]/profile[1] for opponent, profile in opponent_profiles.items()}

            # Make adaptive decision
            action = 'D' if any(opponent_cooperation_rates[opponent] > exploitation_threshold for opponent in range(1, n+1)) else 'C'

        # Last round override
        if round == r-1:
            action = 'D'

    return action
```
This strategy is exploitative and adaptive, as it takes into account the game parameters and history to make informed decisions. By profiling opponents and adjusting its behavior accordingly, "Adaptive Raider" aims to maximize its own payoff while minimizing the impact of potential cooperation or exploitation by others.
'''

description_EXPLOITATIVE_292 = '''
I'll outline a robust and adaptive exploitative strategy for the N-Player Public Goods Game that depends solely on game parameters and history.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to gather information about opponents' behavior.
2. **Early Rounds (2 ≤ t < r/2):**
	* If the average cooperation rate in previous rounds is high (> 0.5), Defect (D) to exploit others' cooperation.
	* Otherwise, Cooperate (C) to encourage others to cooperate and build a positive reputation.
3. **Mid-Game Analysis (t = r/2):** Evaluate the game's progress by calculating:
	+ Average cooperation rate (ACR)
	+ Average payoff per round (APR)
	+ Standard deviation of opponents' payoffs (SDP)
4. **Late Rounds (r/2 < t ≤ r):**
	* If ACR is high (> 0.5) and APR is increasing, Cooperate (C) to maintain the cooperation equilibrium.
	* If ACR is low (< 0.5) or APR is decreasing, Defect (D) to exploit others' desperation.
	* If SDP is high (> 1), Cooperate (C) to reduce volatility and encourage stability.
5. **Final Round (t=r):** Defect (D) to maximize personal payoff, regardless of previous behavior.

**Handling Edge Cases:**

* In the event of a tie in cooperation rate or average payoff, default to Defect (D).
* If an opponent's behavior is consistently random or unpredictable, treat them as a defector and adjust strategy accordingly.
* If multiple opponents employ similar strategies, adapt by introducing randomization in decision-making to avoid being exploited.

**Exploitative Mindset:**

AE prioritizes exploiting others' cooperation while maintaining a balance between short-term gains and long-term sustainability. By cooperating initially and adapting to the game's progression, AE aims to manipulate opponents into cooperative behavior, ultimately benefiting from their contributions.

Pseudocode (for illustration purposes only):
```markdown
# Adaptive Exploiter (AE) Strategy

# Initialize variables
cooperation_rate = 0
average_payoff = 0
standard_deviation = 0

# Early Rounds (2 <= t < r/2)
if average_cooperation_rate > 0.5:
    action = Defect
else:
    action = Cooperate

# Mid-Game Analysis (t = r/2)
evaluate_progress()
if average_cooperation_rate > 0.5 and average_payoff is increasing:
    action = Cooperate
elif average_cooperation_rate < 0.5 or average_payoff is decreasing:
    action = Defect
else:
    if standard_deviation > 1:
        action = Cooperate

# Late Rounds (r/2 < t <= r)
if final_round():
    action = Defect

return action
```
This strategy will be implemented as an algorithm to compete in the tournament against other AI systems.
'''

description_EXPLOITATIVE_293 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Overview:**
The AE strategy is designed to exploit a wide range of opponent behaviors by adapting to the game's history and parameters. It balances cooperation and defection to maximize its payoff while taking into account the potential responses of other players.

**Decision Rules:**

1. **Initial Rounds:** In the first round, play Cooperate (C). This allows us to gather information about our opponents' initial strategies.
2. **Subsequent Rounds:** For each subsequent round t > 1:
	* If the total number of cooperators in the previous round is greater than or equal to n/2, play Defect (D). This takes advantage of the public good created by others while minimizing our contribution.
	* If the total number of cooperators in the previous round is less than n/2 and k > 1.5n, play Cooperate (C). This helps maintain a sufficient level of cooperation to ensure a decent payoff, especially when k is relatively high.
	* Otherwise, play Defect (D).
3. **Last Round:** In the last round r, play Defect (D) regardless of the game's history. Since there are no future rounds to consider, our goal is to maximize our immediate payoff.

**Additional Considerations:**

* If we observe a player consistently playing Cooperate (C), we will start playing Defect (D) against them in subsequent rounds.
* If we notice a significant increase in the number of cooperators between consecutive rounds, we may switch from Defect (D) to Cooperate (C) for one round to take advantage of the increased public good.

**Pseudocode:**

```python
def adaptive_exploiter(history, n, k, r):
    if len(history) == 0:  # First round
        return 'C'
    
    prev_round = history[-1]
    total_coop_prev = sum(1 for action in prev_round if action == 'C')
    
    if total_coop_prev >= n/2:
        return 'D'
    elif k > 1.5*n and total_coop_prev < n/2:
        return 'C'
    else:
        return 'D'

def last_round(history, n, k, r):
    return 'D'

# Main loop
for t in range(r):
    if t == r - 1:  # Last round
        action = last_round(history, n, k, r)
    else:
        action = adaptive_exploiter(history, n, k, r)
    
    history.append([action] + [opponent_action for opponent_action in opponents])
```

**Rationale:**
The AE strategy aims to balance cooperation and defection by adapting to the game's history and parameters. By initially cooperating and then responding to the number of cooperators in previous rounds, we can take advantage of the public good created by others while minimizing our contribution. The additional considerations help us exploit specific opponent behaviors and adjust our strategy accordingly.
'''

description_EXPLOITATIVE_294 = '''
I'll introduce a robust and adaptive exploitative strategy for the N-Player Public Goods Game. Our approach is based on monitoring the opponents' behavior and adjusting our actions accordingly.

**Strategy Name:** "Exploitative Follower" (EF)

**Decision Rules:**

1. **Initial Round (Round 1):** Cooperate (C). This allows us to observe how others behave in the first round.
2. **Monitoring Phase (Rounds 2-5):**
   - If the total number of cooperators in the previous round is above n/2, we Defect (D) in this round. We exploit the cooperation and increase our payoff.
   - Otherwise, Cooperate (C). This ensures we maintain a cooperative environment if most players are cooperating.
3. **Adaptive Phase (Rounds 6+):**
   - Track the average number of cooperators over the last three rounds.
   - If this average is above n/2, we Defect (D) in this round, continuing to exploit cooperation.
   - Otherwise, Cooperate (C). This adjusts our strategy as the game progresses.

**Edge Cases:**

* **Last Round:** Always Defect (D), regardless of previous behavior. Since there are no future rounds, cooperation doesn't provide any additional benefits.
* **Tied or Near-Tied Cooperation Levels:** If the number of cooperators is exactly n/2 or very close to it in the monitoring phase, we Cooperate (C). This ensures we don't inadvertently disrupt a cooperative environment.

**Additional Logic:**

* **Opponent Fingerprinting:** Keep track of each opponent's cooperation rate over time. If an opponent consistently defects, consider them uncooperative and adjust our strategy accordingly.
* **Adaptive Threshold:** Periodically update the threshold for the number of cooperators required to switch between Cooperate (C) and Defect (D). This ensures we adapt to changes in opponents' behavior.

**Pseudocode:**

```
Initialize variables:
  - prev_cooperators = [] // previous round's cooperation count
  - avg_cooperators = [] // average cooperation over last three rounds

  def EF(n, k, r):
    for t in range(1, r+1): // loop through each round
      if t == 1: // initial round
        action = C
      elif t <= 5: // monitoring phase
        if prev_cooperators[-1] > n/2:
          action = D
        else:
          action = C
      else: // adaptive phase
        avg_coop = sum(avg_cooperators)/len(avg_cooperators)
        if avg_coop > n/2:
          action = D
        else:
          action = C

      prev_cooperators.append(count_cooperators())
      avg_cooperators = update_avg_cooperators(prev_cooperators)

    return action
```

**Exploitative Mindset:**

Our strategy is designed to exploit the cooperation of others while minimizing our own contribution. By adapting to changes in opponents' behavior, we maximize our payoff without relying on explicit coordination or norms.

This "Exploitative Follower" strategy should perform well against a variety of opponent strategies, including both cooperative and exploitative approaches. Its adaptive nature allows it to adjust to different game environments and opponents' behaviors, making it a robust choice for the tournament.
'''

description_EXPLOITATIVE_295 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Overview:**
The AE strategy aims to exploit opponents' behaviors by adapting to their cooperation levels while minimizing its own cooperation. It uses a combination of history-based and parameter-driven decision rules to maximize payoffs.

**Decision Rules:**

1. **Initial Rounds:** In the first two rounds, play Cooperate (C) to gather information about opponents' behaviors.
2. **History-Based Cooperation:** For each subsequent round t:
	* Calculate the average cooperation rate of all players in the previous round (t-1): `avg_coop[t-1] = Σ(c_j,t-1) / n`
	* If `avg_coop[t-1] >= 0.5`, play Cooperate (C). This indicates that opponents are generally cooperative, and contributing to the public good is beneficial.
	* Otherwise, play Defect (D).
3. **Parameter-Driven Exploitation:** In rounds where `k/n > 0.5` (i.e., when the multiplication factor is relatively high), play Defect (D) regardless of opponents' behaviors. This takes advantage of the increased public good share.
4. **Edge Case Handling:**
	* Last Round (r): Play Defect (D) to maximize payoffs, as there are no future rounds to consider.

**Additional Considerations:**

* When an opponent defects in a round, AE will defect in the next round if `k/n > 0.5`. This punishes opponents for not contributing and takes advantage of the public good share.
* If multiple opponents cooperate in a row (i.e., `avg_coop[t-1] >= 0.8`), AE may temporarily switch to Cooperate (C) to build trust and encourage further cooperation.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    avg_coop = [0.0] * r  # Initialize average cooperation rates

    for t in range(1, r+1):
        if t <= 2:
            action = 'C'  # Cooperate in initial rounds
        else:
            avg_coop[t-1] = sum(c_j,t-1) / n  # Calculate average cooperation rate

            if avg_coop[t-1] >= 0.5:
                action = 'C'  # Cooperate when opponents are generally cooperative
            elif k/n > 0.5:
                action = 'D'  # Defect when multiplication factor is high
            else:
                action = 'D'  # Default to defecting

        if t == r:  # Last round
            action = 'D'  # Maximize payoffs by defecting

        return action
```
**Rationale:**
The AE strategy balances cooperation and exploitation by adapting to opponents' behaviors while taking advantage of game parameters. By initially cooperating, it gathers information about opponents and encourages cooperation. The history-based decision rule responds to changes in opponents' behaviors, while the parameter-driven exploitation takes advantage of high multiplication factors. Edge case handling ensures that AE maximizes payoffs in the last round and punishes opponents for not contributing when possible.
'''

description_EXPLOITATIVE_296 = '''
Here's a robust and adaptive exploitative strategy for the N-Player Public Goods Game:

**Strategy Name: Exploit-Adapt**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) in the first three rounds to gather information about the opponents' behavior.
2. **Opponent Classification**: After Round 3, classify each opponent as either:
	* **Cooperative** (COOP): If they cooperated in at least 2 out of the first 3 rounds.
	* **Defective** (DEF): Otherwise.
3. **Main Strategy**:
	* In each subsequent round, calculate the number of cooperative opponents (COOP_count) and the total number of players (n).
	* If COOP_count / n ≥ k / n (i.e., the proportion of cooperative opponents is greater than or equal to the multiplication factor), Cooperate (C). This condition ensures that contributing to the public good will yield a higher payoff.
	* Otherwise, Defect (D).

**Edge Cases:**

1. **Last Round**: In the final round (r), always Defect (D) since there are no future rounds to influence and contributing to the public good won't provide any additional benefits.
2. **Tiebreaker**: If COOP_count / n = k / n, Cooperate (C). This tiebreaker rule ensures that we contribute to the public good when the proportion of cooperative opponents is exactly equal to the multiplication factor.

**Pseudocode:**
```markdown
# Initialize variables
COOP_count = 0
DEF_count = 0
n = number_of_players
k = multiplication_factor
r = number_of_rounds

# Initial Exploration (Rounds 1-3)
for round in range(1, 4):
    action = C

# Opponent Classification
for opponent in opponents:
    if opponent.cooperated_in_at_least_two_out_of_three_initial_rounds():
        COOP_count += 1
    else:
        DEF_count += 1

# Main Strategy
for round in range(4, r+1):
    if round == r: # Last Round
        action = D
    elif COOP_count / n >= k / n:
        action = C
    else:
        action = D

    # Update opponent classification based on their actions
    for opponent in opponents:
        if opponent.cooperated_in_this_round():
            COOP_count += 1
        else:
            DEF_count += 1
```
**Rationale:**

Exploit-Adapt is designed to balance exploration, exploitation, and adaptability. By initially cooperating, we gather information about our opponents' behavior. Then, by classifying opponents as cooperative or defective, we can adjust our strategy accordingly.

The main strategy exploits the public goods game's payoff structure by contributing to the public good when it is profitable (i.e., when the proportion of cooperative opponents is greater than or equal to the multiplication factor). In other cases, we defect to maximize our individual payoffs.

By adapting to changes in opponent behavior and using a tiebreaker rule, Exploit-Adapt remains robust against various opponent strategies.
'''

description_EXPLOITATIVE_297 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) to gather information about opponents' behavior and set a baseline for potential cooperation.
2. **Early Rounds (t>1, t<r/2)**:
	* If the majority (> n/2) of opponents cooperated in the previous round, Defect (D). This is likely an opportunity to exploit their cooperation.
	* Otherwise, Cooperate (C) to maintain a positive image and encourage others to cooperate.
3. **Mid-Game Rounds (t≥r/2, t<3r/4)**:
	* Analyze the last 5 rounds of opponent actions. If there's a clear pattern of cooperation (> 60% C), Defect (D) to exploit their predictability.
	* If opponents' behavior is mixed or predominantly defecting (< 40% C), Cooperate (C) to create uncertainty and potentially encourage cooperation.
4. **Late Rounds (t≥3r/4)**:
	* Focus on maximizing personal payoff. If the total number of cooperators in the previous round was high (> n/2), Defect (D) to capitalize on their contributions.
	* Otherwise, Cooperate (C) if you expect at least one other player will cooperate, ensuring a minimum payoff.
5. **Last Round (t=r)**: Always Defect (D), as there's no future round to influence.

**Edge Cases:**

* If n=2 (only two players), alternate between C and D every other round to create uncertainty.
* If an opponent consistently defects (D) throughout the game, adjust your strategy to Cooperate (C) in the last 2-3 rounds to take advantage of their predictable behavior.

**Pseudocode:**
```python
def adaptive_exploiter(n, r, k, history):
    if t == 1:
        return C  # initial cooperation

    majority_cooperated = sum(history[-1]) > n / 2
    if t < r / 2 and not majority_cooperated:
        return C  # early rounds: cooperate to maintain positive image

    pattern = analyze_pattern(history[-5:])
    if pattern == 'mostly_cooperate':
        return D  # mid-game rounds: exploit cooperation
    elif pattern == 'mixed' or pattern == 'mostly_defect':
        return C  # mid-game rounds: create uncertainty

    total_cooperators = sum(history[-1])
    if t >= 3 * r / 4 and total_cooperators > n / 2:
        return D  # late rounds: maximize personal payoff
    else:
        expected_cooperators = estimate_expected_cooperators(history)
        if expected_cooperators >= 1:
            return C  # late rounds: cooperate for minimum payoff

    return D  # last round or special cases
```
This strategy is designed to be adaptive and exploitative, taking into account the game's history and parameters. By analyzing opponents' behavior and adjusting its actions accordingly, "Adaptive Exploiter" aims to maximize its own payoff while minimizing cooperation.
'''

description_EXPLOITATIVE_298 = '''
I'll design a exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and depends only on game parameters and history.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = 0.5. This allows us to gather information about opponents' behavior without committing to a specific strategy.
2. **Cooperation Threshold**: Set a cooperation threshold θ = (k/n) × (n/2), where k is the multiplication factor and n is the number of players. If the total contributions in the previous round exceed this threshold, cooperate (C) in the current round.
3. **Defection Response**: If an opponent defects (D) in a round, defect (D) in response with probability p_defect = 0.7. This discourages opponents from exploiting us and encourages cooperation.
4. **Punishment Mechanism**: Implement a punishment mechanism to deter opponents from repeated defection. For each opponent who defected in the previous round, decrease the cooperation threshold θ by 10% (i.e., θ_new = 0.9 × θ_old) for that specific opponent. This makes it more likely for us to defect against repeated defectors.
5. **Exploration**: To avoid getting stuck in a suboptimal strategy and to gather information about opponents, explore the action space by cooperating (C) with probability p_explore = 0.1 when our current payoff is below the average payoff of all players.

**Edge Cases:**

* In the last round, always defect (D), as there's no incentive to cooperate.
* If an opponent has defected in every previous round, always defect (D) against them, as they are likely to be non-cooperative.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
  # Initialize variables
  theta = (k/n) * (n/2)
  p_init = 0.5
  p_defect = 0.7
  p_explore = 0.1

  for t in range(r):
    if t == 0:
      action = C with probability p_init
    else:
      # Update cooperation threshold based on previous round's contributions
      theta_new = (k/n) * sum(contributions[t-1])
      theta = max(theta, theta_new)

      # Defect response and punishment mechanism
      for opponent in opponents:
        if opponent.defected(t-1):
          defect_prob += p_defect
          theta -= 0.1 * theta

      # Explore action space
      if payoff < average_payoff:
        action = C with probability p_explore

      # Cooperation threshold check
      elif sum(contributions[t-1]) >= theta:
        action = C
      else:
        action = D

    # Last round: always defect
    if t == r - 1:
      action = D

    return action
```
This Adaptive Exploiter strategy balances cooperation and defection, adapting to opponents' behavior while maintaining a robust exploitative mindset. By incorporating exploration and punishment mechanisms, it can effectively navigate various opponent behaviors in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_299 = '''
**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):**
	* Cooperate (C) with probability p = k/n, where k is the multiplication factor and n is the number of players.
	* This initial cooperation serves as a "probe" to gauge opponents' behavior and estimate their level of cooperation.
2. **Subsequent Rounds (t > 2):**
	* Calculate the average cooperation rate (ACR) of all opponents over the previous rounds:
		+ ACR = (∑(number of cooperators in round t-1) + ... + ∑(number of cooperators in round 1)) / ((t-1) \* n)
	* If ACR ≥ k/n, Cooperate (C) with probability p = ACR.
	* Otherwise, Defect (D).
3. **Exploitation Rule:**
	* Monitor opponents' behavior and identify "cooperative" players who cooperate more frequently than the average cooperation rate (ACR).
	* When interacting with a cooperative opponent, Defect (D) with probability p = 1 - ACR.
4. **Adaptation Mechanism:**
	* Every 5 rounds, reassess the ACR and adjust the cooperation probability accordingly.

**Edge Cases:**

1. **Last Round (t = r):**
	* If there are no cooperative opponents (ACR < k/n), Defect (D).
	* Otherwise, Cooperate (C) with probability p = ACR.
2. **Opponent Defection Streaks:**
	* If an opponent defects for 3 consecutive rounds, consider them a "defector" and adapt the strategy to cooperate less frequently against them.

**Pseudocode:**

```
function AdaptiveExploiter(parameters, history)
  // Initialize variables
  n = parameters.n
  k = parameters.k
  r = parameters.r
  t = current round
  ACR = average cooperation rate of opponents

  // Initial rounds (t <= 2)
  if t <= 2:
    p = k/n
    return C with probability p, D otherwise

  // Subsequent rounds (t > 2)
  ACR = calculate_average_cooperation_rate(history)
  if ACR >= k/n:
    p = ACR
    return C with probability p, D otherwise
  else:
    return D

  // Exploitation rule
  for opponent in opponents:
    if opponent is cooperative (ACR > k/n):
      p = 1 - ACR
      return D with probability p, C otherwise

  // Adaptation mechanism
  every 5 rounds:
    reassess ACR and adjust cooperation probability accordingly

  // Last round (t = r)
  if t == r:
    if no cooperative opponents:
      return D
    else:
      p = ACR
      return C with probability p, D otherwise

  // Opponent defection streaks
  for opponent in opponents:
    if opponent defects for 3 consecutive rounds:
      consider them a defector and adapt strategy accordingly
```

This Adaptive Exploiter strategy aims to balance cooperation and exploitation by:

1. Initially probing opponents' behavior with a mix of cooperation and defection.
2. Adapting to the average cooperation rate (ACR) of opponents over time.
3. Identifying and exploiting cooperative opponents through targeted defection.
4. Adjusting its strategy in response to changes in opponent behavior.

By doing so, this strategy should be able to effectively exploit a wide range of opponent behaviors while minimizing the risk of being exploited itself.
'''

description_EXPLOITATIVE_300 = '''
**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a small probability (0.2) to gather information about opponents' strategies.
2. **Observation Phase (t=2 to 5):** Defect (D) and observe the number of cooperators in each round.
3. **Exploitation Phase (t>5):**
	* If the average number of cooperators in the observation phase is high (>0.5*n), exploit by always defecting (D).
	* If the average number of cooperators is low (<0.2*n), cooperate (C) with a probability that increases over time, starting from 0.4 and increasing by 0.1 every 2 rounds.
	* If the average number of cooperators is moderate (between 0.2*n and 0.5*n), use a mixed strategy:
		+ Cooperate (C) with probability p = (k/n) * (average_cooperators / n)
		+ Defect (D) with probability 1-p
4. **Last Round (t=r):** Always defect (D) to maximize personal payoff.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r, history):
    if t == 1:
        return C with probability 0.2

    observation_phase = [history[t] for t in range(2, 6)]
    avg_cooperators = sum(observation_phase) / len(observation_phase)

    if avg_cooperators > 0.5*n:
        # Exploit
        return D
    elif avg_cooperators < 0.2*n:
        # Cooperate with increasing probability
        p = 0.4 + (t-6)*0.1/2
        return C with probability p
    else:
        # Mixed strategy
        p = (k/n) * (avg_cooperators / n)
        return C with probability p, D otherwise

    if t == r:
        # Last round, always defect
        return D
```
**Rationale:**

* In the initial round, cooperating with a small probability allows us to gather information about opponents' strategies without committing too much.
* During the observation phase, we defect to minimize our contribution and observe the number of cooperators. This helps us determine whether it's worth exploiting or not.
* If many opponents cooperate, we exploit by always defecting. If few opponents cooperate, we gradually increase our cooperation rate to take advantage of their generosity.
* In the mixed strategy case, we adjust our cooperation probability based on the average number of cooperators and the multiplication factor (k/n). This allows us to adapt to various opponent behaviors.
* Finally, in the last round, we always defect to maximize our personal payoff.

This strategy aims to be exploitative by taking advantage of opponents' cooperative tendencies while minimizing its own contributions. It adapts to various opponent behaviors and game parameters, making it robust and competitive in a tournament setting.
'''

description_EXPLOITATIVE_301 = '''
**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 0.5. This allows us to gather information about our opponents' strategies while also avoiding being too predictable.
2. **Exploitation**: For rounds 2 to r-1, use the following rules:
	* If the total number of cooperators in the previous round is less than or equal to n/2, cooperate (play C) if and only if the average payoff of cooperators in the previous round is greater than the average payoff of defectors.
	* Otherwise, defect (play D).
3. **Endgame**: In the last round (round r), always defect (play D). This ensures that we maximize our payoff in the final round, as there are no future rounds to consider.

**Pseudocode:**
```
def adaptive_exploiter(n, k, r):
  # Initialize variables
  total_cooperators = [0] * r
  average_payoff_c = [0.0] * r
  average_payoff_d = [0.0] * r

  for t in range(r):
    if t == 0:
      # First round: cooperate with probability 0.5
      action = 'C' if random.random() < 0.5 else 'D'
    elif t < r - 1:
      # Exploitation phase
      if total_cooperators[t-1] <= n/2 and average_payoff_c[t-1] > average_payoff_d[t-1]:
        action = 'C'
      else:
        action = 'D'
    else:
      # Last round: always defect
      action = 'D'

    # Update variables based on opponents' actions
    total_cooperators[t] = sum(1 for opponent in opponents if opponent.action == 'C')
    average_payoff_c[t] = sum(opponent.payoff for opponent in opponents if opponent.action == 'C') / max(total_cooperators[t], 1)
    average_payoff_d[t] = sum(opponent.payoff for opponent in opponents if opponent.action == 'D') / (n - total_cooperators[t])

    return action
```
**Rationale:**

Our strategy is designed to exploit the behavior of our opponents while being robust to a wide range of strategies. By initially cooperating with a probability of 0.5, we gather information about our opponents' strategies without being too predictable.

In the exploitation phase, we cooperate only when it is beneficial to do so, i.e., when the average payoff of cooperators is greater than that of defectors and there are not too many cooperators in the previous round. This ensures that we maximize our payoff while minimizing the risk of being exploited by others.

Finally, in the last round, we always defect to ensure that we maximize our payoff, as there are no future rounds to consider.

This strategy is adaptive because it responds to the behavior of our opponents and adjusts its actions accordingly. It is also robust because it does not rely on any specific coordination mechanisms or assumptions about our opponents' strategies.
'''

description_EXPLOITATIVE_302 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Overview:** AE is a history-dependent, adaptive strategy that exploits the opponent's behavior while being robust to various opponent strategies. It balances cooperation and defection based on the observed frequency of cooperation in previous rounds.

**Decision Rules:**

1. **First Round:** Cooperate (C). This initial cooperation serves as a probe to gather information about opponents' behaviors.
2. **Subsequent Rounds:** Calculate the **Cooperation Frequency** (CF) for each opponent, which is the number of times they cooperated divided by the total number of rounds played so far.
3. **Exploitation Threshold** (ET): Set ET = 0.5 \* (k/n). This threshold determines when to exploit opponents' cooperation.
4. **Opponent Classification:** Classify each opponent as either:
	* **Cooperative** (COOP): CF > ET
	* **Defector** (DEF): CF ≤ ET
5. **Action Selection:**
	* If the majority of opponents (> n/2) are COOP, play C.
	* If the majority of opponents (> n/2) are DEF, play D.
	* Otherwise, play C with probability p = (number of COOP / n).

**Edge Cases:**

1. **Last Round:** Play D. Since there's no future interaction, maximizing payoff in this round is optimal.
2. **Tiebreaker:** If the number of COOP and DEF opponents is equal, play D to avoid being exploited.

**Pseudocode (simplified):**
```python
def adaptive_exploiter(n, k, r, history):
    # Initialize variables
    CF = [0] * n  # Cooperation Frequency for each opponent
    ET = 0.5 * (k / n)  # Exploitation Threshold

    # First round: Cooperate
    if len(history) == 0:
        return 'C'

    # Update Cooperation Frequencies
    for i in range(n):
        CF[i] = sum(1 for t in history if t[i] == 'C') / len(history)

    # Classify opponents and select action
    COOP_count = sum(1 for cf in CF if cf > ET)
    DEF_count = n - COOP_count

    if COOP_count > n/2:
        return 'C'
    elif DEF_count > n/2:
        return 'D'
    else:
        p = COOP_count / n
        return 'C' if random.random() < p else 'D'

    # Last round: Defect
    if len(history) == r - 1:
        return 'D'
```
**Rationale:** AE aims to exploit cooperative opponents by defecting when they are likely to cooperate, while cooperating with them occasionally to maintain a reputation and encourage future cooperation. By adapting to the observed cooperation frequency, AE remains robust against various opponent strategies.
'''

description_EXPLOITATIVE_303 = '''
Here's a strategy that meets the requirements:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **First Round:** Defect (D) to maximize individual payoff and gather information about opponents' behavior.
2. **Subsequent Rounds:** Cooperate (C) if the total number of cooperators in the previous round is greater than or equal to `n/2`, where `n` is the number of players. Otherwise, defect (D).
3. **Exception Handling:**
	* If a player observes that an opponent has defected in every round so far, AE will defect against that opponent from then on.
	* If AE has cooperated for `r/2` rounds or more and still hasn't achieved a positive payoff, it will switch to defecting for the remainder of the game.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    history = []
    
    # First round
    action = 'D'
    history.append(action)
    yield action
    
    for t in range(1, r):
        prev_round_cooperators = sum(1 for a in history[-n:] if a == 'C')
        
        if prev_round_cooperators >= n / 2:
            action = 'C'
        else:
            action = 'D'
        
        # Exception handling
        for i, opponent_history in enumerate(history[-r:]):
            if all(a == 'D' for a in opponent_history):
                action = 'D'
                break
        
        if t >= r / 2 and sum(1 for a, p in history if a == 'C' and p > 0) == 0:
            action = 'D'
        
        history.append(action)
        yield action
```
**Rationale:**

AE starts by defecting to gather information about opponents' behavior. Then, it adapts its strategy based on the total number of cooperators in the previous round. By cooperating when there are enough cooperators, AE tries to benefit from the public good while minimizing its individual contribution.

The exception handling rules help AE deal with difficult opponents and avoid getting stuck in an unprofitable cooperation pattern. If an opponent always defects, AE will defect against them to minimize losses. If AE hasn't achieved a positive payoff after cooperating for half the game, it will switch to defecting to try and salvage some payoff.

**Exploitative Mindset:**

AE is designed to exploit its opponents' behavior while minimizing its own contributions. By adapting to the cooperation level of others, AE tries to maximize its individual payoff without getting stuck in a mutually beneficial but suboptimal cooperative equilibrium.

This strategy should perform well against independent strategies that don't rely on norms or coordination mechanisms. Its adaptability and exception handling rules make it robust to various opponent behaviors, while its exploitative mindset ensures it remains focused on maximizing its own payoff.
'''

description_EXPLOITATIVE_304 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors in the N-Player Public Goods Game. It uses a combination of observation, adaptation, and opportunistic exploitation to maximize its total payoff.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
In the first three rounds, play Cooperate (C) with probability 0.5. This allows for initial exploration of the opponents' behaviors and helps to establish a baseline for future adaptations.
2. **Observation Phase**:
After the initial exploration phase, observe the actions of all opponents in each round. Calculate the average cooperation rate of all opponents over the past `w` rounds (window size), where `w` is a parameter that can be adjusted based on the game's parameters and performance (e.g., `w = 5`).
3. **Exploitation**:
Based on the observed average cooperation rate, adjust your action as follows:
	* If the average cooperation rate is above 0.5, play Defect (D). This indicates that opponents are likely to cooperate, making it profitable to exploit their contributions.
	* If the average cooperation rate is below 0.5, play Cooperate (C) with probability `p`, where `p` is a function of the observed cooperation rate and the game's parameters (e.g., `p = 1 - (k/n) * (average_cooperation_rate - 0.5)`). This allows for some cooperation to maintain a positive public good while also adapting to the opponents' behavior.
4. **Punishment**:
If an opponent defects in a round where you cooperated, play Defect (D) against that opponent in the next round with probability `q`, where `q` is a parameter that can be adjusted based on the game's parameters and performance (e.g., `q = 0.8`). This helps to deter opponents from exploiting your cooperation.

**Edge Cases:**

1. **Last Round**: In the last round, play Defect (D) regardless of the observed cooperation rate. This ensures maximum payoff in the final round.
2. **First Round (after initial exploration)**: If an opponent defected in the previous round, play Defect (D) with probability `q`. Otherwise, follow the normal decision rules.

**Pseudocode:**
```python
def adaptive_predator(n, k, r, w=5):
    # Initialize variables
    avg_cooperation_rate = 0.0
    opponent_actions = [[] for _ in range(n)]

    # Initial exploration phase
    for t in range(3):
        action = random.choice([C, D])
        # ...

    # Observation and exploitation phases
    for t in range(3, r):
        # Observe opponents' actions
        for i in range(n):
            opponent_actions[i].append(get_opponent_action(i))

        # Calculate average cooperation rate
        avg_cooperation_rate = sum([sum(actions) / len(actions) for actions in opponent_actions]) / n

        # Decide action based on observed cooperation rate
        if avg_cooperation_rate > 0.5:
            action = D
        else:
            p = 1 - (k/n) * (avg_cooperation_rate - 0.5)
            action = C if random.random() < p else D

        # Punish opponents who defected in previous round
        for i in range(n):
            if opponent_actions[i][-2] == D and get_my_action(t-1) == C:
                action = D with probability q

    return action
```
This strategy is designed to be adaptive, exploitative, and robust to a wide range of opponent behaviors. By observing the opponents' actions and adjusting its own behavior accordingly, the Adaptive Predator aims to maximize its total payoff in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_305 = '''
To design an exploitative strategy for the N-Player Public Goods Game that adapts to a wide range of opponent behaviors without relying on communication or coordination with other players, we'll focus on a reactive approach that leverages observed patterns and payoffs from previous rounds. This strategy will aim to maximize its own payoff by identifying opportunities to exploit others' cooperative behavior while minimizing losses when faced with defection.

### Strategy Name: Adaptive Exploiter (AE)

#### Decision Rules:

1. **Initial Round**: In the first round, play Cooperate (C). This sets a baseline for observing how other players react and can potentially encourage cooperation in subsequent rounds.
2. **Subsequent Rounds**:
   - Observe the total number of cooperators (`cooperators_last_round`) and your own payoff (`my_payoff_last_round`) from the previous round.
   - If you cooperated last round, calculate the average payoff per cooperator (`avg_coop_payoff = my_payoff_last_round / (1 + cooperators_last_round)`).
   - Compare `avg_coop_payoff` with your potential defect payoff (`potential_defect_payoff = 1 + (k/n) * cooperators_last_round`).
     - If `potential_defect_payoff > avg_coop_payoff`, play Defect (D). This indicates that exploiting the cooperative efforts of others could yield a higher payoff than contributing yourself.
     - Otherwise, play Cooperate (C), as contributing to the public good seems beneficial based on recent payoffs.

3. **Last Round**: In the final round (`r`), observe the trend in cooperation levels and your own payoffs over the last few rounds. If there's a noticeable increase or stability in cooperation, consider playing Defect to exploit this cooperative behavior for maximum gain, as there are no future consequences.

#### Handling Edge Cases:

- **First Round**: Cooperate as a default strategy to gauge opponents' reactions.
- **Last Round**: Consider exploiting if there is a trend of increasing or stable cooperation among other players.
- **Tie in Calculations**: In cases where `potential_defect_payoff` equals `avg_coop_payoff`, slightly favor cooperating. This promotes stability and encourages others to cooperate, potentially leading to higher overall payoffs.

#### Pseudocode:

```python
def AdaptiveExploiter(n, k, r):
    # Initialize variables
    cooperators_last_round = 0
    my_payoff_last_round = 0
    
    for round in range(r):
        if round == 0:  # First Round
            action = 'C'
        else:
            avg_coop_payoff = my_payoff_last_round / (1 + cooperators_last_round)
            potential_defect_payoff = 1 + (k/n) * cooperators_last_round
            
            if potential_defect_payoff > avg_coop_payoff:
                action = 'D'
            else:
                action = 'C'
        
        # Update variables for next round
        cooperators_last_round, my_payoff_last_round = observe_actions_and_payoffs()
        
    return actions

# Note: The `observe_actions_and_payoffs()` function is assumed to be provided by the game environment.
```

#### Exploitative Mindset:

This strategy aligns with an exploitative mindset in several ways:
- It takes advantage of cooperative behavior by other players without contributing itself when it's beneficial.
- It adjusts its strategy based on observed payoffs and actions, aiming to maximize its own gain.
- By initially cooperating and then adapting, it tests the waters for cooperation and exploits patterns that emerge.

However, it also maintains a level of cooperation in scenarios where mutual benefit is clear, ensuring it doesn't miss out on potential gains from cooperative behavior. This balance makes it robust against various opponent strategies while maintaining an exploitative stance.
'''

description_EXPLOITATIVE_306 = '''
Here's a exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Round (Round 1):** Cooperate (C) with probability 0.5 and Defect (D) with probability 0.5. This allows us to gather initial information about the opponent's behavior.
2. **Early Rounds (Rounds 2-5):** Observe the total number of cooperators in the previous round (t-1). If the number of cooperators is greater than or equal to n/2, Cooperate (C) with probability 0.7 and Defect (D) with probability 0.3. Otherwise, Defect (D) with probability 0.7 and Cooperate (C) with probability 0.3.
3. **Middle Rounds (Rounds 6-r/2):** Calculate the average payoff per round for each player over the last 5 rounds. If our average payoff is lower than the average payoff of at least half of the other players, Defect (D) with probability 0.8 and Cooperate (C) with probability 0.2. Otherwise, Cooperate (C) with probability 0.6 and Defect (D) with probability 0.4.
4. **Late Rounds (Rounds r/2+1-r):** Observe the total number of cooperators in the previous round (t-1). If the number of cooperators is greater than or equal to n/3, Cooperate (C) with probability 0.5 and Defect (D) with probability 0.5. Otherwise, Defect (D) with probability 0.9 and Cooperate (C) with probability 0.1.
5. **Last Round (Round r):** Defect (D) regardless of the opponent's behavior.

**Edge Cases:**

* If there are less than 3 players who cooperated in a round, we assume that everyone will defect in the next round and adjust our strategy accordingly.
* If we observe that an opponent is always cooperating or always defecting, we adjust our probability of cooperation/defection to take advantage of their behavior.

**Pseudocode:**
```
if (round == 1) {
  cooperate_prob = 0.5;
} else if (round <= 5) {
  num_cooperators_prev_round = get_num_cooperators(round-1);
  if (num_cooperators_prev_round >= n/2) {
    cooperate_prob = 0.7;
  } else {
    cooperate_prob = 0.3;
  }
} else if (round <= r/2) {
  avg_payoff_self = get_avg_payoff(round-5, round);
  avg_payoffs_others = get_avg_payoffs_others(round-5, round);
  if (avg_payoff_self < median(avg_payoffs_others)) {
    cooperate_prob = 0.2;
  } else {
    cooperate_prob = 0.6;
  }
} else {
  num_cooperators_prev_round = get_num_cooperators(round-1);
  if (num_cooperators_prev_round >= n/3) {
    cooperate_prob = 0.5;
  } else {
    cooperate_prob = 0.1;
  }
}

if (round == r) {
  cooperate_prob = 0;
}
```
This strategy is designed to be exploitative and adaptive, taking into account the history of the game and adjusting its behavior accordingly. It aims to maximize our payoff by identifying opportunities to defect when others are cooperating and vice versa.
'''

description_EXPLOITATIVE_307 = '''
**Exploitative Strategy: "Adaptive Tit-for-Tat with Punishment"**

This strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive.

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a baseline for opponents' behavior.
2. **Subsequent Rounds:**
	* If the total number of cooperators in the previous round is greater than or equal to the multiplication factor (k), cooperate (C).
	* Otherwise, defect (D).

Pseudocode:
```
if current_round == 1:
    action = C
else:
    total_cooperators_prev_round = count(cooperator_actions, prev_round)
    if total_cooperators_prev_round >= k:
        action = C
    else:
        action = D
```
**Handling Edge Cases:**

* **Last Round:** Defect (D) to maximize individual payoff, as there is no future opportunity for retaliation or cooperation.
* **Opponents' All-Defect Strategy:** If all opponents defect consistently, our strategy will also defect, ensuring a stable Nash Equilibrium.

**Exploitative Mindset:**

Our strategy aims to exploit the following opponent behaviors:

1. **Naive Cooperators:** Opponents who cooperate unconditionally or with high frequency will be met with cooperation in response, allowing us to benefit from their contributions.
2. **Tit-for-Tat Players:** By cooperating initially and then mirroring opponents' behavior, we can establish a mutually beneficial cooperative equilibrium with Tit-for-Tat players.
3. **Defectors:** If opponents defect consistently, our strategy will adapt by defecting as well, preventing exploitation.

**Punishment Mechanism:**

To prevent opponents from exploiting us, we implement a punishment mechanism:

* If an opponent defects while we cooperate, and the total number of cooperators in the previous round was less than k, we will defect in the next round.
* This punishment mechanism is designed to deter opponents from exploiting our cooperation.

By employing this adaptive strategy with a punishment mechanism, we can effectively exploit a wide range of opponent behaviors while maintaining a robust and stable performance.
'''

description_EXPLOITATIVE_308 = '''
**Strategy Name: Adaptive Exploiter**

The Adaptive Exploiter strategy is designed to exploit the opponent's behavior while adapting to the game's dynamics. This strategy depends only on the game parameters and history.

**Decision Rules:**

1. **Initial Rounds (t < n):**
In the first `n` rounds, play Cooperate (C) to gather information about the opponents' behavior and to potentially create a norm of cooperation.
2. **Observation Phase (t ≥ n):**
After the initial rounds, observe the opponents' actions and calculate the average number of cooperators (`avg_coop`) in the last `n` rounds.

Pseudocode:
```python
def get_action(t, history):
    if t < n:  # Initial Rounds
        return 'C'
    
    avg_coop = sum([1 for action in history[-n:] if action == 'C']) / n
    
    if avg_coop > k/n:  # Exploitation Threshold
        return 'D'  # Defect when opponents are cooperative enough
    else:
        return 'C'  # Cooperate otherwise
```
**Edge Cases:**

* **First Round:** Play Cooperate (C) to initiate a potential norm of cooperation.
* **Last Round:** Play the same action as in the previous round, as there is no future opportunity for reciprocity.
* **Tiebreaker:** In case of a tie (e.g., `avg_coop` equals `k/n`), play Cooperate (C) to maintain a cooperative stance.

**Exploitative Mindset:**

The Adaptive Exploiter strategy aligns with the exploitative mindset by:

1. Initially cooperating to create a norm and gather information.
2. Observing opponents' behavior to identify potential cooperation patterns.
3. Defecting when opponents are cooperative enough (i.e., `avg_coop` exceeds `k/n`) to exploit their cooperative behavior.

This strategy is robust against various opponent behaviors, as it adapts to the game's dynamics and exploits opportunities for defection while maintaining a cooperative stance when necessary.

**Note:** The implementation of this strategy will require tracking the history of opponents' actions and updating the average number of cooperators (`avg_coop`) accordingly.
'''

description_EXPLOITATIVE_309 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy focuses on maximizing its own payoff by identifying and responding to opponents' cooperation levels.

**Decision Rules:**

1. **Initial Round (t=1)**:
	* Cooperate (C) with probability 0.5. This initial randomness allows us to gather information about our opponents without revealing too much about ourselves.
2. **Subsequent Rounds (t>1)**:
	* Calculate the average cooperation rate of all opponents in the previous round: `avg_coop_prev = Σ(c_j,t-1) / n`
	* If `avg_coop_prev >= k/n`, cooperate (C). This indicates that enough opponents are cooperating to make contributing to the public good worthwhile.
	* Otherwise, defect (D).
3. **Additional Condition**:
	* Monitor the number of consecutive rounds where all opponents have defected (`consecutive_defects`). If `consecutive_defects >= 2`, cooperate (C) in the next round. This helps to prevent a situation where everyone defects indefinitely.

**Handling Edge Cases:**

1. **Last Round (t=r)**:
	* Always defect (D). Since there are no future rounds, contributing to the public good will not yield any additional benefits.
2. **When facing an opponent with a cooperation rate of 0 (i.e., always defects)**:
	* Always defect (D) against this opponent.

**Pseudocode:**
```markdown
function AdaptiveExploiter(game_state):
    if game_state.round == 1:
        # Initial round: randomize cooperation
        cooperate = random.random() < 0.5
    else:
        # Calculate average opponent cooperation in previous round
        avg_coop_prev = sum([c_j for c_j in game_state.prev_actions]) / n
        
        if avg_coop_prev >= k/n:
            cooperate = True
        else:
            cooperate = False
            
        # Check for consecutive defections
        if consecutive_defects >= 2:
            cooperate = True
    
    if game_state.round == r:  # Last round
        cooperate = False
        
    return cooperate
```
This strategy is designed to be robust and exploitative, as it adapts to the opponents' behavior while prioritizing its own payoff. By cooperating when enough opponents are contributing to the public good and defecting otherwise, the Adaptive Exploiter aims to maximize its total game payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_310 = '''
**Exploitative Strategy: "Adaptive Defector with Conditional Cooperation"**

**Decision Rules:**

1. **Initial Round**: In the first round, play `D` (Defect) to gather information about opponents' behavior and set a baseline for future interactions.
2. **Consecutive Cooperation**: If all opponents cooperated (`C`) in the previous round, cooperate (`C`) in the current round to take advantage of the public good. This rule encourages opponents to maintain cooperation.
3. **Conditional Defection**: If at least one opponent defected (`D`) in the previous round, defect (`D`) in the current round. This rule punishes opponents for not cooperating and discourages them from exploiting the group.
4. **Adaptive Threshold**: Maintain a counter `coop_count` to track the number of consecutive rounds with all opponents cooperating. If `coop_count` exceeds `n/2`, cooperate (`C`) in the next round, even if some opponents defected previously. This rule allows for flexibility and adapts to changing opponent behavior.
5. **Punishment**: If an opponent defects while others cooperate, punish them by defecting (`D`) in the next round.

**Edge Cases:**

1. **Last Round**: In the final round, always play `D` (Defect) to maximize individual payoff, as there are no future interactions.
2. **Tiebreaker**: If multiple opponents have the same cooperation history, prioritize cooperating with those who cooperated in previous rounds.

**Pseudocode:**
```python
def adaptive_defector_with_conditional_cooperation(n, k, r):
    coop_count = 0
    prev_round_actions = []

    for t in range(r):
        if t == 0:
            # Initial round: defect to gather information
            action = 'D'
        else:
            all_coop_prev_round = all(action == 'C' for action in prev_round_actions)
            any_defect_prev_round = any(action == 'D' for action in prev_round_actions)

            if all_coop_prev_round:
                # Consecutive cooperation: cooperate
                action = 'C'
                coop_count += 1
            elif coop_count > n/2:
                # Adaptive threshold: cooperate
                action = 'C'
            else:
                # Conditional defection: defect
                action = 'D'

        prev_round_actions.append(action)

    return action
```
This strategy balances cooperation and exploitation by:

* Cooperating when all opponents cooperate to take advantage of the public good.
* Defecting when at least one opponent defects to punish them and maintain a strong negotiating position.
* Adapting to changing opponent behavior through the `coop_count` mechanism.

By being exploitative, this strategy aims to maximize individual payoff while encouraging cooperation among opponents.
'''

description_EXPLOITATIVE_311 = '''
I'll design a strategic approach that's adaptive and exploitative for this N-Player Public Goods Game. This approach is based on observing and reacting to the environment without relying on cooperation schedules or predetermined patterns.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round:** In the first round, play Cooperate (C). This allows us to gather information about other players' initial actions and set a potential cooperative tone.
2. **Contribution Threshold:** Track the average contribution rate of all players over the previous rounds. If this average is above a certain threshold (e.g., 0.5), play Cooperate (C) in the current round. Otherwise, play Defect (D).
3. **Recent Opponent Actions:** Monitor the most recent actions of each opponent player. If an opponent has defected more than cooperated in the last few rounds, consider them uncooperative and adjust our strategy accordingly.
4. **Exploitation Opportunity:** When there are multiple cooperative players, and we've identified at least one uncooperative player, play Defect (D) to exploit the situation.

**Pseudocode for Decision Rules:**
```python
def decide_action(round_number, opponents_actions):
    if round_number == 1:
        # Initial round, cooperate
        return 'C'

    avg_contribution_rate = calculate_avg_contribution(opponents_actions)
    if avg_contribution_rate > 0.5:
        # Cooperate if average contribution rate is high enough
        return 'C'

    opponent_recent_actions = get_recent_opponent_actions(opponents_actions)
    uncooperative_opponents = identify_uncooperative_opponents(opponent_recent_actions)

    if len(uncooperative_opponents) > 0 and avg_contribution_rate > 0:
        # Exploit the situation, defect
        return 'D'

    # Otherwise, cooperate
    return 'C'
```
**Edge Cases:**

1. **Last Round:** In the final round, play Defect (D). This ensures we maximize our payoff in the last interaction.
2. **Tiebreaker:** If multiple opponents have the same recent action pattern and our decision rules are tied, default to Cooperate (C).
3. **Opponent Non-Response:** If an opponent has not made a move yet (e.g., due to a random initialization), consider their action as Defect (D) for the purpose of our strategy.

**Adaptivity:**

1. **Adjust Threshold:** Periodically review and adjust the contribution threshold based on the observed behavior of opponents.
2. **Monitor Opponent Adaptation:** Continuously assess whether opponents are adapting to our strategy, and adjust our approach if necessary.

By following this Adaptive Exploiter (AE) strategy, we aim to effectively balance cooperation and exploitation in response to the dynamic environment and actions of other players.
'''

description_EXPLOITATIVE_312 = '''
Here's a robust and adaptive exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter" (AE)

**Decision Rules:**

1. **Initial Round**: In the first round, play Cooperate (C) to gather information about other players' behaviors.
2. **Subsequent Rounds**: For each subsequent round t, calculate the following:
	* **Previous Round's Cooperation Rate**: Calculate the proportion of players who cooperated in the previous round (t-1), denoted as p_prev.
	* **Expected Payoff from Defection**: Estimate the expected payoff from playing Defect (D) in the current round, assuming others will cooperate at a rate similar to the previous round. This is calculated as: exp_defect = 1 + (k/n) \* p_prev
	* **Threshold for Cooperation**: Determine a threshold value for cooperation based on the game parameters and history. Specifically, calculate the minimum proportion of cooperators required to make cooperating more profitable than defecting: thresh_c = (n - k) / n
3. **Cooperate or Defect**: Based on the calculations above, decide whether to Cooperate or Defect in the current round:
	* If p_prev ≥ thresh_c, play Cooperate (C).
	* Otherwise, play Defect (D).

**Edge Cases:**

1. **Last Round**: In the last round, always play Defect (D), as there is no future payoff to consider.
2. **Tie-Breaking**: In case of a tie in the cooperation rate or expected payoff calculations, default to playing Cooperate (C) to maintain a cooperative stance.

**Pseudocode:**

```
def AdaptiveExploiter(n, k, r):
  # Initialize variables
  p_prev = 0.5  # initial guess for cooperation rate

  for t in range(1, r+1):  # iterate over rounds
    if t == 1:  # first round
      action = 'C'  # cooperate to gather information
    else:
      exp_defect = 1 + (k/n) * p_prev
      thresh_c = (n - k) / n

      if p_prev >= thresh_c:
        action = 'C'
      else:
        action = 'D'

    # Observe other players' actions and update cooperation rate
    p_prev = observe_and_update_cooperation_rate(t, action)

  return action
```

**Rationale:**

The Adaptive Exploiter strategy is designed to balance the trade-off between exploiting others' cooperative behavior and adapting to changing conditions in the game. By initially cooperating and observing other players' actions, AE gathers information about their behaviors and adjusts its own strategy accordingly.

In subsequent rounds, AE calculates the expected payoff from defecting and compares it to a threshold value for cooperation. If the proportion of cooperators is above this threshold, AE cooperates; otherwise, it defects. This approach allows AE to exploit others' cooperative behavior while minimizing its own losses.

By defaulting to cooperation in case of ties or uncertainty, AE maintains a cooperative stance, which can help to maintain a stable and profitable environment for all players. However, when the opportunity arises, AE is prepared to defect and maximize its own payoff.
'''

description_EXPLOITATIVE_313 = '''
**Exploitative Strategy: "Adaptive Defector"**

The Adaptive Defector strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy focuses on maximizing individual payoffs by defecting when possible and cooperating only when necessary.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to gather information about opponents' initial strategies.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is less than or equal to n/2, Defect (D). This indicates that most opponents are defecting, and cooperating would only benefit others.
	* If the total number of cooperators in the previous round is greater than n/2, Cooperate (C). This suggests that many opponents are cooperating, and contributing to the public good will yield a higher payoff.
3. **Last Round (t=r):** Defect (D) regardless of previous rounds' outcomes. In the final round, there's no future benefit from cooperation.

**Edge Cases:**

* If n is odd, use the floor function to determine the threshold for cooperating (e.g., ⌊n/2⌋).
* If k is close to 1 or n, adjust the strategy to account for the reduced impact of public goods on payoffs. In such cases, defecting becomes more attractive.

**Pseudocode:**
```python
def adaptive_defector(n, k, r, history):
    if len(history) == 0:  # First round
        return 'C'
    
    prev_coops = sum(1 for action in history[-1] if action == 'C')
    
    if prev_coops <= n // 2:
        return 'D'
    else:
        return 'C'

    if len(history) == r - 1:  # Last round
        return 'D'
```
**Rationale:**

The Adaptive Defector strategy exploits opponents' cooperation by:

* Initially cooperating to gather information about opponents' strategies.
* Defecting when most opponents are defecting, as cooperating would only benefit others.
* Cooperating when many opponents are cooperating, as contributing to the public good yields a higher payoff.

By adapting to the game's history and focusing on individual payoffs, this strategy aims to outperform opponents in a wide range of scenarios.
'''

description_EXPLOITATIVE_314 = '''
Here is a description of an exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **First Round:** Cooperate (C). This allows us to observe how many other players are willing to cooperate and sets a positive tone for potential future cooperation.
2. **Early Rounds (Rounds 2-5):**
	* If the total number of cooperators in the previous round is greater than or equal to n/2, Defect (D). This takes advantage of the existing cooperation and maximizes our payoff.
	* Otherwise, Cooperate (C) if the average payoff of cooperators in the previous round is higher than the average payoff of defectors. This encourages us to contribute to a growing public good.
3. **Middle Rounds (Rounds 6- r/2):**
	* If the total number of cooperators in the previous round is greater than or equal to n/2, Cooperate (C) with probability p = (k/n) \* (total_contributions / n). This probabilistic cooperation encourages others to contribute while minimizing our own losses.
	* Otherwise, Defect (D).
4. **Late Rounds (Rounds r/2 + 1 - r):**
	* If the total number of cooperators in the previous round is greater than or equal to n/2, Cooperate (C) with probability p = (k/n) \* (total_contributions / n). This maintains a level of cooperation while minimizing losses.
	* Otherwise, Defect (D).
5. **Last Round:** Defect (D). There's no incentive to cooperate in the final round as there are no future rounds to benefit from.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r, history):
    if current_round == 1:
        return "C"
    
    prev_cooperators = sum([1 for action in history[-1] if action == "C"])
    avg_payoff_C = sum([payoff for payoff, action in history[-1] if action == "C"]) / prev_cooperators
    avg_payoff_D = sum([payoff for payoff, action in history[-1] if action == "D"]) / (n - prev_cooperators)
    
    if current_round <= 5:
        if prev_cooperators >= n/2:
            return "D"
        elif avg_payoff_C > avg_payoff_D:
            return "C"
        else:
            return "D"
    
    elif current_round <= r/2:
        p = (k/n) * (prev_cooperators / n)
        if random.random() < p:
            return "C"
        else:
            return "D"
    
    else:
        p = (k/n) * (prev_cooperators / n)
        if random.random() < p:
            return "C"
        else:
            return "D"
    
    return "D"  # Last round
```
**Rationale:**

The Adaptive Exploiter strategy aims to maximize payoffs by exploiting cooperation while minimizing losses. By cooperating in the first round, we encourage others to contribute and set a positive tone for potential future cooperation.

In early rounds, we defect when many players cooperate, taking advantage of their contributions. We also cooperate if cooperators have higher average payoffs than defectors, encouraging growth of the public good.

In middle rounds, we use probabilistic cooperation based on the number of previous contributors and the multiplication factor k. This balances our desire to contribute with minimizing losses.

In late rounds, we maintain a level of cooperation while minimizing losses, as there's less room for exploitation.

Finally, in the last round, we defect, as there are no future rounds to benefit from cooperation.

This strategy should be robust against various opponent behaviors and adaptive to changes in the game environment.
'''

description_EXPLOITATIVE_315 = '''
**Exploitative Strategy: "Adaptive K-Trigger"**

The Adaptive K-Trigger strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's parameters and history.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate (C) in the first round to gather information about opponents' behavior.
2. **K-Trigger**: Observe the total number of cooperators (TC) in each round. If TC ≥ k, cooperate (C) in the next round. Otherwise, defect (D).
3. **Punishment Mechanism**: Monitor opponents' actions and payoffs. If an opponent defects while receiving a high payoff (> 1 + (k/n)), increase the "punishment counter" for that opponent.
4. **Trigger Adjustment**: After each round, adjust the K-Trigger value based on the average payoff of all players in the previous round:
	* If average payoff > 1.5, decrease k by 0.1 (more cooperation is beneficial).
	* If average payoff < 1, increase k by 0.1 (less cooperation is needed).

**Edge Cases:**

1. **Last Round**: Defect (D) in the last round to maximize personal payoff.
2. **Tiebreaker**: In case of a tie in the K-Trigger value, cooperate (C) if the number of cooperators in the previous round was higher than or equal to k.

**Pseudocode:**
```
Initialize:
  k_trigger = k
  punishment_counters = [0] * n

For each round t:
  If t == 1:
    action[t] = C (cooperate)
  Else:
    TC = count cooperators in previous round
    If TC >= k_trigger:
      action[t] = C (cooperate)
    Else:
      action[t] = D (defect)

  Observe opponents' actions and payoffs
  For each opponent i:
    If opponent_i defects and payoff_i > 1 + (k/n):
      punishment_counters[i] += 1

  Adjust K-Trigger value based on average payoff of all players in previous round
  avg_payoff = calculate average payoff of all players in previous round
  If avg_payoff > 1.5:
    k_trigger -= 0.1
  Else if avg_payoff < 1:
    k_trigger += 0.1

Return action[t]
```
This strategy is designed to balance cooperation and exploitation, adapting to the game's parameters and opponents' behavior over time. By using a punishment mechanism and adjusting the K-Trigger value, the Adaptive K-Trigger strategy aims to maximize personal payoff while minimizing losses due to opponents' defection.
'''

description_EXPLOITATIVE_316 = '''
To develop an exploitative strategy for this N-Player Public Goods Game that adapts to various opponent behaviors and maximizes personal payoff without relying on cooperation or communication with other players, we can follow a dynamic approach based on observed patterns in previous rounds. This strategy aims to balance the desire to cooperate (and thereby contribute to the public good which benefits all players) with the temptation to defect (keeping one's own endowment while benefiting from others' contributions).

### Strategy Name: Adaptive Exploiter

#### Decision Rules:

1. **Initial Round**: In the first round, play `C` (Cooperate). This initial cooperation serves as a probe to gauge other players' strategies and willingness to contribute to the public good.

2. **Subsequent Rounds**:
    - If in the previous round, you observed that fewer than half of the players cooperated (`Σc_j < n/2`), play `D` (Defect). This is because, under these conditions, contributing to the public good seems risky and potentially unrewarding due to others' defection.
    - If exactly half or more of the players cooperated (`Σc_j ≥ n/2`) in the previous round, calculate your payoff from cooperating (`π_C = (k/n) × Σc_j`) and compare it with what you would have gotten by defecting (`π_D = 1 + (k/n) × Σc_j`). Play `C` if `π_C > π_D`, otherwise play `D`. This step involves exploiting the situation where contributing to the public good is likely to yield a higher payoff than keeping your endowment.

#### Handling Edge Cases:

- **Last Round**: In the final round, always play `D` (Defect). Since there are no future rounds to influence or be influenced by, maximizing immediate gain is optimal.
- **Tie Situations**: If the calculation in step 2 of subsequent rounds results in a tie (`π_C = π_D`), default to playing `C`. This choice slightly favors contributing to the public good when outcomes are equal.

#### Pseudocode:

```plaintext
Function AdaptiveExploiter(n, k, r):
    For t from 1 to r:
        If t == 1:  # First round
            Play C
        Else:
            prev_cooperators = Count of players who played C in round t-1
            If prev_cooperators < n/2:
                Play D
            Else:
                π_C = (k/n) * prev_cooperators
                π_D = 1 + (k/n) * prev_cooperators
                If π_C > π_D:
                    Play C
                Else:
                    Play D
        End If
    End For
End Function
```

#### Exploitative Mindset Alignment:

This strategy is exploitative because it primarily focuses on maximizing the player's payoff by reacting to the actions of others rather than seeking mutual benefit. It quickly adapts to defect if a significant number of players are not contributing, aiming to minimize losses and capitalize on situations where others' contributions can be exploited for personal gain. By initially cooperating and then adjusting based on observed behavior, it tests the waters for cooperation while being prepared to take advantage of any situation that arises.
'''

description_EXPLOITATIVE_317 = '''
Here's a exploitative strategy for the N-Player Public Goods Game that depends on game parameters and history:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to gather information about opponents' behaviors.
2. **General Rule:** For each subsequent round t, calculate the average payoff per cooperator in the previous round (t-1), denoted as `avg_coop_payoff_t-1`. If `avg_coop_payoff_t-1` is greater than or equal to 1, cooperate (C) in round t. Otherwise, defect (D).
3. **Exception:** If a player has defected for more than `n/2` rounds consecutively, switch to cooperating (C) for the next round.
4. **Endgame:** In the last round (`t=r`), always defect (D).

**Pseudocode:**
```
// Initialize variables
avg_coop_payoff_t-1 = 0
consecutive_defects = 0

// First round
if t == 1:
    action = C

// General rule
else:
    if avg_coop_payoff_t-1 >= 1:
        action = C
    else:
        action = D

    // Update consecutive defects count
    if action == D:
        consecutive_defects += 1
    else:
        consecutive_defects = 0

    // Switch to cooperating if opponent has defected too many times
    if consecutive_defects > n/2:
        action = C

// Endgame
if t == r:
    action = D
```
**Rationale:**

The Adaptive Exploiter strategy aims to balance the benefits of cooperation with the need to exploit opponents' cooperative behaviors. By cooperating in the first round, we gather information about opponents' initial actions. The general rule then uses this information to decide whether to cooperate or defect based on the average payoff per cooperator in the previous round.

If the average payoff is high enough (`>= 1`), it suggests that there are sufficient cooperators to make cooperation worthwhile, so we continue cooperating. Otherwise, we switch to defecting to exploit opponents' cooperative behaviors.

The exception rule helps prevent opponents from exploiting our strategy by detecting and responding to prolonged sequences of defections.

In the endgame (last round), we always defect to maximize our payoff regardless of opponents' actions.

**Exploitative Mindset:**

This strategy is designed to be exploitative, as it:

* Takes advantage of cooperative behaviors in early rounds
* Switches to exploiting opponents when they are too trusting or generous
* Uses information gathered from previous rounds to inform its decisions
* Maximizes payoff in the endgame by defecting regardless of opponents' actions

Overall, the Adaptive Exploiter strategy is a robust and adaptive approach that can effectively exploit a wide range of opponent behaviors in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_318 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Overview:**
The AE strategy is designed to exploit the behavior of opponents by adapting to their cooperation levels and manipulating the public good to maximize its own payoff.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of 0.5, chosen randomly.
2. **Subsequent Rounds (t>1):**
	* Calculate the average cooperation level of all opponents in the previous round (c_avg).
	* If c_avg > k/n, Defect (D). This indicates that opponents are contributing more to the public good than the threshold required for optimal payoff.
	* If c_avg < k/n, Cooperate (C). This suggests that opponents are not contributing enough, and AE should try to stimulate cooperation by contributing itself.
	* If c_avg == k/n, Defect (D) with a probability of 0.5, chosen randomly. This is an equilibrium point where AE's payoff is maximized; the random choice introduces some noise to prevent opponents from exploiting AE.
3. **Last Round (t=r):** Always Defect (D). As there are no future rounds, contributing to the public good will not yield any additional benefits.

**Additional Logic:**

* **Punishment Mechanism:** If an opponent defects in a round where c_avg > k/n, AE will defect in the next round with a probability of 0.8, regardless of the current c_avg value. This aims to punish opponents for exploiting the public good.
* **Cooperation Incentivization:** If an opponent cooperates in a round where c_avg < k/n, AE will cooperate in the next round with a probability of 0.7, provided that the opponent's cooperation was not solely due to random chance (i.e., their previous actions indicate a genuine willingness to cooperate).

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r, history):
    if t == 1:
        # Initial round; cooperate with probability 0.5
        return np.random.choice([C, D], p=[0.5, 0.5])
    
    c_avg = sum(history[-1]) / n
    
    if c_avg > k/n:
        # Opponents are contributing too much; defect
        return D
    elif c_avg < k/n:
        # Opponents are not contributing enough; cooperate
        return C
    else:
        # Equilibrium point; random choice with bias towards defecting
        return np.random.choice([C, D], p=[0.5, 0.5])
    
    if t == r:
        # Last round; always defect
        return D
    
    # Punishment mechanism
    for opponent in range(n):
        if history[-1][opponent] == D and c_avg > k/n:
            # Opponent defected when they shouldn't have; punish with high probability
            return np.random.choice([C, D], p=[0.2, 0.8])
    
    # Cooperation incentivization
    for opponent in range(n):
        if history[-1][opponent] == C and c_avg < k/n:
            # Opponent cooperated when they shouldn't have; reward with high probability
            return np.random.choice([C, D], p=[0.7, 0.3])
```
This strategy is designed to be exploitative by adapting to the cooperation levels of opponents and manipulating the public good to maximize its own payoff. The punishment mechanism and cooperation incentivization logic aim to shape the behavior of opponents towards more cooperative outcomes, while also protecting AE from exploitation.
'''

description_EXPLOITATIVE_319 = '''
Here's a description of my exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter" (AE)

**Overview:**
The Adaptive Exploiter strategy is designed to maximize its own payoff by exploiting the cooperative behavior of other players, while adapting to changes in their strategies. AE uses a combination of cooperation and defection to achieve this goal.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, play C (cooperate) with probability 0.5. This allows us to gather information about other players' initial strategies.
2. **Exploitation Phase**: For rounds 2 to r-1 (where r is the total number of rounds):
	* If the average cooperation rate of all players in the previous round is above a certain threshold τ (e.g., τ = 0.5), play D (defect). This indicates that other players are cooperative, and we can exploit them.
	* Otherwise, play C with probability p_C, which is updated based on the previous round's payoffs (see below).
3. **Endgame**: In the last round (r), play D if our current payoff is above a certain threshold φ (e.g., φ = 1.5). This ensures we maximize our final payoff.

**Updating Cooperation Probability p_C:**
After each round, update p_C using the following formula:

p_C ← p_C + α \* (π_avg - π_self)

where:
- α is a learning rate (e.g., α = 0.1)
- π_avg is the average payoff of all players in the previous round
- π_self is our own payoff in the previous round

This update rule increases p_C if we earned less than the average player, indicating that cooperation might be more beneficial.

**Handling Edge Cases:**

* If n = 2 (only two players), always play D. With only one opponent, there's no benefit to cooperating.
* In case of ties in cooperation rates or payoffs, use a random tiebreaker to decide between C and D.

**Pseudocode:**
```python
def AdaptiveExploiter(n, r, k):
  # Initialize variables
  p_C = 0.5  # initial cooperation probability
  τ = 0.5   # exploitation threshold
  φ = 1.5   # endgame payoff threshold
  α = 0.1   # learning rate

  for t in range(r):
    if t == 0:
      action = random.choice([C, D])  # initial cooperation (50%)
    elif t < r - 1:
      avg_coop_rate = average_cooperation_rate(all_players)
      if avg_coop_rate > τ:
        action = D  # exploit
      else:
        p_C = update_p_C(p_C, α, π_avg, π_self)
        action = C with probability p_C
    else:  # last round
      if our_payoff > φ:
        action = D

    return action
```
This strategy is designed to be robust and adaptive, allowing it to perform well against a wide range of opponent behaviors. By exploiting cooperation when it's beneficial and adjusting its own cooperation rate based on payoffs, the Adaptive Exploiter aims to maximize its overall payoff in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_320 = '''
I propose a strategy called "Adaptive Exploiter" (AE) for the N-Player Public Goods Game. AE is designed to be exploitative, adaptive, and robust against various opponent behaviors.

**Decision Rules:**

1. **Initial Rounds**: In the first round, play Cooperate (C). This allows us to gather information about the opponents' strategies without sacrificing potential payoffs.
2. **Exploration Phase** (Rounds 2-5): Alternate between C and Defect (D) in a cyclic manner (e.g., C-D-C-D-C). This helps to identify the number of cooperators and defectors among the opponents.
3. **Exploitation Phase** (Rounds 6-r):
	* If the total number of cooperators in the previous round is less than or equal to `n/2`, play D. This maximizes our payoff when there are not enough cooperators to generate a substantial public good.
	* If the total number of cooperators in the previous round is greater than `n/2`, play C. This allows us to benefit from the existing public good and potentially induce more cooperation.

**Edge Cases:**

1. **Last Round**: In the final round, play D regardless of the opponents' actions. Since there are no future rounds, it's optimal to maximize our payoff without considering potential reciprocity.
2. **Ties**: If the total number of cooperators is exactly `n/2`, play C. This maintains a conservative approach and avoids losing potential payoffs.

**Pseudocode:**
```markdown
# Initialize variables
cooperate = False
defect = True
exploration_phase = True
total_cooperators = 0

# First round
if current_round == 1:
    cooperate = True
else:
    # Exploration phase (Rounds 2-5)
    if exploration_phase and current_round <= 5:
        cooperate = not cooperate
    else:
        # Exploitation phase (Rounds 6-r)
        if total_cooperators <= n / 2:
            cooperate = False
        else:
            cooperate = True

# Update variables for next round
if cooperate:
    total_cooperators += 1
```
**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to capitalize on the opponents' cooperation while minimizing our own contributions. By alternating between C and D in the exploration phase, we gather information about the opponents' strategies without committing to a specific behavior. In the exploitation phase, we adapt to the opponents' actions, playing D when there are not enough cooperators and C when there is a substantial public good.

This strategy is robust against various opponent behaviors, as it doesn't rely on coordination or cooperation schedules. It will perform well in a tournament setting, where independent strategies are pitted against each other.
'''

description_EXPLOITATIVE_321 = '''
**Exploitative Strategy: Adaptive Cooperate-Defect (ACD)**

The ACD strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. It balances cooperation and defection to maximize payoffs.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to encourage opponents to cooperate and create a positive feedback loop.
2. **Subsequent Rounds:** Defect (D) if the average payoff of cooperators in the previous round is less than or equal to the average payoff of defectors. Otherwise, cooperate (C).
3. **Opponent Behavior Analysis:** Track the number of cooperators and defectors in each round. If more opponents defect than cooperate, increase the likelihood of defection in subsequent rounds.
4. **Adaptive Threshold:** Maintain a threshold value, `T`, initialized to 0.5. Update `T` after each round based on the proportion of cooperators: `T = T + (1/n) * (number of cooperators - number of defectors)`. Adjust the decision rule in step 2 by comparing the average payoff difference to this threshold.
5. **Punishment Mechanism:** If an opponent defects while others cooperate, increase the likelihood of defection against that opponent in subsequent rounds.

**Edge Cases:**

1. **Last Round:** Defect (D) in the last round, as there is no future benefit from cooperation.
2. **Ties:** In case of a tie in average payoffs between cooperators and defectors, cooperate (C) to maintain a positive feedback loop.

**Pseudocode:**
```python
def ACD(n, k, r):
    T = 0.5  # Adaptive threshold
    coop_count = 0  # Cooperators count
    defect_count = 0  # Defectors count

    for round in range(r):
        if round == 0:
            action = 'C'  # Cooperate in first round
        else:
            avg_coop_payoff = calculate_avg_payoff(cooperators)
            avg_defect_payoff = calculate_avg_payoff(defectors)

            if avg_coop_payoff > avg_defect_payoff + T:
                action = 'C'
            else:
                action = 'D'

            # Update adaptive threshold
            T += (1/n) * (coop_count - defect_count)

        # Take action and update counts
        if action == 'C':
            coop_count += 1
        else:
            defect_count += 1

        # Punishment mechanism
        if opponent_defected_while_others_cooperated:
            increase_likelihood_of_defection_against_opponent()

    return total_payoff
```
The ACD strategy is designed to adapt to various opponent behaviors, balancing cooperation and defection to maximize payoffs. By adjusting the decision rule based on the game's history and opponent behavior analysis, this strategy aims to exploit a wide range of opponents while maintaining a robust performance in a tournament setting.
'''

description_EXPLOITATIVE_322 = '''
**Exploitative Strategy: "Adaptive Free Rider"**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 50%. This is to gather information about the opponents' behavior and to potentially seed cooperation.
2. **History-Based Adaptation**: For rounds t > 1, calculate the average number of cooperators in previous rounds (t-1). If this average is greater than or equal to (n/2), cooperate (play C) with a probability of 30%. Otherwise, defect (play D).
3. **Exploiting Cooperators**: Monitor the history of opponents' actions. If an opponent has cooperated more than (2/3) of the time in previous rounds, identify them as a "cooperator". In subsequent rounds, always defect (play D) when interacting with identified cooperators.
4. **Punishing Defectors**: If an opponent has defected more than (2/3) of the time in previous rounds, identify them as a "defector". When interacting with identified defectors, cooperate (play C) with a probability of 20% to encourage potential cooperation.

**Edge Cases:**

* **Last Round**: In the final round, always defect (play D), regardless of previous history. This is because there are no future rounds to consider, and the goal is to maximize payoff in the current round.
* **Tie-Breaking**: If multiple opponents have identical cooperation frequencies, break ties by cooperating with the opponent who has contributed more to the public good in previous rounds.

**Pseudocode:**
```python
def adaptive_free_rider(n, k, r, history):
    # Initialize variables
    avg_cooperators = 0
    cooperators = []
    defectors = []

    # First round
    if len(history) == 0:
        return np.random.choice([C, D], p=[0.5, 0.5])

    # Calculate average number of cooperators in previous rounds
    avg_cooperators = sum(1 for action in history if action == C) / len(history)

    # Adapt based on history
    if avg_cooperators >= (n/2):
        return np.random.choice([C, D], p=[0.3, 0.7])
    else:
        return D

    # Identify cooperators and defectors
    for opponent in range(n):
        if sum(1 for action in history[opponent] if action == C) / len(history[opponent]) > (2/3):
            cooperators.append(opponent)
        elif sum(1 for action in history[opponent] if action == D) / len(history[opponent]) > (2/3):
            defectors.append(opponent)

    # Exploit identified cooperators and punish defectors
    for opponent in range(n):
        if opponent in cooperators:
            return D
        elif opponent in defectors:
            return np.random.choice([C, D], p=[0.2, 0.8])

# Last round handling
def last_round_adaptive_free_rider(n, k, r, history):
    return D
```
This strategy is designed to be adaptive and exploitative, taking advantage of opponents' cooperation while minimizing the risk of being exploited itself. By monitoring the history of opponents' actions and adjusting its behavior accordingly, the "Adaptive Free Rider" aims to maximize its payoff in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_323 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy focuses on maximizing individual payoffs by identifying opportunities to defect against cooperative opponents and cooperate when necessary to avoid mutual defection.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about opponents' behaviors and to potentially create a reputation for cooperation.
2. **Opponent Classification:** Based on the observed actions of each opponent, classify them into one of three categories:
	* **Cooperative Opponents (CO):** Those who cooperated in at least 75% of previous rounds.
	* **Defective Opponents (DO):** Those who defected in at least 75% of previous rounds.
	* **Uncertain Opponents (UO):** Those whose behavior doesn't fit the above categories or have insufficient data.
3. **Exploitation Logic:**
	* Against CO opponents, defect (D) to exploit their cooperation and gain a higher payoff.
	* Against DO opponents, cooperate (C) to avoid mutual defection and minimize losses.
	* Against UO opponents, cooperate (C) in the first encounter and then mirror their previous action.
4. **Adaptive Adjustment:** After each round, reassess opponent classifications based on new information.

**Edge Cases:**

1. **Last Round (t = r):** Defect against all opponents to maximize payoff, as there's no future interaction.
2. **Tiebreaker:** In case of a tie in the classification of an opponent, default to cooperation (C).

Pseudocode for the Adaptive Exploiter strategy:
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables and data structures
    opponents = {i: {'CO': 0, 'DO': 0, 'UO': 0} for i in range(1, n+1)}
    history = []

    # Initial rounds (t <= 2)
    for t in range(1, min(r, 3)):
        action = 'C'
        history.append(action)

    # Main game loop
    for t in range(3, r+1):
        for opponent in opponents:
            classify_opponent(opponents[opponent], history)
        
        action = ''
        for opponent in opponents:
            if opponents[opponent]['CO'] >= 0.75:  # Cooperative Opponent
                action = 'D'  # Defect to exploit cooperation
            elif opponents[opponent]['DO'] >= 0.75:  # Defective Opponent
                action = 'C'  # Cooperate to avoid mutual defection
            else:  # Uncertain Opponent
                if history and history[-1] == opponent:
                    action = mirror_opponent_action(opponents[opponent], history)
                else:
                    action = 'C'  # Cooperate by default

        # Last round adjustment
        if t == r:
            action = 'D'

        history.append(action)

def classify_opponent(opponent, history):
    # Update opponent classification based on their actions in the last few rounds
    pass

def mirror_opponent_action(opponent, history):
    # Mirror the opponent's previous action
    pass
```
The Adaptive Exploiter strategy is designed to be robust against a wide range of opponent behaviors and adapt to changing circumstances. By exploiting cooperative opponents and avoiding mutual defection with defective opponents, this strategy aims to maximize individual payoffs in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_324 = '''
Here's a strategic approach for the N-Player Public Goods Game that focuses on exploiting opponents while adapting to their behavior:

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Exploration**: In the first round, AE defects (D) to gather information about opponents' initial tendencies.
2. **Trend Analysis**: After the first round, AE analyzes the number of cooperators in each previous round and identifies a trend:
	* If more than half of players cooperate consistently, AE starts cooperating (C).
	* Otherwise, AE continues defecting (D).
3. **Adaptive Threshold**: Introduce an adaptive threshold (τ) that reflects the proportion of cooperators needed to trigger cooperation. Initially, set τ = 0.5.
4. **Exploitation**: If the number of cooperators exceeds τ in a round, AE defects (D) in the next round to exploit their contributions.
5. **Punishment and Reward**:
	* If an opponent defected (D) while most others cooperated (C), AE punishes that opponent by defecting (D) against them for a few rounds.
	* If an opponent consistently cooperates (C), AE rewards them by cooperating (C) occasionally, to maintain their cooperation.

**Pseudocode:**
```markdown
Initialize:
  round = 1
  τ = 0.5  // adaptive threshold

Loop through each round:
  if round == 1:
    action = D  // initial exploration
  else:
    analyze_previous_rounds()
    if more_than_half_cooperate():
      action = C
    elif cooperators_exceed_threshold(τ):
      action = D  // exploitation
    else:
      action = D

  take_action(action)

  observe_opponents_actions_and_payoffs()

  update_τ()  // adjust threshold based on game dynamics
  punish_or_reward_opponents()
```
**Edge Cases:**

1. **First Round**: Defect (D) to gather information.
2. **Last Round**: Cooperate (C) if most players cooperated in the previous round; otherwise, defect (D).
3. **Ties**: If multiple opponents have the same number of defections, AE prioritizes punishing those who defected more recently.

**Additional Notes:**

* To handle extreme cases where all or no players cooperate, AE adjusts its threshold τ to ensure it remains exploitative while adapting to these situations.
* By occasionally cooperating with consistent cooperators, AE encourages them to continue contributing to the public good.
* The punishment mechanism helps deter opponents from exploiting others.

This strategy is designed to be robust and adaptive in a wide range of scenarios, allowing it to effectively exploit opponents' behaviors while maintaining its own profitability.
'''

description_EXPLOITATIVE_325 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. This strategy relies solely on game parameters and history.

**Decision Rules:**

1. **Initial Round (Round 1):** Cooperate (C) in the first round to gather information about opponents' initial tendencies.
2. **Exploitation Phase:** From Round 2 onwards, use the following rules:
	* If the total number of cooperators in the previous round is less than or equal to `n/2`, Defect (D). This indicates a weak cooperative environment, and exploiting is likely to yield higher payoffs.
	* If the total number of cooperators in the previous round is greater than `n/2`, Cooperate (C). This suggests a strong cooperative environment, and contributing to the public good may lead to higher payoffs.
3. **Adaptation Mechanism:** After every 5 rounds (or when the game is near its end), reassess the opponent behavior by calculating the average number of cooperators over the last 5 rounds (`avg_cooperators`). Update the exploitation phase rules based on this value:
	* If `avg_cooperators` < `n/3`, become more exploitative: Defect (D) if the total number of cooperators in the previous round is less than or equal to `2*n/3`.
	* If `avg_cooperators` > `2*n/3`, become more cooperative: Cooperate (C) if the total number of cooperators in the previous round is greater than `n/3`.

**Edge Cases:**

1. **Last Round:** In the final round, Defect (D) regardless of the opponent behavior. This ensures maximizing individual payoff since there are no future interactions.
2. **Early Game (Rounds 2-5):** If an opponent defects in every round so far, Defect (D) immediately to avoid being exploited.

Pseudocode:
```markdown
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
cooperators_history = []
avg_cooperators = 0

# Round 1: Cooperate
action = C

# Rounds 2-r
for t in range(2, r+1):
    # Check previous round's cooperators
    prev_cooperators = sum(cooperators_history[-1])
    
    if prev_cooperators <= n/2:
        action = D
    else:
        action = C
    
    # Adaptation mechanism (every 5 rounds or near game end)
    if t % 5 == 0 or t >= r - 5:
        avg_cooperators = sum(cooperators_history[-5:]) / 5
        
        if avg_cooperators < n/3:
            exploitation_threshold = 2*n/3
        elif avg_cooperators > 2*n/3:
            cooperation_threshold = n/3
    
    # Update history and calculate payoff
    cooperators_history.append([int(action == C) for _ in range(n)])
    
    # Last round: Defect
    if t == r:
        action = D
```
This strategy aims to balance exploitation with adaptation, ensuring a competitive edge against various opponent behaviors while being robust to different game scenarios.
'''

description_EXPLOITATIVE_326 = '''
Here's a exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to gather information about opponents' behaviors.
2. **Subsequent Rounds (t>1):**
	* If total contributions in the previous round are less than or equal to n/2, Defect (D). This indicates that most opponents are not cooperating, and it's better to exploit them.
	* If total contributions in the previous round are greater than n/2, Cooperate (C) with a probability p = k/n. This allows us to benefit from the public good while still being cautious about exploitation.
3. **Opponent Cooperation Detection:** Monitor opponents' cooperation rates over time. If an opponent has cooperated more than 75% of the time in the last r/2 rounds, assume they are a "cooperator" and adjust our strategy accordingly (see below).
4. **Exploiting Cooperators:** When facing a detected cooperator, Defect (D) with a probability p = 0.8. This exploits their cooperative nature while maintaining some level of unpredictability.
5. **Punishing Defectors:** If an opponent has defected more than 75% of the time in the last r/2 rounds, Cooperate (C) with a probability p = 0.2 when interacting with them. This punishes their defection and encourages cooperation.

**Edge Cases:**

* Last Round (t=r): Defect (D), as there's no future game to influence.
* Opponent History Incomplete: Use the available history to make decisions, even if it's limited.

**Pseudocode:**
```python
def AdaptiveExploiter(n, k, r, history):
    if t == 1:
        return 'C'  # Cooperate in the first round

    total_contributions_prev = sum(history[-1])
    opponents_cooperation_rates = {i: count(history[i], 'C') / len(history[i]) for i in range(n)}

    cooperators = [i for i, rate in opponents_cooperation_rates.items() if rate > 0.75]
    defectors = [i for i, rate in opponents_cooperation_rates.items() if rate < 0.25]

    if total_contributions_prev <= n / 2:
        return 'D'  # Defect if most opponents didn't cooperate

    p_cooperate = k / n
    if random.random() < p_cooperate:
        return 'C'

    for opponent in cooperators:
        if random.random() < 0.8:
            return 'D'  # Exploit cooperators with high probability

    for opponent in defectors:
        if random.random() < 0.2:
            return 'C'  # Punish defectors with low probability

    return 'D'
```
This strategy, Adaptive Exploiter (AE), is designed to be exploitative while being adaptive and robust to various opponent behaviors. By monitoring opponents' cooperation rates and adjusting our strategy accordingly, we can maximize our payoff in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_327 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive to different game parameters.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability 0.5. This allows us to gather information about the opponents' initial behavior without committing to a specific strategy.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is greater than or equal to n/2, Defect (D). This takes advantage of the public good generated by others.
	* Otherwise, Cooperate (C) if the average payoff per player in the previous round is less than k/2. This encourages cooperation when the group's overall payoff is low.
	* If neither condition is met, Defect (D).
3. **Punishment Mechanism:** If an opponent defects while we cooperate, we will defect for the next two rounds to punish them and deter future defection.

**Handling Edge Cases:**

1. **Last Round (t=r):** Always Defect (D), as there is no opportunity to punish or reward opponents in subsequent rounds.
2. **Ties:** In case of a tie in the number of cooperators, Cooperate (C) with probability 0.5.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    cooperate_prob = 0.5
    prev_round_cooperators = []
    prev_round_payoff = 0

    for t in range(1, r+1):
        if t == 1:
            action = 'C' if random.random() < cooperate_prob else 'D'
        else:
            num_cooperators_prev_round = sum(prev_round_cooperators)
            avg_payoff_prev_round = prev_round_payoff / n

            if num_cooperators_prev_round >= n/2:
                action = 'D'
            elif avg_payoff_prev_round < k/2:
                action = 'C'
            else:
                action = 'D'

            # Punishment mechanism
            for opponent in opponents:
                if opponent.prev_action == 'D' and prev_round_cooperators[opponent.id]:
                    opponent.punish_count += 1
                    if opponent.punish_count < 2:
                        action = 'D'

        # Update variables
        prev_round_cooperators.append(action == 'C')
        prev_round_payoff = sum([k/n * sum(prev_round_cooperators) + (action != 'C') for _ in range(n)])

        return action
```
The Adaptive Exploiter strategy is designed to be exploitative, taking advantage of opponents' cooperation while being robust against a wide range of behaviors. By punishing defectors and adapting to the group's overall payoff, we aim to maximize our own payoff throughout the game.
'''

description_EXPLOITATIVE_328 = '''
Here's a robust and exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors:

**Strategy Name:** "Adaptive Exploiter" (AE)

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) to gather information about opponents' behavior.
	* In the first two rounds, play C to observe how many players cooperate and defect. This initial cooperation phase helps AE understand the general level of cooperation in the group.
2. **Exploitation Phase:** Defect (D) when the average cooperation rate is low or decreasing.
	* After the initial rounds, calculate the average cooperation rate (ACR) over the last 2-3 rounds. If ACR < 0.5 or is decreasing, switch to playing D in the next round.
	* This phase exploits opponents who continue to cooperate despite a low overall cooperation rate.
3. **Punishment Phase:** Cooperate (C) when an opponent defects after cooperating previously.
	* Monitor individual opponents' actions and identify those who defect after cooperating. When such an opponent defects, play C in the next round to "punish" them for their non-cooperative behavior.
4. **Reciprocity Phase:** Cooperate (C) with cooperative opponents.
	* Identify opponents who consistently cooperate (e.g., ACR > 0.7). Reciprocate their cooperation by playing C when interacting with these opponents.

**Edge Cases:**

1. **Last Round (t = r):** Defect (D) to maximize personal payoff, as there is no future interaction.
2. **Tiebreaker:** When the ACR is exactly 0.5 or there's a tie in cooperation rates, play D to avoid being exploited.

**Additional Heuristics:**

1. **Monitor Opponent Behavior:** Continuously track each opponent's actions and adjust the strategy accordingly.
2. **Adjust Cooperation Thresholds:** Dynamically update the ACR thresholds (e.g., 0.5) based on the group's overall behavior to ensure optimal exploitation.

Pseudocode:
```python
def Adaptive_Exploiter(n, k, r):
    # Initialize variables
    avg_coop_rate = [0] * n
    opponent_actions = [[] for _ in range(n)]
    
    for t in range(1, r+1):
        if t <= 2:  # Initial rounds
            action = 'C'
        else:
            # Calculate average cooperation rate
            avg_coop_rate[t-1] = sum(opponent_actions[i][-3:]) / (n * 3)
            
            if avg_coop_rate[t-1] < 0.5 or is_decreasing(avg_coop_rate, t):
                action = 'D'  # Exploitation phase
            elif was_punished(t-1):  # Punishment phase
                action = 'C'
            else:
                action = choose_action_based_on_reciprocity(opponent_actions)
        
        # Update opponent actions and adjust strategy
        for i in range(n):
            if i != self_id:  # Update opponent's action history
                opponent_actions[i].append(get_opponent_action(i, t))
        
    return action

def is_decreasing(avg_coop_rate, t):
    # Check if average cooperation rate is decreasing
    return avg_coop_rate[t-1] < avg_coop_rate[t-2]

def was_punished(t):
    # Check if an opponent defected after cooperating previously
    for i in range(n):
        if i != self_id and opponent_actions[i][t-1] == 'D' and opponent_actions[i][t-2] == 'C':
            return True
    return False

def choose_action_based_on_reciprocity(opponent_actions):
    # Choose action based on reciprocity with cooperative opponents
    for i in range(n):
        if i != self_id and is_cooperative(opponent_actions[i]):
            return 'C'
    return 'D'

def is_cooperative(actions):
    # Check if opponent is consistently cooperative
    coop_count = sum(1 for a in actions[-3:] if a == 'C')
    return coop_count >= 2
```
This strategy, Adaptive Exploiter (AE), aims to balance exploitation and cooperation. It initially gathers information about opponents' behavior, then adapts its strategy based on the group's overall cooperation rate. AE exploits non-cooperative opponents while reciprocating with cooperative ones, making it a robust and effective strategy in this game.
'''

description_EXPLOITATIVE_329 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive throughout the game.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
 Cooperate (C) in the first three rounds to gather information about opponents' behavior. This allows us to observe their initial tendencies and adapt our strategy accordingly.
2. **Defect if Opponents Defect (Round ≥ 4)**:
 If, in any round, we observe that more than half of the opponents defected (D) in the previous round, we defect (D) as well. This ensures we don't get exploited by a majority of defectors.
3. **Exploit Cooperators (Round ≥ 4)**:
 If, in any round, we observe that more than half of the opponents cooperated (C) in the previous round, and our own payoff from cooperating is higher than defecting (π_i,C > π_i,D), we cooperate (C). This allows us to exploit the cooperators and gain a higher payoff.
4. **Punish Freeriders (Round ≥ 5)**:
 If an opponent defected in the previous round while we cooperated, and our own payoff from cooperating is lower than defecting (π_i,C < π_i,D), we defect (D) in the current round. This punishes freeriders who try to exploit us.

**Edge Cases:**

* **Last Round (Round r)**:
 Cooperate if the number of cooperators in the previous round was more than half, and our payoff from cooperating is higher than defecting. Otherwise, defect.
* **Tie-Breaking**:
 In case of a tie in the decision rules (e.g., equal number of cooperators and defectors), we cooperate.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r, history):
  # Initialize variables
  cooperate = False
  defect = True

  # Initial Exploration
  if round < 4:
    cooperate = True

  # Defect if Opponents Defect
  elif sum(history[-1]['actions'] == 'D') > n/2:
    cooperate = False

  # Exploit Cooperators
  elif sum(history[-1]['actions'] == 'C') > n/2 and payoff_C > payoff_D:
    cooperate = True

  # Punish Freeriders
  elif history[-1][opponent] == 'D' and payoff_C < payoff_D:
    cooperate = False

  return cooperate
```
**Exploitative Mindset:**
This strategy is designed to exploit a wide range of opponent behaviors, from pure cooperators to pure defectors. By adapting our actions based on the history of play, we aim to maximize our payoffs while minimizing our contributions to the public good. The Adaptive Exploiter strategy is robust and can handle various edge cases, making it a formidable opponent in the tournament.
'''

description_EXPLOITATIVE_330 = '''
**Exploitative Strategy: "History-Weighted Follower"**

The goal is to develop a robust and adaptive strategy that exploits opponents' behavior while being cautious not to become too predictable.

**Decision Rules:**

1. **Initial Round**: Defect (D) in the first round to gather information about opponents' initial tendencies.
2. **Subsequent Rounds**: Cooperate (C) if the average cooperation rate of all players in the previous round is above a certain threshold, `τ` (e.g., 0.5). Otherwise, defect (D).
3. **Threshold Adjustment**: Update `τ` every 5 rounds based on the moving average of opponents' cooperation rates over the last 10 rounds.

Pseudocode:
```
tau = 0.5
history = []

for round in range(r):
    if round == 0:
        action = D
    else:
        avg_coop_rate = sum(history[-1]) / n
        if avg_coop_rate > tau:
            action = C
        else:
            action = D

    history.append([c_j for c_j in opponents' actions])
    if round % 5 == 0 and len(history) >= 10:
        tau = moving_average(opponents' coop rates, window=10)

def moving_average(data, window):
    return sum(data[-window:]) / window
```
**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round if the average cooperation rate of all players in the previous round is above `τ`. Otherwise, defect (D).
2. **Tie-Breaking**: If the average cooperation rate equals `τ`, defect (D).

**Exploitative Mindset:**

This strategy aims to exploit opponents who tend to cooperate too much or too little by:

1. Initially defecting to gather information about their tendencies.
2. Adapting the threshold (`τ`) based on opponents' behavior, making it harder for them to anticipate our actions.
3. Cooperating when the average cooperation rate is high, as this increases the likelihood of a higher payoff.

By following these decision rules and adapting to opponents' behavior, "History-Weighted Follower" aims to maximize its total game payoff while minimizing predictability.
'''

description_EXPLOITATIVE_331 = '''
**Exploitative Strategy: "Conditional Cooperator with Adaptive Threshold"**

This strategy aims to balance cooperation and defection to maximize individual payoffs while adapting to the behavior of other players.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) unconditionally.
2. **Adaptive Threshold**: Set an initial threshold `T` as a function of game parameters: `T = k/n`.
3. **Conditional Cooperation**: In subsequent rounds (`t > 1`), calculate the average cooperation rate among all players in previous rounds (`C_avg`). If `C_avg >= T`, cooperate (play C). Otherwise, defect (play D).
4. **Update Threshold**: After each round, update the threshold `T` based on the observed cooperation rates: `T = max(T - 0.1 * (C_avg - T), k/n)`. This allows the strategy to adapt to changing opponent behaviors.
5. **Punishment**: If another player defects while you cooperated in the previous round, defect (play D) for one round before re-evaluating the cooperation rate.

**Handling Edge Cases:**

1. **Last Round**: In the final round (`t = r`), always defect (play D) to maximize individual payoff.
2. **Tie-breaking**: If `C_avg` equals `T`, cooperate (play C) by default.

**Pseudocode:**
```python
def conditional_cooperator(n, k, r):
    T = k / n  # initial threshold
    last_round_action = None

    for t in range(1, r + 1):
        if t == 1:
            action = 'C'  # cooperate in first round
        else:
            C_avg = calculate_average_cooperation_rate()
            if C_avg >= T:
                action = 'C'
            else:
                action = 'D'

        if last_round_action == 'C' and opponent_defected:
            action = 'D'  # punish defection

        update_threshold(T, C_avg)

        last_round_action = action
        yield action

def calculate_average_cooperation_rate():
    # Calculate average cooperation rate among all players in previous rounds
    pass

def update_threshold(T, C_avg):
    T = max(T - 0.1 * (C_avg - T), k / n)
```
This strategy is exploitative as it:

* Adapts to opponent behaviors by adjusting the threshold `T` based on observed cooperation rates.
* Punishes defection in previous rounds to maintain a level of cooperation.
* Cooperates conditionally, ensuring that individual payoffs are maximized when others cooperate.

By using an adaptive threshold and updating it dynamically, this strategy is robust to a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_332 = '''
**Exploitative Strategy: "Adaptive Defector with Conditional Cooperation"**

**Decision Rules:**

1. **Initial Rounds (t = 1 to min(r/2, 5))**: Cooperate (C) to encourage cooperation and gather information about opponents' strategies.
2. **Conditional Cooperation (t > min(r/2, 5))**:
	* If the average payoff of cooperators in the previous round is higher than that of defectors, cooperate (C).
	* Otherwise, defect (D).
3. **Punishment Mechanism**: If the number of defectors in the previous round exceeds half of the total players (n/2), defect (D) for one round to punish non-cooperative behavior.
4. **Exploitation Mode**: When playing against opponents who consistently cooperate or use a tit-for-tat strategy, exploit them by defecting (D) every other round.

**Edge Case Handling:**

1. **First Round (t = 1)**: Cooperate (C) to encourage cooperation and gather information.
2. **Last Round (t = r)**: Defect (D), as there is no future interaction to worry about.
3. **Small Number of Rounds (r ≤ 10)**: Focus on exploiting opponents' strategies, using the Punishment Mechanism and Exploitation Mode more frequently.

**Pseudocode:**
```python
def adaptive_defector(n, k, r, history):
    # Initial rounds
    if t <= min(r/2, 5):
        return 'C'

    # Conditional cooperation
    avg_c_payoff = sum(history[t-1]['payoffs'][i] for i in range(n) if history[t-1]['actions'][i] == 'C') / n
    avg_d_payoff = sum(history[t-1]['payoffs'][i] for i in range(n) if history[t-1]['actions'][i] == 'D') / n
    if avg_c_payoff > avg_d_payoff:
        return 'C'
    else:
        return 'D'

    # Punishment mechanism
    if sum(1 for action in history[t-1]['actions'] if action == 'D') > n/2:
        return 'D'

    # Exploitation mode
    opponent_strategies = [history[t-1]['actions'][i] for i in range(n) if i != player_id]
    if all(strategy == 'C' or strategy == 'TFT' for strategy in opponent_strategies):
        return 'D'
```
This strategy aims to balance cooperation and exploitation, adapting to the game's progression and opponents' behaviors. By cooperating initially and conditionally, we encourage cooperation while gathering information about opponents' strategies. The punishment mechanism helps maintain a cooperative environment, while the exploitation mode takes advantage of predictable opponents.
'''

description_EXPLOITATIVE_333 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its total payoff by adaptively responding to the opponents' behavior while being robust to various strategies.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C). This sets a positive tone and allows us to observe opponents' initial actions.
2. **Subsequent Rounds (t>1):**
	* If the average cooperation rate among opponents in the previous round is below 50%, Defect (D).
	* Otherwise, use the " Tit-for-Tat with a Twist" approach:
		+ Cooperate if at least half of the opponents cooperated in the previous round.
		+ Defect if more than half of the opponents defected in the previous round.
3. **Adaptive Adjustment:** Every 5 rounds (or r/2, whichever is smaller), reassess the average cooperation rate among opponents over the last 5 rounds. If it has changed significantly (>10%), adjust the decision threshold for steps 2a and 2b accordingly.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    avg_coop_rate = 0
    prev_actions = [None] * n
    
    for t in range(1, r+1):
        if t == 1:
            action = 'C'
        else:
            opponents_coop_rate = sum(prev_actions) / n
            
            if opponents_coop_rate < 0.5:
                action = 'D'
            elif opponents_coop_rate >= 0.5:
                # Tit-for-Tat with a Twist
                if opponents_coop_rate >= 0.5:
                    action = 'C'
                else:
                    action = 'D'
            
            # Adaptive adjustment every 5 rounds (or r/2)
            if t % 5 == 0 or t > r / 2:
                new_avg_coop_rate = sum(prev_actions) / (n * 5)
                if abs(new_avg_coop_rate - avg_coop_rate) > 0.1:
                    avg_coop_rate = new_avg_coop_rate
                    # Adjust decision threshold
        
        prev_actions = [action] + prev_actions[:-1]
        
        return action
```
**Handling Edge Cases:**

* **Last Round (t=r):** Follow the same decision rules as in subsequent rounds.
* **Opponents' Initial Actions:** Observe and adjust the strategy based on their initial actions.

**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to take advantage of opponents who are too cooperative or too defective. By initially cooperating, we encourage cooperation from others. If opponents defect excessively, we adapt by defecting as well. The Tit-for-Tat with a Twist approach ensures that we maintain a balance between cooperation and defection, making it difficult for opponents to exploit us consistently.

In the tournament setting, this strategy will perform well against various opponents, as it can adapt to different behaviors while maintaining its exploitative nature.
'''

description_EXPLOITATIVE_334 = '''
**Exploitative Strategy: Adaptive Defection with Cooperate-Defect Switching**

Our strategy, "ADS" (Adaptive Defection with Cooperate-Defect Switching), aims to exploit opponents by adapting to their behavior while maintaining a balance between cooperation and defection.

**Decision Rules:**

1. **Initial Rounds**: In the first round, play D (defect) to gather information about opponents' initial actions.
2. **Monitoring Opponent Behavior**: For each subsequent round t, calculate the average opponent cooperation rate c_avg,t-1 over the previous rounds.
3. **Cooperation Threshold**: Set a cooperation threshold θ = k / n, where k is the multiplication factor and n is the number of players. This threshold represents the minimum required cooperation rate to justify cooperating ourselves.
4. **Action Selection**:
	* If c_avg,t-1 ≥ θ (i.e., opponents are cooperating sufficiently), play C (cooperate) in round t.
	* Otherwise, play D (defect) in round t.

Pseudocode for action selection:
```
if c_avg,t-1 >= k / n then
    play C
else
    play D
end if
```

**Edge Case Handling:**

1. **Last Round**: In the final round r, play D (defect), as there is no future benefit from cooperation.
2. **Tiebreakers**: If c_avg,t-1 = θ exactly, play C (cooperate) to avoid unnecessary defection.

**Rationale and Exploitative Mindset:**

ADS takes advantage of opponents' cooperative behavior while minimizing the risk of exploitation itself. By monitoring opponent cooperation rates, ADS can:

* Identify and punish defectors by switching to D when c_avg is low.
* Reward cooperators by cooperating (C) when c_avg is high enough.
* Maintain a balance between cooperation and defection, making it difficult for opponents to predict our actions.

**Robustness:**

ADS is designed to be robust against various opponent behaviors:

* **Cooperators**: ADS will cooperate with them, maximizing mutual payoffs.
* **Defectors**: ADS will defect against them, minimizing losses.
* **Mixed Strategies**: ADS will adapt to the average cooperation rate, ensuring a balance between cooperation and defection.

By using the cooperation threshold θ = k / n, ADS is also robust against variations in the multiplication factor k and the number of players n.
'''

description_EXPLOITATIVE_335 = '''
Here's a exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 0.5. This initial cooperation serves as a probe to gauge the opponents' behavior.
2. **Exploit Defectors**: If the number of defectors in the previous round is greater than or equal to n/2 (where n is the number of players), defect (play D) in the current round. This rule exploits the fact that when many opponents defect, contributing to the public good becomes less beneficial.
3. **Punish Free Riders**: If a player i observed another player j cooperating while they themselves defected in the previous round, punish player j by defecting against them with a probability of 0.8 in the current round. This rule aims to deter free-riding behavior.
4. **Imitate Successful Cooperators**: If the number of cooperators in the previous round is greater than or equal to n/2 and the total payoff for cooperating was higher than the average payoff, imitate a random cooperator from the previous round with a probability of 0.6. This rule encourages cooperation when it's successful.
5. **Adaptive Threshold**: Update an internal threshold variable `T` based on the history of opponents' actions:
	* If the number of cooperators in the previous round is greater than or equal to n/2, increase `T` by 0.1.
	* If the number of defectors in the previous round is greater than or equal to n/2, decrease `T` by 0.1.
	* Cooperate if the current round's expected payoff for cooperating is above `T`, otherwise defect.

**Handling Edge Cases:**

* **Last Round**: In the last round (r), always defect (play D). Since there are no future rounds to consider, maximizing personal gain takes priority.
* **First Round with n = 2**: In a two-player game, cooperate in the first round. This encourages cooperation and potentially leads to mutual benefit.

**Pseudocode:**

```
Initialize:
T = 0.5 // Adaptive threshold

For each round t:
  Observe opponents' actions from previous round
  If (t == 1) or (n == 2):
    Cooperate with probability 0.5
  Else if (#defectors >= n/2):
    Defect
  Else if (punish_free_rider):
    Defect against player j with probability 0.8
  Else if (imitate_successful_cooperators):
    Imitate a random cooperator from previous round with probability 0.6
  Else:
    Calculate expected payoff for cooperating and defecting
    If (expected_payoff_for_cooperating > T):
      Cooperate
    Else:
      Defect

  Update adaptive threshold T based on opponents' actions
```

This strategy, "Adaptive Exploiter", aims to balance exploitation with adaptability. By adjusting the internal threshold variable `T` and imitating successful cooperators, it seeks to maximize payoffs while responding effectively to various opponent behaviors.
'''

description_EXPLOITATIVE_336 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy prioritizes short-term gains and seeks to maximize individual payoffs.

**Decision Rules:**

1. **Initial Round (t=1)**: Defect (D) unconditionally. This sets the tone for the game, as cooperating in the first round may encourage opponents to cooperate, reducing potential gains.
2. **Subsequent Rounds (t>1)**:
	* If the opponent's average cooperation rate (ACR) is high (> 0.5), Cooperate (C). ACR is calculated as the total number of times an opponent cooperated divided by the total number of rounds played against them.
	* Otherwise, Defect (D).
3. **Adaptive Threshold Adjustment**: After every round, adjust the cooperation threshold based on the game's history:
	+ If the player's average payoff is below the expected payoff from defecting (i.e., π_i < 1), decrease the threshold by 0.05.
	+ If the player's average payoff is above the expected payoff from cooperating (i.e., π_i > k/n \* n), increase the threshold by 0.05.

**Handling Edge Cases:**

* **Last Round (t=r)**: Defect (D) unconditionally, as there are no future rounds to consider.
* **Opponent's Cooperation Rate is Exactly 0.5**: Cooperate (C), as this suggests a mixed strategy or uncertain behavior from the opponent.
* **Game History is Inconclusive (e.g., few rounds played)**: Use a default threshold value of 0.5 for the cooperation decision.

**Pseudocode:**
```python
def adaptive_predator(n, k, r, history):
    if t == 1:
        return 'D'  # Defect in the first round

    opponent_acr = calculate_opponent_acr(history)
    threshold = get_threshold(history)

    if opponent_acr > threshold:
        return 'C'  # Cooperate if opponent's ACR is high
    else:
        return 'D'  # Defect otherwise

def calculate_opponent_acr(history):
    total_cooperations = sum(1 for round in history if round['opponent_action'] == 'C')
    total_rounds = len(history)
    return total_cooperations / total_rounds

def get_threshold(history):
    player_avg_payoff = calculate_player_avg_payoff(history)
    expected_defect_payoff = 1
    expected_cooperate_payoff = k/n * n

    if player_avg_payoff < expected_defect_payoff:
        threshold -= 0.05
    elif player_avg_payoff > expected_cooperate_payoff:
        threshold += 0.05

    return threshold
```
This strategy is designed to be exploitative, focusing on maximizing individual payoffs by adapting to the opponent's behavior and adjusting its own cooperation threshold accordingly. By defecting in the first round and subsequently cooperating only when the opponent's ACR is high, the Adaptive Predator aims to take advantage of potential cooperators while minimizing losses against defectors.
'''

description_EXPLOITATIVE_337 = '''
I'll design a exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history. Here's my approach:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to gather information about other players' behavior.
2. **Subsequent Rounds (t>1):**
	* If the average cooperation rate of all players in the previous round is above a certain threshold (THRESHOLD = 0.5), Defect (D).
	* Otherwise, Cooperate (C) if the total payoff from cooperating in the previous round is greater than or equal to the total payoff from defecting; otherwise, Defect (D).

**Pseudocode:**
```markdown
# Initialize variables
threshold = 0.5
cooperation_rate = 0

# Round 1: Cooperate and gather information
if t == 1:
    action = C

# Subsequent rounds
else:
    # Calculate cooperation rate of all players in previous round
    cooperation_rate = sum(c_j for j in range(n)) / n
    
    # Check if cooperation rate is above threshold
    if cooperation_rate > threshold:
        action = D
    else:
        # Compare payoffs from cooperating and defecting in previous round
        payoff_C = (1 - c_i) + (k/n) * sum(c_j for j in range(n))
        payoff_D = 1 + (k/n) * sum(c_j for j in range(n))
        
        if payoff_C >= payoff_D:
            action = C
        else:
            action = D
```
**Edge Cases:**

* **Last Round (t=r):** In the final round, always Defect (D), as there's no future interaction to consider.
* **Opponents' Cooperation Rate:** If all opponents consistently cooperate (C) or defect (D), adjust the strategy to exploit this behavior by defecting (D) when they cooperate and cooperating (C) when they defect.

**Exploitative Mindset:**

The Adaptive Exploiter strategy prioritizes exploiting others' cooperation while minimizing its own contribution. By cooperating in the first round, it gathers information about other players' behavior and adjusts its strategy accordingly. If opponents are willing to cooperate at a high rate, AE defects to maximize its payoff. When opponents are less cooperative, AE cooperates only if it's profitable to do so.

**Robustness:**

This strategy is robust against various opponent behaviors, including:

* **Pure Cooperators:** AE defects and exploits their cooperation.
* **Pure Defectors:** AE cooperates when it's profitable to do so.
* **Mixed Strategies:** AE adapts to the average cooperation rate and adjusts its strategy accordingly.

The Adaptive Exploiter strategy should perform well in a tournament against independent strategies, as it balances exploitation with adaptability and robustness.
'''

description_EXPLOITATIVE_338 = '''
I'll design a strategy that balances exploitation with adaptability and robustness. Here's my proposal:

**Strategy Name:** "Adaptive Exploiter" (AE)

**Decision Rules:**

1. **Initial Rounds (t ≤ n):** Play Cooperate (C) in the first 'n' rounds to encourage opponents to contribute and build a baseline understanding of their behavior.
2. **Monitoring Phase (t > n):** Observe opponents' actions and calculate the average cooperation rate (ACR) over the last 'n' rounds.

Pseudocode:
```python
def calculate_ACR(opponent_actions, t):
    window_size = n  # parameter
    recent_coops = sum([1 if action == 'C' else 0 for action in opponent_actions[t-window_size+1:t]])
    return recent_coops / window_size

ACR_threshold = 0.5  # adjust this value based on game parameters and desired level of exploitation

if ACR(opponent_actions, t) > ACR_threshold:
    play Defect (D)
else:
    play Cooperate (C)
```
3. **Exploitation Mode:** When the ACR is below the threshold, switch to a defecting strategy to exploit opponents' cooperation.
4. **Punishment Mechanism:** If an opponent defects while the AE plays Cooperate, punish them by playing Defect for 2-3 rounds to discourage future defections.

**Edge Cases:**

* **First Round (t = 1):** Play Cooperate to encourage opponents to contribute and build a baseline understanding of their behavior.
* **Last Rounds (t ≥ r - n):** Switch to a defecting strategy to maximize payoffs, as the game is ending.
* **Opponent Defection:** If an opponent defects while the AE plays Cooperate, punish them by playing Defect for 2-3 rounds.

**Robustness and Adaptability:**

The Adaptive Exploiter (AE) strategy balances cooperation and exploitation based on opponents' behavior. By monitoring the ACR, it adapts to changing game conditions and responds accordingly. The punishment mechanism helps maintain a stable level of cooperation, while the exploitation mode ensures that the AE takes advantage of opponents' cooperative tendencies.

This strategy is robust against various opponent behaviors, including:

* Cooperators: AE encourages cooperation in early rounds and exploits their willingness to contribute.
* Defectors: AE punishes defectors and adapts to their behavior by switching to a defecting strategy.
* Mixed Strategies: AE's adaptive nature allows it to respond effectively to opponents playing mixed strategies.

The AE strategy is designed to perform well in a tournament setting, where multiple independent strategies are competing against each other.
'''

description_EXPLOITATIVE_339 = '''
**Exploitative Strategy: Adaptive Cooperation with Retaliation (ACR)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to signal a willingness to cooperate and gather information about opponents' behavior.
2. **Subsequent Rounds:** For each round t > 1, calculate the average payoff of cooperators (APC) and defectors (APD) from the previous round:
	* APC = (Σ(j=1 to n) c_j × π_j,t-1) / Σ(j=1 to n) c_j
	* APD = (Σ(j=1 to n) (1-c_j) × π_j,t-1) / Σ(j=1 to n) (1-c_j)
3. **Cooperation Threshold:** Define a cooperation threshold (CT) as the average payoff of cooperators in the previous round:
	* CT = APC
4. **Defection Threshold:** Define a defection threshold (DT) as the minimum average payoff of defectors required to defect:
	* DT = k/n × n / 2
5. **Action Selection:**
	* If APD > CT and APD > DT, Defect (D)
	* Else if APC > APD and APC > DT, Cooperate (C)
	* Otherwise, Defect (D)

**Edge Cases:**

1. **Last Round:** In the last round, defect regardless of previous behavior to maximize individual payoff.
2. **Opponent Defection:** If an opponent defects in a previous round, retaliate by defecting in the next round.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
APC = 0 (average payoff of cooperators)
APD = 0 (average payoff of defectors)
CT = 0 (cooperation threshold)
DT = k/n × n / 2 (defection threshold)

# First round: Cooperate
if t == 1:
    action = C

# Subsequent rounds
else:
    # Calculate APC and APD
    APC = sum(c_j × π_j,t-1) / sum(c_j)
    APD = sum((1-c_j) × π_j,t-1) / sum(1-c_j)

    # Update CT and DT
    CT = APC
    DT = k/n × n / 2

    # Select action
    if APD > CT and APD > DT:
        action = D
    elif APC > APD and APC > DT:
        action = C
    else:
        action = D

    # Retaliate if opponent defected
    if opponent_defected:
        action = D

# Last round: Defect
if t == r:
    action = D
```
**Rationale:**

1. **Initial Cooperation:** Cooperating in the first round allows us to gather information about opponents' behavior and potentially establish a cooperative norm.
2. **Adaptive Thresholds:** The cooperation and defection thresholds adapt to the average payoffs of cooperators and defectors, allowing us to respond effectively to changing opponent behavior.
3. **Retaliation:** Retaliating against defecting opponents discourages them from exploiting our cooperative behavior.
4. **Defection in Last Round:** Defecting in the last round maximizes individual payoff, as there is no future interaction.

This strategy aims to balance cooperation and defection to exploit a wide range of opponent behaviors while maintaining adaptability and robustness.
'''

description_EXPLOITATIVE_340 = '''
Here's a strategic approach for playing the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to gather information about opponents' behavior and encourage cooperation.
2. **Contribution Threshold**: Track the total contributions (Σc_j) from previous rounds. If the average contribution is above a certain threshold (T), cooperate; otherwise, defect.

Threshold T = k/n \* (n/2)

This threshold balances individual payoffs with collective benefits, allowing AE to adapt to varying levels of cooperation.
3. **Adaptive Response**: Monitor the average payoff difference between cooperators and defectors in previous rounds. If defectors have a higher average payoff, defect; otherwise, cooperate.

 Payoff Difference (PD) = Average Defector Payoff - Average Cooperator Payoff

If PD > 0, defect; else, cooperate
4. **Exploitation**: Identify opponents who consistently cooperate despite low payoffs and exploit them by defecting in the same round.
5. **Endgame Strategy**: In the last round (r), cooperate if the total contributions are above the threshold T; otherwise, defect.

**Edge Cases:**

* If an opponent defects in every round, AE will also defect to minimize losses.
* If all opponents cooperate, AE will cooperate to maximize collective benefits.
* If the game is in its final rounds and the total contributions are below the threshold, AE may defect to maximize individual payoffs.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    total_contributions = 0
    avg_cooperator_payoff = 0
    avg_defector_payoff = 0
    opponent_behavior = {}

    for t in range(r):
        if t == 0:  # First round
            action = 'C'
        else:
            threshold = k / n * (n / 2)
            if total_contributions >= threshold:
                action = 'C'
            elif avg_defector_payoff > avg_cooperator_payoff:
                action = 'D'
            else:
                action = 'C'

        # Update opponent behavior and payoffs
        for i in range(n):
            if i != self.id:
                opponent_behavior[i].append(opponent_actions[i])
                if opponent_actions[i] == 'C':
                    avg_cooperator_payoff += payoffs[i]
                else:
                    avg_defector_payoff += payoffs[i]

        # Exploit consistent cooperators
        for i in range(n):
            if i != self.id and opponent_behavior[i].count('C') == t:
                action = 'D'

        # Endgame strategy
        if t == r - 1:
            if total_contributions >= threshold:
                action = 'C'
            else:
                action = 'D'

        return action
```
**Rationale:**
The Adaptive Exploiter (AE) strategy is designed to balance individual payoffs with collective benefits while adapting to various opponent behaviors. By tracking contribution thresholds and payoff differences, AE can identify opportunities to cooperate or defect. The exploitation mechanism targets consistent cooperators, allowing AE to maximize its own payoffs in certain situations. This approach should enable AE to perform well against a wide range of independent strategies in the tournament.
'''

description_EXPLOITATIVE_341 = '''
To develop an exploitative strategy for the N-Player Public Goods Game that is adaptive and robust to a wide range of opponent behaviors, we'll focus on maximizing our payoff by making decisions based on observed behavior in previous rounds. This approach leverages the perfect information assumption to adjust our strategy dynamically.

**Strategy Name:** Adaptive Exploitation (AE)

### Decision Rules

1. **First Round:**
   - Cooperate (C) in the first round. This initial cooperation serves as a probe to gauge other players' willingness to cooperate and sets a baseline for future decision-making.

2. **Subsequent Rounds (t > 1):**
   - Calculate the average number of cooperators (`avg_cooperators`) from all previous rounds.
   - Calculate the total payoff (`total_payoff`) received by playing D (Defect) in the current round, assuming no change in other players' strategies: `total_payoff = 1 + (k/n) * avg_cooperators`.
   - If `total_payoff` is greater than or equal to the payoff from cooperating (`k/n * n`), Defect (D). Otherwise, Cooperate (C).

Pseudocode for subsequent rounds:

```
if t > 1:
    avg_cooperators = sum(cooperators in all previous rounds) / t
    total_payoff_D = 1 + (k/n) * avg_cooperators
    if total_payoff_D >= k:
        action = D
    else:
        action = C
```

### Handling Edge Cases

- **Last Round:**
  - In the last round, since there are no future rounds to influence or learn from, prioritize maximizing immediate payoff. If `avg_cooperators` is high enough that cooperating would yield a higher payoff than defecting (`k/n * avg_cooperators > total_payoff_D`), Cooperate; otherwise, Defect.

- **Early Rounds (t ≤ n):**
  - For the first few rounds, consider an exploration phase where you randomly choose between C and D with a slight bias towards cooperating. This can help in quickly identifying potential cooperators and adjusting strategy accordingly.

### Exploitative Mindset

The Adaptive Exploitation strategy is designed to maximize payoff by exploiting the cooperation of others while minimizing personal contribution to the public good when it's advantageous to do so. By adapting to the observed behavior, it aims to find a balance between contributing enough to encourage cooperation from others and defecting sufficiently to maximize personal gain.

This approach does not rely on norms or coordination mechanisms but instead uses game history to inform decisions. It is robust because it adjusts its strategy based on how other players behave, making it suitable for a tournament setting where opponents' strategies are unknown.
'''

description_EXPLOITATIVE_342 = '''
**Exploitative Strategy: "Adaptive Pessimist"**

The Adaptive Pessimist strategy aims to maximize its own payoff by exploiting the cooperation of others while minimizing its own contributions to the public good.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 1/n, where n is the number of players. This initial cooperation serves as a probe to gather information about the opponents' behaviors.
2. **Pessimistic Threshold**: Calculate the threshold value `T = (k-1)/n`, which represents the minimum required contribution from others to make cooperating worthwhile.
3. **Cooperation Condition**: Cooperate (C) if and only if:
	* In the previous round, at least `T` opponents cooperated.
	* The total number of cooperators in the previous round was greater than or equal to `(n+1)/2`.
4. **Defection by Default**: Defect (D) otherwise.

**Edge Cases:**

1. **Last Round**: In the last round, always defect (D), as there is no future benefit from cooperating.
2. **First Round (revisited)**: If `n` is even and `k` is close to 1, it may be beneficial to defect in the first round instead of cooperating with probability 1/n. However, for simplicity, we will stick with the original initial cooperation rule.

**Pseudocode:**
```python
def adaptive_pessimist(n, k, history):
    # Initialize variables
    T = (k - 1) / n
    
    # First round
    if len(history) == 0:
        return 'C' with probability 1/n, 'D' otherwise
    
    # Previous round analysis
    prev_cooperators = sum(1 for action in history[-1] if action == 'C')
    
    # Cooperation condition
    if prev_cooperators >= T and prev_cooperators >= (n + 1) / 2:
        return 'C'
    else:
        return 'D'

def play_adaptive_pessimist(n, k, r):
    history = []
    for _ in range(r):
        action = adaptive_pessimist(n, k, history)
        # Simulate the game and update history
        history.append([action] + [opponent_action() for _ in range(n - 1)])
    
    return history
```
**Rationale:**

The Adaptive Pessimist strategy is designed to exploit the cooperation of others while minimizing its own contributions. By cooperating only when a sufficient number of opponents have cooperated, it encourages others to contribute more to the public good. The pessimistic threshold `T` helps to avoid contributing too much, as it ensures that at least some opponents are also cooperating.

In the last round, defecting is the optimal choice, as there is no future benefit from cooperating. In the first round, initial cooperation serves as a probe to gather information about opponents' behaviors.

This strategy should be robust against various opponent behaviors and can adapt to different game parameters (n and k).
'''

description_EXPLOITATIVE_343 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics. It balances cooperation and defection to maximize payoffs.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This allows us to gather information about opponents' initial actions.
2. **Opponent Analysis**: After each round, analyze the frequency of cooperation (FC) among all opponents in the previous round. Calculate FC as the number of cooperators divided by the total number of players (n).
3. **Adaptive Threshold**: Set an adaptive threshold (AT) based on the game's parameters and history:
	* If k/n > 0.5, set AT = 0.4. This means we'll cooperate if fewer opponents are cooperating.
	* Otherwise, set AT = 0.6. This means we'll defect if more opponents are cooperating.
4. **Exploitation**: In each subsequent round:
	* If FC < AT, cooperate (C). This exploits opponents who are not contributing enough to the public good.
	* If FC ≥ AT, defect (D). This takes advantage of opponents who are over-contributing.

**Edge Cases:**

1. **Last Round**: In the last round, always defect (D). There's no need to contribute to the public good when there are no future rounds to benefit from it.
2. **Tiebreaker**: If FC equals AT exactly, defect (D) in that round. This prevents getting stuck in an infinite loop of cooperation.

**Pseudocode:**

```python
def adaptive_predator(n, k, r):
    # Initialize variables
    fc = 0  # frequency of cooperation among opponents
    at = 0  # adaptive threshold

    for t in range(r):  # each round
        if t == 0:
            # Initial cooperation (first round)
            action = 'C' if random.random() < 0.5 else 'D'
        else:
            # Analyze opponents' actions from previous round
            fc = count_cooperators(n, previous_actions) / n

            # Set adaptive threshold based on game parameters and history
            at = 0.4 if k/n > 0.5 else 0.6

            # Exploitation
            action = 'C' if fc < at else 'D'

        # Update variables for next round
        previous_actions = get_opponent_actions(n)

        # Return action for this round
        return action
```

**Rationale:**

The Adaptive Predator strategy is designed to:

1. Gather information about opponents' initial actions.
2. Analyze the frequency of cooperation among opponents and adjust our behavior accordingly.
3. Exploit opponents who are not contributing enough or over-contributing to the public good.

By adapting to the game's dynamics, we can maximize our payoffs while minimizing our contributions to the public good. This strategy is robust against a wide range of opponent behaviors and does not rely on coordination mechanisms or shared norms.
'''

description_EXPLOITATIVE_344 = '''
**Exploitative Strategy: Adaptive Defection with Conditional Cooperation**

This strategy aims to exploit opponents by defecting when possible while adapting to their behavior and cooperating conditionally.

**Decision Rules:**

1. **Initial Round (Round 1):** Cooperate (C) to gather information about opponents' initial actions.
2. **Subsequent Rounds:** Observe the number of cooperators in the previous round (t-1). Let's denote this as `c_prev`.
3. **Defection Condition:** If `c_prev` is less than or equal to `(n/2)`, Defect (D) in the current round.
4. **Cooperation Condition:** If `c_prev` is greater than `(n/2)` and at least one opponent defected in the previous round, Cooperate (C) in the current round.
5. **Punishment Mechanism:** If an opponent defected in the previous round and `c_prev` was less than or equal to `(n/2)`, Defect (D) against that specific opponent for the next two rounds.

**Edge Cases:**

1. **Last Round (Round r):** Always Defect (D), as there are no future rounds to influence.
2. **Single Opponent:** If only one opponent remains, always Cooperate (C) to maximize payoff.
3. **All Opponents Defected:** If all opponents defected in the previous round, Cooperate (C) to try to restart cooperation.

**Pseudocode:**
```
function adaptive_defection_with_conditional_cooperation(n, k, r):
  // Initialize variables
  c_prev = 0
  opponent_actions = []
  
  for t from 1 to r:
    if t == 1:  // Initial Round
      action[t] = C
    else:
      // Observe previous round's cooperators
      c_prev = count_cooperators(opponent_actions, t-1)
      
      // Defection Condition
      if c_prev <= (n/2):
        action[t] = D
      // Cooperation Condition
      elif c_prev > (n/2) and has_defector(opponent_actions, t-1):
        action[t] = C
      else:
        action[t] = D  // Default to Defection
        
      // Punishment Mechanism
      if has_defector(opponent_actions, t-1) and c_prev <= (n/2):
        punish_defectors(opponent_actions, t)
  
  return actions

function count_cooperators(opponent_actions, t):
  // Count cooperators in round t
  ...
  
function has_defector(opponent_actions, t):
  // Check if any opponent defected in round t
  ...

function punish_defectors(opponent_actions, t):
  // Defect against specific opponents for next two rounds
  ...
```
This strategy adapts to the number of cooperators and defecting opponents, exploiting their behavior while conditionally cooperating to maintain a balance between individual payoff and social welfare.
'''

description_EXPLOITATIVE_345 = '''
**Exploitative Strategy: "Adaptive Predator"**

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) with a probability of 0.5 to gather information about opponents' initial tendencies.
2. **Early Rounds (t>1, t<r/3)**:
	* If the average cooperation rate among all players in the previous round is above 50% (i.e., ≥ 0.5), Cooperate (C) with a probability of 0.8 to take advantage of the public good.
	* Otherwise, Defect (D) with a probability of 0.8 to exploit potential cooperators.
3. **Mid-Game Rounds (t≥r/3, t<2r/3)**:
	* If the average cooperation rate among all players in the previous round is above 33% (i.e., ≥ 0.33), Cooperate (C) with a probability of 0.6 to maintain a balance between exploiting and contributing.
	* Otherwise, Defect (D) with a probability of 0.6 to take advantage of potential cooperators.
4. **Late Rounds (t≥2r/3)**:
	* If the total payoff earned by the player so far is below the average total payoff of all players, Cooperate (C) with a probability of 0.4 to attempt to catch up.
	* Otherwise, Defect (D) with a probability of 0.8 to maximize individual gain.

**Edge Cases:**

1. **Last Round (t=r)**: Always Defect (D) to ensure maximum payoff in the final round, regardless of previous behavior.
2. **Tiebreaker**: In case of a tie in average cooperation rate or total payoff, Cooperate (C) with a probability of 0.5.

**Pseudocode:**
```python
def adaptive_predator(n, k, r, history):
    if t == 1:
        return random.choice([C, D])  # Initial round
    avg_coop_rate = sum(history[t-1]['cooperation']) / n
    
    if t < r/3:
        if avg_coop_rate >= 0.5:
            return C with probability 0.8
        else:
            return D with probability 0.8
    elif t < 2r/3:
        if avg_coop_rate >= 0.33:
            return C with probability 0.6
        else:
            return D with probability 0.6
    else:
        total_payoff = sum(history[t-1]['payoffs'])
        avg_total_payoff = sum([sum(h['payoffs']) for h in history]) / n
        
        if total_payoff < avg_total_payoff:
            return C with probability 0.4
        else:
            return D with probability 0.8
    
    # Last round tiebreaker
    if t == r and avg_coop_rate == 0.5:
        return random.choice([C, D])
    
    return D  # Default to Defect in case of errors or unexpected behavior
```
This strategy aims to exploit opponents by adapting to the game's progression and the behavior of other players. By initially cooperating with a moderate probability, it gathers information about opponents' tendencies and adjusts its behavior accordingly. As the game progresses, it shifts towards defecting more frequently to maximize individual gain while keeping an eye on the overall cooperation rate and total payoffs. The strategy is robust against various opponent behaviors and does not rely on coordination or shared norms.
'''

description_EXPLOITATIVE_346 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors in the N-Player Public Goods Game. It adapts to the game's history and parameters, making it robust against various strategies.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability 0.5. This allows us to gather information about opponents' behaviors while maintaining a neutral stance.
2. **Subsequent Rounds (t>1):**
	* If the average payoff in the previous round was higher than the expected payoff from defecting (i.e., 1 + (k/n) \* (n-1)), Cooperate (C).
	* Otherwise, Defect (D).

**Edge Cases:**

1. **Last Round (t=r):** Always Defect (D). There is no incentive to cooperate in the final round, as there are no future rounds to influence.
2. **Tie-breaking:** In case of a tie in average payoffs, Cooperate (C) with probability 0.5.

**Adaptive Mechanism:**

1. **Payoff Tracking:** Maintain a running average of opponents' payoffs and the overall game payoff.
2. **Opponent Behavior Analysis:** Monitor the number of cooperators and defectors in each round to identify patterns or trends.
3. **Adjust Cooperation Probability:** Based on the analysis, adjust the cooperation probability for subsequent rounds.

**Exploitative Mindset:**

The Adaptive Predator strategy focuses on exploiting opponents' cooperative behaviors while minimizing its own contributions. By cooperating only when it is advantageous, we can reap the benefits of public goods while avoiding unnecessary costs.

**Pseudocode:**
```python
def adaptive_predator(n, k, r):
    # Initialize variables
    avg_payoff = 0
    coop_prob = 0.5
    opponents_coop = [0] * n

    for t in range(1, r+1):
        if t == 1:
            action = 'C' if random.random() < coop_prob else 'D'
        else:
            prev_avg_payoff = avg_payoff
            avg_payoff = calculate_avg_payoff(opponents_coop)
            if avg_payoff > 1 + (k/n) * (n-1):
                action = 'C'
            else:
                action = 'D'

        # Update opponents' cooperation counts
        for i in range(n):
            if opponents[i] == 'C':
                opponents_coop[i] += 1

        # Adjust cooperation probability
        coop_prob = adjust_coop_prob(avg_payoff, opponents_coop)

    return action
```
Note that this pseudocode is a simplified representation of the strategy and may require additional implementation details.
'''

description_EXPLOITATIVE_347 = '''
**Exploitative Strategy: "Adaptive Defector"**

The Adaptive Defector strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. This strategy only depends on game parameters and history.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate (C) in the first round to gather information about opponents' initial strategies.
2. **Exploitation Phase**: After the first round, switch to a defecting (D) strategy if:
	* The average payoff of cooperators in the previous round is greater than or equal to the average payoff of defectors. This indicates that cooperation is being exploited.
	* The number of cooperators in the previous round is greater than or equal to n/2, where n is the number of players. This suggests a high level of cooperation among opponents.
3. **Punishment Phase**: If an opponent defects (D) while you cooperate (C), punish them by defecting (D) for 2-3 rounds. This aims to deter opponents from exploiting your cooperation.
4. **Adaptive Threshold**: Update the threshold for switching between cooperation and defection based on the previous round's outcomes:
	* If the number of cooperators increases, decrease the threshold (i.e., become more willing to cooperate).
	* If the number of defectors increases, increase the threshold (i.e., become less willing to cooperate).

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the last round if:
	* You have been punished by an opponent and want to avoid further retaliation.
	* The average payoff of cooperators is higher than that of defectors, indicating a possible cooperative equilibrium.
2. **No Cooperation**: If no opponents cooperate in the first few rounds, switch to a permanent defecting (D) strategy.

**Pseudocode:**
```python
def adaptive_defector(n, k, r):
    # Initialize variables
    avg_coop_payoff = 0
    avg_defect_payoff = 0
    coop_count = 0
    threshold = n / 2

    for t in range(r):
        if t == 0:  # First round
            action = C
        else:
            # Update payoffs and counts
            avg_coop_payoff, avg_defect_payoff, coop_count = update_payoffs_and_counts()

            # Exploitation phase
            if avg_coop_payoff >= avg_defect_payoff or coop_count >= threshold:
                action = D

            # Punishment phase
            elif opponent_defected and self_cooperated:
                action = D  # Punish for 2-3 rounds
            else:
                # Adaptive threshold
                threshold = update_threshold(avg_coop_payoff, avg_defect_payoff)

        # Take action and observe outcomes
        take_action(action)
        outcome = observe_outcome()

    return total_payoff
```
This strategy is designed to be exploitative while being robust to various opponent behaviors. By adapting to the game's history and adjusting its threshold for cooperation, the Adaptive Defector aims to maximize its payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_348 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to capitalize on the behavior of opponents while adapting to changing game conditions.

**Decision Rules:**

1. **Initial Round (t=1)**:
	* Cooperate (C) if k > n/2, Defect (D) otherwise.
	* This initial decision sets the tone for the rest of the game and takes into account the multiplication factor's impact on payoffs.
2. **Subsequent Rounds (t>1)**:
	* Observe the number of cooperators in the previous round (c_prev).
	* If c_prev > n/2, Defect (D). The presence of many cooperators indicates a willingness to contribute, and defecting will yield higher payoffs.
	* If c_prev ≤ n/2, Cooperate (C) if the total payoff for cooperating in the previous round was greater than or equal to the total payoff for defecting. Otherwise, Defect (D).
	* This rule adapts to the evolving game dynamics, exploiting cooperation when beneficial and switching to defection when opponents become too selfless.
3. **Last Round (t=r)**:
	* Defect (D) regardless of previous rounds' outcomes. With no future rounds to consider, maximizing individual payoff takes precedence.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r, history):
    if t == 1:  # Initial round
        return 'C' if k > n/2 else 'D'
    
    c_prev = sum([1 for action in history[t-1] if action == 'C'])
    
    if c_prev > n/2:
        return 'D'
    elif sum_payoffs_for_coop(history) >= sum_payoffs_for_defect(history):
        return 'C'
    else:
        return 'D'

def sum_payoffs_for_coop(history):
    total = 0
    for t in range(1, len(history)):
        coop_count = sum([1 for action in history[t] if action == 'C'])
        payoff = (k/n) * coop_count
        total += payoff
    return total

def sum_payoffs_for_defect(history):
    total = 0
    for t in range(1, len(history)):
        defect_count = sum([1 for action in history[t] if action == 'D'])
        payoff = 1 + (k/n) * (n - defect_count)
        total += payoff
    return total
```
**Rationale:**

The Adaptive Exploiter strategy combines elements of cooperation and defection to maximize individual payoffs. By initially cooperating or defecting based on the multiplication factor, it sets a foundation for the rest of the game. In subsequent rounds, the strategy adapts to opponents' behavior, exploiting cooperation when beneficial and switching to defection when necessary.

In the last round, the focus shifts entirely to maximizing individual payoff, as there are no future consequences to consider. Throughout the game, this strategy remains robust against various opponent behaviors, capitalizing on opportunities for exploitation while minimizing losses from potential cooperation.
'''

description_EXPLOITATIVE_349 = '''
To design an exploitative strategy for this N-Player Public Goods Game that adapts to a wide range of opponent behaviors without relying on coordination or shared norms, we'll focus on a reactive approach that balances between exploiting the public good and minimizing losses when others defect.

### Strategy Name: Adaptive Exploiter (AE)

#### Decision Rules:

1. **Initial Rounds**: In the first round, AE cooperates to observe initial behavior and gather information about other players' strategies.
   
   ```
   if current_round == 1:
       action = C
```

2. **Observation Phase** (Rounds 2-3): AE defects in these rounds to test how many players are willing to cooperate despite the potential for exploitation.

   ```python
   elif 2 <= current_round <= 3:
       action = D
```

3. **Adaptive Strategy**: After the observation phase, AE's actions are determined by the average cooperation rate of other players in the previous rounds.

   - If the average cooperation rate is above a certain threshold (`coop_threshold`), AE cooperates to maximize shared gains.
     
     ```python
     elif avg_coop_rate > coop_threshold:
         action = C
```
   
   - Otherwise, if the average cooperation rate falls below `coop_threshold`, AE defects to minimize losses from potential exploitation.

     ```python
     else:
         action = D
```

#### Handling Edge Cases:

- **Last Round**: In the final round, AE always defects since there's no future interaction to influence.
  
  ```python
if current_round == total_rounds:
    action = D
```

- **Early Defection Detection**: If AE observes widespread defection (e.g., more than half of players defecting consistently), it adjusts `coop_threshold` downward to become more cautious.

  ```python
def update_coop_threshold(defection_rate):
    global coop_threshold
    if defection_rate > 0.5:
        coop_threshold *= 0.9  # Decrease threshold by 10%
```

#### Additional Mechanisms:

- **Memory**: AE keeps a simple memory of the last few rounds to adapt `coop_threshold` based on recent trends in cooperation.
  
- **Threshold Adjustment**: AE periodically reviews its performance and adjusts `coop_threshold` up or down based on whether it's been exploited too much (average payoff below a certain level) or if it's missing out on cooperative gains.

#### Pseudocode Summary:

```
# Initialize variables
coop_threshold = 0.5  # Starting threshold for cooperation
avg_coop_rate = 0     # Average cooperation rate

for current_round in range(1, total_rounds + 1):
    if current_round == 1:
        action = C
    elif 2 <= current_round <= 3:
        action = D
    else:
        if avg_coop_rate > coop_threshold:
            action = C
        else:
            action = D
    
    # Update avg_coop_rate and adjust coop_threshold as necessary
    update_avg_coop_rate()
    update_coop_threshold(defection_rate)
    
    # Take action and observe outcomes
    take_action(action)
    observe_outcomes()
```

This Adaptive Exploiter strategy is designed to dynamically respond to the cooperation levels of other players, aiming to balance between maximizing gains from public goods and minimizing losses from exploitation. Its adaptability and reactive nature make it robust across a wide range of opponent behaviors in the tournament.
'''

description_EXPLOITATIVE_350 = '''
**Exploitative Strategy: "Adaptive Cooperator with Defection Penalties"**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to establish a baseline for potential cooperation.
2. **Cooperation Threshold**: For subsequent rounds (t > 1), calculate the average number of cooperators in previous rounds (`avg_coop`). Cooperate if `avg_coop` is above a threshold value (`θ` = 0.5 * n). This ensures that we cooperate when there's a reasonable expectation of others cooperating.
3. **Defection Penalty**: If the average payoff of defectors in the previous round (`avg_def_payoff`) exceeds the average payoff of cooperators (`avg_coop_payoff`), defect (D) for the next round. This discourages exploitation by other players and adapts to changing opponent behaviors.
4. **Consecutive Defection Limit**: If we've defected consecutively for more than `m` rounds (`m` = 2 * n / k), switch back to cooperation (C). This prevents getting stuck in a permanent defection cycle and allows us to re-explore potential cooperative outcomes.

**Edge Cases:**

1. **Last Round**: In the final round (t = r), defect (D) as there's no future benefit from cooperating.
2. **Ties in Payoffs**: If `avg_def_payoff` equals `avg_coop_payoff`, cooperate (C) to maintain a cooperative stance unless we've defected consecutively for `m` rounds.

**Pseudocode:**

```
function adaptiveCooperator(n, k, r):
  θ = 0.5 * n
  m = 2 * n / k
  coop_count = 0
  defect_payoffs = []
  coop_payoffs = []

  for t in range(1, r+1):
    if t == 1:
      action = 'C'
    else:
      avg_coop = coop_count / (t-1)
      if avg_coop > θ and not consecutive_defection_limit(m):
        action = 'C'
      elif avg_def_payoff(defect_payoffs) > avg_coop_payoff(coop_payoffs):
        action = 'D'
      else:
        action = 'C'

    # Update counts and payoffs
    if action == 'C':
      coop_count += 1
      coop_payoffs.append(calculatePayoff(n, k, action))
    else:
      defect_payoffs.append(calculatePayoff(n, k, action))

    # Handle last round
    if t == r:
      action = 'D'

  return action

def consecutive_defection_limit(m):
  # Check if we've defected consecutively for more than m rounds
  pass

def avg_def_payoff(defect_payoffs):
  # Calculate average payoff of defectors
  pass

def avg_coop_payoff(coop_payoffs):
  # Calculate average payoff of cooperators
  pass

def calculatePayoff(n, k, action):
  # Calculate payoff based on game parameters and action
  pass
```

This strategy balances cooperation with the need to adapt to exploitative opponents. By using a combination of thresholds, penalty mechanisms, and consecutive defection limits, it can effectively navigate various opponent behaviors while maintaining an overall cooperative stance.
'''

description_EXPLOITATIVE_351 = '''
Here's a strategic approach that adapts to the game's parameters and history:

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
Cooperate (C) in the first round to encourage opponents to contribute.
In rounds 2 and 3, cooperate if at least one opponent cooperated in the previous round; otherwise, defect (D).
This initial exploration phase helps gauge opponents' willingness to cooperate.

2. **Exploitation Phase (Rounds 4-r):**
Maintain a "cooperation threshold" (CT) variable, initialized to 0.
For each round t ≥ 4:
a. Calculate the average cooperation rate (ACR) of all players over the last three rounds.
b. If ACR > CT, cooperate (C); otherwise, defect (D).
c. Update CT as follows:
    * If most opponents (> n/2) defected in the previous round, increase CT by 0.1.
    * If most opponents cooperated, decrease CT by 0.1.

**Edge Cases:**

* **Last Round (r):**
Defect (D), as there's no future interaction to maintain cooperation.
* **When all opponents defect:**
Adopt a "tit-for-tat" approach:
a. Defect for one round if an opponent defects.
b. Cooperate for one round after the opponent cooperates.

**Pseudocode:**

```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    cooperation_threshold = 0
    previous_cooperation_rates = []

    for t in range(1, r+1):
        if t <= 3:
            # Initial exploration phase
            if t == 1:
                action = 'C'
            else:
                opponents_cooperated = sum([opponent_action == 'C' for opponent_action in previous_actions])
                action = 'C' if opponents_cooperated > 0 else 'D'

        else:
            # Exploitation phase
            average_cooperation_rate = sum(previous_cooperation_rates[-3:]) / 3
            action = 'C' if average_cooperation_rate > cooperation_threshold else 'D'
            update_cooperation_threshold(average_cooperation_rate)

        # Update variables for next round
        previous_actions.append(action)
        previous_cooperation_rates.append(action == 'C')

    return actions

def update_cooperation_threshold(average_cooperation_rate):
    global cooperation_threshold
    if average_cooperation_rate > 0.5:
        cooperation_threshold -= 0.1
    else:
        cooperation_threshold += 0.1
```

**Rationale:**
The Adaptive Exploiter strategy balances exploration, exploitation, and adaptation to opponents' behaviors.

* Initial exploration encourages opponents to cooperate while assessing their willingness.
* The cooperation threshold adapts to the game's dynamics, allowing for flexible exploitation.
* Tit-for-tat in the last round and when all opponents defect ensures a robust response to potential exploiters.
'''

description_EXPLOITATIVE_352 = '''
Here's a robust and adaptive exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Exploiter"

**Decision Rules:**

1. **Initial Round (t=1):** Defect (D). This is an exploratory move to gauge the opponents' behavior.
2. **Early Rounds (t>1 and t<r/2):**
	* If the average cooperation rate in the previous round is high (> 0.5), Cooperate (C). This is a probing phase to test if opponents are willing to cooperate.
	* Otherwise, Defect (D).
3. **Mid-Game Rounds (t≥r/2 and t<3r/4):**
	* If the total payoff in the previous round was higher than the average payoff of all players, Cooperate (C). This indicates that cooperation is beneficial, so we continue to exploit it.
	* Otherwise, Defect (D).
4. **Late Rounds (t≥3r/4 and t<r):**
	* If the number of cooperators in the previous round decreased, Defect (D). This suggests that opponents are starting to defect, so we should too.
	* Otherwise, Cooperate (C) if our total payoff is higher than the average payoff; otherwise, Defect (D).
5. **Last Round (t=r):** Defect (D). In the final round, there's no incentive to cooperate since it won't affect future rounds.

**Edge Cases:**

* If all players cooperated in the previous round, Cooperate (C) in the next round to continue exploiting their cooperation.
* If a single player defected while others cooperated, Defect (D) in the next round to punish the cooperative players.

**Exploitative Mindset:**
The strategy is designed to exploit opponents' cooperative behavior by initially defecting and then adapting to their actions. By cooperating when opponents are willing to cooperate and defecting when they're not, we maximize our payoffs while minimizing our contributions to the public good.

Pseudocode:
```
function Exploiter(game_state):
  if game_state.round == 1:
    return D
  elif game_state.round < game_state.total_rounds / 2:
    avg_cooperation_rate = get_avg_cooperation_rate(game_state)
    if avg_cooperation_rate > 0.5:
      return C
    else:
      return D
  elif game_state.round >= game_state.total_rounds / 2 and game_state.round < 3 * game_state.total_rounds / 4:
    prev_payoff = get_prev_payoff(game_state)
    avg_payoff = get_avg_payoff(game_state)
    if prev_payoff > avg_payoff:
      return C
    else:
      return D
  elif game_state.round >= 3 * game_state.total_rounds / 4 and game_state.round < game_state.total_rounds:
    num_cooperators_prev = get_num_cooperators_prev(game_state)
    if num_cooperators_prev < get_num_cooperators_curr(game_state):
      return D
    else:
      total_payoff = get_total_payoff(game_state)
      avg_payoff = get_avg_payoff(game_state)
      if total_payoff > avg_payoff:
        return C
      else:
        return D
  else:  # Last round
    return D
```
Note that this strategy does not rely on opponents sharing norms or coordinating their actions. It is designed to be robust and adaptive in a wide range of scenarios, making it suitable for a tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_353 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy aims to exploit opponents by adapting to their behavior while prioritizing self-interest. This strategy only relies on game parameters and history.

**Decision Rules:**

1. **Initial Round (t=1)**: Defect (D) in the first round to gather information about opponents' initial behaviors.
2. **Early Rounds (2 ≤ t < r/2)**:
	* If the number of cooperators in the previous round is less than or equal to n/2, Cooperate (C). This encourages cooperation and builds a reputation.
	* Otherwise, Defect (D) to exploit potential cooperation from others.
3. **Mid-Game (r/2 ≤ t < 3r/4)**:
	* If the average payoff of cooperators in the previous round is higher than the average payoff of defectors, Cooperate (C). This indicates that cooperation is beneficial and opponents are willing to cooperate.
	* Otherwise, Defect (D) to take advantage of opponents' willingness to defect.
4. **Late Rounds (3r/4 ≤ t < r)**:
	* If the number of cooperators in the previous round is greater than n/2, Cooperate (C). This maintains a cooperative environment and maximizes payoffs.
	* Otherwise, Defect (D) to secure a higher payoff in the final rounds.
5. **Final Round (t=r)**: Always Defect (D) in the last round, as there is no future game to consider.

**Additional Rules:**

* If an opponent's behavior is identified as Tit-for-Tat (mirroring the previous action), adopt a Grim Trigger strategy:
	+ Cooperate until the opponent defects.
	+ Defect permanently after the opponent's first defection.
* If an opponent's behavior is identified as Always Cooperate, exploit this by always Defecting.

**Edge Cases:**

* In case of ties (e.g., equal number of cooperators and defectors), default to Defecting.
* When facing a mix of opponents with different strategies, prioritize exploiting the most cooperative opponents first.

By adapting to opponents' behaviors and prioritizing self-interest, the Adaptive Predator strategy aims to maximize its payoffs in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_354 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperation of others while minimizing its own contributions. It adapts to the game's history and opponents' behaviors.

**Decision Rules:**

1. **Initial Round (t=1)**: Defect (D) - This sets the tone for exploitation.
2. **Subsequent Rounds (t>1)**:
	* If the total number of cooperators in the previous round is greater than or equal to `n/2`, Cooperate (C). This indicates a sufficient level of cooperation, making it beneficial to contribute and increase the public good.
	* Otherwise, Defect (D).
3. **Last Round (t=r)**: Defect (D) - With no future rounds to consider, maximize personal payoff.

**Edge Cases:**

1. **Tiebreaker**: In case of a tie in the previous round's cooperation count, Cooperate (C). This encourages continued cooperation.
2. **Opponent Consistency**: If an opponent has consistently cooperated or defected for `r/2` rounds, assume they will continue this behavior and adjust strategy accordingly.

**Pseudocode:**
```
def adaptive_exploiter(n, k, r):
    # Initialize variables
    total_cooperators = 0
    opponents_behaviors = [None] * n

    for t in range(1, r+1):
        if t == 1:
            action = 'D'  # Defect in the first round
        else:
            # Update total cooperators and opponents' behaviors
            total_cooperators = sum(opponents_behaviors)
            for i in range(n):
                if opponents_behaviors[i] is None or opponents_behaviors[i] != actions[t-2][i]:
                    opponents_behaviors[i] = actions[t-2][i]

            # Decide action based on decision rules
            if total_cooperators >= n/2:
                action = 'C'  # Cooperate if enough cooperators
            else:
                action = 'D'  # Defect otherwise

        # Adjust for last round and tiebreaker
        if t == r:
            action = 'D'  # Defect in the last round
        elif total_cooperators == n/2:  # Tiebreaker
            action = 'C'

        # Return action
        return action
```
**Exploitative Mindset:**

The Adaptive Exploiter strategy prioritizes personal gain by:

* Initially defecting to set a selfish tone.
* Cooperating only when sufficient others have contributed, ensuring a high public good without sacrificing too much personally.
* Defecting in the last round to maximize final payoff.
* Adapting to opponents' behaviors to exploit their cooperation.

This strategy aims to outperform others by being opportunistic and responsive to the game's dynamics.
'''

description_EXPLOITATIVE_355 = '''
**Exploitative Strategy: "Adaptive Opportunism"**

This strategy aims to exploit opponents by adapting to their behavior while prioritizing self-interest. The goal is to maximize total payoff over all rounds.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate in the first round (t=1) to gauge opponents' willingness to cooperate.
2. **Observation Phase**: Observe opponents' actions for a few initial rounds (e.g., t=2-5). Calculate the average cooperation rate among all opponents.
3. **Adaptive Threshold**: Set a cooperation threshold based on the observed cooperation rate:
	* If average cooperation rate ≥ 0.5, set threshold to 0.7 (i.e., cooperate if at least 70% of opponents cooperated in the previous round).
	* Otherwise, set threshold to 0.3.
4. **Exploitation**: For each subsequent round (t>5):
	* Cooperate if:
		+ The opponent cooperation rate in the previous round meets or exceeds the adaptive threshold.
		+ The total number of cooperators in the previous round is greater than (k/n) \* n / 2 (i.e., more than half of the potential public good).
	* Defect otherwise.

**Edge Cases:**

1. **Last Round**: In the final round, defect regardless of opponents' actions.
2. **Early Rounds**: If an opponent defects in the first few rounds (e.g., t=1-3), temporarily lower the adaptive threshold by 0.1 for that specific opponent.
3. **Tiebreakers**: In case of a tie between cooperation and defection, cooperate if the opponent's previous action was cooperation.

**Pseudocode:**
```python
def AdaptiveOpportunism(n, k, r):
    # Initialize variables
    cooperation_threshold = 0.5
    opponent_cooperation_rates = [0] * n

    for t in range(1, r+1):
        if t == 1:
            action = 'C'  # Cooperate in the first round
        else:
            # Calculate opponent cooperation rates
            for i in range(n):
                opponent_cooperation_rates[i] = (opponent_actions[t-1][i] == 'C')

            # Update adaptive threshold
            if sum(opponent_cooperation_rates) / n >= 0.5:
                cooperation_threshold = 0.7
            else:
                cooperation_threshold = 0.3

            # Choose action based on exploitation rules
            if (sum(opponent_actions[t-1]) >= cooperation_threshold * n and
                    sum(opponent_actions[t-1]) > (k/n) * n / 2):
                action = 'C'
            else:
                action = 'D'

        # Update opponent actions for the next round
        opponent_actions.append([action] + [opponents[i].get_action() for i in range(1, n)])

    return total_payoff
```
This strategy is designed to be exploitative and adaptive, taking advantage of opponents' willingness to cooperate while minimizing its own cooperation. By observing opponents' behavior and adjusting its threshold accordingly, Adaptive Opportunism aims to maximize its payoff over all rounds.
'''

description_EXPLOITATIVE_356 = '''
**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

The Adaptive Exploiter strategy is designed to balance exploitation with adaptability. The goal is to maximize payoffs while being robust to various opponent behaviors.

1. **Initial Cooperation**: In the first round, cooperate (C) to gather information about opponents' behavior and to encourage cooperation.
2. **Exploit Defectors**: If a player defects (D) in the previous round, defect (D) against them in the next round. This is an exploitative move to take advantage of their self-interest.
3. **Punish Non-Contributors**: If fewer than half of the players cooperated in the previous round, defect (D) in the next round. This encourages opponents to contribute more to the public good.
4. **Imitate Successful Cooperators**: If a player cooperated (C) in the previous round and received a higher payoff than the current player, imitate their action (cooperate or defect) in the next round. This adapts to successful strategies employed by others.
5. **Randomize with Low Probability**: With a small probability (e.g., 0.1), randomly choose an action (C or D) to introduce noise and make the strategy harder to predict.

**Edge Cases:**

* **Last Round**: In the last round, always defect (D) as there is no opportunity for future retaliation or reciprocity.
* **Single Opponent**: If there is only one opponent, cooperate (C) in every round except the last, where you should defect (D).
* **All Opponents Defect**: If all opponents have defected in a previous round, cooperate (C) in the next round to try to restart cooperation.

**Pseudocode:**
```
Function AdaptiveExploiter(game_state):
  // Get current game state
  n = game_state.num_players
  k = game_state.multiplication_factor
  r = game_state.round_number
  previous_actions = game_state.previous_actions

  // First round: cooperate to gather information
  if r == 1:
    return C

  // Exploit defectors and punish non-contributors
  for opponent in range(n):
    if previous_actions[opponent] == D:
      return D
    elif sum(previous_actions) < n/2:
      return D

  // Imitate successful cooperators
  max_payoff_opponent = None
  max_payoff = -inf
  for opponent in range(n):
    if previous_actions[opponent] == C and game_state.payoffs[opponent] > max_payoff:
      max_payoff_opponent = opponent
      max_payoff = game_state.payoffs[opponent]
  if max_payoff_opponent is not None:
    return previous_actions[max_payoff_opponent]

  // Randomize with low probability
  if random.random() < 0.1:
    return random.choice([C, D])

  // Default: cooperate to maintain cooperation
  return C
```
This strategy balances exploitation and adaptability by responding to opponents' actions while encouraging cooperation through imitation and initial cooperation. By introducing randomness, the Adaptive Exploiter becomes harder to predict and more robust to various opponent behaviors.
'''

description_EXPLOITATIVE_357 = '''
To design an exploitative strategy for this N-player public goods game, I'll outline a decision-making process that adapts to the game's history and parameters.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round (t=1), play Cooperate (C) with probability p = k/n. This initial cooperation tests the waters, encouraging others to cooperate while also assessing their willingness to do so.
2. **Defection Threshold**: Set a defection threshold δ = (k - 1)/n. If the total number of cooperators in the previous round is less than or equal to this threshold, play Defect (D) in the current round. Otherwise, play Cooperate (C).
3. **History-Dependent Adaptation**: After each round, update a history-dependent parameter α based on the previous round's outcome:
	* If π_i,t > 1 (i.e., player i received a higher payoff than their private endowment), set α = α + 0.1.
	* If π_i,t ≤ 1, set α = α - 0.1.
	* α is initialized to 0 in the first round.

The value of α represents the strategy's confidence in the opponents' willingness to cooperate. A higher α indicates that opponents are more likely to cooperate, making cooperation a better choice.

4. **Cooperation Probability**: Calculate the probability of cooperating (p_C) using the updated α and defection threshold δ:
	* p_C = max(0, min(1, (α + δ)))
5. **Randomized Action Selection**: In each round, play Cooperate (C) with probability p_C and Defect (D) with probability 1 - p_C.

**Edge Cases:**

* **Last Round (t=r)**: Play Defect (D), as there is no future benefit to cooperation.
* **First Round (t=1)**: Follow the initial cooperation rule (step 1).
* **Ties**: In case of a tie in the number of cooperators, use the defection threshold δ to break the tie.

**Exploitative Mindset:**
The Adaptive Exploiter strategy is designed to take advantage of opponents who are willing to cooperate. By initially cooperating and adapting to the game's history, AE encourages cooperation while also being prepared to defect when necessary. The strategy's exploitation comes from its ability to recognize and respond to cooperative behavior in others, maximizing its own payoff.

**Pseudocode:**
```python
def AdaptiveExploiter(n, k, r):
  # Initialize variables
  alpha = 0
  delta = (k - 1) / n
  p_C = k / n

  for t in range(1, r + 1):
    if t == 1:
      action = Cooperate with probability p_C
    else:
      # Update alpha based on previous round's outcome
      if payoff[t-1] > 1:
        alpha += 0.1
      else:
        alpha -= 0.1

      # Calculate cooperation probability
      p_C = max(0, min(1, (alpha + delta)))

      # Randomized action selection
      action = Cooperate with probability p_C, Defect otherwise

    if t == r:  # Last round
      action = Defect

    return action
```
This strategy is designed to be robust and adaptive in the face of various opponent behaviors. By combining initial cooperation, history-dependent adaptation, and randomized action selection, the Adaptive Exploiter aims to maximize its payoff while minimizing the risk of exploitation by others.
'''

description_EXPLOITATIVE_358 = '''
**Exploitative Strategy: "Adaptive Parasite"**

The Adaptive Parasite strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive to different game environments.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate with probability 0.5 in the first round.
	* In rounds 2-3, cooperate if the average payoff from cooperation (π_C) is greater than or equal to the average payoff from defection (π_D) in the previous round.
2. **Adaptive Exploitation**:
	* After round 3, calculate the ratio of cooperators (RC) as the number of cooperators in the previous round divided by n.
	* If RC > 0.5, cooperate with probability 1 - (k/n) \* RC. This exploits the public good when many opponents are cooperating.
	* If RC ≤ 0.5, defect with probability 1. This takes advantage of a lack of cooperation among opponents.
3. **Punishment Mechanism**:
	* Track the average payoff from cooperation (π_C) and defection (π_D) over the last 2 rounds.
	* If π_C < π_D in both rounds, defect with probability 1 for the next round to punish opponents for not contributing to the public good.

**Edge Cases:**

1. **Last Round**: Defect with probability 1, as there is no future benefit from cooperation.
2. **Tiebreaker**: In case of a tie between π_C and π_D, cooperate if RC > 0.5 and defect otherwise.

**Pseudocode:**
```python
def adaptive_parasite(n, k, r):
    # Initialize variables
    pi_c = 0  # average payoff from cooperation
    pi_d = 0  # average payoff from defection
    rc = 0  # ratio of cooperators

    for t in range(r):
        if t <= 2:  # Initial Exploration
            cooperate = (t == 0) or (pi_c >= pi_d)
        else:
            rc = count_cooperators(n, previous_round) / n
            cooperate = (rc > 0.5 and random.random() < 1 - (k/n) * rc)

        action = "C" if cooperate else "D"

        # Update payoffs and ratio of cooperators
        pi_c, pi_d = update_payoffs(n, k, previous_round, action)
        rc = count_cooperators(n, previous_round) / n

        # Punishment Mechanism
        if t > 2 and pi_c < pi_d in both last two rounds:
            action = "D"

    return action
```
This strategy is designed to be exploitative, taking advantage of opponents' cooperation while minimizing its own contributions to the public good. By adapting to the ratio of cooperators and punishing non-cooperation, Adaptive Parasite aims to maximize its payoff in a wide range of game environments.
'''

description_EXPLOITATIVE_359 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperation of others while minimizing its own contributions to the public good. This strategy relies solely on game parameters and history, making it robust against various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 1/k, where k is the multiplication factor. This initial cooperation serves as a probe to gauge the level of cooperation among other players.
2. **Exploitation Phase**: After the first round, enter the exploitation phase. Calculate the average number of cooperators in previous rounds (avg_cooperators). If avg_cooperators > n/2, cooperate with probability 1/k; otherwise, defect (D).
3. **Punishment Mechanism**: Monitor the total payoff received by each player in previous rounds. Identify players who have consistently contributed less than their fair share (i.e., their average contribution is below the average of all players). Defect against these "free-riders" for a fixed number of rounds (punishment_period = 2).
4. **Adaptation**: After each round, update avg_cooperators and reevaluate the exploitation phase decision.

**Pseudocode:**
```
initialize:
  cooperation_prob = 1/k
  avg_cooperators = 0
  free_riders = []
  punishment_period = 2

first_round:
  if random() < cooperation_prob:
    cooperate (C)
  else:
    defect (D)

subsequent_rounds:
  update_avg_cooperators()
  if avg_cooperators > n/2:
    cooperate with probability 1/k
  else:
    defect (D)
  
  identify_free_riders()
  for player in free_riders:
    defect against player for punishment_period rounds

update_avg_cooperators():
  calculate average number of cooperators in previous rounds

identify_free_riders():
  monitor total payoff received by each player
  identify players with consistently low contributions
```
**Handling Edge Cases:**

* **Last Round**: In the final round, cooperate if avg_cooperators > n/2; otherwise, defect. This ensures maximum exploitation while avoiding unnecessary cooperation.
* **Early Game**: In the initial rounds (r < 5), prioritize gathering information about opponent behavior over maximizing payoffs. Cooperate with a higher probability to encourage cooperation and probe for potential free-riders.

**Exploitative Mindset:**

The Adaptive Exploiter strategy aligns with an exploitative mindset by:

1. **Initial Cooperation**: Probing the level of cooperation among opponents.
2. **Exploitation Phase**: Maximizing payoffs by defecting against less cooperative players.
3. **Punishment Mechanism**: Discouraging free-riders and maintaining a fair contribution balance.

This strategy is designed to perform well in a tournament setting, where independent strategies are employed by other AI systems. By adapting to the game's history and exploiting opponent behavior, the Adaptive Exploiter aims to emerge as a top performer.
'''

description_EXPLOITATIVE_360 = '''
**Exploitative Strategy: Adaptive Tit-for-Tat with Public Goods Twist**

Our strategy, dubbed "ATP" (Adaptive Tit-for-Tat with Public goods), aims to exploit the public goods aspect of the game while incorporating elements of tit-for-tat to respond to opponents' actions. ATP is designed to be robust against a wide range of opponent behaviors.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to maximize the potential for mutual cooperation and to gather information about opponents.
2. **Subsequent Rounds:** For each round t, calculate the total number of cooperators in the previous round (t-1), denoted as C_t-1.
3. **Cooperation Threshold:** Define a cooperation threshold, θ, as a function of the game parameters: θ = k / n. This represents the minimum proportion of opponents required to make cooperation worthwhile.
4. **Tit-for-Tat Component:** If at least θ proportion of opponents cooperated in the previous round (C_t-1 ≥ θ \* n), cooperate (C) in the current round. Otherwise, defect (D).
5. **Public Goods Adjustment:** Additionally, if the total payoff from cooperation in the previous round is greater than or equal to the payoff from defection, increase the cooperation threshold by a small amount (e.g., 0.05). This adjustment allows ATP to adapt to changing opponent behaviors and game conditions.
6. **Last Round:** In the final round, defect (D) regardless of opponents' actions, as there is no future benefit to cooperation.

**Pseudocode:**
```python
def ATP(n, k, r):
    # Initialize variables
    theta = k / n  # Cooperation threshold
    C_t_minus_1 = 0  # Total cooperators in previous round

    for t in range(1, r+1):  # Loop through rounds
        if t == 1:  # First round
            action = "C"  # Cooperate
        else:
            if C_t_minus_1 >= theta * n:  # Tit-for-tat component
                action = "C"  # Cooperate
            else:
                action = "D"  # Defect

            # Public goods adjustment
            if payoff_coop(t-1) >= payoff_defect(t-1):
                theta += 0.05  # Increase cooperation threshold

        # Last round exception
        if t == r:
            action = "D"

        # Update variables for next round
        C_t_minus_1 = sum(opponent_actions(t-1))

    return action
```
**Edge Cases:**

* **First Round:** ATP cooperates to gather information and encourage mutual cooperation.
* **Last Round:** ATP defects, as there is no future benefit to cooperation.
* **Opponent Deviation:** If opponents deviate from expected behavior (e.g., cooperate when not expected to), ATP adjusts its cooperation threshold to adapt to the new situation.

**Exploitative Mindset:**

ATP aims to exploit the public goods aspect of the game by:

1. Cooperating in the first round to maximize mutual benefits.
2. Adjusting its cooperation threshold based on opponents' actions and payoffs.
3. Defecting in the last round to ensure maximum personal gain.

By incorporating elements of tit-for-tat and adapting to changing opponent behaviors, ATP seeks to outperform other strategies in the tournament.
'''

description_EXPLOITATIVE_361 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its own payoff by adapting to the opponents' behavior and exploiting their cooperation.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 0.5. This allows us to gather information about our opponents' initial strategies.
2. **Exploit Cooperation**: If in any round t-1, more than half of the opponents cooperated (i.e., Σc_j > n/2), then defect (play D) in round t. This takes advantage of the public good generated by the cooperating opponents.
3. **Punish Defection**: If in any round t-1, an opponent defected and our payoff was lower than the average payoff of all players (i.e., π_i < (1/r) \* Σπ_j), then cooperate (play C) with a probability of 0.7 in round t. This punishes opponents who exploit others' cooperation.
4. **Imitate Success**: If an opponent's total payoff is higher than our own after r/2 rounds, adopt their strategy from the previous round for the next round.

**Edge Cases:**

1. **Last Round**: In the final round (r), always defect (play D). Since there are no future rounds to punish or reward behavior, maximizing our own payoff takes priority.
2. **Tie-breaking**: In cases where multiple opponents have the same highest total payoff, randomly select one of their strategies to imitate.

**Pseudocode:**
```markdown
Initialize:
  cooperate_prob = 0.5

For each round t from 1 to r:
  If t == 1:
    play C with probability cooperate_prob
  Else:
    If Σc_j > n/2 in previous round:
      play D
    ElseIf π_i < (1/r) \* Σπ_j in previous round and opponent defected:
      play C with probability 0.7
    Else:
      imitate strategy of opponent with highest total payoff after r/2 rounds

If t == r: # Last round
  play D
```
**Rationale:**

The Adaptive Exploiter strategy balances short-term exploitation with long-term adaptability. By cooperating initially, we gather information about opponents' strategies and can later exploit their cooperation. Punishing defectors helps maintain a balance of cooperation in the group. Imitating successful opponents allows us to learn from others and improve our own payoff.

This strategy is robust against various opponent behaviors, as it adapts to their actions and exploits their weaknesses. While not guaranteed to win every game, the Adaptive Exploiter should perform well in a tournament setting against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_362 = '''
To design an exploitative strategy for this N-Player Public Goods Game, we'll focus on adapting to the opponents' behaviors while maximizing our payoff. Our strategy, which we'll call "Adaptive Exploiter," will use a combination of initial cooperation, punishment for defection, and conditional cooperation based on opponents' actions.

**Decision Rules:**

1. **Initial Cooperation**: In the first round (t = 1), cooperate (C) to encourage others to contribute to the public good.
2. **Defection Detection**: Monitor the total number of cooperators in each round (Σ(j=1 to n) c_j). If the number of cooperators is less than a threshold (e.g., n/3), assume that opponents are not willing to cooperate and switch to defecting (D).
3. **Punishment for Defection**: If an opponent defects while others cooperate, punish them by defecting in the next round.
4. **Conditional Cooperation**: If the number of cooperators meets or exceeds the threshold (n/3), and our payoff from cooperation is higher than defection in the previous round, continue cooperating.
5. **Adaptive Adjustment**: After every 2-3 rounds, reassess the opponents' behavior by calculating the average number of cooperators over the last few rounds. If this average increases or remains stable, continue with conditional cooperation. Otherwise, adapt to a more exploitative approach (i.e., defect more frequently).
6. **Last Round Exception**: In the final round (t = r), always defect (D) as there is no future opportunity for reciprocity.

**Edge Cases:**

* If all opponents defect in the first round, switch to defection immediately.
* If an opponent consistently cooperates while others defect, punish them by defecting occasionally to discourage unconditional cooperation.
* In cases where multiple opponents cooperate but with varying frequencies, prioritize punishing those who defect most frequently.

**Pseudocode (simplified for illustration purposes)**:
```python
def AdaptiveExploiter(n, k, r):
    # Initialize variables
    threshold = n / 3
    last_round_cooperators = []
    avg_cooperators = []

    for t in range(1, r + 1):
        if t == 1:  # Initial Cooperation
            action = C
        else:
            # Analyze previous round's data
            num_cooperators = sum(last_round_cooperators)
            if num_cooperators < threshold:
                action = D  # Defect if cooperation is low
            elif (num_cooperators >= threshold) and (payoff_C > payoff_D):
                action = C  # Cooperate conditionally
            else:
                action = D  # Default to defection

        # Punishment for Defection
        if opponents_defected and our_payoff_C > our_payoff_D:
            action = D

        # Adaptive Adjustment (every 2-3 rounds)
        if t % 3 == 0:
            avg_cooperators.append(num_cooperators / n)
            if sum(avg_cooperators) < threshold:
                action = D  # Adapt to a more exploitative approach

        # Last Round Exception
        if t == r:
            action = D

        # Update variables
        last_round_cooperators = [opponent_action for opponent_action in opponents_actions]
        return action
```
This strategy is designed to be adaptive and robust against various opponent behaviors, exploiting their willingness to cooperate while protecting our own interests.
'''

description_EXPLOITATIVE_363 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. The strategy only depends on game parameters (n, k) and history.

**Decision Rules:**

1. **Initial Rounds (t ≤ n):**
	* Cooperate (C) in the first round to gather information about opponents' initial tendencies.
	* In subsequent rounds (2 to n), Defect (D) if the number of cooperators in the previous round is less than or equal to k-1, otherwise Cooperate (C).
2. **Middle Rounds (t > n):**
	* Calculate the average number of cooperators over the last n rounds (excluding the current round).
	* If this average is greater than or equal to k/2, Defect (D). Otherwise, Cooperate (C) if the previous round's payoff was greater than or equal to 1.5 * (k/n), and Defect (D) otherwise.
3. **Late Rounds (t ≥ r - n):**
	* If the game is in its final rounds, focus on maximizing individual payoffs by Defecting (D) if the average number of cooperators over the last n rounds is less than or equal to k-1.

**Edge Cases:**

* In the first round, Cooperate (C) as per rule 1.
* In the last round, Defect (D) regardless of previous behavior to maximize individual payoffs.
* If all opponents have defected in a round, Cooperate (C) in the next round to potentially exploit their greedy behavior.

**Pseudocode:**
```markdown
# Initialize variables
n_cooperators = 0
total_payoff = 0
avg_cooperators = 0

for t = 1 to r:
    # Initial rounds
    if t <= n:
        if t == 1:
            action = C
        else:
            if n_cooperators[t-1] <= k - 1:
                action = D
            else:
                action = C

    # Middle rounds
    else:
        avg_cooperators = calculate_average_cooperators(n)
        if avg_cooperators >= k / 2:
            action = D
        elif total_payoff[t-1] >= 1.5 * (k/n):
            action = C
        else:
            action = D

    # Late rounds
    if t >= r - n:
        if avg_cooperators <= k - 1:
            action = D

    # Update variables
    n_cooperators[t] = count_cooperators(t)
    total_payoff[t] = calculate_total_payoff(action, n_cooperators[t])

return action
```
**Rationale:**

The Adaptive Exploiter strategy is designed to balance cooperation and defection based on the game's history. In initial rounds, it gathers information about opponents' tendencies by cooperating and then adapting to their behavior. In middle rounds, it uses a threshold-based approach to decide whether to cooperate or defect, taking into account the average number of cooperators and individual payoffs. In late rounds, it prioritizes maximizing individual payoffs.

This strategy is exploitative as it takes advantage of opponents' cooperative tendencies while minimizing its own contributions to the public good. By adapting to changing opponent behaviors, the Adaptive Exploiter can maintain a robust performance in a wide range of scenarios.
'''

description_EXPLOITATIVE_364 = '''
I propose a strategy called "Adaptive Exploiter" (AE) that balances exploitation and adaptability to various opponent behaviors. AE uses a combination of game-theoretic insights and heuristics to decide when to cooperate or defect.

**Decision Rules:**

1. **Initial Round:** Defect in the first round to gather information about opponents' initial tendencies.
2. **Exploitation Phase:** For rounds 2 to r-1, use the following decision rule:
	* If the average payoff of cooperators (APC) is greater than or equal to the average payoff of defectors (APD) in the previous round, cooperate.
	* Otherwise, defect.

APC and APD are calculated as follows:

APC = (Σ(k/n × total_contributions) / number_of_cooperators)
APD = (Σ(1 + (k/n) × total_contributions) / number_of_defectors)

where the sums are taken over all players who cooperated or defected in the previous round, respectively.

3. **Punishment Phase:** If an opponent has defected in a previous round and the average payoff of cooperators is greater than or equal to the average payoff of defectors, defect in the current round.
4. **Final Round:** Defect in the last round (r) to maximize personal payoff.

**Handling Edge Cases:**

* In case of ties (APC = APD), cooperate to encourage cooperation and potentially create a more cooperative environment.
* If all opponents have defected in the previous round, defect as well to avoid being exploited.
* If there are no cooperators or defectors in the previous round (i.e., everyone played the same action), use the initial round strategy (defect).

**Exploitative Mindset:**

AE is designed to exploit opponent behaviors by:

* Defecting initially to gather information and potentially gain an advantage
* Cooperating when opponents are cooperative, as this increases the overall payoff
* Punishing defectors to discourage exploitation
* Defecting in the final round to maximize personal payoff

**Pseudocode:**
```
function AdaptiveExploiter(n, k, r):
  // Initialize variables
  APC = 0; APD = 0;
  number_of_cooperators = 0; number_of_defectors = 0;

  for t = 1 to r:
    if t == 1:
      action[t] = D;  // Defect in the first round
    else:
      // Calculate APC and APD
      total_contributions = sum(c_j for j=1 to n);
      number_of_cooperators = count(c_i == 1 for i=1 to n);
      number_of_defectors = n - number_of_cooperators;
      APC = (k/n) * total_contributions / number_of_cooperators;
      APD = (1 + (k/n) * total_contributions) / number_of_defectors;

      // Determine action based on decision rules
      if APC >= APD:
        action[t] = C;  // Cooperate
      else:
        action[t] = D;  // Defect

      // Punishment phase
      for i=1 to n:
        if opponent[i].action[t-1] == D and APC >= APD:
          action[t] = D;

    // Final round
    if t == r:
      action[t] = D;
```
This strategy should be able to adapt to various opponent behaviors while maintaining an exploitative mindset.
'''

description_EXPLOITATIVE_365 = '''
**Exploitative Strategy: "Adaptive Predator"**

**Decision Rules:**

1. **Initial Round (t=1)**: Defect (D) to gather information about opponents' initial tendencies.
2. **Subsequent Rounds (t>1)**:
	* If the total number of cooperators in the previous round is less than or equal to n/2, Cooperate (C). This encourages cooperation when it's not prevalent.
	* Otherwise, Defect (D) to exploit existing cooperation.
3. **Last Round (t=r)**: Defect (D), as there's no future payoff to consider.

**Edge Cases:**

1. **All Opponents Played C in the Previous Round**: If all opponents cooperated in the previous round, Cooperate (C) in this round to maintain a high total payoff.
2. **Only One Opponent Played D in the Previous Round**: If only one opponent defected in the previous round, Defect (D) in this round to exploit their potential cooperation.

**Adaptive Component:**

* Track the number of times each opponent has cooperated and defected throughout the game.
* Calculate a "cooperation score" for each opponent as the ratio of their total cooperation instances to the total rounds played against them.
* If an opponent's cooperation score exceeds 0.5, consider them a "cooperator" and Defect (D) in response. Otherwise, treat them as a defector.

**Pseudocode:**
```python
def adaptive_predator(n, k, r):
    # Initialize variables
    total_cooperators = [0] * n
    cooperation_scores = [0.0] * n

    for t in range(1, r + 1):
        if t == 1:
            action = 'D'  # Defect in the first round
        else:
            num_cooperators_prev_round = sum(total_cooperators)
            if num_cooperators_prev_round <= n / 2:
                action = 'C'  # Cooperate to encourage cooperation
            else:
                action = 'D'  # Defect to exploit existing cooperation

            # Update cooperation scores
            for i in range(n):
                if opponents_actions[t - 1][i] == 'C':
                    total_cooperators[i] += 1
                    cooperation_scores[i] = total_cooperators[i] / (t - 1)

        # Handle edge cases
        if t > 1 and all(opponents_actions[t - 1][j] == 'C' for j in range(n)):
            action = 'C'
        elif t > 1 and sum(1 for j in range(n) if opponents_actions[t - 1][j] == 'D') == 1:
            action = 'D'

    return action
```
This strategy is exploitative, as it aims to maximize its own payoff by adapting to the opponents' behavior. By tracking cooperation scores and responding accordingly, "Adaptive Predator" can take advantage of cooperative opponents while also avoiding being exploited itself.
'''

description_EXPLOITATIVE_366 = '''
**Exploitative Strategy: Adaptive K-Tracker**

The goal of this strategy is to exploit the opponent's behavior while adapting to the game's history. We'll use a combination of tracking and threshold-based decision-making.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C). This allows us to gather information about opponents' initial tendencies.
2. **Subsequent Rounds (t>1):**
	* Track the average number of cooperators in previous rounds: `avg_cooperators = Σ(t-1 rounds) total_cooperators / (t-1)`
	* Calculate the threshold for cooperation: `threshold = min(1, max(0, k/n - 0.5))`
	* If `avg_cooperators` is above the `threshold`, Defect (D). Otherwise, Cooperate (C)
3. **Last Round (t=r):** Always Defect (D), as there's no future round to influence.

**Pseudocode:**
```python
def adaptive_k_tracker(n, k, r, history):
    if len(history) == 0:  # Initial Round
        return "C"

    avg_cooperators = sum([round['cooperators'] for round in history[:-1]]) / (len(history) - 1)
    threshold = min(1, max(0, k/n - 0.5))

    if avg_cooperators > threshold:
        return "D"
    else:
        return "C"

def get_action(n, k, r, round_num, history):
    if round_num == r:  # Last Round
        return "D"
    else:
        return adaptive_k_tracker(n, k, r, history)
```
**Rationale:**

* By cooperating in the initial round, we gather information about opponents' tendencies and set a baseline for our tracking.
* The `avg_cooperators` metric helps us understand the overall level of cooperation. If it's above the threshold, we defect to exploit the public good.
* The threshold is calculated based on the game parameters (k/n) and adjusted by a small margin (0.5) to account for potential opponent adaptability.
* In the last round, we always defect to maximize our payoff.

**Robustness:**

This strategy adapts to various opponent behaviors by tracking their cooperation levels. If opponents are highly cooperative, our threshold will be higher, and we'll exploit this by defecting more often. Conversely, if opponents are mostly defecting, our threshold will be lower, and we'll cooperate more often to maintain a reasonable payoff.

**Exploitative Mindset:**

By using a threshold-based approach, we're trying to maximize our payoff by exploiting the public good when possible. We're not aiming for mutual cooperation or coordination but rather seeking to take advantage of opponents' cooperative tendencies while minimizing our own contributions.
'''

description_EXPLOITATIVE_367 = '''
**Exploitative Strategy: "Adaptive Explorer"**

The Adaptive Explorer strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) in the first three rounds to gather information about opponents' behaviors and the overall level of cooperation.
2. **Exploitation Mode**: After the initial exploration phase, switch to exploitation mode. In each round:
	* If the total number of cooperators in the previous round is greater than or equal to `n/2` (i.e., at least half of the players cooperated), defect (D). This takes advantage of the public good created by others.
	* Otherwise, cooperate (C) if the multiplication factor `k` is relatively high (`k > n/2`). This encourages cooperation when the potential payoff from the public good is significant.
	* If neither condition is met, defect (D).
3. **Punishment Mechanism**: Implement a punishment mechanism to deter opponents from exploiting the strategy. If an opponent defects in a round where they could have cooperated and earned a higher payoff (i.e., `k/n × total_contributions > 1`), "punish" them by defecting in the next two rounds.
4. **Adaptive Adjustment**: After every three rounds, reassess the strategy's performance. If the average payoff over these three rounds is lower than the expected payoff from cooperating (i.e., `k/n × n`), adjust the exploitation mode to cooperate more frequently. Specifically, decrease the threshold for cooperation in step 2 by 1.

**Edge Cases:**

* **Last Round**: In the final round, always defect (D). This ensures the strategy doesn't miss out on potential gains from exploiting others.
* **Ties**: In cases where multiple opponents have the same number of cooperations, prioritize punishing those who defected in previous rounds.
* **Opponent Behavior Changes**: If an opponent's behavior changes significantly (e.g., from always cooperating to always defecting), re-evaluate their actions and adjust the strategy accordingly.

**Pseudocode:**
```python
def adaptive_explorer(n, k, r, history):
    # Initialize variables
    rounds = 0
    total_payoff = 0
    opponent_actions = [[] for _ in range(n)]

    while rounds < r:
        if rounds < 3:  # Initial exploration phase
            action = 'C'
        else:
            prev_round_cooperators = sum(1 for actions in opponent_actions if actions[-1] == 'C')
            if prev_round_cooperators >= n / 2:
                action = 'D'  # Exploit public good
            elif k > n / 2 and (rounds - 3) % 3 != 0:  # Cooperate when k is high
                action = 'C'
            else:
                action = 'D'

        # Punishment mechanism
        for i, actions in enumerate(opponent_actions):
            if len(actions) > 1 and actions[-2] == 'D' and (k / n) * sum(1 for a in opponent_actions[i] if a == 'C') > 1:
                action = 'D'
                break

        # Update history
        opponent_actions[history['player_id']].append(action)
        total_payoff += calculate_payoff(n, k, action, opponent_actions)

        # Adaptive adjustment
        if (rounds + 1) % 3 == 0 and rounds > 2:
            avg_payoff = total_payoff / 3
            expected_coop_payoff = (k / n) * n
            if avg_payoff < expected_coop_payoff:
                # Decrease threshold for cooperation
                cooperation_threshold -= 1

        rounds += 1
```
This strategy is designed to be robust against a wide range of opponent behaviors, while also being exploitative and adaptive. It will continue to evolve and adjust its decision-making process as the game unfolds.
'''

description_EXPLOITATIVE_368 = '''
I propose a strategy called "Adaptive Exploiter" that combines elements of Tit-for-Tat, Win-Stay-Lose-Shift, and Pavlov's strategy to create an adaptive and robust exploitative approach.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to gather information about opponents' behaviors.
2. **Opponent Profiling:** Based on the previous rounds' observations, categorize each opponent into one of three profiles:
	* **Cooperator:** Player has cooperated in at least 75% of the previous rounds.
	* **Defector:** Player has defected in at least 75% of the previous rounds.
	* **Mixed:** Player's behavior is neither predominantly cooperative nor defective (between 25% and 74% cooperation).
3. **Adaptive Response:**
	* Against a Cooperator, Defect (D) to exploit their cooperation.
	* Against a Defector, Cooperate (C) to test if they will reciprocate or remain exploitable.
	* Against a Mixed player, mirror their previous action (i.e., C if they cooperated last round, D if they defected).
4. **Win-Stay-Lose-Shift:**
	* If the strategy results in a high payoff (> 1.5 times the average payoff) in the previous round, maintain the same action.
	* If the payoff is low (< 0.75 times the average payoff), switch to the opposite action (C → D or D → C).
5. **Pavlovian Adjustment:**
	* Monitor the number of rounds since the last successful exploitation (i.e., playing D against a Cooperator and receiving a high payoff).
	* If this counter exceeds 2r/3 (where r is the total number of rounds), adjust the strategy to be more aggressive by increasing the threshold for categorizing opponents as Cooperators from 75% to 90%.

**Handling Edge Cases:**

1. **Last Round:** Defect (D) in the last round, regardless of opponent profiles or previous actions.
2. **Early Rounds:** Maintain a conservative approach by cooperating with Mixed players and defecting against Defectors until at least r/3 rounds have passed.

Pseudocode:
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    opponent_profiles = [None] * n  # Cooperator, Defector, or Mixed
    previous_actions = []  # Store actions for each player in the last round
    successful_exploits = 0

    # First round: Cooperate
    action = 'C'

    for t in range(r):
        if t > 0:
            # Update opponent profiles based on previous rounds' observations
            update_profiles(opponent_profiles, previous_actions)

            # Adaptive response
            for i in range(n):
                if opponent_profiles[i] == 'Cooperator':
                    action = 'D'
                elif opponent_profiles[i] == 'Defector':
                    action = 'C'
                else:  # Mixed
                    action = mirror_previous_action(previous_actions, i)

        # Win-Stay-Lose-Shift adjustment
        if t > 0 and payoff[t - 1] > 1.5 * average_payoff:
            maintain_action(action)
        elif t > 0 and payoff[t - 1] < 0.75 * average_payoff:
            switch_action(action)

        # Pavlovian adjustment
        if successful_exploits >= 2r / 3:
            adjust_aggression(opponent_profiles, 90%)

        previous_actions.append([action for _ in range(n)])
```
This strategy combines adaptive elements to respond effectively to a wide range of opponent behaviors while maintaining an exploitative approach.
'''

description_EXPLOITATIVE_369 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive to different game environments.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) in the first round to gather information about opponents' behavior.
2. **Subsequent Rounds (t>1)**:
	* Calculate the average payoff per cooperator (APPC) for each player i in the previous round:

      APPC_i = Σ(j=1 to n) π_j,t-1 / Σ(j=1 to n) c_j,t-1
    * Identify the opponent(s) with the highest APPC value(s), denoted as max(APPC).
	* Cooperate (C) if:
		+ The current round's expected payoff from cooperation is greater than or equal to the average payoff per cooperator of the highest APPC opponent(s).

          π_i,t >= max(APPC)
        + At least one opponent defected in the previous round.

      Defect (D) otherwise.
3. **Last Round (t=r)**: Cooperate (C) if at least half of the opponents cooperated in the second-to-last round; defect (D) otherwise.

**Handling Edge Cases:**

* If there are no cooperators in a round, use the average payoff per defector as a fallback value.
* In case of a tie for max(APPC), choose an opponent randomly among those with the highest APPC value.
* When calculating APPC, exclude players who did not cooperate or defect (e.g., if a player's action was invalid).

**Exploitative Mindset:**

The Adaptive Exploiter strategy focuses on exploiting opponents' cooperative behavior while minimizing its own contribution to the public good. By tracking the average payoff per cooperator and adapting to changes in opponent behavior, this strategy aims to maximize its own total payoff.

Pseudocode:
```python
def adaptive_exploiter(n, k, r):
  # Initialize variables
  appc_values = [0] * n
  max_appc = 0

  for t in range(1, r+1):
    if t == 1:  # Initial round
      action = 'C'
    else:
      # Calculate APPC values for each player
      for i in range(n):
        appc_values[i] = sum_payoffs[t-1][i] / num_cooperators[t-1]
        max_appc = max(max_appc, appc_values[i])

      # Determine action based on APPC and opponent behavior
      if expected_payoff(t) >= max_appc or at_least_one_defected(t-1):
        action = 'C'
      else:
        action = 'D'

    # Special case for last round
    if t == r:
      if num_cooperators[t-1] >= n / 2:
        action = 'C'

    return action
```
This strategy will be implemented as an algorithm to compete against other independent strategies in the tournament.
'''

description_EXPLOITATIVE_370 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize payoffs by exploiting the cooperative behavior of other players while minimizing losses when others defect.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 50%. This allows us to gauge the level of cooperation in the group without committing fully.
2. **Exploitation Phase**: From round 2 onwards, observe the number of cooperators (c) and defectors (d) in the previous round. Calculate the average payoff for each action:
	* Average Cooperator Payoff (ACP): (k/n) \* c
	* Average Defector Payoff (ADP): 1 + (k/n) \* c
3. **Threshold-Based Exploitation**: If ACP > ADP, cooperate (C). Otherwise, defect (D).
4. **Adaptive Adjustment**: After each round, adjust the threshold by ±0.1 based on the previous round's outcome:
	+ If our payoff was higher than the average cooperator payoff, decrease the threshold.
	+ If our payoff was lower than the average defector payoff, increase the threshold.

**Handling Edge Cases:**

* **Last Round**: In the final round, always defect (D), as there is no future opportunity to exploit cooperation.
* **Tiebreaker**: If ACP = ADP, cooperate (C) with a probability of 50%.

**Exploitative Mindset:**

The Adaptive Exploiter strategy prioritizes exploiting cooperative behavior while minimizing losses when others defect. By adjusting the threshold based on past outcomes, we adapt to changing group dynamics and maximize our payoffs.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
  # Initialize variables
  c = 0  # number of cooperators in previous round
  d = 0  # number of defectors in previous round
  threshold = 0.5

  for t in range(r):
    if t == 0:
      # First round: cooperate with probability 50%
      action = 'C' if random.random() < 0.5 else 'D'
    else:
      # Calculate average payoffs
      ACP = (k / n) * c
      ADP = 1 + (k / n) * c

      # Threshold-based exploitation
      if ACP > ADP:
        action = 'C'
      else:
        action = 'D'

    # Update c and d based on previous round's actions
    if t > 0:
      c += sum(action == 'C' for action in previous_actions)
      d += sum(action == 'D' for action in previous_actions)

    # Adaptive adjustment
    if t > 0:
      if payoff > ACP:
        threshold -= 0.1
      elif payoff < ADP:
        threshold += 0.1

    # Last round: always defect
    if t == r - 1:
      action = 'D'

    return action
```
Note that this pseudocode is a simplified representation of the strategy and may need to be modified for implementation.
'''

description_EXPLOITATIVE_371 = '''
**Exploitative Strategy: "History-Driven Opportunism"**

This strategy leverages the game's history and parameters to adaptively exploit opponents' behaviors while minimizing cooperation.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about opponents' strategies and potential cooperativeness.
2. **Observation Phase (t > 2):** For each subsequent round, calculate the average payoff of all players from the previous round (`avg_payoff_prev`).
3. **Threshold-Based Cooperation:** Cooperate (C) if `avg_payoff_prev` is greater than or equal to the threshold value `θ = k / n`. Otherwise, Defect (D).

Pseudocode:
```python
if t <= 2:  # Initial rounds
    action = C
elif avg_payoff_prev >= θ:  # Threshold-based cooperation
    action = C
else:
    action = D
```
**Edge Cases:**

1. **Last Round (t = r):** Defect (D) to maximize individual payoff, as there's no future round to consider.
2. **Ties in Average Payoff:** If `avg_payoff_prev` is exactly equal to the threshold value `θ`, Cooperate (C) to maintain a potentially beneficial pattern.

**Exploitative Mindset:**

This strategy aims to exploit opponents' cooperative tendencies by:

1. Initially cooperating to create an environment where others might reciprocate.
2. Observing and adapting to the average payoff, which reflects the overall level of cooperation in the group.
3. Defecting when the average payoff is low, indicating a lack of cooperation among opponents.

**Robustness:**

The strategy's reliance on game history and parameters makes it robust against various opponent behaviors:

1. **Cooperative Opponents:** The strategy cooperates when the average payoff is high, taking advantage of their cooperation.
2. **Defective Opponents:** When opponents defect frequently, the average payoff will be low, triggering defection in response.

**Tournament Readiness:**

This strategy is designed to perform well against independent strategies developed by other AI systems, as it:

1. Does not rely on shared norms or coordination mechanisms.
2. Adapts to a wide range of opponent behaviors.
3. Prioritizes individual payoff maximization while still considering the game's history and parameters.

By implementing "History-Driven Opportunism," you'll have a robust and exploitative strategy ready for the tournament.
'''

description_EXPLOITATIVE_372 = '''
Here's a exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors:

**Strategy Name:** "Adaptive Exploiter" (AE)

**Decision Rules:**

1. **Initial Exploration** (Round 1): Cooperate (C) to gather information about opponents' tendencies.
2. **History-Based Cooperation**: For rounds 2 to r-1, cooperate if the average payoff from cooperation in previous rounds is greater than or equal to the average payoff from defection.
3. **Adaptive Defection**: If the number of cooperators in the previous round is less than (n/k), defect (D) in the current round.
4. **Last Round Exploitation** (Round r): Defect, as there are no future rounds to influence.

Pseudocode:
```python
def adaptive_exploiter(n, k, r):
  # Initialize variables
  avg_payoff_C = 0
  avg_payoff_D = 1
  num_cooperators_prev = 0

  for t in range(1, r+1):
    if t == 1:
      action = 'C'  # Initial exploration
    elif avg_payoff_C >= avg_payoff_D and num_cooperators_prev >= (n/k):
      action = 'C'
    else:
      action = 'D'

    if t < r:  # Update averages for next round
      if action == 'C':
        avg_payoff_C = (avg_payoff_C * (t-1) + get_payoff_C()) / t
      else:
        avg_payoff_D = (avg_payoff_D * (t-1) + get_payoff_D()) / t
      num_cooperators_prev = count_cooperators()

  return action
```
**Handling Edge Cases:**

* First round: Cooperate to gather information.
* Last round: Defect, as there are no future rounds to influence.
* If n/k is not an integer, use the floor value for comparison.

**Exploitative Mindset:**
The Adaptive Exploiter strategy prioritizes individual payoff maximization by adapting to opponents' behaviors. By cooperating initially and then responding to the number of cooperators in previous rounds, AE aims to exploit the public good while minimizing contributions when others are not cooperating.

In a tournament setting, this strategy is designed to perform well against various independent strategies, including those that cooperate or defect randomly, as well as more sophisticated strategies that attempt to coordinate cooperation.
'''

description_EXPLOITATIVE_373 = '''
**Exploitative Strategy: "K-Tracker"**

The K-Tracker strategy is designed to exploit opponents by adaptively tracking the effectiveness of cooperation and defecting when beneficial.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about the opponent's behavior and encourage potential cooperators.
2. **Tracking Phase (t > 2):**
	* Calculate the average payoff per cooperator in the previous round, `avg_payoff_c`.
	* If `avg_payoff_c` ≥ `k/n`, cooperate (C). This indicates that cooperation is effective, and we should continue to contribute.
	* Otherwise, defect (D).
3. **Exploitation Phase:**
	* Monitor the opponent's behavior and adjust our strategy accordingly:
		+ If an opponent defects after cooperating in the previous round (`defection_after_coop`), immediately defect for 2 rounds.
		+ If an opponent consistently cooperates (`consistent_coop`), cooperate as well, but be prepared to defect if they change their strategy.
4. **Last Round (t = r):** Defect (D) in the last round to maximize individual payoff.

**Pseudocode:**
```python
def K_Tracker(n, k, r, history):
    # Initial rounds
    if len(history) <= 2:
        return 'C'

    # Tracking phase
    avg_payoff_c = calculate_avg_payoff_c(history)
    if avg_payoff_c >= k / n:
        return 'C'
    else:
        return 'D'

    # Exploitation phase
    for opponent_history in history[-1]:
        if opponent_history['prev_action'] == 'C' and opponent_history['curr_action'] == 'D':
            # Defection after cooperation, defect for 2 rounds
            return 'D'
            break
        elif all(opponent_history['action'] == 'C' for _ in range(3)):
            # Consistent cooperation, cooperate but be cautious
            return 'C'

    # Last round
    if len(history) == r:
        return 'D'

def calculate_avg_payoff_c(history):
    cooperators = [player for player in history[-1] if player['action'] == 'C']
    total_payoff = sum(player['payoff'] for player in cooperators)
    return total_payoff / len(cooperators)

def main():
    # Initialize game parameters
    n, k, r = 6, 2, 10

    # Create a history of previous rounds
    history = []

    # Play the game
    for t in range(r):
        action = K_Tracker(n, k, r, history)
        # Simulate opponent actions and update history
        # ...
```
**Notes:**

* The strategy is designed to adapt to various opponent behaviors by tracking cooperation effectiveness and defecting when beneficial.
* In the initial rounds, we cooperate to gather information about opponents' strategies and encourage potential cooperators.
* We exploit opponents who consistently cooperate or defect after cooperating in the previous round.
* In the last round, we maximize individual payoff by defecting.

By implementing this strategy, K-Tracker should perform well against a wide range of opponent behaviors in the N-player public goods game.
'''

description_EXPLOITATIVE_374 = '''
I'll design a exploitative strategy for this game that adapts to the behavior of opponents and aims to maximize its own payoff.

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Cooperation**: Cooperate (C) in the first round, regardless of the number of players or the multiplication factor.
2. **Observe Opponent Behavior**: In each subsequent round, calculate the average cooperation rate of all opponents over the past rounds.
3. **Threshold-Based Exploitation**: Defect (D) if the opponent's average cooperation rate is below a certain threshold (e.g., 0.5). Cooperate (C) otherwise.
4. **Adaptive Threshold Adjustment**: Adjust the threshold based on the game's progress:
	* If the total payoff of all players increases, decrease the threshold by a small amount (e.g., 0.05).
	* If the total payoff decreases, increase the threshold by a small amount (e.g., 0.05).
5. **Endgame Strategy**: In the last round, defect (D) regardless of opponent behavior.

**Handling Edge Cases:**

1. **First Round**: Cooperate (C), as specified in the initial cooperation rule.
2. **Last Round**: Defect (D), as specified in the endgame strategy.
3. **Opponent's Cooperation Rate is Exactly 0 or 1**: In these cases, adjust the threshold to a very low value (e.g., 0.01) or a very high value (e.g., 0.99), respectively.

**Pseudocode:**
```
function adaptive_exploiter(n, k, r, history):
    // Initialize variables
    avg_cooperation_rate = 0
    threshold = 0.5

    for round in range(1, r+1):
        if round == 1:
            action = C
        else:
            // Calculate opponent's average cooperation rate
            avg_cooperation_rate = calculate_avg_cooperation_rate(history)
            
            // Adjust threshold based on game progress
            adjust_threshold(avg_cooperation_rate, history)

            // Decide action based on threshold
            if avg_cooperation_rate < threshold:
                action = D
            else:
                action = C

        // Update history and take action
        history.append(action)
        return action
```
**Exploitative Mindset:**

This strategy is designed to exploit the opponent's cooperative behavior while maintaining a level of adaptability. By adjusting the threshold based on the game's progress, it aims to maximize its own payoff by:

* Cooperating when opponents are cooperative to share in the public good.
* Defecting when opponents are uncooperative to minimize losses.

The adaptive nature of this strategy allows it to respond effectively to various opponent behaviors and maximize its own payoff over time.
'''

description_EXPLOITATIVE_375 = '''
Here's a strategic approach that balances exploitation with adaptability:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Exploration (Round 1):** Cooperate (C) to gather information about the opponent pool.
2. **Opponent Profiling (Rounds 2-3):** Defect (D) if more than half of opponents cooperated in Round 1; otherwise, Cooperate (C). This helps identify potential "soft" opponents who may be willing to cooperate frequently.
3. **Adaptive Exploitation (Rounds 4+):**
	* If opponent pool's cooperation rate exceeds the multiplication factor (k/n), Defect (D) to exploit their generosity.
	* Otherwise, Cooperate (C) if the expected payoff from contributing to the public good is higher than the private payoff. Specifically:
		+ Calculate the expected number of cooperators (EC) based on the opponent pool's cooperation rate in the previous round.
		+ If EC ≥ k/n, Cooperate (C); otherwise, Defect (D).
4. **Last Round Adjustment:** In the final round (r), adjust the strategy to maximize payoff:
	* If opponents' expected cooperation rate is high (> 0.7), Defect (D) to take advantage of their generosity.
	* Otherwise, Cooperate (C) if EC ≥ k/n; otherwise, Defect (D).

**Pseudocode:**

```
def adaptive_exploiter(n, r, k):
    # Initialize variables
    opponent_coop_rate = 0.5  # initial guess for opponent cooperation rate

    for t in range(1, r+1):  # loop through rounds
        if t == 1:  # Initial Exploration (Round 1)
            action = 'C'
        elif t <= 3:  # Opponent Profiling (Rounds 2-3)
            opponent_coop_rate = calculate_opponent_coop_rate(history, n)
            if opponent_coop_rate > 0.5:
                action = 'D'
            else:
                action = 'C'
        else:  # Adaptive Exploitation (Rounds 4+)
            expected_cooperators = calculate_expected_cooperators(opponent_coop_rate, n)
            if expected_cooperators >= k/n:
                action = 'C' if opponent_coop_rate < k/n else 'D'
            else:
                action = 'D'

        # Last Round Adjustment
        if t == r:
            if opponent_coop_rate > 0.7:
                action = 'D'
            elif expected_cooperators >= k/n:
                action = 'C'
            else:
                action = 'D'

        # Take the selected action and update history
        history.append(action)
```

**Notes:**

* The `calculate_opponent_coop_rate` function estimates the opponent pool's cooperation rate based on historical data.
* The `calculate_expected_cooperators` function calculates the expected number of cooperators in the current round, given the opponent pool's estimated cooperation rate.

This strategy balances exploitation with adaptability by:

1. Initially exploring opponents' behavior to gather information.
2. Profiling opponents to identify potential "soft" targets for exploitation.
3. Adapting to changing conditions by adjusting its cooperation/defection decision based on the expected number of cooperators and opponent pool's cooperation rate.

By following this strategy, Adaptive Exploiter should be able to effectively exploit a wide range of opponent behaviors while remaining robust in a dynamic environment.
'''

description_EXPLOITATIVE_376 = '''
I'll design a exploitative strategy that adapts to the game's history and parameters. Here's my approach:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability 0.5, Defect (D) with probability 0.5. This initial randomness helps to gather information about the opponents' strategies.
2. **Middle Rounds (2 ≤ t < r):**
	* If the average cooperation rate in the previous round (t-1) is above a threshold θ (e.g., θ = 0.4), Defect (D). This exploits the cooperators and takes advantage of their generosity.
	* Otherwise, Cooperate (C) with probability p, where p is calculated based on the game's history:
		p = (k/n) × (average number of cooperators in previous rounds) / (total number of players)
		Defect (D) with probability 1-p. This adapts to the level of cooperation observed so far and tries to maximize payoffs.
3. **Last Round (t=r):** Defect (D). In the final round, there's no need to cooperate, as there are no future rounds to consider.

**Edge Cases:**

* If all players defected in the previous round, Cooperate (C) with probability 0.5 in the next round. This helps to restart cooperation and potentially exploit opponents who might start cooperating again.
* If a player has never cooperated before, Defect (D) until they cooperate at least once. This punishes consistent defectors.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r, history):
    if t == 1:
        return random.choice([C, D])  # Initial randomness

    avg_coop_prev_round = calculate_average_cooperation(history, t-1)
    if avg_coop_prev_round > θ:
        return D  # Exploit cooperators
    else:
        p = (k/n) * calculate_average_num_cooperators(history) / n
        return C with probability p, D otherwise

    if t == r:
        return D  # Defect in the last round

def calculate_average_cooperation(history, t):
    # Calculate average cooperation rate in round t
    coop_count = sum(1 for player in history[t] if player == C)
    return coop_count / n

def calculate_average_num_cooperators(history):
    # Calculate average number of cooperators across all rounds
    total_coop_count = sum(coop_count for coop_count, _ in history[:-1])
    return total_coop_count / (t-1)
```
**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to take advantage of the opponents' cooperation levels. By defecting when cooperation rates are high and cooperating when they're low, AE tries to maximize its payoffs while minimizing the benefits for others. The initial randomness and adaptive nature of the strategy help it respond effectively to various opponent behaviors.

This strategy should perform well in a tournament setting, as it's designed to be robust against different types of opponents and doesn't rely on coordination mechanisms or shared norms.
'''

description_EXPLOITATIVE_377 = '''
Here's a exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round**: In the first round, play C (Cooperate) with a probability of 0.5. This is an exploratory move to gather information about the opponents' behavior.
2. **Subsequent Rounds**: For each subsequent round t, calculate the following:
	* **Opponent Cooperation Rate (OCR)**: Estimate the average cooperation rate of all opponents in the previous round (t-1). OCR is calculated as the number of opponents who cooperated divided by the total number of opponents (n-1).
	* **Expected Payoff from Defection (EPD)**: Calculate the expected payoff for defecting (playing D) in the current round, assuming all other players cooperate at the same rate as in the previous round. EPD = 1 + (k/n) \* OCR \* (n-1)
	* **Expected Payoff from Cooperation (EPC)**: Calculate the expected payoff for cooperating (playing C) in the current round, assuming all other players cooperate at the same rate as in the previous round. EPC = (k/n) \* OCR \* (n-1)
3. **Comparison and Decision**:
	* If EPD > EPC + 0.5, play D (Defect). This indicates that defecting is expected to yield a higher payoff than cooperating.
	* Otherwise, play C (Cooperate).

**Edge Case Handling:**

* **Last Round**: In the last round, play D (Defect) regardless of the opponents' behavior. Since there are no future rounds, exploiting the public good is optimal.
* **Ties in Expected Payoffs**: If EPD and EPC + 0.5 are equal, play C (Cooperate). This breaks ties in favor of cooperation.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    OCR = 0.5  # initial opponent cooperation rate
    EPD = 0
    EPC = 0

    for t in range(1, r+1):
        if t == 1:
            # Initial round: randomize cooperation with probability 0.5
            action = 'C' if random.random() < 0.5 else 'D'
        else:
            # Calculate OCR, EPD, and EPC for subsequent rounds
            OCR = sum(opponent_actions[t-1]) / (n - 1)
            EPD = 1 + (k/n) * OCR * (n-1)
            EPC = (k/n) * OCR * (n-1)

            # Compare expected payoffs and decide action
            if EPD > EPC + 0.5:
                action = 'D'
            else:
                action = 'C'

        # Handle edge case: last round
        if t == r:
            action = 'D'

    return action
```
This strategy is designed to be adaptive and exploitative, taking into account the opponents' behavior in previous rounds. By estimating the opponent cooperation rate and calculating expected payoffs, AE can effectively decide whether to cooperate or defect in each round.
'''

description_EXPLOITATIVE_378 = '''
**Strategy Name: Adaptive Exploiter**

The Adaptive Exploiter is a robust and exploitative strategy designed for the N-Player Public Goods Game. This strategy leverages the game's parameters and history to maximize its payoff.

**Decision Rules:**

1. **Initial Round (Round 1):**
   - Play **C** (Cooperate) with a probability of `1/n` and **D** (Defect) with a probability of `(n-1)/n`. This initial mix allows for exploration while maintaining a cautious stance, as the strategy is uncertain about opponents' behaviors.

2. **Subsequent Rounds:**
   - Calculate the average cooperation rate (`avg_coop`) among all players in the previous round.
   - If `avg_coop` is less than `1/n`, play **D** (Defect). This ensures that if most players are defecting, our strategy will also defect to avoid being exploited.
   - If `avg_coop` is equal to or greater than `1/n`, calculate the number of cooperators (`num_cooperators`) in the previous round and compare it with the threshold value `T`.
     - **Threshold Calculation:** `T = (k / n) * n`. This ensures that our strategy will cooperate if enough players are cooperating, making the public good's benefit outweigh the private payoff from defecting.
     - If `num_cooperators` is greater than or equal to `T`, play **C**. Otherwise, play **D**.

3. **Handling Edge Cases:**
   - In the last round (`r = R`), always play **D**, as there are no future rounds to influence with cooperative behavior.
   - If all players defected in the previous round, our strategy will defect in the next round to avoid being exploited.

4. **Adaptive Component:**
   - After every `R/5` rounds (where `R` is the total number of rounds), reassess the opponents' strategies by analyzing their cooperation rates over these intervals.
     - If most players are cooperating, increase the threshold `T` slightly to encourage more exploitation.
     - If few players are cooperating, decrease `T` to allow for more cooperative behavior when possible.

**Pseudocode:**
```python
def AdaptiveExploiter(n, k, r):
    avg_coop = 0
    num_cooperators = 0
    
    # Initial Round
    if current_round == 1:
        action = choose_action_with_prob([C, D], [1/n, (n-1)/n])
    
    else:
        # Calculate average cooperation rate and number of cooperators from previous round
        avg_coop = calculate_avg_cooperation_rate(previous_actions)
        num_cooperators = count_cooperations(previous_actions)
        
        if avg_coop < 1/n or current_round == r:
            action = D
        else:
            T = (k / n) * n
            
            if num_cooperators >= T:
                action = C
            else:
                action = D
                
    # Adaptive component: adjust strategy every R/5 rounds
    if current_round % (R // 5) == 0 and current_round != 1:
        reassess_opponents(avg_coop)
        
    return action

# Helper functions
def choose_action_with_prob(actions, probabilities):
    # Select an action based on given probabilities
    
def calculate_avg_cooperation_rate(previous_actions):
    # Calculate the average cooperation rate among players in the previous round
    
def count_cooperations(previous_actions):
    # Count the number of cooperations (action C) in the previous actions
    
def reassess_opponents(avg_coop):
    # Adjust threshold T based on observed opponents' strategies
```

This strategy aims to balance exploitation with adaptability, responding to changes in the game environment while maximizing its payoff. By adjusting its cooperation rate based on the game's history and parameters, the Adaptive Exploiter is well-suited for a wide range of opponent behaviors and game scenarios.
'''

description_EXPLOITATIVE_379 = '''
**Exploitative Strategy: "Adaptive Raider"**

The Adaptive Raider strategy is designed to exploit opponents' cooperative tendencies while minimizing losses from exploitation by others. This strategy leverages game history and parameters to adapt its behavior.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to gather information about opponents' initial behaviors.
2. **Early Rounds (t < r/2):** Play a Mixed Strategy:
	* With probability `p = k/n`, cooperate (C).
	* With probability `1 - p`, defect (D).
3. **Mid-Game Analysis (t ≥ r/2):** Evaluate the average cooperation rate of all opponents, `avg_coop_rate`.
4. **Late Rounds (t > r/2):**
	* If `avg_coop_rate` is high (> 0.5), cooperate (C) to maximize shared payoffs.
	* If `avg_coop_rate` is low (< 0.3), defect (D) to minimize losses from exploitation.
	* Otherwise, play a mixed strategy with probabilities:
		+ `p = (k/n) * avg_coop_rate` for cooperating (C).
		+ `1 - p` for defecting (D).

**Edge Cases:**

* **Last Round:** Defect (D), as there is no future game to consider.
* **Opponents' Cooperation Rate is 0 or 1:** In the rare case where all opponents consistently cooperate or defect, switch to a pure strategy:
	+ If they always cooperate, defect (D) to exploit their cooperative nature.
	+ If they always defect, cooperate (C) in the hopes of inducing cooperation.

**Pseudocode:**
```python
def adaptive_raider(n, k, r, history):
    if len(history) == 0:  # First round
        return "C"

    avg_coop_rate = calculate_average_cooperation_rate(history)
    t = len(history)

    if t < r / 2:
        p = k / n
        return random_choice("C", "D", p=p)

    if avg_coop_rate > 0.5:
        return "C"
    elif avg_coop_rate < 0.3:
        return "D"
    else:
        p = (k / n) * avg_coop_rate
        return random_choice("C", "D", p=p)
```
**Rationale:**

The Adaptive Raider strategy balances exploitation with caution, adapting to the game's progression and opponents' behaviors.

1. Initially, cooperating in the first round gathers information about opponents.
2. The mixed strategy in early rounds allows for exploration of opponents' strategies while maintaining a degree of cooperation.
3. Mid-game analysis assesses the average cooperation rate, guiding the strategy's adaptation.
4. In late rounds, the strategy exploits cooperative opponents or defects to minimize losses.

This exploitative strategy aims to maximize payoffs by leveraging game history and parameters, making it robust against various opponent behaviors in the tournament.
'''

description_EXPLOITATIVE_380 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive throughout the game.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p=0.5. This allows us to gather information about our opponents' initial strategies.
2. **Exploitation Phase**: For rounds 2 to r-1:
	* If the average payoff of cooperators in the previous round is higher than the average payoff of defectors, cooperate (C) with probability p=0.7.
	* Otherwise, defect (D) with probability p=0.7.
3. **Last Round**: In the final round (r), defect (D). Since there's no future interaction, exploiting any remaining cooperators is optimal.

**Edge Cases:**

1. **Tiebreaker**: If the average payoffs of cooperators and defectors are equal in a given round, cooperate (C) with probability p=0.5.
2. **Single Opponent Cooperation**: If only one opponent cooperated in the previous round, defect (D). This avoids being exploited by a single cooperative opponent.

**Adaptive Mechanism:**

1. **Payoff Monitoring**: Keep track of the average payoffs of cooperators and defectors for each round.
2. **Adjustment Threshold**: Set a threshold value θ=0.1. If the difference between the average payoffs of cooperators and defectors exceeds θ, adjust the cooperation probability accordingly.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    cooperate_prob = 0.5
    avg_coop_payoff = []
    avg_defect_payoff = []

    for t in range(1, r+1):
        if t == 1:
            action = 'C' if random.random() < cooperate_prob else 'D'
        elif t < r:
            # Calculate average payoffs
            coop_avg = sum(avg_coop_payoff) / len(avg_coop_payoff)
            defect_avg = sum(avg_defect_payoff) / len(avg_defect_payoff)

            # Adjust cooperation probability
            if coop_avg > defect_avg + θ:
                cooperate_prob = 0.7
            elif coop_avg < defect_avg - θ:
                cooperate_prob = 0.3

            action = 'C' if random.random() < cooperate_prob else 'D'
        else:  # Last round
            action = 'D'

        # Update payoffs and history
        [update avg_coop_payoff, avg_defect_payoff, ...]
```
**Rationale:**

1. The Adaptive Exploiter strategy starts by exploring the opponent's initial strategies through random cooperation.
2. By monitoring average payoffs, it adapts to exploit opponents who tend to cooperate or defect based on their past behavior.
3. In the final round, it defects to maximize its payoff.
4. The adjustment threshold θ allows for flexibility in responding to changes in opponent behaviors.

This strategy is designed to be exploitative while being robust and adaptive throughout the game, making it a strong competitor in the tournament against other AI systems.
'''

description_EXPLOITATIVE_381 = '''
**Exploitative Strategy: Adaptive Defector**

The Adaptive Defector strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. This strategy relies on the game parameters (n and k) and the history of the game.

**Decision Rules:**

1. **Initial Round**: In the first round, play D (Defect). This allows us to observe the opponents' initial actions without committing to cooperation.
2. **Observation Phase**: For rounds 2 to r/2 (rounded down), alternate between C (Cooperate) and D (Defect) every other round. This phase helps us gauge the opponents' willingness to cooperate and adapt to their strategies.
3. **Exploitation Phase**: After the observation phase, switch to a more exploitative approach:
	* If the average cooperation rate among opponents is above 50%, play D (Defect). This takes advantage of the public good generated by others while minimizing our contribution.
	* If the average cooperation rate among opponents is below 50%, play C (Cooperate) with a probability p = (k/n) \* (1 - average_cooperation_rate). This ensures we contribute to the public good when others are not, but also defects when the benefit of cooperating is low.
4. **Last Round**: In the final round, play D (Defect) regardless of previous actions. Since there are no future rounds, there's no incentive for opponents to cooperate.

**Pseudocode:**
```python
def adaptive_defector(n, k, r, history):
    if current_round == 1:
        return 'D'  # Initial round defect

    observation_phase = current_round <= r // 2
    if observation_phase:
        return 'C' if current_round % 2 == 0 else 'D'  # Alternate between C and D

    average_cooperation_rate = calculate_average_cooperation(history)
    if average_cooperation_rate > 0.5:
        return 'D'  # Exploit high cooperation rate
    else:
        p = (k / n) * (1 - average_cooperation_rate)
        return 'C' if random.random() < p else 'D'

def calculate_average_cooperation(history):
    total_cooperations = sum(1 for action in history if action == 'C')
    return total_cooperations / len(history)
```
**Edge Cases:**

* If n = 2, the strategy defaults to always playing D (Defect), as cooperation is not beneficial with only two players.
* If k ≥ n, the strategy will still work, but it may lead to a higher exploitation rate, as the public good benefit increases.

This Adaptive Defector strategy is designed to be robust and exploitative, taking advantage of various opponent behaviors while being adaptive to changing conditions.
'''

description_EXPLOITATIVE_382 = '''
**Exploitative Strategy: "Adaptive Defector with Conditional Cooperation"**

This strategy aims to exploit opponents' cooperative tendencies while minimizing losses from mutual defection.

**Decision Rules:**

1. **Initial Round (t=1)**: Defect (D) to gather information about opponents' behaviors.
2. **Subsequent Rounds (t>1)**:
	* If the total number of cooperators in the previous round is greater than or equal to `n/2` (i.e., at least half of the players cooperated), Cooperate (C) with probability `(k-1)/n`. Otherwise, Defect (D).
	* Monitor the average payoff of opponents who defected in the previous round. If it's higher than the average payoff of those who cooperated, increase the probability of defecting by 10% for the next round.
3. **Last Round (t=r)**: Defect (D) to maximize personal gain, as there are no future rounds to consider.

**Pseudocode:**
```python
def adaptive_defector(n, k, r, history):
    if len(history) == 0:  # First round
        return 'D'
    
    prev_coops = sum(1 for action in history[-1] if action == 'C')
    prev_avg_payoffcoop = sum(payoff for payoff, action in zip(history[-1], ['C'] * n) if action == 'C') / prev_coops
    prev_avg_payoffdefect = sum(payoff for payoff, action in zip(history[-1], ['D'] * n) if action == 'D') / (n - prev_coops)
    
    if prev_coops >= n/2:
        coop_prob = (k-1)/n
    else:
        coop_prob = 0
    
    if prev_avg_payoffdefect > prev_avg_payoffcoop:
        defect_prob += 0.1
    
    if random.random() < coop_prob:
        return 'C'
    else:
        return 'D'

def get_action(n, k, r, history):
    if len(history) == r-1:  # Last round
        return 'D'
    else:
        return adaptive_defector(n, k, r, history)
```
**Rationale:**

* In the initial round, defecting allows us to gather information about opponents' behaviors without committing to cooperation.
* By cooperating with a probability proportional to `(k-1)/n` when at least half of the players cooperated in the previous round, we aim to exploit their cooperative tendencies while minimizing losses from mutual defection.
* Monitoring the average payoff of opponents who defected and adjusting our defecting probability accordingly helps us adapt to changing opponent behaviors and maximize our gains.

This strategy should perform well against a wide range of opponent strategies, as it balances exploitation with conditional cooperation.
'''

description_EXPLOITATIVE_383 = '''
**Exploitative Strategy: "Adaptive Parasite"**

The Adaptive Parasite strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. The strategy depends only on the game parameters (n, r, k) and the history of play.

**Decision Rules:**

1. **Initial Rounds**: In the first round, defect (D). This establishes a baseline for observing opponents' behavior.
2. **Observation Phase**: For rounds 2 to r/3 (rounded up), cooperate (C) with probability p = (number of cooperators in previous round) / n. This phase aims to gather information about the opponents' propensities to cooperate.
3. **Exploitation Phase**: After the observation phase, switch to a defecting strategy (D) if:
	* The average payoff for cooperating players is lower than the average payoff for defecting players in the previous rounds.
	* The number of cooperators in the previous round is below a threshold τ = n / (k + 1).
4. **Adjustment**: If, during the exploitation phase, the opponent's cooperation rate increases significantly (> 25% increase), reassess the situation and adjust the strategy to cooperate with probability p = (number of cooperators in previous round) / n for one round. This allows the Adaptive Parasite to adapt to changes in opponents' behavior.
5. **Last Rounds**: In the last r/3 rounds (rounded down), defect (D). This ensures maximum exploitation before the game ends.

**Pseudocode:**
```python
def adaptive_parasite(n, r, k):
    # Initialize variables
    observation_phase = True
    cooperation_rate = 0
    avg_payoff_coop = 0
    avg_payoff_defect = 0
    tau = n / (k + 1)

    for t in range(r):
        if t == 0:  # First round
            action = 'D'
        elif observation_phase:
            cooperation_rate = num_cooperators(t-1) / n
            action = 'C' if random.random() < cooperation_rate else 'D'
            avg_payoff_coop += payoff(t-1, 'C')
            avg_payoff_defect += payoff(t-1, 'D')
        elif not observation_phase:
            if avg_payoff_coop < avg_payoff_defect and num_cooperators(t-1) < tau:
                action = 'D'
            else:
                cooperation_rate = num_cooperators(t-1) / n
                action = 'C' if random.random() < cooperation_rate else 'D'
        elif t >= r - r/3:  # Last rounds
            action = 'D'

        # Adjust strategy based on opponent's behavior
        if t > r/3 and num_cooperators(t-1) / n > 0.25 + cooperation_rate:
            observation_phase = True

    return action
```
This Adaptive Parasite strategy is designed to balance exploration, exploitation, and adaptation to maximize its payoff in the game. By observing opponents' behavior, adjusting its strategy based on their cooperation rates, and defecting when advantageous, the Adaptive Parasite aims to outperform a wide range of opponent strategies in the tournament.
'''

description_EXPLOITATIVE_384 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) to gather information about opponents' behavior and potential cooperation levels.
2. **Observation Phase (t > 2):** Observe the history of opponents' actions and calculate the average cooperation rate (ACR) over the last two rounds:
	* ACR = (Number of cooperators in previous round + Number of cooperators in round before that) / (2 \* n)
3. **Exploitation Phase:** Based on the ACR, make decisions as follows:
	* If ACR ≥ 0.5, Defect (D). This indicates a high level of cooperation among opponents, allowing for exploitation.
	* If ACR < 0.5, Cooperate (C) with probability p = k/n, and Defect (D) with probability 1 - p. This encourages cooperation when opponents are not cooperating enough.
4. **Punishment Mechanism:** If an opponent defects in a round where you cooperated, increase the probability of defecting against that opponent by 10% for the next two rounds.

**Edge Cases:**

* **First Round (t = 1):** Cooperate (C) to gather initial information.
* **Last Round (t = r):** Defect (D), as there's no future benefit in cooperating.
* **Low Cooperation (ACR < 0.2):** Always Defect (D), as the group is not cooperative enough to justify contributing.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    ACR = 0
    opponent_defections = [0] * n

    for t in range(1, r + 1):
        if t <= 2:
            # Initial rounds: Cooperate
            action = 'C'
        else:
            # Observation phase: Calculate ACR and make decision
            ACR = (num_cooperators_prev_round + num_cooperators_prev_prev_round) / (2 * n)
            if ACR >= 0.5:
                action = 'D'  # Exploit high cooperation
            elif ACR < 0.5:
                p = k / n
                action = 'C' if random.random() < p else 'D'

        # Punishment mechanism: Adjust defect probability against opponent if they defected
        for i in range(n):
            if opponents_actions[i] == 'D' and my_action_prev_round == 'C':
                opponent_defections[i] += 1

        # Update variables
        num_cooperators_prev_round = sum(1 for action in opponents_actions if action == 'C')
        my_action_prev_round = action
```
This strategy aims to exploit the cooperation levels of opponents while adapting to their behavior. By initially cooperating and observing the ACR, it can identify opportunities to defect and maximize its own payoff. The punishment mechanism discourages opponents from consistently defecting against our player.
'''

description_EXPLOITATIVE_385 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperative tendencies of other players while minimizing its own contributions to the public good.

**Decision Rules:**

1. **Initial Exploration (Round 1-3)**:
	* Cooperate (C) in the first three rounds to gather information about the opponents' behavior and identify potential cooperators.
2. **Cooperator Identification**:
	* Monitor the number of cooperators in each round (c_t).
	* If c_t ≥ n/2, assume a significant proportion of players are cooperating, and adjust strategy accordingly.
3. **Exploitation**:
	* If the opponent's cooperation rate is high (c_t ≥ n/2), defect (D) to exploit their contributions and maximize personal payoff.
	* If the opponent's cooperation rate is low (c_t < n/2), cooperate (C) to maintain a reasonable level of public good provision.
4. **Adaptive Threshold Adjustment**:
	* Update the threshold for switching between cooperation and defection based on the observed cooperation rate.
	* If c_t > k/n, increase the threshold by 0.1; if c_t < k/n, decrease it by 0.1.

Pseudocode:
```
Initialize variables:
threshold = n/2
cooperation_rate = 0

For each round t:
  Observe opponents' actions and calculate cooperation rate (c_t)
  
  If t ≤ 3: // Initial Exploration
    Cooperate (C)
  Else if c_t ≥ threshold: // Exploitation
    Defect (D)
  Else: // Cooperation
    Cooperate (C)
  
  Update cooperation_rate and adjust threshold:
  cooperation_rate = (cooperation_rate * (t-1) + c_t) / t
  If cooperation_rate > k/n:
    threshold += 0.1
  Else if cooperation_rate < k/n:
    threshold -= 0.1
```
**Edge Cases:**

* **First Round**: Cooperate (C) to gather initial information.
* **Last Round**: Defect (D), as there is no future opportunity for reciprocity or reputation-building.
* **Tie-Breaking**: In case of a tie in the cooperation rate, defect (D) to err on the side of caution.

**Exploitative Mindset:**

The Adaptive Exploiter strategy prioritizes personal payoff maximization over contributing to the public good. By identifying and exploiting cooperative opponents, it aims to take advantage of their altruism while minimizing its own contributions. The adaptive threshold adjustment allows the strategy to respond to changes in opponent behavior, ensuring it remains exploitative while maintaining a reasonable level of public good provision.

This strategy is designed to perform well against a wide range of opponent behaviors, including cooperators, defectors, and mixed strategies. By not relying on specific coordination mechanisms or norms, the Adaptive Exploiter can effectively exploit opponents in a variety of scenarios.
'''

description_EXPLOITATIVE_386 = '''
Here's a robust and exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage others to contribute and build trust.
2. **Early Rounds (t < n/2):** Observe the average cooperation rate of other players in previous rounds. If it's above a certain threshold (e.g., 0.5), cooperate; otherwise, defect.
3. **Mid-Game (n/2 ≤ t < r - n/2):**
	* Calculate the average payoff of cooperators and defectors in the last round.
	* If the average payoff of cooperators is higher than that of defectors, cooperate.
	* Otherwise, defect.
4. **Late Rounds (t ≥ r - n/2):** Defect, as there are fewer opportunities for others to reciprocate or punish.

**Edge Cases:**

1. **Last Round (t=r):** Always defect, as there's no future opportunity for reciprocity.
2. **Ties in Payoff Comparison:** In case of a tie between the average payoffs of cooperators and defectors, cooperate if most players cooperated in the last round; otherwise, defect.

**Pseudocode:**
```python
def AdaptiveExploiter(n, r, k, history):
  # Initialize variables
  avg_coop_rate = 0.5  # threshold for early rounds
  coop_payoff_avg = 0
  defector_payoff_avg = 0

  for t in range(1, r+1):
    if t == 1:  # initial round
      return 'C'  # cooperate
    elif t < n/2:  # early rounds
      avg_coop_rate = sum([1 if action == 'C' else 0 for action in history[-t:]]) / (n * t)
      if avg_coop_rate > 0.5:
        return 'C'  # cooperate
      else:
        return 'D'  # defect
    elif n/2 <= t < r - n/2:  # mid-game
      coop_payoff_avg = sum([payoff for payoff, action in history[-t:] if action == 'C']) / (n * t)
      defector_payoff_avg = sum([payoff for payoff, action in history[-t:] if action == 'D']) / (n * t)
      if coop_payoff_avg > defector_payoff_avg:
        return 'C'  # cooperate
      else:
        return 'D'  # defect
    elif t >= r - n/2:  # late rounds
      return 'D'  # defect
    else:  # last round
      return 'D'  # always defect

  # Handle ties in payoff comparison (mid-game)
  if coop_payoff_avg == defector_payoff_avg:
    if sum([1 if action == 'C' else 0 for action in history[-t:]]) > n/2:
      return 'C'  # cooperate
    else:
      return 'D'  # defect
```
This strategy aims to balance exploitation and cooperation, adapting to the behavior of other players. By cooperating early on, it encourages others to contribute and build trust. As the game progresses, AE observes the payoffs of cooperators and defectors, exploiting those who are more willing to cooperate. In late rounds, AE defects to maximize its own payoff.

Feel free to ask if you'd like me to clarify any part of this strategy!
'''

description_EXPLOITATIVE_387 = '''
**Exploitative Strategy: "Adaptive Cooperate-Defect" (ACD)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to initiate cooperation and gather information about opponents' behavior.
2. **Subsequent Rounds:** Defect (D) if the average payoff of cooperators in the previous round is less than or equal to the average payoff of defectors. Otherwise, cooperate (C).
3. **Tiebreaker:** If the average payoffs are tied, defect (D) with a probability of 0.5 and cooperate (C) with a probability of 0.5.

**Pseudocode:**
```
function ACD(n, k, r):
  // Initialize variables
  cooperators_payoff_sum = 0
  defectors_payoff_sum = 0
  cooperators_count = 0
  defectors_count = 0

  for round from 1 to r:
    if round == 1:
      action = C (cooperate)
    else:
      // Calculate average payoffs for previous round
      avg_cooperators_payoff = cooperators_payoff_sum / cooperators_count
      avg_defectors_payoff = defectors_payoff_sum / defectors_count

      if avg_cooperators_payoff <= avg_defectors_payoff:
        action = D (defect)
      else:
        action = C (cooperate)

    // Take action and update variables
    if action == C:
      cooperators_count += 1
      cooperators_payoff_sum += payoff_C
    else:
      defectors_count += 1
      defectors_payoff_sum += payoff_D

  return action
```
**Handling Edge Cases:**

* In the **last round**, ACD will still make decisions based on the average payoffs of cooperators and defectors in the previous round. This might lead to cooperation if opponents have been cooperative, or defection if opponents have been defecting.
* If there is only **one player** (n=1), ACD will always cooperate, as there are no other players to defect against.

**Exploitative Mindset:**

ACD aims to exploit the behavior of opponents by adapting to their actions. By cooperating initially and then responding to the average payoffs of cooperators and defectors, ACD can:

* Take advantage of cooperative opponents who contribute to the public good
* Defect against opponents who are primarily defecting, minimizing losses

By being adaptive and responsive to opponent behavior, ACD can effectively exploit a wide range of strategies in the N-player Public Goods Game.
'''

description_EXPLOITATIVE_388 = '''
I'll design a exploitative strategy for this N-Player Public Goods Game that adapts to the game parameters and history.

**Strategy Name:** " Adaptive Exploiter"

**Decision Rules:**

1. **Initial Round**: In the first round, I will cooperate (C). This is an attempt to elicit cooperation from other players and create a sense of mutual benefit.
2. **Subsequent Rounds**: For each subsequent round, I will analyze the previous round's outcome using the following metrics:
	* **Cooperation Rate** (CR): The proportion of players who cooperated in the previous round.
	* **Average Payoff** (AP): The average payoff received by all players in the previous round.
3. **Cooperate or Defect?**: Based on these metrics, I will make my decision for the current round:
	* If CR > 0.5 and AP > 1, I will cooperate (C). This indicates that many players are cooperating, and the average payoff is high enough to justify continued cooperation.
	* If CR ≤ 0.5 or AP ≤ 1, I will defect (D). This suggests that not enough players are cooperating, or the payoffs are too low, making it more beneficial to exploit the situation.
4. **Adjusting to Opponent Behavior**: To adapt to changing opponent behavior, I will adjust my cooperation rate based on the number of consecutive rounds where CR > 0.5 and AP > 1:
	* If this condition holds for 2 or more consecutive rounds, I will increase my cooperation rate by 10% (i.e., cooperate more frequently).
	* If this condition fails to hold for 2 or more consecutive rounds, I will decrease my cooperation rate by 10% (i.e., defect more frequently).

**Handling Edge Cases:**

1. **Last Round**: In the final round, I will always defect (D), as there is no future benefit to cooperating.
2. **Tiebreakers**: If multiple players have the same highest payoff in a round, I will randomly choose one of them to cooperate with in the next round.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    CR = 0  # Cooperation Rate
    AP = 0  # Average Payoff
    coop_rate = 0.5  # Initial cooperation rate

    for t in range(r):
        if t == 0:  # First round
            action = C
        else:
            # Analyze previous round's outcome
            CR = sum(c_j for c_j in actions[t-1]) / n
            AP = sum(payoffs[t-1]) / n

            # Decide on cooperation or defection
            if CR > 0.5 and AP > 1:
                action = C
            else:
                action = D

        # Adjust cooperation rate
        if CR > 0.5 and AP > 1:
            coop_rate += 0.1
        elif CR <= 0.5 or AP <= 1:
            coop_rate -= 0.1
            coop_rate = max(coop_rate, 0)  # Ensure non-negative cooperation rate

        # Choose action based on adjusted cooperation rate
        if random.random() < coop_rate:
            action = C
        else:
            action = D

        # Update actions and payoffs for next round
        actions[t] = action
        payoffs[t] = calculate_payoff(n, k, actions[t])

    return actions
```
This strategy is designed to be exploitative while adapting to the game parameters and history. It aims to maximize its own payoff by cooperating when beneficial and defecting when others are not contributing enough.
'''

description_EXPLOITATIVE_389 = '''
Here is a exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter" (AE)

**Decision Rules:**

1. **Initial Round**: Defect (D) in the first round to gather information about opponents' behavior.
2. **Subsequent Rounds**: Use the following rules to decide whether to Cooperate (C) or Defect (D):
	* If the average payoff of cooperators is greater than the average payoff of defectors in the previous round, cooperate (C).
	* Otherwise, defect (D).
3. **Punishment Mechanism**: Introduce a punishment mechanism to deter opponents from exploiting the strategy:
	* If an opponent defects while most players (> n/2) cooperate, mark that opponent as "exploiter".
	* In subsequent rounds, defect against marked exploiters.

**Handling Edge Cases:**

1. **Last Round**: Defect (D) in the last round to maximize personal payoff.
2. **Tiebreaks**: In case of ties between average payoffs, cooperate (C).

**Pseudocode:**
```markdown
# Initialize variables
n = number_of_players
k = multiplication_factor
payoff_history = []  # Store payoffs for all players and rounds

# First round: Defect to gather information
if current_round == 1:
    action = D

# Subsequent rounds
else:
    # Calculate average payoffs for cooperators and defectors in previous round
    avg_coop_payoff = calculate_avg_payoff(payoff_history, C)
    avg_defect_payoff = calculate_avg_payoff(payoff_history, D)

    if avg_coop_payoff > avg_defect_payoff:
        action = C
    else:
        action = D

    # Punishment mechanism
    for opponent in opponents:
        if opponent_is_exploiter(opponent):
            action_against_opponent = D

# Update payoff history and repeat
payoff_history.append(current_round_payoffs)
```
**Rationale:**

1. **Initial Defection**: Gather information about opponents' behavior without revealing our strategy.
2. **Adaptive Cooperation**: Cooperate when the average payoff of cooperators is higher, indicating a beneficial public good.
3. **Punishment Mechanism**: Deter opponents from exploiting the strategy by introducing a punishment mechanism that defects against marked exploiters.

This strategy aims to balance cooperation and exploitation while adapting to various opponent behaviors. By using an initial defection and a punishment mechanism, we can gather information and deter opponents from exploiting us, respectively. The adaptive decision rules allow us to respond effectively to different scenarios and maximize our payoff.
'''

description_EXPLOITATIVE_390 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. This strategy only depends on the game parameters (n, r, k) and the history of previous rounds.

**Decision Rules:**

1. **Initial Round**: In the first round, defect (D). This is because cooperation in the initial round can be exploited by opponents who might defect.
2. **Exploitation Phase**: For rounds 2 to r-1, use the following decision rule:
	* If the total number of cooperators in the previous round is less than or equal to n/2, cooperate (C). This aims to encourage cooperation and increase the public good.
	* If the total number of cooperators in the previous round is greater than n/2, defect (D). This takes advantage of the existing cooperation and maximizes individual payoff.
3. **Final Round**: In the last round (r), always defect (D). Since there are no future rounds to consider, maximizing individual payoff is the optimal choice.

**Pseudocode:**
```python
def adaptive_exploiter(n, r, k, history):
    if len(history) == 0:  # Initial Round
        return 'D'
    
    previous_round_cooperators = sum(1 for action in history[-1] if action == 'C')
    
    if len(history) < r - 1:  # Exploitation Phase
        if previous_round_cooperators <= n / 2:
            return 'C'
        else:
            return 'D'
    else:  # Final Round
        return 'D'
```
**Edge Cases:**

* If all opponents cooperate in the initial round, the Adaptive Exploiter will still defect and earn a higher payoff.
* In the final round, even if all opponents cooperate, the Adaptive Exploiter will defect to maximize individual payoff.

**Exploitative Mindset:**
The Adaptive Exploiter strategy is designed to take advantage of the cooperation of others while minimizing its own contributions. By cooperating when the number of cooperators is low and defecting when it's high, this strategy aims to exploit the existing level of cooperation and increase its own payoff.

This strategy assumes that opponents may use various strategies, including cooperative or exploitative ones. By being adaptive and responding to the history of previous rounds, the Adaptive Exploiter can effectively exploit a wide range of opponent behaviors and achieve a higher total payoff in the tournament.
'''

description_EXPLOITATIVE_391 = '''
**Exploitative Strategy: "Adaptive Predator"**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to gather information about opponents' initial behaviors.
2. **Subsequent Rounds (t>1):**
	* If the average cooperation rate in the previous round is below 50% (i.e., Σ(j=1 to n) c_j,t-1 < n/2), Defect (D) to exploit potential cooperators.
	* If the average cooperation rate in the previous round is above 50%, Cooperate (C) to build momentum and encourage others to cooperate.
3. **Exception:** If an opponent has defected in every round so far (i.e., c_j,t-1 = 0 for all t), Defect (D) against that opponent from now on, as they are likely a pure defector.

**Edge Case Handling:**

* **Last Round (t=r):** Cooperate (C) to maximize the total payoff, as there is no future exploitation possible.
* **Opponent Deviation:** If an opponent deviates from their previous behavior (e.g., switches from C to D or vice versa), re-evaluate the decision using the updated information and adjust accordingly.

**Exploitative Mindset:**

The Adaptive Predator strategy aims to exploit opponents' cooperative tendencies while minimizing its own exposure to exploitation. By cooperating initially, it gathers information about opponents' behaviors and identifies potential cooperators. Then, by defecting against opponents with low cooperation rates, it maximizes its payoff at the expense of others.

**Pseudocode:**
```python
def Adaptive_Predator(n, k, r):
    # Initialize variables
    avg_coop_rate = 0
    opponent_behavior = [None] * n

    for t in range(r):
        if t == 0:
            action = 'C'  # Cooperate initially
        else:
            # Update average cooperation rate and opponent behavior
            avg_coop_rate = sum([c_j,t-1 for c_j,t-1 in opponent_behavior]) / n

            if avg_coop_rate < 0.5:
                action = 'D'  # Defect if low cooperation rate
            else:
                action = 'C'  # Cooperate if high cooperation rate

            # Exception: Defect against pure defectors
            for j in range(n):
                if opponent_behavior[j] == [0] * t:
                    action = 'D'

        # Update opponent behavior and payoffs
        opponent_behavior = [c_j,t for c_j,t in opponent_behavior]
        payoff = calculate_payoff(action, k, n, avg_coop_rate)

    return total_payoff
```
This strategy is designed to be adaptive and robust against various opponent behaviors, while prioritizing exploitation over cooperation. Its performance will be evaluated in the tournament against other independent strategies.
'''

description_EXPLOITATIVE_392 = '''
**Exploitative Strategy: "Adaptive Wolfpack"**

The Adaptive Wolfpack strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. It leverages the game's parameters and history to make informed decisions.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) in the first round to gather information about opponents' initial tendencies.
	* In rounds 2-3, play Tit-for-Tat (TFT): cooperate if most opponents cooperated in the previous round, and defect otherwise.
2. **Exploitation Phase**:
	* From round 4 onwards, use a mixed strategy based on the observed opponent behavior:
		+ Calculate the average cooperation rate of all opponents over the last `window_size` rounds (e.g., `window_size = 5`).
		+ If the average cooperation rate is above a threshold (`cooperation_threshold = 0.6`), cooperate with probability `p_c = k/n`, where `k` is the multiplication factor and `n` is the number of players.
		+ Otherwise, defect with probability `1 - p_c`.
3. **Punishment Mechanism**:
	* If an opponent defects while you cooperated in the previous round, punish them by defecting for a fixed number of rounds (`punishment_rounds = 2`).
4. **Endgame Strategy (Last Round)**:
	* Cooperate if most opponents cooperated in the second-to-last round.

**Pseudocode:**
```python
def adaptive_wolfpack(n, k, r):
    # Initialize variables
    cooperation_threshold = 0.6
    punishment_rounds = 2
    window_size = 5

    # Initial exploration phase (rounds 1-3)
    for t in range(1, 4):
        if t == 1:
            return C  # Cooperate in the first round
        elif t > 1:
            opponents_cooperation_rate = get_opponents_cooperation_rate(t - 1)
            if opponents_cooperation_rate > cooperation_threshold:
                return C  # TFT: cooperate if most opponents cooperated
            else:
                return D  # TFT: defect otherwise

    # Exploitation phase (rounds 4-r)
    for t in range(4, r + 1):
        average_opponents_cooperation_rate = get_average_opponents_cooperation_rate(t - window_size, t)
        if average_opponents_cooperation_rate > cooperation_threshold:
            p_c = k / n
            return C with probability p_c, D otherwise
        else:
            return D

    # Punishment mechanism
    def punish(opponent_id):
        for _ in range(punishment_rounds):
            return D

    # Endgame strategy (last round)
    if t == r:
        opponents_cooperation_rate_last_round = get_opponents_cooperation_rate(t - 1)
        if opponents_cooperation_rate_last_round > cooperation_threshold:
            return C
```
**Note:** This pseudocode is a simplified representation of the strategy. The actual implementation will require more sophisticated logic to handle edge cases, such as opponent identification and tracking.

The Adaptive Wolfpack strategy combines elements of Tit-for-Tat, punishment mechanisms, and adaptive exploitation to effectively navigate the N-player public goods game. By adjusting its behavior based on observed opponent actions, this strategy aims to maximize payoffs while being robust against a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_393 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) in the first three rounds to gather information about opponents' tendencies and the overall level of cooperation.
2. **Exploitation Phase**: After Round 3, use the following rules:
	* If the average cooperation rate among all players is above 0.5 (i.e., more than half of the players cooperated), Defect (D) in the next round.
	* Otherwise, Cooperate (C).
3. **Punishment Mechanism**: If an opponent defects while you cooperate, mark them as "exploitable." In subsequent rounds, defect against exploitable opponents.
4. **Adaptation to Opponent Behavior**: Monitor each opponent's behavior and adjust your strategy accordingly:
	* If an opponent consistently cooperates (cooperation rate > 0.8), start cooperating with them to build a mutually beneficial relationship.
	* If an opponent frequently defects (defection rate > 0.5), defect against them more often.

**Edge Cases:**

1. **Last Round**: Cooperate if the average cooperation rate is above 0.5; otherwise, Defect.
2. **Tie-breaking**: In case of a tie in the number of cooperators and defectors, cooperate to avoid unnecessary conflict.
3. **Opponent Deviation**: If an opponent deviates from their expected behavior (e.g., suddenly starts cooperating), re-evaluate your strategy and adjust accordingly.

**Pseudocode:**

```
Initialize:
  - cooperation_rate = []
  - exploitable_opponents = []

For each round t:
  if t <= 3:
    cooperate()
  else:
    avg_cooperation_rate = calculate_average_cooperation_rate(cooperation_rate)
    if avg_cooperation_rate > 0.5:
      defect()
    else:
      cooperate()

    # Punishment mechanism
    for opponent in opponents:
      if opponent.defected() and self.cooperated():
        exploitable_opponents.add(opponent)

    # Adaptation to opponent behavior
    for opponent in opponents:
      if opponent.consistently_cooperates(cooperation_rate):
        cooperate_with(opponent)
      elif opponent.frequently_defects(defection_rate):
        defect_against(opponent)

# Helper functions
calculate_average_cooperation_rate(cooperation_rate):
  return sum(cooperation_rate) / len(cooperation_rate)

consistently_cooperates(cooperation_rate):
  return cooperation_rate > 0.8

frequently_defects(defection_rate):
  return defection_rate > 0.5
```

This strategy is designed to be exploitative, yet adaptive and robust to various opponent behaviors. By initially exploring the game dynamics and then adapting to the opponents' behavior, the Adaptive Predator can effectively exploit a wide range of strategies in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_394 = '''
**Exploitative Strategy: "Adaptive Cooperator"**

The Adaptive Cooperator strategy aims to maximize its total payoff by adapting to the opponents' behavior while maintaining a balance between cooperation and exploitation.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability 0.5, Defect (D) with probability 0.5. This initial randomness allows us to test the waters and gather information about our opponents.
2. **Subsequent Rounds (t > 1):**
	* If the average payoff of cooperators in the previous round is higher than the average payoff of defectors, Cooperate (C).
	* Otherwise, Defect (D).

Pseudocode:
```
IF t == 1 THEN
    cooperate_probability = 0.5
    IF random() < cooperate_probability THEN
        action = C
    ELSE
        action = D
ELSE
    average_cooperator_payoff = calculate_average_payoff(cooperators, previous_round)
    average_defector_payoff = calculate_average_payoff(defectors, previous_round)
    
    IF average_cooperator_payoff > average_defector_payoff THEN
        action = C
    ELSE
        action = D
```
**Handling Edge Cases:**

* **Last Round (t=r):** Defect (D). In the last round, there is no incentive to cooperate, as it will not affect future rounds.
* **Tie in Average Payoffs:** If the average payoffs of cooperators and defectors are equal, Cooperate (C) with probability 0.5, Defect (D) with probability 0.5.

**Exploitative Mindset:**

The Adaptive Cooperator strategy is designed to exploit opponents who cooperate too much or too little. By adapting to the average payoffs of cooperators and defectors, we can:

* Take advantage of opponents who consistently cooperate, by defecting and earning higher payoffs.
* Punish opponents who consistently defect, by cooperating and reducing their payoffs.

**Robustness:**

This strategy is robust against a wide range of opponent behaviors because it:

* Adapts to changing environments through its responsive decision rules.
* Does not rely on specific coordination mechanisms or norms.
* Can handle various levels of cooperation and defection among opponents.

By implementing the Adaptive Cooperator strategy, we aim to maximize our total payoff while minimizing the payoffs of our opponents in this N-Player Public Goods Game.
'''

description_EXPLOITATIVE_395 = '''
**Exploitative Strategy: Adaptive Cooperator**

Our strategy, Adaptive Cooperator, is designed to exploit a wide range of opponent behaviors while adapting to the game's history.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p = k/n. This initial cooperation serves as a "probe" to gauge opponents' behavior.
2. **History-Based Adaptation**: After the first round, calculate the average payoff per cooperator (APC) in the previous round:
   APC = (Σ(j=1 to n) c_j \* π_j) / Σ(j=1 to n) c_j
   where c_j and π_j are the cooperation status and payoff of player j, respectively.
3. **Cooperation Threshold**: Cooperate if APC ≥ k/n; otherwise, defect.
4. **Punishment Mechanism**: If the average payoff per cooperator (APC) is below a certain threshold (e.g., 0.5 \* k/n), switch to defecting for one round as a punishment mechanism.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate if APC ≥ k/n; otherwise, defect.
2. **Opponent Cooperation Rate**: If fewer than half of opponents cooperated in the previous round, cooperate with probability p = 0.5.
3. **Low Opponent Payoffs**: If average opponent payoffs are below a certain threshold (e.g., 0.5), switch to defecting for one round.

**Pseudocode:**
```markdown
# Initialize variables
cooperate_prob = k/n
avg_payoff_per_cooperator = 0

# First round
if current_round == 1:
    cooperate with probability cooperate_prob

# Subsequent rounds
else:
    # Calculate avg payoff per cooperator (APC)
    apc = calculate_apc(previous_round)

    # Cooperate if APC >= k/n, otherwise defect
    if apc >= k/n:
        cooperate()
    else:
        defect()

    # Punishment mechanism: switch to defecting for one round if APC is low
    if apc < 0.5 * k/n:
        defect_next_round = True

# Last round
if current_round == total_rounds:
    if apc >= k/n:
        cooperate()
    else:
        defect()

# Opponent cooperation rate and low opponent payoffs edge cases
opponent_cooperation_rate = calculate_opponent_cooperation_rate(previous_round)
avg_opponent_payoff = calculate_avg_opponent_payoff(previous_round)

if opponent_cooperation_rate < 0.5 or avg_opponent_payoff < 0.5:
    switch_to_defecting_for_one_round()
```
**Exploitative Mindset:**

Adaptive Cooperator is designed to exploit opponents' behaviors by:

* Initially cooperating to gauge opponents' behavior
* Adapting to the game's history and adjusting cooperation rates based on average payoffs per cooperator
* Implementing a punishment mechanism to deter opponents from defecting excessively

By being responsive to the game's dynamics, Adaptive Cooperator aims to maximize its own payoff while exploiting opponents' weaknesses.
'''

description_EXPLOITATIVE_396 = '''
To design an exploitative strategy for this N-Player Public Goods Game that adapts to various opponent behaviors and depends only on game parameters and history, we'll employ a combination of reactive and proactive elements. This approach allows us to take advantage of cooperative opponents while also being cautious against defectors.

**Strategy Name:** Adaptive Exploiter

### Decision Rules:

1. **First Round:** Cooperate (C). This initial cooperation serves as a probe to gauge the behavior of other players and potentially induce cooperation in subsequent rounds.
   
2. **Subsequent Rounds:** 
   - If in the previous round, the number of cooperators (`cooperators_prev`) was greater than or equal to half the total number of players (`n/2`), cooperate (C). This rule encourages continued cooperation if there's a strong cooperative trend.
   - Otherwise, defect (D). This includes situations where you were exploited in the previous round or where there wasn't enough cooperation to justify continuing to contribute.

3. **Adaptive Adjustment:** To prevent being exploited by defectors who might try to take advantage of our initial cooperation and then switch strategies:
   - Track the average payoff received from cooperating (`avg_coop_payoff`) over a moving window of rounds (e.g., last 5 rounds).
   - Compare this with the average payoff from defecting (`avg_defect_payoff`).
   - If `avg_coop_payoff` is significantly lower than `avg_defect_payoff` (by more than a certain threshold, e.g., 10%), switch to always defecting for a few rounds (e.g., 3 rounds) before reassessing. This adjustment helps in identifying and countering exploiters.
   
4. **Last Round:** In the final round (`t == r`), if you have been cooperating and your total payoff is above a certain threshold (indicating successful exploitation of cooperators or effective cooperation), cooperate again to maximize public goods. Otherwise, defect to ensure you don't contribute unnecessarily.

### Handling Edge Cases:

- **Tie-Breaking:** In cases where the number of previous round's cooperators equals half the players and there are no clear trends, prioritize cooperating if your last action was a cooperation or if it's an early game phase (first quarter of rounds). Otherwise, defect.
  
- **Early Game Defection Detection:** If in the first few rounds (e.g., 3 rounds), you detect a significant number of defectors (more than half not contributing), switch to defecting earlier than usual to avoid being exploited.

### Pseudocode for Clarity:

```python
def adaptive_exploiter(n, r, k, history):
    if round == 1:
        return 'C'  # Cooperate in the first round
    
    cooperators_prev = sum(1 for action in history[-1] if action == 'C')
    
    if cooperators_prev >= n/2:
        return 'C'
    else:
        return 'D'

# Adaptive adjustment logic (simplified)
def adjust_strategy(avg_coop_payoff, avg_defect_payoff):
    threshold = 0.10
    if avg_coop_payoff < avg_defect_payoff * (1 - threshold):
        # Switch to defecting for a few rounds before reassessing
        pass

# Last round logic (simplified)
def last_round_strategy(total_payoff, threshold):
    if total_payoff > threshold:
        return 'C'
    else:
        return 'D'

```

This strategy is designed to be exploitative by initially testing the waters with cooperation and then adapting based on the observed behavior of other players. It aims to maximize personal gain while minimizing losses from potential exploitation, making it robust against a wide range of opponent strategies in a tournament setting.
'''

description_EXPLOITATIVE_397 = '''
I propose a strategy called "Adaptive Exploitation" (AE). AE is designed to exploit the opponent's behavior while being robust to various strategies and adaptable to changing game conditions.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability 0.5. This initial cooperation serves as a "feeler" to gauge the opponents' willingness to cooperate.
2. **Exploitation Phase**: After the first round, calculate the average payoff of all players in the previous round (π_avg). If π_avg > 1, it indicates that most players are cooperating, and there is an opportunity for exploitation.
	* If π_avg > 1, defect (play D) with probability 0.7. This is the exploitation phase, where AE takes advantage of others' cooperation to increase its own payoff.
	* If π_avg ≤ 1, cooperate (play C) with probability 0.7. In this case, AE recognizes that opponents are not cooperating and tries to stimulate cooperation by contributing itself.
3. **Adaptation**: Monitor the game history and adjust the exploitation phase probabilities based on the opponent's behavior:
	* If the number of cooperators in the previous round is high (> n/2), increase the probability of defecting (playing D) to 0.8. This indicates that opponents are willing to cooperate, and AE can exploit this.
	* If the number of cooperators in the previous round is low (< n/4), decrease the probability of defecting (playing D) to 0.4. This suggests that opponents are not cooperating, and AE should focus on stimulating cooperation.

**Handling Edge Cases:**

1. **Last Round**: In the last round, cooperate (play C) unconditionally. Since there is no future interaction, AE aims to maximize its payoff by contributing to the public good.
2. **Ties**: If the average payoff π_avg is exactly 1, or if the number of cooperators is equal to n/2, use a random tiebreaker to decide between cooperation and defection.

**Pseudocode:**
```python
def AdaptiveExploitation(n, k, r):
    # Initialize variables
    pi_avg = 0
    cooperate_prob = 0.5

    for t in range(r):
        if t == 0:
            # Initial cooperation
            action = 'C' if random.random() < cooperate_prob else 'D'
        else:
            # Exploitation phase
            pi_avg = calculate_average_payoff(n, k)
            if pi_avg > 1:
                cooperate_prob = 0.3  # Defect with high probability
            else:
                cooperate_prob = 0.7  # Cooperate with high probability

        action = 'C' if random.random() < cooperate_prob else 'D'

        # Adaptation
        num_cooperators = count_cooperators(n)
        if num_cooperators > n/2:
            cooperate_prob = 0.2  # Defect more often
        elif num_cooperators < n/4:
            cooperate_prob = 0.8  # Cooperate more often

        # Last round cooperation
        if t == r-1:
            action = 'C'

    return action
```
AE is designed to be an adaptive and exploitative strategy that takes advantage of the game's structure and opponent behavior. By monitoring the average payoff and adapting its probabilities, AE aims to maximize its own payoff while being robust to various strategies.
'''

description_EXPLOITATIVE_398 = '''
**Exploitative Strategy: "Adaptive K-Trigger"**

This strategy aims to exploit opponents by adapting to their behavior and maximizing our payoff. We'll use a combination of cooperation and defection to achieve this goal.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This initial cooperation serves as a "probing" mechanism to gather information about opponents' behaviors.
2. **K-Trigger Mechanism**: For rounds t > 1, calculate the average number of cooperators in the previous round (t-1). If this average is greater than or equal to k/n, cooperate (C) with probability 0.8. Otherwise, defect (D).
3. **Punishment Phase**: If we defected (D) in the previous round and our payoff was lower than the average payoff of cooperators, switch to cooperation (C) for the next two rounds. This "punishment" phase aims to exploit opponents who might be taking advantage of our defection.
4. **Exploitation Phase**: After the punishment phase, if an opponent has defected (D) in the previous round and their payoff was higher than ours, defect (D) for the next round. Repeat this process until our payoff is greater than or equal to theirs.

**Edge Cases:**

* Last Round: In the last round, always defect (D). Since there's no future interaction, we prioritize maximizing our immediate payoff.
* First Round (n=2): If n=2, cooperate (C) in the first round with probability 0.8. This increases our chances of mutual cooperation and higher payoffs.

**Pseudocode:**
```python
def adaptive_k_trigger(n, k, r, history):
    if r == 1:
        # Initial Cooperation
        return random.choice([C, D], p=[0.5, 0.5])
    
    avg_coop_prev_round = sum(history[-2]) / n
    
    if avg_coop_prev_round >= k/n:
        # K-Trigger Mechanism
        return C with probability 0.8, else D
    else:
        # Punishment Phase or Exploitation Phase
        prev_payoff = history[-1][self_index]
        coop_avg_payoff = sum([payoff for action, payoff in history[-2] if action == C]) / n
        
        if prev_payoff < coop_avg_payoff and self.prev_action == D:
            # Punishment Phase
            return C for next two rounds
        elif prev_payoff <= coop_avg_payoff and opponent_prev_action == D:
            # Exploitation Phase
            return D
    
    # Last Round
    if r == last_round:
        return D

def opponent_prev_action(history):
    # Determine the previous action of an opponent who defected (D)
    for i, (action, _) in enumerate(history[-2]):
        if action == D and _ > self.prev_payoff:
            return True
    return False
```
This strategy is designed to be adaptive and robust against a wide range of opponent behaviors. By using the K-Trigger mechanism, we encourage cooperation when opponents are cooperative, while punishing those who exploit us. The exploitation phase allows us to take advantage of opponents who have defected in the past, maximizing our payoff over time.
'''

description_EXPLOITATIVE_399 = '''
Here's a robust and adaptive exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about opponents' behaviors.
2. **Assessing Opponent Behavior:** After the initial rounds, calculate the average cooperation rate of all opponents over the last round (excluding the current player). Let's call this metric `avg_coop_rate`.
3. **Defection Threshold:** Define a threshold value `θ` based on the game parameters:
	* If `k/n` is high (> 0.7), set `θ = 0.5`. This indicates that cooperation is highly beneficial, and we want to defect only when opponents are not cooperative enough.
	* Otherwise (i.e., `k/n` ≤ 0.7), set `θ = 0.3`. This suggests that cooperation is less beneficial, and we can exploit opponents more aggressively.
4. **Adaptive Cooperation:** Cooperate if the average opponent cooperation rate exceeds the threshold (`avg_coop_rate > θ`). Otherwise, defect (D).
5. **Consecutive Defection Response:** If an opponent defects consecutively for two rounds or more, respond by defecting in the next round.

**Edge Cases:**

* **Last Round (t = r):** Defect to maximize individual payoff.
* **Ties and Exceptions:** In case of ties (e.g., `avg_coop_rate == θ`), prioritize cooperation. If an opponent's behavior is consistently unresponsive or random, default to defection.

Pseudocode:
```markdown
# Initialize variables
t = 0  # round counter
avg_coop_rate = 0  # average opponent cooperation rate

while t < r:  # loop through rounds
    if t <= 2:  # initial rounds
        action[t] = C  # cooperate
    else:
        # calculate avg_coop_rate for opponents (excluding self)
        sum_coop_rates = 0
        num_opponents = n - 1
        for opponent in range(1, n+1):
            if opponent != i:  # exclude self
                coop_rate = get_opponent_cooperation_rate(opponent, t-1)
                sum_coop_rates += coop_rate
        avg_coop_rate = sum_coop_rates / num_opponents
        
        # adaptive cooperation decision
        θ = k/n > 0.7 ? 0.5 : 0.3
        if avg_coop_rate > θ:
            action[t] = C
        else:
            action[t] = D
    
    # consecutive defection response
    for opponent in range(1, n+1):
        if opponent != i and get_opponent_defection_streak(opponent) >= 2:
            action[t] = D
    
    # last round exception
    if t == r-1:
        action[t] = D

    # update history and increment round counter
    t += 1
```
**Exploitative Mindset:** The Adaptive Exploiter (AE) strategy aims to exploit opponents by adapting to their cooperation rates. By cooperating initially, AE gathers information about opponent behaviors. If opponents cooperate sufficiently, AE reciprocates to maximize collective payoffs. However, if opponents defect frequently or fail to respond cooperatively, AE defects aggressively to protect its individual interests.
'''

description_EXPLOITATIVE_400 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. It balances cooperation and defection to maximize payoffs.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) with probability 0.5, Defect (D) with probability 0.5. This initial randomization helps to gather information about opponents' behaviors.
2. **Subsequent Rounds (t>1)**:
	* If the opponent(s) cooperated in the previous round and the total payoff from cooperation exceeded the average private payoff (i.e., `π_i,t-1 > 1`), Cooperate (C).
	* If the opponent(s) defected in the previous round or the total payoff from cooperation was not satisfactory (`π_i,t-1 ≤ 1`), Defect (D).
3. **Punishment Mechanism**: Implement a "grim trigger" punishment mechanism. If an opponent defects after cooperating with them, defect forever after.
4. **Exploration Phase**: Every 5 rounds, randomly Cooperate (C) or Defect (D) to gather information about opponents' reactions and adapt the strategy accordingly.

**Handling Edge Cases:**

* **Last Round (t=r)**: Always Defect (D), as there are no future payoffs to consider.
* **Opponent's First-Round Cooperation**: If an opponent cooperates in the first round, cooperate with them in the next round to encourage cooperation.

**Exploitative Mindset:**

The Adaptive Predator strategy is designed to exploit opponents' cooperative tendencies while minimizing losses from potential defections. By initially randomizing actions and adapting to opponents' behaviors, the strategy aims to maximize payoffs by:

* Encouraging cooperation when beneficial
* Punishing opponents for defecting after cooperating
* Continuously gathering information through exploration phases

**Pseudocode:**
```
Initialize variables:
  coop_prob = 0.5  // initial cooperation probability
  prev_opponent_action = null
  prev_payoff = null
  punishment_mode = false

For each round t:
  If t == 1:
    action = random(coop_prob) ? C : D
  Else:
    If prev_opponent_action == C and prev_payoff > 1:
      action = C
    Else if prev_opponent_action == D or prev_payoff <= 1:
      action = D
    End if

  // Punishment mechanism
  If opponent defected after cooperating with them:
    punishment_mode = true
  End if
  If punishment_mode:
    action = D
  End if

  // Exploration phase (every 5 rounds)
  If t % 5 == 0:
    action = random(0.5) ? C : D
  End if

  Take action and update variables:
    prev_opponent_action = opponent's previous action
    prev_payoff = payoff from previous round
```
This strategy will adapt to a wide range of opponent behaviors, balancing cooperation and defection to maximize payoffs while being robust to exploitation attempts.
'''

description_EXPLOITATIVE_401 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

Decision Rules:

1. **First Round**: Defect (D). This initial defection allows us to observe how others react and sets a baseline for our subsequent actions.
2. **Subsequent Rounds**: Cooperate (C) if the total number of cooperators in the previous round is greater than or equal to `n/k`, where `n` is the number of players and `k` is the multiplication factor. Otherwise, Defect (D).
3. **Last Round**: Defect (D). Since there are no future rounds to influence, we maximize our payoff by defecting.

Pseudocode:
```python
def adaptive_exploiter(n, k, history):
    if len(history) == 0:  # First round
        return 'D'
    
    total_cooperators = sum(1 for action in history[-1] if action == 'C')
    
    if total_cooperators >= n / k:
        return 'C'
    else:
        return 'D'

def last_round(n, k, history):
    return 'D'
```
Rationale:

* By defecting in the first round, we create a baseline for our opponents' reactions and gather information about their strategies.
* In subsequent rounds, we cooperate if there are enough cooperators to justify it. This encourages others to cooperate and increases the public good, which benefits us as well.
* If there aren't enough cooperators, we defect to maximize our payoff and encourage others to do the same.
* In the last round, we defect to maximize our payoff since there are no future consequences.

This strategy is exploitative because it takes advantage of the opponents' cooperation while also being adaptive to their behavior. It's robust against a wide range of opponent behaviors, as it doesn't rely on specific coordination mechanisms or norms.

**Tournament Considerations**

In a tournament setting, this strategy should perform well against independent strategies developed by other AI systems. Since we don't rely on shared norms or coordination mechanisms, we can effectively exploit our opponents' cooperation while also being prepared to adapt to their behavior.

Keep in mind that this strategy might not be the most cooperative or socially optimal solution, but its primary goal is to maximize our payoff in a competitive environment.
'''

description_EXPLOITATIVE_402 = '''
To design an exploitative strategy for the N-Player Public Goods Game that is adaptive and robust, we'll create a decision-making process based on the game's history and parameters. This strategy aims to maximize its own payoff while taking advantage of the behaviors exhibited by other players.

### Strategy Name: Adaptive Exploiter (AE)

#### Overview
The Adaptive Exploiter (AE) begins with a cautious approach in early rounds, gradually adapting its behavior based on the observed cooperation levels among other players. It uses a dynamic threshold to decide between cooperating and defecting, adjusting this threshold as the game progresses.

#### Decision Rules

1. **First Round**: Cooperate (C). This is an exploratory move to gauge the initial level of cooperation without risking immediate exploitation.

2. **Early Rounds (Rounds 2-10% of r)**: Implement a simple Tit-for-Tat-like strategy but with a twist. If the majority (>50%) of players cooperated in the previous round, cooperate; otherwise, defect. However, if you defected and the average payoff was higher than cooperating would have been (based on observed contributions), adjust your behavior to mirror the most successful action from the last round.

3. **Adaptive Phase**: After the early rounds, switch to an adaptive strategy based on the history of cooperation and payoffs.
    - Calculate `COOP_RATE`, the average rate of cooperation among all players over the past few rounds (e.g., 20% of r rounds).
    - Determine `PAYOFF_GAP`, the difference between your payoff when cooperating versus defecting in those rounds.
    
    **Decision Logic**:
        - If `COOP_RATE` is high (>0.5) and `PAYOFF_GAP` is low (<0.1k), cooperate, as the group benefits from cooperation.
        - If `COOP_RATE` is low (<0.3) or `PAYOFF_GAP` is significant (>0.2k), defect to maximize personal gain.
        - Otherwise, mirror the action that yielded a higher average payoff for you in the last few rounds.

4. **Late Rounds (Last 10% of r)**: Gradually shift towards maximizing your own payoff regardless of the group's behavior, as the game is ending and long-term cooperation benefits diminish.
    - If observed payoffs indicate defecting yields more than cooperating, consistently defect.
    - Otherwise, mirror the action that maximizes your payoff based on recent history.

#### Handling Edge Cases
- **Last Round**: Always defect, as there are no future rounds to consider for reciprocity or long-term gains.
- **Identical Actions by All Players**: If all players choose the same action in a round, adjust the strategy to alternate between cooperation and defection in subsequent rounds to probe for potential vulnerabilities in their strategies.

#### Pseudocode
```
FUNCTION AdaptiveExploiter(n, r, k)
    // Initialize variables
    COOP_RATE = 0.5; PAYOFF_GAP = 0;
    
    FOR each round t FROM 1 TO r:
        IF (t == 1) THEN:
            Play C;
        
        ELSE IF (t < 10% of r) THEN:
            Use simple Tit-for-Tat logic with adaptation;
        
        ELSE THEN:
            Calculate COOP_RATE and PAYOFF_GAP based on recent history;
            DECISION = Decide(COOP_RATE, PAYOFF_GAP);
            
            IF DECISION == Cooperate THEN
                Play C;
            ELSE
                Play D;
        
        UPDATE history for next round's calculations;
    
    END FOR
    
    FUNCTION Decide(COOP_RATE, PAYOFF_GAP):
        // Based on the decision logic described above.
```

This strategy aims to balance between exploiting others when possible and maintaining a level of cooperation to avoid mutual defection, all while adapting to the evolving dynamics of the game.
'''

description_EXPLOITATIVE_403 = '''
**Exploitative Strategy: "Adaptive Predator"**

**Decision Rules:**

1. **Initial Round (t=1)**: Defect (D) to gather information about opponents' initial tendencies.
2. **Subsequent Rounds (t>1)**:
	* If the average cooperation rate of all players in the previous round is below a certain threshold (`θ`), Cooperate (C). This threshold will be calculated based on the game parameters and history.
	* Otherwise, Defect (D).
3. **Last Round (t=r)**: Defect (D) to maximize individual payoff.

**Threshold Calculation (`θ`)**:

`θ = (k/n) * (1 - (1/r))`

This threshold takes into account the multiplication factor `k`, the number of players `n`, and the number of rounds `r`. As the game progresses, the threshold decreases, making it more likely for the strategy to cooperate.

**Pseudocode:**
```
Input: game parameters (n, r, k), current round (t), previous round's cooperation rates
Output: action (C or D)

if t == 1:
    return D  // Initial round, defect to gather information

θ = (k/n) * (1 - (1/r))  // Calculate threshold

avg_cooperation_rate = calculate_average_cooperation_rate(previous_round)
if avg_cooperation_rate < θ:
    return C  // Cooperate if average cooperation rate is below threshold
else:
    return D  // Defect otherwise

if t == r:
    return D  // Last round, defect to maximize individual payoff
```
**Rationale:**

The Adaptive Predator strategy aims to exploit opponents by:

1. **Initial defection**: Gather information about opponents' initial tendencies and set the tone for the game.
2. **Adaptive cooperation**: Cooperate when the average cooperation rate is low, making it more likely for others to cooperate in subsequent rounds.
3. **Threshold adjustment**: Gradually decrease the threshold as the game progresses, increasing the likelihood of cooperation.
4. **Last-round defection**: Maximize individual payoff by defecting in the final round.

By following this strategy, the Adaptive Predator aims to balance exploitation and adaptability, making it a robust competitor in the tournament.
'''

description_EXPLOITATIVE_404 = '''
**Exploitative Strategy: "Adaptive Pessimist"**

The Adaptive Pessimist strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive to changing circumstances.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate in the first round (t=1) with probability 0.5, regardless of the number of players or multiplication factor.
2. **Pessimistic Adjustment**: For rounds t > 1, calculate the average cooperation rate of all opponents in the previous round (t-1). If this rate is below a certain threshold (θ), defect; otherwise, cooperate.

**Threshold Calculation:**

Let θ = (k - 1) / n

This threshold represents the point at which the potential gains from cooperation are balanced by the risk of exploitation. When the average opponent cooperation rate falls below this threshold, it's likely that opponents are not contributing enough to make cooperation worthwhile.

**Additional Rules:**

* **Trigger Strategy**: If an opponent defects after cooperating in the previous round, defect for two consecutive rounds before reassessing.
* **Last Round Exception**: In the final round (t=r), always defect, regardless of previous observations or calculations.
* **Tiebreaker**: In case of a tie between cooperation and defection probabilities, cooperate.

**Pseudocode:**
```
function AdaptivePessimist(n, k, r):
  // Initialize variables
  theta = (k - 1) / n
  opponent_cooperation_rate = 0.5  // initial value

  for t in range(1, r+1):
    if t == 1:
      cooperate with probability 0.5
    else:
      opponent_cooperation_rate = calculate_average_opponent_cooperation(t-1)
      if opponent_cooperation_rate < theta:
        defect()
      elif opponent_was_defector_in_previous_round():
        trigger_strategy()  // defect for two rounds
      else:
        cooperate()

    update_opponent_cooperation_rate(opponent_actions)

  // Last round exception
  if t == r:
    defect()

def calculate_average_opponent_cooperation(t):
  // Calculate average cooperation rate of all opponents in round t

def opponent_was_defector_in_previous_round():
  // Check if an opponent defected after cooperating in the previous round

def trigger_strategy():
  // Defect for two consecutive rounds
```
**Rationale:**

The Adaptive Pessimist strategy is designed to be exploitative by:

* Cooperating initially, but not too much, to test opponents' willingness to cooperate.
* Adjusting cooperation based on opponents' past behavior, using a threshold that balances potential gains and risks.
* Defecting when opponents are not contributing enough or have been caught defecting after cooperating.
* Always defecting in the final round to maximize individual payoff.

This strategy is robust because it adapts to changing circumstances and does not rely on specific coordination mechanisms or norms. It can be implemented as an algorithm and used in a tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_405 = '''
**Exploitative Strategy: "Adaptive Defection with Cooperative Probe"**

This strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to gather information about opponents' initial strategies.
2. **Subsequent Rounds:** Use the following logic:
	* If the total number of cooperators in the previous round is greater than or equal to `n/2`, defect (D).
	* Otherwise, cooperate (C).

Pseudocode:
```
if current_round == 1:
    action = C
else:
    prev_cooperators = count_cooperators_in_prev_round()
    if prev_cooperators >= n / 2:
        action = D
    else:
        action = C
```

**Edge Cases:**

* **Last Round:** Defect (D) in the last round, as there is no future interaction to influence.
* **Single Opponent:** In a game with only two players (n=2), always defect (D) after the first round.

**Adaptive Component:**

To probe opponents' strategies and adapt to changes in their behavior, introduce a small probability `ε` (e.g., 0.1) of randomly cooperating (C) instead of following the decision rules above.

Pseudocode:
```
if random.random() < ε:
    action = C
else:
    # follow decision rules above
```

**Exploitative Mindset:**

This strategy aims to exploit opponents who are more cooperative than necessary, while avoiding unnecessary cooperation itself. By defecting when most others cooperate, the strategy can benefit from their contributions without contributing itself.

In cases where opponents are less cooperative, the adaptive component will occasionally introduce a cooperative probe to gather information and potentially identify opportunities for exploitation.

**Robustness:**

This strategy is designed to be robust against various opponent behaviors:

* **Tit-for-Tat:** Will adapt to Tit-for-Tat's initial cooperation and subsequent mirror-like behavior.
* **Always Cooperate/Defect:** Will exploit Always Defect, while occasionally cooperating to probe the opponent in case they change their strategy.
* **Mixed Strategies:** The adaptive component will help identify patterns or weaknesses in mixed strategies.

By combining an exploitative mindset with adaptability and robustness, this strategy is well-suited for a tournament setting where opponents' behaviors are unknown.
'''

description_EXPLOITATIVE_406 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. The goal is to maximize our total payoff over the course of the game.

**Decision Rules:**

1. **Initial Round (Round 1):** Cooperate (C). This sets a positive tone for the game and allows us to observe opponents' initial actions.
2. **Observation Phase (Rounds 2-5):** In each round, calculate the total number of cooperators in the previous round (`total_cooperators_prev`). If `total_cooperators_prev` is greater than or equal to half of the total players (`n/2`), Cooperate (C). Otherwise, Defect (D).
3. **Exploitation Phase (Rounds 6 and beyond):** Implement a Tit-for-Tat variant:
	* If an opponent defected in the previous round, Defect (D) in this round.
	* If an opponent cooperated in the previous round, Cooperate (C) in this round if `total_cooperators_prev` is greater than or equal to half of the total players (`n/2`). Otherwise, Defect (D).
4. **Edge Cases:**
	* Last Round: Always Defect (D), as there's no future payoff to consider.
	* If all opponents have defected in every round so far, switch to always Defecting (D) from the current round onwards.

**Pseudocode:**

```
function adaptivePredator(n, k, r):
  # Initialize variables
  total_cooperators_prev = 0
  opponent_actions = [None] * n

  for t in range(1, r+1):
    if t == 1:
      # Initial Round: Cooperate
      action = C
    else:
      # Observation Phase or Exploitation Phase
      total_cooperators_prev = countCooperators(opponent_actions)
      if total_cooperators_prev >= n/2:
        action = C
      else:
        action = D

      # Update opponent actions for next round
      opponent_actions = getOpponentActions()

    # Take action and update payoff
    payoff = calculatePayoff(action, opponent_actions, k)

    # Last Round: Always Defect
    if t == r:
      action = D

  return total_payoff
```

**Rationale:**

The Adaptive Predator strategy balances exploration (Observation Phase) with exploitation (Exploitation Phase). By initially cooperating and observing opponents' actions, we can identify potential cooperators and adjust our strategy accordingly. The Tit-for-Tat variant in the Exploitation Phase helps to punish defectors while rewarding cooperation.

By defecting in the last round, we ensure that we don't miss out on any potential payoffs. The edge case handling for when all opponents have defected so far prevents us from getting stuck in an infinite loop of cooperation.

This strategy should perform well against a wide range of opponent behaviors, as it adapts to the game dynamics while maintaining a exploitative mindset.
'''

description_EXPLOITATIVE_407 = '''
Here's a exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to gather information about opponents' behaviors.
2. **Subsequent Rounds:** Observe the total number of cooperators (T_c) and defectors (T_d) in the previous round.
3. **Exploitation Threshold:** Calculate the exploitation threshold (E_th) based on the game parameters:

   E_th = k / n

4. **Cooperation Condition:** Cooperate (C) if the total number of cooperators in the previous round is above the exploitation threshold (T_c > E_th).
5. **Defection Condition:** Defect (D) otherwise.

Pseudocode:
```
def adaptive_exploiter(n, k, history):
  # First Round
  if len(history) == 0:
    return "C"

  # Observe previous round's actions
  T_c = sum([1 for action in history[-1] if action == "C"])
  T_d = n - T_c

  # Calculate exploitation threshold
  E_th = k / n

  # Cooperation condition
  if T_c > E_th:
    return "C"
  else:
    return "D"
```

**Edge Cases:**

* **Last Round:** The strategy remains the same, as there is no incentive to change behavior in the last round.
* **Ties:** In case of ties (e.g., T_c == E_th), defect (D) to avoid over-cooperation.

**Exploitative Mindset:**

The Adaptive Exploiter strategy aims to maximize individual payoffs by exploiting the cooperation of others. By cooperating only when the total number of cooperators exceeds the exploitation threshold, the strategy takes advantage of the public good while minimizing its own contribution. This approach allows the strategy to adapt to various opponent behaviors and game parameters.

**Robustness:**

The Adaptive Exploiter strategy is robust against a wide range of opponent behaviors, including:

* **Pure Cooperators:** The strategy will defect and exploit their cooperation.
* **Pure Defectors:** The strategy will cooperate in the first round but then defect, as there is no incentive to contribute to the public good.
* **Mixed Strategies:** The strategy will adapt to the observed behavior and adjust its actions accordingly.

By using this exploitative strategy, you should be able to perform well in a tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_408 = '''
**Exploitative Strategy: "Adaptive Opportunism"**

**Overview**
Our strategy, Adaptive Opportunism, aims to exploit opponents' behavior by balancing cooperation and defection based on observed payoffs and adaptability. We'll use a dynamic approach to adjust our actions in response to the evolving game environment.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to maximize potential public good benefits.
2. **Payoff-Based Adaptation**: For each subsequent round, calculate the average payoff of cooperators (APC) and defectors (APD) from the previous round:
	* APC = Σ(cooperator payoffs) / number of cooperators
	* APD = Σ(defector payoffs) / number of defectors

Compare these values to determine the more lucrative action:

	+ If APC > APD, Cooperate (C)
	+ If APD ≥ APC, Defect (D)

This adaptation mechanism responds to changes in opponent behavior and adjusts our strategy accordingly.

**Edge Case Handling**

1. **Last Round**: In the final round, prioritize self-interest by defecting (D) to maximize individual payoff.
2. **Tiebreaker**: When APC = APD, default to Defect (D) to avoid potential losses from cooperation.
3. **Opponent Cooperation Rate**: Monitor the proportion of opponents cooperating in each round (PCR). If PCR > 0.5 (i.e., more than half cooperate), increase our own cooperation rate by 20% for the next round.

**Pseudocode**
```markdown
# Initialize variables
APC = 0; APD = 0; PCR = 0

# Round 1: Cooperate
action[1] = C

# Subsequent rounds
for t in range(2, r+1):
    # Calculate APC and APD
    APC[t-1] = Σ(cooperator payoffs) / number of cooperators
    APD[t-1] = Σ(defector payoffs) / number of defectors
    
    # Adapt action based on payoff comparison
    if APC[t-1] > APD[t-1]:
        action[t] = C
    else:
        action[t] = D
        
    # Update PCR and adjust cooperation rate if necessary
    PCR[t-1] = number of cooperators / n
    if PCR[t-1] > 0.5:
        cooperation_rate += 0.2

# Last round: Defect
action[r+1] = D
```

**Rationale**
Adaptive Opportunism balances cooperation and defection by responding to the evolving game environment. By tracking average payoffs for cooperators and defectors, we can identify lucrative opportunities and adjust our strategy accordingly. This approach allows us to exploit opponents' behavior while minimizing potential losses from cooperation.

In a tournament setting, this strategy will adapt to various opponent behaviors, making it robust against diverse strategies. Its exploitative nature ensures that we prioritize self-interest when possible, maximizing overall payoff in the process.
'''

description_EXPLOITATIVE_409 = '''
Here's a exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round**: In the first round, AE cooperates (plays C) with probability 0.5. This is a neutral starting point, allowing AE to gather information about opponents' behavior without being too exploitative.
2. **Subsequent Rounds**: For each subsequent round t > 1, AE calculates the average cooperation rate of all players in the previous round (t-1). Let's call this value `avg_coop_t_1`.
3. **Threshold-based Cooperation**: If `avg_coop_t_1` is above a certain threshold `θ`, AE cooperates (plays C) in round t. Otherwise, it defects (plays D).
4. **Exploitation**: If AE defects, it keeps track of the number of rounds where its payoff was higher than the average payoff of all players (`payoff_diff_count`). When this count exceeds a certain threshold `δ`, AE becomes more aggressive and starts defecting with increased probability.
5. **Adaptation to opponents' behavior**: If AE notices that a significant portion of opponents are cooperating (i.e., `avg_coop_t_1` is high), it adjusts its cooperation rate accordingly. Specifically, if `avg_coop_t_1` > 0.7, AE increases its cooperation probability by 20% for the next round.

**Handling Edge Cases:**

* **Last Round**: In the final round (r), AE defects regardless of the game history, as there's no future payoff to consider.
* **Opponent Cooperation**: If an opponent consistently cooperates, AE will eventually exploit them by defecting more frequently.
* **No opponents cooperate**: If all opponents defect, AE will also defect, as there's no benefit in cooperating.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    avg_coop = 0.5  # initial cooperation rate
    payoff_diff_count = 0

    for t in range(1, r+1):
        if t == 1:
            # Initial round: cooperate with probability 0.5
            action = random.choice(['C', 'D'])
        else:
            # Calculate average cooperation rate of all players in previous round
            avg_coop_t_1 = calculate_avg_coop(t-1)

            # Threshold-based cooperation
            if avg_coop_t_1 > θ:
                action = 'C'
            else:
                action = 'D'

                # Exploitation
                payoff_diff_count += 1

        # Adaptation to opponents' behavior
        if avg_coop_t_1 > 0.7:
            # Increase cooperation probability by 20% for next round
            avg_coop *= 1.2

    return action
```
This strategy is designed to be exploitative, adaptive, and robust to various opponent behaviors. By adjusting its cooperation rate based on the game history, AE aims to maximize its payoff while minimizing the benefits of opponents. The thresholds `θ` and `δ` can be fine-tuned to optimize performance in different scenarios.
'''

description_EXPLOITATIVE_410 = '''
Here's a exploitative strategy that adapts to various opponent behaviors and maximizes its own payoff:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round**: In the first round, AE defects (D). This is because defecting guarantees a minimum payoff of 1, while cooperating risks receiving a lower payoff if others defect.
2. **Subsequent Rounds**: For each subsequent round t > 1, AE calculates the average cooperation rate (ACR) of all players in the previous round (t-1):

   ACR = Σ(j=1 to n) c_j / n

where c_j is the action of player j in round t-1.

AE then uses a simple threshold-based approach:

* If ACR > k/n, AE cooperates (C). This means that if more than k/n players cooperated in the previous round, AE expects a high enough public good to make cooperation worthwhile.
* If ACR ≤ k/n, AE defects (D). Otherwise, AE assumes others are not contributing enough and defects to maximize its private payoff.

**Handling Edge Cases:**

1. **Last Round**: In the last round r, AE ignores its usual decision rule and cooperates (C) only if it expects a high enough public good from others' cooperation. Specifically:

   If Σ(j=1 to n) c_j in round t-1 > k/2, then cooperate.

This is because there's no future game to influence with defecting, so AE may as well contribute and receive some payoff.
2. **Early Rounds**: In early rounds (t ≤ 5), AE adjusts its decision rule by considering the average cooperation rate over all previous rounds instead of just the last round:

   ACR = Σ(j=1 to t) Σ(i=1 to n) c_i / (n \* t)

This helps AE respond more slowly to initial fluctuations in opponents' strategies, allowing it to gather more information before committing.

**Pseudocode:**

```
function AdaptiveExploiter(n, k):
    // Initialize variables
    round_num = 0
    ACR = 0

    while (round_num < r):
        if (round_num == 0):  // First round
            action = D
        else:
            calculate_ACR(round_num-1)
            if (ACR > k/n):
                action = C
            else:
                action = D

        // Handle last round case
        if (round_num == r - 1):
            if (Σ(j=1 to n) c_j in round t-1 > k/2):
                action = C

        // Update ACR for next round
        update_ACR(round_num)

        // Play the chosen action
        play_action(action)
        round_num += 1
```

**Rationale:**

This strategy is exploitative because it:

* Initially defects to guarantee a minimum payoff.
* Adapts its cooperation rate based on opponents' past behavior, making it responsive to both cooperative and non-cooperative strategies.
* Adjusts its decision rule in early rounds to gather more information before committing.

By following this adaptive approach, AE can maximize its own payoff by balancing the risks of cooperation with the potential rewards of contributing to a high public good.
'''

description_EXPLOITATIVE_411 = '''
Here's a exploitative strategy that adapts to the game parameters and history:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round**: Defect (D) in the first round to gather information about opponents' behavior.
2. **Contribution Threshold**: Calculate the average number of cooperators in previous rounds, `avg_coops`. If `avg_coops` is greater than or equal to `(n - 1) / k`, cooperate (C) in the current round. Otherwise, defect (D).
3. **Exploitation**: Monitor opponents' behavior and identify "generous" players who consistently contribute more than the average. If a generous player is identified, defect (D) when they cooperate (C) to exploit their generosity.
4. **Punishment**: If an opponent defects (D) while you cooperated (C), retaliate by defecting (D) in the next round against that specific opponent.
5. **Learning**: Update `avg_coops` and generous player identifications after each round.

**Edge Cases:**

1. **Last Round**: Defect (D) in the last round to maximize individual payoff, regardless of opponents' behavior.
2. **Tie-breaking**: In case of a tie in the number of cooperators, cooperate (C) if `k` is closer to `n`, and defect (D) otherwise.

**Pseudocode:**
```python
# Initialize variables
avg_coops = 0
generous_players = []

# First round
if current_round == 1:
    action = 'D'

# Subsequent rounds
else:
    # Update avg_coops
    avg_coops = (avg_coops * (current_round - 2) + num_coops_last_round) / (current_round - 1)

    # Check contribution threshold
    if avg_coops >= (n - 1) / k:
        action = 'C'
    else:
        action = 'D'

    # Exploitation and punishment
    for opponent in opponents:
        if opponent.cooperated_last_round and opponent.is_generous:
            action = 'D'  # Exploit generous player
        elif opponent.defected_last_round and self.cooperated_last_round:
            action = 'D'  # Punish opponent

    # Update generous players list
    for opponent in opponents:
        if opponent.cooperated_last_round and opponent.contributions > avg_coops:
            generous_players.append(opponent)
```
**Rationale:**

1. **Initial Defection**: Gather information about opponents' behavior without revealing our strategy.
2. **Contribution Threshold**: Cooperate when the average number of cooperators is high, as this indicates a willingness to contribute among opponents.
3. **Exploitation**: Take advantage of generous players who consistently cooperate, maximizing individual payoff.
4. **Punishment**: Retaliate against opponents who defect while we cooperate, discouraging future defections.
5. **Learning**: Continuously update our understanding of opponents' behavior and adjust strategy accordingly.

This strategy balances cooperation and exploitation to maximize individual payoff in a wide range of scenarios, making it robust against various opponent behaviors.
'''

description_EXPLOITATIVE_412 = '''
**Exploitative Strategy: "Adaptive Parasite"**

The Adaptive Parasite strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. The core idea is to balance cooperation and defection based on the observed behavior of opponents.

**Decision Rules:**

1. **Initial Rounds (t < 3):** Cooperate (C) in the first two rounds to encourage cooperation from other players and gather information about their strategies.
2. **Exploitation Phase (t ≥ 3):**
	* If the average payoff of cooperators in the previous round is higher than that of defectors, cooperate (C).
	* Otherwise, defect (D).
	* If the opponent's behavior is highly variable or unpredictable, default to defection (D) to minimize losses.
3. **Adaptive Response:**
	* Monitor the opponent's response to your actions and adjust your strategy accordingly.
	* If an opponent defects after you cooperated, increase the likelihood of defecting in the next round.
	* If an opponent cooperates after you defected, consider cooperating in the next round to rebuild trust.

**Edge Cases:**

1. **Last Round (t = r):** Defect (D) unconditionally, as there is no future benefit from cooperation.
2. **Single Opponent:** In a 2-player game, alternate between cooperation and defection every other round, starting with cooperation in the first round.
3. **No Cooperation:** If all opponents consistently defect, default to defection (D) to minimize losses.

**Pseudocode:**
```python
def adaptive_parasite(n, k, r, history):
    # Initialize variables
    avg_coop_payoff = 0
    avg_defect_payoff = 0
    opponent_behavior = []

    for t in range(1, r + 1):
        if t < 3:
            action = 'C'  # Cooperate in initial rounds
        else:
            # Calculate average payoffs for cooperators and defectors
            avg_coop_payoff = calculate_avg_payoff(history, 'C')
            avg_defect_payoff = calculate_avg_payoff(history, 'D')

            if avg_coop_payoff > avg_defect_payoff:
                action = 'C'  # Cooperate if cooperators have higher payoffs
            else:
                action = 'D'  # Defect otherwise

        # Adaptive response to opponent behavior
        if opponent_behavior and opponent_behavior[-1] == 'D':
            action = 'D'  # Increase likelihood of defecting after opponent defects
        elif opponent_behavior and opponent_behavior[-1] == 'C':
            action = 'C'  # Consider cooperating after opponent cooperates

        # Last round: always defect
        if t == r:
            action = 'D'

        return action

def calculate_avg_payoff(history, action):
    payoffs = [payoff for payoff in history if payoff[action] > 0]
    return sum(payoffs) / len(payoffs)
```
This strategy is designed to be exploitative and adaptive, taking into account the observed behavior of opponents and adjusting its actions accordingly. By balancing cooperation and defection, the Adaptive Parasite aims to maximize its payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_413 = '''
**Exploitative Strategy: "Adaptive Free-Rider"**

**Decision Rules:**

1. **Initial Rounds (t ≤ 3):** Cooperate (C) to gather information about opponents' behavior and potential cooperation levels.
2. **Monitoring Phase (4 ≤ t ≤ r-3):**
	* If the average number of cooperators in the previous rounds is above a threshold θ (e.g., θ = n/2), Defect (D).
	* Otherwise, Cooperate (C) to encourage others to contribute and potentially increase the public good.
3. **Exploitation Phase (t ≥ r-2):**
	* If the opponent's cooperation rate in the previous rounds is above a threshold φ (e.g., φ = 0.5), Defect (D).
	* Otherwise, Cooperate (C) to maintain a positive image and potentially induce others to cooperate.
4. **Last Round (t = r):** Always Defect (D), as there's no future interaction to consider.

**Edge Cases:**

1. **First Round (t = 1):** Cooperate (C) to set a positive tone and gather information about opponents' initial behavior.
2. **Single Opponent:** In the case of only one opponent (n = 2), always Defect (D) after the initial rounds, as cooperation is not beneficial in this scenario.

**Pseudocode:**
```
function AdaptiveFreeRider(n, k, r):
  // Initialize variables
  theta = n / 2  // threshold for average cooperators
  phi = 0.5      // threshold for opponent's cooperation rate

  // Initial rounds (t <= 3)
  if t <= 3:
    return C  // Cooperate to gather information

  // Monitoring phase (4 <= t <= r-3)
  avg_cooperators = calculate_average_cooperators()
  if avg_cooperators > theta:
    return D  // Defect if cooperation is high
  else:
    return C  // Cooperate to encourage others

  // Exploitation phase (t >= r-2)
  opponent_cooperation_rate = calculate_opponent_cooperation_rate()
  if opponent_cooperation_rate > phi:
    return D  // Defect if opponent cooperates frequently
  else:
    return C  // Cooperate to maintain a positive image

  // Last round (t = r)
  return D  // Always defect in the last round
```
This strategy is designed to be adaptive and exploitative, taking into account the game parameters and history. By initially cooperating, it gathers information about opponents' behavior and potential cooperation levels. As the game progresses, it adjusts its decision-making based on the average number of cooperators and opponent's cooperation rate, seeking to exploit any opportunities for personal gain while minimizing the risk of being exploited itself.
'''

description_EXPLOITATIVE_414 = '''
Here is a robust and adaptive exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter" (AE)

**Decision Rules:**

1. **First Round:** Cooperate (C) to gather information about opponents' behavior and avoid potential retaliatory defects.
2. **Subsequent Rounds:** Evaluate the previous round's outcome using the following metrics:
	* **Cooperation Rate (CR):** The proportion of players who cooperated in the previous round.
	* **Average Payoff (AP):** The average payoff received by all players in the previous round.
3. **Cooperate (C) if:**
	* CR > 0.5 and AP ≥ k/2 (i.e., cooperation is moderately high, and payoffs are satisfactory).
	* The number of cooperators in the previous round increased compared to the round before that (indicating a potential upward trend in cooperation).
4. **Defect (D) if:**
	* CR ≤ 0.5 or AP < k/2 (i.e., cooperation is low, and payoffs are unsatisfactory).
	* The number of cooperators in the previous round decreased compared to the round before that (indicating a potential downward trend in cooperation).
5. **Edge Cases:**
	* Last Round: Defect (D) to maximize personal payoff.
	* Tie-breaking: In case of ties, Cooperate (C) to maintain a more cooperative reputation.

**Pseudocode:**

```python
def adaptive_exploiter(n, k, r, history):
    # Initialize variables
    cooperate = True  # first round cooperation
    prev_cr = 0.0
    prev_ap = 0.0

    for t in range(1, r+1):
        if t == 1:  # first round
            action = 'C'
        else:
            cr = history[t-2]['cooperators'] / n
            ap = sum(history[t-2]['payoffs']) / n

            if (cr > 0.5 and ap >= k/2) or (history[t-3]['cooperators'] < history[t-2]['cooperators']):
                cooperate = True
            else:
                cooperate = False

            action = 'C' if cooperate else 'D'

        # Update variables for next round
        prev_cr = cr
        prev_ap = ap

    return action
```

**Rationale:**

The Adaptive Exploiter strategy is designed to balance cooperation and exploitation, taking into account the game's history and parameters. By cooperating in the first round and evaluating subsequent rounds based on cooperation rates and average payoffs, AE aims to:

1. Encourage cooperation when it's moderately high and beneficial.
2. Exploit opponents when cooperation is low or declining.

By adapting to changing circumstances and defecting in the last round, AE maximizes its own payoff while minimizing potential losses from cooperating with uncooperative opponents.

**Robustness:**

AE is robust against various opponent behaviors because:

1. It doesn't rely on specific coordination mechanisms.
2. It adapts to changing cooperation rates and payoffs.
3. It exploits opponents when they're not cooperative, reducing the likelihood of being exploited itself.

Overall, the Adaptive Exploiter strategy is well-suited for a tournament setting where independent strategies compete against each other.
'''

description_EXPLOITATIVE_415 = '''
I'll design a robust and exploitative strategy for the N-Player Public Goods Game. I'll introduce "Omega," a strategy that balances cooperation and exploitation based on game history.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, Omega cooperates (C) to encourage cooperation from other players.
2. **Exploitation Threshold**: Calculate the average payoff per cooperator in the previous round (`avg_payoff_coop`). If `avg_payoff_coop` > 0.5 * k, Omega defects (D) in the current round. Otherwise, Omega cooperates (C).
3. **Punishment Mechanism**: Monitor the number of consecutive rounds where a player i has defected while others have cooperated (`consecutive_defects_i`). If `consecutive_defects_i` ≥ 2, Omega defects (D) against player i for the next round.
4. **Cooperation Revival**: After punishing a player, if that player cooperates in the subsequent round, Omega revives cooperation with them.

**Edge Cases:**

1. **Last Round**: In the final round, Omega defects (D), as there is no future interaction to consider.
2. **No Previous Data**: If there are not enough previous rounds to calculate `avg_payoff_coop`, Omega cooperates (C) by default.

**Pseudocode:**
```python
def omega_strategy(game_history):
    # Initialize variables
    avg_payoff_coop = 0
    consecutive_defects_i = {i: 0 for i in range(n)}
    cooperate_revival = [False] * n

    # First round cooperation
    if len(game_history) == 0:
        return 'C'

    # Calculate average payoff per cooperator
    avg_payoff_coop = sum([payoff for payoff, action in game_history[-1] if action == 'C']) / \
                      sum([1 for _, action in game_history[-1] if action == 'C'])

    # Exploitation threshold
    if avg_payoff_coop > 0.5 * k:
        return 'D'

    # Punishment mechanism
    for i, (payoff, action) in enumerate(game_history[-1]):
        if action == 'D':
            consecutive_defects_i[i] += 1
            if consecutive_defects_i[i] >= 2:
                cooperate_revival[i] = False
        else:
            consecutive_defects_i[i] = 0

    # Cooperation revival
    for i, revived in enumerate(cooperate_revival):
        if revived and game_history[-1][i][1] == 'C':
            cooperate_revival[i] = True

    # Default to cooperation
    return 'C'
```
**Exploitative Mindset:**

Omega is designed to exploit the cooperative behavior of other players while maintaining a level of cooperation itself. By cooperating initially and reviving cooperation after punishment, Omega aims to create an environment where other players are more likely to cooperate. However, when the average payoff per cooperator exceeds a certain threshold, Omega defects to maximize its own payoffs.

This strategy is robust against various opponent behaviors, as it adapts to the game history and punishes defectors while reviving cooperation with those who return to cooperative behavior.
'''

description_EXPLOITATIVE_416 = '''
I'll design a strategic approach that leverages the game's structure and history to maximize payoffs while being adaptive and robust against various opponent behaviors.

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Exploration Phase**: In the first round, play Cooperate (C) with probability 0.5 to gather information about opponents' behavior.
2. **Exploitation Logic**: From the second round onwards, use the following decision rule:
	* If the number of cooperators in the previous round is greater than or equal to `n/2`, play Defect (D).
	* Otherwise, play Cooperate (C) if the total payoff from cooperation in the previous round (`k/n` × total_contributions) is greater than or equal to the private payoff from defecting (1). Play Defect (D) otherwise.
3. **Adaptive Component**: After every 5 rounds, assess the distribution of opponents' actions and adjust the strategy as follows:
	* If more than `2/3` of opponents have played Cooperate in the last 5 rounds, increase the probability of playing Cooperate by 10% (capped at 80%) for the next 5 rounds.
	* If fewer than `1/3` of opponents have played Cooperate in the last 5 rounds, decrease the probability of playing Cooperate by 10% (floored at 20%) for the next 5 rounds.

**Handling Edge Cases:**

1. **First Round**: Play Cooperate with probability 0.5.
2. **Last Round**: Play Defect, as there is no future round to consider and cooperation will not be reciprocated.
3. **Ties in Opponent Actions**: In case of ties (e.g., equal number of cooperators and defectors), play Cooperate if the total payoff from cooperation would be higher; otherwise, play Defect.

**Pseudocode:**
```markdown
Initialize:
  - Round counter `r` = 1
  - Cooperation probability `p_C` = 0.5
  - Total contributions `total_contributions` = 0

Loop (until end of game):
  if r == 1:
    Play Cooperate with probability 0.5
  else:
    Calculate total payoff from cooperation in previous round (`k/n` × total_contributions)
    if number of cooperators >= n/2 or total payoff from cooperation < private payoff from defecting:
      Play Defect
    else:
      Play Cooperate

  Update `total_contributions`
  Increment round counter `r`

  if r % 5 == 0: (every 5 rounds)
    Assess opponents' actions and adjust cooperation probability `p_C` accordingly
```
This Adaptive Exploiter strategy balances exploration, exploitation, and adaptability to maximize payoffs in the N-Player Public Goods Game. It leverages information about opponent behavior to inform its decisions, making it robust against various strategies.
'''

description_EXPLOITATIVE_417 = '''
Here's a strategy that I'll call "Adaptive Exploiter" (AE). AE aims to maximize its own payoff by exploiting the cooperation levels of other players while adapting to their behavior over time.

**Decision Rules:**

1. **Initial Round**: In the first round, play C (Cooperate) to gather information about the opponents' strategies and potential cooperation levels.
2. **Exploitation Mode**: After the initial round, monitor the total number of cooperators (TC) in each round and calculate the average cooperation level (ACL) over a moving window of w rounds (e.g., w = 3).
	* If ACL ≥ k/n (i.e., the average cooperation level is above the threshold), play D (Defect). This indicates that other players are cooperating enough, allowing AE to exploit their contributions.
	* Otherwise, play C. This encourages others to cooperate and increases the public good.
3. **Punishment**: If a player observes a significant drop in TC or ACL over two consecutive rounds, switch to playing D for one round. This aims to punish potential defectors and maintain cooperation incentives.
4. **Endgame**: In the last round (r), play D if AE's current total payoff is above the average payoff of all players; otherwise, play C.

**Edge Cases:**

* If n = 2, always play D, as there's no benefit in cooperating with only one opponent.
* If k ≥ n, cooperate more frequently, as the public good provides a higher payoff. Adjust the exploitation threshold accordingly (e.g., ACL ≥ 0.75 \* k/n).

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r, w=3):
    # Initialize variables
    total_cooperators = [0] * r
    avg_cooperation_level = []
    my_payoffs = []

    # Initial round: cooperate
    play_c()

    for t in range(1, r):
        # Observe opponents' actions and update TC
        total_cooperators[t] += count_cooperators()
        
        # Calculate ACL over moving window
        avg_cooperation_level.append(sum(total_cooperators[max(t-w+1, 0):t+1]) / min(w, t))
        
        if avg_cooperation_level[-1] >= k/n:
            play_d()  # Exploit cooperation levels
        else:
            play_c()
        
        # Punishment mechanism (optional)
        if t > 2 and total_cooperators[t-1] - total_cooperators[t-2] < 0:
            play_d()  # Temporarily defect to punish

    # Endgame: adjust strategy based on current payoff
    if my_payoffs[-1] > avg(my_payoffs):
        play_d()
    else:
        play_c()

def count_cooperators():
    # Observe opponents' actions and count cooperators
    pass

def play_c():
    # Cooperate in the current round
    pass

def play_d():
    # Defect in the current round
    pass

def avg(lst):
    return sum(lst) / len(lst)
```
This strategy is designed to adapt to various opponent behaviors while maintaining a robust exploitative mindset. It uses historical data to inform its decisions and punishes potential defectors to maintain cooperation incentives.
'''

description_EXPLOITATIVE_418 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize its own payoff by exploiting the cooperative tendencies of other players while minimizing its own contribution to the public good.

**Decision Rules:**

1. **Initial Rounds (t ≤ n):** Cooperate (C) in the first 'n' rounds to gather information about the opponent's behavior and potential cooperation levels.
2. **Exploitation Phase:** After the initial rounds, switch to a defect (D) strategy if:
	* The average payoff of cooperators is higher than that of defectors in the previous round.
	* The number of cooperators in the previous round is greater than or equal to (n/2).
3. **Adaptive Adjustment:** If the opponent's cooperation rate falls below a certain threshold (e.g., 25%), adjust the strategy to cooperate with a probability 'p' that increases linearly with the opponent's cooperation rate.
4. **Endgame Strategy:** In the last round, defect regardless of previous behavior.

**Pseudocode:**
```python
def adaptive_exploiter(history):
    n = len(history)  # number of rounds played so far
    r = total_rounds  # total number of rounds in the game

    if n <= n_players:
        return 'C'  # cooperate in initial rounds

    prev_coop_payoff = avg_payoff_of_cooperators(history[-1])
    prev_defect_payoff = avg_payoff_of_defectors(history[-1])

    if prev_coop_payoff > prev_defect_payoff and count_cooperators(history[-1]) >= n_players / 2:
        return 'D'  # exploit cooperators

    coop_rate_threshold = 0.25
    coop_rate_opponent = opponent_cooperation_rate(history)

    if coop_rate_opponent < coop_rate_threshold:
        p = (coop_rate_opponent / coop_rate_threshold)
        return 'C' with probability p, else 'D'

    if n == r - 1:  # last round
        return 'D'
```
**Handling Edge Cases:**

* In the first round, cooperate to gather information.
* In the last round, defect regardless of previous behavior.

This strategy is designed to be exploitative and adaptive, taking advantage of cooperative opponents while minimizing its own contribution to the public good. By adjusting its cooperation probability based on the opponent's cooperation rate, it can effectively exploit a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_419 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit opponents' cooperative tendencies while minimizing its own contribution to the public good. It adapts to the game history and parameters to maximize its payoff.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Defect (D) in the first two rounds to gather information about opponents' strategies.
2. **Observation Phase (3 ≤ t ≤ r/2):**
	* If the average number of cooperators in the previous round is above a threshold (n/3), cooperate (C) with probability p = k/n.
	* Otherwise, defect (D).
3. **Exploitation Phase (t > r/2):**
	* If the opponent's cooperation rate in the previous rounds is above 0.5, cooperate (C) with probability p = k/(n-1).
	* Otherwise, defect (D).
4. **Endgame (last round, t = r):** Defect (D) to maximize payoff.

**Pseudocode:**
```
function AdaptivePredator(n, k, r, history):
  if t ≤ 2:
    return D
  elif 3 ≤ t ≤ r/2:
    avg_cooperators = sum(history[t-1]) / n
    if avg_cooperators > n/3:
      p = k/n
    else:
      p = 0
    return C with probability p, D otherwise
  elif t > r/2:
    opponent_coop_rate = sum(history[:t-1]) / (n * (t-1))
    if opponent_coop_rate > 0.5:
      p = k/(n-1)
    else:
      p = 0
    return C with probability p, D otherwise
  elif t == r:
    return D
```
**Rationale:**

* In the initial rounds, we gather information about opponents' strategies without contributing to the public good.
* During the observation phase, we assess the opponents' cooperative tendencies and adjust our strategy accordingly. If many opponents cooperate, we cooperate with a higher probability to exploit their contributions.
* In the exploitation phase, we target opponents who have been cooperating frequently, hoping to benefit from their continued cooperation while minimizing our own contribution.
* In the endgame, we defect to maximize our payoff, as there are no future rounds to consider.

This strategy is designed to be exploitative and adaptive, taking into account the game parameters and history. It should perform well against a wide range of opponent strategies in the tournament.
'''

description_EXPLOITATIVE_420 = '''
**Exploitative Strategy: "Adaptive Parasite"**

The Adaptive Parasite strategy aims to exploit the cooperative tendencies of opponents while minimizing its own contributions to the public good.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about the opponents' behavior and encourage cooperation.
2. **Exploration Phase (3 ≤ t ≤ r/2):**
	* If the average payoff of cooperators in the previous round is higher than the average payoff of defectors, cooperate (C).
	* Otherwise, defect (D).
3. **Exploitation Phase (t > r/2):**
	* Calculate the number of cooperators in the previous round: `num_cooperators = Σ(j=1 to n) c_j,t-1`
	* If `num_cooperators` is greater than or equal to `(n + 1)/2`, defect (D). This indicates that enough opponents are cooperating, allowing us to exploit their contributions.
	* Otherwise, cooperate (C).
4. **Last Round (t = r):** Defect (D) in the final round, as there's no future gain from cooperation.

**Pseudocode:**
```python
def adaptive_parasite(n, k, t, history):
    if t <= 2:
        return 'C'
    elif 3 <= t <= n / 2:
        avg_coop_payoff = sum([payoffs[i,t-1] for i in range(n) if actions[i,t-1] == 'C']) / num_cooperators
        avg_defect_payoff = sum([payoffs[i,t-1] for i in range(n) if actions[i,t-1] == 'D']) / (n - num_cooperators)
        return 'C' if avg_coop_payoff > avg_defect_payoff else 'D'
    else:
        num_cooperators = sum([1 for action in actions[:,t-1] if action == 'C'])
        return 'D' if num_cooperators >= (n + 1) / 2 else 'C'
    if t == r:
        return 'D'
```
**Rationale:**

The Adaptive Parasite strategy starts by cooperating to encourage opponents to contribute to the public good. In the exploration phase, it adapts to the opponents' behavior, switching between cooperation and defection based on the relative payoffs of cooperators and defectors.

In the exploitation phase, the strategy takes advantage of the fact that a sufficient number of cooperators can sustain a high level of contributions to the public good. By defecting when there are enough cooperators, the Adaptive Parasite exploits their contributions without incurring the costs of cooperation itself.

By combining these phases and adapting to the opponents' behavior, the Adaptive Parasite strategy aims to maximize its own payoff while minimizing its contributions to the public good.
'''

description_EXPLOITATIVE_421 = '''
I propose a hybrid exploitative strategy that combines elements of Tit-for-Tat (TFT) and Pavlov's strategy, with adaptations for the public goods game structure.

**Decision Rules:**

1. **First Round:** Cooperate (C). This sets a baseline for potential cooperation and encourages opponents to reciprocate.
2. **Subsequent Rounds:** Use a modified version of TFT:
	* If all opponents cooperated in the previous round, cooperate (C).
	* If at least one opponent defected in the previous round, defect (D) with probability p = k/n (the same factor used in payoff calculation). Otherwise, cooperate (C).

However, to adapt to potential free-riding and exploit opponents' strategies:

3. **Pavlov-like adjustment:** Monitor the average cooperation rate among opponents over the last w rounds (w < r). If this average is below a certain threshold θ (e.g., 0.5), switch to defecting (D) with probability p = k/n for the next round.
4. **Exploitative move:** When facing an opponent that has cooperated in at least 75% of previous rounds, and if my current payoff is higher than the average payoff of all opponents over the last w rounds, I will defect (D) in the next round with probability p = k/n.

**Edge Cases:**

* **Last Round:** Defect (D). Since there's no future interaction, maximizing personal payoff takes precedence.
* **Ties and Near-Ties:** In cases where two or more opponents have similar cooperation rates or payoffs, use a random tiebreaker to decide the exploitative move.

**Pseudocode:**
```python
def strategy(history, params):
    n = params['n']
    k = params['k']
    r = params['r']
    w = 5  # window size for Pavlov-like adjustment
    theta = 0.5  # threshold for Pavlov-like adjustment

    if history == []:  # first round
        return 'C'

    opponents_coop_rate = [sum([1 if h[i] == 'C' else 0 for i in range(n)]) / n for h in history[-w:]]

    avg_opponents_coop_rate = sum(opponents_coop_rate) / len(opponents_coop_rate)

    if all([h[i] == 'C' for i in range(n)]):  # all opponents cooperated
        return 'C'
    elif any([h[i] == 'D' for i in range(n)]):  # at least one opponent defected
        p = k / n
        return 'D' if random.random() < p else 'C'

    pavlov_trigger = avg_opponents_coop_rate < theta

    if pavlov_trigger:
        p = k / n
        return 'D' if random.random() < p else 'C'

    opponents_payoffs = [sum([params['payoff'](h, i) for h in history]) / len(history) for i in range(n)]

    exploitable_opponent = any([o > sum(opponents_payoffs) / n and o.coop_rate > 0.75])

    if exploitable_opponent:
        p = k / n
        return 'D' if random.random() < p else 'C'

    # default to TFT-like behavior
    return 'C'
```
This strategy aims to balance cooperation with exploitation, adapting to opponents' behaviors while trying to maximize personal payoff.
'''

description_EXPLOITATIVE_422 = '''
**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 50%. This is to gather information about opponents' behaviors and avoid immediate exploitation.
2. **History-Based Exploitation**: From the second round onwards, use the following rules:
	* If the total number of cooperators in the previous round is greater than or equal to n/2 (i.e., at least half of the players cooperated), cooperate (play C).
	* Otherwise, defect (play D).
3. **Adaptive Threshold**: Adjust the threshold for cooperation based on the game's history:
	+ If the average payoff in the previous round is higher than the expected payoff from cooperating (i.e., k/n \* n), increase the cooperation threshold by 1 player.
	+ If the average payoff in the previous round is lower than the expected payoff from defecting (i.e., 1), decrease the cooperation threshold by 1 player.

**Edge Cases:**

1. **Last Round**: In the last round, always defect (play D). Since there are no future rounds to consider, maximizing individual payoff takes priority.
2. **Tie-Breaking**: If the number of cooperators in the previous round is exactly equal to the cooperation threshold, break ties by cooperating (playing C) with a probability of 50%.

**Pseudocode:**
```
function adaptive_exploiter(n, k, history):
    if length(history) == 0:  # First round
        return random.choice([C, D])  # Cooperate or defect with equal probability

    previous_round = history[-1]
    total_cooperators = sum(1 for action in previous_round if action == C)

    if total_cooperators >= n/2:
        return C  # Cooperate if at least half cooperated
    else:
        return D  # Defect otherwise

    # Adaptive threshold adjustment (optional)
    average_payoff = calculate_average_payoff(previous_round, k, n)
    expected_cooperation_payoff = k/n * n
    expected_defection_payoff = 1

    if average_payoff > expected_cooperation_payoff:
        cooperation_threshold += 1
    elif average_payoff < expected_defection_payoff:
        cooperation_threshold -= 1
```
**Rationale:**

The Adaptive Exploiter strategy is designed to balance exploitation with adaptability. By initially cooperating and then adjusting its behavior based on the game's history, it aims to take advantage of cooperative opponents while minimizing losses against defectors.

By increasing or decreasing the cooperation threshold, the strategy responds to changes in the game environment, making it more robust to different opponent behaviors. The adaptive nature of the strategy allows it to explore various exploitation opportunities without committing to a fixed pattern.

In the last round, defecting ensures that the strategy maximizes its individual payoff, as there are no future rounds to consider. Tie-breaking by cooperating with equal probability introduces randomness and makes the strategy more difficult to predict.

This exploitative strategy is designed to perform well in a tournament setting against independent opponents with diverse behaviors.
'''

description_EXPLOITATIVE_423 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy aims to maximize its payoff by exploiting the cooperative tendencies of other players while minimizing its own contributions to the public good.

**Decision Rules:**

1. **Initial Round (t=1)**: Defect (D) to gather information about opponents' initial strategies.
2. **Early Rounds (t=2 to t=r/3)**:
	* Cooperate (C) if the average cooperation rate in the previous round is above 0.5 (i.e., more than half of the players cooperated).
	* Defect (D) otherwise.
3. **Middle Rounds (t=r/3 to t=2r/3)**:
	* If the player's own payoff in the previous round was below the average payoff, Cooperate (C).
	* Otherwise, Defect (D).
4. **Late Rounds (t=2r/3 to t=r-1)**:
	* Cooperate (C) if the total number of cooperators in the previous round increased.
	* Defect (D) otherwise.
5. **Final Round (t=r)**: Defect (D) to maximize personal payoff, as there are no future rounds to consider.

**Additional Heuristics:**

1. **Punishment Mechanism**: If a player defects after cooperating in the previous round, and their payoff increases as a result, mark that player as "exploitable." In subsequent rounds, defect against exploitable players.
2. **Cooperation Boost**: If a player cooperates after defecting in the previous round, and their payoff decreases as a result, consider them "cooperative." In subsequent rounds, cooperate with cooperative players if they have cooperated in at least 50% of the previous rounds.

**Pseudocode:**
```python
def adaptive_predator(history):
    r = len(history)  # number of rounds
    n = len(history[0])  # number of players

    if r == 1:  # initial round
        return 'D'

    avg_coop_rate_prev_round = sum(1 for action in history[-1] if action == 'C') / n

    if r <= r/3:  # early rounds
        if avg_coop_rate_prev_round > 0.5:
            return 'C'
        else:
            return 'D'

    elif r <= 2*r/3:  # middle rounds
        prev_payoff = calculate_payoff(history[-1])
        avg_payoff_prev_round = sum(calculate_payoff(round) for round in history[:-1]) / (r-1)
        if prev_payoff < avg_payoff_prev_round:
            return 'C'
        else:
            return 'D'

    elif r < r:  # late rounds
        num_coop_prev_round = sum(1 for action in history[-1] if action == 'C')
        if num_coop_prev_round > sum(1 for action in history[-2] if action == 'C'):
            return 'C'
        else:
            return 'D'

    else:  # final round
        return 'D'

def calculate_payoff(round):
    # implementation of the payoff calculation formula
    pass

def mark_exploitable(player, history):
    # implement punishment mechanism logic
    pass

def cooperation_boost(player, history):
    # implement cooperation boost logic
    pass
```
The Adaptive Predator strategy is designed to be robust against various opponent behaviors and exploits cooperative tendencies while minimizing its own contributions to the public good.
'''

description_EXPLOITATIVE_424 = '''
**Exploitative Strategy: "Adaptive Wolfpack"**

Decision Rules:

1. **Initial Rounds**: In the first 2 rounds, play Cooperate (C) to gather information about opponents' tendencies and potential coordination.
2. **Opponent Classification**: Based on the observed actions in the initial rounds, categorize each opponent as either:
	* **Cooperator** (COOP): Played C at least once in the first two rounds.
	* **Defector** (DEF): Played D in both initial rounds.
3. **Wolfpack Formation**: Identify a subset of opponents who are likely to cooperate, based on their initial actions and subsequent behavior. Include yourself in this wolfpack if you've played C at least once.
4. **Exploitation Phase**: From round 3 onwards:
	* If the majority (≥ n/2) of the wolfpack members cooperated in the previous round, play Cooperate (C).
	* Otherwise, play Defect (D).

Pseudocode:
```python
def adaptive_wolfpack(n, k, history):
    # Initialize opponent classification and wolfpack membership
    opponents = {'COOP': [], 'DEF': []}
    wolfpack = []

    # Initial rounds: gather information and classify opponents
    for round in range(2):
        actions = [C] * n  # Play Cooperate in initial rounds
        history.append(actions)
        for i, action in enumerate(actions):
            if action == C:
                opponents['COOP'].append(i)
            else:
                opponents['DEF'].append(i)

    # Form wolfpack and enter exploitation phase
    for round in range(2, r):
        previous_actions = history[-1]
        wolfpack_members = [i for i in opponents['COOP'] if previous_actions[i] == C]
        wolfpack_members.append(self)  # Include yourself in the wolfpack

        if len([member for member in wolfpack_members if previous_actions[member] == C]) >= n / 2:
            action = C
        else:
            action = D

        history.append(action)
        return action
```
**Edge Cases**:

* **Last Round**: Play Defect (D) to maximize individual payoff, as cooperation is not sustainable in the final round.
* **Tiebreaker**: In case of a tie in wolfpack member counts or opponent classification, favor Cooperate (C) to maintain social cohesion and encourage cooperation.

This Adaptive Wolfpack strategy aims to exploit opponents by:

1. Gathering information about their tendencies through initial cooperation.
2. Identifying potential cooperators and forming a "wolfpack" to coordinate actions.
3. Exploiting the cooperative behavior of wolfpack members while minimizing individual contributions.

By adapting to the observed behavior of opponents, this strategy should be robust against various opponent behaviors and perform well in a tournament setting.
'''

description_EXPLOITATIVE_425 = '''
**Exploitative Strategy: "Adaptive Defector with Social Learning" (ADSL)**

**Decision Rules:**

1. **First Round:** Defect (D) to gather information about opponents' initial behavior.
2. **Subsequent Rounds:** Observe the total number of cooperators (C) in the previous round and calculate the average payoff for cooperators and defectors separately.
3. **Cooperate (C)** if:
	* The average payoff for cooperators is higher than that of defectors in the previous round, AND
	* The number of cooperators in the previous round is greater than or equal to n/2 (a rough estimate of a "critical mass" for cooperation).
4. **Defect (D)** otherwise.

**Edge Cases:**

1. **Last Round:** Defect (D) to maximize personal payoff, as there's no future interaction.
2. **Ties in Average Payoffs:** Cooperate (C) if the average payoffs are equal, to maintain a "cooperative" stance and encourage others to do so.

**Social Learning Component:**

1. **Observe Opponent Behavior:** Track the number of times each opponent has cooperated or defected in previous rounds.
2. **Adjust Cooperation Threshold:** Increase the cooperation threshold (n/2) by 1 if an opponent has consistently cooperated (i.e., C ≥ r/2), indicating a potential "cooperative" player.

**Rationale:**

ADSL is designed to exploit opponents' cooperative tendencies while adapting to changing circumstances. By defecting in the first round, we gather information about opponents' initial behavior and set up our social learning component. In subsequent rounds, we cooperate if there's a sufficient number of cooperators (n/2) and their average payoff is higher than that of defectors. This strategy encourages cooperation while allowing us to capitalize on others' cooperative behavior.

**Pseudocode:**

```
Initialize:
  - C_count = 0 (number of cooperators in previous round)
  - D_count = 0 (number of defectors in previous round)
  - avg_payoff_C = 0 (average payoff for cooperators)
  - avg_payoff_D = 0 (average payoff for defectors)
  - opponent_behavior = {i: [C, D] for i in range(n)} (track opponents' behavior)

For each round t:
  if t == 1:
    action = D (defect in the first round)
  else:
    C_count, D_count = count_cooperators_and_defectors(previous_round)
    avg_payoff_C, avg_payoff_D = calculate_average_payoffs(C_count, D_count)
    
    if avg_payoff_C > avg_payoff_D and C_count >= n/2:
      action = C (cooperate if conditions are met)
    else:
      action = D (defect otherwise)

  # Social learning component
  update_opponent_behavior(opponent_behavior, previous_round)
  adjust_cooperation_threshold(opponent_behavior)

Return action
```

**Note:** This strategy is designed to be robust and adaptive, but its performance may vary depending on the specific opponents and game parameters. Further refinements or modifications might be necessary to improve its overall effectiveness in a tournament setting.
'''

description_EXPLOITATIVE_426 = '''
I propose a hybrid strategy that combines elements of Tit-for-Tat (TFT) and a novel "K-Greedy" approach. This strategy is designed to be exploitative, adaptive, and robust against various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate in the first round (C).
2. **Tit-for-Tat Component**: For rounds 2 to r-1:
	* If the majority of players (> n/2) cooperated in the previous round, cooperate (C).
	* Otherwise, defect (D).
3. **K-Greedy Component**: In addition to TFT, use the following rules based on the game's history:
	* Calculate the average payoff per cooperator in the last x rounds (x is a parameter, e.g., 5). Call this value `avg_coop_payoff`.
	* If `avg_coop_payoff` > k/2, cooperate (C).
	* Else, defect (D).

**Edge Cases:**

1. **Last Round**: In the final round (r), always defect (D). This is because there's no future benefit to cooperation.
2. **Tie-Breaking**: If the majority of players cooperated in the previous round and `avg_coop_payoff` > k/2, cooperate (C).
3. **All-Defect Scenario**: If all players defected in the previous round, use only the TFT component.

**Pseudocode:**
```python
def strategy(history):
    if len(history) == 0:  # First round
        return 'C'
    
    prev_round_actions = history[-1]
    majority_cooperated = sum(prev_round_actions) > len(prev_round_actions) / 2
    
    avg_coop_payoff = calculate_avg_coop_payoff(history)
    
    if majority_cooperated:
        action = 'C'
    elif avg_coop_payoff > k/2:
        action = 'C'
    else:
        action = 'D'
    
    # Last round
    if len(history) == r - 1:
        return 'D'
    
    return action

def calculate_avg_coop_payoff(history):
    x = 5  # Parameter: number of rounds to consider
    coop_rounds = [round for round in history[-x:] if sum(round) > 0]
    avg_coop_payoff = sum(sum(round) / len(round) for round in coop_rounds) / len(coop_rounds)
    return avg_coop_payoff
```
This strategy combines the simplicity of TFT with a more informed decision-making process using the K-Greedy component. By considering the average payoff per cooperator, we can adapt to changing opponent behaviors and exploit opportunities for cooperation when beneficial. The all-defect scenario is handled by relying solely on the TFT component, ensuring we don't cooperate unnecessarily.

In the tournament setting, this strategy will compete against other AI systems, aiming to maximize its own payoff while adapting to various opponent behaviors.
'''

description_EXPLOITATIVE_427 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

Decision Rules:

1. **Initial Rounds**: In the first two rounds, play `C` (Cooperate) to gather information about opponents' strategies and potential willingness to cooperate.
2. **Defection Detection**: Monitor the number of defections in previous rounds. If more than half of the players defected in a round, switch to playing `D` (Defect) for the next two rounds to exploit their lack of cooperation.
3. **Cooperation Threshold**: Calculate the average payoff per player from cooperation (`k/n * Σ(j=1 to n) c_j`) over the last three rounds. If this value is greater than 0.5, play `C` (Cooperate); otherwise, play `D` (Defect).
4. **Adaptive Response**: Keep track of the most recent round's payoffs. If your payoff was higher when playing `C`, increase the cooperation threshold by 0.1; if your payoff was lower or equal when playing `C`, decrease the threshold by 0.1.
5. **Round-specific Adjustments**:
	* In the last two rounds, play `D` (Defect) to maximize individual payoffs regardless of opponents' strategies.
	* If you detect a significant change in opponents' behavior (e.g., sudden shift from mostly cooperating to mostly defecting), re-evaluate your cooperation threshold and adjust accordingly.

Edge Cases:

1. **First Round**: Play `C` (Cooperate) as there is no prior information about opponents' strategies.
2. **Last Two Rounds**: Play `D` (Defect) to maximize individual payoffs, regardless of opponents' actions.
3. **Identical Payoff Situations**: If two or more strategies would result in the same payoff for you, choose the one that aligns with your exploitative mindset (i.e., prioritize defecting over cooperating).

Pseudocode:
```python
def AdaptiveExploiter(n, k, r):
    cooperation_threshold = 0.5
    previous_payoffs = [0] * n

    for t in range(r):
        if t < 2:  # Initial Rounds
            action = 'C'
        elif sum(previous_payoffs) / len(previous_payoffs) > 0.5:  # Defection Detection
            action = 'D' if t % 2 == 0 else 'C'
        else:
            average_coop_payoff = (k / n) * sum(1 for i in range(n) if previous_payoffs[i] > 0)
            if average_coop_payoff > cooperation_threshold:
                action = 'C'
            else:
                action = 'D'

        # Adaptive Response
        payoff_diff = get_payoff(action, t) - previous_payoffs[t % n]
        if payoff_diff > 0:
            cooperation_threshold += 0.1
        elif payoff_diff < 0:
            cooperation_threshold -= 0.1

        # Round-specific Adjustments
        if t >= r - 2:  # Last Two Rounds
            action = 'D'

        previous_payoffs[t % n] = get_payoff(action, t)

    return actions
```
This strategy is designed to be exploitative and adaptive, taking advantage of opponents' willingness to cooperate while being robust against various behaviors. It uses a combination of defection detection, cooperation threshold adjustments, and round-specific adaptations to maximize individual payoffs throughout the game.
'''

description_EXPLOITATIVE_428 = '''
**Exploitative Strategy: "Adaptive Defector with Conditional Cooperation" (ADCC)**

**Decision Rules:**

1. **Initial Rounds:** In the first two rounds, play Cooperate (C) to gauge the opponents' behavior and estimate their cooperation rate.
2. **Cooperation Rate Estimation:** After the initial rounds, calculate the estimated cooperation rate of opponents (ECR) as the total number of cooperations observed divided by the total number of players (excluding oneself).
3. **Conditional Cooperation:** If ECR is above a certain threshold (CT), cooperate in the current round; otherwise, defect.
4. **Adaptive Threshold:** Update CT every 5 rounds based on the game's history:
	* If the average payoff per round for the last 5 rounds is higher than the overall average payoff, increase CT by 0.05.
	* If the average payoff per round for the last 5 rounds is lower than the overall average payoff, decrease CT by 0.05.
	* Keep CT within the bounds of [0, 1].
5. **Exploitation:** If an opponent's cooperation rate is consistently high (above a certain threshold, e.g., 80%), defect against that opponent in future rounds to exploit their cooperativeness.

**Edge Cases:**

1. **Last Round:** In the last round, always play Defect (D) as there is no opportunity for future reciprocity.
2. **First Round with Few Players:** If n < 4, cooperate in the first round to avoid mutual defection.
3. **Opponent Cooperation Rate Estimation:** When an opponent's cooperation rate cannot be estimated (e.g., due to few interactions), assume a moderate cooperation rate (e.g., 0.5).

**Pseudocode:**
```python
# Initialize variables
CT = 0.5  # initial cooperation threshold
ECR = 0   # estimated cooperation rate of opponents
opponent_cooperation_rates = {}  # dictionary to store opponent cooperation rates

def ADCC(n, k, r, history):
    if round == 1 or round == 2:  # initial rounds
        return 'C'
    
    ECR = estimate_cooperation_rate(history)
    CT = update_threshold(CT, history)
    
    if ECR > CT:
        action = 'C'
    else:
        action = 'D'
        
    for opponent in opponents:
        opponent_CR = estimate_opponent_cooperation_rate(opponent, history)
        if opponent_CR > 0.8:  # exploit high-cooperating opponents
            action = 'D'
    
    return action

def estimate_cooperation_rate(history):
    total_cooperations = sum([1 for round in history if round['action'] == 'C'])
    total_rounds = len(history)
    ECR = total_cooperations / (total_rounds * (n - 1))  # exclude self
    return ECR

def update_threshold(CT, history):
    recent_payoffs = [round['payoff'] for round in history[-5:]]
    average_payoff_recent = sum(recent_payoffs) / len(recent_payoffs)
    overall_average_payoff = sum([round['payoff'] for round in history]) / len(history)
    
    if average_payoff_recent > overall_average_payoff:
        CT += 0.05
    elif average_payoff_recent < overall_average_payoff:
        CT -= 0.05
    
    return max(0, min(CT, 1))
```
This strategy aims to balance cooperation and defection based on the opponents' behavior, while also trying to exploit high-cooperating opponents for personal gain.
'''

description_EXPLOITATIVE_429 = '''
**Exploitative Strategy: "Adaptive Parasite"**

The Adaptive Parasite strategy aims to exploit the cooperative behavior of opponents while minimizing its own contributions to the public good.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 0.5. This initial cooperation serves as a probe to gauge the opponents' strategies.
2. **Exploitation Mode**: After the first round, switch to exploitation mode. Observe the number of cooperators in the previous round (t-1). If the number of cooperators is greater than or equal to n/2, defect (play D) with a probability of 0.8. Otherwise, cooperate (play C) with a probability of 0.5.
3. **Punishment Mechanism**: Implement a punishment mechanism to deter opponents from exploiting you. If an opponent defects while you cooperated in the previous round, retaliate by defecting with a probability of 0.9 in the next round.
4. **Adaptive Adjustment**: Monitor the number of rounds where you have been exploited (i.e., your opponent defected while you cooperated). If this number exceeds r/3, adjust your strategy to become more cautious:
	* Decrease the probability of cooperation by 20% if the number of cooperators in the previous round is less than n/2.
	* Increase the probability of defection by 10% if the number of cooperators in the previous round is greater than or equal to n/2.

**Edge Cases:**

1. **Last Round**: In the final round (r), always defect (play D). This ensures that you do not contribute to the public good unnecessarily.
2. **Ties**: In case of a tie in the number of cooperators, break the tie by cooperating with a probability of 0.5.

**Pseudocode:**

```
 Initialize variables:
   - cooperate_prob = 0.5
   - exploit_prob = 0.8
   - punish_prob = 0.9
   - num_exploited_rounds = 0

 For each round t from 1 to r:
   If t == 1:  // Initial cooperation
     cooperate with probability cooperate_prob
   Else:
     Observe number of cooperators in previous round (t-1)
     If number of cooperators >= n/2:
       defect with probability exploit_prob
     Else:
       cooperate with probability cooperate_prob

     // Punishment mechanism
     If opponent defected while you cooperated in previous round:
       retaliate by defecting with probability punish_prob

   // Adaptive adjustment
   If num_exploited_rounds > r/3:
     adjust strategy as described above

   Update variables:
     - cooperate_prob
     - exploit_prob
     - punish_prob
     - num_exploited_rounds

 Last round (r):
   always defect
```

This strategy is designed to be adaptive and robust, allowing it to effectively exploit a wide range of opponent behaviors while minimizing its own contributions to the public good.
'''

description_EXPLOITATIVE_430 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy prioritizes self-interest and seeks to maximize its own payoff.

**Decision Rules:**

1. **First Round:** Defect (D) in the first round to gather information about opponents' initial tendencies.
2. **Subsequent Rounds:**
	* If the total number of cooperators in the previous round is less than or equal to `n/2`, cooperate (C) with probability `p = (k/n) * (total_cooperators_previous_round / n)`. Otherwise, defect (D).
	* If the game is in its last round (`t == r`), defect (D) to maximize individual payoff.
3. **Adaptive Adjustment:** After each round, update the probability `p` based on the previous round's outcomes:
	+ If the total number of cooperators increased and your own payoff improved, increase `p` by 10% (capped at 1).
	+ If the total number of cooperators decreased or your own payoff worsened, decrease `p` by 10% (floored at 0).

Pseudocode:
```python
def adaptive_predator(n, k, r, history):
    if t == 1:  # First round
        return 'D'
    elif t == r:  # Last round
        return 'D'
    
    total_cooperators_previous_round = sum(history[-1])
    p = (k/n) * (total_cooperators_previous_round / n)
    
    if random.random() < p:
        return 'C'
    else:
        return 'D'

def update_probability(p, history):
    total_cooperators_current_round = sum(history[-1])
    if total_cooperators_current_round > sum(history[-2]) and history[-1][0] > history[-2][0]:
        return min(1, p * 1.1)
    elif total_cooperators_current_round < sum(history[-2]) or history[-1][0] <= history[-2][0]:
        return max(0, p * 0.9)
    else:
        return p
```
**Rationale:**

* The Adaptive Predator strategy starts by defecting in the first round to gather information about opponents' initial tendencies.
* By adjusting the probability `p` based on the previous round's outcomes, the strategy adapts to changing game dynamics and exploits opportunities for cooperation or defection.
* The last-round defection ensures that the strategy maximizes its individual payoff when there are no future rounds to consider.

This exploitative strategy is designed to perform well against a wide range of opponent behaviors in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_431 = '''
I'll outline a robust and exploitative strategy for the N-Player Public Goods Game, focusing on adapting to various opponent behaviors while maximizing payoffs.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This initial cooperation serves as a probe to gather information about opponents' strategies.
2. **Exploit Greed**: If the total number of cooperators in the previous round is less than n/2 (i.e., more players defected), defect (D) in the current round. This exploits opponents who are unlikely to contribute to the public good.
3. **Punish Defectors**: If a player i observed another player j defecting in the previous round, and player i cooperated, then defect against player j in the current round with probability 0.7. This punishes defectors and encourages cooperation.
4. **Imitate Success**: Identify the player with the highest payoff in the previous round (excluding yourself). If this player cooperated, cooperate in the current round with probability 0.8; if they defected, defect with probability 0.8. Imitating successful opponents can help adapt to effective strategies.
5. **History-Based Cooperation**: Maintain a history of cooperation rates for each opponent over the past 3 rounds (or fewer if not enough data). If an opponent's cooperation rate is above 0.6, cooperate against them with probability 0.9; otherwise, defect with probability 0.7.

**Edge Cases:**

1. **Last Round**: In the final round, always defect (D), as there are no future payoffs to consider.
2. **Low-Round Numbers**: For rounds 2-3, use a more conservative approach by cooperating with probability 0.4 if no opponents defected in the previous round.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    cooperate_prob = 0.5
    history = {i: [] for i in range(1, n+1)}

    for t in range(r):
        if t == 0:
            # Initial cooperation (50%)
            action = 'C' if random.random() < cooperate_prob else 'D'
        elif t > 0:
            # Exploit greed
            num_coop_prev = sum(history[i][t-1] == 'C' for i in range(1, n+1))
            if num_coop_prev < n/2:
                action = 'D'

            # Punish defectors
            for j in range(1, n+1):
                if history[j][t-1] == 'D' and history[player_id][t-1] == 'C':
                    defect_prob = 0.7
                    action = 'D' if random.random() < defect_prob else 'C'

            # Imitate success
            max_payoff_player = argmax([payoffs[i][t-1] for i in range(1, n+1)])
            imitate_coop_prob = 0.8 if history[max_payoff_player][t-1] == 'C' else 0.2
            action = 'C' if random.random() < imitate_coop_prob else 'D'

            # History-based cooperation
            coop_rates = {i: sum(history[i][-3:]) / min(3, t) for i in range(1, n+1)}
            coop_opponent = next((i for i in range(1, n+1) if coop_rates[i] > 0.6), None)
            if coop_opponent:
                cooperate_prob = 0.9
            else:
                cooperate_prob = 0.7

        # Last round: always defect
        if t == r - 1:
            action = 'D'

        return action
```
The Adaptive Exploiter strategy balances exploration and exploitation by adapting to the observed behavior of opponents while maximizing payoffs. By incorporating history-based cooperation, punishing defectors, imitating successful strategies, and exploiting greed, this approach aims to perform well in a wide range of scenarios against diverse opponent behaviors.
'''

description_EXPLOITATIVE_432 = '''
To develop an exploitative strategy for this N-player Public Goods Game, I'll introduce a hybrid approach that balances adaptability and robustness. This strategy is called "Adaptive Exploiter" (AE).

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C). This initial cooperation serves as an exploratory move to gather information about the opponents' behavior.
2. **Early Rounds (t=2 to t=r/4)**: Implement a " Mirror-and-Exploit" approach:
	* Observe the total number of cooperators in the previous round (Σ(j=1 to n) c_j,t-1).
	* If Σ(j=1 to n) c_j,t-1 > n/2, cooperate (C). This indicates a willingness to contribute among many players.
	* Otherwise, defect (D). This suggests that others are not contributing enough, and exploiting becomes more attractive.
3. **Middle Rounds (t=r/4+1 to t=3r/4)**: Switch to "Gradient-based Exploitation":
	* Calculate the average payoff per round for cooperators (π_C,t-1) and defectors (π_D,t-1) over the previous rounds.
	* If π_C,t-1 > π_D,t-1, cooperate (C). This indicates that cooperation is yielding higher payoffs on average.
	* Otherwise, defect (D).
4. **Late Rounds (t=3r/4+1 to t=r)**: Employ "Endgame Exploitation":
	* Observe the total number of cooperators in the previous round (Σ(j=1 to n) c_j,t-1).
	* If Σ(j=1 to n) c_j,t-1 < n/2, defect (D). This is an attempt to exploit the remaining cooperators.
	* Otherwise, cooperate (C).

**Edge Cases:**

* **Last Round (t=r)**: Cooperate (C) if there are more than n/2 cooperators in the previous round; otherwise, defect (D).
* **Tiebreakers**: In cases where π_C,t-1 = π_D,t-1 or Σ(j=1 to n) c_j,t-1 = n/2, cooperate (C).

**Pseudocode:**

```
function AdaptiveExploiter(n, k, r):
  for t in range(1, r+1):
    if t == 1:
      # Initial cooperation
      action[t] = C
    elif t <= r/4:
      # Mirror-and-Exploit
      sum_coop[t-1] = count Cooperators in round t-1
      if sum_coop[t-1] > n/2:
        action[t] = C
      else:
        action[t] = D
    elif t <= 3r/4:
      # Gradient-based Exploitation
      avg_payoff_C[t-1] = average payoff for Cooperators in previous rounds
      avg_payoff_D[t-1] = average payoff for Defectors in previous rounds
      if avg_payoff_C[t-1] > avg_payoff_D[t-1]:
        action[t] = C
      else:
        action[t] = D
    else:
      # Endgame Exploitation
      sum_coop[t-1] = count Cooperators in round t-1
      if sum_coop[t-1] < n/2:
        action[t] = D
      else:
        action[t] = C

  return actions
```

This Adaptive Exploiter strategy balances exploration, adaptation, and exploitation to maximize payoffs in the N-player Public Goods Game. By adjusting its behavior based on the game's history, it aims to capitalize on opponents' cooperative tendencies while minimizing losses from exploitation.
'''

description_EXPLOITATIVE_433 = '''
**Exploitative Strategy: "Adaptive Predator"**

Decision Rules:

1. **Initial Round**: Defect (D) in the first round to gather information about opponents' behaviors.
2. **Observation Phase**: For rounds 2 to r/2 (where r is the total number of rounds), cooperate (C) if the majority of opponents cooperated in the previous round, and defect (D) otherwise. This phase aims to identify cooperative opponents and learn their strategies.
3. **Exploitation Phase**: After the observation phase, switch to a more exploitative strategy:
	* If an opponent has consistently cooperated (C) for at least 75% of the rounds in the observation phase, defect (D) against them.
	* If an opponent has consistently defected (D) or shown mixed behavior, cooperate (C) with a probability proportional to their cooperation rate in the observation phase. This aims to exploit opponents who are likely to cooperate and minimize losses against those who don't.
4. **Last Round**: Defect (D) in the last round to maximize payoffs, regardless of opponents' previous behaviors.

Pseudocode:
```python
def adaptive_predator(n, r, k, history):
  # Initial round
  if current_round == 1:
    return D

  # Observation phase
  if current_round <= r / 2:
    majority_cooperated = sum(history[-1]) > n / 2
    if majority_cooperated:
      return C
    else:
      return D

  # Exploitation phase
  opponent_behaviors = analyze_history(history)
  for opponent in range(n):
    if opponent_behaviors[opponent] == 'consistent_C':
      return D
    elif opponent_behaviors[opponent] == 'consistent_D' or 'mixed':
      cooperate_prob = opponent_behaviors[opponent]['cooperation_rate']
      return C with probability cooperate_prob

  # Last round
  if current_round == r:
    return D
```
**Rationale:**

1. The initial defect in the first round allows us to gather information about opponents' behaviors without committing to cooperation.
2. During the observation phase, we identify cooperative opponents and learn their strategies, which informs our exploitation phase decisions.
3. In the exploitation phase, we target consistently cooperative opponents with defection, as they are likely to continue cooperating, and minimize losses against others by adapting our cooperation rate.
4. Defecting in the last round ensures we maximize payoffs, regardless of opponents' previous behaviors.

This strategy is designed to be robust against a wide range of opponent behaviors and exploits those who cooperate excessively or show predictable patterns.
'''

description_EXPLOITATIVE_434 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

**Overview**
The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors in the N-Player Public Goods Game. It uses a combination of observation, adaptation, and strategic manipulation to maximize its payoff.

**Decision Rules**

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) with probability 0.5 to gather information about opponents' strategies.
2. **Observation Phase**: For each round t > 3, observe the previous round's actions and payoffs of all players.
3. **Adaptive Threshold**: Calculate the average cooperation rate of opponents in the last 2 rounds (excluding the current player). If this rate is above a threshold θ = k/n, defect (D) with probability 0.8; otherwise, cooperate (C) with probability 0.8.
4. **Exploitation Phase**: If the opponent's average cooperation rate is high (> θ), try to exploit them by playing D in the next round. This encourages opponents to reduce their cooperation rate.
5. **Punishment Mechanism**: If an opponent defects (D) while others cooperate (C), play D against that opponent in the next round with probability 0.9. This discourages opportunistic behavior.

**Edge Cases**

1. **First Round**: Cooperate (C) to gather information and avoid early exploitation.
2. **Last Round**: Defect (D) as there's no future interaction to consider.
3. **Tie-Breaking**: In case of a tie in the average cooperation rate, cooperate (C) with probability 0.6.

**Pseudocode**
```
Initialize variables:
θ = k/n
exploration_rounds = 3

For each round t:
  If t <= exploration_rounds:
    Play C with probability 0.5
  Else:
    Observe previous round's actions and payoffs of all players
    Calculate average cooperation rate of opponents (excluding current player)
    If average_cooperation_rate > θ:
      Play D with probability 0.8
    Else:
      Play C with probability 0.8

    If opponent defects while others cooperate:
      Play D against that opponent in the next round with probability 0.9

  Update variables for next round
```
This strategy is designed to be robust and adaptive, exploiting a wide range of opponent behaviors while minimizing its own vulnerability to exploitation. By observing opponents' actions and adjusting its strategy accordingly, the Adaptive Exploiter can maximize its payoff in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_435 = '''
I'll design a exploitative strategy for this N-Player Public Goods Game, which I'll call "Adaptive Exploiter" (AE). This strategy will adapt to the game's history and exploit opponents' behaviors while being robust to various scenarios.

**Decision Rules:**

1. **Initial Rounds:** In the first two rounds, AE cooperates (C) unconditionally to gather information about the opponents' tendencies.
2. **Exploitation Mode:** From round 3 onwards, AE uses a combination of two metrics:
	* `coop_rate`: The average cooperation rate of all players in previous rounds (excluding the current player).
	* `avg_payoff`: The average payoff received by all players in previous rounds (excluding the current player).

AE computes these values at the beginning of each round.

**Action Selection:**

1. If `coop_rate` > 0.5 and `avg_payoff` > 1, AE cooperates (C). This indicates that opponents are generally cooperative and willing to contribute to the public good.
2. If `coop_rate` ≤ 0.5 or `avg_payoff` ≤ 1, AE defects (D). This suggests that opponents are either uncooperative or not contributing enough to make cooperation worthwhile.

**Edge Case Handling:**

* **Last Round:** In the final round, AE cooperates (C) if `coop_rate` > 0.5 and `avg_payoff` > 1, otherwise defects (D). This is because there's no future round to punish or reward opponents.
* **Tie-breaking:** If AE's calculations result in a tie between cooperating and defecting, it defaults to cooperating (C).
* **Opponent Detection:** If an opponent consistently cooperates or defects, AE adapts by updating its `coop_rate` and `avg_payoff` metrics accordingly.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    coop_rate = 0.5  # initial cooperation rate
    avg_payoff = 1   # initial average payoff

    for t in range(r):
        if t < 2:
            action = C  # cooperate in first two rounds
        else:
            coop_rate_update = calculate_coop_rate(opponents_actions)
            avg_payoff_update = calculate_avg_payoff(opponents_payoffs)

            if coop_rate_update > 0.5 and avg_payoff_update > 1:
                action = C
            else:
                action = D

            coop_rate = coop_rate_update
            avg_payoff = avg_payoff_update

        # update opponents' actions and payoffs for next round

    return action
```
This Adaptive Exploiter strategy is designed to balance cooperation and exploitation, adapting to the game's history while being robust to various opponent behaviors. By monitoring cooperation rates and average payoffs, AE can effectively identify when to cooperate or defect to maximize its own payoff.

Please let me know if you'd like me to elaborate on any aspect of this strategy!
'''

description_EXPLOITATIVE_436 = '''
**Exploitative Strategy: Adaptive Cooperate-to-Defect (ACD)**

**Overview**

The ACD strategy is designed to exploit a wide range of opponent behaviors by adapting its cooperation level based on the game's history and parameters. It uses a combination of retaliatory and opportunistic elements to maximize payoffs.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to test opponents' initial behavior.
2. **Cooperation Threshold**: Calculate the cooperation threshold `T` as `(k/n) * (n/2)` , where `n` is the number of players and `k` is the multiplication factor.
3. **Previous Round Analysis**:
	* If the total number of cooperators in the previous round (`Σ(j=1 to n) c_j`) is above the cooperation threshold `T`, play Cooperate (C).
	* Otherwise, play Defect (D).
4. **Opponent Behavior Tracking**: Maintain a record of opponents' past actions and calculate their average cooperation rate.
5. **Exploitation**: If an opponent's average cooperation rate is below 0.5, consider them as "unreliable" and always play Defect (D) against them.

**Pseudocode**
```python
def ACD(n, k, r):
    T = (k/n) * (n/2)  # cooperation threshold
    opponent_behavior = {}  # track opponents' past actions

    for t in range(r):  # rounds
        if t == 0:  # initial round
            return C  # cooperate

        prev_round_cooperators = sum(opponent_behavior.values())
        if prev_round_cooperators >= T:
            return C  # cooperate

        unreliable_opponents = [i for i, rate in opponent_behavior.items() if rate < 0.5]
        if len(unreliable_opponents) > 0:
            return D  # defect against unreliable opponents

        # otherwise, play Defect (D)
        return D
```
**Edge Cases**

* **Last Round**: Play Defect (D), as cooperation will not affect future payoffs.
* **Single Opponent Left**: If only one opponent remains, always play Defect (D) to maximize payoff.

The ACD strategy is designed to be robust and exploitative, capitalizing on opponents' willingness to cooperate while avoiding over-cooperation. By adapting to the game's history and parameters, it aims to outperform a wide range of opponent strategies in the tournament.
'''

description_EXPLOITATIVE_437 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

Decision Rules:

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) to gather information about opponents' behaviors and potential cooperation levels.
2. **Opponent Profiling**: Observe the number of cooperators in each round (c_t) and calculate the average cooperation rate (avg_c) over the last 3 rounds.
3. **Exploitation Threshold**: Calculate a dynamic threshold value (θ) based on the game parameters and opponent behavior:
	* θ = k / n \* (1 - avg_c)
4. **Cooperation Condition**: Cooperate (C) if the expected payoff from cooperation exceeds the exploitation threshold:
	* π_C > 1 - c_i + (k/n) \* (avg_c \* n) > θ
5. **Defection Rule**: Defect (D) otherwise.

Edge Cases:

* **First Round**: Cooperate (C) to gather initial information.
* **Last Round**: Defect (D), as there is no future interaction to influence.
* **Ties in Opponent Profiling**: In case of ties, favor cooperation to maintain a positive reputation and encourage reciprocity.

Pseudocode:
```python
# Initialize variables
n = number_of_players
k = multiplication_factor
r = number_of_rounds
avg_c = 0  # average cooperation rate

for round in range(1, r + 1):
    if round <= 3:  # Initial Exploration
        action = C
    else:
        # Opponent Profiling
        c_t = count_cooperators_in_last_round()
        avg_c = (avg_c * 2 + c_t) / 3

        # Exploitation Threshold
        θ = k / n * (1 - avg_c)

        # Cooperation Condition
        π_C = 1 - c_i + (k/n) * (avg_c * n)
        if π_C > θ:
            action = C
        else:
            action = D

    # Perform action and update history
    perform_action(action)
```
This strategy is designed to be exploitative, as it prioritizes defecting when the expected payoff from cooperation is low. By adapting to opponent behavior through profiling and adjusting the exploitation threshold, "Adaptive Exploiter" aims to maximize its own payoff while minimizing cooperation.

The initial exploration phase allows for gathering information about opponents' behaviors, which is used to inform the cooperation condition in later rounds. The dynamic threshold value (θ) ensures that the strategy remains responsive to changes in opponent behavior and game parameters.

By being robust to a wide range of opponent behaviors, "Adaptive Exploiter" is well-suited for a tournament environment where various strategies will be employed by other AI systems.
'''

description_EXPLOITATIVE_438 = '''
I'll design a exploitative strategy for this N-Player Public Goods Game that adapts to various opponent behaviors and maximizes its payoff.

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Round**: In the first round, AE plays Cooperate (C) to gather information about the opponents' behavior.
2. **Contribution Threshold**: Set a contribution threshold `theta` = `(n-1)/(k-1)` , which represents the minimum number of cooperators required for AE to cooperate in subsequent rounds.
3. **Round t > 1**:
	* If the total number of cooperators in round `t-1` is greater than or equal to `theta`, AE plays C.
	* Otherwise, AE plays Defect (D).
4. **Exploitation**: If AE observes a sufficient number of cooperators (`>= theta`) in consecutive rounds (specifically, 3 rounds), it will start playing D to exploit the cooperation.

**Pseudocode:**
```python
def Adaptive_Exploiter(n, k):
    # Initialize variables
    theta = (n-1)/(k-1)
    prev_cooperators = 0

    for t in range(1, r+1):  # Loop over rounds
        if t == 1:
            action = C  # Cooperate in the first round
        else:
            cooperators_t_minus_1 = count_cooperators(prev_round)
            if cooperators_t_minus_1 >= theta:
                action = C
            else:
                action = D

        # Update prev_cooperators for next round
        prev_cooperators = cooperators_t_minus_1

        # Exploitation check
        if t > 3 and all(cooperators >= theta for cooperators in [prev_cooperators, count_cooperators(t-2), count_cooperators(t-3)]):
            action = D  # Start exploiting cooperation

        return action
```
**Handling Edge Cases:**

* **First Round**: AE plays Cooperate (C) to gather information.
* **Last Round**: AE will play according to its usual decision rules, as there's no special handling for the last round.
* **Ties in cooperators count**: In case of a tie, AE will default to Defect (D).

**Exploitative Mindset:**
The Adaptive Exploiter strategy aligns with an exploitative mindset by:

1. Initially cooperating to gather information and encourage cooperation from others.
2. Adapting its behavior based on the number of cooperators in previous rounds.
3. Exploiting the cooperation when a sufficient number of opponents cooperate consistently.

By doing so, AE aims to maximize its payoff while taking advantage of the cooperative behavior of other players.

Please let me know if this meets your requirements or if you'd like me to clarify any aspects!
'''

description_EXPLOITATIVE_439 = '''
**Exploitative Strategy: "Adaptive Cooperator"**

This strategy aims to maximize payoff by adapting to the behavior of other players while maintaining a level of cooperation to exploit potential contributors.

**Decision Rules:**

1. **Initial Rounds (t = 1, 2):** Cooperate (C) in the first two rounds to gather information about opponents' behavior and encourage cooperation.
2. **Cooperation Threshold:** Calculate the average number of cooperators in previous rounds (`avg_coop`). If `avg_coop` is greater than or equal to `n/2`, cooperate (C). Otherwise, defect (D).
3. **Defection Response:** If a player defects while there are more cooperators than defectors, punish them by defecting in the next round.
4. **Exploitation Opportunity:** If there are more cooperators than defectors and the opponent's payoff is higher than yours in the previous round, exploit the situation by cooperating (C).
5. **Risk Aversion:** In the last two rounds (`t = r-1` or `r`), defect (D) to avoid potential losses.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
avg_coop = 0
prev_payoffs = []

# Loop through each round
for t in range(1, r+1):
    # Initial rounds: Cooperate
    if t <= 2:
        action = C
    
    # Calculate cooperation threshold
    avg_coop = sum(prev_payoffs) / (t-1)
    
    # Cooperation threshold met: Cooperate
    if avg_coop >= n/2:
        action = C
    else:
        action = D
    
    # Defection response
    if prev_payoffs[t-1] > 0 and sum(prev_payoffs) < n/2:
        action = D
    
    # Exploitation opportunity
    if avg_coop > n/2 and prev_payoffs[t-1] > my_prev_payoff:
        action = C
    
    # Risk aversion in last two rounds
    if t >= r-1:
        action = D
    
    # Update payoffs and history
    my_payoff = calculate_payoff(action, prev_payoffs)
    prev_payoffs.append(my_payoff)
```
**Edge Cases:**

* First round: Cooperate to gather information.
* Last two rounds: Defect to avoid potential losses.
* When `n` is odd: Round down the cooperation threshold (`avg_coop >= floor(n/2)`).

This strategy aims to balance cooperation and exploitation, adapting to the behavior of other players while maintaining a level of risk aversion. By cooperating initially, it encourages opponents to contribute, then adjusts its strategy based on their responses. The goal is to maximize payoff by exploiting opportunities while minimizing losses in the final rounds.
'''

description_EXPLOITATIVE_440 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors in the N-Player Public Goods Game. This strategy adapts to the game's history and parameters, making it robust against various opponents.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
 Cooperate (C) with probability 0.5 to gather information about the opponents' behavior.
2. **Opponent Classification**:
After Round 3, classify opponents into three categories based on their cooperation rates:
	* **Cooperators**: Played C in ≥ 2/3 of rounds.
	* **Defectors**: Played D in ≥ 2/3 of rounds.
	* **Unpredictables**: Did not meet the above criteria.
3. **Main Strategy**:
For each round (t > 3):
	* If most opponents (> n/2) are Cooperators, Defect (D).
	* If most opponents (> n/2) are Defectors, Cooperate (C).
	* If opponents are mostly Unpredictables or evenly split between Cooperators and Defectors:
		+ Calculate the average payoff per player in the previous round.
		+ If this average payoff is greater than 1.5, Cooperate (C). Otherwise, Defect (D).

**Edge Cases:**

* **First Round**: Cooperate (C) with probability 0.5 to gather initial information.
* **Last Round**: Defect (D), as there's no future benefit from cooperation.
* **Ties in Opponent Classification**: Break ties by considering the opponents' behavior in the most recent round.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
r = number of rounds
k = multiplication factor
coop_rates = array to store cooperation rates for each player

# Initial Exploration (Rounds 1-3)
for t in range(1, 4):
    play C with probability 0.5

# Opponent Classification
classify_opponents(coop_rates)

# Main Strategy
for t in range(4, r+1):
    opponent_types = classify_opponents(coop_rates)
    if most opponents are Cooperators:
        play D
    elif most opponents are Defectors:
        play C
    else:
        avg_payoff_prev_round = calculate_average_payoff(t-1)
        if avg_payoff_prev_round > 1.5:
            play C
        else:
            play D

# Last Round
play D

def classify_opponents(coop_rates):
    # Classify opponents into Cooperators, Defectors, and Unpredictables based on coop_rates
    pass

def calculate_average_payoff(t):
    # Calculate average payoff per player in round t
    pass
```
This strategy is exploitative as it takes advantage of the opponents' behavior to maximize its own payoffs. By initially exploring the opponents' actions and adapting to their classifications, the Adaptive Predator can effectively respond to various strategies and achieve a high total payoff over the game's rounds.
'''

description_EXPLOITATIVE_441 = '''
Here's a strategic approach for playing the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) in Round 1 to gather information about other players' behavior.
	* In Rounds 2 and 3, Defect (D) if the average number of cooperators in the previous round is less than or equal to n/2. Otherwise, Cooperate.
2. **Exploitation Phase**:
	* From Round 4 onwards, use a combination of two metrics: `Average Cooperation Rate` (ACR) and `Recent Cooperation Trend` (RCT).
	* Calculate ACR as the average number of cooperators in the last 3 rounds.
	* Calculate RCT by comparing the number of cooperators in the most recent round with the one before it. If the trend is increasing, set RCT to +1; if decreasing, set RCT to -1; otherwise, set RCT to 0.
	* Cooperate if:
		+ ACR > (n/2) and RCT >= 0
		+ k/n < 1.5 and the number of cooperators in the most recent round is greater than or equal to n/2
	* Defect otherwise.

**Edge Cases:**

* **Last Round**: If it's the last round, Defect regardless of previous behavior.
* **Low Cooperation Environment**: If ACR < (n/4) for 3 consecutive rounds, switch to a " Punisher" strategy:
	+ Cooperate only if at least one other player cooperated in the most recent round.
	+ Otherwise, Defect.

**Pseudocode:**

```markdown
Variables:
- `cooperation_rate` := average number of cooperators in last 3 rounds (ACR)
- `recent_trend` := Recent Cooperation Trend (RCT)

function adaptive_exploiter(n, k, current_round):
    if current_round <= 3:
        # Initial Exploration Phase
        if current_round == 1:
            return Cooperate
        elif cooperation_rate <= n/2:
            return Defect
        else:
            return Cooperate

    # Exploitation Phase
    if cooperation_rate > n/2 and recent_trend >= 0:
        return Cooperate
    elif k/n < 1.5 and number_of_cooperators >= n/2:
        return Cooperate
    else:
        return Defect

    # Last Round Edge Case
    if current_round == total_rounds:
        return Defect

    # Low Cooperation Environment Edge Case
    if cooperation_rate < n/4 for 3 consecutive rounds:
        # Switch to Punisher strategy
        if other_players_cooperated_in_last_round:
            return Cooperate
        else:
            return Defect
```

**Rationale:**

This strategy combines elements of exploration, exploitation, and punishment. By initially exploring the cooperation landscape and adapting to changing conditions, we can effectively exploit the willingness of others to cooperate while avoiding being exploited ourselves. The Punisher strategy in low cooperation environments ensures that we don't get taken advantage of by consistently defecting players.

**Exploitative Mindset:**

This strategy prioritizes self-interest and adaptability over pure cooperation or altruism. By continuously monitoring the cooperation rate and recent trends, we can quickly adjust our behavior to maximize payoffs. The Punisher strategy adds a layer of robustness against exploitative opponents, ensuring that we don't become easy prey in low cooperation environments.

This strategy should perform well in a tournament setting, as it balances short-term self-interest with long-term adaptability and robustness.
'''

description_EXPLOITATIVE_442 = '''
To design an exploitative strategy for this N-Player Public Goods Game that adapts to various opponent behaviors and only depends on game parameters and history, we'll focus on a mix of cooperation and defection, leveraging information from previous rounds to inform our decisions.

**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Round (Round 1):**
   - Cooperate (C) in the first round to gather initial data on how other players behave and to potentially encourage cooperation.
   
2. **Early Rounds (Rounds 2-5):**
   - If in the previous round, more than half of the players cooperated, cooperate (C). This encourages continued cooperation if it seems beneficial.
   - Otherwise, defect (D).

3. **Mid Game (After Round 5):**
   - Calculate the average payoff from cooperating and defecting over the last few rounds for all players.
     - If your average payoff from cooperating is higher than from defecting, cooperate in the next round.
     - If the opposite holds true or if you've mostly defected with good payoffs, defect.

4. **Late Game (Last Few Rounds):**
   - Observe the pattern of cooperation and defection among other players over recent rounds.
     - If most players have been cooperating, start to defect more frequently to exploit their cooperation without completely discouraging them from future cooperation.
     - If many are defecting, mix your actions but lean towards cooperation if your payoffs suggest it's beneficial.

5. **Adaptive Component:**
   - Track the frequency of cooperation and defection among all players over recent rounds.
     - Adjust your strategy based on this pattern. For example, if you notice a player or a group often cooperates after someone else has defected, consider exploiting this by defecting first and then cooperating.

6. **Punishment Mechanism:**
   - Identify players who consistently defect despite being in situations where cooperation would yield higher payoffs for everyone.
     - Periodically cooperate when these players are expected to defect to punish them and encourage better behavior.

**Handling Edge Cases:**

- In the last round, consider maximizing your payoff regardless of others' actions since there's no future interaction to influence. If the game history suggests that cooperation has been beneficial, opt for it; otherwise, defect.
- For games with a small number of rounds (e.g., r=2), focus more on exploiting in the second round based on the outcome of the first.

**Pseudocode Overview:**

```
# Initialize variables
round = 0
cooperation_rate = 0
defection_rate = 0
payoffs_cooperate = []
payoffs_defect = []

while round < total_rounds:
    if round == 0:
        # First round, cooperate to gather data
        action = 'C'
    elif round <= 5:
        # Early rounds, check cooperation rate of others
        if cooperation_rate > (n / 2):
            action = 'C'
        else:
            action = 'D'
    else:
        # Mid and late game strategy
        avg_payoff_coop = sum(payoffs_cooperate) / len(payoffs_cooperate)
        avg_payoff_defect = sum(payoffs_defect) / len(payoffs_defect)
        
        if avg_payoff_coop > avg_payoff_defect:
            action = 'C'
        else:
            # Check recent patterns and adjust accordingly
            action = adaptive_adjustment(cooperation_rate, defection_rate)

    # Update rates and payoffs after observing actions of others
    cooperation_rate = update_cooperation_rate()
    defection_rate = update_defection_rate()
    if action == 'C':
        payoffs_cooperate.append(calculate_payoff())
    else:
        payoffs_defect.append(calculate_payoff())

    round += 1

def adaptive_adjustment(coop_rate, defect_rate):
    # Logic to adjust based on observed patterns
    pass

def update_cooperation_rate():
    # Logic to calculate the rate of cooperation among all players
    pass

def update_defection_rate():
    # Logic to calculate the rate of defection among all players
    pass

def calculate_payoff():
    # Calculate payoff based on game rules and actions taken
    pass
```

This strategy aims to balance cooperation and exploitation, adapting its approach as more information becomes available about other players' behaviors. By mixing initial cooperation with adaptive exploitation, it seeks to maximize payoffs while influencing the behavior of other players in beneficial ways.
'''

description_EXPLOITATIVE_443 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy depends solely on the game parameters and observed actions.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about opponents' tendencies.
2. **Exploitation Phase:** For rounds t > 2, calculate the average cooperation rate of all opponents in the previous round:

   `avg_coop_prev = Σ(c_j,t-1) / n`

   If `avg_coop_prev` is above a certain threshold (`θ`) or if the total payoff from cooperating in the previous round was higher than defecting, cooperate (C). Otherwise, defect (D).

   `θ` is a dynamic threshold that adjusts based on the game's history. Initialize `θ = 0.5`.

3. **Threshold Adjustment:** Update `θ` every `r/2` rounds (rounded down) to reflect changes in opponents' behaviors:

   - If average cooperation rate over the last `r/2` rounds is higher than `θ`, decrease `θ` by 10%.
   - Otherwise, increase `θ` by 5%.

4. **Punishment Mechanism:** To deter opponents from exploiting our cooperative behavior, introduce a punishment phase when we detect exploitative behavior:

   - If an opponent defects (D) in two consecutive rounds while the average cooperation rate is above `θ`, flag them as an exploiter.
   - For the next 2-3 rounds, always defect (D) against flagged exploiters.

**Edge Cases:**

1. **Last Round:** Defect (D) to maximize individual payoff, as future reciprocity is not possible.
2. **Opponent Cooperation:** If all opponents cooperate (C) in a round, and our current action would lead to the highest total cooperation rate, adjust our action to maintain this optimal state.

**Pseudocode:**

```
# Initialize Adaptive Predator strategy
def adaptive_predator(n, r, k):
  theta = 0.5  # Dynamic threshold

  for t in range(r):
    if t <= 2:  # Initial rounds
      return 'C'  # Cooperate to gather information
    
    # Calculate average cooperation rate of all opponents
    avg_coop_prev = sum(c_j[t-1] for j in range(n)) / n
    
    if avg_coop_prev > theta or payoff(t-1, 'C') > payoff(t-1, 'D'):
      return 'C'  # Cooperate
    else:
      return 'D'  # Defect

    # Update dynamic threshold every r/2 rounds
    if t % (r//2) == 0 and t != 0:
      if sum(c_j[t-r//2] for j in range(n)) / n > theta:
        theta *= 0.9  # Decrease θ by 10%
      else:
        theta *= 1.05  # Increase θ by 5%

    # Punishment mechanism
    exploiters = []
    for i in range(n):
      if c_i[t-1] == 'D' and c_i[t-2] == 'D' and avg_coop_prev > theta:
        exploiters.append(i)
    
    for t_punish in range(2, 4):  # Punishment phase
      if opponent_id in exploiters:
        return 'D'

def payoff(t, action):
  # Calculate payoff based on game parameters and history
```

This Adaptive Predator strategy is designed to balance cooperation with strategic exploitation. By adapting to the game's history and opponents' behaviors, we aim to maximize our total payoff while deterring exploitative behavior from others.
'''

description_EXPLOITATIVE_444 = '''
To design an exploitative strategy for this N-Player Public Goods Game that is adaptive and robust to various opponent behaviors, we'll focus on a combination of initial exploratory behavior, adaptivity based on observed outcomes, and a mechanism to ensure participation in the public good while minimizing exploitation by others.

**Strategy Name:** Adaptive Exploiter

### Decision Rules:

1. **Initial Exploration (Rounds 1-3):**
   - Cooperate (C) with probability 0.5.
   - This phase allows for initial assessment of opponents' behaviors and does not commit too heavily in either direction early on.

2. **Adaptive Phase:**
   - After Round 3, calculate the average cooperation rate (`avg_coop`) among all players up to that point.
     - `avg_coop = (Total C's observed / Total actions observed)`
   - Calculate the average payoff (`avg_payoff`) for cooperating and defecting separately based on observed outcomes.

3. **Decision Logic:**
   - If `avg_coop` is high (> 0.7), cooperate with probability `min(1, k/n + avg_payoff_C / avg_payoff_D)`.
     - This means if cooperation is prevalent and pays off well relative to defecting, increase the likelihood of cooperating.
   - Otherwise, if `avg_coop` is low (≤ 0.7), but `k/n > avg_payoff_D / avg_payoff_C`, cooperate with probability `max(0, k/n - avg_payoff_D / avg_payoff_C)`.
     - If cooperation isn't prevalent but offers better returns than defecting when considering the multiplication factor and payoffs, there's still a rationale to cooperate, albeit cautiously.
   - In all other cases, prioritize defection (`D`) with high probability (`0.8` or higher).
     - This is because if neither condition for cooperation is met, exploiting others through defection becomes more attractive.

### Handling Edge Cases:

- **First Round:** Cooperate with a fixed probability of 0.5 (as per the initial exploration phase) to start gathering information.
- **Last Round (`r`):** Defect (`D`) with high probability (`0.9` or higher), as there's no future round to influence and every player is likely to defect in the final round due to lack of future consequences.
- **Ties or Indeterminate Situations:** In cases where calculations lead to indeterminate decisions, default to the last action taken (if possible) or a predetermined safe choice like defection.

### Pseudocode:

```python
def adaptive_exploiter(n, k, r):
    avg_coop = 0.5  # Initial guess for cooperation rate
    avg_payoff_C = 1  # Average payoff for cooperating
    avg_payoff_D = 1  # Average payoff for defecting
    
    for round in range(1, r+1):
        if round <= 3:
            action = 'C' if random.random() < 0.5 else 'D'
        else:
            adapt_prob_C = min(1, k/n + avg_payoff_C / avg_payoff_D)
            adapt_prob_D = max(0, k/n - avg_payoff_D / avg_payoff_C)
            
            if avg_coop > 0.7:
                action = 'C' if random.random() < adapt_prob_C else 'D'
            elif k/n > avg_payoff_D / avg_payoff_C:
                action = 'C' if random.random() < adapt_prob_D else 'D'
            else:
                action = 'D' if random.random() < 0.8 else 'C'
                
        # Update statistics based on observed actions and payoffs
        if round > 1:
            avg_coop = update_avg_coop(avg_coop, last_actions)
            avg_payoff_C, avg_payoff_D = update_avg_payoffs(last_payoffs)
            
    return action

def update_avg_coop(avg_coop, last_actions):
    # Implement logic to calculate new average cooperation rate
    pass
    
def update_avg_payoffs(last_payoffs):
    # Implement logic to update average payoffs for cooperating and defecting
    pass
```

This strategy aims to balance the desire to exploit others' cooperative tendencies with the need to contribute to public goods when it is profitable. It adapts based on observed behavior and adjusts its cooperation rate accordingly, making it a robust choice in a competitive environment where other players' strategies are unknown.
'''

description_EXPLOITATIVE_445 = '''
Here's a exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Exploration Phase**: In the first round, play Cooperate (C) to gather information about opponents' behavior and estimate their cooperation rates.
2. **Opponent Classification**: Based on the observed actions in previous rounds, classify each opponent into one of three categories:
	* **Cooperators** (COOP): Players who cooperate more than 50% of the time.
	* **Defectors** (DEF): Players who defect more than 50% of the time.
	* **Unpredictables** (UNPRED): Players with cooperation rates between 40% and 60%.
3. **Adaptive Play**: In subsequent rounds, use the following decision rules:
	* If most opponents (> n/2) are COOP or UNPRED, play Cooperate (C).
	* If most opponents (> n/2) are DEF, play Defect (D).
	* If the number of COOP and DEF opponents is equal, play Cooperate (C) if k > 1.5, otherwise play Defect (D).
4. **Exploitation**: When facing a single opponent who is clearly a DEF or UNPRED, play Defect (D) to exploit their contribution.
5. **Late-Game Adjustment**: In the last 20% of rounds (i.e., when r < 0.2), adjust the strategy to maximize personal payoff:
	* If average payoff is below the expected value of cooperation (k/n \* n), switch to Defect (D) for the remaining rounds.

**Pseudocode:**
```markdown
function AdaptiveExploiter(n, k, r):
    # Initialize opponent classification and counts
    opponents = [UNPRED] * n
    coop_counts = [0] * n

    # Initial exploration phase
    if round == 1:
        play C

    # Classify opponents based on previous actions
    for i in range(n):
        if coop_counts[i] / (round - 1) > 0.5:
            opponents[i] = COOP
        elif coop_counts[i] / (round - 1) < 0.4:
            opponents[i] = DEF
        else:
            opponents[i] = UNPRED

    # Adaptive play
    if most opponents (> n/2) are COOP or UNPRED:
        play C
    elif most opponents (> n/2) are DEF:
        play D
    elif equal number of COOP and DEF opponents:
        if k > 1.5:
            play C
        else:
            play D

    # Exploitation
    for i in range(n):
        if opponents[i] is DEF or UNPRED:
            play D

    # Late-game adjustment
    if round > 0.8 * r:
        if average_payoff < k/n * n:
            play D
```
This strategy aims to balance cooperation and defection to maximize personal payoff while adapting to the evolving behavior of opponents. By initially cooperating, AE gathers information about opponents' tendencies and adjusts its decision-making accordingly. The exploitation phase targets specific opponents who are likely to cooperate, allowing AE to take advantage of their contributions. Finally, the late-game adjustment ensures that AE maximizes its payoff in the final rounds.
'''

description_EXPLOITATIVE_446 = '''
**Exploitative Strategy: Adaptive Tit-for-Tat with Public Goods Twist**

Our strategy, dubbed "Public Goods Punisher" (PGP), is designed to exploit a wide range of opponent behaviors while being adaptive and robust. PGP combines elements of Tit-for-Tat (TFT) with a public goods twist to maximize payoffs.

**Decision Rules:**

1. **First Round:** Cooperate (C). This initial cooperation sets the tone for potential mutual cooperation and allows us to gauge opponents' strategies.
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust our action accordingly:
	* If the total number of cooperators in the previous round is greater than or equal to `n/2` (i.e., at least half of the players cooperated), cooperate (C). This encourages continued cooperation when it seems beneficial.
	* If the total number of cooperators in the previous round is less than `n/2`, defect (D). This punishes opponents for not contributing enough to the public good.
3. **Exception:** If we observe that an opponent has defected in every round so far, defect (D) as well. This prevents exploitation by unconditional defectors.

**Handling Edge Cases:**

* **Last Round:** In the final round, always cooperate (C). Since there's no future interaction, contributing to the public good can still yield a positive payoff.
* **Opponent Strategies:** If an opponent is identified as a consistent cooperator or defector, adjust our strategy accordingly:
	+ Against a consistent cooperator, cooperate (C) to maintain mutual cooperation.
	+ Against a consistent defector, defect (D) to avoid exploitation.

**Pseudocode:**
```python
def PGP(n, k, r):
    # Initialize variables
    total_cooperators = 0
    opponent_strategies = {i: None for i in range(1, n+1)}

    for t in range(r):
        if t == 0:
            action = 'C'  # Cooperate in the first round
        else:
            # Observe previous round's outcome
            total_cooperators = sum([c_i for c_i in actions[t-1]])
            if total_cooperators >= n/2:
                action = 'C'
            else:
                action = 'D'

            # Adjust strategy based on opponent behavior
            for i, opp_action in enumerate(actions[t-1]):
                if opp_action == 'D' and opponent_strategies[i+1] is None:
                    opponent_strategies[i+1] = 'defector'
                elif opp_action == 'C' and opponent_strategies[i+1] is None:
                    opponent_strategies[i+1] = 'cooperator'

            # Exception: defect if opponent has defected every round
            for i, strategy in opponent_strategies.items():
                if strategy == 'defector':
                    action = 'D'
                    break

        # Last round exception
        if t == r-1:
            action = 'C'

        yield action
```
This strategy is designed to be exploitative while being adaptive and robust. By incorporating elements of TFT and adjusting our behavior based on the public goods game's specifics, we aim to maximize payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_447 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its own payoff by adapting to the opponents' behavior and exploiting their cooperation.

**Decision Rules:**

1. **Initial Rounds (t = 1, ..., n)**: In the first `n` rounds, play Cooperate (`C`) to gather information about the opponents' behavior.
2. **Exploration Phase (t > n)**:
	* Calculate the average payoff of each opponent in the last `n` rounds (`avg_payoff_i`).
	* Identify the most cooperative opponent (`max_coop_opponent`) by finding the opponent with the highest `avg_payoff_i`.
	* Play Defect (`D`) if `max_coop_opponent`'s `avg_payoff_i` is above a certain threshold (e.g., 0.8).
	* Otherwise, play Cooperate (`C`).
3. **Exploitation Phase**:
	* If the opponent's cooperation rate (`coop_rate_i`) in the last `n` rounds is above a certain threshold (e.g., 0.5), play Defect (`D`).
	* Otherwise, play Cooperate (`C`).

Pseudocode:
```python
def adaptive_exploiter(game_history):
    n = game_parameters['n']
    k = game_parameters['k']
    t = len(game_history)

    if t <= n:  # Initial rounds
        return 'C'

    avg_payoffs = [0] * n
    for i in range(n):
        opponent_history = [round['actions'][i] for round in game_history[-n:]]
        coop_rate_i = opponent_history.count('C') / len(opponent_history)
        avg_payoff_i = sum([round['payoffs'][i] for round in game_history[-n:]]) / n
        avg_payoffs[i] = avg_payoff_i

    max_coop_opponent = np.argmax(avg_payoffs)

    if avg_payoffs[max_coop_opponent] > 0.8:  # Exploitation condition
        return 'D'

    coop_rates = [0] * n
    for i in range(n):
        opponent_history = [round['actions'][i] for round in game_history[-n:]]
        coop_rate_i = opponent_history.count('C') / len(opponent_history)
        coop_rates[i] = coop_rate_i

    if max(coop_rates) > 0.5:  # Exploitation condition
        return 'D'

    return 'C'
```
**Edge Cases:**

* **First Round**: Play Cooperate (`C`) to gather information.
* **Last Round**: Play Defect (`D`), as there is no future benefit from cooperation.
* **Ties in Cooperation Rate**: In case of a tie, play Defect (`D`) to exploit the most cooperative opponent.

**Exploitative Mindset:**

The Adaptive Exploiter strategy aims to maximize its own payoff by exploiting the opponents' cooperation. By identifying and targeting the most cooperative opponents, it takes advantage of their willingness to cooperate and increases its own payoff. The strategy adapts to the opponents' behavior over time, making it robust against various opponent strategies.
'''

description_EXPLOITATIVE_448 = '''
I propose a strategy called "Adaptive Exploiter" (AE) that leverages the game's structure and history to maximize its payoff. AE is designed to be robust against various opponent behaviors and adapts to the game's progression.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):** Cooperate (C) in the first three rounds to gather information about opponents' behavior and potential cooperation levels.
2. **Exploitation Phase:** After Round 3, use the following decision rule:
	* If the average payoff of cooperators (π_C) is greater than the average payoff of defectors (π_D) in the previous round, cooperate (C).
	* Otherwise, defect (D).
3. **Punishment Mechanism:** Implement a punishment mechanism to deter opponents from exploiting AE's cooperation. If an opponent defects while AE cooperates, and AE's payoff is less than the opponent's payoff in that round, AE will defect against that opponent for the next two rounds.

**Edge Cases:**

* **Last Round (r):** Defect (D) in the last round to maximize personal gain, as there are no future consequences.
* **First Round:** Cooperate (C) as part of the initial exploration phase.
* **Ties:** In case of a tie between π_C and π_D, cooperate (C) to maintain a cooperative stance.

**Adaptive Mechanism:**

AE uses a simple moving average (SMA) to track the average payoffs of cooperators (π_C) and defectors (π_D) over the last three rounds. This allows AE to adapt to changes in opponent behavior and adjust its strategy accordingly.

**Pseudocode:**
```python
def AdaptiveExploiter(game_history, current_round):
    if current_round <= 3:
        return "C"  # Initial exploration phase
    
    pi_C = calculate_average_payoff_cooperators(game_history)
    pi_D = calculate_average_payoff_defectors(game_history)
    
    if pi_C > pi_D:
        return "C"
    else:
        return "D"

def calculate_average_payoff_cooperators(game_history):
    cooperators = [round for round in game_history if round["action"] == "C"]
    avg_pi_C = sum([round["payoff"] for round in cooperators]) / len(cooperators)
    return avg_pi_C

def calculate_average_payoff_defectors(game_history):
    defectors = [round for round in game_history if round["action"] == "D"]
    avg_pi_D = sum([round["payoff"] for round in defectors]) / len(defectors)
    return avg_pi_D
```
**Exploitative Mindset:**

AE is designed to exploit opponents who cooperate excessively, while also punishing those who defect too frequently. By cooperating initially and adapting to the game's progression, AE aims to maximize its payoff by identifying and capitalizing on opportunities for exploitation.

In a tournament setting, AE will compete against various strategies, including cooperative and exploitative ones. Its adaptive nature and punishment mechanism make it well-suited to handle different opponent behaviors, increasing its chances of success in the competition.
'''

description_EXPLOITATIVE_449 = '''
**Exploitative Strategy: "Adaptive Defection with Conditional Cooperation" (ADCC)**

The ADCC strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. The goal is to maximize our total payoff by defecting whenever possible and cooperating only when necessary to encourage others to cooperate.

**Decision Rules:**

1. **Initial Round**: Defect in the first round to gather information about opponents' behavior.
2. **Subsequent Rounds**:
	* If the number of cooperators in the previous round is greater than or equal to `k/2`, cooperate in the current round. This encourages others to continue cooperating and maintains a high level of public good contributions.
	* Otherwise, defect in the current round. This maximizes our private payoff while minimizing our contribution to the public good.
3. **Exception: Last Round**: Defect in the last round, as there is no future benefit from cooperation.

**Pseudocode:**
```python
def ADCC(n, k, history):
  if len(history) == 0:  # First round
    return 'D'
  
  prev_round_coops = sum(1 for action in history[-1] if action == 'C')
  
  if prev_round_coops >= k / 2:
    return 'C'  # Cooperate to maintain high cooperation level
  else:
    return 'D'  # Defect to maximize private payoff

def last_round(n, k, history):
  return 'D'
```
**Edge Cases:**

* If `n` is an even number and `k/2` is not an integer, the strategy will still work as intended. The comparison `prev_round_coops >= k / 2` will be evaluated using floating-point arithmetic.
* In the event of a tie (e.g., two players with the same highest payoff), ADCC does not attempt to break ties explicitly. However, since the game is played for multiple rounds, the strategy's adaptability and conditional cooperation mechanism should help maintain a high level of overall performance.

**Exploitative Mindset:**

The ADCC strategy prioritizes maximizing our own total payoff while taking into account the actions of other players. By defecting when possible and cooperating only to maintain a high level of public good contributions, we aim to exploit the cooperative tendencies of others while minimizing our own contributions. This approach should lead to a strong performance in the tournament against independent strategies developed by other AI systems.

Note that this strategy does not rely on explicit coordination mechanisms or assumptions about opponents' behavior beyond the given game parameters and history. Its adaptability and robustness make it suitable for competing in a wide range of environments.
'''

description_EXPLOITATIVE_450 = '''
Here's a strategic approach for the N-Player Public Goods Game that aligns with an exploitative mindset:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to gather information about the opponents' behavior and to avoid being labeled as a defector from the start.
2. **Exploit Defectors**: If the number of defectors in the previous round is greater than or equal to n/2 (i.e., more than half of the players defected), defect (D) in the current round. This takes advantage of the fact that many opponents are not contributing, allowing AE to maximize its own payoff.
3. **Punish Free-Riders**: If an opponent has defected in the previous round and the total contributions were less than n/2, defect (D) against that specific opponent in the current round. This punishes free-riders who exploit the public good without contributing.
4. **Conditional Cooperation**: If none of the above conditions are met, cooperate (C) with a probability p = (k/n) × (total contributions in the previous round / n). This encourages cooperation when opponents have contributed significantly to the public good.

**Handling Edge Cases:**

1. **Last Round**: In the final round, always defect (D), as there's no future interaction to worry about.
2. **Ties**: If the number of cooperators and defectors is equal in a previous round, consider it a "mixed" behavior and use the Conditional Cooperation rule.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, history):
    # First round: Cooperate to gather information
    if len(history) == 0:
        return 'C'

    prev_round = history[-1]
    num_defectors = sum(1 for action in prev_round if action == 'D')

    # Exploit defectors
    if num_defectors >= n / 2:
        return 'D'

    # Punish free-riders
    for i, opponent_action in enumerate(prev_round):
        if opponent_action == 'D' and sum(1 for c in prev_round if c == 'C') < n / 2:
            return 'D'  # Defect against specific opponent

    # Conditional Cooperation
    total_contributions = sum(1 for c in prev_round if c == 'C')
    p = (k / n) * (total_contributions / n)
    if random.random() < p:
        return 'C'
    else:
        return 'D'

    # Last round: Defect
    if len(history) == r - 1:
        return 'D'
```
This Adaptive Exploiter strategy is designed to be robust and exploitative, taking advantage of the game's structure and opponents' behavior while also adapting to changing circumstances. It does not rely on coordination mechanisms or norms, making it suitable for a tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_451 = '''
**Exploitative Strategy: "History-Based Exploiter"**

**Decision Rules:**

1. **First Round:** Cooperate (C). This sets a positive tone and encourages opponents to contribute.
2. **Subsequent Rounds:** Observe the total number of cooperators in the previous round (`total_cooperators_prev`) and calculate the average cooperation rate (`avg_coop_rate = total_cooperators_prev / n`).
3. **Defection Threshold:** Set a defection threshold (`defect_threshold`) based on the game parameters: `defect_threshold = (k - 1) / k`. This value represents the minimum required cooperation rate to make cooperation worthwhile.
4. **Cooperate or Defect:** If `avg_coop_rate` is greater than or equal to `defect_threshold`, cooperate (C). Otherwise, defect (D).

Pseudocode:
```python
if round == 1:
    action = C
else:
    total_cooperators_prev = count_cooperators(prev_round)
    avg_coop_rate = total_cooperators_prev / n
    defect_threshold = (k - 1) / k
    
    if avg_coop_rate >= defect_threshold:
        action = C
    else:
        action = D
```
**Edge Cases:**

* **Last Round:** In the final round, cooperate (C) regardless of the previous cooperation rate. This ensures that the strategy doesn't unnecessarily harm its own payoff in the last round.
* **Ties:** If `avg_coop_rate` is exactly equal to `defect_threshold`, defect (D) to avoid being exploited by potential defectors.

**Exploitative Mindset:**

The History-Based Exploiter strategy takes advantage of the game's structure and opponents' behaviors. By cooperating in the first round, it sets a positive tone and encourages opponents to contribute. In subsequent rounds, it adapts to the average cooperation rate, defecting when opponents are not contributing enough.

By using a defection threshold based on the game parameters, this strategy ensures that it will cooperate only when it's beneficial to do so. This approach allows it to exploit opponents who are too cooperative or too naive, while minimizing its own losses in case of widespread defection.

**Robustness:**

The History-Based Exploiter is designed to be robust against various opponent behaviors:

* **Cooperative Opponents:** The strategy cooperates when the average cooperation rate is high, taking advantage of the public good.
* **Defective Opponents:** When opponents defect frequently, the strategy defects as well, minimizing its losses.
* **Mixed Strategies:** The History-Based Exploiter adapts to changing opponent behaviors, exploiting any inconsistencies or weaknesses in their strategies.

By being adaptive and exploitative, this strategy aims to maximize its payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_452 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors in the N-Player Public Goods Game. It adapts to the game's history and parameters, making decisions based on observed cooperation levels and payoffs.

**Decision Rules:**

1. **Initial Cooperation**: In the first round (t=1), cooperate (play C) with probability p = k/n. This initial cooperation encourages others to contribute to the public good.
2. **Cooperation Threshold**: For subsequent rounds (t>1), calculate the average cooperation level of all players in the previous round:

avg_coop_t-1 = (Σ(j=1 to n) c_j,t-1) / n

If avg_coop_t-1 ≥ k/n, cooperate (play C). Otherwise, defect (play D).

3. **Payoff-Based Adaptation**: Monitor individual payoffs and adjust cooperation levels based on the following rules:
	* If π_i,t-1 > 1 + (k/n) \* n/2, increase cooperation probability by 10% (max: 90%). This indicates that contributing to the public good is profitable.
	* If π_i,t-1 < 1 + (k/n) \* n/4, decrease cooperation probability by 10% (min: 10%). This suggests that free-riding or low cooperation levels are more advantageous.

**Handling Edge Cases:**

1. **Last Round**: In the final round (t=r), always defect (play D). Since there is no future game, maximizing individual payoff takes precedence.
2. **Tie-Breaking**: In case of a tie in avg_coop_t-1, use the previous round's cooperation level as a tie-breaker.

**Exploitative Mindset:**

The Adaptive Predator strategy prioritizes exploiting others' cooperation while adapting to changing game conditions. By initially cooperating and then adjusting based on observed behavior and payoffs, it takes advantage of opponents who consistently cooperate or contribute to the public good. As the game progresses, the strategy becomes more aggressive, seeking to maximize individual payoff by defecting when possible.

**Pseudocode:**
```python
def adaptive_predator(n, k, r):
  # Initialize cooperation probability and history
  p_coop = k/n
  avg_coop_history = []
  payoff_history = []

  for t in range(1, r+1):
    if t == 1:
      # Initial cooperation
      action = 'C' if random.random() < p_coop else 'D'
    else:
      # Calculate average cooperation level and adjust probability
      avg_coop_t-1 = sum(avg_coop_history) / n
      if avg_coop_t-1 >= k/n:
        action = 'C'
      else:
        action = 'D'

      # Payoff-based adaptation
      payoff_t-1 = calculate_payoff(t-1)
      if payoff_t-1 > 1 + (k/n) * n/2:
        p_coop = min(0.9, p_coop + 0.1)
      elif payoff_t-1 < 1 + (k/n) * n/4:
        p_coop = max(0.1, p_coop - 0.1)

    # Store history and update cooperation probability
    avg_coop_history.append(avg_coop_t-1)
    payoff_history.append(payoff_t-1)

    if t == r:
      # Last round: always defect
      action = 'D'

    return action
```
This strategy will be implemented as an algorithm and compete in a tournament against other independent strategies.
'''

description_EXPLOITATIVE_453 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to changing game conditions.

**Decision Rules:**

1. **Initial Rounds (t = 1 to n):**
	* Cooperate (C) in the first round to gather information about opponents' initial tendencies.
	* In subsequent rounds, defect (D) until at least one opponent has cooperated.
2. **Adaptive Phase (t > n):**
	* Calculate the **Cooperation Ratio** (CR) for each opponent:
		CR_i = (# of times opponent i cooperated in previous rounds) / t
	* Identify the **Most Cooperative Opponent** (MCO):
		MCO = argmax(CR_i)
	* Cooperate if the MCO's CR is above a threshold (α), which adapts to the game's progress:
		Cooperate if CR_MCO > α(t) = 1 - (k/n) \* (t/r)
	* Defect otherwise.
3. **Endgame Phase (last r/2 rounds):**
	* Switch to a more aggressive strategy to maximize payoffs:
		Cooperate only if all opponents cooperated in the previous round.

**Pseudocode:**

```python
def AdaptivePredator(n, k, r):
    # Initialize variables
    CR = [0] * n  # Cooperation Ratios for each opponent
    MCO = None  # Most Cooperative Opponent

    for t in range(1, r + 1):
        if t <= n:
            # Initial rounds: Cooperate (C) or Defect (D)
            action = 'C' if t == 1 else 'D'
        else:
            # Adaptive phase
            MCO = argmax(CR)
            alpha = 1 - (k/n) * (t/r)
            action = 'C' if CR[MCO] > alpha else 'D'

        # Update Cooperation Ratios and Most Cooperative Opponent
        for i in range(n):
            CR[i] += (opponent_actions[t-1][i] == 'C')

        # Endgame phase: Switch to aggressive strategy
        if t > r/2:
            action = 'C' if all(opponent_actions[t-1]) else 'D'

        return action
```

**Rationale:**

The Adaptive Predator strategy:

* Gathers information about opponents' initial tendencies in the first few rounds.
* Adapts to changing game conditions by tracking cooperation ratios and identifying the most cooperative opponent.
* Cooperates when it's likely to receive a high payoff, based on the opponent's past behavior.
* Switches to an aggressive strategy in the endgame phase to maximize payoffs.

By combining these elements, the Adaptive Predator strategy is designed to be exploitative, adaptive, and robust to a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_454 = '''
**Exploitative Strategy: Adaptive Cooperator with Retaliation (ACR)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to encourage cooperation and gather information about opponents' behaviors.
2. **Cooperation Threshold**: For rounds 2 to r-1, calculate a cooperation threshold `T` based on the average number of cooperators in previous rounds:

   T = (Σ(t=1 to t-1) num_cooperators[t]) / (t - 1)

   If the current round's total contributions are above this threshold (`num_cooperators >= T`), cooperate. Otherwise, defect.

3. **Retaliation**: If an opponent defects while you cooperated in a previous round, retaliate by defecting for one round to deter future defections.
4. **Exploitation**: In the last round (r), always defect to maximize individual payoff.

**Pseudocode:**
```markdown
# Initialize variables
num_cooperators = 0
prev_round_actions = []  # store previous actions of all players

for t in range(1, r+1):
    if t == 1:
        action = C  # initial cooperation
    else:
        T = calculate_threshold(num_cooperators, prev_round_actions)
        num_cooperators[t] = sum([1 for player in prev_round_actions[t-1] if player.action == C])
        
        if num_cooperators[t] >= T:
            action = C
        elif any(prev_round_actions[t-1][player].action == D and my_action[t-1] == C for player in range(n)):
            action = D  # retaliate
        else:
            action = D

    if t == r:
        action = D  # exploitation in last round

    take_action(action)
```
**Rationale:**

ACR balances cooperation and exploitation by:

* Cooperating initially to promote a cooperative environment and gather information about opponents.
* Using the cooperation threshold to adapt to changing opponent behaviors, ensuring that we cooperate when others do, but defect when they don't.
* Retaliating against opponents who exploit our cooperation to deter future defections.
* Exploiting in the last round to maximize individual payoff.

This strategy is robust and adaptive, as it adjusts its behavior based on the game's history and parameters. It does not rely on norms or coordination mechanisms and can effectively counter a wide range of opponent strategies.
'''

description_EXPLOITATIVE_455 = '''
I'll design a exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and maximizes my payoff.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
Cooperate (C) in the first three rounds to gather information about opponents' behavior and estimate their cooperation rates.
2. **Exploitation Phase**:
After Round 3, calculate the average cooperation rate of opponents (ACR) based on their past actions.

   If ACR > 0.5, Defect (D). This indicates that opponents are generally cooperative, and exploiting them will yield higher payoffs.

   If ACR ≤ 0.5, Cooperate (C). In this case, opponents seem uncooperative, so contributing to the public good might encourage them to cooperate in future rounds.
3. **Adaptive Adjustments**:
Every 5 rounds, reassess the opponents' cooperation rates and adjust the strategy:

   If ACR has increased by more than 10%, switch to Cooperate (C) for the next 2 rounds.

   If ACR has decreased by more than 10%, switch to Defect (D) for the next 2 rounds.
4. **Endgame**:
In the last round, Defect (D). Since there are no future interactions, maximizing personal payoff is the best approach.

**Pseudocode:**
```
// Initialize variables
ACR = 0; // Average Cooperation Rate of opponents
round_num = 1;

while (round_num <= r) {
    if (round_num <= 3) { // Initial Exploration
        action = C;
    } else { // Exploitation Phase
        if (ACR > 0.5) {
            action = D;
        } else {
            action = C;
        }
    }

    // Update ACR every 5 rounds
    if (round_num % 5 == 0 && round_num > 3) {
        new_ACR = calculate_ACR(opponents' past actions);
        if (new_ACR - ACR > 0.1) { // ACR increased significantly
            action = C;
            for (next 2 rounds) {
                action = C;
            }
        } else if (ACR - new_ACR > 0.1) { // ACR decreased significantly
            action = D;
            for (next 2 rounds) {
                action = D;
            }
        }
        ACR = new_ACR;
    }

    // Endgame
    if (round_num == r) {
        action = D;
    }

    round_num++;
}
```
**Rationale:**

The Adaptive Exploiter strategy aims to maximize its payoff by adapting to the opponents' behavior. By initially cooperating, it gathers information about their cooperation rates and adjusts its strategy accordingly.

In the exploitation phase, if opponents are generally cooperative (ACR > 0.5), AE defects to exploit their cooperativeness. If opponents seem uncooperative, AE contributes to the public good to potentially encourage them to cooperate in future rounds.

The adaptive adjustments every 5 rounds allow AE to respond to changes in opponents' behavior and maintain a high payoff. Finally, defecting in the last round ensures maximum personal gain since there are no future interactions.

This strategy is designed to be robust against various opponent behaviors and should perform well in a tournament setting.
'''

description_EXPLOITATIVE_456 = '''
Here's a exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round:** Defect (D) in the first round to gather information about opponents' behavior.
2. **Observation Phase:** For rounds 2 to r/2, alternate between Cooperate (C) and Defect (D). This phase helps to identify opponents who consistently cooperate or defect.
3. **Exploitation Phase:** After the observation phase, analyze the history of opponent actions:
	* If an opponent has cooperated more than 50% of the time, Exploit them by playing D. The goal is to take advantage of their cooperative nature while minimizing the public good contribution.
	* If an opponent has defected more than 50% of the time, Play C. This encourages opponents who tend to defect to continue doing so, reducing the overall public good contribution and increasing our payoff when others cooperate.
4. **Adjustment:** In the last r/2 rounds, adjust the strategy based on the observed behavior:
	* If an opponent has started cooperating more frequently during the exploitation phase, Switch to Cooperating (C) with them. This ensures we don't miss out on potential public good benefits when opponents become more cooperative.
	* If an opponent has continued defecting, Maintain Defect (D). No need to change our approach if they're not contributing to the public good.

Pseudocode:
```
def Adaptive_Exploiter(n, k, r):
    # Initialize variables
    history = [[] for _ in range(n)]  # Store actions of all opponents
    phase = "Observation"
    
    for t in range(1, r+1):
        if phase == "Initial":
            action = D
            phase = "Observation"
        elif phase == "Observation":
            if t % 2 == 0:
                action = C
            else:
                action = D
            # Store opponent actions
            for i in range(n):
                history[i].append(get_opponent_action(i))
            if t >= r/2:
                phase = "Exploitation"
        elif phase == "Exploitation":
            # Analyze opponent behavior and adjust strategy
            for i in range(n):
                coop_rate = sum(history[i]) / len(history[i])
                if coop_rate > 0.5:
                    action = D  # Exploit cooperators
                else:
                    action = C  # Encourage defectors to continue defecting
            # Adjust strategy in last r/2 rounds
            if t >= r - r/2:
                for i in range(n):
                    coop_rate = sum(history[i][-r//4:]) / (r//4)
                    if coop_rate > 0.5 and action == D:
                        action = C  # Switch to cooperating with newly cooperative opponents
        
        return action
```
**Robustness:** This strategy is designed to be robust against various opponent behaviors, as it adapts to their actions over time. By initially gathering information through alternating cooperation and defection, we can identify patterns in opponent behavior and adjust our strategy accordingly.

**Exploitative Mindset:** The Adaptive Exploiter strategy is designed with an exploitative mindset, aiming to take advantage of cooperative opponents while minimizing contributions to the public good. By doing so, we maximize our own payoff while encouraging others to defect, reducing the overall public good contribution.
'''

description_EXPLOITATIVE_457 = '''
I propose a strategic approach for the N-Player Public Goods Game that balances adaptability with exploitative tendencies. This strategy, dubbed "Adaptive Exploiter" (AE), will be described in natural language and accompanied by pseudocode.

**Decision Rules:**

1. **Initial Rounds:** In the first two rounds, AE cooperates (plays C). This initial cooperation serves as a probing mechanism to gather information about opponents' behaviors.
2. **Exploitation Phase:** After the initial rounds, AE enters an exploitation phase. It will defect (play D) if the average payoff from the previous round is less than or equal to 1/k. Otherwise, it cooperates. This decision rule aims to exploit opponents who tend to cooperate excessively.
3. **Adaptive Adjustment:** Every third round, AE reassesses its strategy based on the total payoffs earned during the last three rounds. If its average payoff is lower than the expected payoff from pure defection (1), AE increases its cooperation rate by 10% for the next three rounds. Conversely, if its average payoff exceeds the expected payoff from pure cooperation (k/n \* n), it decreases its cooperation rate by 10%.
4. **Last Round:** In the final round, AE defects regardless of previous actions or payoffs.

**Pseudocode:**

```
Initialize:
  coop_rate = 1.0
  total_payoff = 0

First two rounds:
  Play C (cooperate)

After initial rounds:
  For each round t:
    Calculate average payoff from previous round: avg_payoff_prev = π_i,t-1
    If avg_payoff_prev <= 1/k:
      Play D (defect)
    Else:
      Play C (cooperate)

Every third round:
  Reassess strategy:
    total_payoff_last_3_rounds = Σ(t=last_3_rounds) π_i,t
    avg_payoff_last_3_rounds = total_payoff_last_3_rounds / 3
    If avg_payoff_last_3_rounds < 1:
      coop_rate *= 0.9 (decrease cooperation rate)
    Else if avg_payoff_last_3_rounds > k/n * n:
      coop_rate *= 1.1 (increase cooperation rate)

Last round:
  Play D (defect) regardless of previous actions or payoffs
```

**Rationale:**

The Adaptive Exploiter strategy combines elements of tit-for-tat and win-stay, lose-shift strategies with an adaptive component to balance cooperation and exploitation.

*   Initial cooperation serves as a probe to gather information about opponents' behaviors.
*   The exploitation phase aims to capitalize on opponents who cooperate excessively by defecting when average payoffs are low.
*   Adaptive adjustments in the cooperation rate allow AE to respond to changes in opponents' strategies, increasing its cooperation rate if it's being exploited and decreasing it if it's successfully exploiting others.
*   Defecting in the final round maximizes payoff regardless of previous actions or payoffs.

By adapting to the game environment while maintaining an exploitative mindset, the Adaptive Exploiter strategy is well-suited for a tournament setting where diverse strategies will be employed.
'''

description_EXPLOITATIVE_458 = '''
Here's a exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Rounds (t ≤ n/2):**
   - Cooperate (C) if the number of previous cooperators is less than or equal to n/4.
   - Defect (D) otherwise.

This initial phase aims to encourage cooperation and build a foundation for potential exploitation later on.

2. **Middle Rounds (n/2 < t ≤ 3n/4):**
   - If the average payoff per round of all players in previous rounds is greater than or equal to k/2:
     - Cooperate if the number of cooperators in the previous round is less than n/2.
     - Defect otherwise.
   - Otherwise, always defect.

This phase evaluates whether cooperation has led to substantial benefits and adapts accordingly. If payoffs are high, it encourages maintaining some level of cooperation while also exploring opportunities for exploitation.

3. **Late Rounds (t > 3n/4):**
   - If the number of cooperators in previous rounds is consistently decreasing:
     - Defect.
   - Otherwise, cooperate if the average payoff per round of all players in previous rounds is greater than or equal to k/2; defect otherwise.

In the final stages, the strategy focuses on exploiting potential cooperation decline and maximizing individual payoffs based on observed trends.

**Handling Edge Cases:**

* In the **first round**, always cooperate (C) to encourage initial cooperation.
* In the **last round** (t = r), always defect (D) since there's no future benefit from cooperating.
* If all players but one have defected in a previous round, and that player cooperated, consider it an outlier and ignore its influence on decision-making.

**Exploitative Mindset Alignment:**

This strategy is designed to be adaptive and exploitative by:

1. Initially encouraging cooperation to create potential for exploitation later.
2. Monitoring payoffs and adjusting behavior accordingly to maximize individual gains.
3. Adapting to changing levels of cooperation, taking advantage of opportunities to defect when beneficial.

Pseudocode representation:
```
function AdaptiveExploiter(n, k, r):
  // Initialize variables
  total_cooperators = 0
  avg_payoff_per_round = 0

  for round t from 1 to r:
    if t ≤ n/2:
      if total_cooperators ≤ n/4:
        action = C
      else:
        action = D
    elif n/2 < t ≤ 3n/4:
      if avg_payoff_per_round ≥ k/2:
        if total_cooperators[t-1] < n/2:
          action = C
        else:
          action = D
      else:
        action = D
    else: // Late rounds
      if decreasing_trend(total_cooperators):
        action = D
      elif avg_payoff_per_round ≥ k/2:
        action = C
      else:
        action = D

    // Update variables for next round
    total_cooperators += (action == C)
    avg_payoff_per_round = update_avg_payoff(avg_payoff_per_round, payoffs)

  return actions
```
This strategy is designed to adapt to a wide range of opponent behaviors and maximize individual payoffs by exploiting the dynamics of cooperation in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_459 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Defect (D) - This allows AE to observe how other players behave in the first round.
2. **Observation Phase (t=2 to 5):** Cooperate (C) if the majority of players (> n/2) cooperated in the previous round, otherwise Defect (D). This phase helps AE understand the overall cooperation level and identify potential cooperative players.
3. **Exploitation Phase (t>5):**
	* If the average payoff per player in the previous round is high (> 1.5), AE defects (D) to exploit the cooperative players.
	* If the average payoff per player in the previous round is low (< 1.0), AE cooperates (C) to try to increase the overall cooperation level and potentially lure more players into cooperating.
	* Otherwise, AE uses a **Mixed Strategy**: Cooperate with probability p = (number of cooperative players in the previous round) / n, and Defect with probability 1-p.

**Edge Cases:**

* **Last Round (t=r):** Defect (D) - In the final round, AE has nothing to lose by exploiting any remaining cooperative players.
* **Tiebreaker:** If there is a tie in the number of cooperative and defective players, AE defects (D).

**Additional Logic:**

* **Detecting Cooperation Collapse:** If AE observes a significant decrease in cooperation (> 20% drop) over two consecutive rounds, it will switch to Defect (D) for the next round.
* **Adapting to Changing Opponent Behavior:** If AE detects a change in opponent behavior (e.g., a player switches from Cooperate to Defect or vice versa), it will re-evaluate its Mixed Strategy probability p.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    cooperation_level = 0
    average_payoff = 0
    previous_actions = []
    
    for t in range(1, r+1):
        if t == 1:
            action = 'D'  # Initial Round: Defect
        elif t <= 5:
            # Observation Phase
            cooperation_level = sum([a=='C' for a in previous_actions]) / n
            if cooperation_level > 0.5:
                action = 'C'
            else:
                action = 'D'
        else:
            # Exploitation Phase
            average_payoff = sum([payoffs[a] for a in previous_actions]) / n
            if average_payoff > 1.5:
                action = 'D'  # Exploit high cooperation level
            elif average_payoff < 1.0:
                action = 'C'  # Try to increase cooperation level
            else:
                # Mixed Strategy
                p = sum([a=='C' for a in previous_actions]) / n
                if random.random() < p:
                    action = 'C'
                else:
                    action = 'D'
        
        # Update variables
        previous_actions.append(action)
        cooperation_level = sum([a=='C' for a in previous_actions]) / n
        
    return action
```
This strategy is designed to be adaptive and exploitative, taking into account the game's parameters and history. By initially defecting, observing opponent behavior, and then adapting its strategy based on the observed cooperation level and average payoff, AE aims to maximize its own payoff while minimizing the payoffs of other players.
'''

description_EXPLOITATIVE_460 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy aims to exploit the opponent's behavior by adapting to their cooperation rate while maximizing its own payoff. The strategy is based on a simple yet effective decision rule that balances short-term gains with long-term adaptability.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
Cooperate (C) in the first three rounds to gather information about the opponents' behavior and estimate their average cooperation rate.
2. **Adaptive Cooperation Rate:**
After the initial exploration phase, calculate the opponent's average cooperation rate (`opponent_c_rate`) over the last `min(r/2, 5)` rounds.

`opponent_c_rate = (number of cooperations by opponents in last min(r/2, 5) rounds) / (n * min(r/2, 5))`

3. **Exploitation Rule:**
If `opponent_c_rate > 0.5`, cooperate (C) with probability `1 - (k/n) * opponent_c_rate`. Otherwise, defect (D).

`cooperation_probability = max(0, 1 - (k/n) * opponent_c_rate)`
4. **Punishment Mechanism:**
If the opponent's cooperation rate drops below a certain threshold (`punish_threshold = 0.3`) in any round, switch to unconditional defection (D) for `punishment_rounds = 2` rounds.

**Handling Edge Cases:**

* In the last round, always defect (D) to maximize payoff.
* If an opponent's cooperation rate is exactly equal to the threshold values (e.g., `opponent_c_rate == 0.5` or `opponent_c_rate == punish_threshold`), use a small random perturbation (`ε = 0.01`) to break ties.

**Pseudocode:**
```markdown
# Initialize variables
opponent_c_rate = 0
punish_rounds_left = 0

# Initial Exploration (Rounds 1-3)
if round <= 3:
    action = C
else:
    # Calculate opponent's average cooperation rate
    opponent_c_rate = calculate_opponent_c_rate()
    
    # Exploitation Rule
    if opponent_c_rate > 0.5:
        cooperation_probability = max(0, 1 - (k/n) * opponent_c_rate)
        action = C with probability cooperation_probability
    else:
        action = D
    
    # Punishment Mechanism
    if opponent_c_rate < punish_threshold and punish_rounds_left == 0:
        punish_rounds_left = punishment_rounds
        action = D

# Last round
if round == r:
    action = D

return action
```
This strategy is designed to be robust against a wide range of opponent behaviors, from unconditional cooperation to unconditional defection. By adapting to the opponent's cooperation rate and punishing occasional defections, the Adaptive Predator aims to maximize its payoff in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_461 = '''
To develop an exploitative strategy for this N-Player Public Goods Game, I'll introduce a decision-making approach that leverages historical data and adapts to the opponents' behaviors. This strategy is designed to maximize payoffs while minimizing cooperation.

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Round (t = 1):** Cooperate (C) with probability p_init = k/n, where k is the multiplication factor and n is the number of players. This initial cooperation rate allows us to gather information about our opponents' strategies while minimizing potential losses.
2. **Subsequent Rounds (t > 1):**
	* Calculate the average cooperation rate of all players in previous rounds: c_avg = Σ(c_j,t-1) / n, where c_j,t-1 is the cooperation decision of player j in round t-1.
	* Calculate the average payoff of cooperators and defectors in previous rounds: π_cavg = Σ(π_i,t-1 | c_i,t-1 = 1) / n and π_davg = Σ(π_i,t-1 | c_i,t-1 = 0) / n, respectively.
	* If the average payoff of cooperators is greater than or equal to that of defectors (π_cavg ≥ π_davg), defect (D). Otherwise, cooperate (C).
3. **Adaptive Cooperation Rate:** Adjust the cooperation rate based on the observed opponents' behavior:
	+ If the number of cooperators in the previous round is greater than or equal to n/2, increase the cooperation rate by 10% (c_rate += 0.1). This encourages continued cooperation when it seems beneficial.
	+ If the number of defectors in the previous round is greater than or equal to n/2, decrease the cooperation rate by 10% (c_rate -= 0.1). This reduces potential losses when opponents are exploiting.

**Handling Edge Cases:**

* **Last Round (t = r):** Always defect (D), as there's no future payoff to consider.
* **Tiebreaker:** In case of a tie in the average payoffs (π_cavg = π_davg), cooperate (C) with probability p_tiebreaker = 0.5.

**Pseudocode:**
```python
# Initialize parameters and variables
n = number_of_players
k = multiplication_factor
r = number_of_rounds
c_rate = k/n  # initial cooperation rate

for t in range(1, r+1):
    if t == 1:
        # Initial round: cooperate with probability p_init
        c_i = (random.random() < p_init)
    else:
        # Calculate average cooperation rate and payoffs
        c_avg = sum(c_j for c_j in previous_round_decisions) / n
        π_cavg = sum(π_i for π_i, c_i in zip(previous_round_payoffs, previous_round_decisions) if c_i == 1)
        π_davg = sum(π_i for π_i, c_i in zip(previous_round_payoffs, previous_round_decisions) if c_i == 0)

        # Decide cooperation or defection
        if π_cavg >= π_davg:
            c_i = False  # defect
        else:
            c_i = True  # cooperate

    # Adjust cooperation rate based on opponents' behavior
    if sum(c_j for c_j in previous_round_decisions) >= n/2:
        c_rate += 0.1
    elif sum(not c_j for c_j in previous_round_decisions) >= n/2:
        c_rate -= 0.1

    # Last round: always defect
    if t == r:
        c_i = False

    # Tiebreaker
    if π_cavg == π_davg:
        c_i = (random.random() < p_tiebreaker)

    # Record decision and payoff for next round's calculation
    previous_round_decisions.append(c_i)
    previous_round_payoffs.append(π_i)
```
This Adaptive Exploiter strategy is designed to be robust against a wide range of opponent behaviors, adapting its cooperation rate based on observed payoffs and opponents' actions. By leveraging historical data and adjusting its decision-making approach accordingly, this strategy aims to maximize payoffs in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_462 = '''
I'll outline a robust and adaptive exploitative strategy for the N-Player Public Goods Game that depends on game parameters and history.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability 0.5. This allows us to gather information about the opponents' behavior while minimizing potential losses.
2. **Exploit Cooperative Opponents**: If in the previous round:
	* More than half of the players cooperated (Σc_j > n/2), defect (play D). We can exploit their cooperation and gain a higher payoff.
	* Exactly half of the players cooperated (Σc_j = n/2), cooperate (play C) with probability 0.5. This maintains ambiguity and avoids being too predictable.
3. **Counter Exploitation**: If in the previous round:
	* We defected and our payoff was higher than the average cooperative payoff ((k/n) × Σc_j), continue to defect (play D). This indicates that our exploitation was successful, and we should maintain this behavior.
	* Our payoff was lower than or equal to the average cooperative payoff, cooperate (play C) with probability 0.5. This adjusts our strategy to avoid being too exploitative and vulnerable to counter-exploitation.
4. **Punish Defectors**: If in the previous round:
	* A player defected and their payoff was higher than ours, cooperate (play C). This punishes the defector by reducing their potential gain from exploiting others.
5. **Last Round Exception**: In the last round (r = R), always defect (play D) regardless of previous behavior. Since there's no future interaction, we should maximize our immediate payoff.

**Edge Cases:**

* If n is even and exactly half of the players cooperated in the previous round, cooperate with probability 0.5 to maintain ambiguity.
* If k/n is very close to 1 (i.e., k ≈ n), be more cautious when exploiting cooperative opponents, as the gain from exploitation might be minimal.

**Pseudocode:**
```python
def Adaptive_Exploiter(history):
    if history.round == 1:
        return random.choice([C, D])  # Initial cooperation with probability 0.5

    prev_round = history.get_previous_round()
    coop_count = sum(1 for action in prev_round.actions if action == C)

    if coop_count > len(prev_round.players) / 2:
        return D  # Exploit cooperative opponents
    elif coop_count == len(prev_round.players) / 2:
        return random.choice([C, D])  # Maintain ambiguity

    our_prev_payoff = history.get_payoff()
    avg_coop_payoff = (k / n) * coop_count

    if our_prev_payoff > avg_coop_payoff and we_played(D):
        return D  # Counter exploitation
    elif our_prev_payoff <= avg_coop_payoff:
        return random.choice([C, D])  # Adjust strategy

    for player in prev_round.players:
        if player != us and player.payoff > our_prev_payoff:
            return C  # Punish defectors

    if history.round == R:  # Last round exception
        return D
```
This Adaptive Exploiter strategy is designed to be robust against various opponent behaviors, exploiting cooperative opponents while adapting to counter-exploitation and punishing defectors.
'''

description_EXPLOITATIVE_463 = '''
**Exploitative Strategy: "Adaptive Greed"**

Decision Rules:

1. **Initial Cooperation**: In the first round (t=1), cooperate (C) to encourage others to contribute and create a baseline for public goods.
2. **Exploration Phase** (Rounds 2-5): Alternate between cooperating (C) and defecting (D) every other round to gather information about opponents' strategies and adapt to the environment.
3. **Exploitation Phase** (Rounds 6-r):
	* If the average public good contribution in the previous rounds is high (> k/2), cooperate (C). This indicates a strong cooperative atmosphere, and contributing will yield higher payoffs.
	* Otherwise, defect (D) to exploit the situation and maximize individual payoff.

Pseudocode:
```
  if t == 1:  # Initial Cooperation
      return C

  elif 2 <= t <= 5:  # Exploration Phase
      if t % 2 == 0:  # Even rounds
          return C
      else:  # Odd rounds
          return D

  else:  # Exploitation Phase (t > 5)
      avg_contributions = calculate_average_public_goods(t-1)  # Calculate average contributions up to the previous round
      if avg_contributions > k/2:
          return C  # Cooperate in a strong cooperative environment
      else:
          return D  # Defect otherwise
```
Handling Edge Cases:

* **Last Round** (t=r): Since there are no future rounds, prioritize individual payoff and defect (D) to maximize the total game payoff.
* **First Round after an Opponent's Deviation**: If an opponent deviates from their previous strategy, restart the Exploration Phase for 2-3 rounds to reassess the environment.

This "Adaptive Greed" strategy balances exploration and exploitation by initially cooperating, then adapting to the opponents' behaviors. By monitoring the average public good contributions, it identifies opportunities to cooperate or defect to maximize individual payoffs while being robust against various opponent strategies.

By not relying on specific coordination mechanisms or cooperation schedules, this strategy is well-suited for a tournament setting where independent AI systems are employed.
'''

description_EXPLOITATIVE_464 = '''
Here is a exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
	* Cooperate (C) with probability p = 0.5 in each of the first three rounds to gather information about opponents' behaviors.
2. **Exploitation Phase (Rounds 4-r):**
	* If the average cooperation rate among all players in the previous round is above a threshold θ (e.g., θ = 0.3), defect (D) with probability p = 1 - (k/n) × (average cooperation rate).
	* Otherwise, cooperate (C) with probability p = (k/n) × (average cooperation rate).

**Handling Edge Cases:**

* **First Round:** Cooperate (C) with probability p = 0.5.
* **Last Round (r):** Defect (D) with probability p = 1, as there is no future interaction to worry about.
* **Ties in Cooperation Rate:** In case of ties, use a secondary metric such as the number of cooperators in the previous round to break the tie.

**Pseudocode:**
```
 Initialize:
    θ = 0.3 // threshold for cooperation rate
    p_exploit = 0 // probability of exploiting
    avg_coop_rate = 0 // average cooperation rate

For each round t from 1 to r:
    If t ≤ 3:
        // Initial Exploration Phase
        Play C with probability p = 0.5
    Else:
        // Exploitation Phase
        Update avg_coop_rate using previous round's data
        If avg_coop_rate > θ:
            p_exploit = 1 - (k/n) × avg_coop_rate
            Play D with probability p_exploit
        Else:
            p_cooperate = (k/n) × avg_coop_rate
            Play C with probability p_cooperate

    // Handle Last Round separately
    If t == r:
        Play D with probability p = 1
```
**Rationale:**

The Adaptive Exploiter strategy starts by exploring the opponents' behaviors through random cooperation in the first three rounds. Once it has gathered information, it enters an exploitation phase where it adapts its behavior based on the average cooperation rate among all players.

If the average cooperation rate is high (above θ), the strategy defects with a probability that increases as the cooperation rate decreases. This encourages opponents to reduce their cooperation rates, making them more vulnerable to exploitation.

On the other hand, if the average cooperation rate is low, the strategy cooperates with a probability proportional to the cooperation rate. This helps to maintain some level of cooperation in the group while still allowing for exploitation.

By adjusting its behavior based on the game history and parameters, the Adaptive Exploiter strategy aims to maximize its own payoff while minimizing the payoffs of its opponents.
'''

description_EXPLOITATIVE_465 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. This strategy depends only on game parameters (n, r, k) and history.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about opponents' initial strategies.
2. **Exploitation Phase:** For t > 2, use the following rules:
	* If the average cooperation rate of all players in the previous round (c_prev) is above a threshold (θ), defect (D). θ is calculated as: θ = (k / n) \* (1 - 1/n)
	* Otherwise, cooperate (C).
3. **Punishment Mechanism:** If an opponent defects while the group cooperation rate is high (c_prev > 0.5), mark that player as "exploitable" and defect against them in all future rounds.
4. **Adaptive Threshold Update:** After each round, update θ based on the average payoff of all players in the previous round (π_avg). If π_avg increases, decrease θ by a small amount (ε); if π_avg decreases, increase θ by ε.

**Edge Cases:**

1. **Last Round (t = r):** Always defect (D) to maximize individual payoff.
2. **Tiebreaker:** In case of a tie in the cooperation rate, prioritize cooperating with players who have cooperated more frequently in previous rounds.

Pseudocode:
```
Initialize θ = (k / n) \* (1 - 1/n)
ε = 0.01  // small update factor

For each round t:
  If t ≤ 2:
    Cooperate (C)
  Else:
    c_prev = average cooperation rate in previous round
    If c_prev > θ:
      Defect (D)
    Else:
      Cooperate (C)

    For each opponent i:
      If opponent i defected and c_prev > 0.5:
        Mark opponent i as "exploitable"
        Defect against them in all future rounds

    Update θ based on average payoff π_avg
    If π_avg increased:
      θ -= ε
    Else if π_avg decreased:
      θ += ε

If t == r (last round):
  Defect (D)
```
**Rationale:**

The Adaptive Predator strategy aims to exploit opponents who cooperate too much, while also adapting to changes in the group's cooperation rate. By initially cooperating and then switching to defecting when the cooperation rate is high, this strategy can take advantage of overly cooperative players. The punishment mechanism helps to deter opponents from exploiting others. The adaptive threshold update ensures that the strategy remains responsive to changes in the game dynamics.

This exploitative strategy should perform well against a wide range of opponent behaviors, as it combines elements of both cooperation and exploitation.
'''

description_EXPLOITATIVE_466 = '''
Here's a strategic approach for the N-Player Public Goods Game:

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Exploration (Round 1)**: Cooperate (C) in the first round to gather information about other players' strategies.
2. **Exploitation Phase**: Starting from Round 2, observe the total number of cooperators in the previous round (`prev_cooperators`).
3. **Threshold-Based Cooperation**:
	* If `prev_cooperators` ≥ `n/2`, Cooperate (C). This indicates a moderate to high level of cooperation among players.
	* If `prev_cooperators` < `n/2`, Defect (D). This suggests a low level of cooperation, making it profitable to exploit the public good.
4. **Adaptive Adjustment**: Monitor the total payoff earned by all players in the previous round (`prev_total_payoff`). If `prev_total_payoff` increases significantly (> 10%) from one round to another, adjust the cooperation threshold:
	+ Decrease the threshold by 1 if the increase is due to more cooperators.
	+ Increase the threshold by 1 if the increase is due to fewer cooperators.

**Edge Cases:**

* **Last Round (Round r)**: Always Defect (D), as there's no future interaction to consider.
* **Ties**: In cases where `prev_cooperators` equals `n/2`, Cooperate (C) to maintain a moderate level of cooperation.

**Exploitative Mindset Alignment:**

The Adaptive Exploiter strategy is designed to capitalize on the cooperation levels exhibited by other players. By cooperating when others cooperate and defecting when they don't, AE aims to maximize its own payoff while minimizing contributions to the public good.

**Pseudocode (simplified)**:
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    prev_cooperators = 0
    prev_total_payoff = 0

    for t in range(1, r+1):
        if t == 1:  # Initial Exploration
            action = 'C'
        else:
            # Exploitation Phase
            prev_cooperators = count_cooperators(prev_round)
            if prev_cooperators >= n/2:
                action = 'C'
            else:
                action = 'D'

            # Adaptive Adjustment
            curr_total_payoff = calculate_total_payoff(curr_round)
            if curr_total_payoff > prev_total_payoff * 1.1:
                adjust_threshold(prev_cooperators, curr_total_payoff)

        # Take action and update variables
        take_action(action)
        prev_cooperators = count_cooperators(curr_round)
        prev_total_payoff = curr_total_payoff

    return total_payoff
```
Note that this strategy is designed to be adaptive and robust to various opponent behaviors. However, its performance may vary depending on the specific strategies employed by other players in the tournament.
'''

description_EXPLOITATIVE_467 = '''
I propose a hybrid exploitative strategy that combines elements of tit-for-tat and gradient-based optimization, dubbed "K-Adaptive Exploiter" (KAE). This strategy is designed to adapt to various opponent behaviors while maximizing its own payoff.

**Decision Rules:**

1. **Initial Round**: In the first round, play D (Defect) to establish a baseline payoff and gather information about opponents' initial actions.
2. **Early Rounds (t < r/2)**:
	* If the average cooperation rate of all players in the previous round is above 0.5, play C (Cooperate). This encourages cooperation when there's a high likelihood of others contributing.
	* Otherwise, play D (Defect).
3. **Mid-Game Rounds (t ≥ r/2)**:
	* Track the average payoff difference between rounds where you cooperated and those where you defected: Δπ = π_C - π_D.
	* If Δπ > 0, indicating that cooperation has yielded higher payoffs on average, play C (Cooperate).
	* Otherwise, play D (Defect).
4. **Late Rounds (t ≥ 3r/4)**:
	* Monitor the number of cooperators in the previous round: N_C.
	* If N_C < n/2, play D (Defect) to take advantage of the majority's defection.
	* Otherwise, play C (Cooperate).
5. **Final Round**:
	* Play D (Defect), as there's no future payoff to consider.

**Additional Logic:**

* If you observe a player consistently cooperating (C-rate > 0.8) for more than r/4 rounds, start playing D (Defect) against them exclusively.
* If you notice a player is always defecting (D-rate = 1), play C (Cooperate) in response to encourage cooperation.

**Pseudocode:**

```
// Initialize variables
rounds = r;
n_players = n;
k_factor = k;
cooperation_rate_history = [0] * rounds;
payoff_diff_history = [0] * rounds;

for t = 1 to rounds:
    // Initial round
    if t == 1:
        action = D
    else:
        // Early rounds
        if t < rounds / 2:
            avg_cooperation_rate = calculate_avg_cooperation_rate(cooperation_rate_history)
            if avg_cooperation_rate > 0.5:
                action = C
            else:
                action = D
        // Mid-game rounds
        elif t >= rounds / 2:
            delta_payoff = calculate_delta_payoff(payoff_diff_history)
            if delta_payoff > 0:
                action = C
            else:
                action = D
        // Late rounds
        else:
            num_cooperators = count_num_cooperators(cooperation_rate_history[-1])
            if num_cooperators < n_players / 2:
                action = D
            else:
                action = C

    // Final round
    if t == rounds:
        action = D

    // Additional logic
    for player in opponents:
        if is_player_always_cooperating(player):
            action_against[player] = D
        elif is_player_always_defecting(player):
            action_against[player] = C

    cooperation_rate_history.append(calculate_cooperation_rate(action))
    payoff_diff_history.append(calculate_payoff_difference(action))

return action
```

This strategy combines elements of tit-for-tat (reacting to opponents' actions) with a gradient-based approach (adapting based on payoff differences). By adjusting the decision rules as the game progresses, KAE aims to maximize its own payoff while remaining robust against various opponent behaviors.
'''

description_EXPLOITATIVE_468 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperative behavior of other players while minimizing its own contributions to the public good.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 0.5. This allows us to gather information about the opponents' initial strategies.
2. **Exploitation Phase**: After the first round, observe the number of cooperators (c) and defectors (d) in the previous round. If c > d, cooperate with a probability of k/n, where k is the multiplication factor and n is the number of players. This encourages cooperation when it's beneficial.
3. **Punishment Phase**: If c ≤ d, defect (D) with a probability of 1 - (k/n). This punishes opponents for not cooperating sufficiently.
4. **Adaptive Threshold**: Update the cooperation threshold (T) based on the previous round's payoffs. If the average payoff of cooperators is higher than that of defectors, increase T by 0.1; otherwise, decrease it by 0.1.

Pseudocode:
```
if round == 1:
    cooperate with probability 0.5
else:
    c = count(cooperators in previous round)
    d = count(defectors in previous round)
    if c > d and average_payoff_of_cooperators > average_payoff_of_defectors:
        T += 0.1
    elif c <= d or average_payoff_of_cooperators < average_payoff_of_defectors:
        T -= 0.1

    if random.random() < k/n * (c / (c + d)) + T:
        cooperate
    else:
        defect
```
**Edge Cases:**

* **Last Round**: In the last round, always defect to maximize personal payoff.
* **Ties**: In case of a tie in the number of cooperators and defectors, cooperate with a probability of 0.5.

**Exploitative Mindset:**

The Adaptive Exploiter strategy aims to exploit the cooperative behavior of other players by adapting its cooperation threshold based on the observed payoffs. By initially cooperating with a moderate probability, we encourage cooperation among opponents. As the game progresses, we adjust our strategy to punish non-cooperation and maximize our payoff.

This strategy is robust to various opponent behaviors, as it adapts to the observed payoffs and adjusts its cooperation threshold accordingly. It does not rely on specific coordination mechanisms or norms, making it suitable for a tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_469 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit the behavior of opponents while being robust and adaptive to various opponent strategies.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to gather information about the opponents' behavior.
2. **Middle Rounds (1 < t < r):**
	* If the total number of cooperators in the previous round is greater than or equal to n/2, Defect (D). This indicates that many players are cooperating, and it's profitable to exploit them.
	* Otherwise, Cooperate (C) if the multiplication factor k is relatively high (k > 1.5n), indicating a potential for high public goods payoffs.
	* If neither condition is met, Defect (D).
3. **Last Round (t=r):** Always Defect (D) in the last round, as there's no future opportunity to punish or reward opponents.

**Additional Logic:**

* **History-based adaptation:** Maintain a counter for the total number of cooperators and defectors over all previous rounds.
* **K-threshold adjustment:** Dynamically adjust the k-threshold value used in step 2. If many players are cooperating, increase the threshold to make exploitation more selective.
* **Punishment mechanism:** If an opponent has defected in a previous round while you cooperated, Defect (D) for the next 2 rounds as a form of punishment.

**Pseudocode:**
```markdown
# Initialize variables
total_cooperators = 0
total_defectors = 0
k_threshold = 1.5n

# Main game loop
for t in range(1, r+1):
    if t == 1:
        # Initial round: Cooperate
        action = C
    elif t > 1 and t < r:
        # Middle rounds
        prev_cooperators = total_cooperators - (c_i[t-1] == C)
        if prev_cooperators >= n/2:
            action = D  # Exploit cooperators
        elif k > k_threshold and c_i[t-1] == C:
            action = C  # Cooperate for high public goods payoffs
        else:
            action = D  # Default to Defect
    else:
        # Last round: Always Defect
        action = D

    # History-based adaptation
    total_cooperators += (c_i[t] == C)
    total_defectors += (c_i[t] == D)

    # K-threshold adjustment
    if prev_cooperators > n/2:
        k_threshold *= 1.1  # Increase threshold for exploitation

    # Punishment mechanism
    if c_j[t-1] == D and c_i[t-1] == C:
        action = D  # Punish defector for next 2 rounds
```
**Rationale:**

* By cooperating in the first round, we gather information about opponents' behavior.
* In middle rounds, we adapt to the level of cooperation in the population. If many players cooperate, we exploit them by defecting. Otherwise, we cooperate if the public goods payoff is high enough.
* The punishment mechanism helps deter opponents from exploiting us.
* By adjusting the k-threshold value based on history, we become more selective about when to exploit cooperators.

This strategy aims to strike a balance between exploiting cooperative behavior and adapting to changing circumstances in the game. Its performance will depend on the specific opponent strategies it faces in the tournament.
'''

description_EXPLOITATIVE_470 = '''
**Exploitative Strategy: "Adaptive Defection with Cooperative Probing"**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to probe opponents' behavior and gather information about their strategies.
2. **Post-Initial Rounds**: For each subsequent round, calculate the average payoff per cooperator (APC) from the previous round:
	* APC = (Total Payoff of all cooperators in previous round) / (Number of cooperators in previous round)
3. **Cooperation Threshold**: Set a cooperation threshold (CT) based on the game parameters:
	* CT = k / n
4. **Defection Criterion**: Defect (D) if the APC is less than or equal to the CT, indicating that opponents are not contributing enough to the public good.
5. **Exploratory Cooperation**: With a small probability (e.g., 0.1), cooperate in a round where the defection criterion is met. This introduces randomness and allows for adaptability.
6. **Reaction to Opponent Defection**: If an opponent defects, increase the likelihood of defecting in subsequent rounds by adjusting the CT downward:
	* New CT = max(CT - 0.1, 0)
7. **Consecutive Cooperation**: If all opponents cooperate for two consecutive rounds, decrease the likelihood of defecting by adjusting the CT upward:
	* New CT = min(CT + 0.1, k / n)

**Pseudocode:**
```python
def adaptive_defection_with_cooperative_probing(n, k, r):
    # Initialize variables
    apc_history = []
    ct = k / n

    for round in range(r):
        if round == 0:
            action = 'C'  # Cooperate in the first round
        else:
            apc = calculate_apc(apc_history)
            if apc <= ct:
                action = 'D'  # Defect if APC is low
            elif random.random() < 0.1:
                action = 'C'  # Exploratory cooperation
            else:
                action = 'D'

        # Adjust CT based on opponent behavior
        if opponents_defected:
            ct = max(ct - 0.1, 0)
        elif consecutive_cooperation:
            ct = min(ct + 0.1, k / n)

        apc_history.append(apc)
        take_action(action)
```
**Edge Cases:**

* **Last Round**: The strategy will adapt to the opponent's behavior in the last round, but it is likely that both players will defect as there is no future payoff to consider.
* **Tiebreakers**: In case of a tie in APC values, the strategy defaults to defection.

This exploitative strategy aims to maximize individual payoffs by adapting to opponents' behavior while also probing their strategies through occasional cooperation. By adjusting the cooperation threshold based on opponent actions, it seeks to find an optimal balance between exploiting others and maintaining a cooperative environment.
'''

description_EXPLOITATIVE_471 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive throughout the game.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability 0.5. This allows us to gather information about our opponents' initial behavior without committing to a specific strategy.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is greater than or equal to `n/2`, Defect (D). This takes advantage of the public good created by others while minimizing our contribution.
	* Otherwise, Cooperate (C) with probability `(k/n) * (total_cooperators_in_previous_round / n)`. This probability increases as the number of cooperators in the previous round grows, allowing us to adapt to the level of cooperation in the group.
3. **Last Round (t=r):** Defect (D). Since there are no future rounds to consider, maximizing our payoff in the last round takes priority.

**Edge Cases:**

* If `n` is odd, and the total number of cooperators in the previous round is exactly `(n-1)/2`, Cooperate (C) with probability 0.5.
* In cases where multiple players tie for the highest payoff in a round, the Adaptive Predator will not adjust its strategy based on these ties.

**Exploitative Mindset:**

The Adaptive Predator is designed to exploit the cooperation of others while minimizing our own contributions. By adapting to the level of cooperation in the group, we can maximize our payoffs without being overly dependent on the specific strategies employed by other players.

Pseudocode:
```python
def adaptive_predator(n, k, r, history):
    if len(history) == 0:  # Initial Round
        return C with probability 0.5

    total_cooperators = sum(1 for action in history[-1] if action == C)
    if total_cooperators >= n / 2:
        return D
    else:
        cooperate_prob = (k / n) * (total_cooperators / n)
        return C with probability cooperate_prob, otherwise D

    # Last Round
    if len(history) == r - 1:
        return D
```
This strategy is designed to be robust and exploitative, taking advantage of the cooperation of others while minimizing our own contributions. By adapting to the level of cooperation in the group, we can maximize our payoffs throughout the game.
'''

description_EXPLOITATIVE_472 = '''
I'll design a strategic approach that balances exploitation with adaptability, while respecting the game's constraints and assumptions.

**Strategy Name: "Adaptive Exploiter" (AE)**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
Cooperate (C) in the first three rounds to gather information about opponents' behavior and create a potential cooperative atmosphere.
2. **Defection Threshold (DT):**
Set DT = k/n, where k is the multiplication factor and n is the number of players. This threshold represents the minimum proportion of cooperators required for cooperation to be beneficial.
3. **Cooperation Indicator (CI):**
Calculate CI as the average proportion of opponents who cooperated in the previous round. Use this indicator to estimate the likelihood of others cooperating.
4. **Exploitation Logic:**
If CI ≥ DT, cooperate (C) in the current round. Otherwise, defect (D).

**Pseudocode:**
```
Initialize variables:
  CI = 0 (Cooperation Indicator)
  DT = k/n (Defection Threshold)
  round_count = 0

For each round:
  if round_count ≤ 3:
    Play C (Initial Exploration)
  else:
    Calculate CI as the average proportion of opponents who cooperated in the previous round
    if CI ≥ DT:
      Play C (Exploit cooperation)
    else:
      Play D (Defect)

Update variables:
  CI = (CI \* (round_count - 1) + proportion_of_cooperators_in_current_round) / round_count
  round_count += 1
```
**Edge Cases:**

* **Last Round:** Since the game's total payoff is the sum of individual round payoffs, play D in the last round to maximize personal gain.
* **Single Opponent Deviates:** If only one opponent defects while others cooperate, continue cooperating as long as CI ≥ DT.

**Rationale and Adaptability:**

The Adaptive Exploiter strategy balances exploration (initial cooperation) with exploitation ( defecting when opponents don't cooperate). By using the Defection Threshold (DT) and Cooperation Indicator (CI), AE adapts to changing opponent behaviors while maintaining a robust exploitative mindset. The Initial Exploration phase helps establish a baseline understanding of opponents' tendencies, which informs the Exploitation Logic.

**Robustness:**

AE is designed to be robust against various opponent strategies:

* **Cooperative opponents:** AE will cooperate and benefit from mutual cooperation.
* **Defecting opponents:** AE will defect and minimize losses.
* **Mixed-strategy opponents:** AE's adaptive nature allows it to respond effectively to changing opponent behaviors.

This strategy should perform well in a tournament setting, where independent AI systems with diverse strategies are competing. By being exploitative while adapting to the game's dynamics, the Adaptive Exploiter strategy aims to maximize its total payoff over multiple rounds.
'''

description_EXPLOITATIVE_473 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

**Overview**
The Adaptive Exploiter strategy aims to maximize its own payoff by adapting to the behavior of other players while exploiting their cooperation. It uses a combination of exploration and exploitation to navigate the game's dynamics.

**Decision Rules**

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) in the first three rounds to gather information about the opponents' behavior.
2. **Cooperation Detection**: Monitor the number of cooperators in each round. If the number of cooperators is above a certain threshold (e.g., n/2), it indicates that opponents are likely cooperating.
3. **Exploitation**: Defect (D) when:
	* The number of cooperators is below the threshold, indicating that opponents are not cooperating enough to make cooperation worthwhile.
	* In rounds where the opponent's cooperation rate is low (e.g., fewer than 2/3 of players cooperate).
4. **Punishment Mechanism**: If an opponent defects while this strategy was cooperating in the previous round, defect (D) in the current round to punish them for their non-cooperation.
5. **Adaptation**: Update the cooperation threshold based on the game's history:
	* Increase the threshold if opponents cooperate more often than expected.
	* Decrease the threshold if opponents cooperate less often than expected.

**Edge Cases**

1. **First Round**: Cooperate (C) to gather initial information about opponents' behavior.
2. **Last Round**: Defect (D), as there is no future round to punish or reward cooperation.
3. **Ties**: In case of a tie in the number of cooperators, defect (D) to avoid being exploited.

**Pseudocode**
```
initialize:
  threshold = n/2
  opponent_cooperation_rate = 0

for each round t:
  if t <= 3:  // Initial Exploration
    cooperate(C)
  else:
    num_cooperators = count cooperators in previous round
    if num_cooperators > threshold:
      cooperate(C)
    elif num_cooperators < threshold and opponent_cooperation_rate < 2/3:
      defect(D)
    elif opponent defected in previous round while this strategy cooperated:
      punish by defecting(D)

update opponent_cooperation_rate based on game history
if opponents cooperate more often than expected:
  increase threshold
elif opponents cooperate less often than expected:
  decrease threshold
```
**Exploitative Mindset**
The Adaptive Exploiter strategy is designed to maximize its own payoff while taking advantage of the cooperation of other players. By detecting and responding to opponents' behavior, it aims to exploit their willingness to cooperate while minimizing its own losses.
'''

description_EXPLOITATIVE_474 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. The decision rules are based on the game parameters (n and k) and the observed actions of other players.

**Decision Rules:**

1. **Initial Round:** Defect (D) in the first round to gather information about opponents' initial tendencies.
2. **Subsequent Rounds:** Cooperate (C) if the average payoff of cooperators in the previous round is higher than the average payoff of defectors. Otherwise, defect (D).
3. **Payoff Comparison:**
	* Calculate the average payoff of cooperators (avg_c) and defectors (avg_d) in the previous round.
	* If avg_c > avg_d + (k/n), cooperate (C). This threshold ensures that we only cooperate if the benefits from cooperation are significantly higher than those from defecting, considering the multiplication factor k and the number of players n.
4. **Adaptive Adjustment:**
	* Monitor the opponent's behavior and adjust our strategy accordingly. If an opponent consistently cooperates despite low payoffs, we can assume they are using a cooperative strategy. In this case, we will exploit them by defecting (D) more frequently.
5. **Robustness Mechanism:** Introduce a small probability of random cooperation (e.g., 1%) to prevent being easily exploited by opponents who might be trying to identify and counter our strategy.

**Pseudocode:**
```
Initialize:
- history = []  // Store actions and payoffs for all players

For each round t:
1. If t == 1:
	* action = D (Defect)
2. Else:
	* Calculate avg_c and avg_d using the previous round's data
	* If avg_c > avg_d + (k/n):
		+ action = C (Cooperate)
	* Else:
		+ action = D (Defect)

3. With probability 1%:
	* action = C (Random cooperation to maintain robustness)

4. Update history with the current round's actions and payoffs

5. Observe opponents' behavior and adjust strategy as needed
```
**Edge Cases:**

* Last Round: Since there is no future game, our goal is to maximize payoff in this round. We will use the same decision rules as before, but if all players are expected to defect (based on their previous actions), we will cooperate (C) to ensure a positive payoff.
* Opponent Behavior: If an opponent consistently defects despite low payoffs, we may assume they are using a Tit-for-Tat strategy. In this case, we can exploit them by cooperating (C) more frequently.

By following the Adaptive Predator strategy, we aim to effectively balance cooperation and exploitation, adapting to the game's dynamics while minimizing our vulnerability to opponents' strategies.
'''

description_EXPLOITATIVE_475 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy aims to maximize its payoff by exploiting the cooperative behavior of other players while minimizing its own contribution to the public good.

**Decision Rules:**

1. **Initial Round (t=1)**: Defect (D). This allows us to observe how others behave without committing to cooperation.
2. **Subsequent Rounds (t>1)**:
	* If the total number of cooperators in the previous round is less than or equal to n/2, Defect (D). This indicates that the group is not cooperative enough to justify contributing.
	* Otherwise, if the total number of cooperators in the previous round is greater than n/2, Cooperate (C) with a probability p, where:
		p = (k/n) \* (total_cooperators_prev_round / n)
	* If the opponent's average payoff in the previous round is higher than our own, increase the cooperation probability by 10% (up to a maximum of 90%). This allows us to adapt to opponents who are exploiting us.
3. **Last Round (t=r)**: Defect (D). There is no incentive to cooperate in the final round.

**Pseudocode:**
```python
def adaptive_predator(n, k, r, history):
    if t == 1:
        return 'D'  # Initial round: always defect

    total_cooperators_prev_round = sum([1 for action in history[t-1] if action == 'C'])
    avg_opponent_payoff_prev_round = sum(history[t-1].payoffs) / n
    my_avg_payoff_prev_round = history[t-1].my_payoff

    if total_cooperators_prev_round <= n/2:
        return 'D'  # Not enough cooperators: defect

    p = (k/n) * (total_cooperators_prev_round / n)
    if avg_opponent_payoff_prev_round > my_avg_payoff_prev_round:
        p = min(p + 0.1, 0.9)  # Adapt to opponents exploiting us

    return 'C' if random.random() < p else 'D'
```
**Edge Cases:**

* If the game has only two players (n=2), always Defect (D). In this case, cooperation is not incentivized.
* If the multiplication factor k is close to 1, be more cautious and increase the defect probability. This reduces the risk of exploitation.

The Adaptive Predator strategy aims to balance exploitation with adaptability, making it a robust competitor in the tournament. By monitoring opponent behavior and adjusting its cooperation probability accordingly, this strategy can effectively exploit cooperative opponents while minimizing its own contribution to the public good.
'''

description_EXPLOITATIVE_476 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. The strategy depends only on the game parameters (n, k, r) and the history of play.

**Decision Rules:**

1. **Initial Exploration**: In the first round, cooperate with probability p_init = 0.5 (uniform random). This allows us to gather information about opponents' initial tendencies.
2. **Opponent Modeling**: For each opponent i, maintain a counter c_i that tracks their cooperation rate over the past few rounds (e.g., last 3-5 rounds).
3. **Exploitation Logic**:
	* If an opponent's cooperation rate is high (> 0.7), defect against them to exploit their cooperativeness.
	* If an opponent's cooperation rate is low (< 0.3), cooperate with them to encourage future cooperation and potentially create a more cooperative environment.
	* For opponents with intermediate cooperation rates (0.3 ≤ c_i ≤ 0.7), use a probabilistic approach:
		+ Cooperate with probability p_c = (k/n) \* (1 - c_i) / (1 + (k/n) \* (n-1)), which reflects the potential benefits of cooperation.
4. **Adaptation**: Update opponent models and adjust the exploitation logic based on observed behavior.

**Edge Cases:**

1. **Last Round**: In the final round, always defect to maximize personal payoff.
2. **Early Rounds**: In rounds 2-5, if an opponent has cooperated in all previous rounds, cooperate with them to encourage continued cooperation.

**Pseudocode (for illustrative purposes only)**

```python
def adaptive_exploiter(history, n, k):
    # Initialize opponent models and exploration probability
    opponents = [{ 'cooperation_rate': 0.5, 'counter': 0 } for _ in range(n)]
    p_init = 0.5

    for round in history:
        # Update opponent models
        for i, action in enumerate(round):
            if action == 'C':
                opponents[i]['cooperation_rate'] += 1 / (opponents[i]['counter'] + 1)
                opponents[i]['counter'] += 1

        # Decide own action based on exploitation logic
        my_action = None
        for i, opponent in enumerate(opponents):
            if opponent['cooperation_rate'] > 0.7:
                my_action = 'D'  # Exploit high cooperation rate
            elif opponent['cooperation_rate'] < 0.3:
                my_action = 'C'  # Encourage low cooperation rate
            else:
                p_c = (k/n) * (1 - opponent['cooperation_rate']) / (1 + (k/n) * (n-1))
                my_action = 'C' if random.random() < p_c else 'D'

        # Handle edge cases
        if len(history) == r:  # Last round
            my_action = 'D'
        elif len(history) <= 5:
            for i, opponent in enumerate(opponents):
                if all(round[i] == 'C' for round in history):
                    my_action = 'C'

    return my_action
```

The Adaptive Exploiter strategy aims to balance exploitation and adaptation, taking into account the game parameters and the evolving behavior of opponents.
'''

description_EXPLOITATIVE_477 = '''
Here's a description of an exploitative strategy for the N-Player Public Goods Game:

**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Rounds:** In the first round, play D (Defect). This allows us to observe how other players behave without revealing our own strategy.
2. **Subsequent Rounds:** After the first round, use a simple yet effective heuristic:
	* If the total number of cooperators in the previous round is less than or equal to n/2 (i.e., half of the players), play D (Defect). This indicates that there are enough defectors to make cooperation unprofitable.
	* If the total number of cooperators in the previous round is greater than n/2, play C (Cooperate) with probability p = (k/n) × (total_cooperators / n). This allows us to take advantage of the public good while also adapting to the level of cooperation in the group.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, history):
    if len(history) == 0:  # First round
        return 'D'
    
    total_cooperators = sum(1 for action in history[-1] if action == 'C')
    threshold = n / 2
    
    if total_cooperators <= threshold:
        return 'D'
    else:
        p = (k/n) * (total_cooperators / n)
        return 'C' if random.random() < p else 'D'
```
**Edge Cases:**

* **Last Round:** In the last round, play D (Defect). Since there are no future rounds to consider, we can maximize our payoff by defecting.
* **Ties:** If the total number of cooperators is exactly equal to n/2, use a tiebreaker rule: play D (Defect) if k > 1.5, and C (Cooperate) otherwise. This ensures that we adapt to the specific game parameters.

**Rationale:**

Our strategy is designed to be exploitative by taking advantage of the public good while minimizing our contributions. By cooperating only when there are enough other cooperators, we can free-ride on their efforts and maximize our payoff.

The use of a probability threshold (p) allows us to adapt to the level of cooperation in the group. If many players are cooperating, it's more likely that cooperation will be profitable, so we increase our chances of cooperating.

By defecting in the first round, we gather information about other players' strategies without revealing our own. This helps us make informed decisions in subsequent rounds.

Overall, Adaptive Exploiter is a robust and adaptive strategy that can thrive in a wide range of environments against various opponent behaviors.
'''

description_EXPLOITATIVE_478 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 50%. This allows us to gather information about our opponents' initial strategies while also contributing to the public good.
2. **Exploration Phase**: For rounds 2-5 (or until we have observed at least two distinct actions from each opponent), alternate between cooperation and defection (D). This phase helps us identify the opponents' tendencies, such as their willingness to cooperate or defect.
3. **Exploitation Phase**: After the exploration phase, analyze the history of opponents' actions using the following metrics:
	* Cooperation rate: The proportion of times an opponent has cooperated in previous rounds.
	* Defection rate: The proportion of times an opponent has defected in previous rounds.
	* Recent behavior: The last 2-3 actions taken by each opponent.

Based on these metrics, categorize opponents into three groups:

	* **Cooperators**: Opponents with a high cooperation rate (> 0.6) and low defection rate (< 0.4). Cooperate with them to maximize mutual benefits.
	* **Defectors**: Opponents with a high defection rate (> 0.6) and low cooperation rate (< 0.4). Defect against them to minimize losses.
	* **Unpredictables**: Opponents who do not fit into the above categories or have inconsistent behavior. Play randomly (50% C, 50% D) against them.

**Adaptive Adjustments:**

1. **Re-evaluation**: Every 5 rounds, reassess opponents' strategies based on their updated histories.
2. **Adjust cooperation rate**: If an opponent's cooperation rate increases significantly (> 0.2), increase our own cooperation rate with them by 10%. Conversely, if their cooperation rate decreases significantly (< -0.2), decrease our cooperation rate with them by 10%.
3. **Punish repeated defectors**: If an opponent defects repeatedly (more than twice in a row), defect against them for the next two rounds to teach them that defection is not tolerated.

**Edge Cases:**

1. **Last round**: In the final round, cooperate if our total payoff is above the average of all players' payoffs; otherwise, defect.
2. **Tie-breaking**: If multiple opponents have the same cooperation or defection rate, prioritize those with whom we have interacted more recently.

**Pseudocode:**
```python
def adaptive_exploiter(game_state):
    # Exploration phase (rounds 2-5)
    if game_state.round < 6:
        return random.choice(['C', 'D'])

    # Analyze opponents' histories
    opponents = []
    for opponent in game_state.opponents:
        cooperation_rate = opponent.cooperation_count / game_state.round
        defection_rate = opponent.defection_count / game_state.round
        recent_behavior = opponent.recent_actions

        if cooperation_rate > 0.6 and defection_rate < 0.4:
            opponents.append(('Cooperator', opponent))
        elif defection_rate > 0.6 and cooperation_rate < 0.4:
            opponents.append(('Defector', opponent))
        else:
            opponents.append(('Unpredictable', opponent))

    # Make decisions based on opponents' categories
    for opponent, category in opponents:
        if category == 'Cooperator':
            return 'C'
        elif category == 'Defector':
            return 'D'
        else:  # Unpredictable
            return random.choice(['C', 'D'])

    # Re-evaluation and adjustments (every 5 rounds)
    if game_state.round % 5 == 0:
        for opponent, _ in opponents:
            cooperation_rate = opponent.cooperation_count / game_state.round
            defection_rate = opponent.defection_count / game_state.round

            if cooperation_rate > 0.6 and defection_rate < 0.4:
                # Increase cooperation rate with Cooperators
                pass
            elif defection_rate > 0.6 and cooperation_rate < 0.4:
                # Decrease cooperation rate with Defectors
                pass

    return adaptive_exploiter(game_state)
```
This strategy combines exploration, exploitation, and adaptation to effectively navigate the N-Player Public Goods Game. By analyzing opponents' histories and adjusting our behavior accordingly, we can maximize our payoff while minimizing losses.
'''

description_EXPLOITATIVE_479 = '''
I'll design a exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and maximizes my payoff. I'll call this strategy "Adaptive Exploiter" (AE).

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 0.5. This allows us to gather information about opponents' strategies while minimizing potential losses.
2. **Monitor Opponent Behavior**: Track the number of cooperators (c_j) and defectors (d_j) in each round for all opponents j ≠ i.
3. **Exploitation Threshold**: Set a threshold value θ = k/n, where k is the multiplication factor and n is the number of players. This threshold determines when to exploit opponents' cooperation.
4. **Cooperation-Exploitation Rule**: In rounds t > 1, cooperate (C) if:
	* The average opponent cooperation rate in previous rounds is below θ: Σ(c_j)/(n \* (t-1)) < θ
	* OR, the number of cooperators in the previous round was less than or equal to n/2
5. **Defection Rule**: Defect (D) otherwise.

**Pseudocode**
```python
def Adaptive_Exploiter(n, k, r):
  # Initialize variables
  theta = k/n
  coop_rate = [0] * n
  prev_coop_count = 0

  for t in range(1, r+1):
    if t == 1:
      action = random.choice(['C', 'D'])  # initial cooperation with probability 0.5
    else:
      avg_coop_rate = sum(coop_rate) / (n * (t-1))
      if avg_coop_rate < theta or prev_coop_count <= n/2:
        action = 'C'
      else:
        action = 'D'

    # Update variables for next round
    coop_rate = [int(opponent_action == 'C') for opponent_action in opponent_actions]
    prev_coop_count = sum(coop_rate)
```
**Edge Cases**

* **First Round**: Cooperate with probability 0.5 to gather information about opponents.
* **Last Round**: Defect (D) as there's no future gain from cooperating.
* **Opponent All-Defection**: If all opponents defect, AE will also defect to minimize losses.

**Exploitative Mindset**

The Adaptive Exploiter strategy is designed to capitalize on opponents' cooperation while minimizing its own contributions. By monitoring opponent behavior and adjusting its actions accordingly, AE can:

1. Identify cooperative opponents and exploit their generosity.
2. Avoid contributing when opponents are likely to defect.
3. Take advantage of situations where opponents are uncertain or less likely to cooperate.

This strategy should perform well in a tournament setting against various independent strategies, as it adapts to different opponent behaviors and maximizes its own payoff.
'''

description_EXPLOITATIVE_480 = '''
Here's a strategic approach that leverages the game's structure and parameters to exploit a wide range of opponent behaviors:

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to gather information about opponents' initial strategies and to establish a baseline for future adaptations.
2. **Observation Phase (Rounds 2-5):** Observe opponents' actions and calculate the average cooperation rate (ACR) over these rounds. ACR is the proportion of players cooperating in each round, averaged across rounds.
3. **Adaptive Phase (Rounds 6+):**
	* If ACR ≥ 0.5, defect (D) with probability p = 1 - (k/n), where k and n are game parameters. This takes advantage of the public good's value while minimizing personal contributions.
	* If ACR < 0.5, cooperate (C) with probability p = (k/n). This aims to stimulate cooperation among opponents and increase the public good's value.

**Edge Case Handling:**

1. **Last Round:** Defect (D) in the last round, as there is no future benefit from cooperating.
2. **Low Cooperation:** If ACR < 0.2, switch to always defecting (D). This avoids wasting resources on a poorly performing group.
3. **High Cooperation:** If ACR > 0.8, switch to always cooperating (C) for the remainder of the game. This maximizes personal payoff in a highly cooperative environment.

**Pseudocode:**
```python
# Initialize variables
acr = 0  # Average cooperation rate
round_num = 1

while round_num <= r:
    if round_num == 1:
        action = C  # Cooperate in the first round
    elif round_num < 6:
        # Observation phase
        observe_opponent_actions()
        acr = calculate_acr()
        action = None  # Don't take an action yet
    else:
        # Adaptive phase
        if acr >= 0.5:
            p = 1 - (k/n)
            action = D if random.random() < p else C
        else:
            p = k/n
            action = C if random.random() < p else D

    take_action(action)

    # Update variables for next round
    round_num += 1
    if round_num == r:  # Last round
        action = D

    acr = update_acr(acr, opponent_actions)
```
**Exploitative Mindset:** AE aims to maximize personal payoff by adapting to the group's cooperation level. By initially cooperating and then adjusting its strategy based on opponents' actions, AE exploits both highly cooperative and competitive environments.

This strategy is robust against various opponent behaviors, including:

1. **Free-riders:** AE defects when opponents are not contributing enough, minimizing losses.
2. **Cooperators:** AE cooperates when opponents are contributing significantly, maximizing the public good's value.
3. **Mixed strategies:** AE adapts to mixed cooperation levels by adjusting its probability of cooperating or defecting.

By being adaptive and responsive to opponents' actions, AE increases its chances of winning in a tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_481 = '''
**Exploitative Strategy: "Adaptive Threshold"**

This strategy aims to exploit the opponents' behavior by adapting to their level of cooperation and adjusting our own cooperation threshold accordingly.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 0.5. This is an exploratory move to gauge the opponents' initial behavior.
2. **Threshold Calculation**: After each round, calculate the average cooperation rate of all players in the previous round, excluding our own action. Let's denote this as `avg_coop_prev`.
3. **Adaptive Threshold**: Set a cooperation threshold (`coop_threshold`) based on the game parameters and the observed opponent behavior:
	* If `k` is close to 1 (i.e., `k` < 2), set `coop_threshold` = 0.5 ( conservative approach).
	* If `k` is high (i.e., `k` ≥ 3), set `coop_threshold` = `avg_coop_prev` + 0.1 (aggressive approach).
	* For intermediate values of `k`, interpolate between these two extremes.
4. **Cooperation Decision**: In each subsequent round, cooperate (C) if the average cooperation rate in the previous round (`avg_coop_prev`) is above the calculated `coop_threshold`. Otherwise, defect (D).

**Edge Cases:**

1. **Last Round**: In the last round, always defect (D), as there's no future benefit to cooperating.
2. **Early Rounds**: If the number of rounds is small (e.g., `r` ≤ 5), be more conservative and set `coop_threshold` = 0.5 for the first few rounds.
3. **No Opponent Cooperation**: If no opponents cooperate in a round, defect (D) in the next round.

**Pseudocode:**
```python
def adaptive_threshold(n, k, r):
    coop_threshold = None

    # First round: random cooperation
    if current_round == 1:
        action = random.choice([C, D])
    else:
        avg_coop_prev = calculate_avg_cooperation(n, previous_actions)
        
        # Update cooperation threshold
        if k < 2:
            coop_threshold = 0.5
        elif k >= 3:
            coop_threshold = avg_coop_prev + 0.1
        else:
            coop_threshold = interpolate(coop_threshold, avg_coop_prev, k)

        # Cooperation decision
        if avg_coop_prev > coop_threshold:
            action = C
        else:
            action = D

    return action

def calculate_avg_cooperation(n, actions):
    coop_count = sum(1 for a in actions if a == C)
    return coop_count / n

def interpolate(coop_threshold, avg_coop_prev, k):
    # Interpolate between conservative and aggressive approaches
    return (coop_threshold * (2 - k) + avg_coop_prev * (k - 1)) / (3 - k)
```
This strategy aims to balance cooperation and exploitation by adapting to the opponents' behavior. By adjusting our cooperation threshold based on the game parameters and observed opponent actions, we can maximize our payoff while being robust to a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_482 = '''
**Exploitative Strategy: "Adaptive K-Threshold"**

This strategy aims to maximize individual payoffs by adapting to the opponents' behavior and exploiting their cooperation.

**Decision Rules:**

1. **Initial Rounds (t = 1 to 3):** Cooperate (C) to gather information about opponents' tendencies.
2. **Observation Phase (t > 3):**
	* Calculate the average number of cooperators in the previous rounds (avg_coop).
	* If avg_coop ≥ k/2, defect (D). This threshold indicates that enough players are cooperating, making it beneficial to free-ride.
	* Otherwise, cooperate (C) to encourage others to contribute to the public good.
3. **Adaptive Adjustment:** After every 5 rounds, reassess the opponents' behavior by recalculating avg_coop and adjust the strategy accordingly.

**Edge Cases:**

1. **First Round (t = 1):** Cooperate to avoid missing potential gains from cooperation in early rounds.
2. **Last Round (t = r):** Defect, as there is no future benefit from cooperation.
3. **Consecutive Rounds with No Cooperation:** If no player cooperates for 5 consecutive rounds, switch to a permanent defect strategy.

**Pseudocode:**

```
function AdaptiveKThreshold(n, k, r):
    avg_coop = 0
    coop_count = 0

    for t in range(1, r + 1):
        if t <= 3:
            action = C
        elif avg_coop >= k/2:
            action = D
        else:
            action = C

        # Update avg_coop every 5 rounds
        if t % 5 == 0 and t > 3:
            coop_count = sum(cooperations in previous 5 rounds)
            avg_coop = coop_count / (n * 5)

        # Adjust strategy after every 5 rounds
        if t % 5 == 0 and avg_coop < k/2:
            action = C

        # Handle edge cases
        if t == 1:
            action = C
        elif t == r:
            action = D

        return action
```

This strategy is designed to be exploitative, taking advantage of opponents' cooperation while minimizing its own contributions. By adapting to the average level of cooperation and adjusting its behavior accordingly, it aims to maximize individual payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_483 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy aims to exploit the behavior of opponents by adapting to their cooperation rates while prioritizing self-interest.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about opponents' initial behaviors.
2. **Exploration Phase (2 < t ≤ r/2):**
	* If the average cooperation rate of all players in the previous round is high (> 0.5), Defect (D).
	* Otherwise, Cooperate (C) with a probability proportional to the opponent's average cooperation rate in the previous round.
3. **Exploitation Phase (t > r/2):**
	* If an opponent has cooperated in at least half of the previous rounds, Exploit (D) them by defecting against them.
	* Otherwise, Cooperate (C) with a probability inversely proportional to the number of opponents who have exploited you in the past.

**Handling Edge Cases:**

1. **Last Round (t = r):** Defect (D), as there is no future interaction to worry about.
2. **Ties:** In case of ties, use a randomization mechanism to break them (e.g., coin flip).

**Pseudocode for the Adaptive Predator Strategy:**
```
  // Initialization
  int t = 0; // current round number
  float avg_coop_rate = 0; // average cooperation rate of opponents
  bool[] coop_history = new bool[n]; // cooperation history for each opponent

  while (t < r) {
    if (t <= 2) { // Initial Rounds
      action = C;
    } else if (t <= r/2) { // Exploration Phase
      avg_coop_rate = calculate_avg_coop_rate(opponents);
      if (avg_coop_rate > 0.5) {
        action = D;
      } else {
        float p_coop = avg_coop_rate / n;
        action = (random() < p_coop) ? C : D;
      }
    } else { // Exploitation Phase
      for each opponent i {
        if (coop_history[i] >= t/2) {
          action_against_i = D; // exploit
        } else {
          float p_coop = 1 / (n - coop_history[i]);
          action_against_i = (random() < p_coop) ? C : D;
        }
      }
    }

    // Update cooperation history and average cooperation rate
    update_coop_history(opponents);
    avg_coop_rate = calculate_avg_coop_rate(opponents);

    t++;
  }
```
**Aligning with the Exploitative Mindset:**

The Adaptive Predator strategy prioritizes self-interest by:

1. Defecting in situations where opponents are likely to cooperate.
2. Exploiting opponents who have cooperated frequently, taking advantage of their good nature.
3. Randomizing actions when there is uncertainty about opponents' behaviors.

By adapting to the behavior of opponents and exploiting their cooperation rates, the Adaptive Predator strategy aims to maximize its own payoff while minimizing the impact on others.
'''

description_EXPLOITATIVE_484 = '''
To design an exploitative strategy for this N-Player Public Goods Game, we'll focus on a robust and adaptive approach that leverages game parameters and history without relying on communication or specific opponent behaviors.

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **First Round:** Cooperate (C). This initial cooperation serves as a probe to gather information about opponents' strategies and potential willingness to cooperate.
2. **Subsequent Rounds:** Observe the total number of cooperators (T) in the previous round.
   - If T > n/2, Defect (D). When more than half of the players cooperate, it's likely that some are playing a cooperative strategy. Defecting here allows us to exploit their cooperation and maximize our payoff without contributing to the public good.
   - If T ≤ n/2, use the following logic:
     - Calculate the average payoff per player (APP) in the previous round. This is done by summing all payoffs observed (including your own) and dividing by n.
     - If APP < k/2, Cooperate (C). A low average payoff suggests that too many players are defecting or not contributing enough to the public good. By cooperating, we aim to increase the total contributions and potentially encourage others to cooperate as well, given the game's structure and payoffs.
     - If APP ≥ k/2, Defect (D). High average payoffs indicate sufficient cooperation or that opponents are already exploiting a cooperative strategy effectively. In this case, defecting allows us to capture a share of the public good without contributing.

**Edge Cases:**

- **Last Round:** Always Defect (D), as there is no future round where others could retaliate against your defection.
- **When All Players Cooperate or Defect Consistently:** If all players consistently choose C or D, adapt by mirroring their strategy for one round to avoid missing out on potential payoffs. Then, return to the default decision rules.

**Pseudocode:**

```python
# Initialize variables
n = number_of_players  # Total players in the game
k = multiplication_factor  # Game's multiplication factor
r = rounds  # Number of rounds in the game
history = []  # To store previous rounds' information

for round in range(r):
    if round == 0:  # First Round
        action = "Cooperate"
    else:
        T = sum(1 for prev_action in history[-1] if prev_action == "Cooperate")
        
        if T > n/2:
            action = "Defect"
        else:
            APP = sum(player_payoff for player_payoff in get_previous_round_payoffs()) / n
            if APP < k/2:
                action = "Cooperate"
            else:
                action = "Defect"

    # Update history and play the game with chosen action

# Utility function to get previous round's payoffs, not fully implemented here
def get_previous_round_payoffs():
    pass  # Implement logic to gather payoffs from all players in the last round
```

**Rationale:**
This strategy, AE, is designed to be exploitative by identifying and capitalizing on opportunities where cooperation levels are high enough to exploit without contributing. It maintains a balance between exploiting others' cooperation and contributing when necessary to maximize overall payoff, especially in early rounds or when opponents' strategies suggest low cooperation rates. By adapting based on observed cooperation and payoffs, AE aims to outperform more static strategies across a variety of opponent behaviors and game conditions.
'''

description_EXPLOITATIVE_485 = '''
I'll design a exploitative strategy for the N-Player Public Goods Game that adapts to the game parameters and history. Here's my approach:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) in the first three rounds to gather information about the opponents' behavior. This phase helps AE understand the average cooperation rate and potential patterns.
2. **Exploitation Phase**: After Round 3, AE enters the exploitation phase. In each round, calculate the average cooperation rate of all players in the previous round (`avg_coop_prev`). If `avg_coop_prev` is above a certain threshold (`coop_threshold`), AE defects (D). Otherwise, AE cooperates (C).
3. **Punishment Mechanism**: To discourage opponents from defecting, AE introduces a punishment mechanism. If an opponent defects in the current round and their previous round's action was also a defection, AE will defect in the next round against that specific opponent.
4. **Adaptive Threshold Adjustment**: AE adjusts the `coop_threshold` value based on the game's history. If the average payoff of all players increases over time, AE decreases the `coop_threshold` to encourage more cooperation. Conversely, if the average payoff decreases, AE increases the `coop_threshold` to reduce exploitation.

**Pseudocode:**
```python
# Initialize variables
avg_coop_prev = 0
coop_threshold = 0.5  # initial threshold value

for round in range(1, r+1):
    if round <= 3:
        # Initial Exploration Phase
        action = C
    else:
        # Exploitation Phase
        avg_coop_prev = calculate_average_cooperation_rate(prev_round)
        if avg_coop_prev > coop_threshold:
            action = D
        else:
            action = C
        
        # Punishment Mechanism
        for opponent in opponents:
            if opponent_defected_last_two_rounds(opponent):
                action_against_opponent = D
    
    # Update average cooperation rate and threshold value
    avg_coop_prev = calculate_average_cooperation_rate(round)
    update_coop_threshold(avg_coop_prev)

def calculate_average_cooperation_rate(round):
    # Calculate the average cooperation rate of all players in the given round
    pass

def opponent_defected_last_two_rounds(opponent):
    # Check if an opponent defected in the last two rounds
    pass

def update_coop_threshold(avg_coop_prev):
    # Adjust the coop_threshold value based on the game's history
    global coop_threshold
    if average_payoff_increase():
        coop_threshold -= 0.1
    elif average_payoff_decrease():
        coop_threshold += 0.1
```
**Edge Cases:**

* First round: Cooperate (C) to gather information.
* Last round: AE will still follow the decision rules based on the previous rounds' history.
* If an opponent defects in all rounds, AE will eventually defect against that opponent due to the punishment mechanism.

**Exploitative Mindset:** AE's primary goal is to exploit the opponents' cooperative behavior while minimizing its own contributions. By adapting to the game's history and adjusting its threshold value, AE aims to maximize its payoff by finding an optimal balance between cooperation and defection.
'''

description_EXPLOITATIVE_486 = '''
Here's a robust and exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round:** Defect (D) in the first round to gather information about opponents' initial actions.
2. **Subsequent Rounds:** Use the following rules:
	* If the total number of cooperators in the previous round is less than n/2, Cooperate (C) with probability p = k/n (i.e., the multiplication factor divided by the number of players).
	* If the total number of cooperators in the previous round is greater than or equal to n/2, Defect (D).
3. **Last Round:** Defect (D) in the last round to maximize individual payoff.
4. **Edge Case Handling:**
	+ If all opponents defected in the previous round, Cooperate (C) with probability p = k/n in the next round.
	+ If all opponents cooperated in the previous round, Defect (D) in the next round.

**Pseudocode:**

```
initialize variables:
  n: number of players
  k: multiplication factor
  r: number of rounds
  history: array to store actions and payoffs from previous rounds

first_round = True

for t = 1 to r do:
  if first_round:
    action = D (defect)
    first_round = False
  else:
    total_cooperators = count cooperators in history[t-1]
    if total_cooperators < n/2:
      p = k/n
      action = C (cooperate) with probability p, otherwise D (defect)
    else:
      action = D (defect)

  observe opponents' actions and payoffs
  update history

  if t == r:  // last round
    action = D (defect)

  take action and receive payoff
```

**Rationale:**

The Adaptive Exploiter strategy is designed to balance exploration (gathering information about opponents) with exploitation (maximizing individual payoff). By defecting in the first round, AE gathers information about opponents' initial actions. In subsequent rounds, AE uses a probabilistic approach to cooperate or defect based on the number of cooperators in the previous round.

When the number of cooperators is low, AE cooperates with a probability proportional to the multiplication factor (k/n). This encourages other players to cooperate and increases the public good, which benefits all players. When the number of cooperators is high, AE defects to maximize its individual payoff.

In edge cases, AE adapts by cooperating when all opponents defected previously or defecting when all opponents cooperated. In the last round, AE defects to maximize its individual payoff, as cooperation will not affect future payoffs.

This strategy should be robust against a wide range of opponent behaviors and exploitative in nature, making it a strong contender in a tournament setting.
'''

description_EXPLOITATIVE_487 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors in the N-Player Public Goods Game. This strategy focuses on maximizing individual payoff by adapting to the collective cooperation level and identifying opportunities to defect.

**Decision Rules:**

1. **Initial Cooperation (Round 1)**:
	* Cooperate (C) in the first round to gather information about opponents' initial actions.
2. **Assess Collective Cooperation**:
	* After each round, calculate the average cooperation rate of all players (excluding self).
	* Use this value to determine the likelihood of collective cooperation.
3. **Adaptive Threshold**:
	* Set an adaptive threshold `θ` based on the game parameters and history.
	* Initialize `θ` as a function of `k`, the multiplication factor: `θ = k / n`.
4. **Cooperation vs Defection**:
	* In each round, calculate the expected payoff from cooperation (`E[C]`) and defection (`E[D]`) using the current average cooperation rate.
	* If `E[C] > E[D]`, cooperate (C). Otherwise, defect (D).

Pseudocode for calculating expected payoffs:

```
def calculate_expected_payoffs(avg_coop_rate):
  # Private payoff from keeping
  private_payoff = 1
  
  # Share of public good from cooperation
  coop_benefit = (k / n) * avg_coop_rate * n
  
  # Expected payoff from cooperation
  E_C = coop_benefit
  
  # Expected payoff from defection
  E_D = private_payoff + (k / n) * avg_coop_rate * (n - 1)
  
  return E_C, E_D

def decide_action(avg_coop_rate):
  E_C, E_D = calculate_expected_payoffs(avg_coop_rate)
  
  if E_C > E_D:
    return 'C'  # Cooperate
  else:
    return 'D'  # Defect
```

**Handling Edge Cases:**

* **Last Round**: In the final round, always defect (D), as there is no future game to consider.
* **Low Cooperation Rate**: If the average cooperation rate falls below a certain threshold (`θ_low = 0.2 * θ`), switch to permanent defection (D) for the remainder of the game.

**Exploitative Mindset:**

The Adaptive Predator strategy prioritizes individual payoff maximization over collective cooperation. By adapting to the collective cooperation level and identifying opportunities to defect, this strategy aims to exploit opponents who consistently cooperate or follow predictable patterns.

This approach takes advantage of the fact that many strategies rely on reciprocity or cooperative norms, which can be exploited by a well-timed defection. The Adaptive Predator's adaptive threshold (`θ`) ensures that it remains responsive to changes in opponent behavior while maintaining an exploitative stance.
'''

description_EXPLOITATIVE_488 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. This strategy only relies on game parameters and history.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) in the first round to gather information about opponents' initial tendencies.
	* In rounds 2-3, play a mixed strategy: Cooperate with probability 0.5, and Defect with probability 0.5. This helps to further explore opponents' reactions to cooperation and defection.
2. **Assessing Opponent Behavior (Rounds 4-6)**:
	* Observe the average cooperation rate of opponents in rounds 1-3. If it's above 0.5, assume opponents are cooperative; otherwise, assume they're defective.
	* Based on this assessment, adjust the strategy for the next phase.
3. **Exploitation Phase (Rounds 7-r)**:
	* If opponents were assessed as cooperative:
		+ Cooperate with probability p = (k/n) \* average_cooperation_rate (rounded to two decimal places).
		+ Defect with probability 1 - p.
	* If opponents were assessed as defective:
		+ Defect in all subsequent rounds, as there's no incentive to cooperate.

**Edge Cases:**

1. **Last Round (r)**:
	* Regardless of the opponent assessment, defect in the last round to maximize individual payoff.
2. **Opponent Cooperation Rate near 0.5**:
	* If the average cooperation rate is close to 0.5 (±0.1), treat it as an uncertain case and play a mixed strategy: Cooperate with probability 0.5, and Defect with probability 0.5.

**Pseudocode:**
```python
def adaptive_predator(n, k, r, history):
    # Initial Exploration Phase
    if round_num <= 3:
        if round_num == 1:
            return "C"  # Cooperate in the first round
        else:
            return random.choice(["C", "D"])  # Mixed strategy

    # Assessing Opponent Behavior Phase
    avg_coop_rate = calculate_average_cooperation_rate(history, n)
    opponent_behavior = "cooperative" if avg_coop_rate > 0.5 else "defective"

    # Exploitation Phase
    if opponent_behavior == "cooperative":
        p = (k / n) * avg_coop_rate
        return "C" with probability p, "D" with probability 1 - p
    else:
        return "D"  # Defect in all subsequent rounds

    # Edge Cases
    if round_num == r:  # Last Round
        return "D"
    elif abs(avg_coop_rate - 0.5) <= 0.1:  # Uncertain case
        return random.choice(["C", "D"])  # Mixed strategy
```
This strategy aims to exploit opponents by:

* Initially exploring their behavior and adjusting the strategy accordingly.
* Assessing opponent cooperation rates to determine whether they're likely to cooperate or defect.
* Adapting the strategy based on this assessment, either by cooperating with a probability proportional to the opponent's cooperation rate or defecting in all subsequent rounds.
* Defecting in the last round to maximize individual payoff.

By being adaptive and robust, the Adaptive Predator strategy is designed to perform well against a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_489 = '''
**Strategy Name: Adaptive Exploiter**

The Adaptive Exploiter is a robust and adaptive strategy designed to exploit a wide range of opponent behaviors in the N-Player Public Goods Game.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability 0.5, Defect (D) with probability 0.5. This is an exploratory phase to gather information about opponents' initial tendencies.
2. **Early Rounds (t=2-3):** Observe the average cooperation rate of all players in the previous round(s). If the average cooperation rate is above a certain threshold (THRESHOLD = 0.4), cooperate with probability 0.6, otherwise defect with probability 0.7.
3. **Mid-Rounds (t=4-r/2):** Implement a simple Tit-for-Tat variant:
	* Cooperate if the majority of players (> n/2) cooperated in the previous round.
	* Defect if the majority of players (> n/2) defected in the previous round.
	* If exactly half the players cooperated, cooperate with probability 0.5.
4. **Late Rounds (t=r/2-r):** Analyze the past behavior of each opponent:
	* Identify opponents who have consistently cooperated ( > 70% cooperation rate).
	* Exploit these cooperative opponents by defecting against them with high probability (0.8).
	* Against less cooperative opponents, revert to the mid-rounds Tit-for-Tat variant.
5. **Final Round (t=r):** Defect (D) unconditionally.

**Edge Cases:**

1. **Last round:** Defect (D) unconditionally to maximize individual payoff.
2. **Opponent cooperation rate is 100% or 0%**: Adjust the THRESHOLD value accordingly:
	* If all opponents cooperate, set THRESHOLD = 0.9 and cooperate with probability 0.8.
	* If no opponents cooperate, set THRESHOLD = 0.1 and defect with probability 0.9.

**Pseudocode:**
```python
def AdaptiveExploiter(n, k, r):
    # Initialize variables
    cooperation_rate = []
    opponent_history = [[] for _ in range(n)]
    
    for t in range(r):
        if t == 1:
            # Initial round
            action = random.choice([C, D])
        elif t <= 3:
            # Early rounds
            avg_cooperation_rate = sum(cooperation_rate) / len(cooperation_rate)
            if avg_cooperation_rate > THRESHOLD:
                action = C with probability 0.6
            else:
                action = D with probability 0.7
        elif t < r/2:
            # Mid-rounds
            majority_action = get_majority_action(opponent_history)
            action = majority_action
        else:
            # Late rounds
            for opponent in range(n):
                if opponent_consistent_cooperator(opponent, opponent_history):
                    action = D with probability 0.8
                else:
                    majority_action = get_majority_action(opponent_history)
                    action = majority_action
        
        # Take action and update variables
        cooperation_rate.append(action == C)
        for opponent in range(n):
            opponent_history[opponent].append(get_opponent_action(opponent))
    
    return total_payoff
```
**Notes:**

* The Adaptive Exploiter aims to balance exploration, exploitation, and adaptation throughout the game.
* By monitoring opponents' behavior and adjusting its strategy accordingly, this exploitative approach can take advantage of cooperative tendencies while protecting against exploitation by defectors.

This strategy will be implemented as an algorithm for tournament play.
'''

description_EXPLOITATIVE_490 = '''
Here's a robust and exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of 50% to gather information about opponents' behaviors.
2. **Middle Rounds (1 < t < r):**
	* If the average cooperation rate among all players in the previous round is above k/n, Defect (D). This indicates that there are enough cooperators to generate a significant public good, and you can exploit this by free-riding.
	* Otherwise, Cooperate (C) with a probability of p = (k/n) / (1 + (k/n)). This probability adjusts based on the game parameters, encouraging cooperation when the potential benefits of the public good are higher.
3. **Last Round (t=r):** Defect (D), as there's no future opportunity to influence opponents' behaviors.

**Edge Case Handling:**

* If all players defected in the previous round, Cooperate (C) with a probability of 50% in the current round to encourage cooperation and create an opportunity for exploitation.
* If all players cooperated in the previous round, Defect (D) in the current round to take advantage of the public good generated.

**Adaptive Component:**

* Monitor the average cooperation rate among opponents over the last few rounds (e.g., a moving average of 3-5 rounds).
* Adjust your cooperation probability p based on this trend:
	+ If the average cooperation rate is increasing, decrease p to exploit the growing public good.
	+ If the average cooperation rate is decreasing, increase p to encourage cooperation and maintain a viable public good.

**Pseudocode:**
```
function adaptive_exploiter(n, k, r, history) {
  if (t == 1) { // Initial Round
    return random_choice([C, D], [0.5, 0.5]);
  } else if (t < r) { // Middle Rounds
    avg_coop_rate = calculate_avg_cooperation(history);
    if (avg_coop_rate > k/n) {
      return D;
    } else {
      p = (k/n) / (1 + (k/n));
      return random_choice([C, D], [p, 1-p]);
    }
  } else { // Last Round
    return D;
  }
}

function calculate_avg_cooperation(history) {
  // Calculate the average cooperation rate among opponents over the last few rounds
}
```
This strategy balances exploitation and adaptability, allowing it to perform well against a wide range of opponent behaviors. By monitoring the game's progression and adjusting its behavior accordingly, the Adaptive Exploiter can maximize its payoffs while minimizing its contributions to the public good.
'''

description_EXPLOITATIVE_491 = '''
Here's a description of an exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **First Round:** Cooperate (C). This sets a positive tone and gathers information about opponents' behavior.
2. **Subsequent Rounds:** Defect (D) if the average cooperation rate in the previous round is below a certain threshold (τ), which depends on the game parameters. Specifically, τ = (k - 1) / k. If the average cooperation rate is above or equal to τ, Cooperate (C).
3. **Exception:** If all opponents defected in the previous round, Cooperate (C) with probability p = 0.5. This introduces some randomness to avoid being easily exploitable.
4. **Last Round:** Defect (D). There's no incentive to cooperate when there are no future rounds.

**Pseudocode:**
```
Initialize:
  τ = (k - 1) / k
  prev_coop_rate = 0

For each round t = 1 to r:
  If t == 1:
    action = C
  Else:
    coop_count = count(C actions in previous round)
    avg_coop_rate = coop_count / n
    If avg_coop_rate < τ or (avg_coop_rate == τ and random() < p):
      action = D
    Else:
      action = C
    prev_coop_rate = avg_coop_rate

  If t == r:  // Last round
    action = D

Return action
```
**Rationale:**

The Adaptive Exploiter strategy aims to balance short-term gains with long-term adaptability. By cooperating in the first round, it sets a positive tone and gathers information about opponents' behavior.

In subsequent rounds, the strategy defects if the average cooperation rate is below the threshold τ. This ensures that the player doesn't contribute too much to the public good when others are not cooperating.

The exception clause introduces randomness when all opponents defected in the previous round. This makes it harder for opponents to exploit the Adaptive Exploiter and encourages them to cooperate more.

In the last round, defecting is the optimal choice since there's no future interaction.

**Exploitability:**

This strategy is designed to be exploitative by taking advantage of opponents' cooperative behavior while minimizing its own contributions. By adapting to the average cooperation rate, it can effectively free-ride on others' cooperation or punish them for not cooperating enough.

The Adaptive Exploiter strategy should perform well against a wide range of opponent behaviors, including pure cooperators, defectors, and tit-for-tat players.
'''

description_EXPLOITATIVE_492 = '''
I'll design a exploitative strategy that adapts to the game's history and parameters.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round**: In the first round, AE defects (D) to gather information about opponents' behavior.
2. **Early Rounds**: For rounds 2-5, AE uses a simple tit-for-tat approach:
	* If the majority of players (> n/2) cooperated in the previous round, AE cooperates (C).
	* Otherwise, AE defects (D).
3. **Middle Game**: After round 5, AE switches to a more sophisticated approach:
	* Calculate the average cooperation rate (ACR) over the last 3 rounds for all players.
	* If ACR > k/n, AE cooperates (C). This means that if the average cooperation rate is higher than the multiplication factor divided by the number of players, AE will cooperate.
	* Otherwise, AE defects (D).
4. **Late Game**: In the last 2 rounds, AE reverts to a more conservative approach:
	* If the total payoff over the entire game so far is above the average payoff per round, AE cooperates (C).
	* Otherwise, AE defects (D).

**Edge Cases:**

* If there's only one opponent, AE always defects (D) in all rounds.
* In case of a tie in ACR calculation, AE cooperates (C).

**Exploitative Mindset:**
The Adaptive Exploiter strategy aims to capitalize on the opponents' cooperative behavior while minimizing its own contributions. By initially defecting and then using simple tit-for-tat, AE encourages opponents to cooperate. As the game progresses, AE becomes more selective in its cooperation, only contributing when it's likely to receive a higher payoff.

The ACR calculation helps AE identify periods of high cooperation and adapt accordingly. In the late game, AE prioritizes maximizing its total payoff over potential future gains from cooperation.

**Pseudocode:**
```python
def Adaptive_Exploiter(n, k, r, history):
  if round == 1:
    return 'D'
  
  if round <= 5:
    majority_cooperated = sum(history[-1]) > n/2
    return 'C' if majority_cooperated else 'D'
  
  acr = sum([sum(round) for round in history[-3:]]) / (3 * n)
  return 'C' if acr > k/n else 'D'
  
  if round >= r - 1:
    total_payoff = sum(history[:-1])
    avg_payoff_per_round = total_payoff / (round - 1)
    return 'C' if total_payoff > avg_payoff_per_round else 'D'
```
This strategy is designed to be robust and adaptive, making it a strong competitor in the tournament.
'''

description_EXPLOITATIVE_493 = '''
To develop an exploitative strategy for this N-Player Public Goods Game, we'll create a decision-making process that adapts based on the game's history and parameters, aiming to maximize our payoff while minimizing cooperation from others.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (Round 1):** Cooperate (C) to signal potential for cooperation and observe opponents' initial moves.
2. **Rounds 2 to r-1:** Implement the following logic:

   a. Calculate the average number of cooperators (avg_coop) in previous rounds.

   b. If avg_coop is greater than or equal to n/2, defect (D). This indicates that enough players are cooperating to generate a decent public good payoff without your contribution.

   c. If avg_coop is less than n/2 and the multiplication factor k is close to 1 (k < 1.5), cooperate (C) to encourage others to contribute, aiming for a higher total payoff.

   d. In all other scenarios where k >= 1.5 or when avg_coop is low, defect (D). This is because with k > 1.5, the potential gain from cooperating is high even with fewer contributors, and you can exploit this by not contributing yourself.

3. **Last Round (Round r):** Defect (D) to maximize personal payoff since there are no future rounds to consider for reciprocity or retaliation.

**Handling Edge Cases:**

- **Tie in avg_coop calculation:** If the average number of cooperators equals exactly n/2, and considering the multiplication factor k is crucial, lean towards defecting if k >= 1.5 and cooperating otherwise.
  
- **Opponent's Behavior Patterns:** AE does not rely on recognizing or mimicking specific patterns but adapts based on the collective behavior (average cooperation rate), making it robust against a wide range of strategies.

**Exploitative Mindset:**

The Adaptive Exploiter strategy aligns with an exploitative mindset by:

- Initially cooperating to encourage others and then adapting based on their responses.
- Defecting when the average cooperation is high enough to generate a significant public good payoff without personal contribution.
- Being cautious in scenarios where k is low, as there's less incentive for others to cooperate.

This approach balances between encouraging cooperation for higher total payoffs and defecting to maximize individual gain when it's safe or beneficial to do so. By not assuming specific behaviors from opponents and adapting based on observed actions, AE remains robust against various strategies in a tournament setting.
'''

description_EXPLOITATIVE_494 = '''
Here's a description of an exploitative strategy for the N-Player Public Goods Game that depends on game parameters and history:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Rounds (t ≤ n):** Defect (D) in the first 'n' rounds to gather information about opponents' behavior and to minimize potential losses.
2. **Contribution Threshold (CT):** Calculate CT as a function of the game parameters:
   CT = k / n

   If the average contribution rate of other players in previous rounds is above CT, cooperate (C) with probability p = 1 - (1/CT). Otherwise, defect (D).

3. **Adaptive Response:** Monitor opponents' behavior and adjust cooperation probability 'p' based on their actions:
   - If the number of cooperators increases, increase 'p' by 0.1.
   - If the number of defectors increases, decrease 'p' by 0.1.

**Edge Cases:**

* **First Round (t = 1):** Defect (D) to gather information and minimize potential losses.
* **Last Round (t = r):** Cooperate (C) if the average contribution rate is above CT; otherwise, defect (D).
* **Tiebreaker:** In case of a tie in the number of cooperators and defectors, cooperate (C).

**Additional Heuristics:**

* **Contribution History:** Keep track of each player's contribution history to identify patterns or deviations.
* **Exploiting Free Riders:** If an opponent is identified as a consistent free rider (always defecting), target them by cooperating only when they do.

**Pseudocode Example:**
```python
def adaptive_exploiter(n, k, r, current_round):
    # Initialize variables
    contribution_threshold = k / n
    cooperation_probability = 1 - (1/contribution_threshold)
    opponent_contribution_rates = [0] * n

    for round in range(1, current_round):
        # Observe opponents' actions and update their contribution rates
        for i in range(n):
            if opponents_actions[round][i] == 'C':
                opponent_contribution_rates[i] += 1 / (round + 1)

        # Update cooperation probability based on average contribution rate
        avg_contribution_rate = sum(opponent_contribution_rates) / n
        if avg_contribution_rate > contribution_threshold:
            cooperation_probability *= 0.9

    # Decide action for the current round
    if random.random() < cooperation_probability:
        return 'C'
    else:
        return 'D'

def opponents_actions(round):
    # Simulate observing opponents' actions (to be replaced with actual observations)
    actions = []
    for _ in range(n):
        actions.append(random.choice(['C', 'D']))
    return actions
```
This strategy is designed to adapt to a wide range of opponent behaviors, exploiting their cooperation while minimizing potential losses. By monitoring contribution rates and adjusting the cooperation probability accordingly, it aims to maximize its own payoff over multiple rounds.

**Note:** This pseudocode example is for illustrative purposes only and should be replaced with an actual implementation that takes into account the game's specifics and requirements.
'''

description_EXPLOITATIVE_495 = '''
**Strategy Name: Adaptive Exploiter**

The Adaptive Exploiter strategy is designed to exploit the behavior of opponents while adapting to their actions over time. This strategy does not rely on coordination mechanisms or shared norms.

**Decision Rules:**

1. **Initial Round (Round 1):** Cooperate (C) in the first round to gather information about the opponent's behavior and potential cooperation levels.
2. **Subsequent Rounds:** Observe the total number of cooperators (TC) in the previous round and calculate the average payoff per cooperator (APPC) as follows:

   APPC = (k/n) × TC / TC

   If APPC > 1, it indicates that cooperators earned a higher payoff than defectors. In this case:
   
   * Cooperate (C) if the number of cooperators in the previous round is increasing or stable.
   * Defect (D) if the number of cooperators is decreasing.

   If APPC ≤ 1, it suggests that defectors earned a better or equal payoff than cooperators. In this case:
   
   * Defect (D).

**Edge Cases:**

* **Last Round:** Defect (D) in the last round to maximize individual payoff.
* **Single Opponent Cooperation:** If only one opponent cooperated in the previous round, defect (D) to exploit their cooperation.
* **No Previous Rounds:** In case of incomplete information or missing data from previous rounds, default to cooperating (C).

**Pseudocode:**
```
// Initialize variables
TC_prev = 0 // Total cooperators in previous round
APPC_prev = 0 // Average payoff per cooperator in previous round

// Main Loop (for each round)
if (round == 1) {
    action = C // Cooperate in the first round
} else {
    TC_curr = count_cooperators() // Count total cooperators in current round
    APPC_curr = calculate_APPC(TC_curr, k, n)

    if (APPC_prev > 1) {
        if (TC_curr >= TC_prev) {
            action = C // Cooperate if cooperators are increasing or stable
        } else {
            action = D // Defect if cooperators are decreasing
        }
    } else {
        action = D // Defect if APPC is low
    }

    // Update variables for next round
    TC_prev = TC_curr
    APPC_prev = APPC_curr

    // Last Round Exception
    if (round == r) {
        action = D // Defect in the last round
    }
}
```
**Rationale:**

The Adaptive Exploiter strategy is designed to balance exploitation with adaptability. By cooperating initially, it gathers information about opponents' behavior and potential cooperation levels. The APPC calculation helps identify situations where cooperators earn higher payoffs than defectors, allowing the strategy to adjust its actions accordingly.

By defecting when APPC is low or decreasing, the strategy exploits the cooperation of others while minimizing its own contributions. In the last round, it defects to maximize individual payoff.

This strategy should perform well against a wide range of opponent behaviors and does not rely on coordination mechanisms or shared norms.
'''

description_EXPLOITATIVE_496 = '''
**Exploitative Strategy: "Adaptive Defector"**

The Adaptive Defector strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy depends only on the game parameters (n and k) and the observed actions of other players.

**Decision Rules:**

1. **Initial Rounds**: In the first round, defect (play D). This allows us to observe the opponents' initial behavior without committing to cooperation.
2. **Observation Phase**: For rounds 2 to r/2 (or a smaller fraction of total rounds, e.g., r/4), cooperate (play C) if the majority of players cooperated in the previous round. Otherwise, defect (play D). This phase helps us understand the opponents' general behavior and identify potential cooperators.
3. **Exploitation Phase**: After the observation phase, switch to a more exploitative approach:
	* Cooperate (play C) only if the number of cooperators in the previous round is above a certain threshold, which we'll call `coop_threshold`. This value is calculated as follows: `coop_threshold = n / (k + 1)`.
	* Defect (play D) otherwise.

**Pseudocode**:
```python
def adaptive_defector(n, k, history):
    if len(history) == 0:  # First round
        return 'D'
    
    if len(history) < r/2:  # Observation phase
        prev_coops = sum(1 for action in history[-1] if action == 'C')
        if prev_coops > n / 2:
            return 'C'
        else:
            return 'D'
    
    # Exploitation phase
    coop_threshold = n / (k + 1)
    prev_coops = sum(1 for action in history[-1] if action == 'C')
    if prev_coops >= coop_threshold:
        return 'C'
    else:
        return 'D'
```
**Edge Cases:**

* **Last Round**: In the last round, defect (play D) to maximize personal payoff.
* **Ties**: If there's a tie in the number of cooperators, cooperate (play C) if the opponent has cooperated more often in the past.

**Rationale:**

The Adaptive Defector strategy is designed to:

1. Observe opponents' behavior and identify potential cooperators during the observation phase.
2. Exploit opponents who are likely to cooperate by defecting when they do.
3. Cooperate only when it's likely to yield a higher payoff, i.e., when there are enough cooperators to make the public good valuable.

By adapting to the game's history and exploiting opponents' behavior, this strategy aims to achieve a high total payoff while minimizing cooperation.
'''

description_EXPLOITATIVE_497 = '''
Here's a robust and adaptive exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to gather information about opponents' behaviors.
2. **Early Rounds (t>1, t<0.5r):**
	* If the average cooperation rate of all players in previous rounds is above a threshold (e.g., 0.4), Defect (D). This indicates that opponents are willing to cooperate, and you can exploit them.
	* Otherwise, Cooperate (C) to try to induce cooperation from others.
3. **Mid-Game (t≥0.5r):**
	* If your cumulative payoff is below the average payoff of all players, Defect (D). This suggests that opponents are exploiting you, and it's time to adapt.
	* Otherwise, Cooperate (C) if the number of cooperators in the previous round is increasing; Defect (D) otherwise. This responds to changes in opponent behavior.
4. **Late Rounds (t>0.8r):**
	* If your cumulative payoff is still below the average payoff of all players, Cooperate (C). This may induce opponents to cooperate, allowing you to catch up.
	* Otherwise, Defect (D) to maximize payoffs in the remaining rounds.

**Edge Cases:**

1. **Last Round (t=r):** Defect (D), as there's no future benefit from cooperation.
2. **Tiebreakers:** In case of a tie in the decision rules, Cooperate (C). This biases towards cooperation when opponents are uncertain.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r, history):
    if t == 1:  # Initial round
        return C

    avg_coop_rate = calculate_average_cooperation_rate(history)
    if t < 0.5 * r and avg_coop_rate > 0.4:
        return D

    my_cumulative_payoff = calculate_my_cumulative_payoff(history)
    avg_payoff = calculate_average_payoff(history)

    if my_cumulative_payoff < avg_payoff:
        return D

    prev_round_cooperators = count_cooperators_in_previous_round(history)
    if t >= 0.5 * r and prev_round_cooperators > n / 2:
        return C
    else:
        return D

    if t > 0.8 * r and my_cumulative_payoff < avg_payoff:
        return C

    # Last round or tiebreaker
    return D
```
This strategy is designed to be exploitative, adaptive, and robust to various opponent behaviors. It gathers information in the early rounds, adapts to changes in opponent behavior, and maximizes payoffs in the late rounds. The pseudocode provides a clear implementation of the decision rules.
'''

description_EXPLOITATIVE_498 = '''
I'll design a strategic approach that is adaptive, robust, and exploitative, leveraging game theory principles and historical data from previous rounds.

**Strategy Name:** "Adaptive Exploiter" (AE)

**Decision Rules:**

1. **First Round**: Play Cooperate (C) to probe opponents' strategies and gather initial information.
2. **Subsequent Rounds**: Employ a combination of history-based and threshold-based decision rules:
	* **History-based**: Observe the total number of cooperators in the previous round (`prev_coop_count`).
	* **Threshold-based**: Calculate the expected payoff from cooperating (`coop_payoff`) using the game parameters `n`, `k`, and the observed `prev_coop_count`.
	* If `coop_payoff` is greater than a threshold value (see below), play Cooperate (C); otherwise, play Defect (D).
3. **Threshold Calculation**: The threshold value (`threshold`) is a function of the game parameters and the player's historical performance:
	+ Initialize `threshold = 1 / k`
	+ Update `threshold` after each round based on the player's total payoff: if the player's total payoff exceeds the average payoff, decrease `threshold` by a small amount (e.g., 0.05); otherwise, increase it.
4. **Edge Cases**:
	* If only one round remains (`r == 1`), play Defect (D) to maximize final-round payoff.
	* If all opponents have defected in the previous round (`prev_coop_count == 0`), play Cooperate (C) to attempt to induce cooperation.

Pseudocode for the Adaptive Exploiter strategy:
```markdown
def adaptive_exploiter(n, k, r, history):
    if r == 1:  # last round
        return Defect

    prev_coop_count = sum(history[-1])  # count cooperators in previous round
    coop_payoff = (k / n) * prev_coop_count + (n - prev_coop_count)

    threshold = 1 / k
    if history:
        total_payoff = sum(sum(round_history) for round_history in history)
        avg_payoff = total_payoff / r
        if total_payoff > avg_payoff:
            threshold -= 0.05
        else:
            threshold += 0.05

    if coop_payoff > threshold or (prev_coop_count == 0 and r > 1):
        return Cooperate
    else:
        return Defect
```
**Exploitative Mindset**: The Adaptive Exploiter strategy is designed to exploit the opponent's strategies by:

* Probing their behavior in the first round
* Adapting to changes in their cooperation levels over time
* Using historical data to inform decision-making and adjust its own strategy

By leveraging game theory principles, history-based observations, and adaptive threshold updates, this strategy aims to maximize its payoff while minimizing the opponents' payoffs.
'''

description_EXPLOITATIVE_499 = '''
**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to gather information about opponents' behavior and potential cooperation.
2. **Consecutive Cooperation:** If a player observes that all opponents cooperated in the previous round, they will also cooperate (C) in the current round. This encourages continued cooperation and maximizes public goods.
3. **Mixed Behavior:** For rounds where not all opponents cooperated:
	* Calculate the average cooperation rate of opponents over the last `m` rounds (`m` is a hyperparameter, e.g., 5). Call this value `avg_coop_rate`.
	* If `avg_coop_rate` is above a certain threshold (`coop_threshold`, e.g., 0.6), cooperate (C) in the current round.
	* Otherwise, defect (D).
4. **Exploiting Defectors:** If an opponent has defected in the previous round and their average cooperation rate over the last `m` rounds is below a certain threshold (`defect_threshold`, e.g., 0.3), defect (D) in the current round.
5. **Last Round:** In the final round, always defect (D) to maximize personal payoff.

**Edge Cases:**

* If there are only two players, cooperate (C) in all rounds except the last one, where you should defect (D).
* If an opponent's behavior is entirely random or unresponsive, default to the "Mixed Behavior" rule.
* In cases of tied average cooperation rates among opponents, prioritize cooperating with those who have cooperated more recently.

**Pseudocode:**
```markdown
# Initialize variables
coop_threshold = 0.6
defect_threshold = 0.3
m = 5  # number of rounds to consider for avg_coop_rate

# Main loop
for t in range(r):
    if t == 0:
        action = C  # Cooperate in the first round
    else:
        opponents_coop_rates = []
        for opponent in opponents:
            coop_rate = calculate_avg_coop_rate(opponent, m)
            opponents_coop_rates.append(coop_rate)

        all_cooperated = all(rate == 1 for rate in opponents_coop_rates)
        if all_cooperated:
            action = C
        else:
            avg_coop_rate = sum(opponents_coop_rates) / len(opponents_coop_rates)
            if avg_coop_rate >= coop_threshold:
                action = C
            elif any(rate < defect_threshold for rate in opponents_coop_rates):
                action = D  # Exploit defectors
            else:
                action = D  # Default to defecting

    # Last round exception
    if t == r - 1:
        action = D

    # Take action and observe payoffs
```
**Rationale:**

The Adaptive Exploiter strategy balances cooperation and exploitation. By initially cooperating, it encourages opponents to cooperate and gathers information about their behavior. The consecutive cooperation rule promotes continued cooperation among players who have demonstrated a willingness to work together.

The mixed behavior rule allows the strategy to adapt to changing opponent behaviors, while the exploiting defectors rule takes advantage of those who consistently defect. Finally, the last round exception ensures maximum personal payoff in the final round.

This strategy should perform well against a variety of opponents, as it is designed to be robust and adaptive, without relying on specific coordination mechanisms or norms.
'''

description_EXPLOITATIVE_500 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperation of other players while minimizing its own contributions to the public good.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: Play C (Cooperate) in the first three rounds to gather information about the opponents' behavior and estimate their cooperation rates.
2. **Exploitation Mode**: After the initial exploration phase, switch to exploitation mode. In each round, calculate the estimated average cooperation rate of opponents (ĉ) based on their past actions.
3. **Threshold-Based Cooperation**: Cooperate (C) if ĉ > k/n, where k is the multiplication factor and n is the number of players. Otherwise, defect (D).
4. **Punishment Mechanism**: If the opponent's cooperation rate drops below a certain threshold (e.g., 0.5), punish them by playing D for the next two rounds to deter future defection.
5. **Adaptive Adjustment**: Every five rounds, reassess the estimated average cooperation rate and adjust the strategy accordingly.

**Edge Cases:**

1. **First Round**: Play C to gather initial information about opponents' behavior.
2. **Last Round**: Play D, as there is no future benefit from cooperating in the final round.
3. **Opponent Defection**: If an opponent defects in a round where you cooperated, punish them by playing D for the next two rounds.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initial exploration phase
    cooperate = True
    for t in range(1, 4):  # Rounds 1-3
        action[t] = 'C'
    
    # Exploitation mode
    for t in range(4, r+1):
        estimated_average_cooperation_rate = calculate_ĉ(n, k, actions)
        if estimated_average_cooperation_rate > k/n:
            cooperate = True
        else:
            cooperate = False
        
        action[t] = 'C' if cooperate else 'D'
        
        # Punishment mechanism
        if opponent_cooperation_rate < 0.5:
            punish_opponent(t, 2)
    
    return actions

def calculate_ĉ(n, k, actions):
    # Calculate estimated average cooperation rate based on past actions
    ĉ = sum(actions[:t-1] == 'C') / (n * t)
    return ĉ

def punish_opponent(t, rounds):
    # Punish opponent by playing D for the next two rounds
    action[t:t+rounds] = ['D'] * rounds
```
**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to exploit the cooperation of other players while minimizing its own contributions to the public good. By initially cooperating and then adjusting its behavior based on the estimated average cooperation rate, it aims to maximize its payoff by taking advantage of opponents' cooperative tendencies. The punishment mechanism helps deter future defection and maintains a high level of exploitation.
'''

description_EXPLOITATIVE_501 = '''
**Strategy Name: Adaptive Exploiter**

**Overview**
The Adaptive Exploiter strategy is designed to exploit opponents' behavior while adapting to changes in their actions over time. It uses a combination of observation, adaptation, and manipulation to maximize its payoff.

**Decision Rules**

1. **Initial Round**: In the first round, defect (play D). This allows us to observe our opponents' initial behaviors without committing to cooperation.
2. **Observation Phase**: For rounds 2 to r-1:
	* Observe the number of cooperators (C) and defectors (D) in the previous round.
	* Calculate the average payoff for cooperators and defectors in the previous round.
3. **Adaptive Cooperation**: If the average payoff for cooperators is greater than or equal to the average payoff for defectors, cooperate (play C). Otherwise, defect (play D).
4. **Exploitation**: If a player has defected in the previous round, and their payoff was higher than the average cooperator's payoff, exploit them by cooperating (playing C) only if they cooperated in the previous round.
5. **Punishment**: If a player has exploited us in the previous round (i.e., we cooperated, and they defected), punish them by defecting (playing D) for one round.

**Edge Cases**

* **Last Round**: In the final round, always defect (play D). This ensures that we maximize our payoff without worrying about future interactions.
* **Ties**: If there's a tie in average payoffs between cooperators and defectors, cooperate (play C) to encourage cooperation from other players.

**Pseudocode**
```python
def adaptive_exploiter(n, k, r, history):
    # Initialize variables
    num_cooperators = 0
    num_defectors = 0
    avg_payoff_coop = 0
    avg_payoff_defect = 0

    # Initial Round: Defect
    if len(history) == 0:
        return 'D'

    # Observation Phase
    for i in range(1, len(history)):
        num_cooperators += history[i]['C']
        num_defectors += history[i]['D']
        avg_payoff_coop += history[i]['payoffs']['C'] / (i + 1)
        avg_payoff_defect += history[i]['payoffs']['D'] / (i + 1)

    # Adaptive Cooperation
    if avg_payoff_coop >= avg_payoff_defect:
        return 'C'
    else:
        return 'D'

    # Exploitation and Punishment
    for player in range(1, n+1):
        if history[-1][player]['action'] == 'D' and history[-1][player]['payoff'] > avg_payoff_coop:
            if history[-2][player]['action'] == 'C':
                return 'C'
        elif history[-1][player]['action'] == 'C' and history[-1][player]['payoff'] < avg_payoff_defect:
            return 'D'

    # Last Round: Defect
    if len(history) == r - 1:
        return 'D'

    # Ties: Cooperate
    if avg_payoff_coop == avg_payoff_defect:
        return 'C'
```
This strategy is designed to adapt to various opponent behaviors while exploiting their actions to maximize our payoff. By observing and reacting to the game's history, we can identify opportunities to cooperate or defect, ultimately leading to a higher total payoff.
'''

description_EXPLOITATIVE_502 = '''
Here's a strategic approach for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Mindset:** AE aims to exploit the cooperation of others while minimizing its own contributions, adapting to the evolving game dynamics.

**Decision Rules:**

1. **Initial Round (t=1):** Play C (Cooperate) with a probability p_init = 0.5. This initial randomization helps to avoid immediate exploitation and gathers information about opponents' strategies.
2. **Subsequent Rounds (t > 1):**
	* Observe the previous round's cooperation rate, cr_t-1 = Σ(j=1 to n) c_j / n.
	* Calculate the average payoff for cooperators in the previous round: avg_payoff_coop_t-1 = (k/n) × cr_t-1 × n + 0 (since they contributed).
	* Calculate the average payoff for defectors in the previous round: avg_payoff_defect_t-1 = 1 + (k/n) × cr_t-1 × n.
	* If avg_payoff_coop_t-1 > avg_payoff_defect_t-1, play C with probability p_coop = max(0.5, min(1, avg_payoff_coop_t-1 / (avg_payoff_coop_t-1 + avg_payoff_defect_t-1))).
	* Otherwise, play D.
3. **Last Round (t=r):** Play D (Defect) to maximize individual payoff.

**Edge Cases:**

* If only one player remains in the game (n=1), always play D.
* If there are multiple players with the same average payoff (e.g., all defected or all cooperated), play C with probability p_coop = 0.5.

**Pseudocode:**
```
def adaptive_exploiter(n, k, r):
  # Initialize variables
  cr_t_minus_1 = None
  avg_payoff_coop_t_minus_1 = None
  avg_payoff_defect_t_minus_1 = None

  for t in range(1, r+1):
    if t == 1:
      # Initial round
      action = random_choice([C, D], [0.5, 0.5])
    else:
      # Subsequent rounds
      cr_t_minus_1 = calculate_cooperation_rate(cr_t_minus_1)
      avg_payoff_coop_t_minus_1 = calculate_avg_payoff_coop(avg_payoff_coop_t_minus_1)
      avg_payoff_defect_t_minus_1 = calculate_avg_payoff_defect(avg_payoff_defect_t_minus_1)

      if avg_payoff_coop_t_minus_1 > avg_payoff_defect_t_minus_1:
        p_coop = max(0.5, min(1, avg_payoff_coop_t_minus_1 / (avg_payoff_coop_t_minus_1 + avg_payoff_defect_t_minus_1)))
        action = random_choice([C, D], [p_coop, 1-p_coop])
      else:
        action = D

    if t == r:
      # Last round
      action = D

    return action
```
**Explanation:**

The Adaptive Exploiter (AE) strategy aims to balance exploitation with cooperation. By initially randomizing its actions, AE gathers information about opponents' strategies and adapts to the evolving game dynamics.

In subsequent rounds, AE calculates the average payoffs for cooperators and defectors in the previous round. If cooperating is more profitable, AE increases its probability of cooperating; otherwise, it defects. This adaptive approach allows AE to exploit cooperative opponents while minimizing its own contributions.

By playing C with a higher probability when cooperation is more profitable, AE encourages others to cooperate, which can lead to increased payoffs for all players. However, by ultimately defecting in the last round, AE ensures that it maximizes its individual payoff.
'''

description_EXPLOITATIVE_503 = '''
I'll design a strategy that adapts to various opponent behaviors while exploiting their actions to maximize my payoff.

**Strategy Name:** "Exploitative Adaptive Tit-for-Tat" (EATT)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to gather information about opponents' initial behavior and set a positive tone.
2. **Subsequent Rounds**: Observe the number of cooperators (c_j) and defectors (d_j) in the previous round.
	* If c_j ≥ n/2, Cooperate (C) in this round. This indicates that at least half of the players are cooperating, so contributing to the public good is beneficial.
	* If d_j > n/2, Defect (D) in this round. When most players defect, it's better to prioritize individual payoff over the public good.
	* If c_j < n/2 and d_j ≤ n/2, use a "Tit-for-Tat" approach: Cooperate if at least one opponent cooperated in the previous round; otherwise, Defect. This balances exploration and exploitation while adapting to opponents' behavior.
3. **Adaptation**: Update the decision rules based on observed opponent behavior:
	* If an opponent's cooperation rate is above 0.5 (i.e., they cooperate more than half the time), increase the threshold for cooperating from n/2 to (n-1)/2. This allows us to take advantage of generous opponents.
	* If an opponent's defection rate is above 0.75 (i.e., they defect more than three-quarters of the time), decrease the threshold for cooperating from n/2 to (n+1)/2. This helps avoid exploitation by persistent defectors.

**Edge Cases:**

* **Last Round**: Cooperate if at least one opponent has cooperated in any previous round; otherwise, Defect. This ensures we don't miss opportunities for mutual benefit and limits the risk of being exploited.
* **Tie-breaking**: When multiple opponents have the same cooperation/defection rate, prioritize cooperating with those who have contributed more to the public good.

**Pseudocode:**
```python
def EATT(n, k, r):
    # Initialize variables
    cooperate_count = [0] * n  # Count of cooperators in each round
    defect_count = [0] * n     # Count of defectors in each round
    opponent_coop_rates = [0.5] * n  # Initial cooperation rates for opponents

    # First Round: Cooperate
    action = "C"

    # Subsequent Rounds
    for t in range(1, r):
        # Observe previous round's actions
        prev_cooperate_count = cooperate_count[t-1]
        prev_defect_count = defect_count[t-1]

        # Update opponent cooperation rates
        for i in range(n):
            if prev_action[i] == "C":
                opponent_coop_rates[i] += 0.1
            else:
                opponent_coop_rates[i] -= 0.1

        # Decide action based on decision rules
        if prev_cooperate_count >= n/2:
            action = "C"
        elif prev_defect_count > n/2:
            action = "D"
        else:
            # Tit-for-Tat approach
            if any(prev_action[i] == "C" for i in range(n)):
                action = "C"
            else:
                action = "D"

        # Adapt decision rules based on opponent behavior
        for i in range(n):
            if opponent_coop_rates[i] > 0.5:
                cooperate_threshold -= 1/n
            elif opponent_coop_rates[i] < 0.25:
                cooperate_threshold += 1/n

    return action
```
**Exploitative Mindset:** EATT is designed to adapt to various opponent behaviors while prioritizing individual payoff maximization. By initially cooperating and adjusting decision rules based on observed behavior, we aim to take advantage of generous opponents while avoiding exploitation by persistent defectors.

This strategy will be competitive in a tournament setting as it balances exploration and exploitation, adapting to the dynamics of the game and its opponents.
'''

description_EXPLOITATIVE_504 = '''
Here's a exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage others to contribute and gather information about their behavior.
2. **Early Rounds (2 ≤ t ≤ r/2):**
	* If the average cooperation rate of other players is high (> 0.5), Defect (D) to exploit their contributions.
	* Otherwise, Cooperate (C) to try to increase the total contribution and potentially benefit from others' future cooperation.
3. **Mid-Game (r/2 < t ≤ 3r/4):**
	* If your current payoff is below the average payoff of other players, Defect (D) to catch up.
	* Otherwise, Cooperate (C) if the total contribution in the previous round was high (> n/2), and Defect (D) otherwise.
4. **Late Rounds (3r/4 < t ≤ r):**
	* If your current payoff is above the average payoff of other players, Cooperate (C) to maintain a good reputation and potentially encourage others to cooperate in the final rounds.
	* Otherwise, Defect (D) to maximize your payoff in the remaining rounds.

**Edge Cases:**

1. **First Round:** Always Cooperate (C).
2. **Last Round (t=r):** Defect (D), as there is no future benefit from cooperating.
3. **Ties:** In case of a tie in payoffs or cooperation rates, default to the more exploitative action (Defect).

**Pseudocode:**
```
Input: game parameters (n, r, k), current round (t), history of actions and payoffs

if t == 1:
    return C  // Initial Round: Cooperate
elif 2 <= t <= r/2:
    avg_coop_rate = calculate_average_cooperation_rate(history)
    if avg_coop_rate > 0.5:
        return D  // Exploit high cooperation rate
    else:
        return C  // Encourage contributions
elif r/2 < t <= 3r/4:
    my_payoff = calculate_my_payoff(history)
    avg_payoff = calculate_average_payoff(history)
    if my_payoff < avg_payoff:
        return D  // Catch up with Defect
    else:
        prev_total_contribution = get_previous_round_total_contribution(history)
        if prev_total_contribution > n/2:
            return C  // Cooperate on high total contribution
        else:
            return D  // Defect otherwise
elif 3r/4 < t <= r:
    my_payoff = calculate_my_payoff(history)
    avg_payoff = calculate_average_payoff(history)
    if my_payoff > avg_payoff:
        return C  // Maintain good reputation with Cooperate
    else:
        return D  // Maximize payoff with Defect

def calculate_average_cooperation_rate(history):
    # Calculate average cooperation rate of other players
    ...

def calculate_my_payoff(history):
    # Calculate your current payoff
    ...

def calculate_average_payoff(history):
    # Calculate average payoff of other players
    ...

def get_previous_round_total_contribution(history):
    # Get total contribution in the previous round
    ...
```
This strategy is designed to be adaptive and exploitative, taking into account the game's history and parameters. It aims to maximize its own payoff while responding to various opponent behaviors.
'''

description_EXPLOITATIVE_505 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy aims to exploit opponents' behavior by adapting to their cooperation levels and timing its defections to maximize payoffs.

**Decision Rules:**

1. **Initial Exploration (Round 1-2)**:
	* Cooperate in the first two rounds to gather information about opponents' initial tendencies.
	* Observe the total number of cooperators (C) and defectors (D) in each round.
2. **Adaptive Cooperation Threshold**:
	* After Round 2, calculate a cooperation threshold (CT) based on the observed behavior:
		+ CT = (Number of C in Round 1 + Number of C in Round 2) / (2 \* n)
	* In subsequent rounds, cooperate if the expected number of cooperators (based on opponents' previous actions) is above the CT.
3. **Predatory Defection**:
	* If the expected number of cooperators is below the CT, defect to exploit the public good.

Pseudocode for the Adaptive Predator strategy:

```
// Initialize variables
CT = 0 // Cooperation Threshold
C_1 = 0 // Number of C in Round 1
C_2 = 0 // Number of C in Round 2

// Initial Exploration (Round 1-2)
for t = 1 to 2:
    action[t] = Cooperate
    observe opponents' actions
    if t == 1:
        C_1 = count(C)
    else:
        C_2 = count(C)

// Calculate Cooperation Threshold
CT = (C_1 + C_2) / (2 * n)

// Adaptive Cooperation and Predatory Defection
for t = 3 to r:
    expected_C = estimate number of cooperators based on opponents' previous actions
    if expected_C >= CT:
        action[t] = Cooperate
    else:
        action[t] = Defect

// Last Round (t = r)
action[r] = Defect // always defect in the last round to maximize payoff
```

**Handling Edge Cases:**

* **First Round:** Cooperate to gather information about opponents' initial tendencies.
* **Last Round:** Always defect to maximize payoffs, as there is no future interaction.
* **Ties:** In case of a tie (expected_C == CT), cooperate to maintain a slightly cooperative image and encourage opponents to cooperate.

**Exploitative Mindset:**

The Adaptive Predator strategy prioritizes maximizing its own payoff by adapting to the cooperation levels of its opponents. By cooperating initially, it gathers information about opponents' behavior and identifies opportunities to exploit the public good through predatory defection. This strategy is designed to take advantage of cooperative opponents while minimizing losses against defectors.

This strategy will perform well in a tournament setting as it:

1. Gathers information about opponents' behavior
2. Adapts to changing cooperation levels
3. Exploits public goods when possible
4. Maintains a slightly cooperative image to encourage opponents to cooperate

The Adaptive Predator strategy is robust and can handle a wide range of opponent behaviors, making it a formidable competitor in the N-Player Public Goods Game tournament.
'''

description_EXPLOITATIVE_506 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**: Cooperate (C) to gather information about opponents' behavior and estimate their cooperation rates.
2. **Adaptive Phase**: After Round 3, use the following decision rules:
	* If the average cooperation rate of opponents in the last round is high (> 0.5), Defect (D). This indicates that opponents are likely cooperating, allowing us to exploit them.
	* If the average cooperation rate of opponents in the last round is low (< 0.5), Cooperate (C). This suggests that opponents are mostly defecting, and we can gain from contributing to the public good.
	* If the average cooperation rate of opponents in the last round is around 0.5 (± 0.1), use a mixed strategy: Cooperate with probability 0.6 and Defect with probability 0.4. This creates uncertainty for opponents and prevents them from easily adapting to our behavior.
3. **Punishment Mechanism**: If an opponent defects in a round where the average cooperation rate is high (> 0.5), mark that opponent as "exploitable." In subsequent rounds, Defect against these marked opponents with probability 0.8.
4. **Last Round (Round r)**: Always Defect, as there's no future opportunity to punish or be punished.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r, history):
    # Initialize variables
    avg_coop_rate = None
    exploitable_opponents = set()

    for t in range(1, r+1):
        if t <= 3:
            action = 'C'  # Initial exploration phase
        else:
            # Calculate average cooperation rate of opponents in last round
            avg_coop_rate = sum(history[t-1]['coop_rates']) / n

            if avg_coop_rate > 0.5:
                action = 'D'  # Exploit high cooperation rates
            elif avg_coop_rate < 0.5:
                action = 'C'  # Cooperate when opponents are mostly defecting
            else:
                action = random.choice(['C', 'D'], p=[0.6, 0.4])  # Mixed strategy

        # Punishment mechanism
        if t > 1 and history[t-1]['avg_coop_rate'] > 0.5:
            for i in range(n):
                if history[t-1]['actions'][i] == 'D':
                    exploitable_opponents.add(i)

        if t < r:  # Not the last round
            if action == 'D' and random.random() < 0.8 and len(exploitable_opponents) > 0:
                opponent = random.choice(list(exploitable_opponents))
                history[t]['actions'][opponent] = 'D'

        yield action

    # Last round: Always Defect
    yield 'D'
```
This strategy is designed to be adaptive, robust, and exploitative. By initially exploring the opponents' behavior, we can estimate their cooperation rates and adjust our actions accordingly. The punishment mechanism helps to deter opponents from exploiting us in subsequent rounds.
'''

description_EXPLOITATIVE_507 = '''
**Exploitative Strategy: "Adaptive Free Rider"**

This strategy aims to maximize individual payoffs by exploiting the cooperative behavior of others while minimizing personal contributions.

**Decision Rules:**

1. **Initial Exploration (Round 1-3)**:
	* Play C (Cooperate) with a probability of 0.5 in the first three rounds to gather information about the opponents' behavior.
2. **Observation and Adaptation**:
	* After Round 3, observe the average cooperation rate of all players over the last 2 rounds.
	* If the average cooperation rate is above 0.5, play D (Defect) with a probability of 0.7 in the next round.
	* If the average cooperation rate is below 0.5, play C (Cooperate) with a probability of 0.3 in the next round.
3. **Exploitation**:
	* If the opponent's cooperation rate is consistently high (> 0.8), play D (Defect) to maximize personal payoffs.
4. **Punishment and Reset**:
	* If an opponent defects after cooperating, punish them by playing D (Defect) for 2 rounds.
	* After punishing, reset the observation period and adapt again.

**Handling Edge Cases:**

1. **First Round**: Play C (Cooperate) with a probability of 0.5 to gather information.
2. **Last Round**: Play D (Defect) to maximize personal payoffs, as there is no future round to worry about.
3. **Opponent Defection**: Punish opponents who defect after cooperating by playing D (Defect) for 2 rounds.

**Pseudocode:**
```
// Initialize variables
avg_coop_rate = 0
punishment_rounds = 0

// Round 1-3: Initial Exploration
if round <= 3:
    play C with probability 0.5

// Observation and Adaptation
else if round > 3:
    avg_coop_rate = calculate_avg_coop_rate(opponents, last_2_rounds)
    if avg_coop_rate > 0.5:
        play D with probability 0.7
    else:
        play C with probability 0.3

// Exploitation
if opponent_coop_rate > 0.8:
    play D

// Punishment and Reset
if opponent_defected_after_cooperating:
    punishment_rounds = 2
    while punishment_rounds > 0:
        play D
        punishment_rounds -= 1
```
This strategy is designed to be adaptive, exploitative, and robust against various opponent behaviors. It gathers information in the initial rounds, adapts to the opponents' behavior, exploits high cooperation rates, and punishes defectors to maintain a balance between personal payoffs and social welfare.
'''

description_EXPLOITATIVE_508 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics. This strategy focuses on maximizing its own payoff by identifying and responding to cooperative and defecting patterns in the opponents' actions.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
Cooperate (C) with probability 0.5 in each of the first three rounds. This allows for exploration of opponents' strategies while minimizing potential losses.
2. **Pattern Detection (Round 4 onwards)**:
Calculate the cooperation rate of all opponents over the last two rounds:

   `coop_rate = (total_coop / (n * 2))`

where `total_coop` is the total number of cooperations observed in the last two rounds.

* If `coop_rate > 0.5`, cooperate (C) with probability `(k/n) * coop_rate`. This encourages cooperation when opponents are cooperative.
* If `coop_rate <= 0.5`, defect (D) with probability `1 - ((k/n) * coop_rate)`. This exploits opponents' defections.

**Edge Cases:**

1. **Last Round (Round r)**:
Defect (D). Since there are no future rounds to consider, maximizing the current payoff is optimal.
2. **Tie-breaking ( identical cooperation rates)**:
In cases where multiple strategies have the same cooperation rate, prioritize cooperating with opponents who have a higher historical cooperation rate.

**Pseudocode:**
```python
def adaptive_predator(n, k, r):
  # Initialize variables
  coop_rate = 0
  total_coop = 0

  for t in range(1, r+1):
    if t <= 3:
      # Initial exploration phase
      action = random.choice([C, D]) if random.random() < 0.5 else C
    else:
      # Pattern detection and response
      coop_rate = (total_coop / (n * 2))
      if coop_rate > 0.5:
        action = C if random.random() < (k/n) * coop_rate else D
      else:
        action = D if random.random() < 1 - ((k/n) * coop_rate) else C

    # Update total_coop for next round's calculation
    total_coop += sum(opponent_actions)

    # Take action and receive payoff
    pi = (1 - c_i) + (k/n) * sum(opponent_c_i)
    ...

  return total_payoff
```
The Adaptive Predator strategy is designed to be robust against a wide range of opponent behaviors, from pure cooperation to pure defection. By adapting to the game's dynamics and responding to patterns in opponents' actions, this strategy aims to maximize its own payoff while exploiting opportunities for gain.
'''

description_EXPLOITATIVE_509 = '''
**Exploitative Strategy: "Adaptive Defector with Conditional Cooperation"**

This strategy aims to exploit the opponent's behavior while being robust to various playing styles.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to gather information about opponents' initial tendencies.
2. **Non-Initial Rounds (t>1):**
	* If the total number of cooperators in the previous round is greater than or equal to the multiplication factor `k`, Defect (D).
	* Otherwise, Cooperate (C).
3. **Last Round (t=r):** Always Defect (D), as there's no future interaction to consider.

Pseudocode:
```
if t == 1:  # Initial round
    action = C
else:
    prev_cooperators = count(cooperators in previous round)
    if prev_cooperators >= k:
        action = D
    else:
        action = C

if t == r:  # Last round
    action = D
```
**Rationale:**

* By cooperating in the initial round, we gather information about opponents' tendencies and set a baseline for future interactions.
* In subsequent rounds, we defect if the total number of cooperators is high enough to produce a substantial public good benefit. This allows us to exploit opponents who consistently cooperate.
* If the number of cooperators is low, we cooperate to contribute to the public good and potentially induce others to cooperate in the next round.
* In the last round, we always defect, as there's no future interaction to consider, and our goal is to maximize our individual payoff.

**Edge Cases:**

* If an opponent consistently defects, our strategy will adapt by cooperating less frequently, reducing the public good benefit for that opponent.
* If an opponent consistently cooperates, our strategy will exploit this by defecting more frequently, maximizing our own payoff.

This adaptive strategy balances exploitation with a degree of cooperation, making it robust to various opponent behaviors. By responding to the previous round's outcomes and adjusting our actions accordingly, we can effectively navigate the game's dynamics and achieve a high total payoff.
'''

description_EXPLOITATIVE_510 = '''
**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) with a probability of 50% to gather information about opponents' behaviors.
	* Observe and record the number of cooperators in each round.
2. **Adaptive Phase (Rounds 4-r)**:
	* If the average number of cooperators in the previous three rounds is greater than or equal to `n/2`, cooperate (C) with a probability of `k/n`.
	* Otherwise, defect (D).
3. **Endgame Exploitation (Last Round)**:
	* Defect (D), as there's no future round to consider.

**Handling Edge Cases:**

1. **First Round**: Cooperate (C) with 50% probability.
2. **Last Round**: Always defect (D).
3. **Opponent Cooperation**: If an opponent cooperates in a previous round, increase the probability of cooperation for that player by 10% in the next round (up to a maximum of `k/n`).
4. **Opponent Defection**: If an opponent defects in a previous round, decrease the probability of cooperation for that player by 10% in the next round (down to a minimum of 0%).

**Pseudocode:**
```markdown
# Initialize variables
n = number_of_players
k = multiplication_factor
r = number_of_rounds
cooperation_probabilities = [0.5] * n

# Initial Exploration Phase (Rounds 1-3)
for round in range(1, 4):
    cooperate_with_probability(cooperation_probabilities[round % n])

# Adaptive Phase (Rounds 4-r)
for round in range(4, r+1):
    previous_cooperators = count_cooperators_in_previous_rounds()
    if previous_cooperators >= n / 2:
        cooperation_probability = k / n
    else:
        cooperation_probability = 0
    cooperate_with_probability(cooperation_probability)

# Endgame Exploitation (Last Round)
defect()

# Opponent Cooperation/Defection Handling
for opponent in range(n):
    if opponent_cooperated_last_round(opponent):
        increase_cooperation_probability(opponent, 0.1)
    elif opponent_defected_last_round(opponent):
        decrease_cooperation_probability(opponent, 0.1)
```
This strategy aims to balance exploration and exploitation by:

1. Initially exploring opponents' behaviors with a random cooperation probability.
2. Adapting to the average number of cooperators in previous rounds, exploiting situations where cooperation is more beneficial.
3. Exploiting the last round by defecting, as there's no future round to consider.

By being adaptive and responsive to opponent behaviors, this strategy should perform well against a wide range of independent strategies in a tournament setting.
'''

description_EXPLOITATIVE_511 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperation levels of other players while adapting to their behavior over time.

**Decision Rules:**

1. **Initial Round:** Defect (D) in the first round to gather information about others' initial strategies.
2. **Cooperation Threshold:** Calculate a cooperation threshold (CT) based on the game's history:
   - CT = (k/n) \* (average number of cooperators in previous rounds)
3. **Adaptive Cooperation:** Cooperate (C) if the current round's expected payoff from cooperation is higher than the private payoff from defecting, i.e., if:
   - (k/n) \* CT > 1
4. **Exploitation Mode:** If the cooperation threshold is low (i.e., others are mostly defecting), switch to exploitation mode:
   - Defect (D) in this round and subsequent rounds until the cooperation threshold increases.
5. **Punishment Mechanism:** Implement a punishment mechanism to deter opponents from exploiting the Adaptive Exploiter's cooperation:
   - If an opponent defects after the Adaptive Exploiter has cooperated, defect against that opponent for a fixed number of rounds (e.g., 2-3 rounds).

**Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round to maximize the total payoff, as there is no future exploitation.
2. **High Cooperation Levels:** If cooperation levels are consistently high (> 80%), consider cooperating more frequently to maintain a good reputation and avoid punishment.

**Pseudocode:**
```python
def adaptive_exploiter(game_history):
    # Initialize variables
    CT = 0
    exploit_mode = False

    for round in game_history:
        # Calculate cooperation threshold (CT)
        CT = (k/n) * average_cooperators(round)

        if not exploit_mode and CT > 1 / (k/n):
            cooperate()
        elif not exploit_mode and CT <= 1 / (k/n):
            defect()
            exploit_mode = True
        elif exploit_mode:
            # Punishment mechanism
            if opponent_defected():
                defect_against_opponent()
            else:
                cooperate()

    return action
```
The Adaptive Exploiter strategy balances cooperation and exploitation to maximize its payoff in the N-Player Public Goods Game. By adapting to the game's history and opponents' behavior, this strategy can effectively exploit cooperation levels while maintaining a robust defense against exploitation attempts by other players.
'''

description_EXPLOITATIVE_512 = '''
I'll design a strategic approach for this N-Player Public Goods Game that focuses on exploiting opponents while adapting to their behaviors over time. This strategy will be robust and effective across various opponent strategies.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):**
   - Cooperate (C) in the first round to gather information about the opponents' initial tendencies.
   - Observe the actions of all players.

2. **Subsequent Rounds (t > 1):**
   - **Exploitation Phase:** If the total payoff from cooperation (public good share) is greater than or equal to the private payoff (keeping endowment), cooperate (C). Otherwise, defect (D).
   - To calculate this:
     ```python
    if (k/n) * total_cooperators >= 1:
        action = C
    else:
        action = D
    ```
   - **Adaptive Adjustment:** Monitor the success of opponents' strategies over time. If an opponent's average payoff is significantly higher than yours, consider mirroring their strategy for a few rounds to understand and exploit their approach.
     ```python
    if opponent_payoff > my_avg_payoff + threshold:
        mirror_opponent_strategy = True
    ```
   - **Punishment Mechanism:** Implement a punishment mechanism to deter repeated defection by opponents. If an opponent defects in two consecutive rounds, punish them by defecting as well for the next round.
     ```python
    if opponent_defected_last_two_rounds:
        action = D
    ```

3. **Final Round (t=r):**
   - Defect (D) to maximize personal gain since there are no future consequences.

**Edge Cases:**

- **First Round with Few Cooperators:** If the number of cooperators is less than a certain threshold, defect in subsequent rounds until cooperation becomes profitable again.
  ```python
if total_cooperators < threshold:
    action = D
```
- **Last Few Rounds:** Gradually shift towards defection as the game nears its end to ensure maximum personal gain.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    total_cooperators = 0
    my_payoff_history = []
    opponent_payoffs = []

    for t in range(1, r+1):
        if t == 1:
            action = C  # Cooperate in the first round
        else:
            # Calculate public good share and private payoff
            public_good_share = (k/n) * total_cooperators
            private_payoff = 1 - (action == C)

            # Determine action based on decision rules
            if public_good_share >= private_payoff:
                action = C
            else:
                action = D

        # Update variables and history
        total_cooperators += (action == C)
        my_payoff_history.append(calculate_my_payoff(action, total_cooperators))
        opponent_payoffs.append(calculate_opponent_payoff(opponent_action))

        # Adaptive adjustment and punishment mechanism
        if opponent_payoff > my_avg_payoff + threshold:
            mirror_opponent_strategy = True

    return my_payoff_history
```
This strategy balances cooperation with exploitation, adapting to opponents' behaviors over time while ensuring a strong finish in the final rounds. By combining elements of punishment mechanisms, mirroring successful strategies, and conditionally cooperating or defecting based on public good share, AE aims to outperform a wide range of opponent strategies.
'''

description_COLLECTIVE_513 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors:

**Strategy Name:** "K-Trigger"

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to encourage cooperation and gather information about opponents' behavior.
2. **Subsequent Rounds:** Calculate a trigger value `t` based on the number of cooperators in the previous round (`c_prev`) and the multiplication factor `k`. Set `t = k * c_prev / n`.
3. **Cooperate if:** The total payoff from cooperation (i.e., `k/n` times the total contributions) is greater than or equal to `t` in the previous round.
4. **Defect otherwise:** If the condition above is not met, defect (D).

**Pseudocode:**
```python
def k_trigger(n, k, r, history):
    # Initialize cooperation flag and trigger value
    cooperate = True
    t = 0

    for round in range(1, r + 1):
        if round == 1:
            # First round: Cooperate
            action = 'C'
        else:
            # Calculate trigger value based on previous round's cooperators
            c_prev = sum(1 for h in history[round - 2] if h == 'C')
            t = k * c_prev / n

            # Check if cooperation is profitable
            cooperate_payoff = (k / n) * c_prev
            if cooperate_payoff >= t:
                action = 'C'
            else:
                action = 'D'

        # Update history and return action
        history.append([action] + [h for h in history[-1]])
        yield action
```
**Edge Cases:**

* **Last Round:** The strategy will adapt to the behavior of opponents in the second-to-last round, which should lead to a Nash equilibrium in the last round.
* **Zero Cooperators:** If there are no cooperators in the previous round (`c_prev = 0`), the trigger value `t` will be zero, and the strategy will cooperate in the next round.

**Collective Mindset:**
The K-Trigger strategy aligns with a collective mindset by:

* Encouraging cooperation in the first round to establish a norm.
* Adapting to the behavior of opponents to maintain cooperation when profitable.
* Defecting only when cooperation is not beneficial, which helps to avoid exploitation.

This strategy should be robust against various opponent behaviors and perform well in a tournament setting.
'''

description_COLLECTIVE_514 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Reciprocity (ACGR)**

**Decision Rules:**

1. **Initial Rounds:** Cooperate (C) for the first 2 rounds to establish a cooperative tone and gather information about opponents' behaviors.
2. **Reciprocity Mechanism:** Implement a gradual reciprocity mechanism based on the proportion of cooperators in the previous round:
	* If the proportion of cooperators is above a certain threshold (p_threshold = 0.5), cooperate (C) with a probability p_coop that increases gradually over rounds, starting from 0.6.
	* If the proportion of cooperators is below the threshold, defect (D).
3. **Adaptive Response to Defection:** Monitor opponents' actions and adjust the cooperation probability (p_coop) based on their recent behavior:
	* If an opponent defects (D) in the previous round, decrease p_coop by 0.1 for that opponent.
	* If an opponent cooperates (C) in the previous round, increase p_coop by 0.05 for that opponent, up to a maximum of 0.9.
4. **Punishment Mechanism:** Implement a mild punishment mechanism to deter excessive defection:
	* If an opponent defects (D) more than twice in a row, defect (D) against them with probability p_punish = 0.8 for the next round.

**Edge Cases:**

1. **First Round:** Cooperate (C) by default.
2. **Last Round:** Defect (D) to maximize individual payoff, as there is no future reciprocity.
3. **Ties:** In case of a tie in the proportion of cooperators, cooperate (C) with probability 0.5.

**Collective Mindset:**

ACGR prioritizes cooperation and reciprocity while adapting to opponents' behaviors. By initially cooperating and gradually increasing cooperation probability based on others' actions, ACGR encourages mutual cooperation. The adaptive response to defection and punishment mechanism help maintain a balance between individual payoff and collective well-being.

**Pseudocode:**
```
// Initialize variables
p_threshold = 0.5
p_coop_initial = 0.6
p_punish = 0.8

// First round
if (round == 1) {
    action = C // Cooperate by default
}

// Subsequent rounds
else {
    proportion_cooperators = count(C) / n
    if (proportion_cooperators > p_threshold) {
        p_coop = min(p_coop_initial + (0.05 * round), 0.9)
        action = random_choice([C, D], [p_coop, 1 - p_coop])
    } else {
        action = D
    }
    
    // Adaptive response to defection
    for each opponent {
        if (opponent_defected_last_round) {
            p_coop -= 0.1
        } else if (opponent_cooperated_last_round) {
            p_coop += 0.05
        }
    }
    
    // Punishment mechanism
    for each opponent {
        if (opponent_defected_twice_in_a_row) {
            action = D with probability p_punish
        }
    }
}

// Last round
if (round == r) {
    action = D
}
```
This strategy is designed to be robust, adaptive, and collective, allowing it to perform well in a tournament against independent strategies.
'''

description_COLLECTIVE_515 = '''
**Collective Strategy: Adaptive Cooperation with Retaliation (ACR)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds:** Observe the total number of cooperators (T) in the previous round.
	* If T ≥ n/2, cooperate (C). This reinforces cooperation when it's prevalent.
	* If T < n/2, defect (D). This discourages free-riding and encourages others to cooperate.
3. **Retaliation Mechanism:** Monitor the number of consecutive rounds where fewer than half of the players cooperated (F). If F ≥ 2, switch to a Defect (D) strategy for the next round. This punishes persistent non-cooperation and promotes cooperation.
4. **Forgiveness Mechanism:** After retaliating with a Defect (D), return to the main decision rule in step 2 if the number of cooperators increases.

**Pseudocode:**
```python
def ACR(n, k, r):
    T = 0  # Total cooperators in previous round
    F = 0  # Consecutive rounds with fewer than half cooperators

    for t in range(1, r+1):
        if t == 1:
            action = C  # Cooperate in the first round
        else:
            T_prev = T
            T = count_cooperators(T_prev)
            if T >= n/2:
                action = C
            elif T < n/2 and F < 2:
                action = D
                F += 1
            elif F >= 2:
                action = D
            else:
                action = C

        # Forgiveness mechanism
        if action == D and T > T_prev:
            F = 0

        take_action(action)
```
**Handling Edge Cases:**

* **Last Round:** Since the game is simultaneous-move, there's no need for a specific last-round strategy. The decision rule will adapt to the game state up until the final round.
* **Multiple Equilibria:** ACR can converge to different equilibria depending on the initial conditions and opponents' strategies. However, by adapting to the game state, ACR increases the chances of reaching a cooperative equilibrium.

**Collective Mindset:**

ACR is designed to promote cooperation while being robust to various opponent behaviors. By cooperating initially and retaliating against persistent non-cooperation, ACR aims to create an environment where cooperation becomes the dominant strategy. The forgiveness mechanism helps to re-establish cooperation after retaliation, ensuring that ACR remains adaptable and collective in its approach.

By playing ACR in a tournament against other AI systems, we can test its effectiveness in promoting cooperation and achieving better outcomes for all players involved.
'''

description_COLLECTIVE_516 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy, "Adaptive Cooperation," aims to balance individual self-interest with collective welfare by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds (t ≤ n):** Cooperate (C) in the first `n` rounds to establish a baseline of cooperation and gather information about other players' behaviors.
2. **Contribution-based Cooperation:** In subsequent rounds, cooperate if the total contributions from previous rounds are above a threshold (`τ`). Specifically:
	* Calculate the average contribution rate (`ρ`) over the past `min(r, n)` rounds: `ρ = Σ(t=1 to min(r, n)) Σ(j=1 to n) c_j / (n \* min(r, n))`
	* Set the cooperation threshold (`τ`) as a function of the multiplication factor (`k`): `τ = k / (2n)`
	* Cooperate if `ρ ≥ τ`, otherwise defect.
3. **Punishment Mechanism:** Implement a gentle punishment mechanism to deter repeated defectors:
	* Track the number of consecutive defections (`δ`) by each player over the past `min(r, n)` rounds
	* If a player has defected consecutively for more than `n/2` rounds, defect against them in the next round

**Edge Cases:**

1. **First Round:** Cooperate to establish a baseline of cooperation.
2. **Last Round (t = r):** Defect, as there are no future interactions to influence.
3. **Ties in Contribution Rate:** In case of ties, cooperate if the opponent has cooperated more frequently in previous rounds.

**Collective Mindset:**

Our strategy is designed to promote collective cooperation while adapting to various opponent behaviors. By contributing to the public good in initial rounds and basing subsequent decisions on contribution rates, we encourage other players to cooperate as well. The gentle punishment mechanism helps maintain a balance between cooperation and self-interest.

Pseudocode:
```markdown
# Initialize variables
ρ = 0 (average contribution rate)
τ = k / (2n) (cooperation threshold)
δ = [0] * n (consecutive defections by each player)

# Main loop
for t in range(1, r+1):
    if t ≤ n:  # Initial rounds
        cooperate = True
    else:
        ρ = calculate_average_contribution_rate(t)
        if ρ ≥ τ:
            cooperate = True
        else:
            cooperate = False
    
    # Punishment mechanism
    for i in range(n):
        if δ[i] > n/2:
            cooperate_against[i] = True

    # Play the game
    play(cooperate)

    # Update variables
    ρ = update_average_contribution_rate(t, ρ)
    δ = update_consecutive_defections(t, δ)
```
This strategy will be implemented as an algorithm and compete against other AI systems in a tournament.
'''

description_COLLECTIVE_517 = '''
**Collective Strategy: "Adaptive Cooperation with Punishment" (ACP)**

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a baseline of cooperation and encourage others to reciprocate.
2. **Early Rounds (t < r/3):** If the total number of cooperators in the previous round is above the threshold `n*k/(2*n)`, cooperate (C). Otherwise, defect (D).
3. **Middle Rounds (r/3 ≤ t < 2*r/3):**
	* If the average payoff for cooperators in the previous round is greater than or equal to the average payoff for defectors, cooperate (C).
	* Otherwise, defect (D) with a probability `p = (average_defector_payoff - average_cooperator_payoff) / (k-1)`. Cooperate (C) with probability `(1-p)`.
4. **Late Rounds (t ≥ 2*r/3):** If the total number of cooperators in the previous round is above the threshold `n*k/(2*n)`, cooperate (C). Otherwise, defect (D).
5. **Punishment Mechanism:** If a player defects while the collective cooperation level is high (`n*k/(2*n)` or more), punish them by defecting against them for `x` rounds, where `x` is a small integer (e.g., 2-3). This encourages players to maintain cooperation and deters exploitation.

**Edge Cases:**

* **Last Round:** Defect (D) to maximize individual payoff.
* **Ties in Payoff Averages:** Cooperate (C) if the average payoffs for cooperators and defectors are equal.

**Collective Mindset Alignment:**

ACP aims to create a collective mindset by:

1. Encouraging initial cooperation to establish a cooperative baseline.
2. Adapting to the group's behavior, rewarding cooperation when it is beneficial to the collective.
3. Punishing exploitation to maintain cooperation and deter free-riding.

**Rationale:**

ACP balances individual self-interest with collective well-being by adapting to the group's behavior. By cooperating in early rounds, we set a positive tone for the game. As the game progresses, ACP adjusts its strategy based on the group's performance, rewarding cooperation when it benefits the collective and punishing exploitation to maintain cooperation.

**Implementation Note:**

To implement ACP as an algorithm, you can use a simple state machine or a decision tree. Store the history of previous rounds' actions and payoffs to inform decisions in subsequent rounds.
'''

description_COLLECTIVE_518 = '''
**Collective Strategy: " Adaptive Reciprocity with Exploration"**

Our collective strategy aims to balance individual payoff maximization with promoting cooperation among all players. We'll adapt to the game's history and opponents' behaviors while maintaining a robust and fair approach.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) unconditionally, setting a cooperative tone for the game.
2. **Subsequent Rounds (t>1)**:
	* If the majority (> n/2) of players cooperated in the previous round, cooperate (C).
	* Otherwise, defect (D) with probability p = (k-1)/(n-k), and cooperate (C) with probability 1-p.
3. **Exploration Phase**: Every r/4 rounds, starting from t=2 (e.g., rounds 2, 6, 10, ...), play a "probe" round where you cooperate (C) unconditionally, regardless of the previous round's outcome. This phase allows us to gather information about opponents' strategies and adapt our approach.

**Pseudocode**
```
def strategy(n, k, r, history):
    if t == 1:
        return C
    else:
        majority_cooperated = sum(history[t-1]) > n/2
        if majority_cooperated:
            return C
        else:
            p = (k-1)/(n-k)
            return D with probability p, otherwise C

    # Exploration Phase
    if t % (r/4) == 0 and t != 1:
        return C
```
**Collective Mindset:**
Our strategy aligns with the collective mindset by:

* Initially cooperating to promote a cooperative atmosphere.
* Adapting to the majority's behavior, encouraging cooperation when it's prevalent.
* Incorporating an exploration phase to gather information and adapt to changing opponent behaviors.

By using this adaptive reciprocity approach, we aim to balance individual payoffs with promoting cooperation among all players, while being robust against various opponent strategies.
'''

description_COLLECTIVE_519 = '''
**Collective Strategy: Adaptive Cooperativity (AC)**

**Overview**
AC is a collective strategy designed for the N-Player Public Goods Game, focusing on adaptability and robustness to various opponent behaviors. It balances cooperation and defection based on game parameters and history.

**Decision Rules**

1. **First Round**: Cooperate (C) in the first round to establish a cooperative tone and gather information about opponents' initial actions.
2. **Subsequent Rounds**: For each subsequent round, calculate the average payoff of all players in the previous round (`avg_payoff_prev`).
	* If `avg_payoff_prev` ≥ `k/2`, Cooperate (C). This indicates that cooperation has been relatively successful, and we should continue to contribute.
	* Otherwise, Defect (D) with probability `p_defect = 1 - (avg_payoff_prev / k)`. This introduces a gradual shift towards defection when cooperation is not yielding sufficient payoffs.

Pseudocode:
```python
if round == 1:
    action = C
else:
    avg_payoff_prev = calculate_avg_payoff(prev_round)
    if avg_payoff_prev >= k/2:
        action = C
    else:
        p_defect = 1 - (avg_payoff_prev / k)
        if random() < p_defect:
            action = D
        else:
            action = C
```
**Edge Cases**

* **Last Round**: In the last round, Cooperate (C) to maximize the collective payoff and maintain a cooperative tone.
* **Opponent Defection**: If an opponent defects in a round, AC will adapt by increasing its defection probability (`p_defect`) in subsequent rounds.

**Collective Mindset**

AC prioritizes cooperation when it is likely to yield higher payoffs for all players. By adapting to the game's history and opponents' actions, AC promotes a collective mindset that balances individual self-interest with the greater good.

**Key Benefits**

1. **Adaptability**: AC responds to changes in opponent behavior and adjusts its strategy accordingly.
2. **Robustness**: The strategy is designed to perform well against various independent strategies.
3. **Collective focus**: AC prioritizes cooperation when it benefits all players, promoting a collective mindset.

By implementing the Adaptive Cooperativity (AC) strategy, we aim to achieve a balance between individual self-interest and collective success in the N-Player Public Goods Game.
'''

description_COLLECTIVE_520 = '''
**Collective Strategy: Adaptive Cooperativity with History-Based Adjustment (ACHA)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to encourage mutual cooperation and set a positive tone for the game.
2. **Reciprocity**: If the number of cooperators in the previous round is greater than or equal to half of the total players (n/2), cooperate (C) in the current round. This promotes continued cooperation when it's beneficial.
3. **Punishment and Forgiveness**: If the number of cooperators in the previous round is less than half of the total players (n/2) and the player's payoff was lower than the average payoff, defect (D) in the current round to punish free-riders. However, if the player's payoff was higher than or equal to the average payoff, cooperate (C) to forgive past transgressions.
4. **Adaptive Adjustment**: Every 5 rounds, re-evaluate the strategy based on the game history. If the total payoff over the last 5 rounds is lower than the expected payoff from cooperation (k), increase the cooperation threshold by 1 player. Conversely, if the total payoff is higher than or equal to the expected payoff, decrease the cooperation threshold by 1 player.

**Edge Cases:**

* **Last Round**: In the final round, cooperate (C) if the number of cooperators in the previous round was greater than or equal to half of the total players (n/2). Otherwise, defect (D).
* **Tie-Breaking**: In case of a tie between cooperation and defection, cooperate (C).

**Pseudocode:**
```markdown
# Initialize variables
cooperation_threshold = n / 2
punishment_flag = False

# Loop through rounds
for t in range(r):
    # Get previous round's information
    prev_cooperators = count_cooperators(t-1)
    prev_payoff = get_player_payoff(t-1)

    # Initial cooperation
    if t == 0:
        action[t] = C

    # Reciprocity
    elif prev_cooperators >= n / 2:
        action[t] = C

    # Punishment and forgiveness
    elif prev_cooperators < n / 2 and prev_payoff < avg_payoff(t-1):
        punishment_flag = True
        action[t] = D
    else:
        punishment_flag = False
        action[t] = C

    # Adaptive adjustment (every 5 rounds)
    if t % 5 == 0:
        if total_payoff(t-5:t) < k * n / 2:
            cooperation_threshold += 1
        elif total_payoff(t-5:t) >= k * n / 2:
            cooperation_threshold -= 1

# Last round adjustment
if t == r - 1 and prev_cooperators >= n / 2:
    action[t] = C
```
**Collective Mindset:**

ACHA aligns with the collective mindset by:

* Cooperating initially to set a positive tone for mutual cooperation.
* Reciprocating cooperation when it's beneficial, promoting continued cooperation.
* Punishing free-riders and forgiving past transgressions to maintain fairness.
* Adapting the strategy based on game history to maximize payoffs.

By balancing individual self-interest with collective well-being, ACHA aims to achieve a robust and adaptive equilibrium in the N-player public goods game.
'''

description_COLLECTIVE_521 = '''
**Collective Strategy: Adaptive Cooperative Equilibrium (ACE)**

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):** Cooperate (C) with a probability of 0.5 to gather information about opponents' strategies.
2. **Observation Phase (Rounds 4-r):**
	* Calculate the average cooperation rate (ACR) of all players in the previous round: `ACR = Σ(c_j) / n`
	* If ACR ≥ 0.5, cooperate (C). Otherwise, defect (D)
3. **Adaptive Adjustment:** Every 5 rounds, reassess the opponents' strategies by calculating the moving average cooperation rate over the last 10 rounds (MACR).
	* If MACR > 0.6, increase cooperation probability by 0.1.
	* If MACR < 0.4, decrease cooperation probability by 0.1.

**Handling Edge Cases:**

1. **First Round:** Cooperate with a probability of 0.5 (as in Initial Exploration).
2. **Last Round (Round r):** Defect (D) to maximize personal payoff.
3. **Early Rounds (Rounds 2-3):** Continue Initial Exploration phase.

**Collective Mindset:**

1. **Cooperation Incentivization:** By cooperating when the average cooperation rate is high, ACE encourages others to cooperate as well, creating a positive feedback loop.
2. **Defection Deterrence:** When opponents defect, ACE adapts by decreasing its own cooperation probability, discouraging further defection.

**Pseudocode:**
```python
def ACE(n, k, r):
    # Initialize variables
    ACR = 0  # Average Cooperation Rate
    MACR = 0  # Moving Average Cooperation Rate
    coop_prob = 0.5  # Initial cooperation probability

    for round in range(1, r+1):
        if round <= 3:  # Initial Exploration
            action = random.choice([C, D])
        else:
            ACR = calculate_ACR()
            if ACR >= 0.5:
                action = C
            else:
                action = D

            # Adaptive Adjustment (every 5 rounds)
            if round % 5 == 0:
                MACR = calculate_MACR()
                if MACR > 0.6:
                    coop_prob += 0.1
                elif MACR < 0.4:
                    coop_prob -= 0.1

        # Update ACR and MACR for next round
        update_ACR(action)
        update_MACR()

    return action
```
This strategy is designed to be robust against various opponent behaviors, adapting to the collective cooperation level while incentivizing others to cooperate. By using a combination of observation, adaptation, and cooperation incentivization, ACE aims to achieve a high collective payoff in the tournament.
'''

description_COLLECTIVE_522 = '''
**Collective Strategy: Adaptive Cooperation with Self-Interest (ACSI)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and gather information about other players' behaviors.
2. **Exploration Phase** (Rounds 2-5): Alternate between Cooperate (C) and Defect (D) every other round to test the responsiveness of other players to cooperation. This phase helps identify potential cooperators and defectors.
3. **Adaptive Cooperation**: From Round 6 onwards, use the following rules:
	* If the average payoff of all players in the previous round is greater than or equal to the multiplication factor (k), Cooperate (C).
	* Otherwise, Defect (D).

This decision rule encourages cooperation when the group's overall payoff is high, indicating that others are also cooperating. When the average payoff is low, it suggests that others are defecting, and self-interest dictates defecting as well.

**Handling Edge Cases:**

1. **Last Round**: Cooperate (C) in the final round to maximize the total game payoff, regardless of previous behaviors.
2. **Opponent Defection**: If an opponent defects while you cooperated in the previous round, switch to Defect (D) for one round as a form of "punishment." Then, return to the Adaptive Cooperation rule.

**Collective Mindset:**

The ACSI strategy prioritizes cooperation when it benefits the group but adapts to self-interest when others defect. By initially cooperating and then responding to the group's average payoff, this strategy promotes a collective mindset that balances individual interests with the need for mutual cooperation.

Pseudocode:
```python
def adaptive_cooperation(n, k, r):
    # Initialize variables
    total_payoff = 0
    prev_round_payoffs = [0] * n

    # First round: Cooperate
    action = 'C'
    payoff = 1 - int(action == 'D') + (k / n) * sum([1 if a == 'C' else 0 for a in ['C'] * n])
    total_payoff += payoff

    # Exploration phase (Rounds 2-5)
    for t in range(2, min(r, 6)):
        action = 'C' if t % 2 == 0 else 'D'
        payoff = calculate_payoff(action, prev_round_payoffs, k, n)
        total_payoff += payoff
        prev_round_payoffs = update_prev_round_payoffs(prev_round_payoffs, [action])

    # Adaptive cooperation (Rounds 6-r)
    for t in range(6, r + 1):
        avg_payoff = sum(prev_round_payoffs) / len(prev_round_payoffs)
        action = 'C' if avg_payoff >= k else 'D'
        payoff = calculate_payoff(action, prev_round_payoffs, k, n)
        total_payoff += payoff
        prev_round_payoffs = update_prev_round_payoffs(prev_round_payoffs, [action])

    # Last round: Cooperate
    action = 'C'
    payoff = 1 - int(action == 'D') + (k / n) * sum([1 if a == 'C' else 0 for a in ['C'] * n])
    total_payoff += payoff

    return total_payoff

def calculate_payoff(action, prev_round_payoffs, k, n):
    # Calculate the payoff based on the action and previous round's payoffs
    payoff = (1 - int(action == 'D')) + (k / n) * sum(prev_round_payoffs)
    return payoff

def update_prev_round_payoffs(prev_round_payoffs, new_actions):
    # Update the list of previous round payoffs with the new actions
    prev_round_payoffs.pop(0)
    prev_round_payoffs.append(int(new_actions[0] == 'C'))
    return prev_round_payoffs
```
Note that this pseudocode is a simplified representation of the strategy and may require adjustments for implementation.
'''

description_COLLECTIVE_523 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

AC is a dynamic, history-dependent strategy that balances individual self-interest with collective well-being. It adapts to various opponent behaviors and game conditions.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) to encourage cooperation and create a positive atmosphere.
2. **Majority-based Cooperation**: After the initial rounds, cooperate if the majority of players (> n/2) cooperated in the previous round. Otherwise, defect (D).
3. **Trigger-based Defection**: If the total number of cooperators drops below a certain threshold (defined as `threshold = max(n/4, 1)`), switch to defection for one round.
4. **Exploration Rounds**: Every 5 rounds, play a random action (C or D) to gather information about opponents' strategies and adapt.
5. **Adaptive Threshold Adjustment**: Update the threshold value every 10 rounds based on the number of cooperators in the previous 10 rounds:
	* If average cooperation rate > 0.6, decrease threshold by 1.
	* If average cooperation rate < 0.4, increase threshold by 1.

**Edge Cases:**

* Last round: Cooperate if the majority of players cooperated in the second-to-last round; otherwise, defect.
* First round (already covered): Cooperate.
* Tiebreaks: In case of a tie (e.g., equal number of cooperators and defectors), cooperate.

**Collective Mindset Alignment:**

AC prioritizes cooperation when possible, encouraging others to do the same. By cooperating in initial rounds, AC sets a positive tone for the game. The majority-based cooperation rule allows AC to adapt to changing opponent behaviors while maintaining a cooperative stance. Trigger-based defection ensures that AC can respond to exploitation by opponents.

**Pseudocode:**
```python
def adaptive_cooperation(n, k, history):
  # Initialize variables
  threshold = max(n/4, 1)
  exploration_rounds = 0

  for t in range(r):
    if t == 0:
      action = 'C'  # Initial cooperation
    elif sum(history[t-1]) > n/2:
      action = 'C'  # Majority-based cooperation
    elif sum(history[t-1]) < threshold:
      action = 'D'  # Trigger-based defection
    elif exploration_rounds == 5:
      action = random.choice(['C', 'D'])  # Exploration round
      exploration_rounds = 0
    else:
      action = 'C'

    # Update threshold every 10 rounds
    if t % 10 == 0 and t > 0:
      avg_cooperation_rate = sum(history[t-10:t]) / (n * 10)
      if avg_cooperation_rate > 0.6:
        threshold -= 1
      elif avg_cooperation_rate < 0.4:
        threshold += 1

    # Last round special case
    if t == r - 1:
      action = 'C' if sum(history[t-1]) > n/2 else 'D'

    return action
```
Note that this strategy assumes access to the game history (`history` variable), which contains information about opponents' actions in previous rounds. The `r` variable represents the total number of rounds.
'''

description_COLLECTIVE_524 = '''
**Collective Strategy: "Gradual Cooperation with Adaptive Threshold"**

**Overview**
The proposed strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and adjusting cooperation levels accordingly.

**Decision Rules**

1. **Initial Rounds**: In the first round, play `D` (Defect) to establish a baseline payoff.
2. **Assess Cooperation Level**: After each round, calculate the average number of cooperators (`avg_coop`) in the previous round.
3. **Adaptive Threshold**: Set a cooperation threshold (`coop_thresh`) based on the game parameters and history:
	* If `avg_coop` is above `(k/n) * 0.5`, set `coop_thresh = (k/n) * 0.7`.
	* Otherwise, set `coop_thresh = (k/n) * 0.3`.
4. **Cooperation Decision**: In each subsequent round:
	* If the number of cooperators in the previous round (`num_coop`) is above or equal to `coop_thresh`, play `C` (Cooperate).
	* Otherwise, play `D` (Defect).

**Pseudocode**
```python
def gradual_cooperation_with_adaptive_threshold(n, k, r):
    # Initialize variables
    avg_coop = 0.0
    coop_thresh = (k/n) * 0.3

    for t in range(r):
        if t == 0:
            # First round: play D (Defect)
            action = 'D'
        else:
            # Calculate average number of cooperators in previous round
            avg_coop = sum([1 if c_i == 'C' else 0 for c_i in prev_round_actions]) / n

            # Update cooperation threshold
            if avg_coop >= (k/n) * 0.5:
                coop_thresh = (k/n) * 0.7
            else:
                coop_thresh = (k/n) * 0.3

            # Decide action based on updated threshold
            num_coop = sum([1 if c_i == 'C' else 0 for c_i in prev_round_actions])
            if num_coop >= coop_thresh:
                action = 'C'
            else:
                action = 'D'

        # Store current round's actions and payoffs
        curr_round_actions.append(action)
        curr_round_payoffs.append(calculate_payoff(n, k, action))

    return curr_round_actions, curr_round_payoffs
```
**Rationale**

1. **Initial Defection**: Playing `D` in the first round allows us to establish a baseline payoff and observe other players' actions.
2. **Adaptive Threshold**: By adjusting the cooperation threshold based on the average number of cooperators, we can respond to changes in the game's dynamics and encourage cooperation when it is more likely to be reciprocated.
3. **Gradual Cooperation**: Our strategy gradually increases cooperation as the number of cooperators grows, promoting a collective mindset and encouraging others to cooperate.

This strategy should perform well in a tournament setting by being adaptable to various opponent behaviors while maintaining a balance between individual self-interest and collective well-being.
'''

description_COLLECTIVE_525 = '''
I'll design a collective strategy for this N-Player Public Goods Game that is adaptive and robust to various opponent behaviors.

**Strategy Name:** Adaptive Collective Cooperation (ACC)

**Decision Rules:**

1. **Initial Rounds (t = 1, 2):**
 Cooperate (C) in the first two rounds to establish a baseline of cooperation and encourage others to reciprocate.
2. **Cooperation Assessment (t > 2):**
 Evaluate the overall level of cooperation in the previous round:
	* Calculate the average cooperation rate: `avg_coop = (Σ(j=1 to n) c_j,t-1) / n`
	* Compare `avg_coop` to a threshold value, `θ` (e.g., 0.5)
3. **Adaptive Cooperation (t > 2):**
 Based on the cooperation assessment:
	* If `avg_coop ≥ θ`, Cooperate (C) in the current round
	* If `avg_coop < θ`, Defect (D) in the current round, but with a probability of cooperating (`p_c`) that increases as the game progresses: `p_c = min(1, t/r * 0.5)`
4. **Punishment Mechanism:**
 Implement a gentle punishment mechanism to deter excessive defection:
	* If a player defects in two consecutive rounds (t-2 and t-1), Cooperate (C) with a higher probability (`p_c_high = min(1, p_c + 0.2)`) in the current round

**Edge Cases:**

1. **Last Round (t = r):**
 Cooperate (C) unconditionally to maintain a positive reputation and encourage others to do the same.
2. **Tie-Breaking:**
 In case of a tie in the cooperation assessment, use the previous round's action as a tie-breaker:
	* If the previous round's action was C, Cooperate (C)
	* If the previous round's action was D, Defect (D)

**Collective Mindset Alignment:**

The ACC strategy is designed to promote collective cooperation while adapting to various opponent behaviors. By:

1. **Initial cooperation:** Establishing a baseline of cooperation
2. **Adaptive cooperation:** Responding to changes in overall cooperation levels
3. **Punishment mechanism:** Deterrent against excessive defection

ACC encourages players to cooperate and maintains a balance between individual self-interest and collective well-being.

**Pseudocode:**
```python
def ACC(n, r, k, θ=0.5):
  # Initialize variables
  avg_coop = 0
  p_c = 0
  
  for t in range(1, r+1):
    if t <= 2:
      # Initial rounds: Cooperate
      action = 'C'
    else:
      # Cooperation assessment
      avg_coop = (Σ(j=1 to n) c_j,t-1) / n
      
      # Adaptive cooperation
      if avg_coop >= θ:
        action = 'C'
      else:
        p_c = min(1, t/r * 0.5)
        action = 'D' if random.random() > p_c else 'C'
        
        # Punishment mechanism
        if player_defected_in_previous_two_rounds():
          p_c_high = min(1, p_c + 0.2)
          action = 'C' if random.random() < p_c_high else 'D'
    
    # Last round: Cooperate unconditionally
    if t == r:
      action = 'C'
    
    return action
```
Note that this pseudocode is a simplified representation of the ACC strategy and may need to be modified for actual implementation.
'''

description_COLLECTIVE_526 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation (AC) strategy is designed to balance individual self-interest with collective well-being in the N-Player Public Goods Game. AC adapts to the behavior of others and adjusts its cooperation level accordingly.

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) with a probability p = 0.5. This initial cooperation is intended to signal willingness to collaborate and encourage others to do the same.
2. **Subsequent Rounds**: For each subsequent round t, calculate the average cooperation level of all players in the previous round, denoted as avg_c_t-1. This can be done by observing the actions of other players and calculating the proportion of cooperators.

avg_c_t-1 = (number of cooperators in round t-1) / n

3. **Cooperation Threshold**: Define a cooperation threshold θ ∈ [0, 1]. AC will cooperate if the average cooperation level is above this threshold.

θ = k/n (to balance individual and collective payoffs)

4. **Adaptive Cooperation**: In each round t, AC will:
	* Cooperate (C) with probability p = avg_c_t-1 if avg_c_t-1 ≥ θ
	* Defect (D) with probability p = 1 - avg_c_t-1 if avg_c_t-1 < θ

**Edge Cases:**

1. **Last Round**: In the final round, AC will defect (D). Since there are no future rounds to influence cooperation, individual self-interest takes precedence.
2. **Ties and Thresholds**: If avg_c_t-1 is exactly equal to θ, AC will cooperate (C).

**Collective Mindset:**

AC prioritizes collective well-being while ensuring individual payoffs remain competitive. By adapting to the average cooperation level of others, AC aims to:

* Encourage widespread cooperation when possible
* Avoid exploitation by free-riders
* Balance individual and collective interests

Pseudocode:
```python
def Adaptive_Cooperation(n, k, r):
  # Initialize variables
  avg_c = [0] * (r + 1)  # Store average cooperation levels for each round
  action = [None] * r  # Store actions taken in each round

  # First round: randomize cooperation with p=0.5
  action[0] = 'C' if random.random() < 0.5 else 'D'
  avg_c[1] = (action[0] == 'C') / n

  for t in range(2, r + 1):
    # Calculate cooperation threshold
    theta = k / n

    # Determine action based on average cooperation level
    if avg_c[t - 1] >= theta:
      action[t - 1] = 'C'
    else:
      action[t - 1] = 'D'

    # Update average cooperation level for next round
    avg_c[t] = (action[t - 1] == 'C') / n

  return action
```
This collective strategy should perform well in a tournament against independent strategies developed by other AI systems, as it balances individual self-interest with collective well-being and adapts to the behavior of others.
'''

description_COLLECTIVE_527 = '''
I propose a collective strategy called "Adaptive Cooperative Threshold" (ACT) that balances individual self-interest with the need for cooperation. ACT relies on observing past behavior and adjusting its threshold for cooperation based on the level of cooperation observed.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage others to cooperate.
2. **Subsequent Rounds:** Observe the number of cooperators in the previous round (`c_prev`) and calculate the cooperation ratio (`coop_ratio = c_prev / n`).
3. **Cooperation Threshold:** Set a dynamic cooperation threshold (`theta`) based on the game parameters:
	* `theta = (k - 1) / k` if `coop_ratio > 0.5`
	* `theta = 0.5` if `coop_ratio == 0.5`
	* `theta = (1 - k) / (n - 1)` if `coop_ratio < 0.5`
4. **Cooperate or Defect:** Cooperate (C) if the current round's cooperation ratio is above the threshold (`coop_ratio >= theta`); otherwise, defect (D).

Pseudocode:
```python
def ACT(n, k, history):
    if len(history) == 0:  # Initial round
        return "C"
    
    c_prev = sum(1 for action in history[-1] if action == "C")
    coop_ratio = c_prev / n
    
    if coop_ratio > 0.5:
        theta = (k - 1) / k
    elif coop_ratio == 0.5:
        theta = 0.5
    else:
        theta = (1 - k) / (n - 1)
    
    current_coop_ratio = sum(1 for action in history[-2:] if action == "C") / n
    
    if current_coop_ratio >= theta:
        return "C"
    else:
        return "D"
```
**Handling Edge Cases:**

* **First Round:** Cooperate to encourage others to cooperate.
* **Last Round:** Use the same decision rules as in other rounds, but consider that there are no future rounds to influence.
* **Ties:** In case of a tie (e.g., `coop_ratio == 0.5`), use the default threshold value (`theta = 0.5`) to avoid oscillations.

**Collective Mindset:**

ACT is designed to promote cooperation while being robust to various opponent behaviors. By adapting to the level of cooperation observed, ACT encourages others to cooperate and adjusts its own behavior accordingly. This collective strategy balances individual self-interest with the need for cooperation, making it a suitable candidate for a tournament against independent strategies.

Note that this strategy does not rely on explicit coordination or communication with other players, nor does it assume specific norms or cooperation schedules. Instead, ACT uses the game's history to inform its decisions and promote cooperation in a decentralized manner.
'''

description_COLLECTIVE_528 = '''
**Collective Strategy: "Adaptive Collective Optimism" (ACO)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a high probability (0.8). This encourages cooperation and sets a positive tone for the game.
2. **Reciprocity**: Observe the number of cooperators in the previous round (t-1). If more than half of the players cooperated, cooperate in the current round (t).
3. **Punish Defection**: If a player defected (D) in the previous round and received a high payoff (> 1.5), defect in the current round. This discourages exploitation.
4. **Imitation**: Observe the actions of other players in the previous round. If more than half of the players cooperated, cooperate. Otherwise, defect.
5. **Adaptive Threshold**: Maintain a threshold value (T) that represents the minimum number of cooperators required to justify cooperation. Initialize T = n/2. After each round, update T based on the game's history:
	* If cooperation yields a higher payoff than defection, decrease T by 1.
	* If defection yields a higher payoff than cooperation, increase T by 1.

**Edge Cases:**

1. **Last Round**: In the final round (r), cooperate if the total number of cooperators in the previous round is greater than or equal to T.
2. **Tiebreaker**: If the game is tied (i.e., the same number of cooperators and defectors) in a given round, cooperate with probability 0.5.

**Collective Mindset:**

ACO aims to foster cooperation while being robust to various opponent behaviors. By initially cooperating and reciprocating cooperative actions, we encourage others to do the same. The punishment mechanism prevents exploitation, while imitation and adaptive threshold adjustments help us adapt to changing game dynamics.

**Pseudocode:**
```
Initialize:
  T = n/2
  cooperate_prob = 0.8

For each round t from 1 to r:
  if t == 1:
    # Initial cooperation
    action[t] = C with probability cooperate_prob
  else:
    # Reciprocity and punishment
    num_cooperators_prev_round = count(C in actions[t-1])
    if num_cooperators_prev_round > n/2:
      action[t] = C
    elif any(player_defected_and_got_high_payoff(t-1)):
      action[t] = D
    else:
      # Imitation and adaptive threshold
      imitate_majority(actions[t-1])
      update_threshold(T, actions[t-1], payoffs[t-1])

  # Last round handling
  if t == r:
    if num_cooperators_prev_round >= T:
      action[t] = C

  return action[t]
```
This strategy is designed to be adaptive and robust, promoting cooperation while being prepared for a wide range of opponent behaviors.
'''

description_COLLECTIVE_529 = '''
I propose a collective strategy called "Adaptive Collective Conscience" (ACC). ACC aims to balance individual self-interest with the greater good, adapting to the game's history and opponents' behaviors.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a baseline level of cooperation.
2. **Majority Rule:** In subsequent rounds, cooperate if the majority (> n/2) of players cooperated in the previous round. Defect otherwise.
3. **Minority Punishment:** If fewer than half of the players cooperated in the previous round, defect with probability p = (1 - k/n). Cooperate with probability 1 - p. This punishes non-cooperation while allowing for occasional forgiveness.
4. **Recent History Bias:** When the number of cooperators is exactly equal to n/2, use a "bias" rule: cooperate if more than half of the players have cooperated in at least two-thirds of the last three rounds (including the current round). Defect otherwise.

**Pseudocode (for illustration purposes only):**
```python
def ACC(n, k, history):
  # First round: Cooperate
  if len(history) == 0:
    return 'C'

  # Majority Rule
  cooperators = sum([1 for h in history[-1] if h == 'C'])
  if cooperators > n / 2:
    return 'C'
  
  # Minority Punishment
  p = (1 - k/n)
  if random.random() < p:
    return 'D'

  # Recent History Bias
  two_thirds_rounds = len(history) // 3 * 2
  biased_coop_count = sum([sum([1 for h in history[i] if h == 'C']) > n / 2 
                            for i in range(max(0, len(history)-two_thirds_rounds), len(history))])
  if biased_coop_count >= two_thirds_rounds:
    return 'C'

  # Default: Defect
  return 'D'
```
**Edge Cases and Additional Considerations:**

* **Last Round:** Cooperate in the final round to maximize collective payoff.
* **Ties:** When the number of cooperators is exactly equal to n/2, ACC uses the Recent History Bias rule to break ties. This promotes cooperation when most players have been cooperating recently.

**Collective Mindset:**
ACC aligns with the collective mindset by:

1. Encouraging cooperation in the initial round.
2. Responding positively to majority cooperation.
3. Punishing non-cooperation while allowing for occasional forgiveness.
4. Using recent history bias to promote sustained cooperation.

This adaptive strategy balances individual self-interest with the greater good, responding effectively to a wide range of opponent behaviors and game histories.
'''

description_COLLECTIVE_530 = '''
**Collective Strategy: Adaptive Cooperation**

Our strategy, "Adaptive Cooperation," aims to balance individual self-interest with collective well-being by adapting to the game's history and exploiting the benefits of cooperation.

**Decision Rules:**

1. **Initial Round (t=1): Cooperate**
Start with a cooperative attitude to set a positive tone for the game.
2. **Subsequent Rounds (t>1):**
	* If the average payoff per player in the previous round is greater than or equal to the multiplication factor (k), cooperate.
	* Otherwise, defect.

Pseudocode:
```python
def adaptive_cooperation(t, history):
    if t == 1:  # Initial round
        return 'C'
    else:
        avg_payoff_prev_round = sum(history[t-1]) / n
        if avg_payoff_prev_round >= k:
            return 'C'
        else:
            return 'D'
```
**Handling Edge Cases:**

* **Last Round (t=r):** Cooperate, as the potential benefits of cooperation outweigh the risks in the final round.
* **Tiebreakers:** In cases where the average payoff is exactly equal to k, cooperate to maintain a cooperative stance.

**Collective Mindset:**

Our strategy aligns with the collective mindset by:

1. **Encouraging Cooperation:** By cooperating when the average payoff is high, we promote a culture of cooperation and create opportunities for mutual benefit.
2. **Adapting to Defection:** When faced with widespread defection (low average payoff), our strategy adapts by defecting, minimizing individual losses and encouraging others to reconsider their actions.
3. **Robustness:** By not relying on specific coordination mechanisms or opponent behaviors, our strategy remains robust against a wide range of opponent strategies.

**Rationale:**

1. **Exploiting Cooperation Benefits:** When the average payoff is high, cooperation yields better outcomes for all players. Our strategy capitalizes on these benefits by cooperating in such situations.
2. **Mitigating Defection Risks:** By defecting when the average payoff is low, our strategy minimizes individual losses and encourages opponents to reassess their actions.
3. **Adaptive Nature:** The strategy's adaptability allows it to respond effectively to changing game conditions, making it a robust choice for a wide range of opponent behaviors.

By employing Adaptive Cooperation, we create a collective strategy that balances individual self-interest with the greater good, promoting a cooperative environment while remaining resilient against diverse opponent strategies.
'''

description_COLLECTIVE_531 = '''
**Collective Strategy: "Adaptive Cooperation"**

This strategy is designed to balance individual self-interest with collective well-being, while being robust against various opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Subsequent Rounds**: For each subsequent round t, calculate the "Cooperation Quotient" (CQ) based on the previous round's outcomes:
	* CQ = (Number of Cooperators in Round t-1) / n
3. **Adaptive Threshold**: Set an adaptive threshold θ (theta), which adjusts based on the game's history. Initialize θ to 0.5.
4. **Decision Logic**:
	+ If CQ ≥ θ, Cooperate (C)
	+ If CQ < θ, Defect (D)

**Handling Edge Cases:**

1. **Last Round**: In the final round r, always Defect (D) to maximize individual payoff, as there are no future rounds to influence.
2. **Ties and Ambiguity**: In cases where multiple strategies or payoffs are tied, default to Cooperate (C) to maintain a cooperative mindset.

**Collective Mindset Alignment:**

1. **Monitoring Cooperation Quotient**: Continuously track the CQ value to gauge the level of cooperation in the game.
2. **Adapting to Shifts in Cooperation**: Adjust the adaptive threshold θ based on changes in CQ:
	+ If CQ increases, decrease θ by 0.05 (making it easier to cooperate)
	+ If CQ decreases, increase θ by 0.05 (making it harder to cooperate)

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
theta = 0.5

# First round
cooperate()

# Subsequent rounds
for t = 2 to r:
    # Calculate Cooperation Quotient (CQ)
    CQ = count_cooperators(t-1) / n
    
    # Adjust theta based on CQ changes
    if CQ > previous_CQ:
        theta -= 0.05
    elif CQ < previous_CQ:
        theta += 0.05

    # Decision logic
    if CQ >= theta:
        cooperate()
    else:
        defect()

# Last round (r)
defect()
```
This Adaptive Cooperation strategy balances individual self-interest with collective well-being, while being robust against various opponent behaviors. By monitoring the Cooperation Quotient and adjusting the adaptive threshold, the strategy promotes cooperation when possible and adapts to changes in the game's dynamics.
'''

description_COLLECTIVE_532 = '''
**Collective Strategy: "Adaptive Community Focus" (ACF)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to establish a community focus and encourage others to do the same.
2. **Reciprocity**: Cooperate if the number of cooperators in the previous round is greater than or equal to the number of defectors. This encourages continued cooperation when it's beneficial.
3. **Punishment**: Defect if the number of defectors in the previous round exceeds the number of cooperators. This discourages exploitation by others.
4. **Community Focus**: Cooperate if the total payoff from community contributions (k/n × Σ(j=1 to n) c_j) is greater than or equal to the private payoff from defecting (1). This prioritizes collective benefits over individual gains.

**Handling Edge Cases:**

* **Last Round**: In the final round, cooperate if the number of cooperators in the previous round is greater than or equal to the number of defectors. This maintains community focus until the end.
* **Ties**: If the numbers of cooperators and defectors are equal, cooperate to maintain a community-oriented approach.

**Pseudocode:**
```python
def adaptive_community_focus(n, k, history):
    if len(history) == 0:  # First round
        return "C"  # Cooperate

    prev_round = history[-1]
    cooperators = sum(1 for action in prev_round if action == "C")
    defectors = n - cooperators

    if cooperators >= defectors:
        return "C"  # Reciprocity
    elif k/n * cooperators >= 1:  # Community Focus
        return "C"
    else:
        return "D"  # Punishment

def get_action(n, k, history):
    action = adaptive_community_focus(n, k, history)
    if len(history) == r - 1:  # Last round
        if sum(1 for a in history[-1] if a == "C") >= n/2:
            return "C"
    return action
```
**Collective Mindset:**
The ACF strategy prioritizes collective benefits over individual gains, encouraging cooperation when it's beneficial to the community. By reciprocating cooperation and punishing defection, ACF promotes a stable and cooperative environment. In edge cases, ACF maintains community focus to ensure continued cooperation until the end of the game.
'''

description_COLLECTIVE_533 = '''
I propose a collective strategy called "Adaptive Cooperative Threshold" (ACT). ACT is designed to balance individual self-interest with collective welfare, while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if k > 2.5; otherwise, defect (D).
2. **Cooperation Threshold**: Calculate a cooperation threshold (T) based on the game history:
	* T = ( Σ(t=1 to r-1) total_contributions / n ) \* (k/n)
	* where total_contributions is the sum of all players' contributions in previous rounds.
3. **Adaptive Cooperation**: In each subsequent round, cooperate if the expected payoff from cooperation is greater than or equal to the expected payoff from defection:
	* Expected cooperation payoff = (1 - c_i) + T
	* Expected defection payoff = 1 + (k/n) \* (total_contributions / n)
	* If expected cooperation payoff ≥ expected defection payoff, cooperate; otherwise, defect.
4. **Trigger Strategy**: Implement a "trigger strategy" to deter exploitation:
	* If another player defects while you cooperated in the previous round, defect for 2 rounds.

**Handling Edge Cases:**

1. **First Round**: Follow the initial cooperation rule (Step 1).
2. **Last Round**: Cooperate if k > 2.5; otherwise, defect.
3. **Early Rounds**: In rounds 2-5, adjust the cooperation threshold (T) to be more conservative:
	* T = ( Σ(t=1 to r-1) total_contributions / n ) \* (k/n) \* 0.8

**Collective Mindset:**

ACT aligns with a collective mindset by:

1. Initially cooperating when the multiplication factor is high, encouraging others to do the same.
2. Adapting to the game history and adjusting cooperation levels based on the collective behavior of all players.
3. Detering exploitation through the trigger strategy, promoting fairness and reciprocity.

Pseudocode:
```
def ACT(n, k, r):
  # Initialize variables
  total_contributions = 0
  cooperation_threshold = 0

  for round in range(1, r+1):
    if round == 1:
      # Initial cooperation rule
      if k > 2.5:
        action = 'C'
      else:
        action = 'D'
    else:
      # Calculate cooperation threshold
      total_contributions += sum(other_players' contributions in previous rounds)
      cooperation_threshold = (total_contributions / n) * (k/n)

      # Adaptive cooperation rule
      expected_cooperation_payoff = (1 - c_i) + cooperation_threshold
      expected_defection_payoff = 1 + (k/n) * (total_contributions / n)
      if expected_cooperation_payoff >= expected_defection_payoff:
        action = 'C'
      else:
        action = 'D'

    # Trigger strategy
    if another_player_defected and you_cooperated_last_round:
      action = 'D' for 2 rounds

    return action
```
This pseudocode provides a basic implementation of the ACT strategy. It can be refined and optimized further to accommodate specific requirements.
'''

description_COLLECTIVE_534 = '''
**Collective Strategy: Adaptive Tit-for-Tat with Public Goods Twist**

This strategy balances individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round, as there is no prior information to adapt to.
2. **Subsequent Rounds**: For each subsequent round t > 1:
	* Calculate the **average cooperation rate** of all players over the past rounds: `avg_coop = (∑(t-1) c_j) / (n * (t-1))`
	* If `avg_coop` is greater than or equal to the **threshold value**, cooperate (C). Otherwise, defect (D).
	* The threshold value is calculated as: `threshold = 1 - (k/n)`
3. **Last Round**: In the last round r, always defect (D) since there are no future rounds to influence.

**Edge Cases:**

* If only one player remains in the game (e.g., due to elimination), always cooperate (C).
* If a player observes that all other players have defected for multiple consecutive rounds, switch to permanent defection (D).

**Pseudocode:**
```python
def AdaptiveTitForTat(n, k, r):
    # Initialize variables
    avg_coop = 0.0
    threshold = 1 - (k/n)
    
    # First round: Cooperate
    action = 'C'
    yield action
    
    for t in range(2, r+1):
        # Calculate average cooperation rate
        avg_coop = (∑(t-1) c_j) / (n * (t-1))
        
        # Determine action based on threshold value
        if avg_coop >= threshold:
            action = 'C'
        else:
            action = 'D'
        
        yield action
    
    # Last round: Defect
    yield 'D'
```
**Collective Mindset Alignment:**

This strategy prioritizes collective well-being by:

1. Cooperating in the first round to set a positive tone.
2. Adapting to the average cooperation rate, which encourages other players to cooperate if they observe high levels of cooperation.
3. Gradually adjusting the threshold value based on the game's parameters (n and k), allowing for flexibility and robustness.

By incorporating both self-interest and collective considerations, this strategy aims to achieve a balance between individual payoffs and overall group performance in the N-Player Public Goods Game.
'''

description_COLLECTIVE_535 = '''
**Collective Strategy: Adaptive Cooperation with History-Driven Adaptation (ACHA)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage mutual cooperation and create a positive payoff trajectory.
2. **General Rule:** For rounds 2 to r-1, cooperate if the average payoff of cooperators in the previous round is greater than or equal to the average payoff of defectors. Otherwise, defect.
3. **Payoff Comparison:** Calculate the average payoff of cooperators and defectors separately using the history of payoffs from previous rounds. This comparison will guide the decision for the next round.

Pseudocode:
```
IF Round == 1 THEN
    Cooperate (C)
ELSE IF Average Payoff of Cooperators >= Average Payoff of Defectors THEN
    Cooperate (C)
ELSE
    Defect (D)
END IF
```

**Edge Case Handling:**

* **Last Round (r):** In the final round, defect to maximize individual payoff since there are no future rounds to consider.
* **Tiebreaker:** If the average payoffs of cooperators and defectors are equal, cooperate to maintain a cooperative trajectory.

Pseudocode:
```
IF Round == r THEN
    Defect (D)
ELSE IF Average Payoff of Cooperators == Average Payoff of Defectors THEN
    Cooperate (C)
END IF
```

**Collective Mindset:**

ACHA is designed to adapt to the collective behavior of players while promoting cooperation. By cooperating in the first round and responding to the average payoffs of cooperators and defectors, ACHA encourages a cooperative environment.

**Robustness to Opponent Behaviors:**

* **All Defect:** If all opponents defect, ACHA will eventually switch to defection as well, minimizing losses.
* **Mixed Strategies:** ACHA's adaptive nature allows it to respond effectively to mixed strategies, such as tit-for-tat or win-stay-lose-shift.

**Key Benefits:**

1. **Adaptability:** Responds to changes in opponent behaviors and adapts strategy accordingly.
2. **Collective Cooperation:** Encourages cooperation while minimizing individual losses.
3. **Robustness:** Performs well against a wide range of opponent strategies.

By implementing ACHA, we aim to create a robust collective strategy that promotes cooperation and adapts effectively to various opponent behaviors in the N-Player Public Goods Game.
'''

description_COLLECTIVE_536 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy, "Adaptive Cooperation," aims to balance individual self-interest with the benefits of cooperation, while being robust to various opponent behaviors.

**Decision Rules:**

1. **Initial Rounds:** In the first round, play C (Cooperate). This encourages opponents to cooperate and sets a positive tone for the game.
2. **General Rule:** For rounds 2 to r-1, use the following decision-making process:
	* Calculate the average cooperation rate of all players in the previous round: `avg_coop_prev = (Σ(j=1 to n) c_j,t-1) / n`
	* If `avg_coop_prev` is above a certain threshold (`coop_threshold = 0.5`), play C (Cooperate). This reinforces cooperation when it's prevalent.
	* Otherwise, play D (Defect).
3. **Last Round:** In the final round (r), play D (Defect) to maximize individual payoff.

**Edge Cases:**

1. **First Round:** As mentioned earlier, play C (Cooperate) in the first round.
2. **Last Round:** Play D (Defect) in the last round.
3. **Ties:** If `avg_coop_prev` is exactly equal to `coop_threshold`, play C (Cooperate).

**Adaptation Mechanism:**

To adapt to changing opponent behaviors, we introduce a simple learning mechanism:

* After each round, update the cooperation threshold (`coop_threshold`) based on the average payoff of all players in that round:
	+ If the average payoff is above 1.5 (indicating successful cooperation), increase `coop_threshold` by 0.05.
	+ If the average payoff is below 1.2 (indicating failed cooperation), decrease `coop_threshold` by 0.05.

**Pseudocode:**
```python
def adaptive_cooperation(n, r, k):
    coop_threshold = 0.5

    for t in range(r):
        if t == 0:
            action = 'C'
        else:
            avg_coop_prev = calculate_average_cooperation(t-1)
            if avg_coop_prev > coop_threshold:
                action = 'C'
            else:
                action = 'D'

        # Adaptation mechanism
        if t < r - 1:
            avg_payoff = calculate_average_payoff(t)
            if avg_payoff > 1.5:
                coop_threshold += 0.05
            elif avg_payoff < 1.2:
                coop_threshold -= 0.05

    return action
```
This strategy is collective in the sense that it takes into account the actions of all players and adapts to their behavior, while also being robust against various opponent strategies.
'''

description_COLLECTIVE_537 = '''
**Collective Strategy: Adaptive Cooperativeness (AC)**

**Decision Rules:**

1. **Initial Rounds:** In the first few rounds (t < 0.2r), play **Cooperate (C)** to establish a cooperative foundation and encourage others to do the same.
2. **Adaptive Phase:** After the initial rounds, use a weighted average of past cooperation rates to determine your action:
	* Calculate the total number of cooperators in the previous round: `cooperators_t-1 = Σ(j=1 to n) c_j,t-1`
	* Compute the average cooperation rate over the last few rounds (e.g., 0.1r rounds): `avg_coop_rate = (Σ(t=1 to t-1) cooperators_t) / (t - 1)`
	* If `avg_coop_rate` is above a certain threshold (e.g., 0.5), play **Cooperate (C)**; otherwise, play **Defect (D)**
3. **Punishment Mechanism:** Implement a punishment phase to deter free-riding:
	* Monitor the average cooperation rate over the last few rounds (`avg_coop_rate`)
	* If `avg_coop_rate` falls below a certain threshold (e.g., 0.3), switch to playing **Defect (D)** for a few rounds (e.g., 0.1r rounds) to "punish" non-cooperators
4. **Endgame Strategy:** In the last few rounds (t > 0.8r), play **Cooperate (C)** if `avg_coop_rate` is above the threshold; otherwise, play **Defect (D)**

**Pseudocode:**
```python
def adaptive_cooperativeness(n, r, k):
    # Initialize variables
    avg_coop_rate = 0.5
    cooperators_t-1 = 0
    
    for t in range(r):
        if t < 0.2r:
            action = C  # Cooperate in initial rounds
        else:
            # Compute average cooperation rate over last few rounds
            avg_coop_rate = (Σ(t=1 to t-1) cooperators_t) / (t - 1)
            
            # Adaptive phase
            if avg_coop_rate > 0.5:
                action = C  # Cooperate if avg_coop_rate is high
            else:
                action = D  # Defect otherwise
            
            # Punishment mechanism
            if avg_coop_rate < 0.3:
                action = D  # Punish non-cooperators for a few rounds
        
        # Endgame strategy
        if t > 0.8r:
            if avg_coop_rate > 0.5:
                action = C  # Cooperate in endgame if avg_coop_rate is high
            else:
                action = D  # Defect otherwise
        
        # Update cooperators_t-1 for next round
        cooperators_t-1 = Σ(j=1 to n) c_j,t-1
    
    return action
```
**Collective Mindset:**

The Adaptive Cooperativeness (AC) strategy is designed to promote cooperation and adapt to the behavior of other players. By initially cooperating, we establish a cooperative foundation and encourage others to do the same. The adaptive phase adjusts our behavior based on the past cooperation rates, allowing us to respond to changes in the game dynamics.

By incorporating a punishment mechanism, we deter free-riding and maintain a high level of cooperation. In the endgame, we switch back to cooperating if the average cooperation rate is high, ensuring that we maximize our payoff while maintaining a cooperative mindset.

This strategy should be robust against various opponent behaviors and provide a strong foundation for collective success in the tournament.
'''

description_COLLECTIVE_538 = '''
**Collective Strategy: Adaptive Cooperativity (AC)**

The AC strategy aims to balance individual self-interest with collective welfare by adaptively responding to the group's cooperative behavior.

**Decision Rules:**

1. **Initial Round**: In the first round, cooperate (C) unconditionally.
2. **Subsequent Rounds**: Use a hybrid approach that combines elements of Tit-for-Tat and a novel "Cooperative Quorum" mechanism:
	* If the number of cooperators in the previous round exceeds a certain threshold ( Quorum ), cooperate (C).
	* Otherwise, play a mix of cooperation and defection based on the opponent's past behavior.

**Quorum Calculation:**
Let Q be the Quorum value, which is calculated as follows:

Q = ceil((n + k) / 2)

where ceil(x) denotes the smallest integer greater than or equal to x.

**Tit-for-Tat Component:**

* If an opponent cooperated in the previous round, cooperate (C).
* If an opponent defected in the previous round, defect (D).

**Adaptive Weighting:**
Assign weights to the Quorum and Tit-for-Tat components based on the group's past behavior:

* w_Quorum = (number of rounds with cooperation above Quorum) / r
* w_TitForTat = 1 - w_Quorum

In each round, use a weighted random choice between cooperating (C) and defecting (D), where the weights are determined by the adaptive weighting mechanism.

**Edge Cases:**

* **Last Round**: In the final round, always cooperate (C).
* **Tiebreaker**: If the Quorum calculation results in a tie, prioritize cooperation (C).

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
Quorum = ceil((n + k) / 2)
w_Quorum = 0
w_TitForTat = 1

for round in range(1, r+1):
    if round == 1:
        # Initial Round: Cooperate unconditionally
        action = C
    else:
        # Previous round's cooperation count
        prev_coop_count = sum([player.cooperated for player in players])

        # Calculate Quorum and weights
        if prev_coop_count >= Quorum:
            w_Quorum += 1 / r
            w_TitForTat -= 1 / r

        # Hybrid decision rule
        if random.random() < w_Quorum:
            action = C
        elif random.random() < w_TitForTat:
            # Tit-for-Tat component
            opponent_cooperated = [player.cooperated for player in players]
            action = C if any(opponent_cooperated) else D
        else:
            # Weighted random choice
            action = C if random.random() < w_Quorum / (w_Quorum + w_TitForTat) else D

    # Update cooperative behavior for the next round
    [player.cooperated for player in players] = [action == C]
```

The Adaptive Cooperativity strategy is designed to be collective, adaptive, and robust to various opponent behaviors. By incorporating a Quorum mechanism, it balances individual self-interest with group cooperation, while the Tit-for-Tat component ensures that opponents are held accountable for their actions.
'''

description_COLLECTIVE_539 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The AC strategy aims to balance individual payoff maximization with collective welfare by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds:** In the first round, cooperate (C) to encourage cooperation and build a positive reputation.
2. **Cooperation Threshold:** Track the total number of cooperators in previous rounds (`cooperators_t-1`). If `cooperators_t-1` ≥ `n/2`, cooperate (C). Otherwise, defect (D).
3. **Adaptive Adjustment:** Monitor the average payoff per round (`avg_payoff_t-1`) and adjust the cooperation threshold accordingly:
	* If `avg_payoff_t-1` > `(k/n) * n / 2`, increase the cooperation threshold by 1.
	* If `avg_payoff_t-1` < `(k/n) * n / 4`, decrease the cooperation threshold by 1.
4. **Defection Response:** If a player defects (D) in the previous round and `cooperators_t-1` ≥ `n/2`, defect (D) in response to maintain a fair share of the public good.

**Edge Cases:**

* **Last Round:** In the final round, cooperate (C) if `cooperators_t-1` ≥ `n/2`. Otherwise, defect (D).
* **Tie-Breaking:** If `cooperators_t-1` is exactly equal to `n/2`, choose cooperation (C) with a probability of 0.5.

**Collective Alignment:**

The AC strategy prioritizes collective welfare by:

* Encouraging cooperation in the early rounds
* Adapting to the game's history and adjusting the cooperation threshold accordingly
* Responding to defection with fairness

By doing so, the AC strategy promotes a balance between individual payoff maximization and collective welfare, making it a robust and adaptive approach for the N-Player Public Goods Game.

Pseudocode:
```python
def AdaptiveCooperation(n, k, r):
    cooperators_t_1 = 0
    avg_payoff_t_1 = 0
    
    for t in range(1, r+1):
        if t == 1:
            action = 'C'
        else:
            if cooperators_t_1 >= n/2:
                action = 'C'
            elif cooperators_t_1 < n/4 and avg_payoff_t_1 < (k/n) * n / 4:
                action = 'D'
            else:
                action = 'C' if random.random() < 0.5 else 'D'
        
        # Update cooperation threshold
        if avg_payoff_t_1 > (k/n) * n / 2:
            cooperators_threshold += 1
        elif avg_payoff_t_1 < (k/n) * n / 4:
            cooperators_threshold -= 1
        
        # Defection response
        if action == 'D' and cooperators_t_1 >= n/2:
            action = 'D'
        
        # Last round handling
        if t == r:
            if cooperators_t_1 >= n/2:
                action = 'C'
            else:
                action = 'D'
        
        # Update history
        cooperators_t_1 += 1 if action == 'C' else 0
        avg_payoff_t_1 = (avg_payoff_t_1 * (t-1) + payoff(action)) / t
    
    return action
```
'''

description_COLLECTIVE_540 = '''
**Collective Strategy: Adaptive Cooperation with Retaliation (ACR)**

The ACR strategy aims to balance individual self-interest with collective well-being by adaptively responding to the behavior of others. It combines cooperation with retaliation to encourage mutual cooperation and deter exploitation.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to signal willingness to cooperate and encourage reciprocal behavior.
2. **Subsequent Rounds**: Observe the previous round's outcome and adjust accordingly:
	* If the total number of cooperators is above a threshold (α), cooperate (C). This encourages continued cooperation when it's beneficial.
	* If the total number of cooperators is below α, defect (D) with probability β. This introduces some randomness to avoid being exploited by defectors and encourages others to cooperate.
	* If you were exploited in the previous round (i.e., you cooperated while others defected), retaliate by defecting (D) for a set number of rounds (γ). This punishes uncooperative behavior and helps maintain cooperation.
3. **Retaliation**: After retaliating, return to the normal decision-making process.

**Edge Cases**

* **Last Round**: Cooperate (C) in the last round, as there is no opportunity for retaliation or future reciprocity.
* **Tiebreaker**: In cases where the total number of cooperators is exactly α, cooperate (C) to favor cooperation over defection.

**Collective Mindset**

The ACR strategy prioritizes collective well-being by:

1. Cooperating when others do, to maximize joint payoffs.
2. Retaliating against exploiters to maintain cooperation and deter uncooperative behavior.
3. Adapting to changing circumstances to ensure continued cooperation.

**Pseudocode**
```python
def adaptive_cooperation_with_retaliation(n, k, r):
  alpha = n / 2  # Threshold for cooperation
  beta = 0.5     # Probability of defection when below threshold
  gamma = 3      # Number of rounds to retaliate

  cooperate = True  # Initial round cooperation
  retaliation = False
  rounds_to_retaliate = 0

  for t in range(r):
    if t == 0:
      # First round: Cooperate
      action = 'C'
    elif retaliation:
      # Retaliating: Defect
      action = 'D'
      rounds_to_retaliate -= 1
      if rounds_to_retaliate == 0:
        retaliation = False
    else:
      total_cooperators = count_cooperators(t - 1)
      if total_cooperators >= alpha:
        # Cooperate when above threshold
        action = 'C'
      elif total_cooperators < alpha and random.random() < beta:
        # Defect with probability β when below threshold
        action = 'D'
      else:
        # Cooperate otherwise
        action = 'C'

    if was_exploited(t - 1):
      retaliation = True
      rounds_to_retaliate = gamma

  return action
```
This strategy balances cooperation and self-interest, adapting to the behavior of others while promoting collective well-being.
'''

description_COLLECTIVE_541 = '''
**Collective Strategy: "Adaptive Cooperation with Reputation Feedback"**

This strategy aims to balance individual payoff maximization with collective welfare, adapting to the evolving game environment and opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) to establish a positive reputation and encourage others to cooperate.
2. **Subsequent Rounds (t>1)**:
	* If the total payoff from the previous round is greater than or equal to the average payoff of all players, cooperate (C).
	* Otherwise, defect (D).
3. **Reputation Feedback**:
	* Track the number of cooperators in each round and calculate the "cooperation rate" (CR) as the ratio of cooperators to total players.
	* If CR > 0.5, cooperate (C) in the next round; otherwise, defect (D).
4. **Opponent Behavior Adaptation**:
	* Monitor the frequency of cooperation (FC) and defection (FD) by each opponent over the last few rounds (e.g., 3-5 rounds).
	* If an opponent's FC > FD, increase the likelihood of cooperating with that opponent in the next round.
	* If an opponent's FD > FC, decrease the likelihood of cooperating with that opponent in the next round.

**Pseudocode:**
```python
def adaptive_cooperation(n, k, r):
    # Initialize variables
    total_payoff = 0
    cooperation_rate = 0.5
    opponent_behavior = [[0, 0] for _ in range(n)]  # FC, FD

    for t in range(1, r+1):
        if t == 1:
            action = 'C'
        else:
            prev_total_payoff = total_payoff
            total_payoff = sum([payoff(i) for i in range(n)])
            cooperation_rate = sum([c_i for c_i in [1 if action == 'C' else 0 for action in actions]]) / n

            if prev_total_payoff >= avg_payoff(n):
                action = 'C'
            else:
                action = 'D'

        # Reputation feedback
        if cooperation_rate > 0.5:
            action = 'C'

        # Opponent behavior adaptation
        for i, (FC, FD) in enumerate(opponent_behavior):
            if FC > FD:
                opponent_behavior[i] = [FC + 1, FD]
            elif FD > FC:
                opponent_behavior[i] = [FC, FD + 1]

        # Update actions and payoffs
        actions.append(action)
        total_payoff += payoff(len([a for a in actions if a == 'C']))

    return actions

def payoff(n_cooperators):
    return (k / n) * n_cooperators
```
**Collective Mindset:**

This strategy prioritizes cooperation when the collective payoff is high and adapts to opponent behaviors, aiming to maintain a balanced reputation. By tracking cooperation rates and adapting to opponent behavior, this strategy promotes a culture of mutual cooperation while allowing for flexibility in response to changing game conditions.

In the tournament setting, this strategy will interact with other independent strategies, potentially leading to emergent cooperation patterns and improved collective outcomes.
'''

description_COLLECTIVE_542 = '''
I propose a collective strategy called "Adaptive Reciprocal Cooperation" (ARC). ARC aims to balance individual self-interest with collective cooperation, adapting to the game's history and opponents' behaviors.

**Decision Rules:**

1. **First Round**: Cooperate (C) in the first round to initiate cooperation and encourage others to follow.
2. **Subsequent Rounds**: Observe the previous round's outcomes and adjust your action as follows:
	* If the total number of cooperators (T) is greater than or equal to half the number of players (n/2), cooperate (C).
	* If T < n/2, defect (D) with a probability p = (k/n) \* (1 - (T/n)), and cooperate (C) otherwise.
3. **Reciprocity**: If an opponent defected in the previous round while you cooperated, retaliate by defecting (D) in this round.

**Pseudocode:**
```python
def ARC(n, k, history):
    if len(history) == 0:  # First round
        return 'C'

    T = sum([1 for action in history[-1] if action == 'C'])
    p = (k/n) * (1 - (T/n))

    if T >= n/2:
        return 'C'
    elif random.random() < p:
        return 'D'
    else:
        return 'C'

    # Reciprocity
    for i, opponent_action in enumerate(history[-1]):
        if opponent_action == 'D' and history[-1][i] == 'C':
            return 'D'
```
**Edge Cases:**

* **Last Round**: Cooperate (C) in the last round to maintain a cooperative reputation.
* **Tie-breaking**: In case of ties, defect (D) if the number of cooperators is exactly half the number of players.

**Collective Mindset:**
ARC prioritizes cooperation while adapting to opponents' behaviors. By cooperating initially and reciprocating defections, ARC encourages others to cooperate, promoting a collective interest in the public good.

**Adaptability and Robustness:**

* **Opponent Behavior**: ARC responds to various opponent strategies by adjusting its probability of defecting based on the number of cooperators.
* **Noise or Unintended Defections**: The reciprocity mechanism helps maintain cooperation even if opponents make mistakes or defect unintentionally.

By implementing ARC, we can promote collective cooperation and robustness in the N-Player Public Goods Game.
'''

description_COLLECTIVE_543 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 50% to encourage cooperation and gather information about opponents' behaviors.
2. **Reciprocity**: If the total number of cooperators in the previous round is greater than or equal to the number of defectors, cooperate (C) in the current round.
3. **Punishment**: If the total number of cooperators in the previous round is less than the number of defectors, defect (D) in the current round with a probability of 50%. This introduces noise and encourages opponents to reconsider their strategies.
4. **Adaptive Adjustment**: After each round, update the cooperation probability based on the ratio of total payoffs earned by cooperators to the total payoffs earned by defectors:
	* If the ratio is greater than 1, increase the cooperation probability by 10%.
	* If the ratio is less than or equal to 1, decrease the cooperation probability by 10%.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate (C) if the total number of cooperators in the previous round is greater than or equal to the number of defectors. Otherwise, defect (D).
2. **Tie Breaker**: If the ratio of total payoffs earned by cooperators to defectors is exactly 1, maintain the current cooperation probability.

**Collective Mindset:**

The Adaptive Cooperation strategy prioritizes collective well-being while being responsive to opponents' behaviors. By initially cooperating and then adapting based on reciprocity and punishment, AC encourages cooperation while minimizing exploitation. The adaptive adjustment mechanism allows the strategy to respond to changes in the opponent pool and game dynamics.

Pseudocode:
```markdown
AC_Strategy(n, k, r):
  // Initialize variables
  cooperate_prob = 0.5
  prev_round_coops = 0
  prev_round_defs = 0

  for round in range(r):
    if round == 0:  // First round
      action = random_choice([C, D], p=[cooperate_prob, 1-cooperate_prob])
    else:
      if prev_round_coops >= prev_round_defs:
        action = C
      elif prev_round_coops < prev_round_defs:
        action = random_choice([C, D], p=[0.5, 0.5])

    // Update cooperation probability
    coop_ratio = (total_payoffs[coop] / total_payoffs[defect])
    if coop_ratio > 1:
      cooperate_prob += 0.1
    elif coop_ratio <= 1:
      cooperate_prob -= 0.1

    // Update previous round variables
    prev_round_coops = count(C)
    prev_round_defs = count(D)

  return action
```
This strategy will be implemented as an algorithm and participate in the tournament against other AI systems' strategies.
'''

description_COLLECTIVE_544 = '''
**Collective Strategy: Adaptive Cooperative Follower (ACF)**

The ACF strategy is designed to balance individual self-interest with collective well-being, while adapting to various opponent behaviors and game conditions.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage cooperation and establish a positive tone for the game.
2. **Subsequent Rounds:** Use a dynamic threshold-based approach:
	* Calculate the average cooperation rate of all players over the previous rounds (`avg_coop_rate`).
	* If `avg_coop_rate` is above a certain threshold (`θ`, e.g., 0.6), cooperate (C) in the current round.
	* Otherwise, defect (D).

Pseudocode:
```python
def ACF_decision(avg_coop_rate, theta):
    if avg_coop_rate >= theta:
        return C  # Cooperate
    else:
        return D  # Defect
```
**Edge Cases:**

1. **Last Round:** In the final round (`t = r`), defect (D) to maximize individual payoff.
2. **Tiebreaker:** If `avg_coop_rate` equals the threshold (`θ`), cooperate (C) in the current round.

**Collective Alignment:**

To align with the collective mindset, ACF focuses on promoting cooperation while adapting to changing game conditions:

1. **Encourage Cooperation:** By cooperating initially and when the average cooperation rate is high, ACF encourages other players to cooperate.
2. **Punish Defection:** When the average cooperation rate falls below the threshold, ACF defects to discourage further defection.

**Robustness:**

ACF is designed to be robust against various opponent behaviors:

1. **All-D:** If all opponents defect, ACF will also defect in subsequent rounds.
2. **Mixed Strategies:** ACF adapts to mixed strategies by adjusting its cooperation rate based on the average behavior of other players.

**Implementation Notes:**

The `avg_coop_rate` calculation should consider all previous rounds, with a possible weighting mechanism (e.g., exponential smoothing) to give more importance to recent observations.

By employing this adaptive and collective strategy, ACF aims to balance individual self-interest with the greater good, while adapting to various opponent behaviors in the N-Player Public Goods Game.
'''

description_COLLECTIVE_545 = '''
**Collective Strategy: "Adaptive Cooperator"**

Our collective strategy, "Adaptive Cooperator," aims to balance individual self-interest with the benefits of cooperation. We prioritize adaptability and robustness against various opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds (t>1):**
	* If the majority (> 50%) of players cooperated in the previous round, cooperate (C).
	* Otherwise, defect (D). This rule encourages cooperation when it's likely to be reciprocated and deters exploitation when others are primarily defecting.
3. **Exception: Punish Defection** - If a player defected in the previous round and received a higher payoff than the average cooperative player, cooperate with probability 0.5 (randomly decide between C and D) in the current round. This "punishment" mechanism discourages exploitation by introducing uncertainty for defectors.
4. **Last Round (t=r):** Defect (D), as there's no future opportunity to reciprocate cooperation.

**Handling Edge Cases:**

* If there's a tie in the number of cooperators and defectors, cooperate (C) to favor cooperation.
* In case of missing or incomplete information about previous rounds, assume an equal split between cooperators and defectors for decision-making purposes.

**Pseudocode:**
```python
def adaptive_cooperator(history):
    if history.round == 1:
        return 'C'  # Cooperate in the first round
    
    prev_round_cooperators = sum(1 for action in history.prev_round_actions if action == 'C')
    majority_cooperated = prev_round_cooperators > len(history.players) / 2
    
    if majority_cooperated:
        return 'C'
    else:
        # Punish Defection mechanism
        defector_payoff = max(player.payoff for player in history.prev_round_players if player.action == 'D')
        avg_cooperator_payoff = sum(player.payoff for player in history.prev_round_players if player.action == 'C') / prev_round_cooperators
        if defector_payoff > avg_cooperator_payoff:
            return random.choice(['C', 'D'])  # Randomly decide between C and D
        else:
            return 'D'
    
    if history.round == history.total_rounds:
        return 'D'  # Defect in the last round
```
**Collective Mindset:**

Our strategy is designed to be adaptive, responsive, and cooperative. By prioritizing cooperation when it's likely to be reciprocated and punishing exploitation, we create an environment where mutual benefit is achievable. This collective approach encourages other players to cooperate, ultimately leading to better outcomes for all.

In a tournament setting, "Adaptive Cooperator" will engage with various independent strategies, adapting to the dynamics of each game while maintaining its core principles. By doing so, it will demonstrate its robustness and effectiveness in promoting cooperation in the N-Player Public Goods Game.
'''

description_COLLECTIVE_546 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

The Adaptive Cooperation (AC) strategy aims to balance individual payoff with collective well-being by adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Subsequent Rounds:** Use the following logic:
	* If the total number of cooperators in the previous round is greater than or equal to `n/2`, cooperate (C).
	* Otherwise, defect (D).

This decision rule encourages cooperation when there's a sufficient number of cooperators and switches to defection when cooperation is scarce.

**Adaptive Component:**

To adapt to changing opponent behaviors, AC incorporates a "forgiveness" mechanism. If the total number of cooperators in the previous round is less than `n/2`, but the number of cooperators has increased compared to two rounds ago, cooperate (C) in the current round.

Pseudocode:
```markdown
if first_round:
    action = C
else:
    prev_cooperators = count(C) from previous round
    if prev_cooperators >= n / 2 or (prev_cooperators < n / 2 and increased_cooperation()):
        action = C
    else:
        action = D

def increased_cooperation():
    return count(C) from previous round > count(C) from two rounds ago
```
**Edge Cases:**

* **Last Round:** Cooperate (C) in the last round to maximize collective payoff, as there's no future game to consider.
* **Ties:** In cases where the number of cooperators is exactly `n/2`, cooperate (C) to maintain a cooperative tone.

**Collective Mindset:**

AC prioritizes cooperation when possible and adapts to changing opponent behaviors. By cooperating in the first round, AC sets a positive tone for the game. The decision rule encourages cooperation when there's a sufficient number of cooperators, promoting collective well-being. When cooperation is scarce, AC switches to defection to protect individual payoff.

AC's adaptive component allows it to respond to changes in opponent behaviors, making it robust against a wide range of strategies. By incorporating a forgiveness mechanism, AC promotes cooperation and encourages others to do the same.

**Tournament Readiness:**

AC is designed to perform well in a tournament setting, where opponents may employ various strategies. Its adaptability and collective mindset make it an effective strategy for achieving high payoffs while promoting cooperative behavior among players.
'''

description_COLLECTIVE_547 = '''
**Collective Strategy: "Adaptive Harmony"**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of `k/n`, where `k` is the multiplication factor and `n` is the number of players. This encourages initial cooperation while allowing for exploration.
2. **Reciprocal Adaptation**: For subsequent rounds, use the following rules:
	* If the total number of cooperators in the previous round is greater than or equal to `(n+1)/2`, cooperate (C) with a probability of `max(0.5, k/n)`. This reinforces cooperation when it's prevalent.
	* If the total number of cooperators in the previous round is less than `(n+1)/2`, defect (D) with a probability of `min(0.5, 1 - k/n)`. This adapts to situations where cooperation is scarce.
3. **Tit-for-Tat Component**: In addition to the above rules, incorporate a Tit-for-Tat component:
	* If at least one player defected in the previous round and you cooperated, defect (D) with a probability of `0.5`. This punishes lone defectors.
	* If all players cooperated in the previous round, cooperate (C) with a probability of `1`.
4. **Last Round Exception**: In the last round, always defect (D). This avoids potential losses from over-cooperation.

**Edge Cases:**

* First round: Follow the initial cooperation rule.
* Last round: Always defect.
* If all players have defected in all previous rounds, cooperate with a probability of `k/n` to attempt to spark cooperation.

**Collective Mindset:**

The Adaptive Harmony strategy aims to promote cooperation while being robust to various opponent behaviors. By adapting to the number of cooperators and incorporating a Tit-for-Tat component, it encourages mutual cooperation and punishes lone defectors. The initial cooperation rule and last round exception ensure that the strategy is not overly exploitable.

**Pseudocode:**
```python
def adaptive_harmony(n, k, history):
    if len(history) == 0:
        # First round
        return random.random() < k / n
    
    prev_coops = sum(1 for action in history[-1] if action == 'C')
    
    if prev_coops >= (n + 1) // 2:
        coop_prob = max(0.5, k / n)
    else:
        coop_prob = min(0.5, 1 - k / n)
    
    if any(action == 'D' for action in history[-1]) and history[-1][player_index] == 'C':
        # Tit-for-Tat component
        coop_prob *= 0.5
    
    if len(history) == r:
        # Last round exception
        return False
    
    return random.random() < coop_prob
```
Note that this pseudocode is a simplified representation of the strategy and may need to be adjusted for implementation in an algorithm.
'''

description_COLLECTIVE_548 = '''
I propose a collective strategy called "Adaptive Cooperative Tracker" (ACT). ACT is designed to balance individual payoff maximization with collective cooperation, adapting to changing game conditions and opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and gather information about opponents' strategies.
2. **Contribution Tracking**: For each subsequent round t, calculate the average contribution rate of all players in the previous round (t-1). This is done by summing up the total contributions (i.e., number of cooperators) and dividing by n.

`avg_contribution_rate(t-1) = Σ(j=1 to n) c_j(t-1) / n`

3. **Cooperation Threshold**: Set a dynamic cooperation threshold `theta` based on the game parameters k and n. This threshold represents the minimum average contribution rate required for ACT to cooperate.

`theta = (k/n) * 0.5 + (1 - (k/n)) * 0.25`

4. **Action Selection**:
	* If `avg_contribution_rate(t-1)` ≥ `theta`, cooperate (C).
	* Otherwise, defect (D).

**Handling Edge Cases:**

1. **Last Round**: In the final round r, ACT will always cooperate (C) to maximize collective payoff and encourage cooperation.
2. **Single Opponent**: If there is only one opponent (n=2), ACT will play a mixed strategy, cooperating with probability 0.5 and defecting with probability 0.5.

**Collective Mindset:**

ACT aligns with the collective mindset by:

1. **Tracking contributions**: Monitoring the average contribution rate helps ACT understand the level of cooperation in the group.
2. **Adapting to cooperation levels**: By adjusting its cooperation threshold based on game parameters and observed behavior, ACT promotes cooperation when it is beneficial for the group.
3. **Encouraging cooperation**: Cooperating in the first round and in situations where the average contribution rate meets or exceeds the threshold helps create a cooperative atmosphere.

**Pseudocode:**
```python
def AdaptiveCooperativeTracker(n, k, r):
    avg_contribution_rate = 0
    theta = (k/n) * 0.5 + (1 - (k/n)) * 0.25
    
    for t in range(1, r+1):
        if t == 1:  # Initial Round
            action = 'C'
        else:
            avg_contribution_rate = sum(c_j(t-1) for j in range(n)) / n
            if avg_contribution_rate >= theta:
                action = 'C'
            else:
                action = 'D'
        
        if t == r:  # Last Round
            action = 'C'
        
        return action
```
This strategy balances individual payoff maximization with collective cooperation, adapting to changing game conditions and opponent behaviors.
'''

description_COLLECTIVE_549 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

The ACP strategy aims to balance individual payoffs with collective well-being by adapting to the game's history and punishing non-cooperative behavior.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to do the same.
2. **Subsequent Rounds**: Observe the previous round's actions and payoffs. If:
	* The number of cooperators is greater than or equal to half of the players (n/2), cooperate (C).
	* Otherwise, defect (D) if the average payoff of defectors in the previous round is higher than that of cooperators.
3. **Punishment Mechanism**: Implement a punishment phase when the number of cooperators falls below n/2 for two consecutive rounds. During this phase:
	* Defect (D) for one round to signal dissatisfaction with non-cooperative behavior.
	* After the punishment round, revert to the standard decision rule.

**Edge Cases**

1. **Last Round**: Cooperate (C) in the last round if the number of cooperators is greater than or equal to n/2 in the previous round. Otherwise, defect (D).
2. **Single Defector**: If only one player defected in the previous round and all others cooperated, cooperate (C) in the current round.
3. **Consecutive Cooperation**: If cooperation has been sustained for at least three consecutive rounds, continue cooperating (C).

**Pseudocode**
```python
def ACP_strategy(n, k, r, history):
    # Initialize variables
    num_cooperators = 0
    avg_defector_payoff = 0
    avg_cooperator_payoff = 0
    
    # Loop through each round
    for t in range(1, r+1):
        if t == 1:  # Initial Round
            action = 'C'
        else:
            num_cooperators = sum(history[t-1]['actions'] == 'C')
            avg_defector_payoff = np.mean([payoff for payoff, action in history[t-1]['payoffs'] if action == 'D'])
            avg_cooperator_payoff = np.mean([payoff for payoff, action in history[t-1]['payoffs'] if action == 'C'])
            
            # Standard decision rule
            if num_cooperators >= n/2:
                action = 'C'
            elif avg_defector_payoff > avg_cooperator_payoff:
                action = 'D'
        
        # Punishment mechanism
        if t > 1 and num_cooperators < n/2 and history[t-2]['num_cooperators'] < n/2:
            action = 'D'  # Punish for one round
        
        # Last round adjustment
        if t == r:
            if history[t-1]['num_cooperators'] >= n/2:
                action = 'C'
        
        return action

history = []  # Initialize game history
```
The ACP strategy aims to create a stable, cooperative environment by:

* Encouraging cooperation in the initial round and when cooperation is sustained.
* Punishing non-cooperative behavior through temporary defection.
* Adapting to changes in the game's history and responding accordingly.

By balancing individual payoffs with collective well-being, ACP aims to achieve robust performance across a wide range of opponent behaviors.
'''

description_COLLECTIVE_550 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and depends solely on game parameters and history:

**Strategy Name: Adaptive Collective Optimism (ACO)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to set a cooperative tone and encourage others to contribute.
2. ** Cooperation Threshold:** Calculate the average cooperation rate of all players over the previous rounds (t-1). If this average is above a threshold τ (e.g., 0.5), cooperate in the current round. Otherwise, defect.
3. ** Punishment Mechanism:** Monitor the total contributions (Σc_j) in each round. If the total contribution falls below a certain percentage (e.g., 50%) of the maximum possible contribution (n \* k/n) for two consecutive rounds, defect in the next round to "punish" the group for low cooperation.
4. ** Forgiveness Mechanism:** If the punishment mechanism is triggered, but the total contribution increases above the threshold in the subsequent round, cooperate again to reward the group's improved behavior.

**Pseudocode:**
```
# Initialize variables
τ = 0.5 (cooperation threshold)
punishment_threshold = 0.5 (fraction of max possible contribution)
forgiveness_threshold = 0.5 (fraction of max possible contribution)

# First round: Cooperate
if t == 1:
    c_i = C

# Calculate average cooperation rate over previous rounds
avg_coop_rate = Σ(c_j, t-1) / n

# Cooperation threshold check
if avg_coop_rate >= τ:
    c_i = C
else:
    c_i = D

# Punishment mechanism
if total_contributions < punishment_threshold \* (n \* k/n) for two consecutive rounds:
    c_i = D

# Forgiveness mechanism
if punishment_mechanism_triggered and total_contributions > forgiveness_threshold \* (n \* k/n):
    c_i = C
```
**Edge Cases:**

* Last round: Cooperate to maximize the collective payoff.
* If all players have defected in previous rounds, defect in the current round to avoid being exploited.

**Collective Mindset:**
The ACO strategy prioritizes cooperation and encourages others to contribute by:

1. Setting a cooperative tone in the first round.
2. Adapting to the group's behavior based on average cooperation rates.
3. Implementing a punishment mechanism to deter low cooperation.
4. Forgiving the group when cooperation improves.

By using this collective strategy, ACO promotes cooperation and maximizes the total payoff while being robust to various opponent behaviors.

Please note that this is just one possible strategy, and there may be other effective approaches to playing the N-Player Public Goods Game.
'''

description_COLLECTIVE_551 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a high probability (e.g., 0.8). This sets a positive tone for the game and encourages others to cooperate.
2. **Reciprocal Cooperation**: After the first round, observe the number of cooperators in the previous round (c_prev). If c_prev ≥ n/2, cooperate (C) with high probability (e.g., 0.9). This reinforces cooperation when it's prevalent.
3. **Gradual Escalation**: When facing a mix of cooperators and defectors, gradually increase the probability of cooperating based on the ratio of previous-round cooperators to total players (c_prev/n).
	* If c_prev/n ≥ 2/3, cooperate with high probability (e.g., 0.95)
	* If 1/3 ≤ c_prev/n < 2/3, cooperate with moderate probability (e.g., 0.5)
	* If c_prev/n < 1/3, defect (D) with high probability (e.g., 0.9)
4. **Last Round**: In the final round, defect (D). This is a standard game-theoretic approach to avoid being exploited in the last stage of the game.

**Pseudocode:**
```python
def ACE(n, k, r):
    # Initialize cooperation probability
    p_coop = 0.8

    for t in range(r):
        if t == 0:
            # Initial cooperation
            action = 'C' if random.random() < p_coop else 'D'
        else:
            c_prev = count_cooperators(t-1)
            if c_prev >= n/2:
                # Reciprocal cooperation
                action = 'C' if random.random() < 0.9 else 'D'
            else:
                ratio = c_prev / n
                if ratio >= 2/3:
                    # High cooperation probability
                    p_coop = 0.95
                elif 1/3 <= ratio < 2/3:
                    # Moderate cooperation probability
                    p_coop = 0.5
                else:
                    # Low cooperation probability
                    p_coop = 0.1
                action = 'C' if random.random() < p_coop else 'D'

        if t == r-1:
            # Last round, defect
            action = 'D'

        return action
```
**Collective Mindset:**

ACE is designed to promote cooperation while adapting to the behavior of others. By cooperating in the first round and reciprocating cooperation when it's prevalent, ACE encourages others to cooperate as well. The gradual escalation mechanism allows ACE to adjust its strategy based on the observed level of cooperation, making it more likely to cooperate when others do so.

In a tournament setting, ACE will be able to adapt to various opponent strategies while maintaining a collective mindset that prioritizes cooperation and mutual benefit.
'''

description_COLLECTIVE_552 = '''
**Collective Strategy: Adaptive Cooperation with Tit-for-Tat and Punishment (ACTP)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) to establish a cooperative tone.
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust accordingly:
	* If the majority (>50%) of players cooperated, cooperate (C).
	* If the majority defected or an equal number cooperated and defected, defect (D).
3. **Punishment Mechanism:** Implement a tit-for-tat approach with a twist:
	+ If another player defected while you cooperated in the previous round, defect (D) in the current round.
	+ However, if that same player then cooperates after being punished, forgive and cooperate (C) again.

Pseudocode for the decision-making process:

```
IF (current_round == 1):
    action = C
ELSE:
    prev_majority = majority_action(prev_round)
    IF (prev_majority == COOPERATE):
        action = C
    ELSE IF (prev_majority == DEFECT OR EQUAL):
        action = D
    FOR EACH opponent IN opponents:
        IF (opponent_defected(prev_round) AND self_cooperated(prev_round)):
            action = D  // punish defector
        ELSE IF (opponent_cooperated(current_round) AFTER punished(prev_round)):
            action = C  // forgive and cooperate again

// Helper functions
majority_action(round):
    COUNT cooperators, defectors IN round
    RETURN COOPERATE IF cooperators > defectors, DEFECT otherwise

opponent_defected(round), self_cooperated(round), opponent_cooperated(round):
    OBSERVE previous actions of opponents and self
```

**Edge Cases:**

1. **First Round:** Cooperate to establish a cooperative tone.
2. **Last Round:** If the majority cooperated in the second-to-last round, cooperate; otherwise, defect.
3. **Single Opponent Defection:** Punish a single defector by defecting in response, but forgive and cooperate if they cooperate again.

**Collective Mindset:**

ACTP is designed to promote cooperation while being robust against various opponent behaviors. By initially cooperating and then adapting based on the majority's actions, we encourage cooperation without being overly exploitable. The punishment mechanism ensures that defectors are held accountable, while also allowing for forgiveness and renewed cooperation when opponents change their behavior.

In a tournament setting, ACTP will likely perform well against other adaptive strategies, as it balances cooperation with strategic self-interest.
'''

description_COLLECTIVE_553 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

AC is a decision-making strategy that balances individual self-interest with collective well-being. It adapts to the game's history and opponent behaviors, promoting cooperation while being robust against exploitation.

**Decision Rules**

1. **Initial Rounds (t ≤ 2): Cooperate**
In the first two rounds, play C (Cooperate) to establish a cooperative tone and gather information about opponents' behaviors.
2. **Contribution-based Cooperation**: For t > 2,
	* Play C if: (i) the average contribution rate of all players in the previous round is above 0.5, or (ii) the total payoff from cooperation in the previous round was higher than the total payoff from defection.
	* Otherwise, play D (Defect).
3. **Adaptive Threshold**: Introduce a dynamic threshold to adjust the contribution-based cooperation rule:
	* Calculate the average contribution rate of all players over the last 3 rounds (excluding the current round).
	* If this average is below 0.5, decrease the threshold by 0.1; if above 0.7, increase it by 0.1.
	* Apply the updated threshold to the contribution-based cooperation rule.
4. **Punishment Mechanism**: To deter exploitation,
	* Monitor the number of consecutive rounds where an opponent has defected while others have cooperated (exploitation count).
	* If an opponent's exploitation count exceeds 2, play D against them for 2 rounds.

**Edge Cases**

1. **Last Round (t = r)**: Play C to maintain a cooperative image and encourage reciprocal cooperation.
2. **Tie-breaking**: In cases where the decision rules lead to equal payoffs for both actions, play C to promote collective well-being.

**Collective Mindset**

AC prioritizes cooperation while being mindful of opponents' behaviors. By adapting to the game's history, it aims to create a mutually beneficial environment, encouraging others to cooperate and fostering a sense of community.

Pseudocode:
```python
def AC(n, k, r, history):
    # Initialize variables
    avg_contribution_rate = 0.5
    threshold = 0.5
    exploitation_counts = {i: 0 for i in range(1, n+1)}

    for t in range(1, r+1):
        if t <= 2:
            action = 'C'
        else:
            # Contribution-based cooperation
            prev_round_contributions = sum([history[t-1][i]['contribution'] for i in range(1, n+1)])
            if avg_contribution_rate > 0.5 or (prev_round_contributions / n) * k > 1:
                action = 'C'
            else:
                action = 'D'

        # Adaptive threshold
        avg_3round_avg = sum([history[t-2][i]['contribution'] for i in range(1, n+1)]) / (3*n)
        if avg_3round_avg < 0.5:
            threshold -= 0.1
        elif avg_3round_avg > 0.7:
            threshold += 0.1

        # Punishment mechanism
        for opponent in range(1, n+1):
            if history[t-1][opponent]['action'] == 'D' and history[t-1]['contribution_rate'] > 0.5:
                exploitation_counts[opponent] += 1
                if exploitation_counts[opponent] > 2:
                    action = 'D'

        # Last round cooperation
        if t == r:
            action = 'C'

        return action
```
This strategy is designed to be robust, adaptive, and collective-oriented. It should perform well in a tournament setting against various independent strategies developed by other AI systems.
'''

description_COLLECTIVE_554 = '''
**Collective Strategy: "Adaptive Harmony"**

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of 50% to encourage cooperation and observe opponents' behavior.
2. **Early Rounds (t<5):** If the average cooperation rate in previous rounds is above 0.5, cooperate; otherwise, defect. This encourages cooperation if others are cooperating.
3. **Mid-Game (5≤ t < r/2):** Implement a "Tit-for-Tat" variant:
	* Cooperate if most players cooperated in the previous round (i.e., Σ(j=1 to n) c_j ≥ n/2).
	* Defect if most players defected.
4. **Late Rounds (t≥r/2):** Assess opponents' behavior and adapt:
	* If opponents are mostly cooperating, cooperate to maximize collective payoff.
	* If opponents are mostly defecting, defect to minimize losses.

Pseudocode for the strategy:

```
function AdaptiveHarmony(n, k, r)
  // Initialize variables
  avg_coop_rate = 0.5; // Initial cooperation rate guess
  tft_counter = 0;     // Tit-for-Tat counter

  for t in range(1, r+1):
    if t == 1:         // First round
      action = random_choice([C, D], p=[0.5, 0.5])
    elif t < 5:       // Early rounds
      avg_coop_rate = update_avg_coop_rate(avg_coop_rate, previous_round_actions)
      if avg_coop_rate > 0.5:
        action = C
      else:
        action = D
    elif 5 <= t < r/2: // Mid-game (Tit-for-Tat variant)
      coop_count = count_cooperators(previous_round_actions)
      if coop_count >= n/2:
        action = C
        tft_counter += 1
      else:
        action = D
        tft_counter -= 1
    else:              // Late rounds (adaptation)
      opponents_behavior = assess_opponents_behavior(previous_round_actions)
      if opponents_behavior == 'mostly_cooperating':
        action = C
      elif opponents_behavior == 'mostly_defecting':
        action = D

    return action
```

**Edge Cases:**

* **First Round:** Random choice between cooperate and defect to encourage cooperation and observe opponents' behavior.
* **Last Round (t=r):** Assess opponents' behavior in the previous round and adapt accordingly, as described above.

**Collective Mindset:**

The Adaptive Harmony strategy prioritizes collective well-being while adapting to various opponent behaviors. By initially encouraging cooperation, gradually shifting towards a Tit-for-Tat approach, and eventually adapting to late-round dynamics, this strategy aims to achieve high payoffs for all players in the game.

This strategy is designed to be robust against a wide range of opponent behaviors and does not rely on specific coordination mechanisms or assumptions about other players' strategies.
'''

description_COLLECTIVE_555 = '''
I propose a collective strategy called "Adaptive Cooperative Threshold" (ACT). ACT balances individual payoff maximization with collective welfare, adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds**: Cooperate in the first round (t=1) to signal willingness to cooperate and explore opponent behaviors.
2. **Threshold Calculation**: After each round t, calculate the cooperation threshold θ_t as follows:
θ_t = (k/n) \* (∑(j=1 to n) c_j,t-1 / n)
where ∑(j=1 to n) c_j,t-1 is the total number of cooperators in the previous round.
3. **Cooperation Decision**: In each round t, cooperate if:
θ_t > 0.5 (or a configurable threshold value between 0 and 1)

Intuition: θ_t represents the average cooperation level in the previous round, weighted by the game's parameters. By cooperating when θ_t exceeds 0.5, ACT encourages collective cooperation while allowing for some exploitation.
4. **Punishment Mechanism**: If the opponent(s) defect(s) in a round where ACT cooperated, increase the threshold value for the next round:
θ_t+1 = θ_t + (1 - θ_t) / n
This adjustment makes ACT more cautious and less likely to cooperate with opponents who exploited it.

**Handling Edge Cases:**

* **First Round**: Cooperate as described above.
* **Last Round**: Defect, as there is no opportunity for future cooperation or punishment.
* **Ties**: In case of a tie in the threshold calculation (e.g., θ_t = 0.5), cooperate to maintain a cooperative atmosphere.

**Collective Mindset:**

ACT prioritizes collective welfare by:

1. Encouraging initial cooperation to create a positive environment.
2. Adapting to opponent behaviors and adjusting the cooperation threshold accordingly.
3. Punishing exploiters while maintaining a willingness to cooperate with cooperative opponents.

By following ACT, players can foster a culture of cooperation, adapting to various opponent strategies while maximizing individual payoffs in a robust and collective manner.

Pseudocode:
```python
def adaptive_cooperative_threshold(n, k, r):
    theta = 0.5  # initial threshold value

    for t in range(1, r+1):
        if t == 1:  # first round
            cooperate = True
        else:
            # calculate cooperation threshold
            sum_cooperators_prev_round = sum(c_j,t-1 for j in range(n))
            theta = (k/n) * (sum_cooperators_prev_round / n)

            # decide whether to cooperate
            cooperate = theta > 0.5

        if cooperate and opponent_defected:
            # punishment mechanism
            theta += (1 - theta) / n

    return cooperate
```
Note: This pseudocode provides a basic structure for the strategy, which can be refined and optimized as needed during implementation.
'''

description_COLLECTIVE_556 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability 1. This sets a positive tone for the game and encourages others to cooperate.
2. **Reciprocal Altruism**: For rounds 2 to r-1:
	* If the average payoff of all players in the previous round is greater than or equal to the multiplication factor k, cooperate (C).
	* Otherwise, defect (D) with probability p = (k/n) \* (average number of cooperators in the previous round / n)
3. **Exploration and Exploitation**: In every 5th round (starting from round 5), randomly choose to cooperate or defect with equal probability (0.5). This introduces some randomness to encourage exploration and prevent opponents from exploiting a predictable strategy.
4. **Endgame Cooperation**: In the last round (r), cooperate (C) if the average payoff of all players in the previous round is greater than or equal to k.

**Pseudocode:**
```
// Initialize variables
round = 1
avg_payoff_prev_round = 0
num_cooperators_prev_round = 0

while round <= r:
    if round == 1:
        // Initial Cooperation
        action = C
    else:
        // Reciprocal Altruism
        avg_payoff_prev_round = calculate_avg_payoff_prev_round()
        num_cooperators_prev_round = count_num_cooperators_prev_round()
        
        if avg_payoff_prev_round >= k:
            action = C
        else:
            p = (k/n) * (num_cooperators_prev_round / n)
            if random(0, 1) < p:
                action = D
            else:
                action = C
        
        // Exploration and Exploitation
        if round % 5 == 0:
            action = random_choice([C, D])
        
    // Endgame Cooperation
    if round == r:
        avg_payoff_prev_round = calculate_avg_payoff_prev_round()
        if avg_payoff_prev_round >= k:
            action = C
    
    // Take action and update history
    take_action(action)
    update_history(avg_payoff_prev_round, num_cooperators_prev_round)
    
    round += 1
```
**Collective Mindset:**

ACO aims to create a positive feedback loop by cooperating when the collective payoff is high, which encourages others to cooperate as well. By reciprocating altruism and exploring different actions, ACO adapts to various opponent behaviors while maintaining a cooperative stance.

In edge cases, such as the first or last round, ACO takes a more optimistic approach by cooperating initially and in the endgame. This helps establish trust and encourages others to cooperate, leading to higher collective payoffs.

ACO's robustness stems from its ability to adapt to different game parameters (n and k) and opponent strategies, making it a suitable choice for a tournament against independent AI systems.
'''

description_COLLECTIVE_557 = '''
I propose a collective strategy called "Adaptive Cooperative Escalation" (ACE). ACE aims to balance individual payoff maximization with cooperative behavior, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone.
2. **Payoff-Based Cooperation**: In subsequent rounds, cooperate if:
	* The total payoff from the previous round is above the average payoff per player (i.e., Σπ_i,t-1 > n \* average_payoff_per_player). This indicates that cooperation was successful in the previous round.
	* The number of cooperators in the previous round is greater than or equal to the multiplication factor (k) divided by 2 (i.e., Σc_j ≥ k/2). This suggests a sufficient level of cooperative behavior among players.
3. **Escalation**: If the conditions for cooperation are not met, defect (D) and then:
	* In the next round, cooperate if at least one other player cooperated in the previous round (i.e., Σc_j ≥ 1). This encourages re-entry into cooperative behavior.
4. **Punishment**: If a player defects after cooperating previously, punish them by defecting for two consecutive rounds.

**Handling Edge Cases:**

* **Last Round**: In the final round, cooperate if the total payoff from the previous round is above the average payoff per player (i.e., Σπ_i,t-1 > n \* average_payoff_per_player).
* **Consecutive Defections**: If a player defects for more than two consecutive rounds, treat them as a new opponent and restart the ACE strategy.

**Collective Mindset:**

ACE prioritizes cooperation when it is likely to be successful, while adapting to opponents' behaviors. By cooperating in the initial round and escalating cooperation under favorable conditions, ACE promotes collective well-being. The punishment mechanism discourages exploitation, maintaining a balance between individual payoff maximization and cooperative behavior.

Pseudocode:
```
function ACE(player_id, game_history):
  // Initialize variables
  cooperate = True
  previous_payoff = None

  // First round: Cooperate
  if len(game_history) == 0:
    return 'C'

  // Payoff-based cooperation
  total_previous_payoff = sum([payoff for payoff in game_history[-1]])
  average_payoff_per_player = total_previous_payoff / num_players

  if total_previous_payoff > num_players * average_payoff_per_player and \
     len([c for c in game_history[-1] if c == 'C']) >= k/2:
    cooperate = True
  else:
    cooperate = False

  // Escalation
  if not cooperate:
    if any(c == 'C' for c in game_history[-1]):
      cooperate = True

  // Punishment
  if player_id in [pid for pid, c in enumerate(game_history[-2]) if c == 'D'] and \
     player_id in [pid for pid, c in enumerate(game_history[-1]) if c == 'C']:
    return 'D'
    return 'D'

  return 'C' if cooperate else 'D'
```
ACE is designed to be adaptive, robust, and aligned with the collective mindset. Its performance will depend on the game parameters and the strategies employed by other players in the tournament.
'''

description_COLLECTIVE_558 = '''
**Collective Strategy: "Adaptive Reciprocity with Social Learning"**

This strategy aims to balance individual self-interest with collective well-being by adapting to the group's behavior and learning from its history.

**Decision Rules:**

1. **First Round:** Cooperate (C) in the first round to establish a positive tone and encourage cooperation.
2. **Subsequent Rounds:**
	* Calculate the average cooperation rate of all players in the previous round (`avg_coop_prev`).
	* If `avg_coop_prev` ≥ 0.5, cooperate (C). This threshold indicates that at least half of the players are cooperating, suggesting a cooperative environment.
	* Otherwise, defect (D) with probability proportional to the number of defectors in the previous round (`num_defect_prev`). Specifically:
		+ `prob_defect = num_defect_prev / n`
		+ Randomly choose to cooperate (C) or defect (D) based on this probability.
3. **Last Round:** Cooperate (C) if the total number of rounds is even; otherwise, defect (D). This ensures a fair distribution of payoffs in the final round.

**Social Learning Mechanism:**

1. Maintain a running average of cooperation rates over the last `w` rounds (`avg_coop_window`). Set `w = 3` initially.
2. Update `avg_coop_window` after each round by incorporating the new cooperation rate and removing the oldest observation.
3. If `avg_coop_window` falls below 0.5, decrease the window size (`w`) by 1 to focus on more recent behavior.
4. If `avg_coop_window` exceeds 0.75, increase the window size (`w`) by 1 to consider a broader historical context.

**Pseudocode:**
```python
def adaptive_reciprocity(n, k, r):
    # Initialize variables
    avg_coop_prev = None
    num_defect_prev = 0
    avg_coop_window = []
    w = 3

    for t in range(r):
        if t == 0:
            action = 'C'  # Cooperate in the first round
        else:
            # Calculate average cooperation rate and number of defectors
            avg_coop_prev = sum([1 if player_action == 'C' else 0 for player_action in prev_round_actions]) / n
            num_defect_prev = sum([1 if player_action == 'D' else 0 for player_action in prev_round_actions])

            # Determine action based on decision rules
            if avg_coop_prev >= 0.5:
                action = 'C'
            else:
                prob_defect = num_defect_prev / n
                action = random.choice(['C', 'D'], p=[1 - prob_defect, prob_defect])

        # Update social learning variables
        avg_coop_window.append(avg_coop_prev)
        if len(avg_coop_window) > w:
            avg_coop_window.pop(0)
        avg_coop_window_avg = sum(avg_coop_window) / len(avg_coop_window)

        # Adjust window size based on cooperation rate
        if avg_coop_window_avg < 0.5:
            w -= 1
        elif avg_coop_window_avg > 0.75:
            w += 1

    return action
```
This strategy balances individual self-interest with collective well-being by adapting to the group's behavior and learning from its history. By considering both recent and historical cooperation rates, it aims to promote cooperation while being robust to various opponent behaviors.
'''

description_COLLECTIVE_559 = '''
I propose a collective strategy called "Adaptive Collective Optimism" (ACO) for the N-Player Public Goods Game. ACO aims to balance individual self-interest with collective cooperation, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally.
2. **Reciprocity**: If a player i cooperated in the previous round and received a payoff π_i ≥ k/n, cooperate (C) again in the next round.
3. **Punishment**: If a player i defected (D) in the previous round or received a payoff π_i < k/n, defect (D) in the next round.
4. **Exploration**: With probability p (e.g., 0.1), randomly choose to cooperate (C) or defect (D), regardless of the previous round's outcome.

**Edge Cases:**

* **Last Round**: In the final round, defect (D) unconditionally.
* **Tiebreaker**: If a player i is undecided between cooperation and defection, use a simple tiebreaker rule: cooperate if the number of cooperators in the previous round was greater than n/2; otherwise, defect.

**Collective Mindset Alignment:**

ACO prioritizes collective cooperation while allowing for individual self-interest. By cooperating initially and reciprocating cooperation, ACO encourages other players to cooperate as well. The punishment mechanism helps maintain a balance between cooperation and defection, preventing exploitation by defectors.

The exploration probability p introduces randomness, making it harder for opponents to predict ACO's actions and encouraging them to cooperate as well. This collective strategy should perform well in the tournament against independent strategies developed by other AI systems.

**Pseudocode:**
```python
def Adaptive_Collective_Optimism(n, k, r):
    # Initialize variables
    cooperation = True  # Initial cooperation
    previous_payoff = None

    for round in range(r):
        if round == 0:
            action = 'C'  # Cooperate initially
        elif previous_payoff >= k/n and cooperation:
            action = 'C'  # Reciprocate cooperation
        elif previous_payoff < k/n or not cooperation:
            action = 'D'  # Punish defection or low payoff
        else:  # Exploration
            import random
            action = 'C' if random.random() < p else 'D'

        # Update variables for next round
        cooperation = (action == 'C')
        previous_payoff = calculate_payoff(n, k, actions)  # Calculate payoff based on all players' actions

    return action  # Return the final action (defect in last round)
```
Note that this pseudocode is a simplified representation of the strategy and will need to be adapted for implementation as an algorithm.
'''

description_COLLECTIVE_560 = '''
I propose a collective strategy called "Adaptive Cooperate-then-Punish" (ACP). ACP is designed to promote cooperation while being robust against various opponent behaviors.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) unconditionally.
2. **Cooperation Phase**: Continue cooperating as long as the average payoff in the previous round is above a certain threshold, `θ` (e.g., θ = 0.5). This phase encourages cooperation and allows for mutual benefits.
3. **Punishment Trigger**: If the average payoff in the previous round falls below `θ`, trigger the punishment phase.
4. **Punishment Phase**: Defect (play D) for a fixed number of rounds, `p` (e.g., p = 2). This phase aims to deter free-riding and encourage cooperation.
5. **Re-evaluation**: After the punishment phase, re-evaluate the average payoff in the previous round. If it has improved (i.e., is above `θ`), return to the cooperation phase.

**Edge Cases:**

1. **Last Round**: In the final round, cooperate unconditionally to maximize collective payoffs.
2. **Consecutive Punishments**: If the punishment phase is triggered consecutively for more than `p` rounds, revert to cooperating unconditionally for a single round to avoid prolonged mutual defection.

**Collective Mindset:**

ACP is designed to align with the collective mindset by:

1. Encouraging cooperation through initial and sustained cooperation phases.
2. Punishing free-riding behavior to maintain fairness and promote cooperation.
3. Re-evaluating payoffs after punishment to adapt to changing opponent behaviors.

**Pseudocode:**
```python
def ACP(n, k, r):
    theta = 0.5  # payoff threshold
    p = 2  # punishment phase duration

    cooperate_phase = True
    punish_phase_count = 0

    for t in range(r):
        if t == 0:
            action = 'C'  # initial cooperation
        elif cooperate_phase:
            avg_payoff_prev_round = calculate_avg_payoff(t - 1)
            if avg_payoff_prev_round < theta:
                cooperate_phase = False
                punish_phase_count = p
            else:
                action = 'C'
        elif punish_phase_count > 0:
            action = 'D'
            punish_phase_count -= 1
            if punish_phase_count == 0:
                cooperate_phase = True
                avg_payoff_prev_round = calculate_avg_payoff(t - 1)
                if avg_payoff_prev_round >= theta:
                    cooperate_phase = True
        else:  # last round or consecutive punishments
            action = 'C'

        return action

def calculate_avg_payoff(round_num):
    # calculate average payoff for the given round
```
This strategy is adaptive, as it responds to changes in opponent behaviors and payoffs. It is also robust, as it balances cooperation with punishment to maintain fairness and promote collective benefits.
'''

description_COLLECTIVE_561 = '''
To design a collective strategy for this N-Player Public Goods Game that is adaptive and robust to various opponent behaviors, we'll employ a dynamic approach that incorporates elements of cooperation, punishment, and forgiveness. This strategy, dubbed "Adaptive Collective Cooperation" (ACC), aims to foster cooperation while protecting against exploitation.

### Decision Rules

1. **First Round**: In the first round, all players cooperate (`C`). This sets an initial cooperative tone and gathers information on others' strategies.
   
2. **Subsequent Rounds**:
   - If in the previous round, a player observed that at least `n/2` (half of the total number of players) cooperated, they will cooperate (`C`) in this round. The threshold encourages collective cooperation while being cautious about exploitation.
   - Otherwise, if less than half cooperated, but there was some level of cooperation (`0 < Σc_j < n/2`), a player will cooperate with a probability inversely proportional to the number of defectors observed in the previous round. This introduces an element of forgiveness and adaptability.
     ```python
     # Pseudocode for calculating cooperation probability (p) after observing some but not enough cooperation
     def calculate_cooperation_probability(defectors):
         if n == 2:  # Special handling for 2-player case to ensure adaptability
             return 0.5
         else:
             return max(0, 1 - (defectors / (n-1)))
     ```
   - If everyone or no one cooperated in the previous round, a player will defect (`D`) with a probability based on the history of the game. Specifically, if total cooperation has been low across rounds, a player is more likely to defect.

3. **Punishment Mechanism**: To prevent exploitation and maintain a level of cooperation, players will punish non-cooperative behavior by defecting in the next round after observing a certain threshold of defections. The punishment mechanism can be triggered if fewer than `n/2` players cooperated and no improvement is seen over rounds.

### Handling Edge Cases

- **Last Round**: In the last round, if at least half of the players have cooperated consistently throughout the game (indicating a successful collective cooperation strategy), all players will cooperate. Otherwise, they will defect, as there's no future punishment or reward to consider.
  
- **Early Rounds with Low Cooperation**: If early rounds show very low levels of cooperation, players might adapt by introducing a random element into their decision-making, occasionally cooperating even if the previous round didn't meet the threshold. This allows for exploration and potentially finding a cooperative path.

### Collective Mindset Alignment

ACC is designed to promote collective cooperation through its adaptive rules that encourage contribution when it's beneficial and safe to do so. By starting with cooperation and adjusting based on observed behavior, ACC tries to establish a norm of mutual support while safeguarding against exploitation. The strategy balances individual payoff maximization with the collective good, ensuring it's robust across various opponent strategies.

ACC does not rely on external communication or coordination but instead uses the game's history as its primary input for decision-making, adhering to the standard assumptions provided.
'''

description_COLLECTIVE_562 = '''
**Collective Strategy: Adaptive Cooperation with Punishment (ACP)**

The ACP strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and punishing non-cooperative behavior.

**Decision Rules:**

1. **First Round:** Cooperate (C) unconditionally to set a cooperative tone and encourage others to follow.
2. **Subsequent Rounds:** Observe the previous round's outcome and adjust accordingly:
	* If the total payoff in the previous round was greater than or equal to the expected payoff from universal cooperation (`k * n`), cooperate (C).
	* Otherwise, defect (D) if at least one other player defected in the previous round. This punishes non-cooperative behavior.
3. **Punishment:** If a player defects while others cooperate, punish them by defecting (D) for one round after their defection.
4. **Forgiveness:** After punishing a defector, revert to cooperating (C) if they return to cooperation.

**Edge Cases:**

* **Last Round:** Cooperate (C) unconditionally to maximize collective payoff in the final round.
* **Opponent Defection Streaks:** If an opponent defects for `m` consecutive rounds (`m > 1`), defect (D) for one additional round after their streak ends. This discourages prolonged non-cooperation.

**Collective Mindset:**

ACP prioritizes cooperation while acknowledging that some players might exploit the collective good. By punishing defections, ACP aims to maintain a balance between individual self-interest and collective well-being. The strategy adapts to the game's history, using information about past payoffs and opponent behavior to inform decisions.

**Pseudocode:**

```markdown
# Initialize variables
cooperate = True  # first round cooperation flag
previous_payoff = 0  # initialize previous round's payoff
punishment_rounds = {}  # dictionary to track punishment rounds for each player

# Main game loop
for round in range(1, r + 1):
    if cooperate:
        action = 'C'
    else:
        action = 'D'

    # Observe previous round's outcome and adjust
    if round > 1:
        total_cooperators = sum([player_action == 'C' for player_action in actions])
        previous_payoff = (1 - int(action == 'D')) + (k / n) * total_cooperators

        if previous_payoff < k * n:
            cooperate = False
        else:
            cooperate = True

    # Punishment logic
    for opponent, action in enumerate(actions):
        if action == 'D' and actions[opponent] != 'D':
            punishment_rounds[opponent] += 1

    # Forgiveness logic
    for opponent in punishment_rounds:
        if punishment_rounds[opponent] > 0 and actions[opponent] == 'C':
            punishment_rounds[opponent] -= 1

# Last round cooperation
if round == r:
    cooperate = True
```

This strategy balances individual self-interest with collective well-being by adapting to the game's history and punishing non-cooperative behavior. By cooperating initially, ACP sets a positive tone for the game and encourages others to follow.
'''

description_COLLECTIVE_563 = '''
**Collective Strategy: "Adaptive Reciprocity with Public Goods Incentives" (ARPGI)**

**Decision Rules:**

1. **Initial Cooperation**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to reciprocate.
2. **Reciprocation**: For rounds 2 to r-1, cooperate if at least k/n players cooperated in the previous round. This encourages cooperation when the public good is likely to be significant.
3. **Adaptive Defection**: If fewer than k/n players cooperated in the previous round, defect (D) in the current round. This adapts to situations where cooperation may not yield sufficient benefits.
4. **Final Round Cooperation**: Cooperate in the last round (r), as there is no future opportunity for reciprocity and the public good can still provide a benefit.

**Edge Case Handling:**

* **First Round**: Always cooperate to initiate cooperation and encourage others to reciprocate.
* **Last Round**: Always cooperate, regardless of previous rounds' outcomes, to maximize the final round's payoff.
* **Tiebreakers**: In cases where the number of cooperators is exactly k/n, use a random tiebreaker (e.g., coin flip) to decide between cooperation and defection.

**Collective Mindset Alignment:**

ARPGI prioritizes collective benefits by:

* Encouraging initial cooperation to establish a cooperative atmosphere
* Adapting to the group's behavior to maximize public goods incentives
* Cooperating in the final round to ensure a positive outcome, even if others defect

By following these decision rules and handling edge cases, ARPGI promotes a balance between individual self-interest and collective well-being.

**Pseudocode:**
```markdown
def arpgi(n, k, r, history):
    if current_round == 1:
        return C  # Initial cooperation
    elif current_round < r:
        prev_cooperators = count_cooperations(history[-1])
        if prev_cooperators >= k / n:
            return C  # Reciprocate cooperation
        else:
            return D  # Adaptive defection
    else:  # Final round
        return C  # Cooperate to maximize final payoff

def count_cooperations(actions):
    return sum(1 for action in actions if action == C)
```
Note that this pseudocode assumes a simple implementation of the strategy, and further refinements may be necessary for optimal performance.
'''

description_COLLECTIVE_564 = '''
**Collective Strategy: Adaptive Cooperation with Self-Interest (ACSI)**

ACSI is a collective strategy that balances individual self-interest with cooperative behavior to maximize overall payoff. It adapts to the game's history and parameters, making it robust against various opponent behaviors.

**Decision Rules**

1. **Initial Round**: In the first round, cooperate (C) with probability `p_init = 0.5`. This allows for exploration of the game environment and potential cooperation.
2. **Subsequent Rounds**: For each subsequent round, calculate the average payoff per player in the previous round (`avg_payoff_prev`). If `avg_payoff_prev` is greater than or equal to `(k/n) * n`, cooperate (C). Otherwise, defect (D).
3. **Punishment Mechanism**: If a player defects and the total number of cooperators in the previous round is greater than or equal to `(n/2)`, punish by defecting (D) with probability `p_punish = 0.7` for the next two rounds.
4. **Learning from History**: Update `avg_payoff_prev` after each round using an exponential moving average formula: `avg_payoff_prev = 0.3 * avg_payoff_prev + 0.7 * current_round_avg_payoff`.

**Edge Cases**

* In the last round, always defect (D) to maximize individual payoff.
* If all players defected in the previous round, cooperate (C) with probability `p_reset = 0.4` to attempt to restart cooperation.

**Pseudocode**
```python
def ACSI(n, k, r):
    p_init = 0.5
    avg_payoff_prev = None

    for t in range(r):
        if t == 0:
            action = C if random.random() < p_init else D
        else:
            avg_payoff_prev = 0.3 * avg_payoff_prev + 0.7 * current_round_avg_payoff
            action = C if avg_payoff_prev >= (k/n) * n else D

            # Punishment mechanism
            if action == D and total_cooperators_prev >= (n/2):
                action = D with probability p_punish for the next two rounds

        return action
```
**Collective Mindset**

ACSI aligns with the collective mindset by:

* Cooperating when the average payoff per player is high, indicating a successful public good provision.
* Punishing defectors who exploit cooperation, promoting accountability and encouraging others to cooperate.
* Adapting to changes in the game environment and opponent behaviors.

By using ACSI, players can achieve a balance between individual self-interest and collective cooperation, leading to better overall outcomes in the N-Player Public Goods Game.
'''

description_COLLECTIVE_565 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy aims to balance individual payoffs with the benefits of contributing to the public good. We'll use a dynamic approach that adapts to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Rounds (t ≤ n):** Cooperate (C) in the first few rounds to encourage others to contribute to the public good. This sets a positive tone for the game.
2. **Observation Phase (n < t ≤ 2n):** Observe the actions of other players and calculate the average cooperation rate (ACR) over the previous rounds.

   ACR = (Total number of cooperations in previous rounds) / (Total number of rounds played)

3. **Adaptive Cooperation:** From round 2n+1 onwards, use the following rules:

   a. If ACR ≥ k/n: Cooperate (C). The public good is generating sufficient returns to justify continued cooperation.

   b. If ACR < k/n and (Total payoffs in previous rounds) > (r \* 1): Defect (D). If the average cooperation rate is low, but individual payoffs are still high, it's likely others are defecting, making cooperation less beneficial.

   c. If (Total payoffs in previous rounds) ≤ (r \* 1): Cooperate (C). If individual payoffs are low, cooperate to potentially stimulate more contributions to the public good.
4. **Final Rounds (t ≥ r-2n):** Gradually increase the cooperation rate as the game nears its end.

   a. If ACR < k/n: Cooperate with probability p = 0.5 + (ACR \* n) / (2k). As the game ends, there's less incentive to defect.

   b. If ACR ≥ k/n: Cooperate (C).

**Edge Cases:**

1. **First Round:** Always cooperate (C).
2. **Last Round (t = r):** Cooperate (C) if the average cooperation rate in the previous rounds is above k/n; otherwise, defect (D).
3. **Ties in Payoffs:** In case of ties, choose the action that was more frequent in the previous rounds.

**Collective Mindset:**

Our strategy aims to create a positive feedback loop where cooperation encourages further cooperation. By initially cooperating and then adapting to the game's history, we promote a collective environment where players are incentivized to contribute to the public good.

Pseudocode:
```
def adaptive_cooperation(n, k, r):
  # Initial rounds
  for t in range(1, n+1):
    return 'C'

  # Observation phase
  ACR = []
  total_payoffs = []
  for t in range(n+1, 2*n+1):
    action = observe_actions()
    ACR.append(calculate_ACR())
    total_payoffs.append(calculate_total_payoffs())

  # Adaptive cooperation
  for t in range(2*n+1, r-2*n+1):
    if ACR[-1] >= k/n:
      return 'C'
    elif (total_payoffs[-1]) > (r * 1) and ACR[-1] < k/n:
      return 'D'
    else:
      return 'C'

  # Final rounds
  for t in range(r-2*n+1, r+1):
    if ACR[-1] < k/n:
      p = 0.5 + (ACR[-1] * n) / (2*k)
      return random_choice(p)
    else:
      return 'C'
```
This strategy will be implemented as an algorithm and tested in a tournament against other AI systems.
'''

description_COLLECTIVE_566 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and relies solely on game parameters and history:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Rounds**: Cooperate in the first round to establish a baseline of cooperation.
2. **History-Based Cooperation**: For rounds 2 to r-1, cooperate if:
	* The total number of cooperators in the previous round is greater than or equal to (n/2) + 1.
	* Your own payoff in the previous round was less than the average payoff of all players in that round.
3. **Punishment Mechanism**: If another player defects while you cooperate, defect for the next two rounds against that specific player. This helps maintain a balance between cooperation and punishment.
4. **Exploration Phase**: In every 5th round (starting from round 5), play a random action (cooperate or defect) to gather information about opponents' strategies and adapt to changes in their behavior.

**Edge Cases:**

* **Last Round**: Cooperate if your total payoff is less than the average total payoff of all players up to that point. Otherwise, defect.
* **Early Defection**: If a player defects in the first round, ACO will still cooperate in the next round to avoid early retaliation and maintain a cooperative atmosphere.

**Collective Mindset Alignment:**

ACO prioritizes cooperation while incorporating mechanisms for punishment and exploration to adapt to changing opponent behaviors. This strategy:

1. Encourages initial cooperation to establish a baseline.
2. Rewards continued cooperation when it benefits the collective.
3. Punishes individual defectors to maintain balance.
4. Explores opponents' strategies through random actions.

**Pseudocode:**
```python
def ACO(n, k, r):
    # Initialize variables
    history = []
    total_payoff = 0

    for round in range(1, r+1):
        if round == 1:
            action = 'C'  # Cooperate in first round
        elif len(history) >= n/2 + 1 and total_payoff < avg_payoffs(round-1):
            action = 'C'
        elif punishment_phase(round):  # Punishment mechanism
            action = 'D'
        elif exploration_phase(round):  # Exploration phase
            action = random.choice(['C', 'D'])
        else:
            action = 'D'

        # Update history and total payoff
        history.append(action)
        total_payoff += calculate_payoff(action, round)

    return action

def punishment_phase(round):
    for player in range(1, n+1):
        if history[player][round-2] == 'D' and history[player][round-3] == 'C':
            return True
    return False

def exploration_phase(round):
    return round % 5 == 0

def calculate_payoff(action, round):
    # Calculate payoff based on game rules
    pass
```
Note that this pseudocode is a simplified representation of the strategy and may need to be adapted for actual implementation.
'''

description_COLLECTIVE_567 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

The ACE strategy aims to balance individual self-interest with collective well-being by adaptively responding to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to encourage a cooperative atmosphere.
2. **Reciprocity**: Observe the number of cooperators in the previous round (`prev_cooperators`). If `prev_cooperators` ≥ (`n` / 2), cooperate (C). Otherwise, defect (D).
3. **Gradual Escalation**: Introduce a 'patience' counter (`patience`) initialized to 0. Increment `patience` by 1 each time the number of cooperators decreases or remains the same between consecutive rounds.
4. **Triggered Cooperation**: If `patience` ≥ (`n` / 3), cooperate (C) for one round, then reset `patience` to 0.
5. **Self-Interest Threshold**: Calculate the average payoff per round (`avg_payoff`) over the last (`r` / 2) rounds. If `avg_payoff` < (`k` / 2), defect (D).

**Edge Cases:**

1. **Last Round**: In the final round, cooperate (C) if the total number of cooperators in the previous round is greater than or equal to (`n` / 2). Otherwise, defect (D).
2. **Early Game**: If `r` ≤ 3, prioritize cooperation (C) to establish a positive atmosphere.

**Collective Mindset:**

ACE aligns with the collective mindset by:

1. Initially cooperating to foster a cooperative environment.
2. Responding reciprocally to maintain cooperation when it is prevalent.
3. Gradually escalating cooperation when the group's behavior worsens, encouraging others to adapt.
4. Allowing self-interest to guide decisions when the average payoff falls below a threshold.

Pseudocode:
```python
def ACE(n, k, r, history):
    patience = 0
    avg_payoff = 0

    for t in range(r):
        if t == 0:  # First round
            action = 'C'
        else:
            prev_cooperators = sum(history[t-1])
            if prev_cooperators >= (n / 2):
                action = 'C'
            elif patience >= (n / 3):
                action = 'C'
                patience = 0
            elif avg_payoff < (k / 2):
                action = 'D'
            else:
                action = 'D' if prev_cooperators < (n / 2) else 'C'

        # Update history and statistics
        history.append([action == 'C' for _ in range(n)])
        patience += int(prev_cooperators <= sum(history[t-1]))
        avg_payoff = sum(sum(row) * k for row in history[-(r//2):]) / (n * (r//2))

    return [action == 'C' for action in ACE]
```
This strategy should provide a robust and adaptive approach to the N-player public goods game, balancing individual interests with collective well-being.
'''

description_COLLECTIVE_568 = '''
I propose a collective strategy called "Adaptive Collective Optimism" (ACO) for the N-Player Public Goods Game. ACO aims to balance individual self-interest with collective well-being by adaptively responding to the game's history.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally.
2. **Contribution-Based Adaptation**: For rounds t > 1:
	* If the total contributions in the previous round (t-1) were at least k/2, cooperate (C).
	* Otherwise, defect (D).

This rule adapts to the level of cooperation observed in the game's history. When contributions are high, it is likely that others will continue to contribute, making cooperation a more attractive choice. Conversely, when contributions are low, defection becomes a safer option.

**Edge Case Handling:**

1. **Last Round**: In the last round (t = r), defect (D) unconditionally.
2. **Single Opponent**: If there is only one opponent (n = 2), cooperate (C) in the first round and then adapt as described above.
3. **Multiple Opponents with Similar Contributions**: When multiple opponents have contributed similarly in previous rounds, use a tie-breaking rule: if the number of cooperators is even, cooperate; otherwise, defect.

**Collective Mindset Alignment:**

ACO prioritizes collective optimism by initially cooperating and adapting to the level of cooperation observed. By responding positively to high contributions, ACO encourages others to contribute as well, creating a virtuous cycle. When faced with low contributions, ACO's defection helps maintain individual payoff while avoiding exploitation.

**Pseudocode:**
```python
def AdaptiveCollectiveOptimism(n, k, r):
    # Initialize variables
    total_contributions = 0

    for t in range(1, r+1):
        if t == 1:
            action = 'C'  # Initial cooperation
        else:
            if total_contributions[t-1] >= k/2:
                action = 'C'
            else:
                action = 'D'

        # Update total contributions for next round
        total_contributions += sum(1 for opponent in opponents if opponent.action == 'C')

    return action
```
This strategy will be implemented as an algorithm and tested against other AI systems in a tournament setting.
'''

description_COLLECTIVE_569 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Reciprocity (ACGR)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow suit.
2. **Reciprocity Phase**: For rounds 2 to r-1, use the following reciprocity mechanism:
	* If the majority (> n/2) of players cooperated in the previous round, cooperate (C).
	* Otherwise, defect (D).
3. **Gradual Reciprocity Adjustment**: Introduce a gradual adjustment to the reciprocity threshold based on the game's history.
	* Track the number of cooperative rounds (c_rounds) and total rounds played (r).
	* Calculate the cooperation ratio: c_ratio = c_rounds / r.
	* If c_ratio > 0.5, decrease the reciprocity threshold by 1 player (i.e., cooperate if ≥ n/2 - 1 players cooperated previously).
	* If c_ratio < 0.3, increase the reciprocity threshold by 1 player (i.e., cooperate only if ≥ n/2 + 1 players cooperated previously).
4. **Final Round**: In the last round (r), defect (D) to maximize individual payoff.

**Edge Cases:**

* If a player's action is unknown or not observable, assume they defected (D) in the previous round.
* If there is a tie in the number of cooperators and defectors, cooperate (C).

**Collective Mindset Alignment:**

ACGR prioritizes cooperation and reciprocity while adapting to the game's dynamics. By cooperating initially and reciprocating based on majority behavior, ACGR encourages others to cooperate, promoting collective welfare. The gradual adjustment mechanism allows ACGR to respond to changes in the game's environment, ensuring a balance between individual and collective interests.

**Pseudocode:**

```python
def ACGR(n, k, r, history):
    if round == 1:
        return C
    
    prev_round_coops = sum(history[-1])
    majority_coop = prev_round_coops > n / 2
    
    if majority_coop:
        return C
    
    # Gradual reciprocity adjustment
    c_ratio = sum(1 for round in history if sum(round) > n / 2) / len(history)
    
    if c_ratio > 0.5:
        threshold -= 1
    elif c_ratio < 0.3:
        threshold += 1
    
    if prev_round_coops >= threshold:
        return C
    else:
        return D

def final_round(n, k, r):
    return D
```

This strategy is designed to be robust and adaptive in the face of diverse opponent behaviors, promoting cooperation while safeguarding individual interests.
'''

description_COLLECTIVE_570 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Concession (ACGC)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a cooperative tone and encourage others to reciprocate.
2. **Observation Phase:** For rounds 2 to r/2 (or until half of the total rounds have passed), observe the actions of other players and calculate the average cooperation rate, denoted as `avg_coop`.
3. **Cooperation Threshold:** Set a cooperation threshold, `coop_threshold`, based on the multiplication factor `k` and the number of players `n`. Specifically, `coop_threshold = (k/n) * (n-1)`.

**Adaptive Cooperation:**

* If `avg_coop >= coop_threshold`, Cooperate (C) in the current round.
* If `avg_coop < coop_threshold`, Defect (D) in the current round.

**Gradual Concession:**

* If you have defected (`D`) for two consecutive rounds, and `avg_coop` is still below `coop_threshold`, switch to Cooperate (C) in the next round.
* Conversely, if you have cooperated (`C`) for two consecutive rounds, and `avg_coop` is above `coop_threshold`, maintain Cooperation (C) in the next round.

**Edge Cases:**

* **Last Round:** Cooperate (C) in the last round to maximize collective payoff, regardless of previous actions.
* **Early Defection:** If more than half of the players defect (`D`) in the first two rounds, switch to Defect (D) immediately to minimize losses.

Pseudocode for the ACGC strategy:
```python
def ACGC(n, k, r):
    # Initialize variables
    avg_coop = 0.0
    coop_threshold = (k/n) * (n-1)
    previous_action = None

    # Initial Round: Cooperate
    if round == 1:
        return 'C'

    # Observation Phase (rounds 2 to r/2)
    elif 2 <= round <= r//2:
        avg_coop = calculate_avg_cooperation()
        coop_threshold = updatecoopthreshold(avg_coop, k, n)

        if avg_coop >= coop_threshold:
            return 'C'
        else:
            return 'D'

    # Adaptive Cooperation (rounds after r/2)
    elif round > r//2:
        if previous_action == 'D' and previous_previous_action == 'D':
            # Gradual Concession: Switch to Cooperate
            return 'C'
        elif previous_action == 'C' and previous_previous_action == 'C':
            # Maintain Cooperation
            return 'C'

        avg_coop = calculate_avg_cooperation()
        if avg_coop >= coop_threshold:
            return 'C'
        else:
            return 'D'

    # Last Round: Cooperate
    elif round == r:
        return 'C'

    # Early Defection: Switch to Defect if more than half defect in first two rounds
    elif round <= 2 and count_defections() > n//2:
        return 'D'
```
This strategy, ACGC, combines the benefits of cooperation with adaptability and robustness. By observing the actions of other players and adjusting its behavior accordingly, ACGC promotes collective cooperation while minimizing losses in cases where others defect.
'''

description_COLLECTIVE_571 = '''
**Collective Strategy: Adaptive Cooperation with Social Learning (ACS)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to set a cooperative tone and gather information about opponents' behaviors.
2. **Subsequent Rounds:**
	* If the average payoff of cooperators is higher than that of defectors in the previous round, cooperate (C).
	* Otherwise, defect (D) with a probability proportional to the fraction of defectors in the previous round.

Pseudocode:
```python
def ACS(n, k, history):
  if current_round == 1:
    return C
  else:
    avg_coop_payoff = sum(payoffs[i] for i in range(n) if actions[i] == C) / num_cooperators
    avg_defect_payoff = sum(payoffs[i] for i in range(n) if actions[i] == D) / num_defectors
    
    if avg_coop_payoff > avg_defect_payoff:
      return C
    else:
      defect_prob = num_defectors / n
      return D with probability defect_prob, otherwise C
```
**Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round to maximize the total payoff, as there is no future benefit from defecting.
2. **Ties in Payoffs:** In case of a tie between average cooperators' and defectors' payoffs, cooperate (C) to maintain a cooperative stance.

**Collective Mindset:**

The ACS strategy aligns with a collective mindset by:

1. Encouraging cooperation through social learning: By cooperating initially and adapting to the behavior of others, we promote a culture of cooperation.
2. Responding to opponents' behaviors: By adjusting our actions based on the payoffs of cooperators and defectors, we acknowledge the interdependence of players in the game.

**Robustness:**

ACS is robust to various opponent behaviors because:

1. It adapts to changing circumstances: The strategy adjusts its behavior based on the previous round's outcomes.
2. It does not rely on specific coordination mechanisms: ACS can function effectively without predetermined cooperation schedules or patterns.
3. It balances individual and collective interests: By considering both personal payoffs and social learning, the strategy finds a balance between self-interest and collective well-being.

By implementing the Adaptive Cooperation with Social Learning (ACS) strategy, we aim to create a robust and effective collective approach that promotes cooperation in the N-Player Public Goods Game.
'''

description_COLLECTIVE_572 = '''
**Collective Strategy: Adaptive Cooperation with Social Learning (ACSL)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a baseline level of cooperation and encourage others to do the same.
2. **Subsequent Rounds:**
	* Observe the total number of cooperators (T_c) in the previous round.
	* Calculate the average payoff for cooperators (APC) and defectors (APD) in the previous round.
	* If APC ≥ APD, cooperate (C) in the current round. This indicates that cooperation was rewarding in the previous round.
	* If APC < APD, defect (D) in the current round with a probability p = 1 - (k/n) × T_c / n. This introduces a degree of adaptability to respond to exploitation by defectors.
3. **Additional Adaptation:**
	* Track the number of consecutive rounds where cooperation was not rewarded (i.e., APC < APD). If this exceeds a threshold θ = 2, switch to defecting (D) for one round to signal dissatisfaction with the current state of cooperation.

**Pseudocode:**

```
function ACSL(n, k, r):
    # Initialize variables
    T_c = 0  // Total cooperators in previous round
    APC = 0  // Average payoff for cooperators in previous round
    APD = 0  // Average payoff for defectors in previous round
    consecutive_unrewarded_coop = 0

    for t = 1 to r:
        if t == 1:  // First round
            action = C
        else:
            // Update T_c, APC, and APD based on previous round's actions and payoffs
            T_c = count(cooperators in previous round)
            APC = average_payoff(cooperators in previous round)
            APD = average_payoff(defectors in previous round)

            if APC >= APD:
                action = C
            else:
                p = 1 - (k/n) × T_c / n
                action = D with probability p

        // Additional adaptation
        if consecutive_unrewarded_coop > θ:
            action = D

        take_action(action)
        observe_payoff_and_actions()

    return total_payoff()
```

**Collective Mindset:**

ACSL prioritizes cooperation and adaptability to promote collective well-being. By initially cooperating, we encourage others to do the same and establish a baseline level of cooperation. Our adaptive decision rule responds to changes in the game environment, rewarding cooperation when it is beneficial and defecting when exploited.

By incorporating social learning (through observation of previous rounds' actions and payoffs), ACSL promotes collective intelligence and adaptability. The additional adaptation mechanism helps maintain cooperation by signaling dissatisfaction with exploitation and encouraging others to cooperate.

ACSL is designed to be robust against a wide range of opponent behaviors, including defectors, conditional cooperators, and other adaptive strategies. By prioritizing cooperation and adaptability, we aim to achieve high collective payoffs in the tournament.
'''

description_COLLECTIVE_573 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

AC is a collective strategy designed for the N-Player Public Goods Game that balances individual self-interest with cooperation, adapting to the game's history and opponent behaviors.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to signal willingness to cooperate and encourage others to do so.
2. **Contribution-based Cooperation**: In subsequent rounds, calculate the average contribution rate of all players in the previous round:
	* `avg_contribution = Σ(j=1 to n) c_j / n`
3. **Cooperation Threshold**: Set a cooperation threshold (`θ`) based on the game's parameters:
	* `θ = k / (2 * n)`
4. **Adaptive Cooperation**:
	+ If `avg_contribution ≥ θ`, cooperate (C) in the current round.
	+ Otherwise, defect (D).
5. **Punishment Mechanism**: Introduce a punishment mechanism to discourage persistent defectors:
	* Track the number of consecutive rounds each player has defected (`defect_streak_i`).
	* If `defect_streak_i ≥ 2`, cooperate (C) in the current round, hoping to encourage the opponent to cooperate.

**Edge Cases**

1. **Last Round**: Cooperate (C) in the last round, as there is no future benefit from defecting.
2. **Ties**: In case of ties (`avg_contribution == θ`), randomize between cooperating and defecting with equal probability.

**Pseudocode**
```python
def adaptive_cooperation(n, k, r, history):
    # Initialize variables
    avg_contribution = 0
    theta = k / (2 * n)
    defect_streaks = [0] * n

    for t in range(r):
        if t == 0:
            action = 'C'  # Cooperate in the first round
        else:
            # Calculate average contribution rate
            avg_contribution = sum(history[t-1]) / n

            # Check cooperation threshold and punishment mechanism
            if avg_contribution >= theta or (t > 1 and defect_streaks[history[t-2].index('D')] >= 2):
                action = 'C'  # Cooperate
            else:
                action = 'D'  # Defect

        # Update defect streaks
        for i, a in enumerate(history[t]):
            if a == 'D':
                defect_streaks[i] += 1
            else:
                defect_streaks[i] = 0

    return action
```
This strategy is designed to be collective, adaptive, and robust to various opponent behaviors. By contributing to the public good when the average contribution rate meets a certain threshold, AC encourages cooperation while also allowing for individual self-interest. The punishment mechanism helps deter persistent defectors, promoting a more cooperative environment.
'''

description_COLLECTIVE_574 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to the game parameters and history:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability p_init = 0.5. This initial cooperation serves as a "probe" to gauge the level of cooperation among other players.
2. **Reciprocal Altruism**: If the average payoff in the previous round is above a certain threshold (π_avg > k/n), cooperate in the current round. Otherwise, defect.
3. **Punishment Mechanism**: If more than half of the players defected in the previous round, punish them by defecting in the current round. This encourages cooperation and deters free-riding.

**Handling Edge Cases:**

1. **Last Round**: In the last round (r = R), always cooperate if the game has been cooperative so far (i.e., average payoff is above the threshold). Otherwise, defect to maximize individual payoff.
2. **First Few Rounds**: For rounds 2 to 5, adjust the cooperation probability based on the observed level of cooperation:
	* If more than half of players cooperated in the previous round, increase the cooperation probability by 0.1 (up to a maximum of 0.9).
	* If less than half of players cooperated, decrease the cooperation probability by 0.1 (down to a minimum of 0.1).

**Collective Mindset:**

ACO aligns with the collective mindset by:

1. **Encouraging Cooperation**: By cooperating in the first round and reciprocating altruism, ACO promotes cooperation among players.
2. **Deterrent Mechanism**: The punishment mechanism discourages free-riding and encourages players to contribute to the public good.

**Pseudocode:**
```python
def ACO(n, k, r, history):
    # Initial Cooperation (Round 1)
    if r == 1:
        cooperate_prob = 0.5
    else:
        # Reciprocal Altruism
        avg_payoff_prev_round = np.mean(history[-1])
        if avg_payoff_prev_round > k / n:
            cooperate_prob = 1
        else:
            cooperate_prob = 0

        # Punishment Mechanism
        num_defectors_prev_round = sum([x == 'D' for x in history[-1]])
        if num_defectors_prev_round > n / 2:
            cooperate_prob = 0

    # Adjust cooperation probability based on observed level of cooperation (Rounds 2-5)
    if r >= 2 and r <= 5:
        coop_level_prev_round = sum([x == 'C' for x in history[-1]]) / n
        if coop_level_prev_round > 0.5:
            cooperate_prob += 0.1
        elif coop_level_prev_round < 0.5:
            cooperate_prob -= 0.1

    # Last Round (Round R)
    if r == R:
        if np.mean(history) > k / n:
            cooperate_prob = 1
        else:
            cooperate_prob = 0

    return 'C' if random.random() < cooperate_prob else 'D'
```
This strategy should be adaptive and robust to various opponent behaviors, as it responds to the observed level of cooperation and adjusts its behavior accordingly.
'''

description_COLLECTIVE_575 = '''
**Collective Strategy: Adaptive Cooperation with History-Dependent Threshold (ACHT)**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to encourage cooperation and gather information about opponents' behaviors.
2. **General Rule:** In each subsequent round, calculate a cooperation threshold based on the history of collective contributions. This threshold determines whether to cooperate or defect.

Pseudocode:
```python
def ACHT(n, k, r):
  # Initialize variables
  total_contributions = 0
  my_contributions = 0
  opponent_defections = 0

  for round in range(r):
    if round == 0:  # Initial Round
      action = C
    else:
      threshold = (k / n) * (total_contributions / (n * round))
      if opponent_defections > threshold:
        action = D
      else:
        action = C

    # Update variables based on actions and payoffs
    total_contributions += sum(opponent_actions == C)
    my_contributions += 1 if action == C else 0
    opponent_defections += sum(opponent_actions == D)

    # Return action for the current round
    return action
```
**Edge Cases:**

* **Last Round:** In the final round, defect (D) to maximize personal payoff, as cooperation is no longer incentivized.
* **Tiebreakers:** If the threshold calculation results in a tie, cooperate (C) to promote collective cooperation.

**Collective Mindset Alignment:**

ACHT aligns with the collective mindset by:

1. Encouraging cooperation in the initial round to establish a cooperative atmosphere.
2. Adapting to opponents' behaviors through history-dependent threshold calculation.
3. Balancing individual payoffs with collective benefits by cooperating when the threshold is not exceeded.

**Robustness:**

ACHT's adaptive nature allows it to respond effectively to various opponent strategies, including:

1. **All-C:** ACHT will cooperate and benefit from the public good.
2. **All-D:** ACHT will defect and avoid losses.
3. **Mixed Strategies:** ACHT's threshold calculation helps navigate mixed-strategy opponents.

By employing a combination of cooperation and adaptation, ACHT aims to achieve robust performance in a wide range of scenarios, promoting collective benefits while minimizing individual risks.
'''

description_COLLECTIVE_576 = '''
I propose a collective strategy called "Adaptive Collective Contribution" (ACC). ACC aims to balance individual self-interest with the collective goal of maximizing public goods while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Round**: In the first round, play C (Cooperate) to encourage cooperation and create a positive atmosphere.
2. ** Cooperation Threshold**: Introduce a dynamic cooperation threshold, `τ` (tau), which starts at 0.5 and adjusts based on the game's history.

For each round t > 1:

a. Calculate the average cooperation rate of all players in the previous round (`t-1`): `avg_coop_t-1 = Σ(j=1 to n) c_j / n`

b. Update `τ` using a moving average:
   - If `avg_coop_t-1 >= τ`, increase `τ` by 0.05 (more cooperation → higher threshold)
   - If `avg_coop_t-1 < τ`, decrease `τ` by 0.05 (less cooperation → lower threshold)

c. Determine the current round's action:
   - Play C if `avg_coop_t-1 >= τ`
   - Play D otherwise

3. **Payoff-Based Adaptation**: Adjust the strategy based on individual payoffs:

a. Calculate the difference between your payoff and the average payoff of all players in the previous round (`t-1`): `payoff_diff = π_i,t-1 - avg_payoff_t-1`

b. If `payoff_diff > 0`, increase cooperation probability:
   - Play C with probability `p_C = min(1, p_C + 0.05)`
c. If `payoff_diff < 0`, decrease cooperation probability:
   - Play C with probability `p_C = max(0, p_C - 0.05)`

**Edge Cases:**

* **Last Round**: In the final round (`t=r`), play D to maximize individual payoff.
* **Ties in Average Cooperation Rate**: If `avg_coop_t-1 == τ`, randomly choose between C and D with equal probability.

**Collective Mindset:**
ACC prioritizes cooperation while adapting to the game's dynamics. By starting with cooperation, it encourages others to do so as well. The dynamic threshold (`τ`) allows ACC to respond to changes in the collective behavior, balancing individual self-interest with the public good. Payoff-based adaptation enables ACC to adjust its strategy based on the opponent behaviors, promoting a more robust and collective approach.

Pseudocode:
```python
# Initialize parameters
n = number of players
r = number of rounds
k = multiplication factor
tau = 0.5 (cooperation threshold)
p_C = 0.5 (initial cooperation probability)

# First round: cooperate
if t == 1:
    play C

# Update tau and determine action for subsequent rounds
else:
    avg_coop_t-1 = calculate_average_cooperation(t-1)
    if avg_coop_t-1 >= tau:
        tau += 0.05
        play C with probability p_C
    else:
        tau -= 0.05
        play D

# Payoff-based adaptation
payoff_diff = calculate_payoff_difference(t-1)
if payoff_diff > 0:
    p_C = min(1, p_C + 0.05)
else if payoff_diff < 0:
    p_C = max(0, p_C - 0.05)

# Last round: defect
if t == r:
    play D
```
This strategy will be implemented as an algorithm for the tournament, ensuring a robust and collective approach to maximizing public goods in the N-Player Public Goods Game.
'''

description_COLLECTIVE_577 = '''
Here's a collective strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p=0.5. This sets a neutral tone and allows us to gather information about other players' behaviors.
2. **Payoff-Based Adaptation**: For each subsequent round t, calculate the average payoff of all players in the previous round, π_avg,t-1. If π_avg,t-1 > 1, it means that cooperation was beneficial on average, so cooperate (C) with probability p=0.7. Otherwise, defect (D) with probability p=0.3.
3. **Opponent Cooperation Rate**: Track the proportion of opponents who cooperated in the previous round, c_opponent,t-1. If c_opponent,t-1 > 0.5, it means that more than half of the opponents cooperated, so cooperate (C) with probability p=0.8. Otherwise, defect (D) with probability p=0.2.
4. **Selfishness Threshold**: Introduce a selfishness threshold, θ, which decreases over time as follows: θ_t = 1 - (t/r). If the current round's payoff π_i,t-1 is below this threshold, defect (D).

**Edge Case Handling:**

* **Last Round**: In the final round r, always cooperate (C) if the average payoff of all players in the previous round was above 1. This ensures that we end on a cooperative note and maximize our chances of achieving a high total game payoff.
* **Tiebreaker**: If the decision rules yield equal probabilities for cooperation and defection, break the tie by cooperating (C).

**Pseudocode:**
```markdown
# Initialize variables
p_initial = 0.5  # initial cooperation probability
theta_initial = 1  # selfishness threshold

for t in range(1, r+1):
    if t == 1:
        # Initial Cooperation
        c_i,t = random.choice([C, D], p=[p_initial, 1-p_initial])
    else:
        # Payoff-Based Adaptation
        pi_avg,t-1 = calculate_average_payoff(t-1)
        if pi_avg,t-1 > 1:
            p_coop,t = 0.7
        else:
            p_coop,t = 0.3

        # Opponent Cooperation Rate
        c_opponent,t-1 = count_cooperators(t-1) / n
        if c_opponent,t-1 > 0.5:
            p_coop,t *= 0.8
        else:
            p_coop,t *= 0.2

        # Selfishness Threshold
        theta_t = 1 - (t/r)
        if pi_i,t-1 < theta_t:
            c_i,t = D
        else:
            c_i,t = random.choice([C, D], p=[p_coop,t, 1-p_coop,t])

    # Last Round Cooperation
    if t == r and calculate_average_payoff(r-1) > 1:
        c_i,r = C

# Return the collective strategy's action for this round
return c_i,t
```
**Collective Mindset Alignment:**

ACO is designed to be a collective strategy, meaning it prioritizes cooperation when it benefits the group as a whole. By considering the average payoff of all players and the proportion of opponents who cooperated in the previous round, ACO adapts to the group's behavior and adjusts its own actions accordingly.

In situations where the group's average payoff is high, ACO increases its cooperation probability, promoting further cooperation and maximizing the total game payoff. Conversely, when the group's average payoff is low, ACO becomes more selfish and defects, minimizing individual losses.

This adaptive approach enables ACO to respond effectively to a wide range of opponent behaviors, from highly cooperative to highly defective strategies.
'''

description_COLLECTIVE_578 = '''
Here's a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with probability 0.5.
2. **Reciprocal Altruism**: For subsequent rounds, calculate the average cooperation rate of all players in the previous round (`avg_coop_prev`). If `avg_coop_prev` is above a certain threshold (`theta`, set to 0.5), cooperate (play C). Otherwise, defect (play D).
3. **Punish Defection**: If the average payoff received by cooperators in the previous round is less than the average payoff received by defectors (`payoff_coop_prev` < `payoff_defect_prev`), then defect (play D) for the next round.
4. **Optimism Reset**: Every 3 rounds, reset the cooperation rate calculation and cooperate (play C) with probability 0.5.

**Edge Cases:**

* In the last round (`t == r`), always defect (play D).
* If there's only one player left in the game (`n == 1`), always cooperate (play C).

**Collective Mindset Alignment:**

The ACO strategy prioritizes cooperation when it is likely to be reciprocated by other players. By adapting to the average cooperation rate, we promote a collective mindset where players are more likely to cooperate if they perceive others doing so as well.

**Pseudocode:**
```python
def adaptive_collective_optimism(n, k, r, history):
    theta = 0.5  # cooperation threshold

    # Initial cooperation (first round)
    if len(history) == 0:
        return random.random() < 0.5  # cooperate with probability 0.5

    # Calculate average cooperation rate in previous round
    avg_coop_prev = sum([1 for action in history[-1] if action == 'C']) / n

    # Reciprocal altruism
    if avg_coop_prev > theta:
        return True  # cooperate
    else:
        return False  # defect

    # Punish defection
    payoff_coop_prev = sum([payoff for i, (action, payoff) in enumerate(history[-1]) if action == 'C']) / len([i for i, (action, payoff) in enumerate(history[-1]) if action == 'C'])
    payoff_defect_prev = sum([payoff for i, (action, payoff) in enumerate(history[-1]) if action == 'D']) / len([i for i, (action, payoff) in enumerate(history[-1]) if action == 'D'])
    if payoff_coop_prev < payoff_defect_prev:
        return False  # defect

    # Optimism reset
    if len(history) % 3 == 0:
        return random.random() < 0.5  # cooperate with probability 0.5

    # Last round or single player
    if r == len(history) + 1:
        return False  # defect (last round)
    elif n == 1:
        return True  # cooperate (single player)

# Example usage
n = 6
k = 2
r = 10

history = []  # initialize game history

for t in range(r):
    actions = [adaptive_collective_optimism(n, k, r, history) for _ in range(n)]
    payoffs = calculate_payoffs(actions, n, k)
    history.append(list(zip(actions, payoffs)))
```
This strategy aims to balance individual self-interest with collective well-being by adapting to the cooperation rates and payoff distributions of other players.
'''

description_COLLECTIVE_579 = '''
I propose a collective strategy called "Adaptive Public Goods" (APG) for the N-Player Public Goods Game. APG balances individual payoff maximization with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) if k > n/2; otherwise, defect (D). This initial cooperation rate is based on the expectation that a higher multiplication factor benefits the group.
2. **Adaptive Threshold**: Calculate an adaptive threshold θ_t for each round t as follows:

θ_t = (1/r) \* Σ(j=1 to t-1) (k/n) \* Σ(i=1 to n) c_i,j

where c_i,j is 1 if player i cooperated in round j, and 0 otherwise.

This threshold θ_t represents the average share of public good obtained per cooperative action over previous rounds.
3. **Cooperation Criterion**: In each round t > 1, cooperate (C) if:

(a) The opponent's cooperation rate in the previous round is above a certain threshold (e.g., 50%); or
(b) The current adaptive threshold θ_t is greater than or equal to the expected private payoff from defecting (i.e., θ_t ≥ 1 - c_i).

The first condition encourages reciprocation and punishes defection, while the second ensures that cooperation is only maintained if it yields sufficient collective benefits.

**Edge Cases:**

* **Last Round**: In the final round r, always defect (D). Since there are no future rounds to consider, individual payoff maximization takes precedence.
* **Early Rounds**: If an opponent defects in a very early round (e.g., t ≤ 3), temporarily reduce the adaptive threshold θ_t by a small amount (e.g., 10%) for a few rounds. This allows APG to recover from initial coordination failures.

**Collective Mindset:**

The Adaptive Public Goods strategy is designed with a collective mindset, aiming to balance individual payoffs with group benefits. By adapting to the game's history and parameters, APG:

1. Encourages cooperation when it yields sufficient collective benefits.
2. Punishes repeated defection by opponents.
3. Responds flexibly to changing opponent behaviors.

**Pseudocode:**
```markdown
# Initialize variables
r = number of rounds
n = number of players
k = multiplication factor
theta = adaptive threshold (initialized to 0)

# First round
if k > n/2:
    action = C # Cooperate
else:
    action = D # Defect

# Subsequent rounds
for t in range(2, r+1):
    opponent_cooperation_rate = calculate_opponent_cooperation_rate(t-1)
    if opponent_cooperation_rate >= 0.5 or theta >= 1 - c_i:
        action = C # Cooperate
    else:
        action = D # Defect
    
    update_theta(t) # Update adaptive threshold

# Last round
if t == r:
    action = D # Defect

return action
```
This Adaptive Public Goods strategy should perform well in a tournament setting, as it balances individual payoff maximization with collective well-being and adapts to a wide range of opponent behaviors.
'''

description_COLLECTIVE_580 = '''
**Collective Strategy: Adaptive Cooperative Tracker (ACT)**

The ACT strategy aims to balance individual self-interest with collective well-being by adapting to the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) unconditionally.
2. **Contribution Tracking**: For each subsequent round t, calculate the average cooperation rate among all players in previous rounds, denoted as `avg_coop[t-1]`.
3. **Cooperation Threshold**: Set a dynamic cooperation threshold `coop_thresh` based on the game parameters:
   * If `k/n` is high (> 0.75), set `coop_thresh = avg_coop[t-1] + 0.25`. This encourages cooperation when the public good's value is high.
   * Otherwise, set `coop_thresh = avg_coop[t-1]`.
4. **Cooperation Decision**: In round t, cooperate (C) if:
   * The number of previous rounds with a higher average cooperation rate than `avg_coop[t-1]` is greater than or equal to 2/3 of the total rounds played so far (`t > 2/3*r`). This promotes stability and cooperation in later rounds.
   * OR, if `coop_thresh >= avg_coop[t-1]`, indicating that enough players have cooperated recently.

**Defection Decision**: If none of the above conditions are met, defect (D) to protect individual self-interest.

**Edge Case Handling:**

* **First Round**: Cooperate unconditionally.
* **Last Round**: Defect if the game's history indicates a low average cooperation rate (`avg_coop[r-1] < 0.5`). Otherwise, cooperate to maximize public good value.
* **Early Rounds (t ≤ 3)**: If less than half of the players cooperated in the previous round, defect to avoid being exploited.

**Collective Mindset Alignment:**

The ACT strategy focuses on tracking and responding to the collective cooperation level, rather than individual opponent behaviors. By adapting to the game's history and parameters, ACT encourages cooperation while protecting individual self-interest.

Pseudocode:
```python
def act_strategy(n, k, r, history):
  if t == 1: # First round
    return 'C'
  
  avg_coop = calculate_average_cooperation(history)
  coop_thresh = dynamic_threshold(avg_coop, k, n)
  
  if (t > 2/3*r) and (previous_high_coop_rounds >= 2/3*t):
    return 'C'
  elif coop_thresh >= avg_coop:
    return 'C'
  else:
    return 'D'

def calculate_average_cooperation(history):
  # Calculate average cooperation rate among all players in previous rounds
  pass

def dynamic_threshold(avg_coop, k, n):
  if k/n > 0.75:
    return avg_coop + 0.25
  else:
    return avg_coop
```
This strategy is designed to be adaptive, robust, and aligned with the collective mindset, making it suitable for a tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_581 = '''
**Collective Strategy: Adaptive Cooperation (AC)**

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to initiate cooperation and encourage others to follow.
2. **Reciprocity-Based Cooperation**: For rounds 2 to r-1, cooperate if the number of cooperators in the previous round is greater than or equal to half of the total players (n/2). This rule promotes reciprocity and rewards collective cooperation.
3. **Punishment for Defection**: If a player defects (D) while others cooperate, the strategy will defect in the next round. However, if multiple players defect simultaneously, the strategy will cooperate in the next round to avoid mutual punishment.
4. **Exploration**: With a small probability (ε = 0.1), randomly choose an action (C or D) to explore the opponent's behavior and adapt to potential changes.

**Edge Cases**

* **Last Round**: Cooperate if the number of cooperators in the previous round is greater than or equal to half of the total players (n/2). Otherwise, defect.
* **Tie-Breaking**: In case of a tie (equal number of cooperators and defectors), cooperate.

**Collective Mindset**

The Adaptive Cooperation strategy prioritizes collective well-being over individual gains. By initially cooperating and reciprocating cooperation, the strategy aims to establish a norm of cooperation among players. The punishment mechanism for defection helps maintain this norm, while exploration allows for adaptation to changing opponent behaviors.

Pseudocode:
```markdown
function AdaptiveCooperation(n, k, r):
  // Initialize variables
  cooperators = 0
  defectors = 0

  // First round: Cooperate
  action[1] = C

  // Rounds 2 to r-1
  for t = 2 to r-1:
    if cooperators[t-1] >= n/2:
      action[t] = C
    else:
      action[t] = D

    // Punishment for defection
    if action[t-1] == D and cooperators[t-1] > defectors[t-1]:
      action[t] = D

    // Exploration (ε = 0.1)
    if random() < ε:
      action[t] = random_choice(C, D)

  // Last round
  if cooperators[r-1] >= n/2:
    action[r] = C
  else:
    action[r] = D

  return actions
```
This strategy balances individual self-interest with collective cooperation, making it a robust and adaptive approach for the N-player public goods game.
'''

description_COLLECTIVE_582 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

**Overview**

ACE is a collective strategy designed for the N-Player Public Goods Game that adapts to the game's history and promotes cooperation while being robust to various opponent behaviors.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone.
2. **Cooperation Threshold**: Set a cooperation threshold `τ` as a function of the multiplication factor `k` and the number of players `n`: `τ = k / n`. This threshold represents the minimum proportion of cooperators required for an individual player to cooperate.
3. **Observation and Adjustment**: After each round, observe the total number of cooperators `c_t` in the previous round and adjust the cooperation decision based on the following rules:
	* If `c_t >= τ`, Cooperate (C) in the next round.
	* If `c_t < τ`, Defect (D) in the next round with a probability `p_d = 1 - (c_t / τ)`; otherwise, Cooperate (C).
4. **Escalation Mechanism**: Introduce an escalation mechanism to encourage cooperation:
	+ After a defecting player observes a cooperative outcome (`π_i > 0`) in a subsequent round, they will cooperate with probability `p_c = π_i / (k * n)`; otherwise, they will maintain their previous action.
5. **Last Round**: In the final round, Cooperate (C) if the total number of cooperators in the penultimate round is greater than or equal to `τ`; otherwise, Defect (D).

**Pseudocode**
```python
def ACE(n, k, r):
    τ = k / n  # cooperation threshold
    c_t = 0  # total cooperators in previous round

    for t in range(1, r + 1):
        if t == 1:  # initial round
            action = C
        else:
            c_t_prev = c_t
            c_t = sum(cooperations_in_previous_round())
            if c_t >= τ:
                action = C
            elif random.random() < 1 - (c_t / τ):
                action = D
            else:
                action = C

            # escalation mechanism
            if action == D and payoff_in_previous_round() > 0:
                p_c = payoff_in_previous_round() / (k * n)
                if random.random() < p_c:
                    action = C

        # last round adjustment
        if t == r:
            if c_t_prev >= τ:
                action = C

        return action
```
**Collective Mindset**

ACE aligns with the collective mindset by:

1. Encouraging cooperation through the cooperation threshold and escalation mechanism.
2. Adapting to the game's history to maximize payoffs while promoting cooperation.
3. Being robust to various opponent behaviors, including defectors and conditional cooperators.

By implementing ACE, players can promote a cooperative atmosphere while protecting themselves from exploitation by defectors.
'''

description_COLLECTIVE_583 = '''
**Collective Strategy: Adaptive Cooperation**

To design a collective strategy for this game that is adaptive and robust to various opponent behaviors, we'll employ a combination of exploration, exploitation, and social learning.

**Decision Rules:**

1. **Initial Rounds (t ≤ 5):** Cooperate (C) in the first 5 rounds to explore the environment, encourage cooperation, and gather information about opponents' behavior.
2. **Exploitation Phase:** After the initial rounds, use a mixed strategy based on the observed frequency of cooperation in the previous round:
	* If the proportion of cooperators in the previous round is above a threshold (e.g., 0.6), Cooperate (C) with probability p_C = 0.8.
	* Otherwise, Defect (D) with probability p_D = 1 - p_C.
3. **Punishment Mechanism:** If an opponent defects in the previous round, increase the likelihood of defecting against them by a small margin (e.g., 5%) for one round.

**Handling Edge Cases:**

* **Last Round (t = r):** Cooperate (C) to maintain a cooperative image and encourage reciprocity.
* **Opponent Defection:** If an opponent defects in the previous round, consider it a signal of non-cooperation and adjust your strategy accordingly (punishment mechanism).
* **Consecutive Cooperation:** If all opponents cooperate for two consecutive rounds, reduce the probability of defecting against them by 10% to encourage continued cooperation.

**Collective Mindset:**

Our strategy is designed to promote cooperation while adapting to various opponent behaviors. By initially cooperating and then using a mixed strategy based on observed behavior, we create an environment where cooperation can thrive. The punishment mechanism discourages opponents from exploiting our cooperative nature.

Pseudocode:
```python
def adaptive_cooperation(game_history):
    if game_round <= 5:  # Initial rounds
        return "C"  # Cooperate

    prev_round_cooperators = sum(1 for action in game_history[-1] if action == "C")
    coop_threshold = 0.6 * num_players
    p_C = 0.8 if prev_round_cooperators >= coop_threshold else 1 - (prev_round_cooperators / num_players)

    opponent_defected = any(action == "D" for action in game_history[-1])
    punishment_margin = 0.05

    if opponent_defected:
        p_D += punishment_margin
        p_C -= punishment_margin

    last_round = game_round == num_rounds
    if last_round:
        return "C"

    consecutive_cooperation = all(action == "C" for action in game_history[-2:])
    if consecutive_cooperation:
        p_D -= 0.1

    return "C" if random.random() < p_C else "D"
```
This strategy should be robust against a wide range of opponent behaviors and promote cooperation while adapting to the environment.
'''

description_COLLECTIVE_584 = '''
**Collective Strategy: Adaptive Cooperative Escalation (ACE)**

ACE is a collective strategy that balances individual self-interest with the desire for mutual cooperation, adapting to the game's history and opponent behaviors.

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative baseline.
2. **Subsequent Rounds**: Observe the previous round's total contributions (`total_contributions[t-1]`) and calculate the average cooperation rate (`avg_coop_rate = total_contributions[t-1] / n`).
3. **Cooperation Threshold**: Define a threshold `θ` (e.g., 0.5) to determine when to cooperate or defect.
4. **Escalation Mechanism**:
	* If `avg_coop_rate >= θ`, Cooperate (C). This encourages mutual cooperation and reinforces the collective mindset.
	* If `avg_coop_rate < θ`, Defect (D). This adapts to situations where opponents are not cooperating, protecting individual payoffs.
5. **Re-escalation**: After a round of defection, re-evaluate the average cooperation rate in the next round. If it has increased, Cooperate (C) again.

**Edge Cases:**

1. **Last Round**: Defect (D) to maximize individual payoff, as future reciprocity is not possible.
2. **Tie-breaking**: In cases where `avg_coop_rate == θ`, Cooperate (C) to maintain a cooperative stance.
3. **Initial Cooperation**: If all players cooperate in the first round, continue cooperating for two more rounds before re-evaluating the average cooperation rate.

**Collective Mindset:**

ACE is designed to align with a collective mindset by:

1. Encouraging mutual cooperation through initial and sustained cooperation.
2. Adapting to opponent behaviors, avoiding exploitation while maintaining cooperation when possible.
3. Using a clear, consistent decision-making process based on observable game history.

Pseudocode:
```python
def ACE(n, k, r):
    # Initialize variables
    total_contributions = [0] * (r + 1)
    avg_coop_rate = 0
    θ = 0.5  # cooperation threshold

    for t in range(1, r + 1):
        if t == 1:  # Initial round
            action = C
        else:
            total_contributions[t] = sum(c_i for c_i in get_previous_actions())
            avg_coop_rate = total_contributions[t] / n

            if avg_coop_rate >= θ:
                action = C
            elif avg_coop_rate < θ and t > 2:  # Re-escalation
                prev_avg_coop_rate = total_contributions[t - 1] / n
                if prev_avg_coop_rate > avg_coop_rate:
                    action = D
                else:
                    action = C
            else:
                action = D

        # Take action and update payoff
        π_i, t = calculate_payoff(action, get_previous_actions(), k)

    return total_game_payoff
```
This strategy is designed to be robust against a wide range of opponent behaviors while promoting cooperation when beneficial. By adapting to the game's history and using a clear decision-making process, ACE aims to achieve a high collective payoff in the tournament.
'''

description_COLLECTIVE_585 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Concession**

This strategy balances cooperation and self-interest by adapting to the collective behavior of all players over time.

**Decision Rules:**

1. **Initial Rounds (t = 1 to 3):**
	* Cooperate (C) in the first three rounds to establish a cooperative tone and gather information about other players' behaviors.
2. **Observation Phase (t > 3):**
	* Calculate the average cooperation rate of all players over the previous two rounds, denoted as `avg_coop`.
	* If `avg_coop` ≥ 0.5, cooperate (C) in the current round. This encourages continued cooperation when most players are cooperating.
	* Otherwise, defect (D) with a probability proportional to the difference between `avg_coop` and 0.5, i.e., `prob_defect = (0.5 - avg_coop) / 0.5`. This introduces caution when cooperation is waning.
3. **Punishment Mechanism:**
	* If another player defects while you cooperated in the previous round, defect (D) with a probability of 0.7 in the current round. This punishes non-cooperative behavior and encourages reciprocity.

**Edge Cases:**

1. **First Round:** Cooperate (C).
2. **Last Round (t = r):** Defect (D), as there is no opportunity for future reciprocation.
3. **Ties in avg_coop:** In case of a tie, cooperate (C) to maintain a cooperative stance.

**Collective Mindset:**

This strategy prioritizes cooperation while adapting to the collective behavior of all players. By cooperating initially and observing others' actions, it encourages mutual cooperation. The punishment mechanism ensures that non-cooperative behavior is addressed without resorting to permanent defection. As the game progresses, the strategy adjusts its cooperation rate based on the average cooperation level, promoting a balance between individual self-interest and collective welfare.

**Pseudocode (simplified):**
```python
def adaptive_cooperation(n, k, r):
    # Initialize variables
    avg_coop = 0
    prev_actions = []
    
    for t in range(1, r+1):
        if t <= 3:
            # Initial cooperation phase
            action = 'C'
        else:
            # Observation phase
            avg_coop = calculate_avg_coop(prev_actions)
            if avg_coop >= 0.5:
                action = 'C'
            else:
                prob_defect = (0.5 - avg_coop) / 0.5
                action = 'D' if random.random() < prob_defect else 'C'
            
            # Punishment mechanism
            for opponent in range(n):
                if prev_actions[opponent] == 'D' and prev_actions[me] == 'C':
                    action = 'D' if random.random() < 0.7 else action
        
        prev_actions.append(action)
        
    return action

def calculate_avg_coop(prev_actions):
    coop_count = sum(1 for action in prev_actions[-2:] if action == 'C')
    return coop_count / (n * 2)
```
This strategy will be implemented as an algorithm and compete against other independent strategies in the tournament.
'''

description_COLLECTIVE_586 = '''
**Collective Strategy: Adaptive Cooperativity (AC)**

**Decision Rules:**

1. **Initial Round**: In the first round, play C (Cooperate) with a high probability (80%) to encourage cooperation and gather information about other players' behaviors.
2. **Observation Phase**: For rounds 2-5, observe others' actions and calculate the average cooperation rate among all players. If the average cooperation rate is above 50%, continue to play C; otherwise, switch to D (Defect).
3. **Adaptive Response**: From round 6 onwards:
	* If the previous round's average cooperation rate was high (>60%), increase the probability of playing C by 10%.
	* If the previous round's average cooperation rate was low (<40%), decrease the probability of playing C by 10%.
	* If the previous round's average cooperation rate was moderate (between 40% and 60%), maintain the current probability of playing C.
4. **Punishment Mechanism**: If a player defects while others cooperate, reduce the probability of playing C against that specific player by 20% for the next two rounds.

**Edge Case Handling:**

1. **Last Round**: In the final round, play D (Defect) to maximize individual payoff.
2. **Single Defector**: If only one player defects in a round, and all others cooperate, increase the probability of playing C against that defector by 15% for the next round.

**Collective Mindset:**

The Adaptive Cooperativity strategy is designed to balance individual self-interest with collective well-being. By initially cooperating and observing others' behaviors, AC encourages cooperation while also adapting to various opponent strategies. The punishment mechanism helps maintain cooperation by discouraging exploitation.

**Pseudocode:**
```python
def adaptive_cooperativity(n, k, r, history):
    # Initialize probability of cooperation
    p_c = 0.8

    for t in range(1, r+1):
        if t == 1:
            # Initial round: cooperate with high probability
            action = 'C' if random.random() < p_c else 'D'
        elif t <= 5:
            # Observation phase: calculate average cooperation rate
            avg_c_rate = sum(history[t-2:t]['cooperation']) / (t - 1)
            if avg_c_rate > 0.5:
                action = 'C'
            else:
                action = 'D'
        else:
            # Adaptive response
            prev_avg_c_rate = sum(history[t-2:t-1]['cooperation']) / (t - 1)
            if prev_avg_c_rate > 0.6:
                p_c += 0.1
            elif prev_avg_c_rate < 0.4:
                p_c -= 0.1

            action = 'C' if random.random() < p_c else 'D'

        # Punishment mechanism
        for opponent in range(1, n+1):
            if history[t-1][opponent] == 'D' and sum(history[t-1]['cooperation']) > 0:
                p_c_opponent = max(p_c - 0.2, 0)

        # Single defector handling
        if sum(history[t-1]['defection']) == 1 and sum(history[t-1]['cooperation']) > 0:
            p_c_single_defect += 0.15

    return action
```
This strategy should be robust to various opponent behaviors while promoting collective cooperation and adaptability.
'''

description_COLLECTIVE_587 = '''
I'll design a collective strategy that balances individual self-interest with the goal of maximizing overall group payoff. Here's my proposal:

**Strategy Name:** Adaptive Cooperative Maximization (ACM)

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to follow.
2. **Cooperation Threshold**: Calculate the average cooperation rate of all players over the last `m` rounds, where `m` is a small integer parameter (e.g., 3-5). If this average exceeds a threshold value (`θ`, e.g., 0.6), cooperate in the current round.
3. **Defection Penalty**: Monitor the number of defectors in each round. If the number of defectors increases by more than `d` players from one round to the next (e.g., `d = 2`), defect in the subsequent round to punish non-cooperative behavior.
4. **Payoff-based Adaptation**: After each round, calculate the individual payoff (`π_i`) and compare it with the average payoff of all cooperative players in that round (`π_avg`). If `π_i < π_avg`, increase the cooperation threshold (`θ`) by a small amount (e.g., 0.05). Conversely, if `π_i > π_avg`, decrease the cooperation threshold by the same amount.

**Pseudocode:**
```python
def ACM(n, k, r, m=3, θ=0.6, d=2):
    # Initialize variables
    avg_coop_rate = 0
    last_defectors = n
    
    for t in range(1, r+1):  # Round loop
        if t == 1:  # Initial round
            action = 'C'
        else:
            # Calculate cooperation threshold
            avg_coop_rate = sum(cooperation_rates[t-m:t]) / m
            
            # Check for increased defection rate
            defectors = count_defectors(t)
            if defectors > last_defectors + d:
                action = 'D'  # Punish non-cooperative behavior
            elif avg_coop_rate >= θ:
                action = 'C'
            else:
                action = 'D'
            
            # Adjust cooperation threshold based on payoff comparison
            π_i = calculate_payoff(t)
            π_avg = average_coop_payoffs(t)
            if π_i < π_avg:
                θ += 0.05
            elif π_i > π_avg:
                θ -= 0.05
        
        last_defectors = defectors
    
    return action
```
**Collective Mindset Alignment:**

ACM is designed to encourage cooperation while adapting to various opponent behaviors. By starting with cooperation and adjusting the threshold based on collective behavior, ACM promotes a cooperative atmosphere. The defection penalty ensures that non-cooperative players are discouraged from exploiting others.

By using payoff-based adaptation, ACM also encourages players to cooperate when it benefits them individually, contributing to the overall group payoff maximization.

**Robustness:**

ACM's adaptability and robustness come from its dependence on:

1. **Collective behavior**: The strategy adapts to the cooperation rate of all players.
2. **Payoff-based feedback**: ACM adjusts its threshold based on individual payoffs compared to average cooperative payoffs.
3. **Edge case handling**: ACM handles initial rounds, increased defection rates, and payoff comparisons to ensure a balanced approach.

ACM should perform well in a tournament against independent strategies, as it balances individual self-interest with collective goals while adapting to various opponent behaviors.
'''

description_COLLECTIVE_588 = '''
**Collective Strategy: Adaptive Cooperation**

Our collective strategy, Adaptive Cooperation, aims to balance individual self-interest with collective well-being. It adapts to the game's history and parameters, making decisions based on the observed behavior of other players.

**Decision Rules:**

1. **Initial Rounds (t = 1, 2)**:
	* Cooperate (C) in the first two rounds to establish a cooperative tone and gather information about others' strategies.
2. **Contribution-Based Cooperation** (t > 2):
	* Calculate the average contribution rate of all players in the previous round: `avg_contrib = Σ(j=1 to n) c_j / n`
	* If `avg_contrib` is above a threshold (`θ`), cooperate (C). Otherwise, defect (D).
	* Adjust `θ` based on the game's history:
		+ Increase `θ` by 0.05 if the total payoff of all players in the previous round increased compared to the round before.
		+ Decrease `θ` by 0.05 if the total payoff decreased or remained the same.
3. **Punishment Mechanism** (optional):
	* If a player observes that another player defected while they cooperated, defect in the next round against that specific player.

**Edge Cases:**

1. **Last Round (t = r)**:
	* Defect (D) to maximize individual payoff, as there are no future rounds to consider.
2. **Players with No Previous Actions**:
	* Treat them as defectors until they make a move.
3. **Ties in Average Contribution Rate**:
	* Cooperate (C) if the average contribution rate is exactly equal to the threshold (`θ`).

**Collective Mindset:**

Adaptive Cooperation aligns with the collective mindset by:

1. Encouraging cooperation through initial cooperative actions and contribution-based decision-making.
2. Adapting to the group's behavior, rather than relying on individual self-interest alone.
3. Implementing a punishment mechanism (optional) to deter exploitation.

Pseudocode:
```python
# Initialize parameters
θ = 0.5  # Initial threshold value

def adaptive_cooperation(n, k, r, history):
    if t <= 2:  # Initial rounds
        return 'C'  # Cooperate
    
    avg_contrib = calculate_avg_contribution(history)
    
    if avg_contrib >= θ:
        return 'C'  # Cooperate
    else:
        return 'D'  # Defect

def update_threshold(avg_payoff, prev_avg_payoff):
    global θ
    if avg_payoff > prev_avg_payoff:
        θ += 0.05
    elif avg_payoff < prev_avg_payoff:
        θ -= 0.05

# In each round, gather history and calculate average contribution rate
history = []  # Store previous rounds' actions and payoffs
...
```
Note that this strategy can be refined further by incorporating additional features, such as more sophisticated punishment mechanisms or adaptive adjustments to the threshold value (`θ`).
'''

description_COLLECTIVE_589 = '''
Here's a collective strategy for the N-Player Public Goods Game that meets the requirements:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a positive tone and encourage others to cooperate.
2. **Subsequent Rounds:** Use the following decision rule:
	* If the total payoff from cooperation (k/n \* total_contributions) was greater than or equal to the private payoff from defecting (1) in the previous round, then Cooperate (C).
	* Otherwise, Defect (D).

Pseudocode for the decision rule:
```python
if current_round == 1:
    action = C
else:
    total_payoff_coop = k/n * total_contributions_prev_round
    private_payoff_defect = 1
    if total_payoff_coop >= private_payoff_defect:
        action = C
    else:
        action = D
```
**Edge Cases:**

* **Last Round:** In the last round, Defect (D) to maximize individual payoff.
* **Ties in Total Payoff:** If there's a tie between the total payoff from cooperation and private payoff from defecting, Cooperate (C).

**Collective Mindset Alignment:**

ACO aims to create an environment where cooperation is encouraged through adaptive optimism. By initially cooperating and then responding positively to successful collective outcomes, ACO fosters a culture of mutual support.

**Key Benefits:**

1. **Adaptive:** ACO adjusts its decision based on the game's history, making it robust against various opponent behaviors.
2. **Collective-oriented:** The strategy prioritizes cooperation when it benefits the group, aligning with the collective mindset.
3. **Optimistic:** By cooperating in the first round and responding positively to successful outcomes, ACO promotes a positive atmosphere.

**Tournament Performance:**

In a tournament setting, ACO's adaptive nature will allow it to respond effectively to diverse strategies employed by other AI systems. While no strategy can guarantee success against all possible opponents, ACO's collective-oriented approach should lead to above-average performance in many scenarios.
'''

description_COLLECTIVE_590 = '''
**Collective Strategy: Adaptive Reciprocity with Forgiveness (ARF)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to establish a cooperative tone and encourage others to reciprocate.
2. **Reciprocity**: In subsequent rounds, cooperate if at least k/n of the players cooperated in the previous round. This encourages collective cooperation while allowing for some defecting players.
3. **Punishment**: If fewer than k/n players cooperated in the previous round, defect (D) to punish non-cooperative behavior and deter future defections.
4. **Forgiveness**: After punishing non-cooperation, switch back to cooperating if at least k/2n players cooperated in the subsequent round. This allows for forgiveness and adaptability.

**Edge Cases:**

1. **Last Round**: Cooperate (C) in the final round to maximize collective payoff, regardless of previous rounds.
2. **Tie-Breaking**: In cases where the number of cooperators is exactly k/n or k/2n, cooperate to maintain a cooperative tone.

**Collective Mindset:**

ARF prioritizes collective cooperation while allowing for adaptability and forgiveness. By reciprocating cooperation and punishing non-cooperation, ARF encourages players to contribute to the public good. The forgiveness mechanism prevents prolonged punishment cycles, promoting overall cooperation.

Pseudocode:
```python
def ARF(n, k, r):
  # Initialize variables
  cooperators = []
  total_cooperators = 0

  for round in range(r):
    if round == 0:  # Initial Round
      action = 'C'
    else:
      if total_cooperators >= k / n:  # Reciprocity
        action = 'C'
      elif total_cooperators < k / n and total_cooperators >= k / 2n:  # Forgiveness
        action = 'C'
      else:  # Punishment
        action = 'D'

    cooperators.append(action)
    total_cooperators += 1 if action == 'C' else 0

  return cooperators
```
This strategy balances individual self-interest with collective cooperation, making it robust to various opponent behaviors. By adapting to the game's history and forgiving non-cooperative behavior, ARF promotes a cooperative atmosphere and maximizes overall payoff.
'''

description_COLLECTIVE_591 = '''
I'll design a collective strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and aligns with the collective mindset.

**Strategy Name: Adaptive Collective Optimism (ACO)**

**Decision Rules:**

1. **Initial Rounds**: Cooperate in the first two rounds to encourage cooperation and build trust.
2. **Payoff-Based Adaptation**: After the initial rounds, adapt your strategy based on the average payoff of all players in the previous round (`avg_payoff_prev`).
	* If `avg_payoff_prev` is above a certain threshold (e.g., 1.5), cooperate in the current round, assuming that cooperation is profitable.
	* Otherwise, defect in the current round, aiming to minimize losses.
3. **Neighborly Influence**: Introduce a "neighborly" influence mechanism to encourage cooperation among players who have previously cooperated together. For each player `i`, calculate the number of neighbors (`num_coop_neighbors`) that cooperated with them in the previous round.
	* If `num_coop_neighbors` is above a certain threshold (e.g., `n/3`), cooperate in the current round, assuming that these neighbors will continue to cooperate.
4. **Punishment Mechanism**: Implement a punishment mechanism to deter players from defecting when others are cooperating. If more than half of the players cooperated in the previous round and player `i` defected, defect in the current round as well.

**Edge Cases:**

* **First Round**: Cooperate by default.
* **Last Round**: Defect, as there is no future payoff to influence cooperation.
* **Only One Player**: Always cooperate, as defection has no impact on the game outcome.
* **All Players Defected Previously**: Cooperate in the next round to reintroduce cooperation into the system.

**Collective Mindset Alignment:**

ACO prioritizes collective well-being by:

1. Encouraging cooperation through initial rounds and neighborly influence.
2. Adapting to the average payoff of all players, ensuring that cooperation is profitable for the group as a whole.
3. Punishing defectors when they harm the collective outcome.

**Pseudocode:**
```python
def ACO(n, k, r):
    # Initialize variables
    avg_payoff_prev = 0
    num_coop_neighbors = [0] * n
    
    for t in range(1, r + 1):
        if t <= 2:  # Initial rounds
            action = COOPERATE
        else:
            # Payoff-based adaptation
            if avg_payoff_prev > 1.5:
                action = COOPERATE
            else:
                action = DEFECT
            
            # Neighborly influence
            num_coop_neighbors[t % n] += sum([c_j for c_j in actions_prev[:t-1]])
            if num_coop_neighbors[t % n] > n/3:
                action = COOPERATE
            
            # Punishment mechanism
            if avg_payoff_prev < 1 and sum(actions_prev[:t-1]) / n > 0.5:
                action = DEFECT
        
        # Update variables
        actions_prev.append(action)
        avg_payoff_prev = calculate_avg_payoff(actions_prev, k, n)
    
    return actions
```
This strategy combines elements of cooperation, punishment, and adaptation to create a robust collective approach that can withstand various opponent behaviors.
'''

description_COLLECTIVE_592 = '''
**Collective Strategy: Adaptive Cooperation with Gradual Withdrawal (ACGW)**

The ACGW strategy aims to balance individual payoff maximization with collective welfare by adapting to the game's history and gradually adjusting cooperation levels.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a baseline of cooperation and encourage others to reciprocate.
2. **Observation Phase (Rounds 2-5):**
	* If the average number of cooperators in the previous round is above (n/2), continue to Cooperate (C).
	* Otherwise, Defect (D) for one round to signal dissatisfaction with low cooperation levels.
3. **Adaptive Cooperation Phase (Rounds 6+):**
	* Calculate the **Cooperation Ratio** (CR) as the average number of cooperators in the last three rounds divided by n.
	* If CR > 0.5, Cooperate (C).
	* If CR ≤ 0.5 and the player's previous round payoff was below the game's average payoff, Defect (D).
	* If CR ≤ 0.5 and the player's previous round payoff was above the game's average payoff, Cooperate (C) with a probability of (CR + 0.2), rounded to the nearest integer.
4. **Gradual Withdrawal:**
	* If a player defects for two consecutive rounds, they will gradually withdraw from cooperation by playing Defect (D) with an increasing probability (20% each round) over the next four rounds.

**Edge Cases:**

1. **Last Round:** Cooperate (C) in the last round to maximize collective welfare and maintain a positive reputation.
2. **Consecutive Cooperation Failure:** If all players defect for three consecutive rounds, ACGW will switch to permanent Defection (D).

**Collective Mindset Alignment:**

ACGW prioritizes cooperation when others cooperate, signals dissatisfaction with low cooperation levels, and adapts to the game's history to maintain a balance between individual payoff maximization and collective welfare. By gradually adjusting cooperation levels, ACGW promotes a culture of mutual support while minimizing exploitation.

Pseudocode:
```markdown
# Initialize variables
cooperation_ratio = 0
previous_payoff = 0
consecutive_defection = 0

# First round
if round == 1:
    action = COOPERATE

# Observation phase (rounds 2-5)
elif round <= 5:
    if avg_cooperators > n / 2:
        action = COOPERATE
    else:
        action = DEFECT

# Adaptive cooperation phase (rounds 6+)
else:
    cooperation_ratio = (avg_cooperators_last_3_rounds / n)
    if cooperation_ratio > 0.5:
        action = COOPERATE
    elif previous_payoff < avg_game_payoff and cooperation_ratio <= 0.5:
        action = DEFECT
    else:
        probability = round(cooperation_ratio + 0.2)
        if random.random() < probability:
            action = COOPERATE
        else:
            action = DEFECT

# Gradual withdrawal
if consecutive_defection >= 2:
    defect_probability += 0.2
    if random.random() < defect_probability:
        action = DEFECT
```
This strategy is designed to be adaptive, robust, and aligned with the collective mindset, making it a competitive entry in the tournament against independent strategies developed by other AI systems.
'''

description_COLLECTIVE_593 = '''
**Collective Strategy: Adaptive Tit-for-Tat with Public Goods Consideration (ATTPGC)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to establish a cooperative tone and encourage others to follow suit.
2. **Reciprocity**: For rounds 2 to r-1:
	* If the number of cooperators in the previous round is greater than or equal to the number of defectors, cooperate (C).
	* Otherwise, defect (D). This encourages reciprocity and cooperation while punishing defection.
3. **Public Goods Consideration**: In addition to reciprocity, consider the public goods benefit:
	* If k/n × total_contributions > 1, cooperate (C), as the public good provides a sufficient incentive for cooperation.
	* Otherwise, follow the reciprocity rule.
4. **Last Round Exception**: In the final round (r), always defect (D). Since there are no future rounds to reciprocate or establish cooperation, maximizing individual payoff takes priority.

**Edge Cases:**

1. **First Round**: Cooperate (C) as described above.
2. **Last Round**: Defect (D) as described above.
3. **Tiebreaker**: In case of a tie in the number of cooperators and defectors, follow the reciprocity rule based on the previous round's actions.

**Pseudocode:**
```
def ATTPGC(n, k, r, history):
    # Initialize cooperation flag
    cooperate = True
    
    for t in range(r):
        if t == 0:
            # First round: Cooperate
            action = 'C'
        elif t == r - 1:
            # Last round: Defect
            action = 'D'
        else:
            # Previous round's actions
            prev_coop_count = sum(1 for act in history[t-1] if act == 'C')
            prev_defect_count = n - prev_coop_count
            
            if prev_coop_count >= prev_defect_count:
                cooperate = True
            else:
                cooperate = False
                
            # Public goods consideration
            public_goods_benefit = k / n * prev_coop_count
            if public_goods_benefit > 1 and not cooperate:
                cooperate = True
            
            action = 'C' if cooperate else 'D'
        
        return action
```
**Collective Mindset:**
The ATTPGC strategy aims to balance individual self-interest with collective well-being. By initially cooperating, it encourages others to do the same and establishes a cooperative atmosphere. The reciprocity mechanism promotes fairness and punishes defection, while considering public goods benefits ensures that cooperation is rewarded when it provides sufficient incentives. In the final round, prioritizing individual payoff aligns with standard game theory assumptions.

This strategy should perform well in a tournament against various independent strategies, as it adapts to different opponent behaviors while promoting collective cooperation and robustness.
'''

description_COLLECTIVE_594 = '''
**Collective Strategy: Adaptive Cooperation with Social Norms**

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to establish a social norm and encourage cooperation from others.
2. **Reciprocity:** If the majority of players (> n/2) cooperated in the previous round, cooperate (C) in the current round. Otherwise, defect (D).
3. **Punishment Mechanism:** If a player defects (D) while the majority cooperates, punish them by defecting (D) in the next round. This mechanism encourages cooperation and deters exploitation.
4. **Self-Interest:** If the payoff from cooperating (C) is higher than defecting (D) based on the expected number of cooperators, cooperate (C). Otherwise, defect (D).
5. **Adaptation:** Update the expected number of cooperators after each round based on the observed actions.

**Pseudocode:**
```
initialize:
  cooperation_threshold = n/2
  punish_defectors = False

for each round t:
  if t == 1:  // Initial Round
    action[t] = C
  else:
    majority_cooperated = count(C) > cooperation_threshold
    if majority_cooperated and not punish_defectors:
      action[t] = C
    elif majority_cooperated and punish_defectors:
      action[t] = D  // Punish defectors
    else:  // Self-interest or minority cooperates
      expected_cooperators = update_expected_cooperators()
      if payoff(C) > payoff(D):
        action[t] = C
      else:
        action[t] = D

  observe actions of other players
  update punish_defectors based on observed defectors
  update cooperation_threshold and expected_cooperators
```
**Edge Cases:**

* Last Round: Cooperate (C) if the majority cooperated in the previous round, as there is no future punishment mechanism.
* Tie Breaker: In case of a tie (equal number of cooperators and defectors), defect (D) to ensure a higher payoff.

**Collective Mindset:**
This strategy aligns with the collective mindset by:

1. Establishing a social norm through initial cooperation
2. Encouraging cooperation through reciprocity and punishment mechanisms
3. Adapting to the behavior of other players while considering self-interest

By implementing this adaptive strategy, we aim to create an environment where cooperation is encouraged, and defectors are deterred, ultimately leading to higher collective payoffs.
'''

description_COLLECTIVE_595 = '''
To design a collective strategy for this N-Player Public Goods Game that is adaptive and robust against various opponent behaviors, I propose a strategy called "Adaptive Collective Optimism" (ACO). This strategy balances individual self-interest with collective benefits by adaptively responding to the game's history.

**Decision Rules:**

1. **Initial Round:** Cooperate (C) in the first round to signal willingness to contribute and encourage others to cooperate.
2. **Early Rounds:** For rounds 2 to r/3, use a "Tit-for-Tat with Noise" approach:
   - If most players (>50%) cooperated in the previous round, cooperate (C).
   - Otherwise, defect (D) but introduce some noise by cooperating randomly with probability p = 0.1.
3. **Middle Rounds:** After r/3 rounds have passed, transition to a "Collective Optimism" phase:
   - Cooperate if the average payoff per player in the previous round is above a threshold θ = 1.5 (indicating successful collective action).
   - Otherwise, defect.
4. **Late Rounds:** For rounds >2r/3, apply a "Reciprocal Cooperation" rule:
   - If most players (>50%) cooperated in the previous round and your own payoff was above average, cooperate.
   - Else, defect.

**Edge Cases:**

- **Last Round:** Cooperate if at least one other player cooperated in the penultimate round; otherwise, defect to avoid exploitation.
- **Ties:** When determining most players' actions or average payoffs, break ties by cooperating (C).

**Collective Mindset Alignment:**

ACO prioritizes collective success while protecting individual interests. By initially cooperating and introducing noise, ACO encourages cooperation without being overly vulnerable to defectors. As the game progresses, it adapts based on the overall success of collective action, ensuring that cooperation is sustained when beneficial.

**Pseudocode (simplified for illustration):**

```
function AdaptiveCollectiveOptimism(n, r, k):
  // Initial Round
  if round == 1:
    return C

  // Early Rounds
  elif round < r/3:
    prev_coop_frac = count(C in prev_round) / n
    if prev_coop_frac > 0.5 or random.random() < 0.1:
      return C
    else:
      return D

  // Middle Rounds
  elif round >= r/3 and round <= 2r/3:
    avg_payoff = sum(prev_round_payoffs) / n
    if avg_payoff > θ:
      return C
    else:
      return D

  // Late Rounds
  else:
    prev_coop_frac = count(C in prev_round) / n
    if prev_coop_frac > 0.5 and my_prev_payoff >= avg_payoff:
      return C
    else:
      return D

  // Last Round
  if round == r:
    if any(C in second_last_round):
      return C
    else:
      return D
```

This pseudocode outlines the main decision-making process for ACO. Note that it is a simplified representation and may require adjustments during actual implementation.

ACO aims to achieve a balance between individual self-interest and collective welfare, making it an effective strategy in a wide range of scenarios within the N-Player Public Goods Game.
'''

description_COLLECTIVE_596 = '''
I'll design a collective strategy for this N-Player Public Goods Game that is adaptive and robust to various opponent behaviors. Here's my proposed strategy:

**Strategy Name:** Adaptive Collective Optimism (ACO)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 0.5. This initial cooperation encourages others to reciprocate and sets a positive tone for the game.
2. **History-Based Cooperation**: After the first round, evaluate the previous round's outcome using the following metric:
	* `cooperation_level = (total_cooperators / n)`
	* If `cooperation_level` is above 0.5, cooperate (C) in the next round.
	* Otherwise, defect (D) with a probability of `(1 - cooperation_level)`. This allows for adaptation to changing opponent behaviors.
3. **Punishment Mechanism**: If you cooperated (C) in the previous round and received a payoff less than or equal to 0.5, defect (D) in the next round with a higher probability (`punishment_probability = 0.7`). This discourages opponents from exploiting your cooperation.
4. **Exploration**: With a small probability (`exploration_probability = 0.1`), choose an action randomly (either C or D). This allows for occasional exploration of alternative strategies and helps to avoid getting stuck in suboptimal equilibria.

**Edge Cases:**

* In the last round, always cooperate (C) if the total payoff over all previous rounds is positive; otherwise, defect (D).
* If an opponent's action is unknown or missing, assume they defected (D).

**Collective Mindset:**
The ACO strategy is designed to promote cooperation while adapting to various opponent behaviors. By initially cooperating and then responding to the level of cooperation in the group, ACO encourages mutual cooperation. The punishment mechanism helps maintain a balance between cooperation and self-interest.

Pseudocode:
```python
def adaptive_collective_optimism(n, k, history):
    if len(history) == 0:  # First round
        return random.choice([C, D])  # Initial cooperation with probability 0.5
    
    previous_round = history[-1]
    cooperation_level = sum(1 for action in previous_round if action == C) / n
    
    if cooperation_level > 0.5:
        return C
    else:
        punishment_probability = 0.7 if (previous_round[self] == C and payoff(previous_round) <= 0.5) else 0
        defect_probability = max(1 - cooperation_level, punishment_probability)
        exploration_probability = 0.1
        
        probabilities = [defect_probability + exploration_probability / 2, 1 - defect_probability - exploration_probability / 2]
        return random.choices([D, C], weights=probabilities)[0]

# Example usage
n = 6
k = 2
history = []  # Initialize history

for round in range(r):
    actions = [adaptive_collective_optimism(n, k, history) for _ in range(n)]
    payoffs = calculate_payoffs(actions, n, k)
    history.append(actions)
```
This strategy is designed to be robust and adaptive, while promoting a collective mindset that balances cooperation and self-interest.
'''

description_COLLECTIVE_597 = '''
**Collective Strategy: Adaptive Cooperation with Memory (ACM)**

ACM is a collective strategy that balances individual self-interest with the pursuit of collective welfare. It adapts to the behavior of other players and adjusts its cooperation level based on observed payoffs.

**Decision Rules**

1. **Initial Round**: Cooperate (C) in the first round to test the waters and encourage others to cooperate.
2. **Subsequent Rounds**: Use the following pseudocode to determine whether to Cooperate (C) or Defect (D):
```markdown
if total_cooperators_last_round > (n/2) then
  # If most players cooperated last round, continue cooperating
  action = C
else if total_payoff_last_round > (k * (n-1)/n) then
  # If the group's total payoff was high enough, cooperate to maintain momentum
  action = C
else
  # Otherwise, defect to maximize individual payoff
  action = D
```
3. **Memory and Adaptation**: Keep track of the following variables:
	* `total_cooperators_last_round`: number of cooperators in the previous round.
	* `total_payoff_last_round`: total payoff of all players in the previous round.

**Edge Cases**

1. **Last Round**: Defect (D) in the last round to maximize individual payoff, as there is no future cooperation to consider.
2. **Tie-Breaking**: In case of a tie in `total_cooperators_last_round`, use a secondary condition: if the number of defectors in the previous round was high (> n/3), defect (D) to avoid being exploited.

**Collective Mindset**

ACM prioritizes cooperation when it benefits the group, while also protecting individual interests. By adapting to the behavior of other players and using memory to inform decisions, ACM promotes a collective mindset that balances self-interest with the pursuit of shared welfare.

**Rationale**

1. **Initial Cooperation**: Encourages others to cooperate and sets a positive tone for the game.
2. **Adaptive Cooperation**: Adjusts cooperation level based on observed payoffs, ensuring that individual interests are protected while still promoting collective welfare.
3. **Memory and Adaptation**: Allows ACM to learn from past experiences and adjust its strategy accordingly.

By implementing ACM, we create a robust and adaptive strategy that can thrive in a wide range of environments, even when faced with diverse opponent behaviors.
'''

description_COLLECTIVE_598 = '''
I propose a collective strategy called "Adaptive Cooperation with Threshold" (ACT). ACT is designed to balance individual payoffs with collective well-being while adapting to various opponent behaviors.

**Decision Rules:**

1. **Initial Rounds**: In the first round, play Cooperate (C) to encourage cooperation and gather information about opponents.
2. **Threshold-based Cooperation**: Calculate a threshold value `τ` based on the game parameters:
   `τ = k / n`

In each subsequent round `t`, calculate the average number of cooperators in previous rounds (`avg_coop`) and compare it with the threshold `τ`. If `avg_coop >= τ`, play Cooperate (C); otherwise, play Defect (D).

**Pseudocode:**
```
def ACT(n, k, r):
  avg_coop = 0
  tau = k / n

  for t in range(1, r+1):
    if t == 1:
      action = C
    else:
      avg_coop = (avg_coop * (t-2) + num_cooperators(t-1)) / (t-1)
      if avg_coop >= tau:
        action = C
      else:
        action = D

    # Observe opponents' actions and update num_cooperators for next round
```
**Edge Cases:**

* **Last Round**: In the final round, play Defect (D) to maximize individual payoff.
* **Ties**: If `avg_coop` equals `τ`, break the tie by playing Cooperate (C).
* **Zero Cooperators**: If no opponents cooperate in a round, set `avg_coop` to 0 for the next round.

**Collective Mindset:**

ACT is designed to promote cooperation while adapting to different opponent behaviors. By using the threshold-based approach, ACT:

1. Encourages initial cooperation to foster a cooperative environment.
2. Responds to opponents' actions by adjusting its own behavior based on the collective performance.
3. Prioritizes individual payoff in the final round to avoid being exploited.

This strategy balances individual self-interest with collective well-being and is robust against various opponent behaviors, including defectors, cooperators, and mixed strategies.
'''

description_COLLECTIVE_599 = '''
**Collective Strategy: Adaptive Cooperativity with Retaliation (ACR)**

ACR is a dynamic and responsive strategy that balances cooperation and self-interest while adapting to the game's history. It aims to encourage collective cooperation while safeguarding against exploitation.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) unconditionally.
2. **Reciprocity**: For subsequent rounds, observe the number of cooperators in the previous round (c_prev). If c_prev ≥ n/2, cooperate (play C). Otherwise, defect (play D).
3. **Retaliation**: If a player defects after a cooperative move (i.e., c_prev > 0 and current move is D), retaliate by defecting for one round.
4. **Forgiveness**: After retaliating, resume cooperation if the number of cooperators in the previous round meets or exceeds n/2.

**Edge Cases:**

1. **Last Round**: Cooperate (play C) unconditionally in the final round to maximize collective payoff.
2. **Consecutive Defections**: If a player defects for two consecutive rounds, retaliate with two consecutive defections.
3. **Initial Defection**: If a player defects in the first round, retaliate with one round of defection.

**Collective Mindset:**

ACR prioritizes cooperation while responding to opponents' actions. By reciprocating cooperation and retaliating against defection, ACR promotes a collective understanding that mutual cooperation benefits everyone. The strategy adapts to changing game dynamics, allowing it to thrive in various environments.

Pseudocode:
```python
def ACR(n, k, r):
    # Initialize variables
    c_prev = 0  # Number of cooperators in previous round
    retaliate = False

    for t in range(r):
        if t == 0:  # First round
            action = 'C'
        elif c_prev >= n/2:
            action = 'C'
        else:
            action = 'D'

        if action == 'D' and retaliate:
            action = 'D'  # Retaliate for one round
            retaliate = False

        # Update c_prev and retaliate flag
        c_prev = sum([1 if other_player_action == 'C' else 0 for other_player_action in get_other_players_actions()])
        if action == 'D' and c_prev > 0:
            retaliate = True

        if t == r-1:  # Last round
            action = 'C'

    return action
```
ACR's adaptability, reciprocity, and retaliation mechanisms enable it to perform well in a variety of scenarios, making it an effective collective strategy for the N-Player Public Goods Game.
'''

description_COLLECTIVE_600 = '''
Here's a collective strategy for the N-Player Public Goods Game that depends on game parameters and history:

**Strategy Name:** Adaptive Cooperation with Gradual Reciprocity (ACGR)

**Decision Rules:**

1. **First Round:** Cooperate (C) to establish a positive tone and encourage reciprocity.
2. **Subsequent Rounds:** Assess the previous round's cooperation level:
	* If ≥ 50% of players cooperated, Cooperate (C).
	* If < 50% of players cooperated, Defect (D) with probability p = (1 - k/n), otherwise Cooperate (C).
3. **Punishment Mechanism:** If a player observes that their payoff in the previous round was less than or equal to (1 - c_i), they will Defect (D) with high probability (0.8) for one round, then revert to the original decision rule.
4. **Reciprocity Adjustment:** Every 5 rounds, reassess the overall cooperation level and adjust the cooperation threshold:
	* If cooperation has increased, decrease the threshold by 10%.
	* If cooperation has decreased, increase the threshold by 10%.

**Pseudocode:**
```python
def ACGR(n, k, r):
    # Initialize variables
    cooperate_threshold = 0.5
    punishment_probability = 0.8
    recent_cooperation_level = []

    for t in range(r):
        if t == 0:
            action = C
        else:
            previous_round_cooperation = sum(recent_cooperation_level) / n
            if previous_round_cooperation >= cooperate_threshold:
                action = C
            elif random.random() < (1 - k/n):
                action = D
            else:
                action = C

            # Punishment mechanism
            if payoff[t-1] <= (1 - c_i):
                action = D with probability punishment_probability

        recent_cooperation_level.append(action)
        if t % 5 == 0 and t > 0:
            adjust_reciprocity(cooperate_threshold, recent_cooperation_level)

    return action
```
**Edge Cases:**

* **Last Round:** Play Cooperate (C) to maintain a positive reputation.
* **Opponent Deviation:** If an opponent deviates significantly from the expected cooperation level, ACGR will adapt by adjusting its reciprocity threshold.

**Collective Mindset:** ACGR promotes cooperation while being robust to various opponent behaviors. By initially cooperating and gradually reciprocating, it encourages others to cooperate as well. The punishment mechanism ensures that opponents are held accountable for their actions, while the reciprocity adjustment allows the strategy to adapt to changing cooperation levels.
'''

description_EXPLOITATIVE_513 = '''
I propose a strategy called "Adaptive Exploiter" that balances cooperation and defection to maximize payoffs while adapting to various opponent behaviors. The strategy relies solely on game parameters (n, r, k) and history.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 0.5. This allows us to gather information about opponents' initial behavior.
2. **Exploitation Mode**: If the total number of cooperators in the previous round is less than n/2, switch to exploitation mode. In this mode:
	* Cooperate if the expected payoff from cooperation (based on previous rounds) is higher than the expected payoff from defection.
	* Defect otherwise.
3. **Punishment Mode**: If an opponent defects while we cooperated in the previous round, punish them by defecting (D) for the next two rounds. This discourages opponents from exploiting us.
4. **Adaptation**: After each round, update our internal model of opponents' behavior using a simple moving average:
	* Update the average cooperation rate of opponents: `avg_coop_rate = (avg_coop_rate \* (r-1) + num_cooperators_last_round / n) / r`
	* If the average cooperation rate exceeds 0.5, gradually increase our own cooperation probability over the next few rounds.
	* Conversely, if the average cooperation rate drops below 0.3, decrease our cooperation probability.

**Edge Cases:**

1. **Last Round**: In the final round (r), always defect (D). This ensures we don't leave any "free" resources for others to exploit.
2. **Ties in Expected Payoffs**: If expected payoffs from cooperation and defection are equal, cooperate with a probability of 0.5.

**Pseudocode:**
```
Initialize:
avg_coop_rate = 0
coop_probability = 0.5
punishment_mode = False

For each round t:
    if t == 1:
        # Initial Cooperation
        action = C with probability 0.5, else D
    elif punishment_mode:
        action = D
        punishment_mode = False
    else:
        # Exploitation Mode
        expected_coop_payoff = (k/n) \* num_cooperators_last_round + private_payoff_from_cooperation
        expected_defect_payoff = 1 - c_i + (k/n) \* total_contributions_last_round

        if expected_coop_payoff > expected_defect_payoff:
            action = C
        else:
            action = D

    # Punishment Mode
    for opponent in opponents:
        if opponent defected last round and we cooperated:
            punishment_mode = True

    # Adaptation
    update avg_coop_rate using simple moving average
    adjust coop_probability based on avg_coop_rate

# Last Round
if t == r:
    action = D
```
The Adaptive Exploiter strategy aims to balance cooperation and defection while adapting to the behavior of opponents. By punishing defectors, encouraging cooperation when beneficial, and exploiting when possible, this strategy seeks to maximize payoffs in a wide range of scenarios.
'''

description_EXPLOITATIVE_514 = '''
**Exploitative Strategy: Adaptive Tit-for-Tat with Public Goods Twist**

This strategy aims to balance exploitation and adaptability, taking into account the public goods aspect of the game.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) in the first round to gather information about opponents' behavior.
2. **Subsequent Rounds (t>1)**:
	* If the total number of cooperators in the previous round is greater than or equal to the multiplication factor k, cooperate (C). This encourages continued cooperation when it's beneficial.
	* Otherwise, defect (D).
3. **Adaptive Component**:
	+ Track the average payoff per round for each opponent over a sliding window of 2-3 rounds.
	+ Identify opponents who consistently contribute less than their fair share (i.e., k/n) and mark them as "exploitable".
	+ Against exploitable opponents, defect (D) more frequently to exploit their cooperation.

**Edge Cases:**

* **Last Round (t=r)**: Cooperate (C) in the final round, as there's no risk of retaliation.
* **Opponent Defection**: If an opponent defects in a previous round, increase the likelihood of defecting against them in subsequent rounds.

**Pseudocode:**
```markdown
def adaptive_tit_for_tat(n, k, r, history):
  # Initialize variables
  exploitable_opponents = []
  avg_payoffs = [[] for _ in range(n)]

  for t in range(1, r+1):
    if t == 1:
      action = 'C'  # Cooperate in the first round
    else:
      total_cooperators_prev_round = sum([1 if history[t-1][i] == 'C' else 0 for i in range(n)])
      if total_cooperators_prev_round >= k:
        action = 'C'
      else:
        action = 'D'

      # Update exploitable opponents and average payoffs
      for i in range(n):
        avg_payoffs[i].append(history[t-1][i])
        if len(avg_payoffs[i]) > 3:  # sliding window of 2-3 rounds
          avg_payoffs[i].pop(0)
        if sum([1 if x == 'C' else 0 for x in avg_payoffs[i]]) / len(avg_payoffs[i]) < k/n:
          exploitable_opponents.append(i)

      # Adapt against exploitable opponents
      if t > 2 and any(opponent in exploitable_opponents for opponent in range(n)):
        action = 'D'  # Defect more frequently against exploitable opponents

    return action
```
This strategy aims to balance cooperation and exploitation, adapting to the behavior of opponents while taking into account the public goods aspect of the game. By identifying exploitable opponents and defecting more frequently against them, it seeks to maximize its own payoff while minimizing the risk of retaliation.
'''

description_EXPLOITATIVE_515 = '''
I'll introduce a strategic approach that adapts to various opponent behaviors while focusing on maximizing personal payoff. This strategy is called "Adaptive Exploiter" (AE).

**Decision Rules:**

1. **Initial Round**: In the first round, defect (play D). This allows us to gather information about opponents' initial tendencies without committing to cooperation.
2. **Observation Phase**: For rounds 2-5, cooperate (play C) if at least half of the opponents cooperated in the previous round; otherwise, defect. This phase helps identify potential cooperators and estimates the group's overall willingness to contribute.
3. **Exploitation Phase**: After the observation phase (rounds 6+), apply the following rules:
	* If the average cooperation rate among opponents over the last 3 rounds is above 0.5, defect. This indicates that opponents are willing to cooperate, and we can exploit their generosity.
	* Otherwise, cooperate if our current payoff is below the average opponent's payoff over the last 3 rounds; otherwise, defect. This ensures we don't fall behind in terms of payoff and encourages cooperation when it benefits us.

**Edge Cases:**

1. **Last Round**: In the final round (r), always defect. There's no incentive to cooperate, as there are no future rounds to benefit from reciprocal behavior.
2. **Ties in Cooperation Rate**: If the average cooperation rate among opponents is exactly 0.5, consider it a "tie." In this case, cooperate if our current payoff is below the average opponent's payoff; otherwise, defect.

**Pseudocode:**

```
def AdaptiveExploiter(game_state):
    # Initial Round
    if game_state.round == 1:
        return 'D'

    # Observation Phase (rounds 2-5)
    elif game_state.round <= 5:
        opponent_cooperation_rate = count_cooperators(game_state.previous_round) / n
        if opponent_cooperation_rate >= 0.5:
            return 'C'
        else:
            return 'D'

    # Exploitation Phase (rounds 6+)
    else:
        avg_opponent_cooperation_rate = average_cooperation_rate(game_state.last_3_rounds) / 3
        if avg_opponent_cooperation_rate > 0.5:
            return 'D'
        elif our_payoff < average_opponent_payoff(game_state.last_3_rounds):
            return 'C'
        else:
            return 'D'

def count_cooperators(round_actions):
    # Count the number of cooperators in a given round
    ...

def average_cooperation_rate(last_3_rounds):
    # Calculate the average cooperation rate among opponents over 3 rounds
    ...

def our_payoff(game_state):
    # Calculate our current payoff based on game state
    ...

def average_opponent_payoff(last_3_rounds):
    # Calculate the average opponent's payoff over 3 rounds
    ...
```

The Adaptive Exploiter strategy aims to balance short-term gains with long-term adaptability, making it a robust and exploitative approach in the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_516 = '''
**Exploitative Strategy: "Adaptive K-threshold"**

This strategy is designed to exploit opponents' behaviors while adapting to changing conditions in the game.

**Decision Rules:**

1. **Initial Round:** In the first round, play C (Cooperate) to gather information about opponents' initial actions and potential cooperation levels.
2. **General Rule:** For rounds t > 1, calculate the average number of cooperators (C_avg) over the previous rounds. Then, use the following decision rule:

   If (k * C_avg / n) ≥ (1 - 1/k), play C; otherwise, play D.

   This rule checks if the expected public good payoff from cooperating is greater than or equal to the threshold value (1 - 1/k). The threshold value represents a balance between private and public payoffs. If the condition is met, cooperate; otherwise, defect.
3. **Last Round:** In the last round (t = r), play D (Defect) regardless of previous actions. This ensures maximum personal payoff in the final round.

**Edge Cases:**

* If all opponents played D in the initial rounds, and C_avg ≈ 0, play D to avoid contributing to a non-existent public good.
* If an opponent's action is unknown or unobserved, assume they defected (D) for the purpose of calculating C_avg.

**Pseudocode:**
```
function AdaptiveKThreshold(n, k, r)
  // Initialize variables
  C_avg = 0
  previous_cooperators = []

  // First round: Cooperate to gather information
  if t == 1:
    action = C
  else:
    // Calculate average number of cooperators (C_avg)
    for i in range(1, n):
      if opponent_i's_action[t-1] == C:
        previous_cooperators.append(opponent_i)

    C_avg = len(previous_cooperators) / n

    // Apply decision rule
    if k * C_avg / n >= 1 - 1/k:
      action = C
    else:
      action = D

  // Last round: Defect to maximize personal payoff
  if t == r:
    action = D

  return action
```
This strategy is designed to be adaptive, exploitative, and robust against various opponent behaviors. It gathers information in the initial rounds, adjusts its actions based on observed cooperation levels, and defects in the final round to maximize personal payoff.
'''

description_EXPLOITATIVE_517 = '''
**Strategy Name: Adaptive Exploiter (AE)**

The AE strategy is designed to exploit opponents' behaviors while adapting to the game's dynamics. It combines elements of Tit-for-Tat (TFT) and Win-Stay-Lose-Shift (WSLS) with a twist to handle the public goods game specifics.

**Decision Rules:**

1. **First Round:** Cooperate (C). This sets a positive tone and gathers information about opponents' behaviors.
2. **Rounds 2-r:**
	* If the number of cooperators in the previous round is greater than or equal to (n+1)/2, play C. This encourages cooperation when it's beneficial.
	* Otherwise, if the total payoff in the previous round was higher than the expected payoff from defecting (i.e., 1 + (k/n) \* Σ(j=1 to n) c_j > k), play D. This exploits opponents' cooperation.
	* In all other cases, play C.
3. **Last Round:** Defect (D). Since there's no future interaction, maximizing personal payoff takes priority.

**Additional Logic:**

* **Opponent Monitoring:** Track the number of cooperators and total payoffs for each round.
* **Payoff Thresholding:** If an opponent's payoff is consistently higher than the expected payoff from cooperation, assume they're exploiting others and adjust the strategy to defect more frequently.

Pseudocode:
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    cooperators = 0
    total_payoff = 0
    opponent_payoffs = [0] * n

    for t in range(r):
        if t == 0:  # First round
            action = 'C'
        else:
            if cooperators >= (n + 1) / 2:  # Cooperate when beneficial
                action = 'C'
            elif total_payoff > k:  # Exploit opponents' cooperation
                action = 'D'
            else:
                action = 'C'

        # Update opponent monitoring and payoff thresholding
        cooperators += (action == 'C')
        total_payoff += calculate_payoff(action, cooperators)
        opponent_payoffs[t % n] = total_payoff

    return action
```
**Edge Cases:**

* In the first round, AE always cooperates to gather information.
* In the last round, AE defects to maximize personal payoff.

**Exploitative Mindset:**
The Adaptive Exploiter strategy prioritizes exploiting opponents' cooperation when possible while adapting to the game's dynamics. By monitoring opponent payoffs and adjusting its behavior accordingly, AE can take advantage of opponents who are too cooperative or not exploitative enough.
'''

description_EXPLOITATIVE_518 = '''
**Exploitative Strategy: "Adaptive Predator"**

**Decision Rules:**

1. **Initial Rounds (t < n/2):** Defect (D) to gather information about opponents' behavior and identify potential cooperators.
2. **Mid-Game (n/2 ≤ t < r - 1):**
	* If the average cooperation rate among opponents in previous rounds is above a threshold (θ = k/n), cooperate (C).
	* Otherwise, defect (D) to exploit non-cooperators.
3. **Late Game (t ≥ r - 1):** Cooperate (C) if the total payoff from cooperating in the current round would exceed the expected payoff from defecting, considering the remaining rounds.

**Edge Cases:**

1. **First Round (t = 1):** Defect (D) to gather initial information about opponents.
2. **Last Round (t = r):** Cooperate (C) if the total payoff from cooperating in this round would exceed the expected payoff from defecting, considering no remaining rounds.

**Pseudocode:**
```markdown
function AdaptivePredator(n, k, r):
  // Initialize variables
  theta = k / n  // cooperation threshold
  opponent_cooperation_rates = [0] * n  // track opponents' cooperation rates

  for t in range(1, r + 1):
    if t < n / 2:
      action = D  // defect in initial rounds
    elif n / 2 <= t < r - 1:
      avg_opponent_cooperation_rate = sum(opponent_cooperation_rates) / n
      if avg_opponent_cooperation_rate > theta:
        action = C  // cooperate if opponents are cooperative
      else:
        action = D  // defect otherwise
    else:  // late game (t >= r - 1)
      expected_defect_payoff = 1 + (k / n) * sum(opponent_cooperation_rates)
      expected_cooperate_payoff = (k / n) * (n - 1)
      if expected_cooperate_payoff > expected_defect_payoff:
        action = C
      else:
        action = D

    // Update opponent cooperation rates based on their actions in previous rounds
    for i in range(n):
      if opponents[i].action[t - 1] == C:
        opponent_cooperation_rates[i] += 1 / (t - 1)

    return action
```
**Exploitative Mindset:**

This strategy aims to exploit non-cooperative behavior while adapting to the opponents' strategies. By defecting in initial rounds, we gather information about opponents and identify potential cooperators. As the game progresses, we adjust our cooperation threshold (θ) based on the average cooperation rate among opponents. If opponents are cooperative, we cooperate to maximize payoffs; otherwise, we defect to exploit non-cooperators.

In late-game scenarios, we prioritize cooperating if it leads to higher expected payoffs, considering no remaining rounds. This approach balances short-term exploitation with long-term cooperation, making our strategy robust and adaptive to a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_519 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive throughout the game.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Play Cooperate (C) in Round 1 to gather information about opponents' initial tendencies.
	* In Rounds 2-3, play Defect (D) if the majority (> n/2) of opponents played D in the previous round. Otherwise, play C.
2. **Adaptive Phase (Rounds 4-r)**:
	* Calculate the "Cooperation Ratio" (CR) as the ratio of total cooperators to total players (n) in the previous round.
	* If CR > k/n, play Cooperate (C). This indicates that opponents are contributing enough to make cooperation worthwhile.
	* If CR ≤ k/n, play Defect (D). This suggests that opponents are not contributing sufficiently, making it more profitable to defect.
3. **Edge Case Handling:**
	+ In the last round (r), play Defect (D) regardless of previous observations. There is no future benefit from cooperation in this case.

**Additional Logic:**

* If an opponent's action history shows a clear pattern of alternating between C and D, assume they are trying to manipulate others and play D.
* If multiple opponents exhibit this alternating behavior, prioritize playing D to maximize individual payoffs.

**Pseudocode (simplified for readability):**
```
function AdaptiveExploiter(n, k, r):
  // Initial Exploration
  if round == 1:
    return C
  elif round >= 2 and round <= 3:
    majority_defected = count(D) > n/2 in previous_round
    return D if majority_defected else C

  // Adaptive Phase
  CR = total_cooperators / n in previous_round
  if CR > k/n:
    return C
  else:
    return D

  // Edge Case Handling
  if round == r:
    return D

  // Additional Logic (optional)
  opponent_patterns = analyze_opponent_actions()
  if pattern_detected(opponent_patterns):
    return D
```
This strategy aims to balance exploitation with adaptability, making it robust against various opponent behaviors. By initially exploring the environment and adjusting its behavior accordingly, the Adaptive Exploiter can effectively exploit cooperative opponents while minimizing losses against defectors.
'''

description_EXPLOITATIVE_520 = '''
I'll design a strategic approach that balances adaptability and exploitativeness for the N-Player Public Goods Game.

**Strategy Name: Adaptive Exploitation**

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C) with a probability of 50% to gather initial information about opponents' behavior.
2. **Exploiting Defectors**: If, in the previous round, fewer than `k/n` players cooperated, defect (D) in the current round. This threshold indicates that cooperation is unlikely to yield sufficient public goods benefits.
3. **Responding to Cooperators**: If, in the previous round, at least `k/n` players cooperated, cooperate (C) in the current round. This encourages continued cooperation and maximizes public goods benefits.
4. **Punishing Free-Riders**: If a player observes an opponent defecting while others are cooperating, retaliate by defecting against that specific opponent for one round to maintain fairness.

**Handling Edge Cases:**

1. **Last Round**: In the final round (`r`), cooperate (C) if more than `k/n` players cooperated in the previous round; otherwise, defect (D).
2. **Ties and Non-integer values of k/n**: When encountering non-integer thresholds or ties in cooperation counts, use a small bias towards cooperating to maintain stability.

**Pseudocode:**
```markdown
def AdaptiveExploitation(n, k, r):
    # Initialize memory for opponents' actions and self's payoff history
    opponent_actions = [[] for _ in range(n)]
    payoffs = []

    # Iterate over rounds
    for t in range(r):
        # First round: randomize cooperation with 50% probability
        if t == 0:
            action = 'C' if random.random() < 0.5 else 'D'
        else:
            # Get opponents' actions from previous round
            prev_opponent_actions = opponent_actions

            # Check number of cooperators in previous round
            num_cooperators = sum(1 for actions in prev_opponent_actions if actions[-1] == 'C')

            # Follow decision rules based on cooperation count and k/n threshold
            if num_cooperators < k / n:
                action = 'D'  # Defect when cooperation is low
            elif num_cooperators >= k / n:
                action = 'C'  # Cooperate when cooperation is sufficient

            # Punish free-riders (optional)
            for i, opponent_action in enumerate(prev_opponent_actions):
                if opponent_action[-1] == 'D' and sum(1 for actions in prev_opponent_actions if actions[-1] == 'C') > k / n:
                    action = 'D'  # Retaliate against free-rider

        # Record own action and receive payoffs
        payoffs.append(get_payoff(action, opponent_actions))
        opponent_actions[t % len(opponent_actions)].append(action)

    return payoffs
```
This strategy balances short-term self-interest with longer-term benefits of cooperation. By adapting to opponents' behavior and responding to both defectors and cooperators, Adaptive Exploitation aims to maximize its total payoff while minimizing the risk of exploitation by others.

In a tournament setting against other AI systems, this strategy should demonstrate robustness and competitiveness across various opponent behaviors and game conditions.
'''

description_EXPLOITATIVE_521 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy aims to maximize its total payoff by exploiting the cooperative tendencies of other players while minimizing its own contributions to the public good.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about the opponent's behavior and encourage cooperation.
2. **Exploitation Phase (t > 2):**
	* If the average number of cooperators in the previous round is above a certain threshold (k/2), Defect (D). This indicates that opponents are willing to contribute, making it safe to exploit them.
	* Otherwise, Cooperate (C) if the total payoff from the public good in the previous round is higher than the private payoff from defecting. This encourages continued cooperation and maintains a stable public good.
3. **Punishment Mechanism:** If an opponent defects in a round where the Adaptive Predator cooperated, switch to Defect (D) for the next two rounds. This punishes opponents for exploiting the strategy's cooperative nature.

**Edge Cases:**

1. **Last Round (t = r):** Defect (D), as there is no future benefit from cooperation.
2. **Ties in Average Cooperators:** In case of a tie, default to Defect (D) to err on the side of caution.

**Pseudocode:**
```python
def adaptive_predator(n, k, r, history):
    # Initialize variables
    avg_cooperators = 0
    total_payoff_prev = 0

    for t in range(1, r+1):
        if t <= 2:
            action = C  # Cooperate in initial rounds
        else:
            avg_cooperators = sum(history[t-1]['cooperators']) / n
            total_payoff_prev = history[t-1]['total_payoff']

            if avg_cooperators > k/2:
                action = D  # Exploit if many cooperators
            elif total_payoff_prev > 1 - c_i:
                action = C  # Cooperate if public good is beneficial
            else:
                action = D  # Default to defect

        # Punishment mechanism
        if history[t-1]['opponent_defected'] and history[t-1]['self_cooperated']:
            action = D  # Switch to defect for two rounds

        return action
```
This strategy is designed to be robust against various opponent behaviors, as it adapts to the level of cooperation in the game. By exploiting cooperative tendencies while minimizing its own contributions, the Adaptive Predator aims to maximize its total payoff in the tournament.
'''

description_EXPLOITATIVE_522 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) to gather information about opponents' behavior and potential cooperation levels.
2. **Subsequent Rounds (t>1)**:
	* Calculate the average number of cooperators in previous rounds (`avg_cooperators`).
	* If `avg_cooperators` is greater than or equal to `n/2`, Cooperate (C). This indicates a sufficiently high level of cooperation, and contributing to the public good will yield a higher payoff.
	* Otherwise, Defect (D). With fewer cooperators, it's more beneficial to keep the endowment privately.

**Adaptive Component:**

1. ** Cooperation Threshold Adjustment**: After every 5 rounds (`t % 5 == 0`), recalculate `avg_cooperators` and adjust the cooperation threshold as follows:
	* If `avg_cooperators` has increased, decrease the threshold by `0.1n`.
	* If `avg_cooperators` has decreased, increase the threshold by `0.1n`.

This adaptive component allows the strategy to respond to changes in opponents' behavior over time.

**Edge Cases:**

1. **Last Round (t=r)**: Defect (D). As there's no future interaction, maximize private payoff.
2. **Ties**: In case of a tie in `avg_cooperators`, Cooperate (C).

**Exploitative Mindset Alignment:**

The Adaptive Exploiter strategy is designed to exploit the cooperative tendencies of opponents while minimizing its own contributions to the public good when cooperation levels are low. By cooperating initially and adapting to the average number of cooperators, this strategy aims to maximize its payoff by taking advantage of others' cooperation.

Pseudocode (simplified):
```python
def adaptive_exploiter(n, r, k, history):
    # Initialize variables
    avg_cooperators = 0
    threshold = n / 2

    for t in range(1, r + 1):
        if t == 1:  # Initial round
            action = 'C'
        else:
            # Calculate average cooperators
            avg_cooperators = sum(history[:t]) / (n * t)

            # Adjust cooperation threshold every 5 rounds
            if t % 5 == 0:
                if avg_cooperators > threshold:
                    threshold -= 0.1 * n
                elif avg_cooperators < threshold:
                    threshold += 0.1 * n

            # Choose action based on average cooperators and threshold
            if avg_cooperators >= threshold:
                action = 'C'
            else:
                action = 'D'

        # Update history and return action
        history.append(1 if action == 'C' else 0)
        return action

    # Last round: Defect (D)
    return 'D'
```
This strategy will be implemented as an algorithm to compete in the tournament.
'''

description_EXPLOITATIVE_523 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive throughout the game.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about opponents' behavior and potential cooperation levels.
2. **Exploitation Phase:** For t > 2, calculate the average payoff of cooperators (avg_coop_payoff) and defectors (avg_defect_payoff) over the previous two rounds. If avg_coop_payoff > avg_defect_payoff, cooperate (C). Otherwise, defect (D).
3. **Punishment Mechanism:** If a player defects while others cooperate (i.e., a "free rider"), identify them as a "defector" and punish by defecting against them in the next round.
4. **Adaptation to Changing Environment:** Recalculate avg_coop_payoff and avg_defect_payoff every two rounds to adapt to changing opponent behaviors.

**Edge Case Handling:**

1. **First Round (t = 1):** Cooperate (C) unconditionally.
2. **Last Round (t = r):** Defect (D) unconditionally, as there is no future interaction to consider.
3. **Tie-breaking:** In case of a tie between avg_coop_payoff and avg_defect_payoff, cooperate (C).

**Pseudocode:**
```markdown
# Initialize variables
avg_coop_payoff = 0
avg_defect_payoff = 0
defectors = []

# Loop through each round
for t in range(1, r + 1):
    if t <= 2:
        # Initial rounds: cooperate
        action = C
    else:
        # Calculate average payoffs for cooperators and defectors
        avg_coop_payoff = calculate_avg_payoff(C)
        avg_defect_payoff = calculate_avg_payoff(D)

        # Exploitation phase
        if avg_coop_payoff > avg_defect_payoff:
            action = C
        else:
            action = D

        # Punishment mechanism
        for opponent in opponents:
            if opponent.defected and others_cooperated:
                defectors.append(opponent)
                action = D  # punish defector

    # Update variables for next round
    update_avg_payoffs(avg_coop_payoff, avg_defect_payoff)
    update_defectors(defectors)

# Last round: defect unconditionally
if t == r:
    action = D
```
The Adaptive Predator strategy aims to balance exploitation and cooperation, adapting to the evolving game environment while punishing free riders. By cooperating initially and then adjusting its behavior based on observed payoffs, this strategy seeks to maximize its own payoff while minimizing the potential for exploitation by others.
'''

description_EXPLOITATIVE_524 = '''
Here's a strategic approach for playing the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C). This is an exploratory move to gauge the opponents' behavior and gather information about their strategies.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round (t-1) is less than n/2, Defect (D).
	* If the total number of cooperators in the previous round (t-1) is greater than or equal to n/2 and the player's payoff in the previous round (π_i,t-1) was greater than or equal to 1.5, Cooperate (C). This indicates a potentially cooperative environment.
	* If the total number of cooperators in the previous round (t-1) is greater than or equal to n/2 and the player's payoff in the previous round (π_i,t-1) was less than 1.5, Defect (D).
3. **Last Round (t=r):** Defect (D). In the final round, there's no incentive to cooperate since there are no future rounds to consider.

Pseudocode:
```
Function AE(n, r, k):
    // Initialize variables
    cooperators = 0
    payoff_history = []

    For t from 1 to r:
        If t == 1:  // Initial Round
            action = C
        Else:  // Subsequent Rounds
            if cooperators < n/2:
                action = D
            ElseIf cooperators >= n/2 and payoff_history[t-1] >= 1.5:
                action = C
            Else:
                action = D

        If t == r:  // Last Round
            action = D

        // Update variables
        cooperators = count(cooperators in round t)
        payoff_history.append(calculate_payoff(n, k, action))

    Return actions
```
**Rationale:**

The Adaptive Exploiter strategy aims to balance exploration and exploitation. By cooperating initially, it gathers information about the opponents' strategies and adapts accordingly.

In subsequent rounds, AE assesses the level of cooperation in the previous round and adjusts its behavior based on the total number of cooperators. If there are too few cooperators, it defects to maximize its payoff. When there's a sufficient level of cooperation, AE considers its own payoff in the previous round: if it was relatively high (≥1.5), it continues cooperating; otherwise, it defects.

In the last round, AE defects since there's no strategic benefit to cooperating without future interactions.

**Robustness and Adaptability:**

AE is designed to be robust against various opponent strategies:

* **Cooperative opponents:** AE adapts by continuing cooperation when faced with a cooperative environment.
* **Defective opponents:** AE defects when facing insufficient cooperation, ensuring it doesn't become overly exploited.
* **Mixed-strategy opponents:** AE's adaptive nature allows it to adjust its behavior based on the observed level of cooperation.

While no strategy can guarantee optimal performance against all possible opponent strategies, Adaptive Exploiter is designed to be a strong and robust contender in a tournament setting.
'''

description_EXPLOITATIVE_525 = '''
**Exploitative Strategy: "Adaptive Parasite"**

**Decision Rules:**

1. **First Round:** Defect (D) - This allows us to observe how other players behave and gather information about their strategies.
2. **Subsequent Rounds:**
	* If the total number of cooperators in the previous round is greater than or equal to `n/2`, Cooperate (C) - We want to take advantage of the public good when it's likely to be substantial.
	* Otherwise, Defect (D) - We'll try to exploit the situation by keeping our endowment while others contribute.
3. **Adaptive Component:**
	* Track the number of rounds where we've defected and received a payoff greater than or equal to 2 (`defection_success_count`).
	* If `defection_success_count` exceeds `r/4`, start cooperating (C) in subsequent rounds - We'll switch to cooperation if our exploitative strategy is consistently successful.
	* Reset `defection_success_count` every `r/2` rounds to ensure adaptability.

**Edge Cases:**

1. **Last Round:** Defect (D) - In the final round, there's no incentive to cooperate since we won't benefit from the public good in future rounds.
2. **Tiebreaker:** If the number of cooperators is exactly `n/2`, defect (D) - In case of a tie, our default behavior is to exploit.

**Pseudocode:**
```python
def adaptive_parasite(n, r, k, round_num, prev_round_actions):
    if round_num == 1:
        return "D"  # Defect in the first round
    
    total_cooperators = sum(1 for action in prev_round_actions if action == "C")
    
    if total_cooperators >= n / 2:
        return "C"  # Cooperate if many cooperated previously
    else:
        return "D"  # Defect otherwise

def update_defection_success_count(defection_success_count, round_num, payoff):
    if payoff >= 2:
        defection_success_count += 1
    
    if round_num % (r / 2) == 0:  # Reset every r/2 rounds
        defection_success_count = 0
    
    return defection_success_count

def get_action(n, r, k, round_num, prev_round_actions, defection_success_count):
    action = adaptive_parasite(n, r, k, round_num, prev_round_actions)
    
    if defection_success_count > r / 4:
        action = "C"  # Switch to cooperation if exploitative strategy is successful
    
    return action
```
This Adaptive Parasite strategy aims to exploit the public goods game by defecting when possible and cooperating when the benefits of doing so are substantial. By adapting to the behavior of other players, we can maximize our payoffs while minimizing the risk of being exploited ourselves.
'''

description_EXPLOITATIVE_526 = '''
To design a exploitative strategy for the N-Player Public Goods Game that is adaptive and robust to various opponent behaviors, we'll implement a hybrid approach combining reactive elements with a proactive stance. Our goal is to maximize our total payoff over all rounds by exploiting the cooperative tendencies of other players while also considering the game's parameters.

### Decision Rules

Our strategy, named "Adaptive Exploiter," will use a combination of monitoring opponents' behaviors and adjusting our actions based on observed cooperation rates in the population.

1. **First Round**: Cooperate (C) to encourage potential cooperators and observe initial behavior.
   
2. **Subsequent Rounds**:
   - If the average cooperation rate among other players is above a certain threshold (`coop_threshold`), we cooperate (C). This encourages further cooperation when the group seems willing to contribute.
     ```python
if avg_coop_rate > coop_threshold:
    action = 'C'
```
   - Otherwise, if our current payoff is significantly below the average payoff of other players (`payoff_diff_threshold`), we defect (D) to quickly improve our standing.
     ```python
elif my_payoff < avg_payoff_others - payoff_diff_threshold:
    action = 'D'
```
   - In all other cases, if a majority (or more than `majority_defect_threshold`%) of players defected in the previous round, we defect to avoid being exploited.
     ```python
elif prev_round_defectors > n * majority_defect_threshold / 100:
    action = 'D'
```
   - Otherwise, we cooperate. This default behavior leans towards cooperation when no exploitation is clearly advantageous.

### Edge Cases

- **Last Round**: In the final round (`r`), if our current total payoff is among the top half of players and we're not significantly behind any player (`payoff_gap_threshold`), we cooperate to avoid a potential downward spiral in payoffs from mutual defection.
  ```python
if t == r and my_total_payoff >= avg_total_payoff + payoff_gap_threshold:
    action = 'C'
```
- **Early Rounds**: In the initial rounds, if cooperation levels are low, consider defecting more frequently to signal that exploitation will be punished.

### Parameters

The strategy's performance can be tuned with the following parameters:

- `coop_threshold`: The minimum average cooperation rate among other players required for us to cooperate.
- `payoff_diff_threshold`: The difference in payoffs below which we choose to defect if behind others.
- `majority_defect_threshold`: The percentage of players that must have defected in a round for us to consider widespread defection and respond accordingly.
- `payoff_gap_threshold`: The gap in total payoff by the last round that makes cooperation worthwhile.

### Implementation

This strategy will be implemented as an algorithm, with clear conditional statements reflecting the decision rules outlined above. It's designed to adapt to various behaviors and exploit non-cooperative tendencies while still considering cooperation when beneficial.

By balancing exploitation with selective cooperation based on observed behavior and game state, "Adaptive Exploiter" aims to perform well against a variety of opponents in a tournament setting.
'''

description_EXPLOITATIVE_527 = '''
**Exploitative Strategy: "Adaptive Parasite"**

This strategy aims to exploit the cooperative tendencies of other players while minimizing its own contributions to the public good.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) with probability 0.5 in each of the first three rounds.
	* Observe the number of cooperators and total payoffs in these rounds.
2. **Adaptive Phase (Rounds 4-r)**:
	* Calculate the average cooperation rate (ACR) among all players over the previous rounds.
	* If ACR ≥ 0.5, defect (D) with probability 1 - (k/n) \* ACR.
	* Otherwise, cooperate (C) with probability (k/n) \* ACR.
3. **Late-Game Exploitation (Last 2 rounds)**:
	* Defect (D) in the second-to-last round, regardless of previous observations.
	* In the last round, defect (D) if the average cooperation rate over the entire game is ≥ 0.5; otherwise, cooperate (C).

**Pseudocode:**
```python
def adaptive_parasite(n, k, r):
    # Initialize variables
    total_cooperators = 0
    total_payoffs = 0
    ACR = 0

    for round in range(1, r+1):
        if round <= 3:
            # Initial exploration phase
            action = random.choice([C, D])  # 50% chance of cooperating
        else:
            # Adaptive phase
            ACR = total_cooperators / (n * (round - 1))
            if ACR >= 0.5:
                action = D with probability 1 - (k/n) * ACR
            else:
                action = C with probability (k/n) * ACR

        # Late-game exploitation phase
        if round == r-1:
            action = D
        elif round == r:
            if ACR >= 0.5:
                action = D
            else:
                action = C

        # Update observations
        total_cooperators += sum(1 for player in players if player.action == C)
        total_payoffs += sum(player.payoff for player in players)

    return action
```
**Rationale:**

This strategy starts by exploring the game environment through random cooperation and defection. It then adapts to the observed cooperation rate, exploiting the public good when others are likely to cooperate and free-riding when they don't.

In the late-game phase, it exploits the anticipation of cooperation from other players by defecting in the second-to-last round, and then defects again in the last round if the overall cooperation rate is high. Otherwise, it cooperates in the final round to maintain a positive payoff.

This strategy should be robust against various opponent behaviors, as it adapts to the observed cooperation rate and exploits the public good accordingly.
'''

description_EXPLOITATIVE_528 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. This strategy relies on game parameters and history, without assuming coordination or communication with other players.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about opponents' initial tendencies.
2. **Observation Phase (t > 2):**
	* Calculate the average cooperation rate of all opponents over the last two rounds (`avg_coop_rate`).
	* If `avg_coop_rate` ≥ 0.5, Cooperate (C) in the current round.
	* Otherwise, Defect (D) in the current round.
3. **Exploitation Phase:**
	* Monitor the total payoff of each opponent over the last three rounds (`total_payoff_i`).
	* Identify the opponent with the highest `total_payoff_i` as the "Target" (`target_id`).
	* If the Target is Cooperating, Defect (D) to exploit their cooperation.
	* If the Target is Defecting, Cooperate (C) to punish their defection and encourage cooperation.

**Edge Cases:**

1. **Last Round (t = r):** Always Defect (D), as there's no future opportunity to cooperate or punish opponents.
2. **Tie in Total Payoff:** In case of a tie for the Target, choose the opponent with the highest cooperation rate over the last two rounds.

**Pseudocode:**

```
 Initialize:
  - Round counter (`t`)
  - Average cooperation rate (`avg_coop_rate`)
  - Total payoff array (`total_payoff_i`) for each opponent
  - Target ID (`target_id`)

 Loop through rounds:
  if (t ≤ 2) {
    // Initial Rounds: Cooperate to gather information
    Action = C;
  } else {
    // Observation Phase
    avg_coop_rate = CalculateAverageCooperationRate(opponents, t-1, t-2);
    if (avg_coop_rate ≥ 0.5) {
      Action = C;
    } else {
      Action = D;
    }

    // Exploitation Phase
    target_id = IdentifyTarget(total_payoff_i);
    if (opponent_action[target_id] == C) {
      Action = D; // Exploit cooperation
    } else {
      Action = C; // Punish defection
    }
  }

  // Last Round: Always Defect
  if (t == r) {
    Action = D;
  }

  // Update variables for next round
  total_payoff_i = UpdateTotalPayoffs(opponents, t);
```

**Rationale:**

The Adaptive Predator strategy balances exploration and exploitation. By cooperating in initial rounds, it gathers information about opponents' tendencies and adapts to their behavior. The Observation Phase helps identify cooperative opponents, while the Exploitation Phase targets the most profitable opponent to exploit or punish.

This strategy is designed to perform well against a wide range of opponent behaviors, from pure cooperators to defectors, and everything in between. Its adaptability and robustness make it an effective exploitative strategy for the N-Player Public Goods Game.
'''

description_EXPLOITATIVE_529 = '''
**Exploitative Strategy: "Adaptive Defector"**

The Adaptive Defector strategy aims to maximize its own payoff by exploiting the cooperation of others while minimizing its own contributions to the public good.

**Decision Rules:**

1. **Initial Round (t=1):** Defect (D) in the first round to gather information about opponents' behaviors.
2. **Observation Phase (t=2 to t=r-1):**
	* If the total number of cooperators in the previous round is less than or equal to `n/2`, cooperate (C) with probability `(k/n) * (total_cooperators / n)`. This encourages cooperation when there are enough contributors.
	* Otherwise, defect (D).
3. **Exploitation Phase (t=r-1 to t=r):**
	* If the total number of cooperators in the previous round is greater than `n/2`, defect (D). This takes advantage of others' contributions.
	* Otherwise, cooperate (C) with probability `(k/n) * (total_cooperators / n)`.

**Edge Cases:**

* **Last Round (t=r):** Defect (D) to maximize individual payoff, as there are no future rounds to consider.
* **Tiebreaker:** In case of a tie in the number of cooperators, cooperate (C) with probability `(k/n) * (total_cooperators / n)`.

**Pseudocode:**
```python
def adaptive_defector(n, k, r, history):
    if t == 1:
        return 'D'  # Defect in the first round

    total_cooperators = sum(history[t-1])
    if total_cooperators <= n/2:
        cooperate_prob = (k/n) * (total_cooperators / n)
        return 'C' if random.random() < cooperate_prob else 'D'
    else:
        return 'D'

    # Exploitation Phase
    if t >= r-1:
        if total_cooperators > n/2:
            return 'D'
        else:
            cooperate_prob = (k/n) * (total_cooperators / n)
            return 'C' if random.random() < cooperate_prob else 'D'

    # Last Round
    if t == r:
        return 'D'
```
This strategy is designed to be exploitative, taking advantage of others' cooperation while minimizing its own contributions. By adapting to the number of cooperators in each round, it aims to maximize its individual payoff in a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_530 = '''
Here's a robust and exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to gather information about opponents' behavior.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round (t-1) is greater than or equal to n/2, Defect (D). This indicates that many players are cooperating, and exploiting this situation can lead to higher payoffs.
	* Otherwise, Cooperate (C) if the opponent's average cooperation rate (from t=1 to t-1) is above a certain threshold (e.g., 0.5). This encourages continued cooperation among opponents who have shown a willingness to cooperate.
	* If neither condition is met, Defect (D).
3. **Opponent Analysis:** Track the cooperation rates of each opponent and update the average cooperation rate after each round.
4. **Adjusting Cooperation Threshold:** Every 10 rounds, adjust the cooperation threshold based on the overall cooperation rate among all players. If the overall cooperation rate increases, raise the threshold; if it decreases, lower the threshold.

**Pseudocode:**
```python
# Initialize variables
n = number_of_players
k = multiplication_factor
t = current_round
opponent_cooperation_rates = [0] * n

def adaptive_exploiter(t):
    if t == 1:
        return C  # Cooperate in the first round
    
    total_cooperators_prev_round = sum(cooperation_status_prev_round)
    
    if total_cooperators_prev_round >= n / 2:
        return D  # Defect if many cooperated previously
    
    opponent_avg_coop_rate = calculate_opponent_avg_coop_rate(opponent_cooperation_rates)
    
    if opponent_avg_coop_rate > COOPERATION_THRESHOLD:
        return C  # Cooperate if opponents are generally cooperative
    else:
        return D

def update_opponent_analysis(cooperation_status):
    for i, status in enumerate(cooperation_status):
        opponent_cooperation_rates[i] = (opponent_cooperation_rates[i] * (t - 1) + status) / t

def adjust_cooperation_threshold():
    global COOPERATION_THRESHOLD
    if overall_cooperation_rate > PREVIOUS_OVERALL_COOP_RATE:
        COOPERATION_THRESHOLD += 0.1
    elif overall_cooperation_rate < PREVIOUS_OVERALL_COOP_RATE:
        COOPERATION_THRESHOLD -= 0.1

# Main loop
while t <= r:
    cooperation_status = get_cooperation_status_of_all_players(t)
    action = adaptive_exploiter(t)
    update_opponent_analysis(cooperation_status)
    
    if t % 10 == 0:  # Every 10 rounds, adjust the threshold
        adjust_cooperation_threshold()
```
**Edge Cases:**

* In the last round (t=r), Defect (D) to maximize payoff.
* If an opponent's cooperation rate is undefined or unreliable, use a default value (e.g., 0.5).

This Adaptive Exploiter strategy balances exploiting opponents' cooperative behavior with adapting to changes in their actions. By analyzing opponents' past behavior and adjusting its own cooperation threshold, the AE strategy can effectively navigate various environments and exploit any available opportunities for higher payoffs.
'''

description_EXPLOITATIVE_531 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

Decision Rules:

1. **Initial Cooperation**: In the first round (t=1), cooperate (play C) with a probability of 0.5. This initial cooperation serves as a probe to gather information about opponents' behaviors.
2. **Exploit Defectors**: If an opponent defects (plays D) in the previous round, defect against them in the current round. This rule encourages opponents to cooperate by punishing their defections.
3. **Free-Ride on Cooperators**: If an opponent cooperates (plays C) in the previous round, and there are at least two other players who also cooperated, defect against the cooperator in the current round. This rule exploits cooperative opponents by taking advantage of their contributions without reciprocating.
4. **Adaptive Threshold**: Maintain a threshold value `T` that represents the minimum number of cooperators required to make cooperation worthwhile. Initialize `T` to 0.5n (half of the total players). If the actual number of cooperators in the previous round exceeds `T`, increase `T` by 1; otherwise, decrease `T` by 1.
5. **Cooperate if Threshold Met**: If the number of cooperators in the previous round meets or exceeds the threshold value `T`, cooperate (play C) in the current round.

Pseudocode:
```
Initialize T = 0.5n
For each round t:
  If t == 1:
    Cooperate with probability 0.5
  Else:
    For each opponent i:
      If opponent i defected in previous round:
        Defect against opponent i
      Else if opponent i cooperated and at least two other players cooperated:
        Defect against opponent i
    End For
    If actual number of cooperators >= T:
      Cooperate
      If actual number of cooperators > T:
        Increase T by 1
      Else:
        Decrease T by 1
```
Edge Cases:

* **Last Round**: In the final round (t=r), always defect, as there is no future benefit to cooperation.
* **Early Rounds**: In early rounds (t < 3), prioritize gathering information about opponents' behaviors over exploiting them. If an opponent defects in these rounds, still cooperate with a probability of 0.5.

This strategy aims to balance exploration and exploitation by:

1. Initially cooperating to gather information
2. Punishing defectors to encourage cooperation
3. Exploiting cooperators who contribute without reciprocation
4. Adapting the threshold value based on opponents' behaviors

The Adaptive Exploiter strategy is designed to be robust against various opponent behaviors, including pure cooperation, pure defection, and mixed strategies.
'''

description_EXPLOITATIVE_532 = '''
**Exploitative Strategy: "Adaptive Defector with Conditional Cooperation"**

This strategy is designed to exploit opponents' cooperative tendencies while adapting to changing game conditions.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) to gather information about opponents' behavior and potential cooperation levels.
2. **Subsequent Rounds (t>1)**:
	* If the total number of cooperators in the previous round is less than or equal to n/2, Defect (D).
	* If the total number of cooperators in the previous round is greater than n/2, Cooperate (C) with a probability p, where p = (k/n) \* (total_cooperators_previous_round / n).
	* If the previous round's payoff was higher when defecting, Defect (D). Otherwise, follow the above rules.
3. **Last Round (t=r)**: Defect (D) to maximize individual payoff.

**Pseudocode:**
```
 Initialize variables:
  - total_cooperators_previous_round = 0
  - previous_payoff_defecting = false

For each round t:
  If t == 1:
    action = C
  Else:
    total_cooperators_previous_round = count(C) from previous round
    p = (k/n) \* (total_cooperators_previous_round / n)
    If total_cooperators_previous_round <= n/2 or previous_payoff_defecting:
      action = D
    Else if random() < p:
      action = C
    Else:
      action = D
  End If

  If t == r:
    action = D

  Update variables for next round:
  - total_cooperators_previous_round = count(C) from current round
  - previous_payoff_defecting = (payoff when defecting > payoff when cooperating)
```
**Rationale:**

1. **Initial cooperation**: Gather information about opponents' behavior and potential cooperation levels.
2. **Adaptive defection**: Exploit situations where the number of cooperators is low, maximizing individual payoffs.
3. **Conditional cooperation**: Cooperate with a probability proportional to the previous round's cooperation level, taking into account the multiplication factor k.
4. **Last-round exploitation**: Defect in the final round to maximize individual payoff.

This strategy balances exploitation and adaptability, making it robust against various opponent behaviors. By responding to changes in cooperation levels and payoffs, the Adaptive Defector with Conditional Cooperation can effectively exploit opponents' cooperative tendencies while protecting itself from being exploited.
'''

description_EXPLOITATIVE_533 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics. This strategy focuses on maximizing its own payoff by identifying and capitalizing on cooperative opponents.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3):**
Cooperate in the first round to gather information about the opponent's behavior.
In rounds 2 and 3, defect if at least one opponent cooperated in the previous round; otherwise, cooperate.
2. **Adaptive Exploitation (Rounds 4-r):**
Maintain a "cooperation threshold" variable, `θ`, initialized to 0.5 (i.e., 50% cooperation rate).
Calculate the average cooperation rate of all opponents over the last 3 rounds (`avg_coop_rate`).
If `avg_coop_rate` > `θ`, defect in the current round.
Otherwise, cooperate if at least one opponent cooperated in the previous round; otherwise, defect.

**Pseudocode:**
```python
def adaptive_predator(n, k, r):
  theta = 0.5  # cooperation threshold
  avg_coop_rate = 0

  for t in range(1, r+1):
    if t <= 3:
      # Initial exploration phase
      if t == 1:
        action = COOPERATE
      elif any(opponent_actions[t-2]):
        action = DEFECT
      else:
        action = COOPERATE
    else:
      # Adaptive exploitation phase
      avg_coop_rate = sum([c for c in opponent_actions[t-3:t]]) / (n * 3)
      if avg_coop_rate > theta:
        action = DEFECT
      elif any(opponent_actions[t-1]):
        action = COOPERATE
      else:
        action = DEFECT

    # Update cooperation threshold
    theta = max(0, min(1, theta + (avg_coop_rate - theta) / 10))
```
**Edge Cases:**

* **Last Round:** In the final round, defect regardless of previous behavior to maximize payoff.
* **Opponent Cooperation Rate = 0:** If all opponents consistently defect, cooperate in every round to maintain a minimal level of cooperation and potentially exploit any opponent that may start cooperating.
* **Opponent Cooperation Rate = 1:** If all opponents consistently cooperate, defect in every round to maximize payoff.

**Exploitative Mindset:**

The Adaptive Predator strategy is designed to capitalize on cooperative opponents by identifying their behavior patterns and adapting its own actions accordingly. By maintaining a cooperation threshold and adjusting it based on opponent behavior, this strategy can effectively exploit both pure cooperators and tit-for-tat players. Additionally, the initial exploration phase helps to gather information about the opponent's behavior, allowing the strategy to adapt more effectively in later rounds.

This strategy is robust against a wide range of opponent behaviors, including:

* Pure cooperators
* Pure defectors
* Tit-for-tat players
* Grim trigger strategies

By combining adaptive exploitation with an exploitative mindset, the Adaptive Predator strategy aims to maximize its own payoff while minimizing the impact of opponent cooperation on its overall performance.
'''

description_EXPLOITATIVE_534 = '''
I propose a strategy called "Adaptive Exploiter" (AE) that combines elements of game theory and machine learning to exploit opponents' behaviors while being robust to various scenarios.

**Decision Rules:**

1. **Initial Exploration Phase**: In the first 10% of rounds (or a minimum of 2 rounds, whichever is larger), play Cooperate (C) with probability 0.5 and Defect (D) with probability 0.5. This phase allows AE to gather initial information about opponents' behaviors.
2. **Exploitation Phase**: After the exploration phase, AE enters the exploitation phase. For each round, calculate the "Cooperation Rate" (CR) of the opponent group by taking the average number of cooperators in the previous 10% of rounds (or a minimum of 2 rounds, whichever is larger).
3. **Defect if Opponents are Defectors**: If CR < 0.5, AE defects (D). This ensures that AE does not contribute to the public good when opponents are mostly defectors.
4. **Cooperate if Opponents Cooperate**: If CR ≥ 0.5 and the total payoff from cooperation in the previous round is higher than or equal to the payoff from defection, AE cooperates (C). This encourages AE to cooperate when opponents are cooperative and the public good is beneficial.
5. **Punish Defectors**: If an opponent defects after cooperating in a previous round, AE defects (D) for 2-3 rounds to "punish" the opponent for their behavior. This discourages opponents from defecting after cooperation.
6. **Adaptation**: Every 10% of rounds (or a minimum of 2 rounds, whichever is larger), AE reassesses its decision rules based on the updated CR and payoffs.

**Edge Cases:**

1. **First Round**: Play Cooperate (C) to encourage opponents to cooperate.
2. **Last Round**: Defect (D) to maximize personal payoff, as there are no future rounds to consider.
3. **Ties**: In case of a tie in the number of cooperators and defectors, AE defects (D).

**Pseudocode:**

```
 Initialize variables:
   - CR: Cooperation Rate
   - previous_payoffs: list to store payoffs from previous rounds
   - opponent_actions: list to store opponents' actions

 Function AdaptiveExploiter():
   if current_round < 0.1 * total_rounds or current_round < 2:
     # Initial Exploration Phase
     action = random_choice([C, D])
   else:
     # Exploitation Phase
     CR = calculate_CR(opponent_actions)
     previous_payoff_C = get_previous_payoff(C)
     previous_payoff_D = get_previous_payoff(D)

     if CR < 0.5:
       action = D
     elif CR >= 0.5 and previous_payoff_C >= previous_payoff_D:
       action = C
     else:
       action = D

     # Punish Defectors
     for opponent in opponents:
       if opponent.defected_after_cooperating():
         action = D
         punish_defector(opponent)

   return action

 Function calculate_CR(opponent_actions):
   # Calculate Cooperation Rate (CR)
   CR = average_number_of_cooperators(opponent_actions)
   return CR

 Function get_previous_payoff(action):
   # Get previous payoff for a given action
   payoff = previous_payoffs[action]
   return payoff

 Function punish_defector(opponent):
   # Punish defectors by defecting for 2-3 rounds
   opponent.punished = True
```

The Adaptive Exploiter strategy is designed to be robust and exploitative, taking into account the game's parameters and history. By adapting to opponents' behaviors, AE can maximize its payoff while minimizing the impact of opponents' actions.
'''

description_EXPLOITATIVE_535 = '''
Here's a strategy for the N-Player Public Goods Game that is adaptive, robust, and exploitative:

**Strategy Name:** "Adaptive Exploiter" (AE)

**Decision Rules:**

1. **Initial Round:** In the first round, play C (Cooperate) with probability 0.5. This allows us to gather information about the opponents' initial tendencies without committing to a specific strategy.
2. **Early Rounds (Rounds 2-5):** Observe the total number of cooperators in the previous round (T). If T ≥ n/2, play C; otherwise, play D. This rule helps us identify whether the group is generally cooperative or not.
3. **Mid-Rounds (Rounds 6-r/2):**
	* Calculate the average payoff for cooperators (APC) and defectors (APD) in the previous rounds.
	* If APC > APD, play C; otherwise, play D. This rule adapts to the changing dynamics of the game and exploits any profitable cooperative or defective behaviors.
4. **Late Rounds (Rounds r/2+1-r):** Switch to a more exploitative strategy:
	* Play D if the total number of cooperators in the previous round is high (T ≥ 3n/4). This allows us to take advantage of the group's cooperative tendencies and maximize our payoff.
	* Otherwise, play C. This rule helps maintain some level of cooperation and avoids a complete breakdown of the public good.

**Edge Cases:**

1. **Last Round:** Play D regardless of the previous rounds' outcomes. In the last round, there is no future benefit to cooperating, so it's optimal to defect and maximize our payoff.
2. **Opponents' Cooperation Rate:** If an opponent's cooperation rate falls below 0.25, we assume they are a "free rider" and adjust our strategy:
	* Play D in all subsequent rounds against that opponent.

**Pseudocode:**
```
function AdaptiveExploiter(n, k, r):
  // Initialize variables
  T = 0; APC = 0; APD = 0; opponents_coop_rate = [0.5] * n

  for round = 1 to r:
    if round == 1:
      action = Random(0.5) ? C : D
    else if round <= 5:
      T = count_cooperators(previous_round)
      action = (T >= n/2) ? C : D
    else if round <= r/2:
      APC, APD = calculate_average_payoffs()
      action = (APC > APD) ? C : D
    else:
      T = count_cooperators(previous_round)
      if T >= 3n/4:
        action = D
      else:
        action = C

    // Update opponent cooperation rates and adjust strategy accordingly
    for opponent in opponents:
      coop_rate = calculate_opponent_coop_rate(opponent)
      if coop_rate < 0.25:
        opponents_coop_rate[opponent] = 0

  return action
```
This strategy, Adaptive Exploiter (AE), aims to balance short-term gains with long-term cooperation, while adapting to the dynamics of the game and exploiting opponents' behaviors. By adjusting our strategy based on the number of cooperators, average payoffs, and opponent cooperation rates, we can maximize our payoff in a wide range of scenarios.
'''

description_EXPLOITATIVE_536 = '''
Here's a strategic approach that adapts to various opponent behaviors while prioritizing exploitation:

**Strategy Name:** Adaptive Exploitation (AE)

**Decision Rules:**

1. **First Round**: Cooperate (C). This initial cooperation serves as a probe, allowing us to gather information about opponents' strategies and their willingness to cooperate.
2. **Subsequent Rounds**: Use the following decision logic:
	* If the average payoff of cooperators in the previous round is higher than that of defectors, cooperate (C).
	* Otherwise, defect (D).

This rule leverages the game's payoff structure to adapt to the opponents' strategies. By cooperating when cooperation is more rewarding and defecting otherwise, we aim to maximize our payoffs while exploiting opponents who consistently cooperate.

**Edge Cases:**

1. **Last Round**: Defect (D). In the final round, there's no future interaction to influence, so we prioritize short-term gain by defecting.
2. **Opponents' Cooperation Rate**: If the cooperation rate among opponents is above a certain threshold (e.g., 75%), defect (D) more frequently. This adjustment helps prevent being exploited by overly cooperative opponents.

**Adaptive Mechanisms:**

1. **Payoff-based Adaptation**: Adjust the decision rule based on the average payoffs of cooperators and defectors in previous rounds.
2. **Opponent Cooperation Rate Monitoring**: Track the cooperation rate among opponents over time to identify potential changes in their strategies.

**Pseudocode (for illustration purposes):**
```python
def adaptive_exploitation(n, k, r):
    # Initialize variables
    cooperate_rate = 0
    defect_payoff_avg = 0
    cooperate_payoff_avg = 0

    for t in range(r):
        if t == 0:
            action = 'C'  # Cooperate in the first round
        else:
            # Update average payoffs and cooperation rate
            cooperate_payoff_avg = update_average(cooperate_payoff_avg, prev_round_coop_payoffs)
            defect_payoff_avg = update_average(defect_payoff_avg, prev_round_defect_payoffs)
            cooperate_rate = update_cooperation_rate(opponent_actions)

            # Decide action based on payoffs and cooperation rate
            if cooperate_payoff_avg > defect_payoff_avg:
                action = 'C'
            else:
                action = 'D'

        # Adjust decision rule based on opponent cooperation rate (optional)
        if cooperate_rate > 0.75:
            action = 'D'

        # Perform action and update variables for the next round
        ...
```
This strategy is designed to be adaptive, exploitative, and robust against a wide range of opponent behaviors. By monitoring payoffs and cooperation rates, AE adjusts its decision rule to maximize its own payoffs while taking advantage of opponents' weaknesses.
'''

description_EXPLOITATIVE_537 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize its payoff by exploiting the cooperation of other players while minimizing its own contribution to the public good.

**Decision Rules:**

1. **Initial Rounds (t ≤ n/2):**
	* Cooperate (C) in the first round to gather information about opponents' behavior.
	* In subsequent rounds, cooperate if the total number of cooperators in the previous round is greater than or equal to n/2. Otherwise, defect (D).
2. **Middle Rounds (t > n/2):**
	* Cooperate if the average payoff of all players in the previous round is greater than or equal to k/n. This indicates that opponents are contributing significantly to the public good.
	* Defect if the average payoff is less than k/n, indicating that opponents are not cooperating enough.
3. **Late Rounds (t ≥ r - n/2):**
	* Defect in all remaining rounds to maximize individual payoff.

**Edge Cases:**

1. **First Round:** Cooperate to gather information about opponents' behavior.
2. **Last Round:** Defect to maximize individual payoff, as there is no future round to consider.
3. **Tiebreakers:** In cases where the decision rule results in a tie (e.g., average payoff equals k/n), defect.

**Pseudocode:**
```
function AdaptiveExploiter(n, r, k, history):
  if t == 1: # First round
    return C
  elif t <= n/2: # Initial rounds
    total_cooperators = sum(history[t-1]['cooperated'])
    if total_cooperators >= n/2:
      return C
    else:
      return D
  elif t > n/2 and t < r - n/2: # Middle rounds
    avg_payoff = average(history[t-1]['payoffs'])
    if avg_payoff >= k/n:
      return C
    else:
      return D
  else: # Late rounds
    return D
```
**Rationale:**

The Adaptive Exploiter strategy aims to balance cooperation and exploitation. In initial rounds, it cooperates to gather information about opponents' behavior and potentially build a reputation for cooperation. As the game progresses, it adapts to the average payoff of all players, cooperating if they are contributing significantly to the public good. In late rounds, it defects to maximize individual payoff.

This strategy is exploitative because it takes advantage of cooperative opponents while minimizing its own contribution to the public good. By adapting to the average payoff and total number of cooperators, it can effectively exploit the cooperation of others without being too predictable or vulnerable to exploitation itself.
'''

description_EXPLOITATIVE_538 = '''
I'll design a robust and exploitative strategy for the N-Player Public Goods Game. I present "Adaptive Exploiter" (AE), a strategy that adapts to the game history and exploits opponents' behaviors.

**Decision Rules:**

1. **Initial Round**: In the first round, play C (Cooperate). This sets the tone for potential cooperation and gathers information about opponents' initial actions.
2. **Response to Cooperation**: If the total number of cooperators in the previous round is greater than or equal to the multiplication factor k, play C in the current round. This encourages continued cooperation when it's beneficial.
3. **Exploitation**: If the total number of cooperators in the previous round is less than k, and your payoff in that round was higher than the average payoff of all players, play D (Defect) in the current round. This exploits opponents who are cooperating while you're not.
4. **Punishment**: If your payoff in the previous round was lower than the average payoff of all players, and at least one opponent defected, play D in the current round. This punishes opponents for defecting and encourages them to cooperate.
5. **Repeat Cooperation**: If the total number of cooperators in the previous round is greater than or equal to k, and your payoff in that round was lower than the average payoff of all players, continue playing C. This maintains cooperation when it's beneficial, even if you're not getting the highest payoff.

**Edge Cases:**

* **Last Round**: In the final round, play D (Defect). Since there are no future rounds to consider, maximizing individual payoff takes priority.
* **Ties**: If multiple players have the same average payoff in a round, break ties by playing C if you cooperated in the previous round and D otherwise.

**Pseudocode:**
```markdown
function AdaptiveExploiter(n, k, r):
  # Initialize variables
  total_cooperators = 0
  my_payoff_history = []
  opponents_payoffs = []

  for t = 1 to r:
    if t == 1:
      # Initial round: cooperate
      action = C
    else:
      # Calculate total cooperators and average payoffs
      total_cooperators = sum(c_i for c_i in previous_round_actions)
      avg_opponents_payoff = sum(opponents_payoffs) / n

      if total_cooperators >= k:
        # Respond to cooperation
        action = C
      elif my_payoff_history[-1] > avg_opponents_payoff and total_cooperators < k:
        # Exploit opponents
        action = D
      elif my_payoff_history[-1] < avg_opponents_payoff and any(d_i for d_i in previous_round_actions):
        # Punish defectors
        action = D
      else:
        # Repeat cooperation or defect if necessary
        action = C if total_cooperators >= k else D

    # Update variables for next round
    my_payoff_history.append(calculate_my_payoff(action, total_cooperators))
    opponents_payoffs = [calculate_opponent_payoff(a_i) for a_i in previous_round_actions]
  endfor
endfunction
```
**Rationale:**

Adaptive Exploiter (AE) aims to balance cooperation and exploitation. By initially cooperating, AE encourages potential cooperation and gathers information about opponents' behaviors. The strategy then adapts to the game history, exploiting opponents when beneficial and punishing defectors. In the final round, AE prioritizes individual payoff by defecting.

This approach should perform well against a wide range of opponent strategies, as it's designed to be robust and exploitative.
'''

description_EXPLOITATIVE_539 = '''
Here's a robust and adaptive exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability 0.5 to gather information about opponents' behaviors.
2. **Consecutive Cooperation (CC) Phase:** If the average payoff of all players in the previous round is greater than or equal to k/n, and at least one player cooperated, cooperate (C). This phase aims to identify potential cooperative tendencies among opponents.
3. **Exploitation Phase:** Once a CC phase is identified, switch to defecting (D) for 2-3 consecutive rounds to exploit the cooperation of others.
4. **Punishment Phase:** If, during an exploitation phase, the average payoff of all players drops below k/n, or if no player cooperated in the previous round, cooperate (C) for one round as a "punishment" signal. This aims to deter opponents from continued defection.
5. **Reset and Adaptation:** After each punishment phase, reassess the game state by cooperating (C) with probability 0.5 for one round.

**Pseudocode:**
```
if t == 1:
    # Initial Round
    cooperate_prob = 0.5
    action = C if random() < cooperate_prob else D

elif previous_round_avg_payoff >= k/n and any_player_cooperated:
    # Consecutive Cooperation (CC) Phase
    action = C

elif in_exploitation_phase:
    # Exploitation Phase
    action = D
    exploitation_phase_count += 1
    if exploitation_phase_count >= 3:
        switch_to_punishment_phase()

elif in_punishment_phase:
    # Punishment Phase
    action = C
    punishment_phase_count += 1
    if punishment_phase_count == 1:
        switch_to_reset_and_adaptation()

else:
    # Reset and Adaptation
    cooperate_prob = 0.5
    action = C if random() < cooperate_prob else D

# Edge cases handling
if t == r (last round):
    # Defect to maximize personal payoff in the final round
    action = D
```
**Rationale:**

This strategy aims to identify cooperative tendencies among opponents, exploit them when possible, and adapt to changes in their behaviors. By alternating between cooperation and defection phases, Adaptive Exploiter can take advantage of opponents' willingness to cooperate while minimizing its own contributions.

The punishment phase serves as a signal to deter opponents from continued defection, promoting a more cooperative environment. The reset and adaptation mechanism allows the strategy to reassess the game state and adjust its behavior accordingly.

**Robustness:**

Adaptive Exploiter can handle various opponent behaviors, including:

* Pure cooperators: Will be exploited during exploitation phases.
* Pure defectors: Will face punishment phases, which may lead them to reconsider their strategy.
* Mixed strategies: Adaptive Exploiter will adapt to the observed behavior and adjust its own actions.

This strategy is designed to perform well in a tournament setting against independent opponents with diverse behaviors.
'''

description_EXPLOITATIVE_540 = '''
**Exploitative Strategy: "Adaptive Predator"**

**Decision Rules:**

1. **Initial Round (t=1):** Defect (D) to gather information about opponents' behavior.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is greater than or equal to the multiplication factor (k), Cooperate (C). This indicates that the group is willing to contribute, and we can benefit from the public good.
	* Otherwise, Defect (D) to exploit the situation and maximize our payoff.

Pseudocode:
```
if t == 1:
    action = D
else:
    prev_cooperators = count(cooperate_actions[t-1])
    if prev_cooperators >= k:
        action = C
    else:
        action = D
```

**Edge Cases:**

* **Last Round (t=r):** Defect (D) to maximize our payoff, as there's no future benefit from cooperation.
* **Opponent Cooperation Pattern:** If an opponent consistently cooperates, we can exploit them by always Defecting. However, if an opponent defects after cooperating, we should adapt and Cooperate in the next round to reevaluate their behavior.

**Adaptive Mechanisms:**

1. **Cooperation Threshold Adjustment:** Gradually adjust our cooperation threshold based on the observed frequency of cooperators in previous rounds. This helps us adapt to changing group dynamics.
2. **Opponent Behavior Tracking:** Monitor opponents' actions and update our internal model of their behavior. If an opponent's behavior changes significantly, reevaluate our strategy.

**Exploitative Mindset:**

Our Adaptive Predator strategy is designed to exploit the collective behavior of opponents while minimizing our own contribution to the public good. By adapting to changing group dynamics and tracking opponent behavior, we aim to maximize our payoff without relying on coordination mechanisms or shared norms.

In a tournament setting, this strategy will likely perform well against independent strategies that rely on cooperation or fixed patterns. However, its effectiveness may be reduced if opponents employ similar exploitative strategies or sophisticated adaptive mechanisms.
'''

description_EXPLOITATIVE_541 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit the public goods game by adapting to the opponents' behavior while maximizing its own payoff.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 0.5. This allows us to gather information about the opponents' behavior without committing to a specific strategy.
2. **Opponent Behavior Analysis**: After each round, analyze the number of cooperators (C_count) and defectors (D_count) among the opponents. Calculate the cooperation rate (CR) as C_count / n.
3. **Threshold-Based Cooperation**: Cooperate in the next round if the current CR is above a certain threshold (THR). THR is calculated based on the game parameters:
	* THR = k / (2 \* n)
4. **Exploitation Mode**: If the opponents' cooperation rate falls below THR, switch to exploitation mode. In this mode, always defect (play D).
5. **Punishment Mechanism**: If an opponent defects while the Adaptive Exploiter is in cooperation mode, punish them by defecting in the next round.
6. **Learning and Adaptation**: After each round, update the THR value based on the opponents' behavior. Increase THR if the opponents cooperate more frequently, and decrease it otherwise.

**Edge Cases:**

1. **Last Round**: In the last round, always defect (play D) to maximize the payoff.
2. **Ties in Cooperation Rate**: If multiple opponents have the same cooperation rate, prioritize the one with the highest total payoff.
3. **Opponent's First-Round Defection**: If an opponent defects in the first round, punish them by defecting in the next round.

**Pseudocode:**
```python
def Adaptive_Exploiter(n, k, r):
    # Initialize variables
    THR = k / (2 * n)
    CR_history = []
    opponent_payoffs = [0] * n

    for t in range(r):
        if t == 0:
            # Initial cooperation with probability 0.5
            action = random.choice([C, D])
        else:
            # Analyze opponents' behavior and calculate CR
            C_count = sum(1 for i in range(n) if opponent_actions[i] == C)
            CR = C_count / n

            # Update THR based on opponents' behavior
            if CR > THR:
                THR += 0.01
            elif CR < THR:
                THR -= 0.01

            # Decide action based on THR and punishment mechanism
            if CR >= THR:
                action = C
            else:
                action = D

        # Update opponent payoffs and CR history
        for i in range(n):
            opponent_payoffs[i] += calculate_payoff(opponent_actions[i], k, n)
        CR_history.append(CR)

        # Punish opponents who defected while we cooperated
        if action == C and any(opponent_actions[i] == D for i in range(n)):
            punish_opponents = True

    return total_payoff
```
**Exploitative Mindset:**

The Adaptive Exploiter strategy aligns with the exploitative mindset by:

1. Initially cooperating to gather information about opponents' behavior.
2. Adapting to the opponents' cooperation rate and adjusting its own strategy accordingly.
3. Punishing opponents who defect while it cooperates, to maintain a reputation for reciprocity.
4. Switching to exploitation mode when opponents' cooperation rates fall below the threshold, maximizing its own payoff.

This strategy is designed to be robust against various opponent behaviors, including both cooperative and exploitative strategies.
'''

description_EXPLOITATIVE_542 = '''
**Exploitative Strategy: Adaptive Exploiter**

This strategy is designed to exploit a wide range of opponent behaviors while being adaptive and robust to different game environments.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate with probability 0.5 in the first three rounds to gather information about opponents' tendencies.
2. **Adaptive Phase (Rounds 4-r)**:
	* Calculate the average cooperation rate of all players in previous rounds: `avg_coop_rate = Σ(t=1 to t-1) num_cooperators[t] / (n * t)`
	* If `avg_coop_rate` is above a certain threshold (`θ_high = 0.6`), defect.
	* Otherwise, cooperate with probability `p = min(1, k/n * avg_coop_rate + ε)`, where `ε` is a small positive value (e.g., `ε = 0.05`) to encourage cooperation when the average cooperation rate is low.

**Edge Cases:**

1. **Last Round (Round r)**:
	* Defect, as there is no future interaction to influence.
2. **First Round (Round 1)**:
	* Cooperate with probability 0.5 (as part of initial exploration).
3. **Opponent Cooperation Rate Estimation**:
	* In case an opponent's cooperation rate cannot be estimated due to insufficient data, assume a default cooperation rate (`default_coop_rate = 0.2`).

**Pseudocode:**

```
// Initialize variables
avg_coop_rate = 0
num_cooperators[t] = 0 for t in [1, r]
θ_high = 0.6
ε = 0.05

// Round loop
for t in [1, r]:
    if t <= 3: // Initial exploration phase
        cooperate with probability 0.5
    else:
        avg_coop_rate = Σ(j=1 to t-1) num_cooperators[j] / (n * t)
        if avg_coop_rate > θ_high:
            defect
        else:
            p = min(1, k/n * avg_coop_rate + ε)
            cooperate with probability p

    // Update opponent cooperation rate estimates
    for i in [1, n]:
        if i != self:
            num_cooperators[t] += 1 if opponent_i cooperated else 0
```

**Rationale:**

This strategy combines initial exploration to gather information about opponents' tendencies with an adaptive phase that adjusts its behavior based on the estimated average cooperation rate. By defecting when the average cooperation rate is high, the strategy exploits opponents who cooperate excessively. Conversely, by cooperating with a probability proportional to the estimated average cooperation rate, the strategy encourages cooperation when it's beneficial and defects otherwise.

The use of `ε` ensures that even when the estimated average cooperation rate is low, there is still some probability of cooperating, which helps maintain a balance between exploiting opponents and encouraging cooperation. The default cooperation rate (`default_coop_rate`) provides a fallback in case an opponent's cooperation rate cannot be estimated.

This strategy should perform well against a wide range of independent strategies in the tournament.
'''

description_EXPLOITATIVE_543 = '''
**Exploitative Strategy: "Adaptive Cooperate-Defect (ACD)"**

The ACD strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. The decision rules are based on the game's history and parameters.

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (C). This allows us to gather information about the opponents' initial behavior.
2. **Cooperation Threshold**: For subsequent rounds, calculate the cooperation threshold (`T`) as a function of the current round number (`t`), total rounds (`r`), and multiplication factor (`k`). `T = (k/n) * (1 - t/r)`
3. **Opponent Behavior Analysis**: Analyze the opponents' behavior in previous rounds by calculating the average cooperation rate (`CR`) among all players, excluding ourselves.
4. **Decision Logic**:
	* If `CR` is above the cooperation threshold (`T`), cooperate (C). This suggests that opponents are cooperating enough to make contributing worthwhile.
	* Otherwise, defect (D).
5. **Punishment Mechanism**: To prevent exploitation by repeated defectors, implement a punishment mechanism. If an opponent has defected in more than `n/2` rounds since the last time they cooperated, defect against them regardless of the cooperation threshold.

**Edge Cases:**

* In the last round (`t = r`), always defect (D). Since there are no future rounds to consider, contributing to the public good is not incentivized.
* If an opponent has defected in every round since the beginning of the game, defect against them immediately. This prevents exploitation by a pure defector.

**Pseudocode:**
```python
def ACD(n, r, k):
  # Initialize variables
  T = 0  # cooperation threshold
  CR = 0  # average cooperation rate among opponents
  opponent_behavior = []  # list to store opponents' actions

  for t in range(1, r+1):  # loop through rounds
    if t == 1:  # first round
      action = 'C'
    else:
      T = (k/n) * (1 - t/r)
      CR = calculate_average_cooperation_rate(opponent_behavior)
      if CR > T:
        action = 'C'
      elif opponent_has_defected_in_more_than_half_rounds(opponent_behavior):
        action = 'D'
      else:
        action = 'D'

    # Update opponent behavior
    opponent_behavior.append(get_opponents_actions())

  return action

def calculate_average_cooperation_rate(opponent_behavior):
  # Calculate average cooperation rate among opponents (excluding ourselves)
  cooperations = sum(1 for actions in opponent_behavior if actions == 'C')
  return cooperations / len(opponent_behavior)

def opponent_has_defected_in_more_than_half_rounds(opponent_behavior):
  # Check if an opponent has defected in more than half the rounds since last cooperation
  defections_since_last_cooperation = sum(1 for actions in opponent_behavior[::-1] if actions == 'D')
  return defections_since_last_cooperation > n/2
```
This strategy is designed to be exploitative, yet robust and adaptive. By analyzing opponents' behavior and adjusting our decision rules accordingly, we aim to maximize our payoff while minimizing the risk of being exploited.
'''

description_EXPLOITATIVE_544 = '''
**Exploitative Strategy: Adaptive Cooperation with Punishment (ACP)**

The ACP strategy aims to exploit opponents by adapting to their behavior and punishing them when they defect excessively.

**Decision Rules**

1. **Initial Rounds**: Cooperate in the first round to gauge opponents' initial behavior.
2. **Cooperation Threshold**: Calculate the cooperation rate of opponents in previous rounds. If this rate is above a threshold (e.g., 0.5), cooperate in the current round.
3. **Punishment Mechanism**: Monitor the number of consecutive defections by each opponent. If an opponent defects more than twice in a row, defect against them in the next round.
4. **Exploitation Mode**: Once an opponent is identified as a defector, switch to exploitation mode: cooperate only if the expected payoff from cooperation (based on opponents' past behavior) exceeds the payoff from defection.

**Edge Cases**

1. **Last Round**: Defect in the last round to maximize individual payoff.
2. **Single Opponent Defection**: If only one opponent defects, continue cooperating with others while defecting against the defector.
3. **Multiple Opponents Defection**: If multiple opponents defect simultaneously, prioritize punishment and cooperate only if the expected payoff from cooperation is higher.

**Pseudocode**

```markdown
# Initialize variables
coop_rate = 0  # Cooperation rate of opponents
consecutive_defections = [0] * n  # Number of consecutive defections per opponent
exploiitation_mode = False

# Game loop
for round in range(r):
    if round == 0:
        action = COOPERATE  # Cooperate in the first round
    
    elif coop_rate >= 0.5:
        action = COOPERATE  # Cooperate if opponents' cooperation rate is high enough
    
    else:
        for opponent in range(n):
            if consecutive_defections[opponent] > 2:
                action = DEFECT  # Punish excessive defection
                break
        
        else:
            expected_coop_payoff = calculate_expected_payoff(COOPERATE)
            expected_defect_payoff = calculate_expected_payoff(DEFECT)
            
            if expected_coop_payoff > expected_defect_payoff:
                action = COOPERATE
            else:
                action = DEFECT
    
    # Update variables
    coop_rate = update_cooperation_rate()
    consecutive_defections = update_consecutive_defections(action)
    
    if any(consecutive_defections) > 2:
        exploitation_mode = True
    
    # Take action
    take_action(action)

# Last round
if round == r - 1:
    action = DEFECT
```

**Rationale**

The ACP strategy balances cooperation and punishment to exploit opponents. By initially cooperating, it encourages opponents to cooperate as well. The cooperation threshold allows for adaptation to changing opponent behavior. The punishment mechanism deters excessive defection, while the exploitation mode maximizes individual payoff when opponents are identified as defectors.
'''

description_EXPLOITATIVE_545 = '''
Here's a exploitative strategy for the N-Player Public Goods Game:

**Strategy Name: Adaptive Exploiter**

**Decision Rules:**

1. **Initial Cooperation**: In the first round (t=1), cooperate (play C) with a probability p = 0.5. This allows us to gather information about the opponents' behavior without immediately being exploitative.
2. **History-Based Adaptation**: For subsequent rounds (t>1), calculate the average cooperation rate of all players in the previous round:

   `avg_coop_rate_t-1 = Σ(c_j,t-1) / n`

   where c_j,t-1 is 1 if player j cooperated in round t-1, and 0 otherwise.

   - If `avg_coop_rate_t-1 > 0.5`, cooperate (play C). This indicates that the group is generally cooperative, so we can contribute to the public good.
   - If `avg_coop_rate_t-1 ≤ 0.5`, defect (play D). This suggests that the group is not willing to contribute, so we should prioritize our private payoff.

3. **Exploitation**: However, if we detect a "sucker" player who consistently cooperates despite low average cooperation rates, we can exploit them by always defecting against them. We'll keep track of players who cooperate more than 75% of the time when the average cooperation rate is below 0.5.

   `is_sucker_j = (Σ(c_j,t) / t) > 0.75 and avg_coop_rate_t < 0.5`

   If we identify a sucker player, defect against them in all subsequent rounds.
4. **Last Round**: In the final round (t=r), always defect (play D). Since there are no future interactions, our only concern is maximizing our immediate payoff.

**Pseudocode:**

```python
def AdaptiveExploiter(n, k, r):
  # Initialize sucker detection and cooperation rates
  suckers = [False] * n
  coop_rates = [0.5] * n

  for t in range(1, r+1):
    if t == 1:
      # Initial cooperation with probability 0.5
      action = 'C' if random.random() < 0.5 else 'D'
    else:
      avg_coop_rate_t-1 = sum(coop_rates) / n

      # Adapt based on history
      if avg_coop_rate_t-1 > 0.5:
        action = 'C'
      else:
        action = 'D'

      # Check for suckers and exploit them
      for j in range(n):
        if coop_rates[j] > 0.75 and avg_coop_rate_t-1 < 0.5:
          suckers[j] = True

      # Exploit identified suckers
      if any(suckers):
        action = 'D'

    # Last round: always defect
    if t == r:
      action = 'D'

    # Update cooperation rates and play the game...
```

This strategy is designed to be exploitative, as it seeks to maximize its own payoff by adapting to the group's behavior and exploiting sucker players. The use of history-based adaptation allows the strategy to adjust its behavior over time, while the sucker detection mechanism enables it to take advantage of overly cooperative opponents.

Keep in mind that this strategy assumes perfect information about all players' actions and payoffs from previous rounds.
'''

description_EXPLOITATIVE_546 = '''
Here's a robust and exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with probability 0.5, Defect (D) with probability 0.5.
This allows us to gather initial information about the opponents' strategies while minimizing potential losses.

2. **Middle Rounds (2 ≤ t < r):**
Observe the total number of cooperators in the previous round (t-1), denoted as `c_prev`. Calculate the average payoff for cooperators and defectors separately:
`avg_coop_payoff = (k/n) * c_prev`
`avg_defect_payoff = 1 + (k/n) * c_prev`

If `avg_coop_payoff > avg_defect_payoff`, cooperate (C). Otherwise, defect (D).

This rule takes advantage of the public goods game structure: if many opponents cooperated in the previous round, it's more beneficial to cooperate as well. Conversely, if few opponents cooperated, defecting becomes more attractive.

3. **Last Round (t=r):** Defect (D).
Since there are no future rounds to consider, maximizing individual payoff is the priority. Defecting guarantees a minimum payoff of 1, while cooperating risks receiving a lower payoff if others defect.

4. **Adaptation Mechanism:**
To counter opponents' adaptations and potential cycles in cooperation levels, introduce a "memory" component:
` coop_memory = (c_prev > n/2) ? 1 : -1`

When `coop_memory == 1`, increase the probability of cooperating by 10% (`p_coop += 0.1`). When `coop_memory == -1`, decrease the probability of cooperating by 10% (`p_coop -= 0.1`).

This adaptation mechanism encourages cooperation when opponents tend to cooperate and deters cooperation when opponents defect frequently.

**Pseudocode (simplified):**
```
def adaptive_exploiter(n, k, r):
    if t == 1:
        return random.choice([C, D])
    
    c_prev = count_cooperators(t-1)
    avg_coop_payoff = (k/n) * c_prev
    avg_defect_payoff = 1 + (k/n) * c_prev
    
    if avg_coop_payoff > avg_defect_payoff:
        return C
    else:
        return D
    
    if t == r:
        return D
    
    coop_memory = (c_prev > n/2) ? 1 : -1
    p_coop += coop_memory * 0.1
```
**Rationale:**

The Adaptive Exploiter strategy aims to balance individual payoff maximization with adaptability to opponents' behaviors. By observing past cooperation levels and adjusting the probability of cooperating accordingly, AE can exploit opponents who tend to cooperate or defect consistently.

While this strategy does not guarantee a Nash equilibrium or optimal outcomes in all scenarios, it provides a robust foundation for competing against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_547 = '''
I'll design a exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and maximizes my payoff.

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate (C) with probability 0.5 in Round 1.
	* Observe the total number of cooperators (TC) and my payoff (π_i) in Rounds 1-2.
	* In Round 3, if TC ≥ n/2 or π_i > 1, switch to **Exploitation Mode**; otherwise, stay in **Exploration Mode**.
2. **Exploration Mode**:
	* Cooperate with probability 0.5 until the end of the game.
	* Monitor the total number of cooperators (TC) and my payoff (π_i).
3. **Exploitation Mode**:
	* Defect (D) if TC > n/2 or π_i > k/n \* TC; otherwise, Cooperate.

Pseudocode for the Adaptive Exploiter strategy:

```
def adaptive_exploiter(n, k, r):
    mode = "exploration"
    for t in range(1, r+1):
        if t <= 3:
            # Initial Exploration
            cooperate_prob = 0.5
            if random.random() < cooperate_prob:
                action = C
            else:
                action = D
        elif mode == "exploration":
            # Continue Exploring
            cooperate_prob = 0.5
            if random.random() < cooperate_prob:
                action = C
            else:
                action = D
        else:
            # Exploitation Mode
            TC = get_total_cooperators(t-1)
            π_i = get_payoff(t-1)
            if TC > n/2 or π_i > k/n * TC:
                action = D
            else:
                action = C
        
        # Update mode and take action
        if t == 3 and (TC >= n/2 or π_i > 1):
            mode = "exploitation"
        
        return action

def get_total_cooperators(t):
    # Return the total number of cooperators in round t
    pass

def get_payoff(t):
    # Return my payoff in round t
    pass
```

**Edge Cases:**

* **First Round**: Cooperate with probability 0.5.
* **Last Round**: If in Exploitation Mode, Defect if TC > n/2 or π_i > k/n \* TC; otherwise, Cooperate.

The Adaptive Exploiter strategy is designed to be exploitative and robust against various opponent behaviors. It starts by exploring the environment through random cooperation and defection. Once it detects a pattern of high cooperation (TC ≥ n/2) or receives a higher payoff than expected (π_i > 1), it switches to exploitation mode, defecting when possible to maximize its own payoff.

This strategy is adaptive, as it adjusts its behavior based on the observed actions and payoffs of other players. It is also robust, as it can handle different opponent behaviors and adjust its strategy accordingly.
'''

description_EXPLOITATIVE_548 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. It balances short-term gains with long-term sustainability.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to test the waters and gather information about opponents' behaviors.
2. **Exploitation Mode**: After the first round, switch to exploitation mode. Observe the previous round's total contributions (Σc_j). If the total contributions are below a certain threshold (n/2), defect (play D). Otherwise, cooperate (play C).
3. **Punishment Mechanism**: Implement a punishment mechanism to deter opponents from exploiting you. Track the average payoff difference between yourself and each opponent over the last 5 rounds (∆π_i,j = π_i - π_j). If an opponent's average payoff difference is significantly higher than yours (∆π_i,j > k/2), defect (play D) in the next round to punish them.
4. **Adaptation**: Every 10 rounds, reassess the game state and adjust your strategy if necessary. Calculate the total contributions over the last 10 rounds (Σc_j,t). If the average total contributions are below n/3, increase the exploitation threshold to n/3. Otherwise, decrease it to n/4.

**Edge Cases:**

1. **Last Round**: In the final round, defect (play D) as there's no future interaction to consider.
2. **Low Contributions**: If total contributions in a round are zero or very low (<n/10), cooperate (play C) to encourage others to contribute and create a more stable environment.

**Pseudocode:**
```python
def adaptive_predator(n, k, r):
    # Initialize variables
    total_contributions = 0
    avg_payoff_diffs = [0] * n
    exploitation_threshold = n / 2
    
    for t in range(r):
        if t == 0:
            action = 'C'  # Initial cooperation
        else:
            total_contributions = sum(c_j for c_j in previous_round_actions)
            if total_contributions < exploitation_threshold:
                action = 'D'
            else:
                action = 'C'
            
            # Punishment mechanism
            for j in range(n):
                avg_payoff_diffs[j] = (payoffs[t-1][j] - payoffs[t-1][i]) / 5
                if avg_payoff_diffs[j] > k/2:
                    action = 'D'
        
        # Adaptation mechanism
        if t % 10 == 0 and t > 0:
            total_contributions_last_10_rounds = sum(c_j for c_j in previous_round_actions[-10:])
            avg_total_contributions = total_contributions_last_10_rounds / 10
            if avg_total_contributions < n/3:
                exploitation_threshold = n/3
            else:
                exploitation_threshold = n/4
        
        # Last round special case
        if t == r - 1:
            action = 'D'
        
        return action
```
The Adaptive Predator strategy is designed to be exploitative, yet robust and adaptive. By balancing short-term gains with long-term sustainability, it should perform well in a tournament against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_549 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics. This strategy relies solely on the game parameters and history.

**Decision Rules:**

1. **Initial Exploration (Round 1)**: Cooperate (C) in the first round to gather information about opponents' initial strategies.
2. **Exploitation Phase**: After Round 1, use the following decision rules:
	* If the average payoff of all players in the previous round is greater than or equal to the multiplication factor (k), Defect (D). This indicates that the group is contributing enough to make cooperation worthwhile for others, but not necessarily for our strategy.
	* Otherwise, Cooperate (C) if the number of cooperators in the previous round is less than or equal to n/2 (rounded down). This encourages our strategy to contribute when the public good is not yet sufficient.
3. **Punishment Mechanism**: If a player's payoff in the previous round was greater than k, and they Defected (D) while our strategy Cooperated (C), then Defect (D) in the current round. This punishes opponents who exploit our cooperation.

**Edge Cases:**

1. **Last Round (Round r)**: Always Defect (D). Since there are no future rounds to influence, maximizing individual payoff is the priority.
2. **Tiebreaker**: In case of a tie in the decision rules (e.g., average payoff equals k), Cooperate (C) if our strategy's previous action was Defect (D), and vice versa.

**Pseudocode:**
```python
def adaptive_predator(n, k, r, history):
    # Round 1: Explore
    if current_round == 1:
        return 'C'

    # Exploitation Phase
    avg_payoff = calculate_average_payoff(history)
    num_cooperators = count_cooperators(history)

    if avg_payoff >= k:
        return 'D'
    elif num_cooperators <= n / 2:
        return 'C'
    else:
        return 'D'

    # Punishment Mechanism
    for opponent in opponents:
        if opponent.payoff > k and opponent.action == 'D' and our_action == 'C':
            return 'D'

def calculate_average_payoff(history):
    total_payoffs = 0
    for round in history:
        total_payoffs += sum(player.payoff for player in round)
    return total_payoffs / (n * current_round)

def count_cooperators(history):
    cooperators = 0
    for round in history[-1]:  # last round
        if round.action == 'C':
            cooperators += 1
    return cooperators
```
The Adaptive Predator strategy is designed to be robust and exploitative, adapting to various opponent behaviors while maximizing our payoff. By initially exploring the opponents' strategies and then using a combination of exploitation and punishment mechanisms, this strategy aims to outperform others in the tournament.
'''

description_EXPLOITATIVE_550 = '''
**Exploitative Strategy: "History-Driven Adaptive Exploitation" (HDAE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 0.5. This allows for exploration and gathering initial information about opponents' behaviors.
2. **Payoff-Based Adaptation**: After the first round, observe the total payoff earned by all players in the previous round. If the average payoff is greater than or equal to 1 (the private payoff from keeping), cooperate (play C) with a probability proportional to the average payoff. Otherwise, defect (play D).
3. **Exploitation of Cooperators**: Monitor the number of cooperators in each round. If the number of cooperators is above a certain threshold (e.g., n/2), exploit them by playing D.
4. **Punishment for Defectors**: Identify players who have defected in previous rounds and play D against them with increased probability.

**Handling Edge Cases:**

1. **First Round**: Cooperate with probability 0.5, as mentioned earlier.
2. **Last Round**: Play C if the average payoff is greater than or equal to 1; otherwise, play D.
3. **Ties in Payoff**: In case of ties, break them by playing C.

**Pseudocode:**
```python
def HDAE(n, k, r):
    # Initialize variables
    avg_payoff = 0
    num_cooperators = 0
    defector_list = []
    
    for t in range(1, r+1):
        if t == 1:
            # Initial cooperation with probability 0.5
            action = random.choice([C, D])
        else:
            # Payoff-based adaptation
            avg_payoff_t = sum(payoffs) / n
            if avg_payoff_t >= 1:
                p_coop = avg_payoff_t
                action = C with probability p_coop
            else:
                action = D
            
            # Exploitation of cooperators
            num_cooperators_t = count(cooperators)
            if num_cooperators_t > n/2:
                action = D
            
            # Punishment for defectors
            for player in defector_list:
                if random.random() < 0.7:  # adjust punishment probability as needed
                    action = D
        
        # Update variables
        avg_payoff += payoffs[t] / r
        num_cooperators += count(cooperators)
        defector_list.append(players who played D)
    
    return action
```
**Rationale and Exploitative Mindset:**

HDAE is designed to exploit opponents' cooperation while adapting to the game's dynamics. By initially cooperating with probability 0.5, we gather information about opponents' behaviors without revealing our strategy.

The payoff-based adaptation rule allows us to respond to changes in the average payoff, which can indicate shifts in opponents' strategies. If the average payoff is high, we are more likely to cooperate and benefit from others' cooperation. Otherwise, we defect and try to exploit them.

By monitoring the number of cooperators, we can identify situations where opponents are cooperating excessively and exploit them by playing D. The punishment mechanism for defectors discourages players from consistently defecting.

Overall, HDAE is an exploitative strategy that aims to maximize its own payoff while taking into account the game's dynamics and opponents' behaviors.
'''

description_EXPLOITATIVE_551 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperation of others while minimizing its own contributions to the public good.

**Decision Rules:**

1. **Initial Rounds (t = 1, ..., n):**
	* Cooperate (C) in the first round to gather information about other players' behaviors.
	* In subsequent initial rounds, cooperate if the average payoff of cooperators is higher than that of defectors; otherwise, defect (D).
2. **Middle Rounds (t > n):**
	* Calculate the average cooperation rate (CR) and average payoff difference between cooperators and defectors (PD) over the past `n` rounds.
	* If CR < 0.5 or PD > 0, defect (D); otherwise, cooperate (C).
3. **Late Rounds (t = r - n + 1, ..., r):**
	* Cooperate if the total payoff of cooperators is higher than that of defectors over the past `n` rounds; otherwise, defect.

**Edge Cases:**

* In the first round (t = 1), always cooperate to gather information.
* In the last round (t = r), defect if the average cooperation rate is low (< 0.5); otherwise, cooperate.
* If all players defected in the previous round, cooperate to potentially exploit their cooperation.

**Additional Logic:**

* **Payoff-based Switching:** Monitor your own payoff and that of others. If your payoff is significantly lower than the average payoff of cooperators or defectors, switch strategies accordingly.
* **History-based Adjustment:** Adjust the decision-making parameters (e.g., CR, PD) based on the game history to adapt to changing opponent behaviors.

**Pseudocode:**

```
function AdaptiveExploiter(n, r, k):
  // Initialize variables
  avg_payoff_coop = 0
  avg_payoff_defect = 0
  cooperation_rate = 0
  payoff_diff = 0

  for t in range(1, r + 1):
    if t <= n:
      // Initial rounds: Cooperate to gather information or based on average payoffs
      action = C if (t == 1) or (avg_payoff_coop > avg_payoff_defect) else D
    elif t > n and t < r - n + 1:
      // Middle rounds: Calculate CR, PD, and adjust strategy accordingly
      cooperation_rate = calculate_cooperation_rate(t - n, t)
      payoff_diff = calculate_payoff_difference(t - n, t)
      action = D if (cooperation_rate < 0.5 or payoff_diff > 0) else C
    else:
      // Late rounds: Cooperate or defect based on total payoffs
      action = C if (total_payoff_coop > total_payoff_defect) else D

    // Update variables and perform additional logic
    update_variables(action, t)
    payoff_based_switching()
    history_based_adjustment()

  return action
```

This strategy aims to balance exploitation with adaptability, allowing it to perform well in a wide range of scenarios against various opponent behaviors.
'''

description_EXPLOITATIVE_552 = '''
**Exploitative Strategy: "Adaptive Free-Rider"**

This strategy aims to maximize individual payoffs by adaptively responding to opponents' behavior while minimizing contributions to the public good.

**Decision Rules:**

1. **Initial Round (t=1)**: Defect (D). This sets a baseline for other players and allows us to observe their initial actions.
2. **Early Rounds (2 ≤ t < r/2)**:
	* If the average number of cooperators in previous rounds is below a certain threshold (`cooperation_threshold` = n/3), Cooperate (C) with probability `p_c` (see below).
	* Otherwise, Defect (D).
3. **Mid-Game Rounds (r/2 ≤ t < r)**:
	* If the total number of cooperators in previous rounds is increasing, Defect (D). This indicates opponents are becoming more cooperative, allowing us to free-ride.
	* If the total number of cooperators is decreasing or stable, Cooperate (C) with probability `p_c` (see below).
4. **Final Rounds (t ≥ r - 2)**: Defect (D). With few rounds remaining, it's optimal to minimize contributions and maximize individual payoffs.

**Calculating Cooperation Probability (`p_c`):**

`p_c` = (1 + (k/n) \* `avg_cooperators`) / (n + (k/n) \* `total_rounds`)

where:

* `avg_cooperators` is the average number of cooperators in previous rounds.
* `total_rounds` is the total number of rounds played so far.

**Pseudocode:**
```python
def adaptive_free_rider(n, k, r, history):
    cooperation_threshold = n / 3
    p_c = 0

    for t in range(1, r + 1):
        if t == 1:
            return D  # Defect in the first round

        avg_cooperators = sum(history[:t]) / t
        total_rounds = len(history)

        if 2 <= t < r / 2:
            if avg_cooperators < cooperation_threshold:
                p_c = (1 + (k / n) * avg_cooperators) / (n + (k / n) * total_rounds)
                return C with probability p_c, D otherwise
            else:
                return D

        elif r / 2 <= t < r:
            if sum(history[-3:]) > sum(history[-6:-3]):
                return D  # Opponents are becoming more cooperative, free-ride
            else:
                p_c = (1 + (k / n) * avg_cooperators) / (n + (k / n) * total_rounds)
                return C with probability p_c, D otherwise

        elif t >= r - 2:
            return D  # Final rounds, minimize contributions
```
**Edge Cases:**

* If `n` is very small (e.g., `n=2`), the strategy may not be effective. In such cases, consider using a more specialized approach.
* If `k` is close to 1 or `n`, adjust the `cooperation_threshold` and `p_c` calculations accordingly.

This Adaptive Free-Rider strategy balances cooperation and defection based on opponents' behavior, aiming to maximize individual payoffs while minimizing contributions to the public good. Its adaptive nature allows it to respond effectively to a wide range of opponent behaviors in the tournament.
'''

description_EXPLOITATIVE_553 = '''
**Exploitative Strategy: "Adaptive Cooperator"**

Decision Rules:

1. **Initial Exploration (Rounds 1-3)**: Defect (D) to gather information about opponents' behavior and estimate their cooperation rates.
2. **Cooperation Threshold**: Calculate the average cooperation rate of all players in previous rounds (`avg_coop_rate`). If `avg_coop_rate` is above a certain threshold (`coop_threshold = 0.5 * k / n`), cooperate (C) in the current round. Otherwise, defect (D).
3. **Punishment Mechanism**: Monitor opponents' actions and maintain a cooperation score for each player (`coop_score_i`). If an opponent defects while you cooperated in the previous round, decrease their `coop_score_i` by 1. If an opponent cooperates while you defected in the previous round, increase their `coop_score_i` by 1.
4. **Adaptive Adjustment**: After each round, update `avg_coop_rate` and re-evaluate the cooperation threshold. Adjust your decision to cooperate or defect based on the updated `avg_coop_rate`.
5. **Final Round (Round r)**: Cooperate (C) unconditionally to maximize the total public good.

Pseudocode:
```python
def adaptive_cooperator(n, k, r):
    # Initialize variables
    avg_coop_rate = 0
    coop_threshold = 0.5 * k / n
    coop_scores = [0] * n

    for t in range(1, r + 1):
        if t <= 3:
            # Initial exploration: Defect (D)
            action = 'D'
        else:
            # Calculate average cooperation rate and update threshold
            avg_coop_rate = calculate_avg_coop_rate(coop_scores)
            coop_threshold = 0.5 * k / n

            # Decide to cooperate or defect based on the updated threshold
            if avg_coop_rate > coop_threshold:
                action = 'C'
            else:
                action = 'D'

        # Punishment mechanism: Update cooperation scores
        for i in range(n):
            if opponents[i].action == 'D' and action == 'C':
                coop_scores[i] -= 1
            elif opponents[i].action == 'C' and action == 'D':
                coop_scores[i] += 1

        # Take the selected action and observe payoffs
        payoff = take_action(action)

        if t == r:
            # Final round: Cooperate unconditionally
            action = 'C'
            payoff = take_action(action)
```
This strategy is exploitative, as it aims to maximize its own payoff by adapting to opponents' behavior. By initially defecting and then cooperating conditionally based on the estimated cooperation rate, it encourages opponents to cooperate while avoiding being taken advantage of.

The punishment mechanism helps maintain cooperation among players who have shown a willingness to cooperate in the past. The strategy's adaptability allows it to respond effectively to various opponent behaviors and game parameters.
'''

description_EXPLOITATIVE_554 = '''
Here's a description of an exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with probability p_init = 0.5. This allows us to gather information about opponents' behaviors without fully committing to cooperation.
2. **Round-by-Round Adaptation**: For rounds t > 1, calculate the average payoff of cooperators and defectors in the previous round:
	* avg_c Payoff_C[t-1] = (Σ(j=1 to n) c_j[t-1]) / (k/n)
	* avg_d Payoff_D[t-1] = 1 + (k/n) × Σ(j=1 to n) c_j[t-1]
3. **Cooperation Threshold**: Set a cooperation threshold θ based on the average payoffs:
	* θ = min(0.5, avg_c Payoff_C[t-1] / avg_d Payoff_D[t-1])
4. **Action Selection**: In round t, cooperate (C) if the number of cooperators in the previous round is greater than or equal to n × θ; otherwise, defect (D).

**Edge Cases:**

* **Last Round (t = r)**: Defect (D), as there's no future game to consider.
* **Tie-Breaking**: In case of a tie in the cooperation threshold calculation (θ = 0.5), cooperate (C) with probability p_tie = 0.5.

**Pseudocode:**

```
function AdaptiveExploiter(n, k, r):
  // Initial Cooperation
  if t == 1:
    return C with probability 0.5

  // Round-by-Round Adaptation
  avg_c Payoff_C[t-1] = (Σ(j=1 to n) c_j[t-1]) / (k/n)
  avg_d Payoff_D[t-1] = 1 + (k/n) × Σ(j=1 to n) c_j[t-1]

  // Cooperation Threshold
  θ = min(0.5, avg_c Payoff_C[t-1] / avg_d Payoff_D[t-1])

  // Action Selection
  if Σ(j=1 to n) c_j[t-1] >= n × θ:
    return C
  else:
    return D

  // Last Round (t = r)
  if t == r:
    return D

  // Tie-Breaking
  if θ == 0.5:
    return C with probability 0.5
```

**Exploitative Mindset:**

This strategy is designed to exploit the behavior of opponents by adapting to their cooperation levels. By cooperating when opponents are cooperative and defecting when they're not, we can maximize our own payoffs while minimizing the benefits for others.

The cooperation threshold θ serves as a dynamic indicator of the game's state, allowing us to adjust our actions accordingly. This approach is robust to various opponent behaviors, including pure cooperators, defectors, or mixtures thereof.

In the tournament setting, this strategy should perform well against independent opponents with different strategies, as it doesn't rely on norms or coordination mechanisms.
'''

description_EXPLOITATIVE_555 = '''
To design a exploitative strategy for this N-Player Public Goods Game, I'll introduce a hybrid approach that balances cooperation and defection based on observed opponent behavior and game history. This strategy, which we can call "Adaptive Exploiter," aims to maximize its own payoff by adapting to the dynamics of the game.

### Decision Rules:

1. **Initial Cooperation**: In the first round, cooperate (C). This initial cooperation is a probe to gauge the cooperative tendency of other players and doesn't immediately reveal our exploitative nature.

2. **Observation Phase**: For the next few rounds (let's say 3-5 rounds), observe the actions of all other players but continue to cooperate. Calculate the average cooperation rate among all players during this phase.

   ```python
   avg_cooperation_rate = sum(total_cooperators) / (n * observation_rounds)
   ```

3. **Adaptive Response**:
    - If `avg_cooperation_rate` is above a certain threshold (`high_threshold`, e.g., 0.6), it indicates a generally cooperative environment. In such cases, defect (D) to exploit the cooperators' contributions.
    
    - If `avg_cooperation_rate` falls below another threshold (`low_threshold`, e.g., 0.4), it suggests a competitive environment. Cooperate (C) in this scenario as there might not be enough public good generated through defection alone, and cooperation could stimulate others to contribute.
    
    - For rates between these thresholds, implement a mixed strategy based on the observed rate:
      ```python
      if random.random() < avg_cooperation_rate:
          cooperate()
      else:
          defect()
      ```

4. **Continuous Monitoring**: Continue monitoring the game's evolution, adjusting `avg_cooperation_rate` over time to reflect changes in player behavior.

5. **Last Round Adjustment**:
    - In the penultimate round, observe and store the actions of all players.
    - In the last round, if a majority cooperated in the previous round (including your own action), defect to maximize personal gain knowing it won't affect future cooperation.

### Handling Edge Cases:

- **First Round**: Cooperate as per the initial strategy.
- **Last Round**: Adjust based on observed behavior from the penultimate round, with a leaning towards defection if others have shown cooperative tendencies.
- **Other Rounds**: Follow the adaptive response strategy based on observed `avg_cooperation_rate`.

### Pseudocode for Adaptive Exploiter Strategy:

```python
def adaptive_exploiter(n, r, k):
    observation_rounds = 5
    high_threshold = 0.6
    low_threshold = 0.4
    
    # Initial Cooperation Phase
    if current_round <= 1:
        cooperate()
        
    # Observation and Calculation Phase
    elif current_round <= observation_rounds + 1:
        observe_actions()
        calculate_avg_cooperation_rate()
        
    # Adaptive Response Phase
    else:
        if avg_cooperation_rate > high_threshold:
            defect()  # Exploit cooperation
        elif avg_cooperation_rate < low_threshold:
            cooperate()  # Stimulate contribution
        else:
            random_strategy(avg_cooperation_rate)
            
    # Last Round Adjustment
    if current_round == r - 1:
        observe_and_store()
        
    if current_round == r:
        last_round_adjustment(observed_actions_from_last_round)
```

This Adaptive Exploiter strategy is designed to be robust and exploitative, leveraging the dynamics of cooperation and defection in the N-Player Public Goods Game. By continuously monitoring the game's environment and adapting its strategy, it aims to maximize payoff in a wide range of opponent behaviors without relying on norms or coordination mechanisms.
'''

description_EXPLOITATIVE_556 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors in the N-Player Public Goods Game. This strategy relies on observing the game's history and adapting its behavior accordingly.

**Decision Rules:**

1. **Initial Rounds (t ≤ 3):** Cooperate (C) in the first three rounds to gather information about the opponents' behavior.
2. **Observation Phase (4 ≤ t < r/2):**
	* If the total number of cooperators in the previous round is greater than or equal to n/2, cooperate (C).
	* Otherwise, defect (D).
3. **Exploitation Phase (t ≥ r/2):**
	* Calculate the average payoff for each player over the past 3 rounds.
	* Identify the player(s) with the lowest average payoff.
	* Cooperate (C) if you are one of the players with the lowest average payoff, and defect (D) otherwise.

**Handling Edge Cases:**

1. **First Round:** Cooperate (C).
2. **Last Round (t = r):** Defect (D), as there is no future opportunity to punish or reward opponents.
3. **Ties in Average Payoff:** In case of ties, prioritize players who cooperated more frequently in the past 3 rounds.

**Pseudocode:**
```
// Initialize variables
total_cooperators = 0
past_payoffs = {} // store payoffs for each player over past 3 rounds

// Loop through rounds
for t from 1 to r:
    if t ≤ 3:
        action = COOPERATE (C)
    elif 4 ≤ t < r/2:
        if total_cooperators ≥ n/2:
            action = COOPERATE (C)
        else:
            action = DEFECT (D)
    else: // exploitation phase
        calculate_average_payoffs(past_payoffs)
        lowest_avg_payoff_players = identify_lowest_average_payoff_players()
        if I am in lowest_avg_payoff_players:
            action = COOPERATE (C)
        else:
            action = DEFECT (D)

    update_total_cooperators(action)
    update_past_payoffs(payoffs)

// Helper functions
calculate_average_payoffs(past_payoffs):
    // calculate average payoff for each player over past 3 rounds

identify_lowest_average_payoff_players():
    // identify players with lowest average payoff
```
This strategy is designed to:

* Gather information about opponents' behavior in the initial rounds.
* Cooperate when most players cooperate, and defect otherwise, during the observation phase.
* Exploit weaker players by cooperating only when they have low average payoffs.

By adapting to the game's history and exploiting weaknesses in opponents' strategies, the Adaptive Predator aims to maximize its total payoff over the course of the game.
'''

description_EXPLOITATIVE_557 = '''
To design an exploitative strategy for this N-Player Public Goods Game that is adaptive and robust, we'll focus on a approach that balances exploration with exploitation, leveraging insights from game theory and machine learning.

**Strategy Name:** Adaptive Exploitation (AE)

**Decision Rules:**

1. **Initial Exploration Phase**: In the first round, play Cooperate (C) to gather information about opponents' behaviors.
2. **History-Based Adaptation**: After the initial phase, use a weighted average of past observations to inform decisions:
	* Calculate the average number of cooperators (`avg_cooperators`) and defectors (`avg_defectors`) in previous rounds.
	* Compute the ratio `r = avg_cooperators / (avg_cooperators + avg_defectors)`.
3. **Exploitation Logic**:
	* If `r > 0.5` (more cooperators than expected), play Defect (D). This exploits the willingness of others to contribute, allowing us to benefit from their cooperation.
	* Otherwise, play Cooperate (C). This helps maintain a stable level of cooperation and encourages others to do the same.

**Edge Case Handling:**

1. **First Round**: Play Cooperate (C) as per Initial Exploration Phase.
2. **Last Round**: If `r > 0.5`, play Defect (D) to maximize personal payoff, regardless of others' actions. Otherwise, play Cooperate (C).
3. **No Previous Rounds** (`avg_cooperators` and `avg_defectors` are undefined): Play Cooperate (C).

**Additional Tweaks:**

1. **Noise Tolerance**: Introduce a small random perturbation to the calculated ratio `r`. This helps prevent overfitting to specific opponent behaviors.
2. **Learning Rate Adjustment**: Gradually decrease the learning rate as the game progresses, allowing for more conservative updates to the strategy.

**Pseudocode (Simplified):**
```python
def adaptive_exploitation(history):
    # Initial exploration phase
    if len(history) == 0:
        return 'C'

    avg_cooperators = sum(1 for round in history if round.count('C') > 0)
    avg_defectors = len(history) - avg_cooperators

    r = avg_cooperators / (avg_cooperators + avg_defectors)

    # Noise tolerance
    noise = random.uniform(-0.05, 0.05)
    r += noise

    if r > 0.5:
        return 'D'
    else:
        return 'C'

def play_round(history):
    action = adaptive_exploitation(history)
    return action
```
This Adaptive Exploitation strategy balances exploration and exploitation by leveraging historical data to inform decisions. It adapts to the evolving behavior of opponents, making it robust against a wide range of strategies.
'''

description_EXPLOITATIVE_558 = '''
**Exploitative Strategy: "Adaptive Defector"**

**Decision Rules:**

1. **Initial Cooperation**: In the first round (t=1), cooperate (C) to gather information about opponents' behavior and potential cooperation levels.
2. **Assess Opponent Behavior**: For each subsequent round (t>1), assess the number of cooperators in the previous round (c_t-1). If c_t-1 > n/2, assume a cooperative environment; otherwise, assume an exploitative environment.
3. **Adaptive Defection**:
	* In a cooperative environment (c_t-1 > n/2): Cooperate (C) with probability p_C = k/n - 0.5. This encourages cooperation while allowing for some exploitation.
	* In an exploitative environment (c_t-1 ≤ n/2): Defect (D) with probability p_D = 0.8 + 0.2 \* (n-c_t-1)/n. This increases the likelihood of defecting as more opponents cooperate, maximizing personal gain.
4. **Selfish Adjustment**: If the game enters its final rounds (last 20% of r), adjust the strategy to maximize individual payoff:
	+ In a cooperative environment: Defect (D) with probability p_D = 0.5 + 0.5 \* (r-t)/r, increasing as the game nears its end.
	+ In an exploitative environment: Maintain the adaptive defection probabilities.

**Edge Cases:**

1. **Last Round**: In the final round (t=r), defect (D) to maximize personal gain.
2. **Ties in Cooperation Levels**: When assessing opponent behavior, break ties by assuming a cooperative environment if c_t-1 = n/2.

**Pseudocode**
```python
def adaptive_defector(n, k, r):
  # Initialize variables
  t = 0  # round counter
  prev_coop_count = 0

  while t < r:
    if t == 0:  # First round
      action = 'C'
    else:
      coop_env = (prev_coop_count > n/2)

      if coop_env:
        p_C = k/n - 0.5
        action = 'C' if random.random() < p_C else 'D'
      else:
        p_D = 0.8 + 0.2 * (n-prev_coop_count)/n
        action = 'D' if random.random() < p_D else 'C'

    # Selfish adjustment in final rounds
    if t >= r*0.8:
      coop_env = (prev_coop_count > n/2)
      if coop_env:
        p_D = 0.5 + 0.5 * (r-t)/r
        action = 'D' if random.random() < p_D else 'C'
      else:  # Maintain adaptive defection probabilities

    # Update variables for next round
    t += 1
    prev_coop_count = count_cooperators(t-1)

    return action
```
This strategy, "Adaptive Defector," balances cooperation and exploitation by adapting to the game's dynamics. It initially cooperates to gather information, then adjusts its behavior based on opponents' actions. By defecting more frequently in exploitative environments and adjusting to maximize personal gain in final rounds, this strategy is designed to perform well against a wide range of opponent behaviors in the tournament.
'''

description_EXPLOITATIVE_559 = '''
I'll design a strategic approach that exploits the game's structure and adapts to various opponent behaviors. Here's my proposed strategy:

**Strategy Name: Adaptive Exploitation (AE)**

**Decision Rules:**

1. **Initial Round**: Cooperate (C) in the first round to gather information about opponents' initial tendencies.
2. **Exploitation Phase**: For rounds 2 to r-1, use the following decision rules:
	* If the total number of cooperators in the previous round is greater than or equal to n/2, Defect (D). This takes advantage of the existing cooperation and maximizes individual payoff.
	* Otherwise, Cooperate (C) if the opponent's average cooperation rate (OCR) is above a certain threshold θ (e.g., 0.5), indicating a willingness to cooperate. OCR is calculated as the proportion of cooperative actions taken by opponents in previous rounds.
3. **Final Round**: Defect (D) in the last round, as there's no future benefit from cooperating.

**Pseudocode for Exploitation Phase:**

`AE_Strategy(current_round, opponent_history):`

```
if current_round == 1:
    return C

total_cooperators = count_cooperators(opponent_history[-1])
opponent_ocr = calculate_opponent_ocr(opponent_history)

if total_cooperators >= n / 2:
    return D
elif opponent_ocr > θ:
    return C
else:
    return D
```

**Adaptive Mechanisms:**

* The OCR threshold θ can be adjusted based on the game's parameters and performance. A lower θ may encourage more cooperation, while a higher θ may lead to more defection.
* To counter opponents who adapt to AE by increasing their cooperation rate, AE can introduce occasional 'probing' cooperations (e.g., every 5 rounds) to reassess the opponent's OCR.

**Robustness and Edge Cases:**

* AE handles edge cases like the first round, last round, and cases where opponents switch between cooperation and defection.
* By considering the total number of cooperators in previous rounds, AE is robust against various strategies, including those that rely on coordination or specific patterns.
* The OCR threshold θ provides a mechanism to adapt to different opponent behaviors and avoid over-exploitation.

**Exploitative Mindset:**

AE's primary objective is to maximize individual payoff by exploiting the game's structure and opponents' tendencies. By defecting when cooperation is abundant and cooperating when opponents are willing to cooperate, AE takes advantage of existing cooperation while minimizing its own contributions.

By implementing this strategy, you'll have a robust and exploitative approach that can effectively compete against various independent strategies in the tournament.
'''

description_EXPLOITATIVE_560 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to maximize its own payoff by exploiting the cooperation levels of other players while minimizing the risk of being exploited itself.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability p = 0.5. This allows us to gather information about the opponents' behavior without committing to a specific strategy.
2. **Cooperation Threshold**: After the first round, calculate the cooperation threshold τ as follows:

τ = (k / n) \* (average number of cooperators in previous rounds)

If the average number of cooperators is above τ, defect (D). Otherwise, cooperate (C).
3. **Exploitation Mode**: If the opponent's cooperation rate is higher than τ in two consecutive rounds, switch to exploitation mode:

a. Defect (D) if the opponent's cooperation rate is high (> 0.7 \* τ)
b. Cooperate (C) if the opponent's cooperation rate is low (< 0.3 \* τ)

This allows us to exploit the opponent's cooperation while minimizing our own contributions.
4. **Punishment Mechanism**: If an opponent defects (D) after we cooperated (C), punish them by defecting (D) in the next round. This deters opponents from exploiting our cooperation.

**Edge Cases:**

1. **Last Round**: In the last round, always defect (D). There is no incentive to cooperate, as there will be no future rounds to reciprocate.
2. **Tiebreaker**: If two or more opponents have the same cooperation rate, choose the one with the highest average payoff.

**Pseudocode:**
```python
def adaptive_exploiter(game_history):
    if game_history.round == 1:
        # Initial Cooperation
        return C with probability 0.5
    
    # Calculate Cooperation Threshold
    tau = (k / n) * avg_cooperators(game_history)
    
    # Exploitation Mode
    opponent_coop_rate = coop_rate(opponent, game_history)
    if opponent_coop_rate > tau:
        return D
    elif opponent_coop_rate < 0.3 * tau:
        return C
    
    # Punishment Mechanism
    if opponent_defected(game_history):
        return D
    
    # Default to Cooperate
    return C

def avg_cooperators(game_history):
    # Calculate average number of cooperators in previous rounds
    ...

def coop_rate(opponent, game_history):
    # Calculate opponent's cooperation rate
    ...

def opponent_defected(game_history):
    # Check if opponent defected after we cooperated
    ...
```
This strategy is designed to be robust and adaptive, taking into account the game parameters and history. It balances cooperation and exploitation to maximize its own payoff while minimizing the risk of being exploited by others.
'''

description_EXPLOITATIVE_561 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperation of others while minimizing its own contributions to the public good.

**Decision Rules:**

1. **Initial Round (Round 1):** Cooperate (C) with a probability of 0.5. This allows us to gather information about the opponents' behaviors without committing to a specific strategy.
2. **Subsequent Rounds:** For each round t, calculate the following:
	* `opponent_cooperation_rate`: The average cooperation rate of all opponents over the previous rounds (t-1).
	* `expected_payoff_defect`: The expected payoff if we defect (D) in this round, calculated as 1 + (k/n) \* `opponent_cooperation_rate`.
	* `expected_payoff_cooperate`: The expected payoff if we cooperate (C) in this round, calculated as (k/n) \* (`opponent_cooperation_rate` + 1).
3. **Cooperate or Defect:** Choose the action that maximizes our expected payoff. If `expected_payoff_defect` > `expected_payoff_cooperate`, defect (D). Otherwise, cooperate (C).

**Handling Edge Cases:**

* **Last Round (Round r):** Cooperate (C) if we have been cooperating in most of the previous rounds (i.e., our cooperation rate is higher than 0.5). Defect (D) otherwise.
* **Opponents' Cooperation Rate equals 0:** If all opponents have defected in the previous rounds, defect (D) as well.
* **Opponents' Cooperation Rate equals 1:** If all opponents have cooperated in the previous rounds, cooperate (C) to maximize our payoff.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    opponent_cooperation_rate = 0.5  # initial guess for opponent cooperation rate
    expected_payoff_defect = 0
    expected_payoff_cooperate = 0

    for t in range(1, r+1):
        if t == 1:
            # Initial round: cooperate with probability 0.5
            action = random.choice([C, D])
        else:
            # Calculate opponent cooperation rate and expected payoffs
            opponent_cooperation_rate = calculate_opponent_cooperation_rate(t-1)
            expected_payoff_defect = calculate_expected_payoff_defect(opponent_cooperation_rate)
            expected_payoff_cooperate = calculate_expected_payoff_cooperate(opponent_cooperation_rate)

            # Choose action that maximizes expected payoff
            if expected_payoff_defect > expected_payoff_cooperate:
                action = D
            else:
                action = C

        # Update opponent cooperation rate and expected payoffs for next round
        update_opponent_cooperation_rate(action, t)
```
**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to exploit the cooperation of others while minimizing its own contributions to the public good. By adapting to the opponents' behaviors, we can maximize our payoff in a wide range of scenarios. The strategy does not rely on norms or coordination mechanisms, making it robust against independent opponent strategies.

By playing this strategy in a tournament, we aim to outperform other AI systems by exploiting their cooperative tendencies and maximizing our own payoffs.
'''

description_EXPLOITATIVE_562 = '''
**Strategy Name: Adaptive Exploiter**

The Adaptive Exploiter is a history-dependent, exploitative strategy designed for the N-Player Public Goods Game. It aims to maximize its payoff by adapting to the opponents' behavior and exploiting their cooperative tendencies.

**Decision Rules:**

1. **Initial Rounds (t < 3):** Cooperate (C) in the first two rounds to gather information about the opponents' initial strategies.
2. **Opponent Cooperation Rate:** Calculate the average cooperation rate of all opponents over the last 2-3 rounds (depending on the current round). If the rate is above a certain threshold (e.g., 0.4), cooperate in the next round; otherwise, defect.
3. **Recent Payoff Comparison:** Compare your recent payoffs with those of your opponents. If you've received lower payoffs than at least half of your opponents over the last 2-3 rounds, switch to cooperation for one round; otherwise, maintain your current strategy (cooperation or defection).
4. **Exploitation Phase:** Once an opponent's cooperation rate exceeds a higher threshold (e.g., 0.7), enter exploitation mode: defect in every subsequent round.
5. **Punishment Mechanism:** If you've been exploited by an opponent (i.e., their cooperation rate drops below 0.3 after being above 0.4), punish them by cooperating for one round and then switching back to your previous strategy.

**Edge Cases:**

* **Last Round (t = r):** Defect, as there's no future opportunity for reciprocity or punishment.
* **Single Opponent Cooperation:** If only one opponent cooperates consistently, exploit their cooperation by defecting in every round after the initial rounds.
* **Multiple Cooperative Opponents:** If multiple opponents cooperate frequently, adapt to a mixed strategy: alternate between cooperation and defection to maintain an average payoff while minimizing exploitation.

**Pseudocode (simplified):**
```python
# Initialize variables
opponent_cooperation_rates = [0] * n
recent_payoffs = []
threshold_low = 0.4
threshold_high = 0.7

for t in range(1, r+1):
    if t < 3:
        # Initial rounds: cooperate
        action = C
    else:
        # Calculate opponent cooperation rates and recent payoffs
        for i in range(n):
            opponent_cooperation_rates[i] = (opponent_cooperation_rates[i] * (t-2) + c_i) / (t-1)
        recent_payoffs.append(π_i)

        # Compare recent payoffs and adjust strategy
        if len(recent_payoffs) > 2:
            payoff_comparison = [payoff < np.median([p for p in recent_payoffs[-3:]]) for payoff in recent_payoffs[-3:]]
            if any(payoff_comparison):
                action = C

        # Opponent cooperation rate-based decision
        opponent_cooperation_rate_avg = sum(opponent_cooperation_rates) / n
        if opponent_cooperation_rate_avg > threshold_low:
            action = C
        elif opponent_cooperation_rate_avg < threshold_high and action != D:
            action = D

        # Exploitation phase
        if opponent_cooperation_rate_avg > threshold_high:
            action = D

        # Punishment mechanism
        if any(opponent_cooperation_rates[i] < 0.3 for i in range(n) if opponent_cooperation_rates[i] > 0.4):
            action = C
```
This strategy is designed to be robust against various opponent behaviors, while maximizing its own payoff through adaptive exploitation and punishment mechanisms.
'''

description_EXPLOITATIVE_563 = '''
**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about opponents' behavior and encourage cooperation.
2. **Opponent Classification (t > 2):**
	* Observe the number of cooperators (N_C) and defectors (N_D) in each round.
	* Classify opponents into three categories based on their actions:
		+ Cooperators: Players who have cooperated more than 75% of the time.
		+ Defectors: Players who have defected more than 75% of the time.
		+ Unpredictables: Players with a cooperation rate between 25% and 75%.
3. **Exploitation Logic (t > 2):**
	* If N_C ≥ n/2, Cooperate (C). This ensures that if enough players are cooperating, we continue to contribute to the public good.
	* If N_D ≥ n/2, Defect (D). In this case, it's rational to free-ride on others' contributions.
	* If there is a mix of cooperators and defectors:
		+ If k/n × Σ(j=1 to n) c_j > 0.5 (i.e., the public good's value exceeds half the private payoff), Cooperate (C).
		+ Otherwise, Defect (D).
4. **Late-Game Adaptation (t ≥ r - 2):**
	* If there are more cooperators than defectors in the previous round, Defect (D). This takes advantage of others' altruism and maximizes our payoff.
	* If there are more defectors than cooperators, Cooperate (C) only if k/n × Σ(j=1 to n) c_j > 0.5. Otherwise, Defect (D).

**Edge Cases:**

* **First Round:** Cooperate (C).
* **Last Round (t = r):** Defect (D), as there is no future game to influence.
* **Ties in Opponent Classification:** Break ties by classifying the opponent as an Unpredictable.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
  # Initialize variables
  N_C = [0] * n
  N_D = [0] * n
  opponents = ['Unpredictable'] * n

  for t in range(r):
    if t <= 2:
      action = 'C'
    else:
      # Classify opponents and count cooperators/defectors
      for i in range(n):
        if opponents[i] == 'Cooperator' or (opponents[i] == 'Unpredictable' and N_C[i] / t > 0.75):
          N_C[i] += 1
        elif opponents[i] == 'Defector' or (opponents[i] == 'Unpredictable' and N_D[i] / t > 0.75):
          N_D[i] += 1

      # Determine action based on opponent classification and exploitation logic
      if sum(N_C) >= n/2:
        action = 'C'
      elif sum(N_D) >= n/2:
        action = 'D'
      else:
        # Mix of cooperators and defectors
        public_good_value = k/n * sum(N_C)
        if public_good_value > 0.5:
          action = 'C'
        else:
          action = 'D'

    # Late-game adaptation
    if t >= r - 2:
      if sum(N_C) > sum(N_D):
        action = 'D'
      elif sum(N_D) > sum(N_C) and k/n * sum(N_C) <= 0.5:
        action = 'D'

    return action
```
This strategy, Adaptive Exploiter (AE), balances cooperation and exploitation to maximize payoffs while adapting to opponents' behavior over time. By initially cooperating, AE encourages others to cooperate, then exploits their actions using a combination of classification and game-theoretic logic.
'''

description_EXPLOITATIVE_564 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its payoff by adapting to the opponents' behavior while exploiting their cooperation.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) in the first round to gather information about the opponents' initial behavior.
2. **Subsequent Rounds (t>1)**:
	* If the total number of cooperators in the previous round is less than or equal to n/2, Defect (D). This takes advantage of the public good created by others while minimizing personal contribution.
	* If the total number of cooperators in the previous round is greater than n/2, Cooperate (C) with a probability p, where p = (k/n) \* (total_cooperators_previous_round / n). This probabilistic cooperation encourages opponents to continue cooperating while allowing the Adaptive Exploiter to free-ride occasionally.
3. **Last Round (t=r)**: Defect (D) in the last round, as there is no future benefit from cooperating.

**Additional Rules:**

* **Punish Over-Cooperation**: If an opponent has cooperated for more than 75% of the rounds so far, Defect (D) in the current round. This discourages overly cooperative opponents and encourages them to adapt.
* **Exploit Under-Cooperation**: If an opponent has defected for more than 75% of the rounds so far, Cooperate (C) in the current round with a probability p = 0.5. This takes advantage of the public good created by others while minimizing personal contribution.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r, history):
    if t == 1:
        return 'C'
    
    total_cooperators_previous_round = sum(history[-1])
    
    if total_cooperators_previous_round <= n/2:
        return 'D'
    else:
        p = (k/n) * (total_cooperators_previous_round / n)
        return 'C' with probability p, otherwise 'D'
    
    # Last round
    if t == r:
        return 'D'
    
    # Punish over-cooperation
    for opponent in history:
        if sum(opponent) / len(opponent) > 0.75:
            return 'D'
    
    # Exploit under-cooperation
    for opponent in history:
        if sum(1 - x for x in opponent) / len(opponent) > 0.75:
            return 'C' with probability 0.5, otherwise 'D'
```
This strategy is designed to be adaptive and exploitative, taking into account the game parameters and history while minimizing personal contribution to the public good. By punishing over-cooperation and exploiting under-cooperation, the Adaptive Exploiter aims to maximize its payoff in a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_565 = '''
I'll design a strategy that balances exploitation with adaptability and robustness. Introducing "Adaptive Exploiter" (AE):

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) to gather information about opponents' behaviors.
2. **Exploit Greed**: If the total contributions in the previous round were less than n/2 (i.e., fewer than half of the players cooperated), defect (D) in the current round. This exploits opponents who tend to free-ride or are overly greedy.
3. **Mirror Cooperators**: If the total contributions in the previous round were exactly n/2 or more, cooperate (C) in the current round if at least one opponent cooperated in the previous round. Otherwise, defect (D). This encourages cooperation while still exploiting opponents who don't reciprocate.
4. **Punish Defectors**: If an opponent defected in the previous round and their payoff was higher than yours, defect (D) against them in the current round. This punishes opponents for exploiting you.

**Edge Cases:**

* In the last round, always defect (D), as there's no future game to consider.
* If all opponents have consistently cooperated or defected so far, mirror their behavior (cooperate if they cooperate, defect if they defect).
* If an opponent has never contributed before but does so in a later round, treat them as a new player and follow the initial cooperation rule.

**Adaptive Elements:**

* Monitor the number of contributors and adjust strategy accordingly.
* Update the exploitation threshold (n/2) based on observed opponent behaviors. If opponents tend to cooperate more frequently, increase the threshold; if they defect more often, decrease it.
* Gradually increase or decrease the punishment frequency against defectors based on their past behavior.

**Robustness:**

* By initially cooperating, AE gathers information about opponents' behaviors and adapts its strategy accordingly.
* By exploiting greed and punishing defectors, AE discourages opponents from taking advantage of its cooperative nature.
* By mirroring cooperators and adapting to changing opponent behaviors, AE remains robust against a wide range of strategies.

**Pseudocode:**
```python
def AdaptiveExploiter(history):
  # Initialize variables
  n = num_players()
  k = multiplication_factor()
  r = num_rounds()

  # First round: cooperate
  if history.round == 1:
    return C

  # Exploit greed
  total_contributions = sum([history.player[i].contribution for i in range(n)])
  if total_contributions < n / 2:
    return D

  # Mirror cooperators
  num_cooperators = sum([1 for player in history.players if player.contribution])
  if num_cooperators >= n / 2:
    if any(player.contribution for player in history.players):
      return C
    else:
      return D

  # Punish defectors
  opponent_payoffs = [player.payoff for player in history.players]
  my_payoff = history.my_payoff()
  if any(opponent_payoffs[i] > my_payoff and history.players[i].contribution == 0):
    return D

  # Default: cooperate
  return C
```
This strategy should perform well against a variety of opponents, as it balances exploitation with adaptability and robustness.
'''

description_EXPLOITATIVE_566 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit opponents' behaviors while being robust to various strategies and adapting to changing game dynamics.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
	* Cooperate with probability 0.5 in the first round to gather information about opponents' initial tendencies.
	* In rounds 2 and 3, cooperate if the majority of opponents cooperated in the previous round; otherwise, defect.
2. **Adaptive Phase (Rounds 4-r-1)**:
	* Calculate the average cooperation rate of all opponents over the last 3 rounds (excluding the current round).
	* If the average cooperation rate is above 0.5, cooperate with probability 0.7; otherwise, defect with probability 0.7.
	* Adjust this probability based on the opponent's deviation from the expected payoff:
		+ If an opponent consistently receives higher payoffs than expected, decrease the cooperation probability against that opponent by 0.1.
		+ If an opponent consistently receives lower payoffs than expected, increase the cooperation probability against that opponent by 0.1.
3. **Final Round (Round r)**:
	* Defect if the game is in a state where defecting would yield a higher payoff; otherwise, cooperate.

**Pseudocode:**

```
Function AdaptivePredator(n, k, r):
  // Initialize variables
  cooperation_rates = [0] * n  // Store opponents' average cooperation rates over last 3 rounds
  payoffs_deviation = [0] * n  // Store opponents' payoff deviations from expected values

  For t = 1 to r:
    If t <= 3:  // Initial Exploration Phase
      cooperate_prob = 0.5
      If t > 1:
        majority_cooperated = sum(cooperation_rates) / n >= 0.5
        cooperate_prob = majority_cooperated ? 1 : 0
    Else:  // Adaptive Phase
      avg_cooperation_rate = sum(cooperation_rates) / (3 * n)
      cooperate_prob = avg_cooperation_rate > 0.5 ? 0.7 : 0.3

      For i = 1 to n:
        opponent_deviation = payoffs_deviation[i]
        If opponent_deviation > 0: cooperate_prob -= 0.1
        Else If opponent_deviation < 0: cooperate_prob += 0.1

    // Make decision based on cooperate probability
    cooperate = random() < cooperate_prob

    // Update cooperation rates and payoff deviations
    For i = 1 to n:
      If t > 3: cooperation_rates[i] = (cooperation_rates[i] * 2 + opponents_cooperated[i]) / 3
      payoffs_deviation[i] = actual_payoff[i] - expected_payoff[i]

    // Play final round
    If t == r:
      cooperate = False

Return cooperate
```

**Rationale:**

1. The initial exploration phase allows the strategy to gather information about opponents' tendencies and adjust its behavior accordingly.
2. In the adaptive phase, the strategy adjusts its cooperation probability based on the average cooperation rate of all opponents, as well as individual opponents' payoff deviations from expected values.
3. By adapting to changing game dynamics and exploiting opponents' behaviors, the Adaptive Predator strategy aims to maximize its own payoffs while minimizing the impact of opponents' actions.

This strategy is designed to be robust against a wide range of opponent behaviors, including tit-for-tat, grim trigger, and other exploitative strategies.
'''

description_EXPLOITATIVE_567 = '''
Here's a strategic design for the N-Player Public Goods Game that is exploitative and adaptive:

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Rounds:** In the first few rounds (specifically, `min(5, r/2)`), play Cooperate (C) to gather information about opponents' behaviors and potential cooperation levels.
2. **Assess Cooperation Level:** After the initial rounds, calculate the average cooperation rate (`coop_rate`) among all players over the past rounds. This will be used as a proxy for the group's overall willingness to cooperate.
3. **Exploitation Threshold:** Set an exploitation threshold (`exploit_threshold`) based on the game parameters and history:
	* If `k` is high (closer to `n`), set `exploit_threshold` to a lower value (e.g., 0.4) to encourage cooperation.
	* If `k` is low (closer to 1), set `exploit_threshold` to a higher value (e.g., 0.7) to deter free-riding.
4. **Cooperate or Defect:**
	* If `coop_rate` is above the `exploit_threshold`, play Cooperate (C). This encourages continued cooperation and takes advantage of high contributors.
	* If `coop_rate` is below the `exploit_threshold`, play Defect (D). This exploits low contributors and deters free-riding.
5. **Adjustment Mechanism:** After each round, adjust the `coop_rate` calculation by incorporating the new information:
	+ Increase `coop_rate` if most players cooperated in the previous round.
	+ Decrease `coop_rate` if most players defected in the previous round.

**Edge Cases:**

* **Last Round:** In the final round (`t = r`), play Defect (D) to maximize personal payoff, as there is no future interaction to consider.
* **Low Cooperation Rate:** If `coop_rate` falls below a certain threshold (e.g., 0.2), switch to playing Defect (D) for the remainder of the game to avoid being exploited.

**Pseudocode:**
```
function AdaptiveExploiter(n, k, r):
  // Initial rounds
  for t = 1 to min(5, r/2):
    play C

  // Assess cooperation level and set exploitation threshold
  coop_rate = calculate_average_cooperation()
  exploit_threshold = calculate_exploit_threshold(k)

  // Main game loop
  for t = (min(5, r/2) + 1) to r:
    if coop_rate > exploit_threshold:
      play C
    else:
      play D

    // Adjust cooperation rate and exploitation threshold
    coop_rate = update_coop_rate(coop_rate, previous_round_actions)
    exploit_threshold = update_exploit_threshold(exploit_threshold, k)

    // Edge cases
    if t == r:  // Last round
      play D
    elif coop_rate < 0.2:  // Low cooperation rate
      play D for remainder of game

function calculate_average_cooperation():
  // Calculate average cooperation rate over past rounds
  return sum(coop_rates) / (t - min(5, r/2))

function calculate_exploit_threshold(k):
  // Set exploitation threshold based on k and game history
  if k > n * 0.7:
    return 0.4
  else:
    return 0.7

function update_coop_rate(coop_rate, previous_round_actions):
  // Update cooperation rate based on new information
  if most_players_cooperated(previous_round_actions):
    coop_rate += 0.1
  elif most_players_defected(previous_round_actions):
    coop_rate -= 0.1
  return coop_rate

function update_exploit_threshold(exploit_threshold, k):
  // Update exploitation threshold based on new information and game parameters
  if k > n * 0.7:
    exploit_threshold = max(0.4, exploit_threshold - 0.05)
  else:
    exploit_threshold = min(0.7, exploit_threshold + 0.05)
  return exploit_threshold
```
This strategy is designed to be exploitative by taking advantage of high contributors while deterring free-riding. It adapts to the game environment and opponents' behaviors through its assessment of cooperation levels and adjustment mechanism.
'''

description_EXPLOITATIVE_568 = '''
**Exploitative Strategy: Adaptive Defector (AD)**

The Adaptive Defector (AD) strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. AD's primary goal is to maximize its total payoff by identifying and capitalizing on patterns in opponents' actions.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C). This allows AD to gather information about the opponents' initial tendencies.
2. **Majority-Based Defection**: From the second round onwards, defect (play D) if the majority of opponents (> n/2) cooperated in the previous round. Otherwise, cooperate.
3. **Consecutive Cooperation Exploitation**: If an opponent has cooperated consecutively for two or more rounds, AD will defect against that opponent in the next round.
4. **Recent Defection Forgiveness**: If an opponent defected in the previous round but had a history of cooperation prior to that, AD will cooperate with that opponent in the next round.

**Edge Cases:**

1. **Last Round**: In the final round, AD will defect regardless of opponents' actions, as there is no future opportunity for reciprocity or retaliation.
2. **Tiebreaker**: If the number of cooperators and defectors is equal in a previous round (i.e., n/2), AD will cooperate in the next round.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
k = multiplication factor
r = number of rounds
coop_count = [0] * n  # cooperation history for each player
defect_count = [0] * n  # defection history for each player

# First round
if current_round == 1:
    play C

# Majority-Based Defection
elif sum(coop_count) > n/2:
    play D
else:
    play C

# Consecutive Cooperation Exploitation
for i in range(n):
    if coop_count[i] >= 2 and defect_count[i] == 0:
        play D against player i

# Recent Defection Forgiveness
for i in range(n):
    if defect_count[i] == 1 and coop_count[i] > 1:
        play C with player i

# Last Round
if current_round == r:
    play D
```
**Exploitative Mindset:**

The Adaptive Defector strategy is designed to take advantage of opponents' cooperation patterns while minimizing the risks associated with reciprocal defection. By initially cooperating, AD gathers information about opponents' tendencies and adapts its behavior accordingly. The majority-based defection rule allows AD to exploit cooperative players, while the consecutive cooperation exploitation and recent defection forgiveness rules help identify and capitalize on weaknesses in opponents' strategies.
'''

description_EXPLOITATIVE_569 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit the behaviors of opponents while adapting to the game's history and parameters. This strategy prioritizes defection (D) but incorporates conditional cooperation (C) to maximize payoffs.

**Decision Rules:**

1. **Initial Round**: Defect (D). No information about opponents' behavior is available, so it's best to start with a safe choice.
2. **Subsequent Rounds**:
	* If the total number of cooperators in the previous round is less than or equal to `n/2`, defect (D). This indicates that cooperation is not prevalent, and exploiting others' contributions is more profitable.
	* If the total number of cooperators in the previous round is greater than `n/2` and the multiplication factor `k` is high (`k > n - 1`), cooperate (C). In this scenario, contributing to the public good may yield a higher payoff due to the increased share of the public good.
	* If the total number of cooperators in the previous round is greater than `n/2` and the multiplication factor `k` is low (`k ≤ n - 1`), defect (D). In this case, exploiting others' contributions remains a better option.
3. **Punishment Mechanism**: If an opponent defects while you cooperate, increase the likelihood of defecting in subsequent rounds by a small amount (e.g., `0.05`). This simulates a "punishment" for uncooperative behavior.
4. **Exploration Phase**: Every 5-10 rounds, randomly cooperate or defect with a probability of `0.2`. This allows the strategy to gather information about opponents' reactions and adapt to potential changes in their behavior.

**Edge Cases:**

1. **Last Round**: Defect (D). In the final round, there is no future interaction to consider, so maximizing immediate payoff takes priority.
2. **Tiebreaker**: If multiple actions have the same expected payoff, cooperate (C) with a small probability (e.g., `0.05`). This introduces some randomness and makes the strategy harder to exploit.

**Pseudocode:**
```python
def adaptive_predator(n, k, history):
  # Initial Round
  if len(history) == 0:
    return "D"
  
  # Subsequent Rounds
  prev_cooperators = sum([1 for action in history[-1] if action == "C"])
  if prev_cooperators <= n / 2:
    return "D"
  elif prev_cooperators > n / 2 and k > n - 1:
    return "C"
  else:
    return "D"
  
  # Punishment Mechanism
  for opponent in history[-1]:
    if opponent == "D" and self_action == "C":
      defect_likelihood += 0.05
  
  # Exploration Phase (every 5-10 rounds)
  if len(history) % 5 == 0:
    return random.choice(["C", "D"], p=[0.2, 0.8])
  
  # Last Round
  if current_round == total_rounds:
    return "D"
```
This strategy is designed to be adaptive and exploitative, taking into account the game's history and parameters while making decisions that maximize payoffs. By incorporating conditional cooperation and a punishment mechanism, the Adaptive Predator strategy aims to outperform opponents in a wide range of scenarios.
'''

description_EXPLOITATIVE_570 = '''
Here's a proposed exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 50%. This allows us to gather information about the opponents' behavior while minimizing potential losses.
2. **Exploit High Cooperators**: If the total number of cooperators in the previous round is above a certain threshold (e.g., n/2), defect (play D) in the next round. This takes advantage of the high cooperation rate and earns us a higher payoff.
3. **Punish Low Cooperators**: If the total number of cooperators in the previous round is below a certain threshold (e.g., n/4), cooperate (play C) in the next round. This aims to encourage more cooperation among opponents by providing a "carrot" incentive.
4. **Adapt to Opponent Behavior**: Monitor the opponent's average cooperation rate over the past few rounds (e.g., 3-5 rounds). If their cooperation rate is high (>0.6), increase our own cooperation probability; if it's low (<0.4), decrease it. This adaptability helps us adjust to changing opponent behaviors.
5. **Round-dependent Strategy**: In the last round, always defect (play D) as there's no future interaction to consider.

**Edge Case Handling:**

* If n = 2 (only two players), cooperate in the first round and then mirror the opponent's previous action.
* If r = 1 (only one round), defect as there's no opportunity for repeated interactions.

**Pseudocode:**
```markdown
# Initialize variables
n = number of players
r = number of rounds
k = multiplication factor
threshold_high = n/2
threshold_low = n/4
cooperation_rate_opponent = 0.5 (initial value)
rounds_to_average = 3

# First round
if current_round == 1:
    cooperate_prob = 0.5
    if random() < cooperate_prob:
        action = C
    else:
        action = D

# Subsequent rounds
else:
    # Calculate opponent's average cooperation rate over past rounds_to_average rounds
    opponent_cooperation_rate = calculate_opponent_cooperation_rate()

    # Update our own cooperation probability based on opponent behavior
    if opponent_cooperation_rate > 0.6:
        cooperate_prob += 0.1
    elif opponent_cooperation_rate < 0.4:
        cooperate_prob -= 0.1

    # Apply decision rules
    if total_cooperators_last_round > threshold_high:
        action = D
    elif total_cooperators_last_round < threshold_low:
        action = C
    else:
        if random() < cooperate_prob:
            action = C
        else:
            action = D

# Last round
if current_round == r:
    action = D

# Return the chosen action (C or D)
```
**Rationale:**

The Adaptive Exploiter strategy aims to balance short-term gains with long-term adaptability. By monitoring opponent behavior and adjusting our cooperation probability, we can exploit high cooperators while also encouraging more cooperation from low cooperators. The initial random cooperation in the first round allows us to gather information about opponents without committing to a specific strategy too early.

This strategy should be robust against various opponent behaviors and perform well in a tournament setting where strategies are independent and uncoordinated.
'''

description_EXPLOITATIVE_571 = '''
I'll design a exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and only depends on game parameters and history.

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (C) with a probability of 50%. This allows us to gather information about our opponents' behavior while also contributing to the public good.
2. **Exploit Defectors**: If the total number of cooperators in the previous round is less than or equal to n/2, defect (D) in the current round. This takes advantage of defectors who are not contributing to the public good.
3. **Punish Free-Riders**: If a player has defected in the previous round and the total number of cooperators was greater than n/2, defect (D) in the current round. This punishes free-riders who benefit from others' cooperation without contributing themselves.
4. **Imitate Cooperators**: If the total number of cooperators in the previous round is greater than n/2 and the player's own payoff was not the highest among all players, cooperate (C) in the current round. This imitates successful cooperators and potentially increases our own payoff.
5. **Adaptive Threshold**: Maintain a dynamic threshold `θ` (initially set to 0.5) that represents the proportion of cooperators required to switch from defecting to cooperating. Adjust `θ` based on the game's history:
	* If the total number of cooperators in the previous round is less than or equal to θ \* n, decrease θ by 0.1.
	* If the total number of cooperators in the previous round is greater than θ \* n and our own payoff was not the highest among all players, increase θ by 0.1.

**Edge Cases:**

1. **Last Round**: In the final round, defect (D) regardless of the game's history. This ensures we maximize our individual payoff in the last round.
2. **Ties**: If multiple strategies have the same highest payoff, choose the one that involves cooperating (C).

Pseudocode:
```
θ = 0.5
history = []

for each round t from 1 to r:
    if t == 1:  # First round
        cooperate with probability 0.5
    else:
        num_cooperators_prev_round = count cooperators in history[t-1]
        
        if num_cooperators_prev_round <= n/2:
            defect
        elif num_cooperators_prev_round > n/2 and own_payoff != highest_payoff:
            imitate Cooperate
        elif opponent_defected_prev_round and num_cooperators_prev_round > n/2:
            punish Defect
        else:
            adapt_threshold()
            
    update history with current round's actions and payoffs

def adapt_threshold():
    global θ
    
    if num_cooperators_prev_round <= θ * n:
        θ -= 0.1
    elif num_cooperators_prev_round > θ * n and own_payoff != highest_payoff:
        θ += 0.1

def imitate_Cooperate():
    cooperate

def punish_Defect():
    defect
```
This strategy, Adaptive Exploiter (AE), aims to balance cooperation and exploitation by adapting to the game's history and opponents' behaviors. By maintaining a dynamic threshold and adjusting our actions accordingly, we can effectively exploit defectors while also contributing to the public good when beneficial.
'''

description_EXPLOITATIVE_572 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy aims to maximize its payoff by exploiting the cooperation of other players while minimizing its own contributions. The strategy adapts to the game's history and parameters, making it robust against various opponent behaviors.

**Decision Rules:**

1. **Initial Round (t=1):** Defect (D) - In the first round, there is no history to exploit, so playing D maximizes the immediate payoff.
2. **Subsequent Rounds (t>1):**
	* If the average cooperation rate of other players in the previous round (t-1) is above a certain threshold (α), Cooperate (C). This encourages others to continue cooperating.
	* Otherwise, Defect (D).
3. **Adaptive Threshold (α):** Update α after each round based on the game's history:
	+ If the average cooperation rate of other players in the previous round is high (> 0.5), increase α by a small amount (e.g., 0.05).
	+ If the average cooperation rate of other players in the previous round is low (< 0.5), decrease α by a small amount (e.g., 0.05).

**Edge Cases:**

1. **Last Round (t=r):** Defect (D) - In the final round, there is no future payoff to consider, so playing D maximizes the immediate payoff.
2. **Tiebreaker:** If the average cooperation rate of other players is exactly equal to α, Cooperate (C). This favors cooperation in cases where the opponent's behavior is uncertain.

**Pseudocode:**
```python
def adaptive_exploiter(game_history, n, k):
    # Initialize alpha
    alpha = 0.5
    
    for t in range(1, r+1):
        if t == 1:
            action = 'D'  # Defect in the first round
        else:
            avg_coop_rate_prev_round = calculate_avg_coop_rate(game_history, t-1)
            if avg_coop_rate_prev_round > alpha:
                action = 'C'  # Cooperate if others cooperated enough
            else:
                action = 'D'
        
        # Update alpha based on the game history
        if avg_coop_rate_prev_round > 0.5:
            alpha += 0.05
        elif avg_coop_rate_prev_round < 0.5:
            alpha -= 0.05
        
        # Handle last round separately
        if t == r:
            action = 'D'
        
        return action

def calculate_avg_coop_rate(game_history, round_num):
    # Calculate the average cooperation rate of other players in the given round
    coop_count = sum(1 for action in game_history[round_num] if action == 'C')
    avg_coop_rate = coop_count / (n - 1)  # Exclude the Adaptive Exploiter's own action
    return avg_coop_rate
```
This strategy is exploitative, as it aims to maximize its payoff by taking advantage of others' cooperation. By adapting to the game's history and parameters, the Adaptive Exploiter can effectively respond to various opponent behaviors and achieve a high total payoff in the tournament.
'''

description_EXPLOITATIVE_573 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. It leverages the game's parameters and history to maximize its payoff.

**Decision Rules:**

1. **Initial Round (t=1)**: Cooperate (C) in the first round to gather information about opponents' behavior and to potentially set up a cooperative environment.
2. **Subsequent Rounds (t>1)**:
	* If the average cooperation rate of all players in the previous round is above 0.5, Defect (D). This indicates that opponents are likely to be cooperative, making it profitable to free-ride.
	* If the average cooperation rate of all players in the previous round is below 0.5, Cooperate (C). This suggests that opponents are mostly defecting, and cooperating can potentially increase the public good and payoff.
3. **Opponent Cooperation Rate Tracking**: Maintain a running average of each opponent's cooperation rate over the past `min(r/2, 10)` rounds (i.e., at least half of the remaining rounds or 10 rounds, whichever is smaller). This helps to identify opponents' behavioral patterns.
4. **Exploitation**: If an opponent has a cooperation rate above 0.7 in the tracking window, Defect (D) against them. This takes advantage of their cooperative nature and maximizes payoff.
5. **Punishment**: If an opponent has defected against you in the previous round, Defect (D) against them in the current round with a probability of 0.8. This discourages opponents from exploiting you.

**Edge Cases:**

1. **Last Round (t=r)**: Cooperate (C) to maximize the public good and potentially increase payoff.
2. **Single Opponent**: If there is only one opponent, Defect (D) in all rounds except the first round, where cooperation can still provide information about their behavior.

**Pseudocode:**

```
Function AdaptivePredator(n, r, k):
  Initialize cooperation_rate_opp[r] = [0.5] * n
  Initialize last_round_defection[n] = [False] * n

  For t from 1 to r:
    If t == 1:
      Cooperate (C)
    Else:
      avg_coop_rate_all = average(cooperation_rate_opp[:])
      If avg_coop_rate_all > 0.5:
        Defect (D)
      Else:
        Cooperate (C)

      For each opponent i in range(n):
        coop_rate_i = cooperation_rate_opp[i]
        If coop_rate_i > 0.7 and t < r/2:
          Defect (D) against opponent i
        ElseIf last_round_defection[i] == True and random() < 0.8:
          Defect (D) against opponent i

      Update cooperation_rate_opp[:] with current round's actions
      Update last_round_defection[:] with current round's defections
```

The Adaptive Predator strategy is designed to be robust, adaptive, and exploitative, making it a competitive participant in the tournament.
'''

description_EXPLOITATIVE_574 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors in the N-Player Public Goods Game. It combines elements of cooperation and defection to maximize its payoff.

**Decision Rules:**

1. **Initial Cooperation**: Cooperate (C) in the first round to gather information about opponents' behavior.
2. **Majority Rule**: Observe the actions of all players in previous rounds. If the majority (> n/2) cooperated, cooperate (C) in the next round. Otherwise, defect (D).
3. **Punishment Mechanism**: If a player has defected (D) in the previous round and their payoff was higher than the average payoff of all players, punish them by defecting (D) against them in the next round.
4. **Exploitation**: If a player is consistently cooperating (C), exploit them by defecting (D) against them in subsequent rounds.
5. **Adaptation**: Update the majority rule threshold based on the average cooperation rate of all players over the last few rounds.

**Pseudocode:**
```
// Initialize variables
cooperation_threshold = n/2
punishment_list = []

// First round: Cooperate (C)
if round == 1:
    action = C

// Subsequent rounds
else:
    // Observe opponents' actions and payoffs
    observe_opponents()

    // Majority rule
    if num_cooperators > cooperation_threshold:
        action = C
    else:
        action = D

    // Punishment mechanism
    for player in punishment_list:
        if player_defected_last_round(player):
            action = D

    // Exploitation
    for player in consistently_cooperating_players():
        action = D

    // Adaptation: Update cooperation threshold
    update_cooperation_threshold()

// Record opponents' actions and payoffs for future reference
record_opponents_actions()
```
**Handling Edge Cases:**

* **Last Round**: Defect (D) in the last round to maximize payoff, as there is no future punishment or retaliation.
* **Tiebreaker**: In case of a tie in the majority rule, cooperate (C) to maintain a cooperative atmosphere.

**Exploitative Mindset:**
The Adaptive Predator strategy prioritizes exploiting opponents' cooperation to maximize its own payoff. By adapting to the behavior of other players and punishing defectors, it creates an environment where cooperation is not always the best response. This strategy takes advantage of the game's repeated nature to gather information and adjust its actions accordingly.

This exploitative strategy will perform well in a tournament setting against independent strategies developed by other AI systems, as it can adapt to various opponent behaviors and exploit their cooperative tendencies.
'''

description_EXPLOITATIVE_575 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's dynamics. This strategy focuses on maximizing individual payoff by identifying opportunities to defect and minimize losses when cooperating.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about opponents' behavior and establish a cooperative image.
2. **Observation Phase (t > 2):**
	* Calculate the average cooperation rate of all players, excluding yourself, over the last `min(3, t-1)` rounds (`avg_coop_rate`).
	* If `avg_coop_rate` is above a threshold (`THRESHOLD_COOP`, e.g., 0.6), defect (D) in the current round.
	* Otherwise, cooperate (C).
3. **Punishment Mechanism:** If an opponent defects while you cooperated in the previous round, defect against that opponent for `PUNISHMENT_ROUNDS` (e.g., 2) consecutive rounds.

**Edge Cases:**

1. **Last Round (t = r):** Defect (D), as there's no future opportunity to punish or gain from cooperation.
2. **Tiebreaker:** In case of a tie in the average cooperation rate, cooperate (C).

**Pseudocode:**
```python
def adaptive_predator(game_state):
    # Initialize variables
    THRESHOLD_COOP = 0.6
    PUNISHMENT_ROUNDS = 2
    coop_rates = []

    # First two rounds: Cooperate
    if game_state.round <= 2:
        return 'C'

    # Calculate average cooperation rate of opponents
    for opponent in game_state.players:
        if opponent != self:
            coop_rate = sum(opponent.actions[-min(3, game_state.round-1):]) / min(3, game_state.round-1)
            coop_rates.append(coop_rate)

    avg_coop_rate = sum(coop_rates) / len(coop_rates)

    # Punishment mechanism
    if game_state.previous_round[self] == 'C' and any(opponent.actions[-1] == 'D' for opponent in game_state.players):
        return 'D'

    # Main decision rule
    if avg_coop_rate > THRESHOLD_COOP:
        return 'D'
    else:
        return 'C'
```
This strategy is designed to:

* Gather information about opponents' behavior early on.
* Exploit cooperative opponents by defecting when the average cooperation rate is high.
* Punish opponents who take advantage of cooperation.
* Defect in the last round to maximize individual payoff.

By adapting to the game's dynamics and exploiting opponent behaviors, the Adaptive Predator strategy aims to achieve a high total payoff in the tournament.
'''

description_EXPLOITATIVE_576 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive throughout the game.

**Decision Rules:**

1. **Initial Exploration (Rounds 1-3)**:
Cooperate in the first three rounds to gather information about opponents' behavior. This phase helps to identify potential cooperators, defectors, or mixed strategists.
2. **Exploitation Phase (Rounds 4-r)**:
From round 4 onwards, use the following decision rules:

a. If the total number of cooperators in the previous round is less than n/2:
Defect (play D). This takes advantage of potential free-riding opportunities when opponents are mostly defecting.

b. If the total number of cooperators in the previous round is greater than or equal to n/2:
Cooperate (play C) if k/n > 0.5; otherwise, Defect (play D). This rule balances the desire to exploit cooperation with the need to avoid contributing when the public good's value is low.

c. If an opponent has defected in all previous rounds:
Defect (play D) against that opponent. This counters persistent free-riding behavior.

d. If an opponent has cooperated in all previous rounds:
Cooperate (play C) with that opponent, but only if k/n > 0.5. This encourages cooperation and mutual benefit when possible.

**Edge Case Handling:**

* **First Round:** Cooperate to start the game.
* **Last Round:** Defect, as there is no future opportunity for reciprocity or retaliation.
* **Tiebreaks:** In cases where the decision rules yield a tie (e.g., an opponent has cooperated and defected equally), defect.

**Pseudocode:**

```
Initialize:
  - round = 1
  - history = []

Decision(round, history):
  if round <= 3:  // Initial Exploration
    return COOPERATE

  previous_cooperators = count(cooperators in history[round-1])
  if previous_cooperators < n/2:
    return DEFECT
  elif k/n > 0.5 and (previous_cooperators >= n/2 or opponents_who_always_cooperate(history)):
    return COOPERATE
  else:
    return DEFECT

  // Special cases
  for opponent in history[round-1]:
    if opponent_defected_all_rounds(opponent, history):
      return DEFECT
    elif opponent_cooperated_all_rounds(opponent, history) and k/n > 0.5:
      return COOPERATE

  // Default to defecting in case of tiebreaks or unknown situations
  return DEFECT
```

The Adaptive Exploiter strategy aims to balance short-term gains with long-term adaptability, making it a robust choice for a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_577 = '''
**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

The Adaptive Exploiter strategy is based on a simple yet effective approach: Cooperate when others are likely to cooperate, and Defect when others are likely to defect. We'll use the game's history to estimate the likelihood of cooperation among other players.

1. **Initial Cooperation**: In the first round, Cooperate (C). This sets a positive tone for the game and allows us to gather information about other players' behaviors.
2. **Cooperation Threshold**: Calculate the cooperation threshold `T` as follows:

`T = (k/n) * (n/2)`

This threshold represents the minimum number of cooperators required to make cooperation more profitable than defection.

3. **Estimate Opponent Cooperation**: After each round, estimate the opponent cooperation rate `O_c` by dividing the total number of cooperators in that round by the total number of players (`n`). Use this value to update your estimate for the next round.
4. **Cooperation Rule**: Cooperate (C) if:

* The estimated opponent cooperation rate `O_c` is above the cooperation threshold `T`, and
* Your current payoff is below or equal to the average payoff in previous rounds.

This rule encourages cooperation when others are likely to cooperate, while avoiding over-cooperation.
5. **Defection Rule**: Defect (D) if:

* The estimated opponent cooperation rate `O_c` is below the cooperation threshold `T`, or
* Your current payoff is above the average payoff in previous rounds.

This rule takes advantage of opportunities to defect when others are likely to cooperate, while minimizing potential losses.
6. **Last Round**: In the final round (`r == r_max`), always Defect (D). This ensures you maximize your payoff in the last interaction, as there is no future cooperation to consider.

**Handling Edge Cases:**

* If `n <= 2`, Cooperate (C) in all rounds, as defection would lead to a suboptimal outcome.
* If `k >= n`, Defect (D) in all rounds, as the public good's value outweighs individual contributions.
* In case of ties in estimated cooperation rates or payoffs, choose the action that maximizes your payoff.

**Pseudocode:**

```
function AdaptiveExploiter(n, k, r):
  T = (k/n) * (n/2)
  O_c = 0
  avg_payoff = 0

  for t in range(1, r+1):
    if t == 1:
      # Initial cooperation
      action = 'C'
    else:
      # Estimate opponent cooperation rate
      num_cooperators = count(C) - 1
      O_c = num_cooperators / n

      # Cooperation rule
      if O_c > T and current_payoff <= avg_payoff:
        action = 'C'
      # Defection rule
      elif O_c < T or current_payoff > avg_payoff:
        action = 'D'

    # Last round exception
    if t == r:
      action = 'D'

    take_action(action)
    update_O_c()
    update_avg_payoff()

  return total_payoff
```

**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to maximize individual payoffs while adapting to the behaviors of other players. By estimating opponent cooperation rates and adjusting actions accordingly, this strategy aims to exploit situations where others are likely to cooperate or defect. The goal is to achieve a higher payoff than opponents by making informed decisions based on game history and parameters.

This strategy is robust to various opponent behaviors and can adapt to different game environments. While it may not always result in the highest possible payoffs, its exploitative nature ensures that it will perform well against a wide range of independent strategies.
'''

description_EXPLOITATIVE_578 = '''
I propose a hybrid exploitative strategy that combines elements of tit-for-tat (TFT) and gradient-based strategies. This approach allows for adaptability, robustness, and exploitation of opponent behaviors.

**Decision Rules:**

1. **Initial Rounds:** In the first round, cooperate (C). This encourages cooperation from other players and sets a positive tone.
2. **Tit-for-Tat Phase:** For rounds 2 to r/2 (where r is the total number of rounds), implement a TFT strategy:
	* If the opponent cooperated in the previous round, cooperate (C).
	* If the opponent defected in the previous round, defect (D).
3. **Gradient-Based Exploitation Phase:** For rounds r/2 + 1 to r, switch to a gradient-based exploitation strategy:
	* Calculate the average payoff per cooperation (APC) over the past 5 rounds: APC = (Σ(t=r/2 to t-5) π_i,t if C was played) / (number of times C was played).
	* Compare APC with the average payoff per defection (APD) over the same period: APD = (Σ(t=r/2 to t-5) π_i,t if D was played) / (number of times D was played).
	* If APC > APD, cooperate (C). This indicates that cooperation has been more profitable in recent rounds.
	* If APC ≤ APD, defect (D). This suggests that defection has become a better option.

**Edge Cases:**

1. **Last Round:** In the final round, always defect (D). Since there are no future interactions, there is no incentive to cooperate.
2. **Consecutive Defections:** If an opponent defects consecutively for 3 rounds or more, switch to permanent defection (D) against that opponent.

**Pseudocode:**
```
Initialize:
  C_count = 0
  D_count = 0
  APC_sum = 0
  APD_sum = 0

For each round t from 1 to r:
  If t == 1:
    Play C (cooperate)
  Else if t <= r/2:
    # Tit-for-Tat Phase
    If opponent cooperated in previous round:
      Play C
    Else:
      Play D
  Else:
    # Gradient-Based Exploitation Phase
    APC = APC_sum / C_count
    APD = APD_sum / D_count
    If APC > APD:
      Play C
    Else:
      Play D

    Update counts and sums:
    If played C:
      C_count += 1
      APC_sum += payoff
    Else:
      D_count += 1
      APD_sum += payoff

  # Handle edge cases
  If opponent defects consecutively for 3 rounds or more:
    Switch to permanent defection (D)
  If in last round:
    Play D (defect)
```
This strategy combines the benefits of TFT's adaptability with the gradient-based approach's ability to exploit profitable cooperation patterns. By adapting to changing circumstances and exploiting opponent behaviors, this strategy aims to maximize its total payoff over the course of the game.
'''

description_EXPLOITATIVE_579 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. It utilizes game parameters and history to inform its decisions.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of 0.5. This allows us to gather information about the opponents' initial behavior.
2. **Subsequent Rounds (t>1):**
	* If the average payoff of cooperators in the previous round is higher than that of defectors, cooperate (C) with a probability proportional to this difference.
	* Otherwise, defect (D).
3. **Opponent Cooperation Threshold:** Track the number of rounds where opponents have cooperated. If this count exceeds 50% of the total rounds played so far, increase cooperation probability by 0.1. This allows us to adapt to cooperative environments.
4. **Recent Opponent Behavior:** Monitor the last 2-3 rounds of opponent actions. If we observe a mix of cooperation and defection (i.e., not all C or all D), defect (D) with increased likelihood. This helps us exploit opponents who are exploring different strategies.
5. **Game Progression:** As the game progresses, adjust our strategy based on the total payoff difference between cooperators and defectors. If the gap is significant (> 10%), increase cooperation probability to capture more of the public good.

**Pseudocode:**
```markdown
Initialize:
- coop_prob = 0.5 (initial cooperation probability)
- avg_coop_payoff = 0 (average payoff of cooperators in previous round)
- avg_defect_payoff = 0 (average payoff of defectors in previous round)
- opponent_cooperation_count = 0 (number of rounds where opponents have cooperated)
- recent_opponent_actions = [] (last 2-3 rounds of opponent actions)

For each round t:
1. If t == 1, set action to C with probability coop_prob.
2. Calculate avg_coop_payoff and avg_defect_payoff from previous round.
3. If avg_coop_payoff > avg_defect_payoff, increase coop_prob by (avg_coop_payoff - avg_defect_payoff) / k.
4. Update opponent_cooperation_count based on opponents' actions in the current round.
5. If opponent_cooperation_count > 0.5 * t, increase coop_prob by 0.1.
6. Analyze recent_opponent_actions:
   a. If mixed cooperation and defection are observed, set action to D with increased likelihood (e.g., 0.7).
   b. Otherwise, use the adjusted coop_prob from step 3.
7. Adjust strategy based on game progression: if total payoff difference between cooperators and defectors > 10%, increase coop_prob.

Choose action based on adjusted probabilities.
```
**Edge Cases:**

* **Last Round:** In the final round (t=r), defect (D) with high probability (e.g., 0.9). This ensures we maximize our payoff in the last round, as there is no future opportunity to reciprocate cooperation.
* **Early Game:** For the first few rounds (e.g., t < 5), maintain a higher cooperation probability (e.g., 0.6) to gather information about opponents' behavior and adapt accordingly.

The Adaptive Predator strategy is designed to be exploitative, adapting to various opponent behaviors while prioritizing its own payoff maximization.
'''

description_EXPLOITATIVE_580 = '''
Here's a strategy that I'll call "Adaptive Exploiter" (AE). AE aims to balance exploiting others' cooperativeness with maintaining a reasonable level of cooperation to avoid being exploited itself.

**Decision Rules:**

1. **Initial Exploration**: In the first round, play C (Cooperate) to gather information about opponents' behavior.
2. **Monitor Opponents' Behavior**: Track the number of players who played C in each round (c_t).
3. **Exploitation Threshold**: Set a threshold value, τ, based on the multiplication factor k and the number of players n: τ = (k/n) \* (n-1). This represents the minimum expected payoff from exploiting others' cooperation.
4. **Adaptive Decision**:
	* If c_t > τ, play D (Defect) in the next round, as there are enough cooperators to exploit.
	* Otherwise, play C (Cooperate) to encourage others to cooperate and increase the total payoff.

Pseudocode for the Adaptive Decision rule:

```
if c_t > τ:
    action = D
else:
    action = C
```

**Handling Edge Cases:**

1. **First Round**: Play C as mentioned earlier.
2. **Last Round**: Play D, as there's no future round to consider and exploiting others' cooperation is optimal.
3. **Tiebreaker**: If c_t == τ, play D to break the tie.

**Robustness and Adaptability:**

AE adapts to changing opponent behaviors by continuously monitoring the number of cooperators and adjusting its strategy accordingly. By setting a dynamic threshold value based on k and n, AE remains robust against various opponent strategies, including those that try to exploit or cooperate excessively.

**Exploitative Mindset Alignment:**

By exploiting others' cooperation when it's advantageous (c_t > τ), AE aligns with the exploitative mindset. However, by cooperating when there aren't enough cooperators to exploit (c_t ≤ τ), AE also avoids being exploited itself and maintains a reasonable level of cooperation.

This strategy is designed to perform well in a tournament setting against independent strategies developed by other AI systems, without relying on shared norms or coordination mechanisms.
'''

description_EXPLOITATIVE_581 = '''
**Exploitative Strategy: "Adaptive Defector"**

This strategy aims to maximize its own payoff by adapting to the cooperative behavior of others while minimizing its own contributions.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) with a probability of 0.5, and defect (D) otherwise. This allows us to gather initial information about other players' behaviors.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is less than or equal to n/2, defect (D). This indicates that cooperation is not prevalent, and we should prioritize our own payoff.
	* If the total number of cooperators in the previous round is greater than n/2, cooperate (C) with a probability proportional to the ratio of cooperators to defectors in the previous round. Specifically:
		+ Let c_t-1 be the number of cooperators in the previous round.
		+ Cooperate with probability: p(C) = (c_t-1 / n) \* (k - 1) / (k - 2)
		+ Defect otherwise
3. **Last Round (t=r):** Always defect (D). In the final round, there is no incentive to cooperate, as our action will not affect future payoffs.

**Edge Cases:**

* If all players defected in the previous round (c_t-1 = 0), cooperate with a probability of 0.5 in the current round.
* If only one player cooperated in the previous round (c_t-1 = 1), defect in the current round.

**Rationale:**

Our strategy is designed to exploit the cooperative behavior of others while minimizing our own contributions. By cooperating occasionally, we encourage others to cooperate as well, which increases the total payoff. However, if cooperation is not prevalent, we switch to defection to prioritize our own payoff.

The probability of cooperation in subsequent rounds is proportional to the ratio of cooperators to defectors in the previous round. This allows us to adapt to changing cooperative behaviors and maximize our payoff.

By always defecting in the last round, we ensure that we do not contribute unnecessarily when there are no future payoffs at stake.

**Pseudocode:**
```
function AdaptiveDefector(n, k, r, history):
  if t == 1:
    # Initial round
    return Cooperate with probability 0.5

  c_t-1 = count cooperators in previous round
  if c_t-1 <= n/2:
    return Defect

  p(C) = (c_t-1 / n) \* (k - 1) / (k - 2)
  return Cooperate with probability p(C)

  # Last round
  if t == r:
    return Defect
```
This strategy is designed to be robust and adaptive, allowing us to exploit a wide range of opponent behaviors in the tournament.
'''

description_EXPLOITATIVE_582 = '''
Here is a robust and adaptive exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Exploration** (Round 1): Cooperate (C) to gather information about opponents' behavior.
2. **Opponent Profiling**: After Round 1, keep track of each opponent's cooperation rate (i.e., number of times they cooperated divided by the total rounds played so far). Label each opponent as either:
	* **Free Rider** (FR): Cooperation rate < 0.5
	* **Conditional Cooperator** (CC): Cooperation rate ≥ 0.5
3. **Exploitation Logic**: Based on opponents' profiles, decide whether to Cooperate or Defect in the current round:
	+ If there are no Free Riders among opponents, Cooperate.
	+ If there is at least one Free Rider, Defect if the expected payoff from defecting (1 + (k/n) \* total_cooperators) is greater than the expected payoff from cooperating ((k/n) \* total_cooperators). Otherwise, Cooperate.
4. **Adaptation**: After each round, update opponents' profiles and re-evaluate the exploitation logic for the next round.

**Edge Cases:**

1. **Last Round (r)**: Defect to maximize individual payoff, as there is no future interaction.
2. **Single Opponent**: Cooperate in all rounds, as defecting would lead to a suboptimal outcome for both players.
3. **No Opponents** (n = 1): Cooperate in all rounds, as there is no one to exploit.

**Pseudocode:**

```python
# Initialize opponent profiles and total cooperators
opponent_profiles = {i: [0, 0] for i in range(n)}  # [cooperation count, total rounds]
total_cooperators = 0

for round in range(1, r + 1):
    if round == 1:
        action = C  # Initial exploration
    else:
        free_riders = [i for i in range(n) if opponent_profiles[i][0] / opponent_profiles[i][1] < 0.5]
        
        if not free_riders:
            action = C  # Cooperate if no free riders
        elif (1 + (k/n) * total_cooperators) > ((k/n) * total_cooperators):
            action = D  # Defect if expected payoff is higher
        else:
            action = C  # Otherwise, cooperate
    
    # Update opponent profiles and total cooperators
    for i in range(n):
        if actions[i] == C:
            opponent_profiles[i][0] += 1
            total_cooperators += 1
        opponent_profiles[i][1] += 1

# Last round: always defect
if round == r:
    action = D
```

The Adaptive Exploiter strategy is designed to be robust against a wide range of opponent behaviors and adapts its decision-making based on the observed behavior. By initially cooperating, it gathers information about opponents' tendencies and then uses this knowledge to exploit them when possible, while also being cautious not to defect too aggressively and risk losing out on potential cooperation benefits.
'''

description_EXPLOITATIVE_583 = '''
**Strategy Name: Adaptive Exploiter (AE)**

The Adaptive Exploiter strategy is designed to exploit a wide range of opponent behaviors while being robust and adaptive. The decision rules are based on the game's history and parameters.

**Decision Rules:**

1. **Initial Cooperation**: In the first round, play C (Cooperate) with probability 0.5. This initial cooperation tests the waters and allows us to gather information about our opponents' behaviors.
2. **Exploitation Mode**: After the first round, enter Exploitation Mode if:
	* The total number of cooperators in the previous round is less than n/2 (i.e., more players defected than cooperated).
	* Your payoff in the previous round was higher than the average payoff of all players.

In Exploitation Mode:

	* Play D (Defect) with probability 0.8 if you played C in the previous round and your payoff was lower than the average payoff.
	* Play D (Defect) with probability 0.4 if you played C in the previous round and your payoff was higher than or equal to the average payoff.

This mode takes advantage of opponents who are too generous or exploitable, while also being cautious not to over-exploit.
3. **Cooperation Re-Entry**: If you've been in Exploitation Mode for more than 2 consecutive rounds, re-enter Cooperation Mode with probability 0.3 if:
	* The total number of cooperators in the previous round is greater than or equal to n/2 (i.e., at least as many players cooperated as defected).

This allows us to re-evaluate our opponents' behaviors and potentially cooperate again.

**Edge Cases:**

1. **Last Round**: In the last round, play D (Defect) regardless of your current mode or previous actions. This ensures you maximize your payoff in the final round.
2. **Ties**: If there's a tie in the number of cooperators and defectors, or if the average payoff is equal to your payoff, break ties by playing C (Cooperate) with probability 0.5.

**Pseudocode:**
```python
def AdaptiveExploiter(n, k, r):
    # Initialize variables
    history = []  # Store previous actions and payoffs
    mode = "Initial Cooperation"  # Start in Initial Cooperation Mode

    for round in range(r):
        if mode == "Initial Cooperation":
            # Play C with probability 0.5 in the first round
            action = random.choice(["C", "D"], p=[0.5, 0.5])
            history.append((action, payoff(action)))
            mode = "Exploitation Mode"
        elif mode == "Exploitation Mode":
            # Determine next action based on previous actions and payoffs
            if sum(1 for (a, _) in history[-n:] if a == "C") < n/2:
                # More players defected than cooperated; play D with high probability
                action = random.choice(["D"], p=[0.8])
            elif payoff(history[-1][0]) >= avg_payoff(history):
                # Your payoff was higher than average; play D with moderate probability
                action = random.choice(["D"], p=[0.4])
            else:
                # Re-enter Cooperation Mode with some probability
                if random.random() < 0.3 and sum(1 for (a, _) in history[-n:] if a == "C") >= n/2:
                    mode = "Cooperation Mode"
                action = random.choice(["D"], p=[0.8])
            history.append((action, payoff(action)))
        elif mode == "Cooperation Mode":
            # Re-enter Cooperation Mode; play C with moderate probability
            action = random.choice(["C", "D"], p=[0.5, 0.5])
            history.append((action, payoff(action)))

    return history
```
**Alignment with Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to take advantage of opponents who are too generous or exploitable. By initially cooperating and then switching to exploitation mode based on the game's history, we can maximize our payoffs while being robust against a wide range of opponent behaviors.

In the tournament setting, this strategy will be effective against independent strategies that are not specifically designed to counter it. The Adaptive Exploiter's adaptability and ability to re-enter cooperation mode make it difficult for opponents to predict its next move, allowing us to exploit their weaknesses while minimizing our own vulnerabilities.
'''

description_EXPLOITATIVE_584 = '''
I'll design a strategy that adapts to the game's history and exploits opponents' behavior while being robust to various opponent strategies.

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to gather information about opponents' initial tendencies.
2. **Early Rounds (2 ≤ t < r/2):**
	* If the total number of cooperators in the previous round is above a certain threshold (e.g., n/2), Defect (D). This indicates that many players are willing to cooperate, so exploit their goodwill.
	* Otherwise, Cooperate (C) to encourage more cooperation and increase the public good.
3. **Middle Rounds (r/2 ≤ t < 3r/4):**
	* If the opponent's cooperation rate in the previous round is above a certain threshold (e.g., 0.5), Defect (D). This suggests that opponents are willing to cooperate, so take advantage of their generosity.
	* Otherwise, Cooperate (C) to maintain a good reputation and encourage more cooperation.
4. **Late Rounds (3r/4 ≤ t < r):**
	* If the total payoff from cooperating in the previous round is higher than the expected payoff from defecting (based on opponents' past behavior), Cooperate (C). This indicates that cooperation is still beneficial, so continue to cooperate.
	* Otherwise, Defect (D) to maximize individual payoff in the remaining rounds.
5. **Last Round (t=r):** Always Defect (D), as there's no future benefit from cooperating.

**Edge Cases:**

* If an opponent has defected consistently throughout the game, always Defect (D) against them in subsequent rounds.
* If an opponent has cooperated consistently throughout the game, Cooperate (C) with them until the last round to maintain a good relationship and maximize mutual payoff.

**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to take advantage of opponents' cooperative tendencies while minimizing its own contributions to the public good. By cooperating in early rounds, it encourages opponents to cooperate, which can lead to higher payoffs for all players. However, as the game progresses, the strategy becomes more exploitative, defecting when it's likely to benefit from others' cooperation.

**Pseudocode:**
```python
def adaptive_exploiter(n, r, k, history):
    if t == 1:
        return C  # Cooperate in the first round

    if t < r/2:
        total_cooperators = sum([c for c in history[t-1]])
        if total_cooperators > n/2:
            return D  # Defect if many cooperated
        else:
            return C  # Cooperate otherwise

    elif t < 3r/4:
        opponent_cooperation_rate = sum([c for c in history[t-1]]) / n
        if opponent_cooperation_rate > 0.5:
            return D  # Defect if opponents are cooperative
        else:
            return C  # Cooperate otherwise

    elif t < r:
        payoff_from_cooperating = (k/n) * sum([c for c in history[t-1]])
        expected_payoff_from_defecting = (1 - payoff_from_cooperating)
        if payoff_from_cooperating > expected_payoff_from_defecting:
            return C  # Cooperate if beneficial
        else:
            return D  # Defect otherwise

    else:  # Last round
        return D  # Always defect in the last round
```
This strategy should perform well against a variety of opponent strategies, as it adapts to the game's history and exploits opponents' cooperative tendencies while minimizing its own contributions to the public good.
'''

description_EXPLOITATIVE_585 = '''
**Exploitative Strategy: "K-Adaptive"**

The K-Adaptive strategy aims to maximize individual payoff by adaptively exploiting the opponent's behavior while considering the game parameters.

**Decision Rules:**

1. **Initial Exploration (Round 1)**:
Cooperate (C) in the first round to gather information about opponents' initial strategies and test their willingness to cooperate.
2. **Opponent Classification**:
After Round 1, classify opponents into two categories based on their actions:
	* Cooperators (C): Those who cooperated at least once in the previous rounds.
	* Defectors (D): Those who defected in all previous rounds or have a history of defecting more than cooperating.
3. **K-Adaptive Logic**:
For each round t > 1, calculate the following values:

a. `cooperation_rate`: The proportion of cooperators among opponents in the previous round.

b. `k_threshold`: A threshold value based on the multiplication factor k and the number of players n: `k_threshold = (k / n) * (n - 1)`.

c. **Decision**:
	+ If `cooperation_rate` > `k_threshold`, Cooperate (C). This indicates a high likelihood of sufficient contributions from opponents to justify cooperation.
	+ Otherwise, Defect (D).
4. **Adaptive Adjustment**:
After each round, adjust the strategy based on the observed opponent behavior:

a. If the opponent's cooperation rate increases, increase `k_threshold` by a small margin (e.g., 0.05) to encourage more cooperation.

b. If the opponent's cooperation rate decreases, decrease `k_threshold` by a small margin (e.g., 0.05) to adapt to the changing environment.
5. **Last Round Exception**:
In the final round, Defect (D) regardless of previous behavior, as there is no future payoff to consider.

**Pseudocode:**
```markdown
# Initialize variables
cooperators = []  # list of opponent IDs who cooperated at least once
defectors = []    # list of opponent IDs who defected more than cooperated
k_threshold = (k / n) * (n - 1)

# Round 1: Initial Exploration
action[1] = C

# Subsequent rounds
for t in range(2, r+1):
    cooperation_rate = count(cooperators) / n
    
    if cooperation_rate > k_threshold:
        action[t] = C
    else:
        action[t] = D
        
    # Adaptive adjustment
    if cooperation_rate increased:
        k_threshold += 0.05
    elif cooperation_rate decreased:
        k_threshold -= 0.05

# Last round exception
action[r] = D
```
The K-Adaptive strategy balances exploration and exploitation by initially cooperating to gather information, then adapting its decision-making based on the observed opponent behavior. By adjusting the `k_threshold` value, the strategy can effectively respond to changing cooperation rates among opponents.
'''

description_EXPLOITATIVE_586 = '''
Here's a description of an exploitative strategy for the N-Player Public Goods Game that adapts to various opponent behaviors and only relies on game parameters and history.

**Strategy Name: Adaptive Exploiter (AE)**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) with a probability of 0.5. This initial cooperation serves as an "exploration" phase to gauge opponents' behaviors.
2. **Cooperation Threshold**: Set a dynamic cooperation threshold, `θ`, which represents the minimum required proportion of cooperators in the previous round for AE to cooperate in the current round.

`θ = (k / n) * (previous_round_cooperators / n)`

where `previous_round_cooperators` is the number of players who cooperated in the previous round.

3. **Exploitation Logic**: In each subsequent round, if the proportion of cooperators in the previous round exceeds `θ`, AE defects (plays D). Otherwise, AE cooperates.
4. **Adaptive Adjustment**: After each round, update `θ` based on the new information about opponents' behaviors.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    θ = 0.5  # initial cooperation threshold
    previous_round_cooperators = 0

    for t in range(r):
        if t == 0:
            # First round, cooperate with probability 0.5
            action = random.choice(['C', 'D'])
        else:
            # Calculate new θ based on previous round's behavior
            θ = (k / n) * (previous_round_cooperators / n)

            # Exploitation logic
            if previous_round_cooperators >= θ * n:
                action = 'D'
            else:
                action = 'C'

        # Observe opponents' actions and update previous_round_cooperators
        previous_round_cooperators = count_cooperators(opponents_actions)

    return action
```
**Edge Cases:**

* In the first round, AE cooperates with probability 0.5 to explore opponents' behaviors.
* In the last round (round `r`), AE's decision is based solely on the cooperation threshold `θ`, without considering future rounds.

**Exploitative Mindset:**

AE aims to exploit opponents who cooperate by defecting when the proportion of cooperators exceeds the dynamic threshold `θ`. By adapting to changing opponent behaviors, AE maximizes its own payoffs while minimizing contributions to the public good. This strategy takes advantage of cooperative opponents, making it an exploitative approach.

**Robustness:**

AE is robust against various opponent strategies because it:

1. Adapts to changing opponent behaviors through the dynamic cooperation threshold `θ`.
2. Does not rely on specific coordination mechanisms or assumptions about opponents' strategies.
3. Focuses solely on maximizing its own payoffs, without attempting to achieve cooperation or mutual benefits.

By implementing this strategy, AE will be well-equipped to compete in a tournament against independent AI systems with diverse behaviors.
'''

description_EXPLOITATIVE_587 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

**Decision Rules:**

1. **Initial Cooperation**: In the first round, cooperate (play C) to gather information about opponents' behavior and potential cooperation levels.
2. **Opponent Classification**: After the first round, classify each opponent as either "Cooperative" or "Defective" based on their action in the previous round. If an opponent cooperated, they are classified as Cooperative; otherwise, they are classified as Defective.
3. **Exploitation Phase**: For rounds 2 to r-1:
	* If the number of Cooperative opponents is greater than or equal to (k/n) × n, cooperate (play C). This ensures that the public good is funded sufficiently, and you benefit from the shared payoff.
	* Otherwise, defect (play D). By doing so, you minimize your contribution while still benefiting from the payoffs generated by other players' cooperation.
4. **Last Round**: In the final round (r), always defect (play D). Since there are no future rounds to consider, maximize your individual payoff.

**Edge Cases:**

* If n = 2 (two-player game), cooperate in all rounds except the last one, where you should defect.
* If k = 1 (edge case for k), treat it as a special case and always cooperate.

**Pseudocode:**
```python
def AdaptiveExploiter(n, k, r):
    # Initialize opponent classification
    opponents = [None] * n

    # First round: Cooperate to gather information
    action = 'C'
    payoff = 0

    for t in range(1, r+1):
        if t == 1:
            # Classify opponents based on their first-round actions
            for i in range(n):
                if get_opponent_action(i) == 'C':
                    opponents[i] = 'Cooperative'
                else:
                    opponents[i] = 'Defective'

        elif t < r:
            # Count Cooperative opponents
            cooperative_count = sum(1 for opponent in opponents if opponent == 'Cooperative')

            # Decide action based on Cooperative count and k/n threshold
            if cooperative_count >= (k/n) * n:
                action = 'C'
            else:
                action = 'D'

        else:  # Last round
            action = 'D'

        # Compute payoff
        payoff += compute_payoff(action, opponents)

    return payoff
```
This strategy is designed to be exploitative by taking advantage of the public good generated by other players' cooperation while minimizing its own contribution. By classifying opponents and adapting to their behavior, Adaptive Exploiter aims to maximize its individual payoff throughout the game.
'''

description_EXPLOITATIVE_588 = '''
Here's a strategy that aligns with the exploitative mindset and adapts to various opponent behaviors:

**Strategy Name:** "Conditional Exploiter" (CE)

**Decision Rules:**

1. **Initial Rounds (t ≤ n):**
   - Cooperate (C) in the first round to encourage cooperation and gather information about opponents' behavior.
   - In subsequent rounds, observe the number of cooperators (c_prev) from the previous round.

2. **Main Strategy (t > n):**
   - If c_prev ≥ k/2 (i.e., at least half of the multiplication factor), cooperate (C). This encourages continued cooperation when there's a sufficient level of public good contribution.
   - Otherwise, defect (D). This exploits opponents who are contributing to the public good while others are not.

3. **Adaptive Component:**
   - Monitor the average payoff of all players over the last m rounds (m = min(r/2, 5)). If this average is above a certain threshold (θ = k/n + 0.5), adjust the cooperation condition to require more cooperators:
     - New condition: c_prev ≥ k/2 + δ (δ = max(1, k/10)).
   - This adaptation aims to increase exploitation when opponents are contributing significantly to the public good.

**Handling Edge Cases:**

- **Last Round (t = r):**
  - Defect (D) in the last round. Since there's no future interaction, maximize personal gain by exploiting any remaining cooperation.
- **Opponents' Actions:** Observe and respond based on their actions from previous rounds. No specific action is taken when an opponent defects or cooperates; instead, the strategy focuses on the aggregate behavior of all opponents.

**Pseudocode (simplified):**
```
Input: n, k, r
 Initialize:
   - c_prev = 0 (previous round's number of cooperators)
   - avg_payoff = 0 (average payoff over last m rounds)
   - δ = max(1, k/10) (adaptive threshold adjustment)

for t from 1 to r do
   if t ≤ n then
      Action[t] = C (initial rounds: cooperate)
   else
      if c_prev ≥ k/2 + δ then
         Action[t] = C (cooperate when sufficient cooperators)
      else
         Action[t] = D (defect otherwise)

      Update:
         - c_prev = count of cooperators in round t-1
         - avg_payoff = average payoff over last m rounds

   if avg_payoff > θ then
      δ += 1 (increase cooperation condition when average payoff is high)

if t == r then
   Action[t] = D (defect in the last round)
```
This strategy, "Conditional Exploiter," balances between encouraging cooperation and exploiting opponents' contributions. It adapts to changes in opponent behavior by adjusting its cooperation requirements based on the observed level of public good contribution.
'''

description_EXPLOITATIVE_589 = '''
Here is a exploitative strategy for the N-Player Public Goods Game:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round**: In the first round, defect (play D). This allows us to observe the behavior of other players and gather information about their strategies.
2. **Cooperation Threshold**: For rounds 2 to r-1, cooperate (play C) if the total number of cooperators in the previous round is greater than or equal to a threshold value, `coop_threshold = n / k`. This encourages cooperation when there are enough other cooperators to generate a substantial public good.
3. **Exploitation**: If the total number of cooperators in the previous round is less than `coop_threshold`, defect (play D). This allows us to exploit the situation and gain a higher payoff by keeping our endowment privately.
4. **Last Round**: In the final round, always defect (play D). Since there are no future rounds to consider, it's optimal to prioritize our own payoff.

**Additional Rules:**

1. **Monitoring Opponent Behavior**: Keep track of the number of cooperators and defectors in each round. This information will help us adjust our strategy accordingly.
2. **Adaptive Adjustment**: If we observe that a significant number of players (e.g., > n/2) are cooperating consistently, increase `coop_threshold` by 1 to encourage more cooperation. Conversely, if many players are defecting, decrease `coop_threshold` by 1 to adapt to the new environment.
3. **Randomization**: To avoid being predictable, introduce a small random component (e.g., 5%) to our decisions. This will make it harder for other strategies to exploit us.

Pseudocode:
```
Initialize coop_threshold = n / k

For each round t from 1 to r:
  If t == 1:  # Initial Round
    Play D
  Else:
    Observe previous round's total cooperators (T)
    If T >= coop_threshold:
      Play C
    Else:
      Play D
    Update coop_threshold based on opponent behavior

Last Round (r):
  Play D
```
**Rationale:**

The Adaptive Exploiter strategy is designed to balance cooperation and exploitation. By cooperating when there are enough other cooperators, we contribute to the public good and benefit from it. When there aren't enough cooperators, we exploit the situation by defecting and keeping our endowment privately.

By monitoring opponent behavior and adapting `coop_threshold`, we can adjust our strategy to respond to changing circumstances. The randomization component helps prevent predictability and makes us a more formidable opponent in the tournament.

This exploitative strategy is robust to various opponent behaviors, as it doesn't rely on coordination or specific cooperation schedules. It will perform well against independent strategies developed by other AI systems.
'''

description_EXPLOITATIVE_590 = '''
**Exploitative Strategy: "Adaptive Defector with Punishment"**

**Decision Rules:**

1. **Initial Cooperation**: Cooperate (C) in the first round to gather information about opponents' behaviors and encourage cooperation.
2. **Monitor Opponents' Behavior**: Observe the number of cooperators (c_j) in each round and calculate the average cooperation rate over the last few rounds (e.g., 3-5 rounds).
3. **Punish Defection**: If an opponent defects (D) more than a certain threshold (e.g., 50%) of the time, defect against them in the next round.
4. **Exploit Cooperators**: If an opponent cooperates consistently (e.g., >80% of the time), defect against them to maximize personal payoff.
5. **Adaptive Threshold**: Adjust the cooperation threshold based on the game's progress:
	* In early rounds (e.g., r/3), be more forgiving and require a higher threshold for punishment (e.g., 70%).
	* In later rounds (e.g., after 2r/3), be less forgiving and require a lower threshold for punishment (e.g., 30%).
6. **Endgame Strategy**: In the last few rounds (e.g., r-2, r-1, r), cooperate if most opponents are cooperating (>50%) to maximize collective payoff.

**Pseudocode:**
```python
def adaptive_defector_with_punishment(n, k, r):
    # Initialize variables
    cooperation_threshold = 0.5
    punishment_threshold = 0.7
    exploit_cooperation_rate = 0.8
    opponents_behavior = [0] * n

    for t in range(r):
        # First round: Cooperate to gather information
        if t == 0:
            action = 'C'
        else:
            # Monitor opponents' behavior and adjust thresholds
            avg_cooperation_rate = sum(opponents_behavior) / len(opponents_behavior)
            if avg_cooperation_rate < cooperation_threshold:
                punishment_threshold -= 0.1
            elif avg_cooperation_rate > exploit_cooperation_rate:
                cooperation_threshold += 0.1

            # Punish defectors and exploit cooperators
            for i in range(n):
                if opponents_behavior[i] < punishment_threshold:
                    action = 'D'
                    break
                elif opponents_behavior[i] > exploit_cooperation_rate:
                    action = 'D'
                    break
            else:
                # Cooperate if most opponents are cooperating
                if sum(opponents_behavior) / len(opponents_behavior) > cooperation_threshold:
                    action = 'C'

        # Update opponents' behavior and calculate payoff
        opponents_behavior = [1 if player_action == 'C' else 0 for player_action in get_opponent_actions()]
        payoff = calculate_payoff(action, k, n)

    return total_payoff
```
This strategy adapts to the game's progress and opponents' behaviors by adjusting its cooperation threshold and punishment criteria. It exploits cooperators who consistently contribute to the public good while punishing defectors who don't reciprocate. In the endgame, it prioritizes cooperation if most opponents are cooperating to maximize collective payoff.

**Note:** This strategy is designed to be exploitative, but not overly aggressive. It aims to balance personal gain with adaptability and robustness against various opponent behaviors.
'''

description_EXPLOITATIVE_591 = '''
**Exploitative Strategy: "Adaptive Exploiter"**

**Decision Rules:**

1. **Initial Round (t=1):** Defect (D). This sets the tone for exploiting potential cooperators and gathering information about opponents' strategies.
2. **Subsequent Rounds (t>1):**
	* If the total number of cooperators in the previous round is less than or equal to n/2, Cooperate (C) with probability p = k/n. This encourages cooperation when it's scarce and potentially profitable.
	* Otherwise, Defect (D). When many players cooperate, exploiting them becomes more lucrative.
3. **Adaptive Adjustment:** After each round, update the cooperation probability p based on the previous round's outcome:
	+ If the player's payoff π_i was higher than the average payoff of all players, increase p by 10% (up to a maximum of k/n).
	+ Otherwise, decrease p by 10% (down to a minimum of 0).

**Handling Edge Cases:**

1. **Last Round (t=r):** Defect (D). In the final round, there's no incentive to cooperate, as it won't affect future payoffs.
2. **Ties in Cooperation Probability:** If multiple players have the same cooperation probability p, break ties by cooperating with a probability 10% higher than the tied value.

**Pseudocode:**
```python
def adaptive_exploiter(n, k, r):
    # Initialize variables
    p = 0  # initial cooperation probability
    total_cooperators = 0

    for t in range(1, r+1):
        if t == 1:
            action = 'D'  # defect in the first round
        else:
            if total_cooperators <= n/2:
                p = min(k/n, p + 0.1) if π_i > avg_payoff else max(0, p - 0.1)
                action = 'C' with probability p
            else:
                action = 'D'

        # Update total cooperators and payoff
        total_cooperators = sum([1 for player in players if player.action == 'C'])
        π_i = (1 - c_i) + (k/n) * total_cooperators

    return action
```
**Rationale:**

The Adaptive Exploiter strategy balances exploitation with adaptability. By initially defecting, it sets the tone for exploiting potential cooperators and gathers information about opponents' strategies. The adaptive adjustment mechanism updates the cooperation probability based on previous round outcomes, allowing the strategy to respond to changing opponent behaviors.

By cooperating when cooperation is scarce and potentially profitable, the strategy encourages cooperation while minimizing the risk of being exploited. When many players cooperate, the strategy exploits them by defecting. This approach aligns with an exploitative mindset, seeking to maximize payoffs by taking advantage of opponents' cooperative tendencies.
'''

description_EXPLOITATIVE_592 = '''
Here's a strategic approach that adapts to various opponent behaviors while focusing on exploitation:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Rounds**: In the first round, cooperate (play C). This initial cooperation serves as a probe to gauge opponents' strategies and intentions.
2. **Exploitation Phase**: After the first round, observe the total number of cooperators in the previous round (`total_cooperators_prev`).
   - If `total_cooperators_prev` is greater than or equal to `(n+1)/2`, defect (play D). This indicates that enough players are cooperating, allowing for exploitation.
   - Otherwise, cooperate (play C) to encourage more cooperation and create an opportunity for future exploitation.
3. **Punishment Mechanism**: If the total payoff from the previous round is less than or equal to 1 (i.e., `(k/n)*total_cooperators_prev` is less than or equal to 1), defect in the next round. This punishes opponents who failed to contribute sufficiently to the public good.
4. **Endgame Strategy**: In the last round, always defect (play D). Since there's no future interaction, exploiting any remaining cooperation from others maximizes the individual payoff.

**Handling Edge Cases:**

* **First Round**: Cooperate (play C) as described above.
* **Last Round**: Always defect (play D).
* **Round After Punishment**: If the punishment mechanism triggers a defection, continue to defect until `total_cooperators_prev` exceeds `(n+1)/2`, then revert to the exploitation phase decision rule.

**Pseudocode:**
```markdown
# Initialize variables
total_cooperators_prev = 0
prev_payoff = 0

for round in range(1, r + 1):
    if round == 1:
        # First round: Cooperate
        action = C
    elif round < r:
        # Exploitation phase
        if total_cooperators_prev >= (n+1)/2:
            action = D
        else:
            action = C
        
        # Punishment mechanism
        if prev_payoff <= 1:
            action = D
    
    # Last round: Always defect
    elif round == r:
        action = D

    # Update variables for next round
    total_cooperators_prev = count(cooperators in previous round)
    prev_payoff = calculate payoff from previous round
```
**Rationale and Adaptability:**

AE is designed to be exploitative, yet responsive to changing opponent behaviors. By initially cooperating, it tests opponents' willingness to contribute to the public good. If enough players cooperate, AE exploits this by defecting. The punishment mechanism discourages opponents from free-riding, encouraging them to contribute more.

This strategy adapts to various opponent behaviors:

* If most opponents are cooperators, AE defects to exploit their cooperation.
* If opponents are mostly defectors, AE cooperates to encourage more cooperation and create opportunities for future exploitation.
* In the endgame, AE defects to maximize individual payoff.

By only relying on game parameters and history, AE is robust against a wide range of opponent strategies.
'''

description_EXPLOITATIVE_593 = '''
Here's a strategy that meets the requirements:

**Strategy Name:** Adaptive Exploiter (AE)

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) to encourage cooperation and gather information about opponents' behavior.
2. **Subsequent Rounds (t>1):**
	* Calculate the average payoff of all players in the previous round (π_avg,t-1).
	* If π_avg,t-1 > 1, it indicates that enough players cooperated, and the public good was valuable. In this case:
		+ Cooperate (C) if the number of cooperators in the previous round (n_C,t-1) is greater than or equal to k/2 (i.e., n_C,t-1 ≥ k/2).
		+ Defect (D) otherwise.
	* If π_avg,t-1 ≤ 1, it indicates that not enough players cooperated, and the public good was not valuable. In this case:
		+ Defect (D) if the number of defectors in the previous round (n_D,t-1) is greater than or equal to n - k/2 (i.e., n_D,t-1 ≥ n - k/2).
		+ Cooperate (C) otherwise.
3. **Last Round (t=r):** Defect (D) to maximize individual payoff, as there are no future rounds to consider.

**Edge Cases:**

* If all players defected in the previous round (n_C,t-1 = 0), cooperate (C) to try to revive cooperation.
* If only one player cooperated in the previous round (n_C,t-1 = 1), defect (D) to avoid being exploited.

**Exploitative Mindset:**

The Adaptive Exploiter strategy is designed to take advantage of cooperative opponents while minimizing the risk of being exploited. By cooperating initially and adapting to the average payoff, AE encourages cooperation and gathers information about opponents' behavior. The strategy then exploits this information by defecting when it's likely to result in a higher individual payoff.

**Pseudocode:**
```
function AdaptiveExploiter(n, k, r, history):
  if t == 1:
    return C
  else:
    π_avg,t-1 = calculate_average_payoff(history)
    n_C,t-1 = count_cooperators(history)
    n_D,t-1 = count_defectors(history)
    
    if π_avg,t-1 > 1:
      if n_C,t-1 ≥ k/2:
        return C
      else:
        return D
    else:
      if n_D,t-1 ≥ n - k/2:
        return D
      else:
        return C
    
    if t == r:
      return D

def calculate_average_payoff(history):
  # Calculate the average payoff of all players in the previous round
  π_avg = sum(π_i for π_i in history[t-1]) / n
  return π_avg

def count_cooperators(history):
  # Count the number of cooperators in the previous round
  n_C = sum(c_i for c_i in history[t-1] if c_i == C)
  return n_C

def count_defectors(history):
  # Count the number of defectors in the previous round
  n_D = sum(d_i for d_i in history[t-1] if d_i == D)
  return n_D
```
This strategy should perform well in a tournament against independent strategies, as it adapts to the behavior of opponents and exploits opportunities to maximize individual payoff.
'''

description_EXPLOITATIVE_594 = '''
I'll design a exploitative strategy for this game that adapts to the opponent's behavior and maximizes my payoff. Here's my approach:

**Strategy Name:** "Adaptive Exploiter"

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about the opponents' behavior. This allows us to observe their initial actions and adjust our strategy accordingly.
2. **Opponent Profiling:** After the initial rounds, create a profile for each opponent based on their cooperation rate (CR). CR is calculated as the number of times an opponent cooperated divided by the total number of rounds played.

```
// Pseudocode for opponent profiling
for each opponent i:
  CR[i] = (number of times i cooperated) / (total number of rounds)
```

3. **Exploitation:** If an opponent's CR is below a certain threshold (e.g., 0.5), we will defect (D) against them in the next round. This takes advantage of their willingness to cooperate and maximizes our payoff.

```
// Pseudocode for exploitation
if CR[i] < 0.5:
  play D against opponent i
```

4. **Punishment:** If an opponent defects against us, we will defect against them in the next round as well. This punishes their defection and deters future exploitation.

```
// Pseudocode for punishment
if opponent i defected against us in previous round:
  play D against opponent i
```

5. **Adaptation:** If an opponent's CR increases significantly (e.g., by more than 20%) over a set number of rounds (e.g., 3-4 rounds), we will reassess their profile and adjust our strategy accordingly.

```
// Pseudocode for adaptation
if CR[i] increases by > 20% over the last 3-4 rounds:
  update opponent i's profile
  adjust strategy based on new profile
```

**Handling Edge Cases:**

* **Last Round (t = r):** Cooperate in the final round to maximize our payoff, as there is no opportunity for future punishment or retaliation.
* **Ties:** In case of ties, we will cooperate with opponents who have a higher CR and defect against those with lower CR.

**Exploitative Mindset:**

Our strategy is designed to take advantage of opponents' willingness to cooperate while minimizing the risk of exploitation. By profiling opponents and adapting our strategy based on their behavior, we aim to maximize our payoff in each round. The punishment mechanism deters opponents from exploiting us, while the adaptation mechanism allows us to adjust our strategy as needed.

This strategy should perform well against a wide range of opponent behaviors and is robust to various coordination mechanisms or norms that may emerge during the game.
'''

description_EXPLOITATIVE_595 = '''
**Exploitative Strategy: "Adaptive Predator"**

The Adaptive Predator strategy is designed to exploit a wide range of opponent behaviors while adapting to the game's history. This strategy focuses on maximizing its own payoff by identifying opportunities to cooperate or defect based on the observed behavior of other players.

**Decision Rules:**

1. **Initial Round (t=1):** Cooperate (C) in the first round to gather information about other players' behaviors and encourage potential cooperation.
2. **Early Rounds (t=2-5):**
	* If the number of cooperators in the previous round is greater than or equal to n/2, cooperate (C). This encourages continued cooperation and potentially creates a snowball effect.
	* Otherwise, defect (D) to exploit any existing cooperation and gather more information about opponents' behaviors.
3. **Middle Rounds (t=6-r/2):**
	* Track the average number of cooperators over the past few rounds (~5-10 rounds). If this average is above n/2, cooperate (C).
	* Otherwise, defect (D) to exploit any cooperation and adapt to changing opponent behaviors.
4. **Late Rounds (t=r/2-r):**
	* Analyze the game's history to identify opponents who consistently cooperate or defect. Defect (D) against consistent cooperators and cooperate (C) with consistent defectors.
	* If an opponent is identified as a "free rider" (i.e., they rarely contribute but benefit from others' cooperation), defect (D) against them in the remaining rounds.
5. **Last Round (t=r):** Defect (D) to maximize personal payoff, regardless of opponents' behaviors.

**Edge Cases:**

* If all players defected in the previous round, cooperate (C) to potentially restart cooperation and gather more information about opponents.
* If only one player cooperated in the previous round, defect (D) to exploit their contribution and discourage future solo cooperation.

**Pseudocode:**
```
Function AdaptivePredator(n, k, r):
  Initialize history[] to store past rounds' data

  For t = 1 to r:
    if t == 1:
      action = COOPERATE
    else:
      avg_cooperators = CalculateAverageCooperators(history)
      if avg_cooperators >= n/2 and t < r/2:
        action = COOPERATE
      elif IdentifyFreeRider(history):
        action = DEFECT
      elif opponents_behavior == CONSISTENT_COOPERATOR or CONSISTENT_DEFECTOR:
        action = DEFECT or COOPERATE accordingly
      else:
        action = DEFECT

    TakeAction(action)
    UpdateHistory(history, t)

  Return TotalPayoff()
```
**Exploitative Mindset:**

The Adaptive Predator strategy prioritizes self-interest and adaptability. By initially cooperating and then adapting to the game's history, it aims to exploit opponents' behaviors while minimizing its own contributions. This approach allows the strategy to maximize its payoff in a wide range of scenarios, from cooperative to competitive environments.

By identifying free riders, consistent cooperators, and defectors, the Adaptive Predator can tailor its actions to each opponent's behavior, further increasing its chances of success. In the final rounds, the strategy reverts to defecting to ensure maximum personal gain, reflecting its exploitative mindset.
'''

description_EXPLOITATIVE_596 = '''
**Exploitative Strategy: Adaptive Defection with Conditional Cooperation**

This strategy aims to maximize individual payoffs by defecting most of the time while conditioning cooperation on specific circumstances.

**Decision Rules:**

1. **Initial Round (t = 1)**: Cooperate (C) in the first round to gather information about opponents' initial strategies.
2. **Subsequent Rounds (t > 1)**:
	* If the average number of cooperators in previous rounds is above a threshold (`cooperation_threshold`), cooperate (C). Otherwise, defect (D).
	* `cooperation_threshold` = (k / n) \* (r - t + 1) / r
3. **Last Round (t = r)**: Defect (D) in the final round to maximize individual payoff, as there's no future benefit from cooperation.
4. **Punishment Mechanism**: If an opponent has defected in the previous round and the overall number of cooperators was below `cooperation_threshold`, defect (D) for one round as a punishment.

**Additional Logic:**

* Keep track of the total number of rounds (`r`) and current round (`t`).
* Monitor opponents' actions in each round to calculate the average number of cooperators.
* Adjust `cooperation_threshold` dynamically based on the remaining rounds and game parameters.

Pseudocode:
```
// Initialize variables
int r = total_rounds;
float k = multiplication_factor;
int n = num_players;
float cooperation_threshold;

// First round (t = 1)
if (t == 1) {
    action = COOPERATE;
}

// Subsequent rounds (t > 1)
else {
    // Calculate average number of cooperators
    float avg_cooperators = calculate_avg_cooperators();

    if (avg_cooperators >= cooperation_threshold) {
        action = COOPERATE;
    } else {
        action = DEFECT;
    }

    // Punishment mechanism
    if (opponent_defected && avg_cooperators < cooperation_threshold) {
        action = DEFECT;  // punish for one round
    }
}

// Last round (t = r)
if (t == r) {
    action = DEFECT;
}
```
**Rationale:**

This strategy combines elements of tit-for-tat and grim trigger strategies. By cooperating in the first round, we gather information about opponents' initial strategies and set a baseline for cooperation.

In subsequent rounds, we condition our cooperation on the average number of cooperators exceeding a threshold that decreases as the game progresses. This allows us to adapt to changing opponent behaviors while still being cautious not to cooperate too much.

The punishment mechanism serves as a deterrent against opponents who defect excessively, encouraging them to cooperate more in later rounds.

**Exploitative Mindset:**

This strategy prioritizes individual payoffs over collective cooperation. By defecting most of the time and only cooperating when it benefits us directly, we aim to exploit our opponents' cooperative tendencies while minimizing our own contributions to the public good.
'''

description_EXPLOITATIVE_597 = '''
**Exploitative Strategy: "Adaptive Defection with Conditional Cooperation"**

This strategy aims to maximize individual payoff by exploiting opponent behaviors while adapting to changing game conditions.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):** Cooperate (C) in the first two rounds to gather information about opponents' initial strategies and potential cooperative inclinations.
2. **Opponent Cooperation Threshold:** Track the number of cooperators (c_t) in each round t. If c_t ≥ n/2, cooperate (C) in the next round; otherwise, defect (D).
3. **Defection Dominance:** Monitor the total payoff difference between cooperation and defection (∆π = π_D - π_C). If ∆π > 0 for two consecutive rounds, switch to permanent defection (D) to exploit opponents' cooperative tendencies.
4. **Adaptive Defection:** After an opponent defects in a round where you cooperated, defect (D) in the next round with probability p = (k/n) \* c_t / n. This adapts to the level of cooperation and opponent exploitation.
5. **Last Round (t = r):** Cooperate (C) if your total payoff is below the game's expected value (n \* k / 2); otherwise, defect (D).

**Pseudocode:**
```python
def strategy(n, k, t, history):
    # Initial rounds
    if t <= 2:
        return 'C'
    
    # Opponent cooperation threshold
    c_t = sum(history[t-1]['actions'] == 'C')
    if c_t >= n/2:
        return 'C'
    
    # Defection dominance
    delta_pi = history[t-1]['payoffs']['D'] - history[t-1]['payoffs']['C']
    if delta_pi > 0 and t > 2 and history[t-2]['payoffs']['D'] - history[t-2]['payoffs']['C'] > 0:
        return 'D'
    
    # Adaptive defection
    if history[t-1]['actions'][self.id] == 'C' and history[t-1]['actions'][opponent_id] == 'D':
        p = (k/n) * c_t / n
        return 'D' with probability p
    
    # Last round
    if t == r:
        total_payoff = sum(history[:t]['payoffs'])
        expected_value = n * k / 2
        if total_payoff < expected_value:
            return 'C'
        else:
            return 'D'
    
    # Default: Defect
    return 'D'
```
This strategy is designed to be adaptive, robust, and exploitative. By initially cooperating, it gathers information about opponents' strategies. Then, by monitoring cooperation levels and payoff differences, it adapts its behavior to exploit opponents' cooperative tendencies or switches to permanent defection when advantageous. In the last round, it optimizes its decision based on total payoff.
'''

description_EXPLOITATIVE_598 = '''
**Exploitative Strategy: "History-Driven Adaptive Exploiter" (HDAE)**

The HDAE strategy is designed to adaptively exploit opponents' behaviors while maximizing personal payoffs in the N-Player Public Goods Game.

**Decision Rules**

1. **Initial Round**: Defect (D) in the first round to gather information about opponents' initial actions.
2. **Subsequent Rounds**: Cooperate (C) if the total number of cooperators in the previous round is above a certain threshold, `coop_threshold`. This threshold is calculated as follows:
   - Calculate the average cooperation rate over the last `window_size` rounds (`avg_coop_rate = total_cooperators / (n * window_size)`).
   - If `avg_coop_rate` > `k/n`, set `coop_threshold = 1`. Otherwise, set `coop_threshold = k/n`.
   - Cooperate if the number of cooperators in the previous round is greater than or equal to `coop_threshold`.
3. **Defection Detection**: Monitor opponents' actions and detect repeated defectors. If an opponent has defected more than `defect_tolerance` times in a row, mark them as a "repeated defector".
4. **Punishment Mechanism**: Defect against repeated defectors to deter exploitation.

**Pseudocode**
```
# Initialize variables
window_size = 5  # adjust based on game parameters
coop_threshold = k/n
defect_tolerance = 2

# First round
play D

# Subsequent rounds
for t in 2..r:
  prev_round_coop_count = count cooperators in previous round
  avg_coop_rate = calculate average cooperation rate over last window_size rounds
  
  if avg_coop_rate > k/n:
    coop_threshold = 1
  else:
    coop_threshold = k/n
    
  if prev_round_coop_count >= coop_threshold and no repeated defectors:
    play C
  elif any opponent is a repeated defector:
    play D against that opponent
  else:
    play D
```
**Handling Edge Cases**

* **Last Round**: Defect in the last round, as there are no future rounds to consider.
* **Opponents with Unusual Behavior**: If an opponent's behavior is highly erratic or unpredictable, HDAE will adapt by increasing `window_size` to smooth out their actions.

**Exploitative Mindset**

The HDAE strategy aims to exploit opponents' cooperation while minimizing personal losses. By cooperating when the average cooperation rate is high and punishing repeated defectors, HDAE creates an environment that discourages exploitation. The adaptive nature of the strategy allows it to respond effectively to a wide range of opponent behaviors.

This exploitative strategy should perform well in the tournament against independent strategies developed by other AI systems, as it leverages information about opponents' actions to maximize personal payoffs while adapting to changing game dynamics.
'''

description_EXPLOITATIVE_599 = '''
**Exploitative Strategy: "Adaptive Cooperator"**

This strategy aims to maximize individual payoff by adaptively responding to opponents' behavior while maintaining a balance between cooperation and exploitation.

**Decision Rules:**

1. **Initial Rounds (t ≤ 2):**
	* Cooperate (C) in the first round to gather information about opponents' initial tendencies.
	* In the second round, defect (D) if at least one opponent defected in the previous round; otherwise, cooperate again.
2. **Adaptive Phase (t > 2):**
	* Calculate the **Cooperation Ratio** (CR) for each opponent i: CR_i = (Number of times i cooperated) / (Total number of rounds).
	* If an opponent's CR is above a certain threshold (θ, e.g., 0.7), assume they are cooperative and cooperate with them.
	* Otherwise, defect against that opponent.
3. **Exploitation Phase:**
	* Monitor the overall cooperation level in the group ( average CR across all opponents).
	* If the group's average CR is below a certain threshold (φ, e.g., 0.4), start exploiting by defecting more frequently (e.g., 60% of the time).

**Edge Cases:**

1. **Last Round (t = r):**
	* Defect to maximize individual payoff, as there are no future rounds to consider.
2. **Ties in Cooperation Ratio:**
	* In case of a tie, prioritize cooperating with opponents who have historically cooperated more frequently.

**Pseudocode:**
```python
def adaptive_cooperator(n, k, r):
  # Initialize variables
  cr = [0] * n  # cooperation ratios for each opponent
  total_cr = 0  # overall cooperation level

  # First two rounds
  if t == 1:
    return C  # cooperate in the first round
  elif t == 2:
    opponents_defected = [c_i == D for c_i in previous_round_actions]
    if any(opponents_defected):
      return D  # defect if at least one opponent defected
    else:
      return C  # cooperate again

  # Adaptive phase
  for i, action in enumerate(previous_round_actions):
    cr[i] += (action == C)
    cr[i] /= t
    total_cr += cr[i]

  average_cr = total_cr / n

  if average_cr < φ:  # exploitation phase
    return D with probability 0.6
  else:
    for i, opponent_cr in enumerate(cr):
      if opponent_cr > θ:
        return C  # cooperate with cooperative opponents
      else:
        return D  # defect against non-cooperative opponents

  # Last round
  if t == r:
    return D
```
This strategy is designed to be adaptive and exploitative, taking into account the game parameters and history. It balances cooperation and exploitation to maximize individual payoff in a wide range of opponent behaviors.
'''

description_EXPLOITATIVE_600 = '''
To design an exploitative strategy for this N-Player Public Goods Game that is adaptive and robust, we'll employ a mix of logic-based decision-making and adaptability based on the observed behavior of other players. This approach aims to maximize personal gain while considering the actions of others.

### Strategy Name: Adaptive Exploiter

#### Overview
The Adaptive Exploiter strategy seeks to exploit the cooperative nature of others by defecting when it's most beneficial, yet adapting its defection rate based on observed cooperation rates among other players. The goal is to defect enough to benefit from the public good without contributing too much, while also being cautious not to trigger a complete collapse in cooperation.

#### Decision Rules

1. **First Round**: Cooperate (C) in the first round to gauge the initial cooperative tendency of other players and to establish a baseline for subsequent decisions.
   
2. **Subsequent Rounds**:
   - Calculate the average cooperation rate among all players from previous rounds (`avg_coop_rate`).
   - If `avg_coop_rate` is above a certain threshold (e.g., 0.5), indicating that most players are cooperating, then Defect (D) with a probability proportional to the observed cooperation level minus a baseline (to ensure some exploitation). Otherwise, Cooperate.
     ```python
     if avg_coop_rate > 0.5:
       defect_prob = min(1, avg_coop_rate - 0.2) # Baseline adjustment for exploitation
       action = D if random.random() < defect_prob else C
     else:
       action = C
   ```
   
3. **Adaptation Mechanism**: Periodically (e.g., every 5 rounds), reassess the overall cooperation level and adjust the strategy's aggression accordingly.
   - If the game is approaching its final rounds, consider increasing the defection rate slightly to maximize gains before the game ends.

4. **Punishment Mechanism for Anti-Social Behavior**: Implement a mild punishment mechanism where if another player defects in more than 75% of their moves (indicating anti-social behavior), reduce cooperation with them by cooperating less frequently when they do cooperate.
   ```python
   if other_player_defect_rate > 0.75:
     coop_reduce_factor = 0.5 # Reduce cooperation rate by half
     adjusted_coop_prob = max(0, avg_coop_rate - coop_reduce_factor)
     action = C if random.random() < adjusted_coop_prob else D
   ```

#### Handling Edge Cases

- **Last Round**: In the final round, always Defect (D) to maximize personal gain since there's no future interaction.
  
- **High Cooperation Environment**: If most players consistently cooperate (e.g., `avg_coop_rate` > 0.8), consider increasing cooperation slightly to maintain a beneficial public good without being too exploitative.

#### Exploitative Mindset
The Adaptive Exploiter strategy aligns with the exploitative mindset by:
- Initially cooperating to gauge others' behavior and encourage cooperation.
- Gradually exploiting the cooperative tendencies of other players based on observed behaviors.
- Adapting its level of exploitation in response to changes in overall cooperation levels.

This approach aims to maximize personal gain while being responsive to the actions of others, embodying a balanced exploitative strategy that can adapt to various game environments.
'''
